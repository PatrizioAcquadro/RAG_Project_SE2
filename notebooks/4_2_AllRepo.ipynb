{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNpkTgRYJckA"
      },
      "source": [
        "## Section 1: Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6JT4FlEJSQK",
        "outputId": "5c11e4f3-687e-41fc-c83c-d16124cfabab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted successfully.\n"
          ]
        }
      ],
      "source": [
        "# Import the 'drive' module from google.colab library.\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive at the '/content/drive' path in the Colab filesystem.\n",
        "# Requires user authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Confirm successful mounting.\n",
        "print(\"Drive mounted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNe6W1jUJo2g",
        "outputId": "0c8cbae1-0476-44e6-a94c-b9750e919f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing libraries...\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank-bm25) (2.0.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.2\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m188.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m198.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m196.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m544.2/544.2 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17/17\u001b[0m [bitsandbytes]\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m134.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5/5\u001b[0m [datasets]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Libraries installed (or updated) successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing libraries...\")\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# -q ensures minimal output (quiet installation)\n",
        "!pip install rank-bm25\n",
        "# update pip\n",
        "!pip install --upgrade pip -q\n",
        "\n",
        "# install bm25 (worked already, but no harm)\n",
        "!pip install -q rank-bm25\n",
        "\n",
        "!pip install -q \\\n",
        "  \"transformers\" \\\n",
        "  \"datasets\" \\\n",
        "  \"torch\" \\\n",
        "  \"accelerate\" \\\n",
        "  \"bitsandbytes\" \\\n",
        "  \"huggingface_hub\" \\\n",
        "  \"sacrebleu>=2.0.0\" \\\n",
        "  \"codebleu\" \\\n",
        "  \"tree-sitter-python\"\n",
        "  #\"fsspec==2024.12.0\" \\\n",
        "\n",
        "# Upgrade to a good combination ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "!pip install -qU \"datasets>=2.19.0\" \"fsspec>=2024.3.0\"  \\\n",
        "                 \"huggingface_hub>=0.22.2\"  \"aiohttp\"\n",
        "\n",
        "\n",
        "import os\n",
        "import time # for the delay before nvidia-smi\n",
        "import warnings # for non-critical warnings\n",
        "import shutil\n",
        "import tarfile # for .tar archives\n",
        "import ast\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import textwrap # for snipper preview\n",
        "import traceback\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# from typing import List, Optional # For type hinting (optional)\n",
        "# from transformers import PreTrainedTokenizerBase # For type hinting (optional)\n",
        "\n",
        "from datasets import load_dataset, DownloadMode\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub.utils import EntryNotFoundError\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm  # for progress bars (optional)\n",
        "    USE_TQDM = True\n",
        "except ImportError:\n",
        "    USE_TQDM = False\n",
        "    print(\"Library 'tqdm' not found, progress bar will not be shown.\")\n",
        "    print(\"You can install it with: !pip install -q tqdm\")\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "print(\"\\nLibraries installed (or updated) successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prompt Templates**"
      ],
      "metadata": {
        "id": "jYTrT48hEbrS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45yJQWmheb6N"
      },
      "outputs": [],
      "source": [
        "# Prompt templates - version 1\n",
        "\n",
        "def build_baseline_prompt_v1(instruction: str) -> str:\n",
        "    \"\"\"\n",
        "    Baseline: only the task, then a '### Code:' marker to start generation.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "### Task:\n",
        "{instruction.strip()}\n",
        "\n",
        "### Code:\n",
        "\"\"\"\n",
        "\n",
        "def build_rag_prompt_v1(instruction: str, retrieved: str) -> str:\n",
        "    \"\"\"\n",
        "    RAG: first show retrieved examples, then the task, then '### Code:'.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "### Retrieved Examples:\n",
        "{retrieved.strip()}\n",
        "\n",
        "### Task:\n",
        "{instruction.strip()}\n",
        "\n",
        "### Code:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCJ9cVHFez3U"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Prompt templates - version 2\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def build_baseline_prompt_v2(instruction: str) -> str:\n",
        "    \"\"\"\n",
        "    Baseline: very explicit, with sections for clarity.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "### Library:\n",
        "You will use the `seedemu` Python library to build a network emulation.\n",
        "\n",
        "### Task Description:\n",
        "{instruction.strip()}\n",
        "\n",
        "### Requirements:\n",
        "1. Import only from `seedemu` (layers, services, core, compiler).\n",
        "2. Create objects in this order: Emulator ‚Üí Layers ‚Üí Services ‚Üí Bindings ‚Üí Dump.\n",
        "3. Use clear variable names (e.g. `base`, `routing`, `ebgp`, `sim`).\n",
        "4. Target Python 3.8+ syntax.\n",
        "\n",
        "### Output Format:\n",
        "- Provide only valid Python code.\n",
        "- No comments, no extra text.\n",
        "- Start at the first line of code (do not repeat the task).\n",
        "\n",
        "### Code:\n",
        "\"\"\"\n",
        "\n",
        "def build_rag_prompt_v2(instruction: str, retrieved: str) -> str:\n",
        "    \"\"\"\n",
        "    RAG: include retrieved examples plus the detailed task template.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "### Retrieved Examples:\n",
        "{retrieved.strip()}\n",
        "\n",
        "### Library:\n",
        "Use the `seedemu` Python library.\n",
        "\n",
        "### Task Description:\n",
        "{instruction.strip()}\n",
        "\n",
        "### Requirements:\n",
        "1. Imports: `seedemu.layers`, `seedemu.services`, `seedemu.core`, `seedemu.compiler`.\n",
        "2. Instantiate Emulator, then Base, Routing, eBGP layers in order.\n",
        "3. Install the domain name caching service on specified hosts.\n",
        "4. Add private eBGP peerings between ASes.\n",
        "5. Finally, dump the emulator state to `base-component.bin`.\n",
        "\n",
        "### Output Format:\n",
        "- Return **only** runnable Python code.\n",
        "- No comments or markdown.\n",
        "- Do not echo the instructions.\n",
        "\n",
        "### Code:\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQm1GC4LPRg9"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Prompt templates - version 3\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def build_baseline_prompt_v3(instruction: str) -> str:\n",
        "    \"\"\"\n",
        "    Baseline: given only the instruction, ask the model to produce:\n",
        "      1. A clear function signature with type hints\n",
        "      2. A concise docstring explaining behavior, inputs, and outputs\n",
        "      3. The implementation, following PEP8\n",
        "      4. At least one simple unit test demonstrating correct usage\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "You are a senior Python engineer.  Fulfill the following task by writing production-ready code.\n",
        "\n",
        "**Task**:\n",
        "{instruction.strip()}\n",
        "\n",
        "**Requirements**:\n",
        "- Python 3, include type hints\n",
        "- One well-formed function or class with a descriptive name\n",
        "- A docstring (inputs, outputs, edge cases)\n",
        "- PEP8 style (4-space indent, snake_case)\n",
        "- At least one unit test using `assert` or `unittest`\n",
        "\n",
        "**Implementation**:\n",
        "```python\n",
        "\"\"\"\n",
        "\n",
        "def build_rag_prompt_v3(instruction: str, retrieved: str) -> str:\n",
        "    \"\"\"\n",
        "    RAG: first show retrieved examples for inspiration, then the same structured prompt:\n",
        "      instruction, requirements, and a code block marker.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "You are a senior Python engineer.  Use the retrieved examples to guide your implementation.\n",
        "\n",
        "**Retrieved Examples**:\n",
        "{retrieved.strip()}\n",
        "\n",
        "**Task**:\n",
        "{instruction.strip()}\n",
        "\n",
        "**Requirements**:\n",
        "- Python 3 with type hints\n",
        "- Clean function or class design with a docstring\n",
        "- Adhere to PEP8 conventions\n",
        "- Include at least one unit test\n",
        "\n",
        "\n",
        "**Implementation**:\n",
        "```python\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PthHC6m7WeBB"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Prompt templates - version 4\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def build_baseline_prompt_v4(instruction: str) -> str:\n",
        "    \"\"\"Return exactly and only the raw answer to the user request, no extra text with no extra prompt engineering.\"\"\"\n",
        "    return instruction.strip()\n",
        "\n",
        "def build_rag_prompt_v4(instruction: str, retrieved: str) -> str:\n",
        "    \"\"\"\n",
        "    RAG: first show retrieved examples for inspiration, then the same structured prompt:\n",
        "      instruction, requirements, and a code block marker.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "You are a senior Python engineer.  Use the retrieved examples to guide your implementation.\n",
        "\n",
        "**Retrieved Examples**:\n",
        "{retrieved.strip()}\n",
        "\n",
        "**Task**:\n",
        "{instruction.strip()}\n",
        "\n",
        "**Requirements**:\n",
        "- Python 3 with type hints\n",
        "- Clean function or class design with a docstring\n",
        "- Adhere to PEP8 conventions\n",
        "- Include at least one unit test\n",
        "- Do **NOT** copy or repeat the retrieved examples; write a NEW solution\n",
        "- Return exactly and only the raw answer to the user request, no extra text\n",
        "**Implementation**:\n",
        "```python\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KOVpTkl8YUj"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Prompt templates - version 5\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def build_baseline_prompt_v5(instruction: str) -> str:\n",
        "    return f\"\"\"\\\n",
        "Write a complete Python 3 implementation for the following task.  Include type hints, a docstring, and at least one unit test.\n",
        "\n",
        "Task:\n",
        "{instruction.strip()}\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def truncate_to_n_tokens(text: str, n: int = 1800) -> str:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) <= n:\n",
        "        return text\n",
        "    return tokenizer.decode(ids[-n:])\n",
        "\n",
        "def build_rag_prompt_v5(instruction: str, retrieved: str) -> str:\n",
        "    retrieved = truncate_to_n_tokens(retrieved, 1800)\n",
        "    return f\"\"\"\\\n",
        "You are a senior Python engineer.  Use the retrieved examples to inspire your implementation.\n",
        "\n",
        "**Retrieved Examples**:\n",
        "{retrieved}\n",
        "\n",
        "**Task**:\n",
        "{instruction.strip()}\n",
        "\n",
        "**Requirements**:\n",
        "- Python 3 with type hints\n",
        "- Clean function or class design with a docstring\n",
        "- Adhere to PEP8\n",
        "- Include at least one unit test\n",
        "- Do **NOT** copy or repeat the examples; write a NEW solution\n",
        "**Implementation**:\n",
        "```python\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIZQfyJ38ZNP"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Prompt templates - version 6 - markdown\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def build_baseline_prompt_v6(instruction: str) -> str:\n",
        "    \"\"\"\n",
        "    Returns a Markdown-formatted prompt for a standalone task.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "# Python Task Implementation\n",
        "\n",
        "Write a complete **Python 3** implementation for the following task.\n",
        "Include type hints, a docstring, and at least one unit test.\n",
        "\n",
        "---\n",
        "\n",
        "**Task**\n",
        "\n",
        "{instruction.strip()}\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def truncate_to_n_tokens(text: str, n: int = 1800) -> str:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) <= n:\n",
        "        return text\n",
        "    return tokenizer.decode(ids[-n:])\n",
        "\n",
        "\n",
        "\n",
        "def build_rag_prompt_v6(instruction: str, retrieved: str) -> str:\n",
        "    \"\"\"\n",
        "    Returns a Markdown-formatted RAG prompt that includes retrieved examples.\n",
        "    \"\"\"\n",
        "    retrieved_snippet = truncate_to_n_tokens(retrieved, 1800)\n",
        "    return f\"\"\"\\\n",
        "# Python Task Implementation (RAG)\n",
        "\n",
        "You are a senior Python engineer. Use the retrieved examples to inspire your implementation.\n",
        "\n",
        "---\n",
        "\n",
        "## Retrieved Examples\n",
        "\n",
        "{retrieved_snippet}\n",
        "\n",
        "---\n",
        "\n",
        "## Task\n",
        "\n",
        "{instruction.strip()}\n",
        "\n",
        "---\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Python 3 with type hints\n",
        "- Clean function or class design with a docstring\n",
        "- Adhere to PEP 8\n",
        "- Include at least one unit test\n",
        "- **Do NOT** copy or repeat the examples; write a **NEW** solution\n",
        "\n",
        "```python\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqbp1sqaJzkH"
      },
      "source": [
        "## Section 2: LLM and Tokenizer Loading with 4-bit Quantization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEZfMT3QJxJU",
        "outputId": "1d6d9787-e644-4347-bf83-270da6f6813c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting trust_remote_code=False for deepseek-ai/deepseek-coder-1.3b-base\n"
          ]
        }
      ],
      "source": [
        "# check that there is only one selected model\n",
        "\n",
        "# --- Gemma Series (Google) ---\n",
        "# model_name = \"google/codegemma-7b\"\n",
        "# model_name = \"google/codegemma-7b-it\"\n",
        "\n",
        "# --- Qwen Series (Alibaba) ---\n",
        "# model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "# model_name = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
        "# model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
        "\n",
        "# --- Deepseek Coder Series (Deepseek AI) ---\n",
        "# model_name = \"deepseek-ai/deepseek-coder-7b-instruct-v1.5\"\n",
        "#model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "\n",
        "# --- Code Llama Series (Meta) ---\n",
        "# model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "\n",
        "# --- Phi Series (Microsoft) ---\n",
        "# model_name = \"microsoft/Phi-4-mini-instruct\"\n",
        "# model_name = \"microsoft/Phi-4-multimodal-instruct\"\n",
        "\n",
        "TRUST_REMOTE_CODE_MODELS = [\n",
        "    \"microsoft/Phi-\",\n",
        "    \"Qwen/\",\n",
        "]\n",
        "trust_code = any(model_name.startswith(prefix) for prefix in TRUST_REMOTE_CODE_MODELS)\n",
        "\n",
        "print(f\"Setting trust_remote_code={trust_code} for {model_name}\")\n",
        "if trust_code:\n",
        "    print(\"WARNING: trust_remote_code=True will execute Python code from the model's Hugging Face repo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV8HRJKiL4pl"
      },
      "outputs": [],
      "source": [
        "# 4-bit NF4 quantisation\n",
        "QUANT_CFG = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        if (torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n",
        "        else torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j-ATB8rL6pE"
      },
      "outputs": [],
      "source": [
        "# == Where the cached copy will live on your Drive ===========\n",
        "CACHE_ROOT = \"/content/drive/MyDrive/llm_cache\"\n",
        "CACHE_DIR  = os.path.join(\n",
        "    CACHE_ROOT,\n",
        "    model_name.replace(\"/\", \"_\") + \"_4bit_nf4\",\n",
        ")\n",
        "\n",
        "META_FILE  = os.path.join(CACHE_DIR, \"metadata.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXahHcIpMAHf"
      },
      "outputs": [],
      "source": [
        "# == Build the 4‚Äêbit config (for GPU only) ===================\n",
        "# (use bfloat16 on bf16‚Äêcapable GPUs, else float16)\n",
        "compute_dtype = (\n",
        "    torch.bfloat16\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else torch.float16\n",
        ")\n",
        "\n",
        "QUANT_CFG = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "def _qcfg_to_dict(cfg):\n",
        "    return {\n",
        "        \"load_in_4bit\": cfg.load_in_4bit,\n",
        "        \"bnb_4bit_quant_type\": cfg.bnb_4bit_quant_type,\n",
        "        \"bnb_4bit_compute_dtype\": str(cfg.bnb_4bit_compute_dtype),\n",
        "        \"bnb_4bit_use_double_quant\": cfg.bnb_4bit_use_double_quant,\n",
        "    }\n",
        "\n",
        "REQ_META = {\n",
        "    \"model_name\": model_name,\n",
        "    \"quant_cfg\":  _qcfg_to_dict(QUANT_CFG),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-seMg9q8MKYR",
        "outputId": "9002d448-5669-49fc-d4bd-6871a004eab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° Cache metadata match: True\n",
            "‚ö° Loading from Drive cache‚Ä¶\n",
            "üéâ Model & tokenizer ready!\n"
          ]
        }
      ],
      "source": [
        "# == Check for existing cache & metadata ====================\n",
        "use_cache = False\n",
        "if os.path.isfile(META_FILE):\n",
        "    try:\n",
        "        saved = json.load(open(META_FILE))\n",
        "        use_cache = saved == REQ_META\n",
        "        print(\"‚ö° Cache metadata match:\", use_cache)\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è  Could not parse metadata.json; ignoring cache.\")\n",
        "\n",
        "# == 7.  Load tokenizer & model (fast or slow path) ==============\n",
        "trust_code = model_name.startswith((\"microsoft/Phi-\", \"Qwen/\"))\n",
        "\n",
        "try:\n",
        "    if use_cache:\n",
        "        print(\"‚ö° Loading from Drive cache‚Ä¶\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(CACHE_DIR, local_files_only=True, trust_remote_code=trust_code)\n",
        "        model     = AutoModelForCausalLM.from_pretrained(\n",
        "            CACHE_DIR,\n",
        "            device_map=\"auto\",\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=trust_code,\n",
        "        )\n",
        "    else:\n",
        "        # decide whether we *can* do 4-bit quant:\n",
        "        do_4bit = torch.cuda.is_available()\n",
        "        print(f\"‚è≥ No valid cache. CUDA available? {do_4bit}\")\n",
        "        print(f\"‚è≥ {'Quantising 4-bit‚Ä¶' if do_4bit else 'Loading fp16‚Ä¶'} this will happen once\")\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ tokenizer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_code)\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        if do_4bit:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                quantization_config=QUANT_CFG,\n",
        "                device_map=\"auto\",\n",
        "                trust_remote_code=trust_code,\n",
        "                low_cpu_mem_usage=True,\n",
        "            )\n",
        "        else:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=compute_dtype,\n",
        "                device_map=\"auto\",\n",
        "                trust_remote_code=trust_code,\n",
        "            )\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ Save cache for next time ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        print(\"üíæ Saving to Drive cache‚Ä¶\")\n",
        "        os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "        tokenizer.save_pretrained(CACHE_DIR)\n",
        "        model.save_pretrained(CACHE_DIR)\n",
        "        with open(META_FILE, \"w\") as f:\n",
        "            json.dump(REQ_META, f)\n",
        "        print(\"‚úÖ Cache written at\", CACHE_DIR)\n",
        "\n",
        "    # ensure model.pad_token_id\n",
        "    if getattr(model, \"config\", None) and model.config.pad_token_id is None:\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    print(\"üéâ Model & tokenizer ready!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error loading model/tokenizer:\")\n",
        "    traceback.print_exc()\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0G8BVCNQFTg"
      },
      "source": [
        "## Section 3: Dataset Preparation and Validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW9aGepXMMHf",
        "outputId": "46e5d066-1bbc-4529-bb24-e290d65123b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive Directory available: /content/drive/MyDrive/RAG_Project/\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Configuration of Google Drive directory ---\n",
        "\n",
        "drive_save_path = '/content/drive/MyDrive/RAG_Project/' # to store results and outputs\n",
        "# check that the directory exists!\n",
        "\n",
        "try:\n",
        "    os.makedirs(drive_save_path, exist_ok=True)\n",
        "    print(f\"Google Drive Directory available: {drive_save_path}\")\n",
        "except OSError as e:\n",
        "    print(f\"Warning: can not create or verify the existence of the directory: {drive_save_path}. Details: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfD2YFUtQRdQ"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade --quiet datasets==2.16.0 fsspec==2023.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "f238561a83fb43309aa41a9f7a342f8b",
            "ea0c3bae1a0d4bab8f0aacae449a8fa7",
            "790a62f5c0214ff391756d1ea48cc5c2",
            "2929f4f12cce439a91a95ce34f291120",
            "b48d3d15e4054c328106cb7363c2371b",
            "93319521892044faade2d6a84e89ccf3",
            "0fa93830eaac46ce9a30a92afd305ea2",
            "e7483d4d774a4135b2ac31f9a747a648",
            "a1f3049d932d4619952ecc16240ae2f7",
            "2a07014311d640eaa6d5b75404927057",
            "48fe48714a2b4f4fa35b8997e43b1cde",
            "48590beb1910415c8c28762ab3cc4e52",
            "68ea6c0d789742ccbe8f805b506ea3b2",
            "676694761ee74fa0af5ca4785e0ac4d9",
            "476ce1405a5b4c1f9c963e3a15b9d727",
            "f3c0ee3e7bcb419ab1c0370ef2701810",
            "79ab87e49c054d42a3473a94e1ad75c5",
            "e19042e1d748427ea260f37950409968",
            "23e0d32673c644d587dbc819174a188b",
            "4b213024e186412f83f46a2c2e70ae9d",
            "5366ff2cab4547b7a3cda93911ad8d30",
            "4f50bedbc88445b5bba961093bbf8b7d",
            "ea2e32f360034eedbdff4a57c09fffda",
            "594ecb2358da46b0a927efe8684378ec",
            "a5f4a49e240a49f0b445bef32c28cec6",
            "f4549bea47e1496bb2245214f3d6d6c6",
            "16534dac29f642208aab35f431bb433a",
            "ed124a1ee1814b83aab55341338559c9",
            "6f8a438dc1c94f2795a4ae627971d671",
            "2fbe6fa881a74fde8c390130277a5608",
            "e5c90620cd1f43299a86eea4d785f345",
            "fcc3614585da425fa7a6041344a58219",
            "1b89df419a90487b9dfecf7132e55b6f"
          ]
        },
        "id": "FphZD_qGQTVT",
        "outputId": "0caf8f26-e309-4b0e-b049-ede33a364478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\n‚ñ∂Ô∏è  Loading dataset 'JetBrains-Research/lca-library-based-code-generation' (split='test')‚Ä¶\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/5.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f238561a83fb43309aa41a9f7a342f8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(‚Ä¶)-00000-of-00001-518ed46ecbe35ff9.parquet:   0%|          | 0.00/4.58M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48590beb1910415c8c28762ab3cc4e52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/150 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea2e32f360034eedbdff4a57c09fffda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded with 150 examples across all libraries.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, DownloadMode\n",
        "\n",
        "dataset_name = \"JetBrains-Research/lca-library-based-code-generation\"\n",
        "data_split   = \"test\"\n",
        "\n",
        "print(f\"\\\\n‚ñ∂Ô∏è  Loading dataset '{dataset_name}' (split='{data_split}')‚Ä¶\")\n",
        "\n",
        "try:\n",
        "    # To work with all libraries\n",
        "    lca_dataset_all_libraries = load_dataset( # Renamed for clarity\n",
        "        dataset_name,\n",
        "        split=data_split,\n",
        "    )\n",
        "    lca_dataset_split = lca_dataset_all_libraries\n",
        "    print(f\"‚úÖ Dataset loaded with {len(lca_dataset_all_libraries)} examples across all libraries.\")\n",
        "\n",
        "    # To work with only specific libraries for testing:\n",
        "    # target_repos = [\"seed-emulator\", \"another-repo\"] # Examples\n",
        "    # lca_dataset_split = lca_dataset_all_libraries.filter(\n",
        "    #     lambda ex: ex[\"repo_name\"] in target_repos\n",
        "    # )\n",
        "    # print(f\"‚úÖ Filtered to {len(lca_dataset_split)} examples in {target_repos}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR loading or filtering dataset: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4KHU4SrReZ1"
      },
      "source": [
        "## Section 4: GitHub Knowledge Base Access Setup\n",
        "\n",
        "This section configures access to pre-built Knowledge Bases (KBs) hosted on a GitHub repository.\n",
        "\n",
        "It performs two main tasks:\n",
        "1.  Defines a helper function (`download_github_raw_json`) to fetch KB JSON files from GitHub.\n",
        "2.  Sets essential GitHub repository parameters (username, repo name, branch, KB folder path) to construct the base URL for downloading KBs. It also specifies a local temporary directory for these downloads.\n",
        "\n",
        "This setup enables subsequent sections to dynamically load specific KBs from the designated GitHub source."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import json # For loading the JSON after download\n",
        "\n",
        "def download_github_raw_json(raw_url, local_save_dir, filename, overwrite=False):\n",
        "    \"\"\"\n",
        "    Downloads a JSON file from a GitHub raw content URL.\n",
        "    Saves it locally and then loads and returns the JSON content.\n",
        "    Returns None if download or JSON parsing fails.\n",
        "    \"\"\"\n",
        "    os.makedirs(local_save_dir, exist_ok=True)\n",
        "    local_file_path = os.path.join(local_save_dir, filename)\n",
        "\n",
        "    if os.path.exists(local_file_path) and not overwrite:\n",
        "        # print(f\"  File already exists locally: {local_file_path}. Loading it.\")\n",
        "        try:\n",
        "            with open(local_file_path, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e_load:\n",
        "            print(f\"  ERROR loading existing local file {local_file_path}: {e_load}. Will attempt re-download.\")\n",
        "            # Proceed to re-download by falling through\n",
        "\n",
        "    try:\n",
        "        # print(f\"  Downloading from GitHub: {raw_url} to {local_file_path}\")\n",
        "        response = requests.get(raw_url, stream=True)\n",
        "        response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
        "\n",
        "        with open(local_file_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        # print(f\"  Download complete: {local_file_path}\")\n",
        "\n",
        "        with open(local_file_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    except requests.exceptions.RequestException as e_req:\n",
        "        print(f\"  ERROR downloading from GitHub {raw_url}: {e_req}\")\n",
        "    except json.JSONDecodeError as e_json:\n",
        "        print(f\"  ERROR decoding JSON from downloaded file {local_file_path}: {e_json}\")\n",
        "    except Exception as e_generic:\n",
        "        print(f\"  An unexpected error occurred during GitHub download/processing for {filename}: {e_generic}\")\n",
        "\n",
        "    return None # Return None on any failure\n",
        "\n",
        "print(\"GitHub download helper function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs4nqbfkkhj-",
        "outputId": "9e69b4e9-a1b7-4de4-e759-e22d49735964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GitHub download helper function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os # Retain os if other parts of this cell use it\n",
        "\n",
        "GITHUB_USERNAME = \"PatrizioAcquadro\"\n",
        "GITHUB_REPO_NAME = \"RAG_Project_SE2\"\n",
        "GITHUB_BRANCH = \"main\"\n",
        "GITHUB_KBS_FOLDER_PATH = \"knowledge_bases_prod\"\n",
        "\n",
        "# Base URL for raw GitHub content\n",
        "GITHUB_RAW_CONTENT_BASE_URL = f\"https://raw.githubusercontent.com/{GITHUB_USERNAME}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}/{GITHUB_KBS_FOLDER_PATH}\"\n",
        "\n",
        "# Local directory on Colab VM to temporarily store downloaded KBs\n",
        "LOCAL_TEMP_KB_DOWNLOAD_DIR = \"/content/temp_downloaded_kbs\"\n",
        "os.makedirs(LOCAL_TEMP_KB_DOWNLOAD_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"--- RAG Knowledge Base Configuration (GitHub) ---\")\n",
        "print(f\"  KBs will be downloaded from GitHub base URL: {GITHUB_RAW_CONTENT_BASE_URL}/kb_LIBRARY_KEY.json\")\n",
        "print(f\"  Downloaded KBs will be temporarily stored in: {LOCAL_TEMP_KB_DOWNLOAD_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_yE_xRMG2Gi",
        "outputId": "a20ad2f9-1c29-4e8d-e4d4-2bd36405009e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- RAG Knowledge Base Configuration (GitHub) ---\n",
            "  KBs will be downloaded from GitHub base URL: https://raw.githubusercontent.com/PatrizioAcquadro/RAG_Project_SE2/main/knowledge_bases_prod/kb_LIBRARY_KEY.json\n",
            "  Downloaded KBs will be temporarily stored in: /content/temp_downloaded_kbs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGMR7ZvsRz9p"
      },
      "source": [
        "## Section 5: BM25 Retrieval Analysis\n",
        "\n",
        "This section is dedicated to analyzing the BM25 retrieval process for a selected code generation sample. It is divided into two main parts:\n",
        "1.  **Configuration:** Defining all parameters for the retrieval analysis (The sample to inspect, BM25 settings, and the tokenization strategy).\n",
        "2.  **Execution & Display:** Loading the relevant KB, performing BM25 retrieval based on the configurations, and displaying the results.\n",
        "\n",
        "This allows for easy experimentation with different retrieval settings before full-scale evaluation.\n",
        "\n",
        "It uses pre-built KBs from `DRIVE_KBS_ROOT_PATH`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f20m3cZmR2bM",
        "outputId": "0c6502c6-7e9f-42c4-f4f2-2c972569e3e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Configuration for BM25 Retrieval Analysis (Section 5) is set:\n",
            "    Target Library for Analysis: 'pyscf__pyscf'\n",
            "    Instruction Index within this library's samples: 0\n",
            "    BM25 Params: k1=1.5, b=0.75\n",
            "    Number of Snippets to Retrieve (Top-K): 5\n",
            "    Tokenizer for BM25: robust_code_tokenizer_for_s5\n"
          ]
        }
      ],
      "source": [
        "import re # For the default tokenizer\n",
        "\n",
        "# --- 1. Rep & Sample Selection ---\n",
        "ANALYSIS_TARGET_REPO_FULL_NAME = \"pyscf__pyscf\"\n",
        "ANALYSIS_SAMPLE_INDEX_WITHIN_REPO = 0 # If line above = \"None\", it's chosen randomly from all repos\n",
        "\n",
        "# --- 2. BM25 Algorithm & Retrieval Parameters ---\n",
        "ANALYSIS_BM25_K1 = 1.5\n",
        "ANALYSIS_BM25_B = 0.75\n",
        "ANALYSIS_TOP_K_SNIPPETS = 5\n",
        "\n",
        "# --- 3. BM25 Tokenizer Selection ---\n",
        "# Define tokenizer functions here. The selected one will be used by BM25 in Cell 5.2.\n",
        "def robust_code_tokenizer_for_s5(text_input):\n",
        "    if not isinstance(text_input, str): return []\n",
        "    text = text_input.lower()\n",
        "    raw_tokens = re.split(r'[^a-z0-9_]+', text) # Keep alphanumeric and underscore\n",
        "    # Filter out empty strings, single characters (often noise), and pure numbers\n",
        "    return [token for token in raw_tokens if token and len(token) > 1 and not token.isdigit()]\n",
        "\n",
        "# Select the tokenizer function to be used:\n",
        "ANALYSIS_BM25_TOKENIZER = robust_code_tokenizer_for_s5\n",
        "\n",
        "# --- 4. Display Options for Analysis Cell (Cell 5.2) ---\n",
        "ANALYSIS_SHOW_QUERY_TOKENS = True\n",
        "ANALYSIS_HIGHLIGHT_KEYWORDS = True # In retrieved snippets\n",
        "\n",
        "print(\"  Configuration for BM25 Retrieval Analysis (Section 5) is set:\")\n",
        "\n",
        "if ANALYSIS_TARGET_REPO_FULL_NAME:\n",
        "    print(f\"    Target Library for Analysis: '{ANALYSIS_TARGET_REPO_FULL_NAME}'\")\n",
        "    print(f\"    Instruction Index within this library's samples: {ANALYSIS_SAMPLE_INDEX_WITHIN_REPO}\")\n",
        "else:\n",
        "    print(f\"    Sample Index from lca_dataset_split for Analysis: {ANALYSIS_SAMPLE_INDEX_WITHIN_REPO}\")\n",
        "\n",
        "print(f\"    BM25 Params: k1={ANALYSIS_BM25_K1}, b={ANALYSIS_BM25_B}\")\n",
        "print(f\"    Number of Snippets to Retrieve (Top-K): {ANALYSIS_TOP_K_SNIPPETS}\")\n",
        "print(f\"    Tokenizer for BM25: {ANALYSIS_BM25_TOKENIZER.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5.2: Retrieval Analysis - Execution and Display (GitHub KBs, Simplified top_k)\n",
        "\n",
        "import json\n",
        "import os\n",
        "import textwrap\n",
        "from rank_bm25 import BM25Okapi\n",
        "from IPython.display import display, HTML\n",
        "import re # Needed for highlighting if not imported globally for ANALYSIS_BM25_TOKENIZER\n",
        "\n",
        "# --- 1. Ensure Configurations from Cell 5.1 & Globals are available ---\n",
        "config_vars_s5_2_final_github = [\n",
        "    'ANALYSIS_SAMPLE_INDEX_WITHIN_REPO', 'ANALYSIS_TARGET_REPO_FULL_NAME',\n",
        "    'ANALYSIS_BM25_K1', 'ANALYSIS_BM25_B', 'ANALYSIS_TOP_K_SNIPPETS',\n",
        "    'ANALYSIS_BM25_TOKENIZER', 'ANALYSIS_SHOW_QUERY_TOKENS', 'ANALYSIS_HIGHLIGHT_KEYWORDS'\n",
        "]\n",
        "if any(var_name not in globals() for var_name in config_vars_s5_2_final_github):\n",
        "    raise NameError(\"One or more configuration variables from Cell 5.1 are not defined. Run Cell 5.1 first.\")\n",
        "\n",
        "global_vars_s5_2_final_github = ['lca_dataset_split', 'GITHUB_RAW_CONTENT_BASE_URL',\n",
        "                                 'LOCAL_TEMP_KB_DOWNLOAD_DIR', 'download_github_raw_json']\n",
        "if any(var_name not in globals() for var_name in global_vars_s5_2_final_github):\n",
        "    raise NameError(\"One or more global prerequisite variables (dataset, GitHub config, download helper) are not defined.\")\n",
        "\n",
        "print(f\"--- Section 5.2: Executing BM25 Retrieval Analysis (from GitHub KBs) ---\")\n",
        "\n",
        "# --- 2. Select Sample Data based on Configuration from Cell 5.1 ---\n",
        "s5_instruction_to_s6 = None\n",
        "s5_snippets_to_s6 = [] # Initialize for output to Section 6\n",
        "library_key_for_kb_file = None # This will be the 'repo_full_name' from dataset\n",
        "\n",
        "try:\n",
        "    if ANALYSIS_TARGET_REPO_FULL_NAME: # As defined in Cell 5.1\n",
        "        library_key_for_kb_file = ANALYSIS_TARGET_REPO_FULL_NAME\n",
        "        library_samples = lca_dataset_split.filter(lambda ex: ex['repo_full_name'] == ANALYSIS_TARGET_REPO_FULL_NAME)\n",
        "        if not library_samples:\n",
        "            raise ValueError(f\"No samples found for specified library (repo_full_name): '{ANALYSIS_TARGET_REPO_FULL_NAME}'.\")\n",
        "        if not (0 <= ANALYSIS_SAMPLE_INDEX_WITHIN_REPO < len(library_samples)): # ANALYSIS_SAMPLE_INDEX_WITHIN_REPO from Cell 5.1\n",
        "            raise IndexError(f\"ANALYSIS_SAMPLE_INDEX_WITHIN_REPO {ANALYSIS_SAMPLE_INDEX_WITHIN_REPO} is out of bounds for library '{ANALYSIS_TARGET_REPO_FULL_NAME}'.\")\n",
        "        target_sample_data = library_samples[ANALYSIS_SAMPLE_INDEX_WITHIN_REPO]\n",
        "        print(f\"  Analyzing instruction #{ANALYSIS_SAMPLE_INDEX_WITHIN_REPO} from library: '{library_key_for_kb_file}'\")\n",
        "    else:\n",
        "        if not (0 <= ANALYSIS_SAMPLE_INDEX_WITHIN_REPO < len(lca_dataset_split)):\n",
        "            raise IndexError(f\"ANALYSIS_SAMPLE_INDEX_WITHIN_REPO {ANALYSIS_SAMPLE_INDEX_WITHIN_REPO} is out of bounds for the full lca_dataset_split.\")\n",
        "        target_sample_data = lca_dataset_split[ANALYSIS_SAMPLE_INDEX_WITHIN_REPO]\n",
        "        library_key_for_kb_file = target_sample_data.get('repo_full_name') # Derive from sample\n",
        "        print(f\"  Analyzing sample at global index {ANALYSIS_SAMPLE_INDEX_WITHIN_REPO}. Derived Library Key: '{library_key_for_kb_file}'\")\n",
        "\n",
        "    s5_instruction_to_s6 = target_sample_data.get('instruction')\n",
        "    if not s5_instruction_to_s6 or not library_key_for_kb_file:\n",
        "        raise ValueError(\"Selected sample missing 'instruction' or 'repo_full_name' could not be determined.\")\n",
        "\n",
        "    print(f\"\\n  Target Library Key for KB: '{library_key_for_kb_file}'\") # This is the 'repo_full_name'\n",
        "    print(f\"  Instruction Text:\\n    {textwrap.fill(s5_instruction_to_s6, width=100, initial_indent='    ', subsequent_indent='    ')}\")\n",
        "\n",
        "    query_tokens = ANALYSIS_BM25_TOKENIZER(s5_instruction_to_s6) # ANALYSIS_BM25_TOKENIZER from Cell 5.1\n",
        "    if ANALYSIS_SHOW_QUERY_TOKENS: # From Cell 5.1\n",
        "        print(f\"\\n  Query Tokens (using '{ANALYSIS_BM25_TOKENIZER.__name__}'):\\n    {query_tokens}\")\n",
        "\n",
        "except Exception as e_sample_select:\n",
        "    print(f\"  ERROR during sample selection: {type(e_sample_select).__name__}: {e_sample_select}\")\n",
        "    library_key_for_kb_file = None # Prevent further processing if sample selection fails\n",
        "\n",
        "# --- 3. Load KB from GitHub, Build Index, and Perform BM25 Retrieval ---\n",
        "if library_key_for_kb_file: # Proceed only if library key was successfully determined\n",
        "    kb_filename_on_github = f\"kb_{library_key_for_kb_file}.json\"\n",
        "    raw_kb_url_s5 = f\"{GITHUB_RAW_CONTENT_BASE_URL}/{kb_filename_on_github}\" # GITHUB_RAW_CONTENT_BASE_URL from Sec4.Cell1 (GitHub config)\n",
        "\n",
        "    # Download to a subfolder within LOCAL_TEMP_KB_DOWNLOAD_DIR\n",
        "    temp_save_subdir_for_kb_s5 = os.path.join(LOCAL_TEMP_KB_DOWNLOAD_DIR, library_key_for_kb_file) # LOCAL_TEMP_KB_DOWNLOAD_DIR from Sec4.Cell1\n",
        "\n",
        "    print(f\"\\n  Attempting to download/load KB for '{library_key_for_kb_file}' from GitHub...\")\n",
        "    kb_data_from_git = download_github_raw_json( # download_github_raw_json helper function\n",
        "        raw_kb_url_s5,\n",
        "        temp_save_subdir_for_kb_s5,\n",
        "        kb_filename_on_github,\n",
        "        overwrite=True # For analysis, always get fresh from GitHub, or False to use local Colab cache\n",
        "    )\n",
        "\n",
        "    if not kb_data_from_git:\n",
        "        print(f\"  ERROR: Failed to download or parse KB for '{library_key_for_kb_file}' from GitHub.\")\n",
        "    else:\n",
        "        print(f\"  Successfully loaded KB for '{library_key_for_kb_file}' from GitHub ({len(kb_data_from_git)} snippets).\")\n",
        "        valid_kb_docs = [str(doc) for doc in kb_data_from_git if isinstance(doc, str) and str(doc).strip()]\n",
        "\n",
        "        if not valid_kb_docs:\n",
        "            print(\"  No valid string snippets in loaded KB for BM25 indexing.\")\n",
        "        else:\n",
        "            tokenized_corpus = [ANALYSIS_BM25_TOKENIZER(doc) for doc in valid_kb_docs]\n",
        "            final_bm25_corpus_docs, map_idx_bm25_to_valid_docs = [], []\n",
        "            for i, tokens in enumerate(tokenized_corpus):\n",
        "                if tokens:\n",
        "                    final_bm25_corpus_docs.append(tokens)\n",
        "                    map_idx_bm25_to_valid_docs.append(i)\n",
        "\n",
        "            if not final_bm25_corpus_docs:\n",
        "                print(\"  Tokenized KB is empty after filtering. BM25 index not built.\")\n",
        "            else:\n",
        "                print(f\"  Creating BM25 index from {len(final_bm25_corpus_docs)} processable documents...\")\n",
        "                # ANALYSIS_BM25_K1 and ANALYSIS_BM25_B are from Cell 5.1\n",
        "                bm25_index = BM25Okapi(final_bm25_corpus_docs, k1=ANALYSIS_BM25_K1, b=ANALYSIS_BM25_B)\n",
        "                print(\"  BM25 index built.\")\n",
        "\n",
        "                # ANALYSIS_TOP_K_SNIPPETS is from Cell 5.1\n",
        "                if query_tokens and ANALYSIS_TOP_K_SNIPPETS > 0:\n",
        "                    print(f\"\\n  --- Retrieving and Displaying Top {ANALYSIS_TOP_K_SNIPPETS} Snippets ---\")\n",
        "                    num_docs = len(final_bm25_corpus_docs)\n",
        "                    top_indices = bm25_index.get_top_n(\n",
        "                        query_tokens, list(range(num_docs)),\n",
        "                        n=min(ANALYSIS_TOP_K_SNIPPETS, num_docs)\n",
        "                    )\n",
        "                    s5_snippets_to_s6 = [valid_kb_docs[map_idx_bm25_to_valid_docs[i]] for i in top_indices]\n",
        "\n",
        "                    if not s5_snippets_to_s6:\n",
        "                        print(f\"    No snippets retrieved for top_k = {ANALYSIS_TOP_K_SNIPPETS}.\")\n",
        "                    else:\n",
        "                        for i_snip, snip_content in enumerate(s5_snippets_to_s6):\n",
        "                            print(f\"\\n    Snippet {i_snip+1}/{len(s5_snippets_to_s6)} (Length: {len(snip_content)} chars):\")\n",
        "                            if ANALYSIS_HIGHLIGHT_KEYWORDS: # From Cell 5.1\n",
        "                                hl_content = snip_content\n",
        "                                unique_qt = sorted(list(set(query_tokens)), key=len, reverse=True)\n",
        "                                for i_t, tkn in enumerate(unique_qt):\n",
        "                                    ph = f\"__HL_S5_{i_t}__\" # More specific placeholder\n",
        "                                    hl_content = re.sub(f\"\\\\b({re.escape(tkn)})\\\\b\", ph, hl_content, flags=re.IGNORECASE)\n",
        "                                for i_t, tkn in enumerate(unique_qt):\n",
        "                                    ph = f\"__HL_S5_{i_t}__\"\n",
        "                                    hl_content = hl_content.replace(ph, f\"<b style='background-color:#FFFACD; color:black; font-weight:bold;'>{tkn}</b>\")\n",
        "                                display(HTML(f\"<pre style='white-space:pre-wrap; word-wrap:break-word; border:1px dashed #ccc; padding:6px; margin-left:20px;'>{hl_content}</pre>\"))\n",
        "                            else:\n",
        "                                print(textwrap.indent(textwrap.fill(snip_content, width=100, subsequent_indent='      '), '      '))\n",
        "                    print(f\"\\n    Stored {len(s5_snippets_to_s6)} snippets in 's5_snippets_to_s6' for Section 6.\")\n",
        "                elif not query_tokens: print(\"  Query tokens empty. BM25 retrieval skipped.\")\n",
        "                else: print(f\"  ANALYSIS_TOP_K_SNIPPETS ({ANALYSIS_TOP_K_SNIPPETS}) is not positive. No snippets retrieved.\")\n",
        "else: # library_key_for_kb_file was None due to sample selection error\n",
        "    print(\"\\n  Sample selection failed earlier. Skipping KB loading and BM25 retrieval.\")\n",
        "\n",
        "if not s5_snippets_to_s6 and library_key_for_kb_file and ('kb_data_from_git' in locals() and kb_data_from_git is not None):\n",
        "    print(\"  INFO: No snippets were ultimately stored for Section 6 from this analysis.\")\n",
        "\n",
        "print(f\"\\n--- Section 5.2: Retrieval Analysis Execution Complete ---\")\n",
        "# Variables `s5_instruction_to_s6` and `s5_snippets_to_s6` are now populated for Section 6."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9405b8edd4e3406c8ce600f61f0bad4f",
            "32ad553f953149ff84b2d87709f4ce6a",
            "5ee8ddd9c3b44cefb5090015c55babe8",
            "865af25810034b9586f2a075e786e0d6",
            "edd1e447740043a3bd47bd96f06b9c56",
            "7394dc71f5ce4b678caaa61d2bd62076",
            "8b2d6f092c334eab92207287cca3b8e8",
            "09c91f6ec60f4447b5e57df9f7ac19af",
            "463fa7577bb849c88c79209cdcc236fe",
            "bb16151345764a88b3e3779c34aa2690",
            "bc9929028e614efa80618181064a1c40"
          ]
        },
        "id": "pxosTnFxSC9Y",
        "outputId": "06a8e299-2afb-44f8-93d7-749e78adcca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Section 5.2: Executing BM25 Retrieval Analysis (from GitHub KBs) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/150 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9405b8edd4e3406c8ce600f61f0bad4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Analyzing instruction #0 from library: 'pyscf__pyscf'\n",
            "\n",
            "  Target Library Key for KB: 'pyscf__pyscf'\n",
            "  Instruction Text:\n",
            "        Generate code that calculates the effective electronic coupling based on single determinant\n",
            "    diabatic states using the pyscf library. The code should first define a molecule with specific\n",
            "    atoms and basis. Then, it should perform two state calculations with DFT, storing molecular\n",
            "    orbital information into separate chkfiles. The code should then read the MO coefficients and\n",
            "    occupation numbers from these chkfiles. Afterwards, it should calculate the overlap between two\n",
            "    determinants, construct density matrices, calculate one-electron and two-electron part\n",
            "    contributions, and calculate new total energy. Finally, the code should calculate the effective\n",
            "    electronic coupling and print the results. The code should also remove the chkfiles at the end.\n",
            "\n",
            "  Query Tokens (using 'robust_code_tokenizer_for_s5'):\n",
            "    ['generate', 'code', 'that', 'calculates', 'the', 'effective', 'electronic', 'coupling', 'based', 'on', 'single', 'determinant', 'diabatic', 'states', 'using', 'the', 'pyscf', 'library', 'the', 'code', 'should', 'first', 'define', 'molecule', 'with', 'specific', 'atoms', 'and', 'basis', 'then', 'it', 'should', 'perform', 'two', 'state', 'calculations', 'with', 'dft', 'storing', 'molecular', 'orbital', 'information', 'into', 'separate', 'chkfiles', 'the', 'code', 'should', 'then', 'read', 'the', 'mo', 'coefficients', 'and', 'occupation', 'numbers', 'from', 'these', 'chkfiles', 'afterwards', 'it', 'should', 'calculate', 'the', 'overlap', 'between', 'two', 'determinants', 'construct', 'density', 'matrices', 'calculate', 'one', 'electron', 'and', 'two', 'electron', 'part', 'contributions', 'and', 'calculate', 'new', 'total', 'energy', 'finally', 'the', 'code', 'should', 'calculate', 'the', 'effective', 'electronic', 'coupling', 'and', 'print', 'the', 'results', 'the', 'code', 'should', 'also', 'remove', 'the', 'chkfiles', 'at', 'the', 'end']\n",
            "\n",
            "  Attempting to download/load KB for 'pyscf__pyscf' from GitHub...\n",
            "  Successfully loaded KB for 'pyscf__pyscf' from GitHub (6618 snippets).\n",
            "  Creating BM25 index from 6618 processable documents...\n",
            "  BM25 index built.\n",
            "\n",
            "  --- Retrieving and Displaying Top 5 Snippets ---\n",
            "\n",
            "    Snippet 1/5 (Length: 716 chars):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style='white-space:pre-wrap; word-wrap:break-word; border:1px dashed #ccc; padding:6px; margin-left:20px;'>def det_ovlp(mo1, mo2, occ1, occ2, ovlp):\n",
              "    r''' <b style='background-color:#FFFACD; color:black; font-weight:bold;'>calculate</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>overlap</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>between</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>two</b> different <b style='background-color:#FFFACD; color:black; font-weight:bold;'>determinants</b>. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>it</b> is <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> product\n",
              "    of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>single</b> values of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>molecular</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>orbital</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>overlap</b> matrix.\n",
              "\n",
              "    Return:\n",
              "        A list:\n",
              "            <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> product of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>single</b> values: float\n",
              "            x_a: :math:`\\mathbf{U} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\dagger`\n",
              "            They are used to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>calculate</b> asymmetric <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b> matrix\n",
              "    '''\n",
              "\n",
              "    if numpy.sum(occ1) != numpy.sum(occ2):\n",
              "        raise RuntimeError('<b style='background-color:#FFFACD; color:black; font-weight:bold;'>electron</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>numbers</b> are not equal. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>electronic</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>coupling</b> does not exist.')\n",
              "    s = reduce(numpy.dot, (mo1[:,occ1>0].T.conj(), ovlp, mo2[:,occ2>0]))\n",
              "    u, s, vt = numpy.linalg.svd(s)\n",
              "    x = numpy.dot(u/s, vt)\n",
              "    return numpy.prod(s), x</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Snippet 2/5 (Length: 1758 chars):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style='white-space:pre-wrap; word-wrap:break-word; border:1px dashed #ccc; padding:6px; margin-left:20px;'>def det_ovlp(mo1, mo2, occ1, occ2, ovlp):\n",
              "    r''' <b style='background-color:#FFFACD; color:black; font-weight:bold;'>calculate</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>overlap</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>between</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>two</b> different <b style='background-color:#FFFACD; color:black; font-weight:bold;'>determinants</b>. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>it</b> is <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> product\n",
              "    of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>single</b> values of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>molecular</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>orbital</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>overlap</b> matrix.\n",
              "\n",
              "    .. math::\n",
              "\n",
              "        S_{12} = \\langle \\Psi_A | \\Psi_B \\rangle\n",
              "        = (\\mathrm{det}\\mathbf{U}) (\\mathrm{det}\\mathbf{V^\\dagger})\n",
              "          \\prod\\limits_{i=1}\\limits^{2N} \\lambda_{ii}\n",
              "\n",
              "    where :math:`\\mathbf{U}, \\mathbf{V}, \\lambda` are unitary <b style='background-color:#FFFACD; color:black; font-weight:bold;'>matrices</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>and</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>single</b>\n",
              "    values generated by <b style='background-color:#FFFACD; color:black; font-weight:bold;'>single</b> value decomposition(SVD) of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>overlap</b> matrix\n",
              "    :math:`\\mathbf{O}` which is <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>overlap</b> matrix of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>two</b> sets of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>molecular</b> orbitals:\n",
              "\n",
              "    .. math::\n",
              "\n",
              "        \\mathbf{U}^\\dagger \\mathbf{O} \\mathbf{V} = \\mathbf{\\Lambda}\n",
              "\n",
              "    Args:\n",
              "        mo1, mo2 : 2D ndarrays\n",
              "            Molecualr <b style='background-color:#FFFACD; color:black; font-weight:bold;'>orbital</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>coefficients</b>\n",
              "        occ1, occ2: 2D ndarrays\n",
              "            <b style='background-color:#FFFACD; color:black; font-weight:bold;'>occupation</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>numbers</b>\n",
              "\n",
              "    Return:\n",
              "        A list: <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> product of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>single</b> values: float\n",
              "            x_a, x_b: 1D ndarrays\n",
              "            :math:`\\mathbf{U} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\dagger`\n",
              "            They are used to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>calculate</b> asymmetric <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b> matrix\n",
              "    '''\n",
              "\n",
              "    if not numpy.array_equal(occ1, occ2):\n",
              "        raise RuntimeError('<b style='background-color:#FFFACD; color:black; font-weight:bold;'>electron</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>numbers</b> are not equal. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>electronic</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>coupling</b> does not exist.')\n",
              "\n",
              "    c1_a = mo1[0][:, occ1[0]>0]\n",
              "    c1_b = mo1[1][:, occ1[1]>0]\n",
              "    c2_a = mo2[0][:, occ2[0]>0]\n",
              "    c2_b = mo2[1][:, occ2[1]>0]\n",
              "    o_a = reduce(numpy.dot, (c1_a.conj().T, ovlp, c2_a))\n",
              "    o_b = reduce(numpy.dot, (c1_b.conj().T, ovlp, c2_b))\n",
              "    u_a, s_a, vt_a = numpy.linalg.svd(o_a)\n",
              "    u_b, s_b, vt_b = numpy.linalg.svd(o_b)\n",
              "    x_a = reduce(numpy.dot, (u_a*numpy.reciprocal(s_a), vt_a))\n",
              "    x_b = reduce(numpy.dot, (u_b*numpy.reciprocal(s_b), vt_b))\n",
              "    return numpy.prod(s_a)*numpy.prod(s_b), numpy.array((x_a, x_b))</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Snippet 3/5 (Length: 566 chars):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style='white-space:pre-wrap; word-wrap:break-word; border:1px dashed #ccc; padding:6px; margin-left:20px;'>def as_r6(m):\n",
              "        # When making derivative over t2, r6 <b style='background-color:#FFFACD; color:black; font-weight:bold;'>should</b> be called <b style='background-color:#FFFACD; color:black; font-weight:bold;'>on</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> 6-index\n",
              "        # tensor. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>it</b> gives <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> equation for lambda2, but not corresponding to\n",
              "        # <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> lambda equation used by RCCSD-lambda <b style='background-color:#FFFACD; color:black; font-weight:bold;'>code</b>.  A transformation was\n",
              "        # applied in RCCSD-lambda equation  F(lambda)_{ijab} = 0:\n",
              "        #       2/3 * # F(lambda)_{ijab} + 1/3 * F(lambda)_{jiab} = 0\n",
              "        # Combining this transformation <b style='background-color:#FFFACD; color:black; font-weight:bold;'>with</b> r6 operation, leads to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b>\n",
              "        # transformation <b style='background-color:#FFFACD; color:black; font-weight:bold;'>code</b> below\n",
              "        return m * 2 - m.transpose(0,1,2,5,4,3) - m.transpose(0,1,2,3,5,4)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Snippet 4/5 (Length: 5338 chars):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style='white-space:pre-wrap; word-wrap:break-word; border:1px dashed #ccc; padding:6px; margin-left:20px;'>class CASSCFWithSolvent(_Solvation, oldCAS):\n",
              "        def __init__(self, mc, solvent):\n",
              "            self.__dict__.update(mc.__dict__)\n",
              "            self.with_solvent = solvent\n",
              "            self._e_tot_without_solvent = 0\n",
              "            self._keys.update(['with_solvent'])\n",
              "\n",
              "        def dump_flags(self, verbose=None):\n",
              "            oldCAS.dump_flags(self, verbose)\n",
              "            self.with_solvent.check_sanity()\n",
              "            self.with_solvent.dump_flags(verbose)\n",
              "            if self.conv_tol < 1e-7:\n",
              "                logger.warn(self, 'CASSCF+ddCOSMO may not be able to '\n",
              "                            'converge to conv_tol=%g', self.conv_tol)\n",
              "\n",
              "            if (getattr(self._scf, 'with_solvent', None) <b style='background-color:#FFFACD; color:black; font-weight:bold;'>and</b>\n",
              "                not getattr(self, 'with_solvent', None)):\n",
              "                logger.warn(self, '''Solvent model %s was found in SCF object.\n",
              "COSMO is not applied to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> CASCI object. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> CASSCF result is not affected by <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> SCF solvent model.\n",
              "To enable <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> solvent model for CASSCF, a decoration to CASSCF object as below needs to be called\n",
              "        <b style='background-color:#FFFACD; color:black; font-weight:bold;'>from</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>pyscf</b> import solvent\n",
              "        mc = mcscf.CASSCF(...)\n",
              "        mc = solvent.ddCOSMO(mc)\n",
              "''',\n",
              "                            self._scf.with_solvent.__class__)\n",
              "            return self\n",
              "\n",
              "        def reset(self, mol=None):\n",
              "            self.with_solvent.reset(mol)\n",
              "            return oldCAS.reset(self, mol)\n",
              "\n",
              "        def update_casdm(self, <b style='background-color:#FFFACD; color:black; font-weight:bold;'>mo</b>, u, fcivec, e_ci, eris, envs={}):\n",
              "            casdm1, casdm2, gci, fcivec = \\\n",
              "                    oldCAS.update_casdm(self, <b style='background-color:#FFFACD; color:black; font-weight:bold;'>mo</b>, u, fcivec, e_ci, eris, envs)\n",
              "\n",
              "# <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> potential is generated <b style='background-color:#FFFACD; color:black; font-weight:bold;'>based</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>on</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b> of current micro iteration.\n",
              "# <b style='background-color:#FFFACD; color:black; font-weight:bold;'>it</b> will be added to hcore in casci function. Strictly speaking, this <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b>\n",
              "# is not <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> same to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> CASSCF <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b> (which was used to measure\n",
              "# convergence) in <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> macro iterations.  When CASSCF is converged, <b style='background-color:#FFFACD; color:black; font-weight:bold;'>it</b>\n",
              "# <b style='background-color:#FFFACD; color:black; font-weight:bold;'>should</b> be almost <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> same to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> CASSCF <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b> of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> last macro iteration.\n",
              "            with_solvent = self.with_solvent\n",
              "            if not with_solvent.frozen:\n",
              "                # <b style='background-color:#FFFACD; color:black; font-weight:bold;'>code</b> to mimic dm = self.make_rdm1(ci=fcivec)\n",
              "                mocore = <b style='background-color:#FFFACD; color:black; font-weight:bold;'>mo</b>[:,:self.ncore]\n",
              "                mocas = <b style='background-color:#FFFACD; color:black; font-weight:bold;'>mo</b>[:,self.ncore:self.ncore+self.ncas]\n",
              "                dm = reduce(numpy.dot, (mocas, casdm1, mocas.T))\n",
              "                dm += numpy.dot(mocore, mocore.T) * 2\n",
              "                with_solvent.e, with_solvent.v = with_solvent.kernel(dm)\n",
              "\n",
              "            return casdm1, casdm2, gci, fcivec\n",
              "\n",
              "# ddCOSMO Potential <b style='background-color:#FFFACD; color:black; font-weight:bold;'>should</b> be added to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>effective</b> potential. However, there\n",
              "# is no hook to modify <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>effective</b> potential in CASSCF. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> workaround\n",
              "# here is to modify hcore. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>it</b> can affect <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> 1-<b style='background-color:#FFFACD; color:black; font-weight:bold;'>electron</b> operator in many CASSCF\n",
              "# functions: gen_h_op, update_casdm, casci.  Note hcore is used to compute <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b>\n",
              "# <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> for core <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b> (Ecore).  <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> resultant <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>from</b> casci\n",
              "# function will include <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> contribution <b style='background-color:#FFFACD; color:black; font-weight:bold;'>from</b> ddCOSMO potential. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b>\n",
              "# duplicated <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> contribution <b style='background-color:#FFFACD; color:black; font-weight:bold;'>from</b> solvent needs to be removed.\n",
              "        def get_hcore(self, mol=None):\n",
              "            hcore = self._scf.get_hcore(mol)\n",
              "            if self.with_solvent.v is not None:\n",
              "                hcore += self.with_solvent.v\n",
              "            return hcore\n",
              "\n",
              "        def casci(self, mo_coeff, ci0=None, eris=None, verbose=None, envs=None):\n",
              "            log = logger.new_logger(self, verbose)\n",
              "            log.debug('Running CASCI <b style='background-color:#FFFACD; color:black; font-weight:bold;'>with</b> solvent. Note <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> '\n",
              "                      'has duplicated <b style='background-color:#FFFACD; color:black; font-weight:bold;'>contributions</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>from</b> solvent.')\n",
              "\n",
              "            # In oldCAS.casci function, dE was computed <b style='background-color:#FFFACD; color:black; font-weight:bold;'>based</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>on</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b>\n",
              "            # <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> without removing <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> duplicated solvent <b style='background-color:#FFFACD; color:black; font-weight:bold;'>contributions</b>.\n",
              "            # However, envs['elast'] is <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> last <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>with</b> correct\n",
              "            # solvent effects. Hack envs['elast'] to make oldCAS.casci <b style='background-color:#FFFACD; color:black; font-weight:bold;'>print</b>\n",
              "            # <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> correct <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> difference.\n",
              "            envs['elast'] = self._e_tot_without_solvent\n",
              "            e_tot, e_cas, fcivec = oldCAS.casci(self, mo_coeff, ci0, eris,\n",
              "                                                verbose, envs)\n",
              "            self._e_tot_without_solvent = e_tot\n",
              "\n",
              "            log.debug('Computing corrections to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b>.')\n",
              "            dm = self.make_rdm1(ci=fcivec, ao_repr=True)\n",
              "\n",
              "            with_solvent = self.with_solvent\n",
              "            if with_solvent.e is not None:\n",
              "                edup = numpy.einsum('ij,ji->', with_solvent.v, dm)\n",
              "                e_tot = e_tot - edup + with_solvent.e\n",
              "                log.info('Removing duplication %.15g, '\n",
              "                         'adding E(solvent) = %.15g to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b>:\\n'\n",
              "                         '    E(CASSCF+solvent) = %.15g', edup, with_solvent.e, e_tot)\n",
              "\n",
              "            # Update solvent effects for next iteration if needed\n",
              "            if not with_solvent.frozen:\n",
              "                with_solvent.e, with_solvent.v = with_solvent.kernel(dm)\n",
              "\n",
              "            return e_tot, e_cas, fcivec\n",
              "\n",
              "        def nuc_grad_method(self):\n",
              "            logger.warn(self, '''\n",
              "<b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>code</b> for CASSCF gradients was <b style='background-color:#FFFACD; color:black; font-weight:bold;'>based</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>on</b> variational CASSCF wavefunction.\n",
              "However, <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> ddCOSMO-CASSCF <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> was not computed variationally.\n",
              "Approximate gradients are evaluated here. A small error may be expected in <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b>\n",
              "gradients which corresponds to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> contribution of\n",
              "  MCSCF_DM * V_solvent[d/dX MCSCF_DM] + V_solvent[MCSCF_DM] * d/dX MCSCF_DM\n",
              "''')\n",
              "            grad_method = oldCAS.nuc_grad_method(self)\n",
              "            return self.with_solvent.nuc_grad_method(grad_method)\n",
              "\n",
              "        Gradients = nuc_grad_method</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Snippet 5/5 (Length: 5880 chars):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style='white-space:pre-wrap; word-wrap:break-word; border:1px dashed #ccc; padding:6px; margin-left:20px;'>def _for_casscf(mc, solvent_obj, dm=None):\n",
              "    '''Add solvent model to CASSCF method.\n",
              "\n",
              "    Kwargs:\n",
              "        dm : if given, solvent does not respond to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> change of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b>\n",
              "            matrix. A frozen ddCOSMO potential is added to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>results</b>.\n",
              "    '''\n",
              "    if isinstance(mc, _Solvation):\n",
              "        mc.with_solvent = solvent_obj\n",
              "        return mc\n",
              "\n",
              "    oldCAS = mc.__class__\n",
              "\n",
              "    if dm is not None:\n",
              "        solvent_obj.e, solvent_obj.v = solvent_obj.kernel(dm)\n",
              "        solvent_obj.frozen = True\n",
              "\n",
              "    class CASSCFWithSolvent(_Solvation, oldCAS):\n",
              "        def __init__(self, mc, solvent):\n",
              "            self.__dict__.update(mc.__dict__)\n",
              "            self.with_solvent = solvent\n",
              "            self._e_tot_without_solvent = 0\n",
              "            self._keys.update(['with_solvent'])\n",
              "\n",
              "        def dump_flags(self, verbose=None):\n",
              "            oldCAS.dump_flags(self, verbose)\n",
              "            self.with_solvent.check_sanity()\n",
              "            self.with_solvent.dump_flags(verbose)\n",
              "            if self.conv_tol < 1e-7:\n",
              "                logger.warn(self, 'CASSCF+ddCOSMO may not be able to '\n",
              "                            'converge to conv_tol=%g', self.conv_tol)\n",
              "\n",
              "            if (getattr(self._scf, 'with_solvent', None) <b style='background-color:#FFFACD; color:black; font-weight:bold;'>and</b>\n",
              "                not getattr(self, 'with_solvent', None)):\n",
              "                logger.warn(self, '''Solvent model %s was found in SCF object.\n",
              "COSMO is not applied to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> CASCI object. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> CASSCF result is not affected by <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> SCF solvent model.\n",
              "To enable <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> solvent model for CASSCF, a decoration to CASSCF object as below needs to be called\n",
              "        <b style='background-color:#FFFACD; color:black; font-weight:bold;'>from</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>pyscf</b> import solvent\n",
              "        mc = mcscf.CASSCF(...)\n",
              "        mc = solvent.ddCOSMO(mc)\n",
              "''',\n",
              "                            self._scf.with_solvent.__class__)\n",
              "            return self\n",
              "\n",
              "        def reset(self, mol=None):\n",
              "            self.with_solvent.reset(mol)\n",
              "            return oldCAS.reset(self, mol)\n",
              "\n",
              "        def update_casdm(self, <b style='background-color:#FFFACD; color:black; font-weight:bold;'>mo</b>, u, fcivec, e_ci, eris, envs={}):\n",
              "            casdm1, casdm2, gci, fcivec = \\\n",
              "                    oldCAS.update_casdm(self, <b style='background-color:#FFFACD; color:black; font-weight:bold;'>mo</b>, u, fcivec, e_ci, eris, envs)\n",
              "\n",
              "# <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> potential is generated <b style='background-color:#FFFACD; color:black; font-weight:bold;'>based</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>on</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b> of current micro iteration.\n",
              "# <b style='background-color:#FFFACD; color:black; font-weight:bold;'>it</b> will be added to hcore in casci function. Strictly speaking, this <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b>\n",
              "# is not <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> same to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> CASSCF <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b> (which was used to measure\n",
              "# convergence) in <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> macro iterations.  When CASSCF is converged, <b style='background-color:#FFFACD; color:black; font-weight:bold;'>it</b>\n",
              "# <b style='background-color:#FFFACD; color:black; font-weight:bold;'>should</b> be almost <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> same to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> CASSCF <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b> of <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> last macro iteration.\n",
              "            with_solvent = self.with_solvent\n",
              "            if not with_solvent.frozen:\n",
              "                # <b style='background-color:#FFFACD; color:black; font-weight:bold;'>code</b> to mimic dm = self.make_rdm1(ci=fcivec)\n",
              "                mocore = <b style='background-color:#FFFACD; color:black; font-weight:bold;'>mo</b>[:,:self.ncore]\n",
              "                mocas = <b style='background-color:#FFFACD; color:black; font-weight:bold;'>mo</b>[:,self.ncore:self.ncore+self.ncas]\n",
              "                dm = reduce(numpy.dot, (mocas, casdm1, mocas.T))\n",
              "                dm += numpy.dot(mocore, mocore.T) * 2\n",
              "                with_solvent.e, with_solvent.v = with_solvent.kernel(dm)\n",
              "\n",
              "            return casdm1, casdm2, gci, fcivec\n",
              "\n",
              "# ddCOSMO Potential <b style='background-color:#FFFACD; color:black; font-weight:bold;'>should</b> be added to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>effective</b> potential. However, there\n",
              "# is no hook to modify <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>effective</b> potential in CASSCF. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> workaround\n",
              "# here is to modify hcore. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>it</b> can affect <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> 1-<b style='background-color:#FFFACD; color:black; font-weight:bold;'>electron</b> operator in many CASSCF\n",
              "# functions: gen_h_op, update_casdm, casci.  Note hcore is used to compute <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b>\n",
              "# <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> for core <b style='background-color:#FFFACD; color:black; font-weight:bold;'>density</b> (Ecore).  <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> resultant <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>from</b> casci\n",
              "# function will include <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> contribution <b style='background-color:#FFFACD; color:black; font-weight:bold;'>from</b> ddCOSMO potential. <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b>\n",
              "# duplicated <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> contribution <b style='background-color:#FFFACD; color:black; font-weight:bold;'>from</b> solvent needs to be removed.\n",
              "        def get_hcore(self, mol=None):\n",
              "            hcore = self._scf.get_hcore(mol)\n",
              "            if self.with_solvent.v is not None:\n",
              "                hcore += self.with_solvent.v\n",
              "            return hcore\n",
              "\n",
              "        def casci(self, mo_coeff, ci0=None, eris=None, verbose=None, envs=None):\n",
              "            log = logger.new_logger(self, verbose)\n",
              "            log.debug('Running CASCI <b style='background-color:#FFFACD; color:black; font-weight:bold;'>with</b> solvent. Note <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> '\n",
              "                      'has duplicated <b style='background-color:#FFFACD; color:black; font-weight:bold;'>contributions</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>from</b> solvent.')\n",
              "\n",
              "            # In oldCAS.casci function, dE was computed <b style='background-color:#FFFACD; color:black; font-weight:bold;'>based</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>on</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b>\n",
              "            # <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> without removing <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> duplicated solvent <b style='background-color:#FFFACD; color:black; font-weight:bold;'>contributions</b>.\n",
              "            # However, envs['elast'] is <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> last <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>with</b> correct\n",
              "            # solvent effects. Hack envs['elast'] to make oldCAS.casci <b style='background-color:#FFFACD; color:black; font-weight:bold;'>print</b>\n",
              "            # <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> correct <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> difference.\n",
              "            envs['elast'] = self._e_tot_without_solvent\n",
              "            e_tot, e_cas, fcivec = oldCAS.casci(self, mo_coeff, ci0, eris,\n",
              "                                                verbose, envs)\n",
              "            self._e_tot_without_solvent = e_tot\n",
              "\n",
              "            log.debug('Computing corrections to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b>.')\n",
              "            dm = self.make_rdm1(ci=fcivec, ao_repr=True)\n",
              "\n",
              "            with_solvent = self.with_solvent\n",
              "            if with_solvent.e is not None:\n",
              "                edup = numpy.einsum('ij,ji->', with_solvent.v, dm)\n",
              "                e_tot = e_tot - edup + with_solvent.e\n",
              "                log.info('Removing duplication %.15g, '\n",
              "                         'adding E(solvent) = %.15g to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>total</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b>:\\n'\n",
              "                         '    E(CASSCF+solvent) = %.15g', edup, with_solvent.e, e_tot)\n",
              "\n",
              "            # Update solvent effects for next iteration if needed\n",
              "            if not with_solvent.frozen:\n",
              "                with_solvent.e, with_solvent.v = with_solvent.kernel(dm)\n",
              "\n",
              "            return e_tot, e_cas, fcivec\n",
              "\n",
              "        def nuc_grad_method(self):\n",
              "            logger.warn(self, '''\n",
              "<b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>code</b> for CASSCF gradients was <b style='background-color:#FFFACD; color:black; font-weight:bold;'>based</b> <b style='background-color:#FFFACD; color:black; font-weight:bold;'>on</b> variational CASSCF wavefunction.\n",
              "However, <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> ddCOSMO-CASSCF <b style='background-color:#FFFACD; color:black; font-weight:bold;'>energy</b> was not computed variationally.\n",
              "Approximate gradients are evaluated here. A small error may be expected in <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b>\n",
              "gradients which corresponds to <b style='background-color:#FFFACD; color:black; font-weight:bold;'>the</b> contribution of\n",
              "  MCSCF_DM * V_solvent[d/dX MCSCF_DM] + V_solvent[MCSCF_DM] * d/dX MCSCF_DM\n",
              "''')\n",
              "            grad_method = oldCAS.nuc_grad_method(self)\n",
              "            return self.with_solvent.nuc_grad_method(grad_method)\n",
              "\n",
              "        Gradients = nuc_grad_method\n",
              "\n",
              "    return CASSCFWithSolvent(mc, solvent_obj)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Stored 5 snippets in 's5_snippets_to_s6' for Section 6.\n",
            "\n",
            "--- Section 5.2: Retrieval Analysis Execution Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 6: RAG Prompt Assembly for Demo Sample\n",
        "\n",
        "This section assembles the final RAG prompt for the LLM, using the instruction and retrieved snippets from the Section 5 analysis.\n",
        "\n",
        "It first sets up by selecting the desired `build_rag_prompt` function (from the globally defined prompt templates) and checks for its dependencies, like the LLM `tokenizer` if needed for snippet truncation.\n",
        "\n",
        "Then, it takes the instruction and retrieved context (snippets) provided by Section 5, formats the snippets into a text block, and calls the chosen `build_rag_prompt` function. The resulting complete prompt string is stored in `s6_final_rag_prompt_output` and a preview is displayed, making it ready for the LLM generation step in Section 7."
      ],
      "metadata": {
        "id": "eltpUcq5U6mU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHHG4wb4R4K2",
        "outputId": "84691c0d-a2cd-4358-c88c-02f13224888f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Using 'build_rag_prompt_v6' for RAG prompt assembly in this section.\n",
            "  INFO: Selected prompt builder 'build_rag_prompt_v6' may use the global 'tokenizer'. Ensure 'tokenizer' is correctly loaded.\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "# --- 1. Ensure Prerequisite variables from Section 5 (5.2) are defined ---\n",
        "if 's5_instruction_to_s6' not in locals() or \\\n",
        "   's5_snippets_to_s6' not in locals():\n",
        "    raise NameError(\"Variables 's5_instruction_to_s6' or 's5_snippets_to_s6' not found. \"\n",
        "                    \"Ensure Section 5 (BM25 Retrieval Analysis - Execution and Display) has been run successfully.\")\n",
        "\n",
        "# --- 2. Select the RAG Prompt Builder Function ---\n",
        "build_rag_prompt_to_use_in_s6 = build_rag_prompt_v6 # Defaulting to v6 as an example\n",
        "\n",
        "# Verify that the chosen function is actually defined\n",
        "if 'build_rag_prompt_to_use_in_s6' not in locals() or not callable(build_rag_prompt_to_use_in_s6):\n",
        "    raise NameError(f\"The function assigned to 'build_rag_prompt_to_use_in_s6' is not defined or not callable. \"\n",
        "                    f\"Please check its definition and assignment in this cell.\")\n",
        "\n",
        "print(f\"INFO: Using '{build_rag_prompt_to_use_in_s6.__name__}' for RAG prompt assembly in this section.\")\n",
        "\n",
        "\n",
        "# --- 3. Check for Tokenizer Dependency if needed by the chosen prompt builder ---\n",
        "# Important if the chosen prompt builder (v5/v6) uses a helper like `truncate_to_n_tokens` which itself requires the LLM's tokenizer.\n",
        "TOKENIZER_NEEDED_BY_SELECTED_PROMPT_BUILDER = False\n",
        "# Heuristic: Check if 'truncate_to_n_tokens' is called by the selected builder.\n",
        "# This assumes 'truncate_to_n_tokens' is the name of the helper that needs the tokenizer.\n",
        "try:\n",
        "    func_code_object = getattr(build_rag_prompt_to_use_in_s6, '__code__', None)\n",
        "    if func_code_object and \"truncate_to_n_tokens\" in func_code_object.co_names:\n",
        "        TOKENIZER_NEEDED_BY_SELECTED_PROMPT_BUILDER = True\n",
        "        # Also, ensure 'truncate_to_n_tokens' itself is defined globally if it's called.\n",
        "        if \"truncate_to_n_tokens\" not in globals():\n",
        "            raise NameError(f\"'truncate_to_n_tokens' function is called by '{build_rag_prompt_to_use_in_s6.__name__}' \"\n",
        "                            \"but 'truncate_to_n_tokens' itself is not defined globally.\")\n",
        "except AttributeError:\n",
        "    # Could happen if build_rag_prompt_to_use_in_s6 is a lambda or other non-standard callable\n",
        "    # For simplicity, we'll assume if we can't inspect, it might not need it, or it will fail at runtime if it does and tokenizer is missing.\n",
        "    pass\n",
        "\n",
        "if TOKENIZER_NEEDED_BY_SELECTED_PROMPT_BUILDER:\n",
        "    if 'tokenizer' not in globals():\n",
        "        raise NameError(f\"LLM 'tokenizer' not defined globally, but it is required by the selected \"\n",
        "                        f\"prompt builder '{build_rag_prompt_to_use_in_s6.__name__}' (likely for snippet truncation). \"\n",
        "                        \"Please ensure the tokenizer is loaded in an earlier section (e.g., Section 2).\")\n",
        "    else:\n",
        "        print(f\"  INFO: Selected prompt builder '{build_rag_prompt_to_use_in_s6.__name__}' may use the global 'tokenizer'. Ensure 'tokenizer' is correctly loaded.\")\n",
        "else:\n",
        "    print(f\"  INFO: Selected prompt builder '{build_rag_prompt_to_use_in_s6.__name__}' does not appear to directly require the global 'tokenizer' for truncation via 'truncate_to_n_tokens'.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s6_final_rag_prompt_output = None # Initialize the output of this section\n",
        "\n",
        "# Prepare the prompt with the instruction\n",
        "if s5_instruction_to_s6 is None:\n",
        "    print(\"  ERROR: No instruction ('s5_instruction_to_s6') available from Section 5. Cannot assemble prompt.\")\n",
        "else:\n",
        "    print(f\"  Using instruction: '{s5_instruction_to_s6[:100]}...'\")\n",
        "    print(f\"  Using {len(s5_snippets_to_s6)} retrieved snippets from 's5_snippets_to_s6'.\")\n",
        "\n",
        "\n",
        "    # Prepare the prompt with the retrieved snippets (`build_rag_prompt` likely expects a single string, so they are joined by separator)\n",
        "    retrieved_snippets_as_text_block = \"\\n\\n# --- Snippet Separator ---\\n\\n\".join(s5_snippets_to_s6) \\\n",
        "                                       if s5_snippets_to_s6 else \"\"\n",
        "    if not s5_snippets_to_s6:\n",
        "        print(\"  INFO: No snippets were provided from Section 5 ('s5_snippets_to_s6' is empty). \"\n",
        "              \"The RAG prompt will be assembled with an empty retrieved context.\")\n",
        "\n",
        "    # --- Build the Prompt ---\n",
        "    try:\n",
        "        s6_final_rag_prompt_output = build_rag_prompt_to_use_in_s6(\n",
        "            s5_instruction_to_s6,\n",
        "            retrieved_snippets_as_text_block\n",
        "        )\n",
        "\n",
        "        if s6_final_rag_prompt_output:\n",
        "            print(\"\\n  Final RAG Prompt Assembled (First 700 Characters):\")\n",
        "            print(textwrap.shorten(s6_final_rag_prompt_output, width=700, placeholder=\"... (prompt truncated) ...\"))\n",
        "            # For the full prompt if needed for debugging:\n",
        "            # print(s6_final_rag_prompt_output)\n",
        "        else:\n",
        "             print(\"  ERROR: Prompt assembly using your 'build_rag_prompt' function \"\n",
        "                   \"resulted in an empty or None prompt. Please check the function's logic.\")\n",
        "    except Exception as e_build_prompt:\n",
        "        print(f\"  ERROR during prompt assembly with '{build_rag_prompt_to_use_in_s6.__name__}': {e_build_prompt}\")\n",
        "        s6_final_rag_prompt_output = None\n",
        "\n",
        "\n",
        "# The variable `s6_final_rag_prompt_output` now holds the complete RAG prompt string.\n",
        "if s6_final_rag_prompt_output:\n",
        "    print(f\"\\n--- Section 6: RAG Prompt Assembly Complete. Output in 's6_final_rag_prompt_output' (Length: {len(s6_final_rag_prompt_output)} chars) ---\")\n",
        "else:\n",
        "    print(f\"\\n--- Section 6: RAG Prompt Assembly Failed or Produced No Output ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG0YOrVwuVvi",
        "outputId": "9d939f61-97ce-4d3e-e039-0ae980f3cd39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Using instruction: 'Generate code that calculates the effective electronic coupling based on single determinant diabatic...'\n",
            "  Using 5 retrieved snippets from 's5_snippets_to_s6'.\n",
            "\n",
            "  Final RAG Prompt Assembled (First 700 Characters):\n",
            "# Python Task Implementation (RAG) You are a senior Python engineer. Use the retrieved examples to inspire your implementation. --- ## Retrieved Examples matrix. A frozen ddCOSMO potential is added to the results. ''' if isinstance(mc, _Solvation): mc.with_solvent = solvent_obj return mc oldCAS = mc.__class__ if dm is not None: solvent_obj.e, solvent_obj.v = solvent_obj.kernel(dm) solvent_obj.frozen = True class CASSCFWithSolvent(_Solvation, oldCAS): def __init__(self, mc, solvent): self.__dict__.update(mc.__dict__) self.with_solvent = solvent self._e_tot_without_solvent = 0 self._keys.update(['with_solvent']) def dump_flags(self, verbose=None):... (prompt truncated) ...\n",
            "\n",
            "--- Section 6: RAG Prompt Assembly Complete. Output in 's6_final_rag_prompt_output' (Length: 6859 chars) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3RCwduHR6iR"
      },
      "source": [
        "## Section 7: LLM Generation for RAG Demo Prompt\n",
        "\n",
        "This section generates code using the LLM for the demo RAG prompt assembled in Section 6.\n",
        "\n",
        "It first **configures LLM generation settings**, including global inference parameters and a custom stopping criteria class (`EosAndCodeEndStoppingCriteria`) to manage output length. This setup is done once.\n",
        "\n",
        "Subsequently, it **executes the generation**:\n",
        "*   Takes the RAG prompt from Section 6.\n",
        "*   Uses the pre-set configurations to call `model.generate()`.\n",
        "*   Decodes, cleans, and displays the LLM-generated code, storing the result in `s7_generated_code_rag_demo`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "import time\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "def generate_llm_code_and_clean(\n",
        "    prompt_text: str,\n",
        "    llm_model,\n",
        "    llm_tokenizer,\n",
        "    max_new_tokens_gen,\n",
        "    do_sample_gen,\n",
        "    temperature_gen,\n",
        "    top_p_gen,\n",
        "    top_k_gen,\n",
        "    repetition_penalty_gen,\n",
        "    stopping_criteria_list_gen, # Can be None\n",
        "    prompt_name: str = \"Prompt\" # For logging\n",
        "    ) -> str | None:\n",
        "    cleaned_generated_code = None\n",
        "    print(f\"  Starting LLM generation for: {prompt_name}\")\n",
        "    try:\n",
        "        inputs = llm_tokenizer(\n",
        "            prompt_text, return_tensors=\"pt\"\n",
        "        ).to(llm_model.device)\n",
        "        prompt_len = inputs['input_ids'].shape[1]\n",
        "        # print(f\"    Tokenized prompt length: {prompt_len} tokens.\") # Optional\n",
        "\n",
        "        gen_args = {\n",
        "            \"input_ids\": inputs['input_ids'], \"attention_mask\": inputs['attention_mask'],\n",
        "            \"max_new_tokens\": max_new_tokens_gen, \"pad_token_id\": llm_tokenizer.eos_token_id,\n",
        "            \"repetition_penalty\": repetition_penalty_gen, \"stopping_criteria\": stopping_criteria_list_gen\n",
        "        }\n",
        "        if do_sample_gen:\n",
        "            gen_args.update({\n",
        "                \"temperature\": temperature_gen, \"top_p\": top_p_gen,\n",
        "                \"top_k\": top_k_gen, \"do_sample\": True\n",
        "            })\n",
        "        else:\n",
        "            gen_args[\"do_sample\"] = False\n",
        "\n",
        "        gen_start = time.time()\n",
        "        with torch.no_grad(): output_ids = llm_model.generate(**gen_args)\n",
        "        print(f\"    LLM generation for '{prompt_name}' finished in {time.time() - gen_start:.2f}s.\")\n",
        "\n",
        "        generated_ids_part = output_ids[0, prompt_len:]\n",
        "        raw_output = llm_tokenizer.decode(generated_ids_part, skip_special_tokens=True)\n",
        "        # print(f\"\\n    Raw LLM Output for '{prompt_name}' (first 300 chars):\\n{textwrap.shorten(raw_output, 300, placeholder='...')}\") # Optional\n",
        "\n",
        "        # Using your preferred cleaning logic (can be the external function if defined)\n",
        "        # For simplicity, embedding it here. If you defined `extract_code_from_llm_output` earlier, call that.\n",
        "        match = re.search(r\"```python\\n(.*?)(?:\\n```|\\Z)\", raw_output, re.DOTALL | re.IGNORECASE)\n",
        "        if match: cleaned_generated_code = match.group(1).strip()\n",
        "        elif \"\\n```\" in raw_output: cleaned_generated_code = raw_output.split(\"\\n```\")[0].strip()\n",
        "        else: cleaned_generated_code = raw_output.strip()\n",
        "\n",
        "        # print(f\"    Cleaned code for '{prompt_name}' (first 300 chars):\\n{textwrap.shorten(cleaned_generated_code, 300, placeholder='...') if cleaned_generated_code else '[No code extracted]'}\")\n",
        "\n",
        "    except torch.cuda.OutOfMemoryError as e: cleaned_generated_code = None; print(f\"  ‚ùå OOM ERROR during '{prompt_name}' generation: {e}\")\n",
        "    except Exception as e: cleaned_generated_code = None; print(f\"  ‚ùå ERROR during '{prompt_name}' generation: {type(e).__name__}: {e}\")\n",
        "\n",
        "    return cleaned_generated_code"
      ],
      "metadata": {
        "id": "GpVd94pKAtGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqOSW9JBR6Fc",
        "outputId": "c7234419-1fce-4c99-97b6-403c2def2003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Global LLM Generation Parameters now set/confirmed:\n",
            "    MAX_NEW_TOKENS=1024, TEMPERATURE=0.6\n",
            "    TOP_P=0.95, TOP_K=50\n",
            "    REPETITION_PENALTY=1.1, DO_SAMPLE=True\n",
            "    Custom Stopping: STOP_ON_EOS=True, STOP_ON_CODE_END=True\n",
            "  Custom 'EosAndCodeEndStoppingCriteria' class defined.\n",
            "  Global 'llm_stopping_criteria_global' (for EOS/code end) created successfully.\n"
          ]
        }
      ],
      "source": [
        "MAX_NEW_TOKENS = globals().get('MAX_NEW_TOKENS', 1024)\n",
        "TEMPERATURE = globals().get('TEMPERATURE', 0.6)\n",
        "TOP_P = globals().get('TOP_P', 0.95)\n",
        "TOP_K = globals().get('TOP_K', 50)\n",
        "REPETITION_PENALTY = globals().get('REPETITION_PENALTY', 1.1)\n",
        "DO_SAMPLE = globals().get('DO_SAMPLE', True)\n",
        "STOP_ON_EOS = globals().get('STOP_ON_EOS', True)           # For custom stopping criteria\n",
        "STOP_ON_CODE_END = globals().get('STOP_ON_CODE_END', True) # For custom stopping criteria\n",
        "\n",
        "print(f\"  Global LLM Generation Parameters now set/confirmed:\")\n",
        "print(f\"    MAX_NEW_TOKENS={MAX_NEW_TOKENS}, TEMPERATURE={TEMPERATURE if DO_SAMPLE else 'N/A (Greedy)'}\")\n",
        "print(f\"    TOP_P={TOP_P if DO_SAMPLE else 'N/A'}, TOP_K={TOP_K if DO_SAMPLE else 'N/A'}\")\n",
        "print(f\"    REPETITION_PENALTY={REPETITION_PENALTY}, DO_SAMPLE={DO_SAMPLE}\")\n",
        "print(f\"    Custom Stopping: STOP_ON_EOS={STOP_ON_EOS}, STOP_ON_CODE_END={STOP_ON_CODE_END}\")\n",
        "\n",
        "# --- 2. Define Custom Stopping Criteria Class (if not already globally defined) ---\n",
        "# This class is defined once.\n",
        "if 'EosAndCodeEndStoppingCriteria' not in globals(): # Define only if not already defined\n",
        "    class EosAndCodeEndStoppingCriteria(StoppingCriteria):\n",
        "        \"\"\"Stops generation on EOS token or a specific code-ending sequence.\"\"\"\n",
        "        def __init__(self, tokenizer_instance, stop_on_eos_token=True, code_end_sequence=\"\\n```\\n\"):\n",
        "            self.tokenizer = tokenizer_instance\n",
        "            self.stop_on_eos = stop_on_eos_token\n",
        "            self.code_end_sequence_str = code_end_sequence\n",
        "            self.code_end_sequence_ids = []\n",
        "            if self.code_end_sequence_str and self.tokenizer: # Ensure tokenizer is valid\n",
        "                try:\n",
        "                    self.code_end_sequence_ids = self.tokenizer.encode(\n",
        "                        self.code_end_sequence_str,\n",
        "                        add_special_tokens=False\n",
        "                    )\n",
        "                except Exception as e_encode:\n",
        "                    print(f\"    WARNING: Failed to encode stop sequence '{self.code_end_sequence_str}': {e_encode}\")\n",
        "                    self.code_end_sequence_ids = [] # Ensure it's empty on failure\n",
        "\n",
        "        def __call__(self, current_ids: torch.LongTensor, current_scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "            if self.stop_on_eos and self.tokenizer and hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:\n",
        "                if current_ids[0, -1] == self.tokenizer.eos_token_id:\n",
        "                    return True\n",
        "            if self.code_end_sequence_ids: # Only check if sequence IDs were successfully created\n",
        "                seq_len = len(self.code_end_sequence_ids)\n",
        "                if current_ids.shape[1] >= seq_len:\n",
        "                    if torch.equal(current_ids[0, -seq_len:], torch.tensor(self.code_end_sequence_ids).to(current_ids.device)):\n",
        "                        return True\n",
        "            return False\n",
        "    print(\"  Custom 'EosAndCodeEndStoppingCriteria' class defined.\")\n",
        "else:\n",
        "    print(\"  Custom 'EosAndCodeEndStoppingCriteria' class already defined.\")\n",
        "\n",
        "\n",
        "# --- 3. Instantiate Global Stopping Criteria List ---\n",
        "# This `llm_stopping_criteria_global` will be used by all generation calls needing these criteria.\n",
        "llm_stopping_criteria_global = None\n",
        "if 'tokenizer' not in globals() or tokenizer is None: # Check if global 'tokenizer' is loaded\n",
        "    print(\"  WARNING (Cell 7.1): Global 'tokenizer' not found or is None. \"\n",
        "          \"Custom stopping criteria cannot be created. LLM will use default stopping.\")\n",
        "elif STOP_ON_EOS or STOP_ON_CODE_END: # Only create if flags are true\n",
        "    try:\n",
        "        # Ensure EosAndCodeEndStoppingCriteria is defined before calling it\n",
        "        if 'EosAndCodeEndStoppingCriteria' not in globals():\n",
        "             raise NameError(\"EosAndCodeEndStoppingCriteria class not defined prior to instantiation.\")\n",
        "\n",
        "        eos_code_ender_criteria_instance = EosAndCodeEndStoppingCriteria(\n",
        "            tokenizer, # Use the globally loaded LLM tokenizer\n",
        "            stop_on_eos_token=STOP_ON_EOS,\n",
        "            code_end_sequence=\"\\n```\\n\" if STOP_ON_CODE_END else None\n",
        "        )\n",
        "        llm_stopping_criteria_global = StoppingCriteriaList([eos_code_ender_criteria_instance])\n",
        "        print(\"  Global 'llm_stopping_criteria_global' (for EOS/code end) created successfully.\")\n",
        "    except Exception as e_stop_crit_create:\n",
        "        print(f\"  WARNING (Cell 7.1): Failed to create global custom stopping criteria: {type(e_stop_crit_create).__name__}: {e_stop_crit_create}\")\n",
        "        llm_stopping_criteria_global = None # Ensure it's None on failure\n",
        "else:\n",
        "    print(\"  Global custom stopping criteria (EOS/code end) are disabled by configuration flags (STOP_ON_EOS/STOP_ON_CODE_END).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 's6_final_rag_prompt_output' not in locals() or not s6_final_rag_prompt_output:\n",
        "    raise NameError(\"Input prompt 's6_final_rag_prompt_output' from Section 6 not found. Run Section 6 first.\")\n",
        "if 'model' not in locals() or 'tokenizer' not in locals():\n",
        "    raise NameError(\"LLM 'model' or 'tokenizer' not defined. Ensure Section 2 has run.\")\n",
        "if 'generate_llm_code_and_clean' not in locals(): # Helper defined before 7.1\n",
        "    raise NameError(\"Helper function 'generate_llm_code_and_clean' not defined. Ensure it's defined before Section 7.1.\")\n",
        "\n",
        "# Ensure global generation params and llm_stopping_criteria_global are available from Cell 7.1\n",
        "required_globals_for_7_2 = [\n",
        "    'MAX_NEW_TOKENS', 'DO_SAMPLE', 'TEMPERATURE', 'TOP_P', 'TOP_K',\n",
        "    'REPETITION_PENALTY', 'llm_stopping_criteria_global' # Note: llm_stopping_criteria_global can be None\n",
        "]\n",
        "if any(p not in globals() for p in required_globals_for_7_2):\n",
        "    raise NameError(f\"One or more global LLM generation parameters/criteria from Cell 7.1 are missing.\")\n",
        "\n",
        "print(f\"  Using RAG prompt (length: {len(s6_final_rag_prompt_output)} chars) from Section 6.\")\n",
        "\n",
        "# --- 2. Call Reusable LLM Generation Function ---\n",
        "s7_generated_code_rag_demo = generate_llm_code_and_clean(\n",
        "    prompt_text=s6_final_rag_prompt_output,\n",
        "    llm_model=model,\n",
        "    llm_tokenizer=tokenizer,\n",
        "    max_new_tokens_gen=MAX_NEW_TOKENS,         # Global param\n",
        "    do_sample_gen=DO_SAMPLE,                  # Global param\n",
        "    temperature_gen=TEMPERATURE,              # Global param\n",
        "    top_p_gen=TOP_P,                          # Global param\n",
        "    top_k_gen=TOP_K,                          # Global param\n",
        "    repetition_penalty_gen=REPETITION_PENALTY,# Global param\n",
        "    stopping_criteria_list_gen=llm_stopping_criteria_global, # From Cell 7.1 (can be None)\n",
        "    prompt_name=\"RAG Demo Prompt (S7.2)\"      # For logging within the helper\n",
        ")\n",
        "\n",
        "# --- 3. Display Result ---\n",
        "if s7_generated_code_rag_demo is not None: # Check if helper returned code (not None for error)\n",
        "    print(\"\\n  --- Cleaned Generated RAG Code (Demo) ---\")\n",
        "    # Displaying a significant portion for review\n",
        "    # print(textwrap.shorten(s7_generated_code_rag_demo, width=1000, placeholder=\"... (cleaned code truncated for display) ...\"))\n",
        "    # To print the entire generated code if needed:\n",
        "    print(\"\\n  Full Cleaned Generated RAG Code (Demo):\\n\", s7_generated_code_rag_demo)\n",
        "    print(f\"\\n--- RAG Demo Generation Complete. Result in 's7_generated_code_rag_demo'. ---\")\n",
        "else:\n",
        "    print(f\"\\n--- RAG Demo Generation Failed or Produced No Valid Code Output. 's7_generated_code_rag_demo' is None. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiiNEkPX9nRT",
        "outputId": "283896da-c4e7-46fd-d472-be650ff139c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Using RAG prompt (length: 6859 chars) from Section 6.\n",
            "  Starting LLM generation for: RAG Demo Prompt (S7.2)\n",
            "    LLM generation for 'RAG Demo Prompt (S7.2)' finished in 33.04s.\n",
            "\n",
            "  --- Cleaned Generated RAG Code (Demo) ---\n",
            "\n",
            "  Full Cleaned Generated RAG Code (Demo):\n",
            " \"\"\"\n",
            "Test whether you can call all functions within the module.\n",
            "If you cannot, try to use asserts instead of exceptions.\n",
            "\"\"\"\n",
            "import sys\n",
            "from pyscf import gto, scf, ao2mo\n",
            "try:\n",
            "    from pyscf import mcscf, scf\n",
            "except ImportError:\n",
            "    raise ImportError(\"Cannot import pyscf\")\n",
            "finally:\n",
            "    if \"--no-pymcscf\" in sys.argv:\n",
            "        del globals()[\"mcscf\"]\n",
            "print(__doc__)\n",
            "if __name__ == \"__main__\":\n",
            "    # Create a molecule\n",
            "    mol = gto.Mole()\n",
            "    mol.atom = [\n",
            "        [\"N\", (0.0, 0.0, 0.0)],\n",
            "        [\"H\", (0.0, 0.0, 1.0)],\n",
            "        [\"H\", (0.0, 0.0, -1.0)]\n",
            "    ]\n",
            "    mol.basis = \"cc-pVDZ\"\n",
            "    mol.build()\n",
            "    \n",
            "    # Perform SCF calculation\n",
            "    mc = scf.RHF(mol)\n",
            "    mc.kernel()\n",
            "    \n",
            "    # Calculate MO coefficients and MO integrals\n",
            "    mo_coeff, mo_integrals = mc.mo_coeff, mc.mo_integrals\n",
            "        \n",
            "    # Construct density matrix\n",
            "    rdm1 = ao2mo.ao2mo([mo_coeff, mo_coeff])\n",
            "            \n",
            "    # Calculate one-electron and two-electron part contributions\n",
            "    e1e2 = mcscf.two_electron(rdm1, mo_coeff, mol)\n",
            "    \n",
            "    # Calculate new total energy\n",
            "    nelec = sum((mo_coeff[i].conjugate().dot(mo_coeff[i]))*mo_integrals[i][i] / 2\n",
            "                 for i in range(len(mo_coeff)))\n",
            "    new_energy = mc.e_tot - ((nelec + 0.5)*e1e2)/mc.nalpha\n",
            "    print(\"%9.4f\" % new_energy)\n",
            "\n",
            "--- RAG Demo Generation Complete. Result in 's7_generated_code_rag_demo'. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0TPBQavR-OM"
      },
      "source": [
        "## Section 8: Baseline LLM Code Generation\n",
        "\n",
        "This section generates code using the LLM for the **baseline prompt** corresponding to the same demo sample instruction analyzed in Section 5 (`s5_instruction_to_s6`). It does **not** use any retrieved RAG context.\n",
        "\n",
        "**Workflow:**\n",
        "1.  **Prompt Builder Selection:** Chooses the appropriate `build_baseline_prompt_vX` function.\n",
        "2.  **Baseline Prompt Construction:** Creates the baseline prompt using the demo instruction.\n",
        "3.  **LLM Generation:** Calls the reusable `generate_llm_code_and_clean` helper function with the baseline prompt. It uses the same global LLM generation parameters and stopping criteria as defined in Section 7.1 for consistency in comparing RAG vs. Baseline outputs.\n",
        "4.  **Output:** Displays the cleaned baseline-generated code and stores it in `s8_generated_code_baseline_demo`.\n",
        "\n",
        "This allows for a direct comparison between the RAG-augmented output (from Section 7) and the LLM's output with only the instruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWWgseLgR_Xd",
        "outputId": "ba82ae19-ed72-4612-bd04-9c0b8d236bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Using Baseline Prompt Builder: build_baseline_prompt_v6\n",
            "  Starting LLM generation for: Baseline Demo Prompt\n",
            "    LLM generation for 'Baseline Demo Prompt' finished in 37.74s.\n",
            "\n",
            "  --- Cleaned Generated Baseline Code (Demo) ---\n",
            "\n",
            "  Full Cleaned Generated RAG Code (Demo):\n",
            " \"\"\"\n",
            "This is my program description: It takes in an input file containing a list of atom types and coordinates, along with atomic basis sets, and uses pyscf to generate a molecule object. It then uses pyscf to perform two state calculations with DFT, storing the molecular orbital information into separate chkfiles. It then reads the MO coefficients and occupation numbers from these chkfiles. Afterwards, it calculates the overlap between two determinants, constructs density matrices, calculates one-electron and two-electron part contributions, and calculates new total energy. Finally, it calculates the effective electronic coupling and prints the results. It also removes the chkfiles after completion.\n",
            "\"\"\"\n",
            "import pyscf\n",
            "from pyscf import gto, scf, ao2mo\n",
            "import numpy as np\n",
            "\n",
            "def main():\n",
            "    # Read input file\n",
            "    with open('input.txt', 'r') as f:\n",
            "        lines = f.readlines()\n",
            "    \n",
            "    # Create molecule object\n",
            "    mol = gto.Mole()\n",
            "    mol.atom = [(int(line.split()[0]), float(line.split()[1]), float(line.split()[2])) for line in lines if int(line.split()[0]) != 0]\n",
            "    mol.basis = 'sto-3g'\n",
            "    mol.build()\n",
            "    \n",
            "    # Perform DFT calculation\n",
            "    mf = scf.RHF(mol)\n",
            "    mf.kernel()\n",
            "    mf.make_rdm1()\n",
            "    mf.make_rdm2()\n",
            "    \n",
            "    # Calculate overlap between two determinants\n",
            "    rdm1 = np.load('h2o_a.npy')\n",
            "    rdm2 = np.load('h2o_b.npy')\n",
            "    overlap = (np.dot(rdm1, rdm2.T))\n",
            "    \n",
            "    # Construct density matrices\n",
            "    rdm1_ao = ao2mo.kernel(mf, ao2mo.basis(mol.basis, mol.nao_up(), mol.nao_down()))\n",
            "    rdm2_ao = ao2mo.kernel(mf, ao2mo.basis(mol.basis, mol.nao_up(), mol.nao_down()))\n",
            "    \n",
            "    # Calculate one-electron and two-electron part contributions\n",
            "    mo_energy = 0\n",
            "    for i in range(len(overlap)):\n",
            "        mo_energy += overlap[i][i]\n",
            "        \n",
            "    # Calculate new total energy\n",
            "    new_energy = -4 * mo_energy / len(mol.atom) + mf.e_tot\n",
            "    \n",
            "    # Calculate effective electronic coupling\n",
            "    effective_coupling = abs((new_energy - mf.e_tot) / mf.e_tot)\n",
            "    \n",
            "    # Print results\n",
            "    print(\"Effective electronic coupling:\", effective_coupling)\n",
            "    \n",
            "if __name__ == '__main__':\n",
            "    main()\n",
            "\n",
            "--- Baseline Demo Generation Complete. Result in 's8_generated_code_baseline_demo'. ---\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Ensure Prerequisite variables and functions are defined ---\n",
        "# From Section 5:\n",
        "if 's5_instruction_to_s6' not in locals() or not s5_instruction_to_s6:\n",
        "    raise NameError(\"Instruction 's5_instruction_to_s6' from Section 5 not found. Run Section 5 first.\")\n",
        "# From earlier sections (or Section 7.1):\n",
        "if 'model' not in locals() or 'tokenizer' not in locals():\n",
        "    raise NameError(\"LLM 'model' or 'tokenizer' not defined.\")\n",
        "if 'generate_llm_code_and_clean' not in locals():\n",
        "    raise NameError(\"Helper function 'generate_llm_code_and_clean' not defined.\")\n",
        "# Ensure global generation params (MAX_NEW_TOKENS, etc.) and llm_stopping_criteria_global are set (typically in Sec 7.1)\n",
        "required_gen_params_s8 = ['MAX_NEW_TOKENS', 'REPETITION_PENALTY', 'DO_SAMPLE', 'TEMPERATURE', 'TOP_P', 'TOP_K', 'llm_stopping_criteria_global']\n",
        "if any(p not in globals() for p in required_gen_params_s8):\n",
        "    raise NameError(f\"One or more global LLM generation parameters/criteria needed for baseline are missing. Run Section 7.1.\")\n",
        "\n",
        "# --- 2. Select the Baseline Prompt Builder Function ---\n",
        "CHOSEN_BASELINE_PROMPT_BUILDER = build_baseline_prompt_v6\n",
        "\n",
        "if 'CHOSEN_BASELINE_PROMPT_BUILDER' not in locals() or not callable(CHOSEN_BASELINE_PROMPT_BUILDER):\n",
        "    if 'build_baseline_prompt_v1' in globals(): # Generic name\n",
        "        CHOSEN_BASELINE_PROMPT_BUILDER = build_baseline_prompt_v1\n",
        "    elif 'build_baseline_prompt_v6' in globals(): # Specific version\n",
        "        CHOSEN_BASELINE_PROMPT_BUILDER = build_baseline_prompt_v6\n",
        "    else:\n",
        "        raise NameError(\"A suitable 'build_baseline_prompt_vX' or 'build_baseline_prompt_v1' function is not defined.\")\n",
        "print(f\"  Using Baseline Prompt Builder: {CHOSEN_BASELINE_PROMPT_BUILDER.__name__}\")\n",
        "\n",
        "# --- 3. Construct Baseline Prompt ---\n",
        "s8_generated_code_baseline_demo = None # Initialize output\n",
        "\n",
        "# Using s5_instruction_to_s6 (the instruction for the demo sample from Section 5)\n",
        "baseline_prompt_s8 = CHOSEN_BASELINE_PROMPT_BUILDER(s5_instruction_to_s6)\n",
        "\n",
        "# print(\"\\n  Baseline Prompt (first 500 chars):\")\n",
        "# print(textwrap.shorten(baseline_prompt_s8, width=500, placeholder=\"...\"))\n",
        "\n",
        "# --- 4. Call Reusable LLM Generation Function ---\n",
        "s8_generated_code_baseline_demo = generate_llm_code_and_clean(\n",
        "    prompt_text=baseline_prompt_s8,\n",
        "    llm_model=model,\n",
        "    llm_tokenizer=tokenizer,\n",
        "    max_new_tokens_gen=MAX_NEW_TOKENS, # Global param\n",
        "    do_sample_gen=DO_SAMPLE,           # Global param\n",
        "    temperature_gen=TEMPERATURE,       # Global param\n",
        "    top_p_gen=TOP_P,                   # Global param\n",
        "    top_k_gen=TOP_K,                   # Global param\n",
        "    repetition_penalty_gen=REPETITION_PENALTY, # Global param\n",
        "    stopping_criteria_list_gen=llm_stopping_criteria_global, # From Sec 7.1\n",
        "    prompt_name=\"Baseline Demo Prompt\"\n",
        ")\n",
        "\n",
        "if s8_generated_code_baseline_demo is not None:\n",
        "    print(\"\\n  --- Cleaned Generated Baseline Code (Demo) ---\")\n",
        "    # print(textwrap.shorten(s8_generated_code_baseline_demo, width=700, placeholder=\"... (code truncated) ...\"))\n",
        "    print(\"\\n  Full Cleaned Generated RAG Code (Demo):\\n\", s8_generated_code_baseline_demo)\n",
        "    print(f\"\\n--- Baseline Demo Generation Complete. Result in 's8_generated_code_baseline_demo'. ---\")\n",
        "else:\n",
        "    print(f\"\\n--- Baseline Demo Generation Failed or No Output. ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLbYVy06SGS5"
      },
      "source": [
        "## Section 9 ¬∑ Metrics Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dubtt5LHdYHJ",
        "outputId": "daf0ab83-de12-471c-aa7f-2104ef165b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Using RAG Prompt Builder for Evaluation: build_rag_prompt_v6\n",
            "  Using Baseline Prompt Builder for Evaluation: build_baseline_prompt_v6\n",
            "  BM25 Params for Evaluation: K1=1.5, B=0.75, Top-K for prompt=5\n",
            "  BM25 Tokenizer for Evaluation: eval_robust_code_tokenizer_s9\n",
            "  Custom 'eval_llm_stopping_criteria' for evaluation run created successfully.\n",
            "  LLM Config: MaxNew=1024, Temp=0.5, DoSample=True, StopCriteriaIsSet=Yes\n"
          ]
        }
      ],
      "source": [
        "import re # For the tokenizer if defined here again\n",
        "\n",
        "# --- 1. Number of Samples for Evaluation ---\n",
        "# NUM_EXAMPLES_TO_EVALUATE = len(lca_dataset_split) if 'lca_dataset_split' in globals() else 10 # Evaluate all (or by defaul 10)\n",
        "NUM_EXAMPLES_TO_EVALUATE = 2  # Subset for a quicker test run\n",
        "\n",
        "\n",
        "# --- 2. Select Prompt Builders for Evaluation (Defined Globally) ---\n",
        "EVAL_RAG_PROMPT_BUILDER = build_rag_prompt_v6 # RAG Prompt\n",
        "EVAL_BASELINE_PROMPT_BUILDER = build_baseline_prompt_v6 # Baseline Prompt\n",
        "\n",
        "print(f\"  Using RAG Prompt Builder for Evaluation: {EVAL_RAG_PROMPT_BUILDER.__name__}\")\n",
        "print(f\"  Using Baseline Prompt Builder for Evaluation: {EVAL_BASELINE_PROMPT_BUILDER.__name__}\")\n",
        "\n",
        "\n",
        "# --- 3. BM25 Parameters for Evaluation ---\n",
        "EVAL_BM25_K1 = 1.5\n",
        "EVAL_BM25_B = 0.75\n",
        "EVAL_TOP_K_SNIPPETS_FOR_PROMPT = 5 # Adjust this to select the number of snippets to retrieve\n",
        "\n",
        "# Define/Select the BM25 tokenizer for evaluation\n",
        "def eval_robust_code_tokenizer_s9(text_input):\n",
        "    if not isinstance(text_input, str): return []\n",
        "    text = text_input.lower()\n",
        "    raw_tokens = re.split(r'[^a-z0-9_]+', text)\n",
        "    return [token for token in raw_tokens if token and len(token) > 1 and not token.isdigit()]\n",
        "EVAL_BM25_TOKENIZER = eval_robust_code_tokenizer_s9\n",
        "\n",
        "print(f\"  BM25 Params for Evaluation: K1={EVAL_BM25_K1}, B={EVAL_BM25_B}, Top-K for prompt={EVAL_TOP_K_SNIPPETS_FOR_PROMPT}\")\n",
        "print(f\"  BM25 Tokenizer for Evaluation: {EVAL_BM25_TOKENIZER.__name__}\")\n",
        "\n",
        "\n",
        "# --- 4. LLM Generation Parameters for Evaluation ---\n",
        "# LLM parameters\n",
        "EVAL_MAX_NEW_TOKENS = 384\n",
        "EVAL_TEMPERATURE = 0.5  # Potentially more deterministic for evaluation\n",
        "EVAL_TOP_P = 0.95\n",
        "EVAL_TOP_K = 50\n",
        "EVAL_REPETITION_PENALTY = 1.1\n",
        "EVAL_DO_SAMPLE = True    # Set to False for deterministic greedy decoding during evaluation if preferred\n",
        "\n",
        "# Stopping criteria\n",
        "EVAL_STOP_ON_EOS = True\n",
        "EVAL_STOP_ON_CODE_END = True\n",
        "\n",
        "eval_llm_stopping_criteria = None\n",
        "if 'tokenizer' not in globals() or tokenizer is None:\n",
        "    print(\"  WARNING: Global 'tokenizer' not found. Custom stopping criteria for evaluation cannot be created.\")\n",
        "elif 'EosAndCodeEndStoppingCriteria' not in globals():\n",
        "    print(\"  WARNING: 'EosAndCodeEndStoppingCriteria' class not defined (expected from Sec 7.1). Cannot create custom stopping criteria.\")\n",
        "elif EVAL_STOP_ON_EOS or EVAL_STOP_ON_CODE_END:\n",
        "    try:\n",
        "        eval_stopper_instance = EosAndCodeEndStoppingCriteria( # Class from 7.1\n",
        "            tokenizer, # Global tokenizer\n",
        "            stop_on_eos_token=EVAL_STOP_ON_EOS,\n",
        "            code_end_sequence=\"\\n```\\n\" if EVAL_STOP_ON_CODE_END else None\n",
        "        )\n",
        "        eval_llm_stopping_criteria = StoppingCriteriaList([eval_stopper_instance])\n",
        "        print(\"  Custom 'eval_llm_stopping_criteria' for evaluation run created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  WARNING: Failed to create custom stopping criteria for evaluation: {e}\")\n",
        "else:\n",
        "    print(\"  Custom stopping criteria (EOS/code end) for evaluation are disabled by EVAL_ flags.\")\n",
        "\n",
        "print(f\"  LLM Config: MaxNew={EVAL_MAX_NEW_TOKENS}, Temp={EVAL_TEMPERATURE if EVAL_DO_SAMPLE else 'N/A'}, DoSample={EVAL_DO_SAMPLE}, StopCriteriaIsSet={'Yes' if eval_llm_stopping_criteria else 'No'}\")\n",
        "\n",
        "\n",
        "# --- 5. Verify other critical dependencies for Cell 9.2 ---\n",
        "critical_deps_for_9_2 = [\n",
        "    'model', 'generate_llm_code_and_clean',\n",
        "    'GITHUB_RAW_CONTENT_BASE_URL', 'LOCAL_TEMP_KB_DOWNLOAD_DIR', 'download_github_raw_json'\n",
        "]\n",
        "if any(dep not in globals() for dep in critical_deps_for_9_2):\n",
        "    raise NameError(f\"One or more critical global dependencies for Cell 9.2 are missing: {critical_deps_for_9_2}. \"\n",
        "                    \"Ensure all prior setup sections (LLM loading, GitHub config, helper functions) have run.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import textwrap # For debug printing snippets\n",
        "\n",
        "# --- 1. Verify All Necessary Configurations and Dependencies ---\n",
        "# (Assuming these checks are passed based on previous discussions and successful 9.1 run)\n",
        "# ... (You can re-add the comprehensive checks from previous versions if desired for absolute safety) ...\n",
        "if 'EVAL_RAG_PROMPT_BUILDER' not in globals() or 'generate_llm_code_and_clean' not in globals(): # Quick check\n",
        "    raise NameError(\"Essential evaluation configurations or helper functions from 9.1 or earlier are missing.\")\n",
        "\n",
        "print(f\"\\n--- Starting Main Evaluation Loop for {NUM_EXAMPLES_TO_EVALUATE} Samples ---\")\n",
        "\n",
        "# --- 2. Initialize Lists for Storing All Evaluation Results ---\n",
        "eval_all_baseline_outputs, eval_all_rag_outputs = [], []\n",
        "eval_all_references, eval_all_reference_apis    = [], []\n",
        "\n",
        "# --- 3. Initialize BM25 Index Cache ---\n",
        "eval_bm25_index_cache = {} # {library_key: (bm25_index, valid_docs, map_idx_to_valid, tokenized_corpus)}\n",
        "\n",
        "# --- 4. Select Dataset Subset for Evaluation ---\n",
        "dataset_for_this_eval_run = lca_dataset_split.select(\n",
        "    range(min(NUM_EXAMPLES_TO_EVALUATE, len(lca_dataset_split)))\n",
        ")\n",
        "actual_num_examples_being_evaluated = len(dataset_for_this_eval_run)\n",
        "\n",
        "# --- 5. Main Evaluation Loop ---\n",
        "for eval_sample_idx, current_eval_sample in enumerate(tqdm(dataset_for_this_eval_run, desc=\"‚öôÔ∏è Evaluating Samples\")):\n",
        "    eval_instruction_text = current_eval_sample.get(\"instruction\")\n",
        "    eval_clean_ref_code = current_eval_sample.get(\"clean_reference\")\n",
        "    eval_ref_api_list = current_eval_sample.get(\"unique_apis\", [])\n",
        "    eval_sample_repo_key = current_eval_sample.get('repo_full_name')\n",
        "\n",
        "    # --- Initialize outputs for this sample to ensure append happens ---\n",
        "    generated_baseline_code_eval = \"\" # Default to empty string\n",
        "    generated_rag_code_eval = \"\"      # Default to empty string\n",
        "\n",
        "    # Minimal print per sample to reduce log verbosity\n",
        "    if (eval_sample_idx + 1) % max(1, actual_num_examples_being_evaluated // 10) == 0 or \\\n",
        "       eval_sample_idx == actual_num_examples_being_evaluated - 1 :\n",
        "        print(f\"\\n  Processing Eval Sample {eval_sample_idx + 1}/{actual_num_examples_being_evaluated}: Library '{eval_sample_repo_key}'\")\n",
        "\n",
        "    if not eval_instruction_text:\n",
        "        print(f\"    WARNING: Instruction missing for sample index {eval_sample_idx} (Lib: {eval_sample_repo_key}).\")\n",
        "        # Appending placeholders directly\n",
        "        eval_all_baseline_outputs.append(\"\")\n",
        "        eval_all_rag_outputs.append(\"\")\n",
        "        eval_all_references.append(eval_clean_ref_code or \"\")\n",
        "        eval_all_reference_apis.append(eval_ref_api_list or [])\n",
        "        continue\n",
        "\n",
        "    # --- A. Baseline Generation ---\n",
        "    try:\n",
        "        baseline_prompt_text_eval = EVAL_BASELINE_PROMPT_BUILDER(eval_instruction_text)\n",
        "        generated_baseline_code_eval = generate_llm_code_and_clean(\n",
        "            prompt_text=baseline_prompt_text_eval, llm_model=model, llm_tokenizer=tokenizer,\n",
        "            max_new_tokens_gen=EVAL_MAX_NEW_TOKENS, do_sample_gen=EVAL_DO_SAMPLE,\n",
        "            temperature_gen=EVAL_TEMPERATURE, top_p_gen=EVAL_TOP_P, top_k_gen=EVAL_TOP_K,\n",
        "            repetition_penalty_gen=EVAL_REPETITION_PENALTY,\n",
        "            stopping_criteria_list_gen=eval_llm_stopping_criteria, # From 9.1\n",
        "            prompt_name=f\"Baseline_Eval_{eval_sample_idx}_{eval_sample_repo_key}\"\n",
        "        )\n",
        "    except Exception as e_base:\n",
        "        print(f\"    ERROR during baseline generation for sample {eval_sample_idx}: {type(e_base).__name__}: {e_base}\")\n",
        "        # generated_baseline_code_eval remains \"\" (or None if generate_llm_code_and_clean returned None on error)\n",
        "    eval_all_baseline_outputs.append(generated_baseline_code_eval or \"\")\n",
        "\n",
        "\n",
        "    # --- B. RAG Generation ---\n",
        "    retrieved_snippets_block_for_rag = \"\"\n",
        "    try:\n",
        "        if eval_sample_repo_key:\n",
        "            current_eval_bm25_index = None\n",
        "            current_eval_valid_docs = []\n",
        "            current_eval_map_bm25idx_to_validdocidx = []\n",
        "            # Variable to hold the tokenized corpus used for building the index, for len check\n",
        "            final_bm25_corpus_for_lib_eval_for_len_check = []\n",
        "\n",
        "\n",
        "            if eval_sample_repo_key in eval_bm25_index_cache:\n",
        "                current_eval_bm25_index, current_eval_valid_docs, current_eval_map_bm25idx_to_validdocidx, final_bm25_corpus_for_lib_eval_for_len_check = eval_bm25_index_cache[eval_sample_repo_key]\n",
        "            else:\n",
        "                kb_filename_for_eval = f\"kb_{eval_sample_repo_key}.json\"\n",
        "                kb_url_for_eval = f\"{GITHUB_RAW_CONTENT_BASE_URL}/{kb_filename_for_eval}\"\n",
        "                temp_kb_save_dir_for_eval_lib = os.path.join(LOCAL_TEMP_KB_DOWNLOAD_DIR, f\"eval_kb_{eval_sample_repo_key}\")\n",
        "                kb_json_content_eval = download_github_raw_json(kb_url_for_eval, temp_kb_save_dir_for_eval_lib, kb_filename_for_eval, overwrite=False)\n",
        "\n",
        "                if kb_json_content_eval and isinstance(kb_json_content_eval, list):\n",
        "                    current_eval_valid_docs = [str(d) for d in kb_json_content_eval if isinstance(d, str) and str(d).strip()]\n",
        "                    if current_eval_valid_docs:\n",
        "                        tokenized_corpus_for_lib_eval = [EVAL_BM25_TOKENIZER(doc) for doc in current_eval_valid_docs]\n",
        "                        # final_bm25_corpus_for_lib_eval_for_len_check is defined here\n",
        "                        for i_valid, doc_tokens_eval in enumerate(tokenized_corpus_for_lib_eval):\n",
        "                            if doc_tokens_eval:\n",
        "                                final_bm25_corpus_for_lib_eval_for_len_check.append(doc_tokens_eval)\n",
        "                                current_eval_map_bm25idx_to_validdocidx.append(i_valid)\n",
        "\n",
        "                        if final_bm25_corpus_for_lib_eval_for_len_check:\n",
        "                            current_eval_bm25_index = BM25Okapi(final_bm25_corpus_for_lib_eval_for_len_check, k1=EVAL_BM25_K1, b=EVAL_BM25_B)\n",
        "                            eval_bm25_index_cache[eval_sample_repo_key] = (\n",
        "                                current_eval_bm25_index, current_eval_valid_docs,\n",
        "                                current_eval_map_bm25idx_to_validdocidx,\n",
        "                                final_bm25_corpus_for_lib_eval_for_len_check # Cache tokenized corpus\n",
        "                            )\n",
        "\n",
        "            if current_eval_bm25_index:\n",
        "                query_tokens_for_rag_eval = EVAL_BM25_TOKENIZER(eval_instruction_text)\n",
        "                if query_tokens_for_rag_eval:\n",
        "                    # Use len(current_eval_bm25_index.doc_len) as corrected\n",
        "                    num_docs_in_lib_bm25_index = len(current_eval_bm25_index.doc_len)\n",
        "                    top_indices_from_lib_bm25 = current_eval_bm25_index.get_top_n(\n",
        "                        query_tokens_for_rag_eval, list(range(num_docs_in_lib_bm25_index)),\n",
        "                        n=min(EVAL_TOP_K_SNIPPETS_FOR_PROMPT, num_docs_in_lib_bm25_index)\n",
        "                    )\n",
        "                    retrieved_snippets_list_for_prompt = [current_eval_valid_docs[current_eval_map_bm25idx_to_validdocidx[i]] for i in top_indices_from_lib_bm25]\n",
        "                    retrieved_snippets_block_for_rag = \"\\n\\n# --- Snippet ---\\n\\n\".join(retrieved_snippets_list_for_prompt)\n",
        "\n",
        "                    # --- DIAGNOSTIC PRINTS FOR RAG (Uncomment to debug specific samples) ---\n",
        "                    # if eval_sample_idx == 2 and \"weihuayi__fealpy\" in eval_sample_repo_key: # Example: For sample 2 if it's fealpy\n",
        "                    #     print(f\"      DEBUG RAG Sample {eval_sample_idx} ({eval_sample_repo_key}):\")\n",
        "                    #     print(f\"        Num retrieved snippets: {len(retrieved_snippets_list_for_prompt)}\")\n",
        "                    #     total_chars = sum(len(s) for s in retrieved_snippets_list_for_prompt)\n",
        "                    #     print(f\"        Total chars in retrieved snippets: {total_chars}\")\n",
        "                    #     for i_debug, snip_debug in enumerate(retrieved_snippets_list_for_prompt):\n",
        "                    #         print(f\"        --- Debug Snippet {i_debug+1} (Len: {len(snip_debug)}) ---\")\n",
        "                    #         print(textwrap.shorten(snip_debug, width=200, placeholder=\"...\"))\n",
        "                    # --------------------------------------------------------------------\n",
        "\n",
        "        # --- Assemble RAG Prompt (outside the BM25 block, uses retrieved_snippets_block_for_rag) ---\n",
        "        eval_pass_tokenizer_flag = False\n",
        "        if 'tokenizer' in globals():\n",
        "            try:\n",
        "                eval_func_code_obj = getattr(EVAL_RAG_PROMPT_BUILDER, '__code__', None)\n",
        "                if eval_func_code_obj and \"truncate_to_n_tokens\" in eval_func_code_obj.co_names:\n",
        "                    eval_pass_tokenizer_flag = True\n",
        "            except AttributeError: pass\n",
        "        if eval_pass_tokenizer_flag and 'tokenizer' not in globals(): eval_pass_tokenizer_flag = False\n",
        "\n",
        "        if eval_pass_tokenizer_flag:\n",
        "            current_rag_prompt_text = EVAL_RAG_PROMPT_BUILDER(eval_instruction_text, retrieved_snippets_block_for_rag, tokenizer)\n",
        "        else:\n",
        "            current_rag_prompt_text = EVAL_RAG_PROMPT_BUILDER(eval_instruction_text, retrieved_snippets_block_for_rag)\n",
        "\n",
        "        # --- DIAGNOSTIC PRINT FOR RAG PROMPT LENGTH (Uncomment to debug) ---\n",
        "        # if eval_sample_idx == 2 and \"weihuayi__fealpy\" in eval_sample_repo_key:\n",
        "        #     if current_rag_prompt_text:\n",
        "        #         rag_prompt_tokens = tokenizer(current_rag_prompt_text, return_tensors=\"pt\")['input_ids'].shape[1]\n",
        "        #         print(f\"      DEBUG RAG Sample {eval_sample_idx} ({eval_sample_repo_key}): Final RAG prompt input token length: {rag_prompt_tokens}\")\n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        generated_rag_code_eval = generate_llm_code_and_clean(\n",
        "            prompt_text=current_rag_prompt_text, llm_model=model, llm_tokenizer=tokenizer,\n",
        "            max_new_tokens_gen=EVAL_MAX_NEW_TOKENS, do_sample_gen=EVAL_DO_SAMPLE, temperature_gen=EVAL_TEMPERATURE,\n",
        "            top_p_gen=EVAL_TOP_P, top_k_gen=EVAL_TOP_K, repetition_penalty_gen=EVAL_REPETITION_PENALTY,\n",
        "            stopping_criteria_list_gen=eval_llm_stopping_criteria, prompt_name=f\"RAG_Eval_{eval_sample_idx}_{eval_sample_repo_key}\"\n",
        "        )\n",
        "    except Exception as e_rag:\n",
        "        print(f\"    ERROR during RAG processing for sample {eval_sample_idx}: {type(e_rag).__name__}: {e_rag}\")\n",
        "        # generated_rag_code_eval remains \"\" (or None if generate_llm_code_and_clean returned None)\n",
        "    eval_all_rag_outputs.append(generated_rag_code_eval or \"\")\n",
        "\n",
        "\n",
        "    # --- Store References ---\n",
        "    eval_all_references.append(eval_clean_ref_code or \"\")\n",
        "    eval_all_reference_apis.append(eval_ref_api_list or [])\n",
        "\n",
        "# --- Final Sanity Check ---\n",
        "if not (len(eval_all_baseline_outputs) == len(eval_all_rag_outputs) == \\\n",
        "        len(eval_all_references) == len(eval_all_reference_apis) == actual_num_examples_being_evaluated):\n",
        "    print(\"\\n‚ùå CRITICAL ERROR: Length mismatch in final evaluation output lists!\")\n",
        "    print(f\"  Expected: {actual_num_examples_being_evaluated} for all lists.\")\n",
        "    print(f\"  Baseline outputs: {len(eval_all_baseline_outputs)}\")\n",
        "    print(f\"  RAG outputs: {len(eval_all_rag_outputs)}\")\n",
        "    print(f\"  References: {len(eval_all_references)}\")\n",
        "    print(f\"  Reference APIs: {len(eval_all_reference_apis)}\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Successfully collected all outputs for {actual_num_examples_being_evaluated} evaluation examples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "3af433502b3d4364b0e5b282e1c7eb05",
            "ab2cc489ee8544db9f3e4c342df9ead9",
            "cf115fd3142642bfaa83d518765fada8",
            "0935704267754e8e9db95b9a35506993",
            "c18f0639ab844a9893b11bb44d1716b7",
            "45b4c387a5964df5ac286ea6a6e84a31",
            "272b44daf4a2431eadecd5d694598898",
            "0b7e34009a2d40129af54451bc51d15e",
            "05a964bdd43f48648632add49ba12988",
            "d68ba320661b4b0495f14b3e9d98317d",
            "a8e04eb2757e4368b02cee8b932704c0"
          ]
        },
        "id": "Mu7JiE23FMNp",
        "outputId": "9bedca6b-1f10-4f27-a5ad-7d106d00210d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Section 9.2: Starting Main Evaluation Loop for 3 Samples ---\n",
            "  Will evaluate 3 samples.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚öôÔ∏è Evaluating Samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3af433502b3d4364b0e5b282e1c7eb05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Processing Eval Sample 1/3: Library 'seed-labs__seed-emulator'\n",
            "  Starting LLM generation for: Baseline_Eval_0_seed-labs__seed-emulator\n",
            "    LLM generation for 'Baseline_Eval_0_seed-labs__seed-emulator' finished in 66.23s.\n",
            "    ERROR during RAG processing for sample 0: TypeError: build_rag_prompt_v6() takes 2 positional arguments but 3 were given\n",
            "\n",
            "  Processing Eval Sample 2/3: Library 'weihuayi__fealpy'\n",
            "  Starting LLM generation for: Baseline_Eval_1_weihuayi__fealpy\n",
            "    LLM generation for 'Baseline_Eval_1_weihuayi__fealpy' finished in 58.90s.\n",
            "    ERROR during RAG processing for sample 1: TypeError: build_rag_prompt_v6() takes 2 positional arguments but 3 were given\n",
            "\n",
            "  Processing Eval Sample 3/3: Library 'weihuayi__fealpy'\n",
            "  Starting LLM generation for: Baseline_Eval_2_weihuayi__fealpy\n",
            "    LLM generation for 'Baseline_Eval_2_weihuayi__fealpy' finished in 52.44s.\n",
            "    ERROR during RAG processing for sample 2: TypeError: build_rag_prompt_v6() takes 2 positional arguments but 3 were given\n",
            "\n",
            "‚úÖ Successfully collected all outputs for 3 evaluation examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOt-AJ_mHOWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "018e4633-ab08-42cc-cc5c-f4c234ec379c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/575.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m575.6/575.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m567.6/567.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for codebleu (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [codebleu]\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# ================================================================\n",
        "#  Metrics helpers ‚Äì with CodeBLEU support & automatic key-detection\n",
        "# ================================================================\n",
        "import importlib, warnings, re\n",
        "from importlib.metadata import version as _get_version, PackageNotFoundError\n",
        "import numpy as np\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ sacrebleu ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "import sacrebleu\n",
        "print(\"‚úÖ sacrebleu\", sacrebleu.__version__)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ codebleu ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "try:\n",
        "    from codebleu import calc_codebleu\n",
        "    try:\n",
        "        cb_ver = _get_version(\"codebleu\")\n",
        "    except PackageNotFoundError:\n",
        "        cb_ver = \"n/a\"\n",
        "    print(\"‚úÖ codebleu\", cb_ver)\n",
        "    _HAS_CODEBLEU = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  codebleu import failed ‚Äî CodeBLEU will be skipped.\")\n",
        "    _HAS_CODEBLEU = False\n",
        "\n",
        "!pip install -q --upgrade tree_sitter tree_sitter_python\n",
        "!pip install -q git+https://github.com/k4black/codebleu.git\n",
        "# run this in a fresh cell *before* any CodeBLEU import\n",
        "!pip install -q --upgrade \"tree_sitter<0.23\" \"tree_sitter_python<0.23\"\n",
        "\"\"\"\n",
        "\n",
        "!pip uninstall -yq codebleu            # throw away 0.7.0\n",
        "!pip install -q --upgrade tree_sitter tree_sitter_python  # stays at 0.24+\n",
        "!pip install -q git+https://github.com/k4black/codebleu.git  # 0.7.1-dev\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL8_lv8BzkjL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "b3bdd6b5-df23-42af-8485-b7fbdbf8d374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now running CodeBLEU 0.7.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'references' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-380e6b5cdc8f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrag_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_apis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'references' is not defined"
          ]
        }
      ],
      "source": [
        "from codebleu import calc_codebleu\n",
        "import sacrebleu\n",
        "import importlib.metadata as md\n",
        "print(\"Now running CodeBLEU\", md.version(\"codebleu\"))\n",
        "_HAS_CODEBLEU = True\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "#  Composite key auto-detector (runs once on your first example)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "_codebleu_key = None\n",
        "def _detect_codebleu_key(sample_pred, sample_ref):\n",
        "    global _codebleu_key\n",
        "    if not _HAS_CODEBLEU:\n",
        "        return None\n",
        "    res = calc_codebleu(\n",
        "        references=[[sample_ref]],\n",
        "        predictions=[sample_pred],\n",
        "        lang=\"python\",\n",
        "        weights=(0.25,0.25,0.25,0.25)\n",
        "    )\n",
        "    print(\"üîç CodeBLEU raw result keys:\", list(res.keys()))\n",
        "    # pick the first key containing ‚Äúcodebleu‚Äù (case-insensitive)\n",
        "    for k in res:\n",
        "        if \"codebleu\" in k.lower():\n",
        "            _codebleu_key = k\n",
        "            break\n",
        "    return _codebleu_key\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "#  Metric functions\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def calculate_chrf(pred, ref):\n",
        "    if not (isinstance(pred, str) and isinstance(ref, str)):\n",
        "        return None\n",
        "    if not pred or not ref:\n",
        "        return 0.0\n",
        "    return sacrebleu.corpus_chrf([pred], [[ref]]).score\n",
        "\n",
        "def calculate_codebleu(pred, ref, lang=\"python\", weights=(0.25,0.25,0.25,0.25)):\n",
        "    if not _HAS_CODEBLEU:\n",
        "        return None\n",
        "    if not (isinstance(pred, str) and isinstance(ref, str)):\n",
        "        return None\n",
        "\n",
        "    # detect key on first call\n",
        "    global _codebleu_key\n",
        "    if _codebleu_key is None:\n",
        "        _detect_codebleu_key(pred, ref)\n",
        "        if _codebleu_key is None:\n",
        "            warnings.warn(\"Could not find a CodeBLEU key in the result; returning 0.0\")\n",
        "            return 0.0\n",
        "\n",
        "    try:\n",
        "        res = calc_codebleu(\n",
        "            references=[[ref]],\n",
        "            predictions=[pred],\n",
        "            lang=lang,\n",
        "            weights=weights\n",
        "        )\n",
        "        return float(res.get(_codebleu_key, 0.0))\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"CodeBLEU failed for one example: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_api_recall(gen, ref_apis):\n",
        "    if not isinstance(gen, str) or not isinstance(ref_apis, list):\n",
        "        return 0.0\n",
        "    valid = [api for api in ref_apis if isinstance(api, str) and api.strip()]\n",
        "    if not gen or not valid:\n",
        "        return 0.0\n",
        "    hits = sum(bool(re.search(rf\"\\b{re.escape(api)}\\b\", gen)) for api in valid)\n",
        "    return hits / len(valid)\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "#  Evaluation driver (baseline vs RAG)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def _mean(fn, preds, gts):\n",
        "    \"\"\"Mean of fn(pred, gt) over a list of pairs.\"\"\"\n",
        "    vals = [fn(p, g) for p, g in zip(preds, gts)]\n",
        "    vals = [v for v in vals if v is not None]\n",
        "    return np.mean(vals) if vals else 0.0\n",
        "\n",
        "def evaluate(baseline_preds, rag_preds, refs, ref_api_lists):\n",
        "    recall_b = _mean(calculate_api_recall, baseline_preds, ref_api_lists)\n",
        "    recall_r = _mean(calculate_api_recall, rag_preds, ref_api_lists)\n",
        "\n",
        "    chrf_b   = _mean(calculate_chrf, baseline_preds, refs)\n",
        "    chrf_r   = _mean(calculate_chrf, rag_preds, refs)\n",
        "\n",
        "    cbleu_b  = _mean(calculate_codebleu, baseline_preds, refs)\n",
        "    cbleu_r  = _mean(calculate_codebleu, rag_preds, refs)\n",
        "\n",
        "    print(\"\\n--- Risultati Metriche Automatiche ---\")\n",
        "    print(f\"| Metrica    | Baseline |   RAG   |\")\n",
        "    print(f\"|------------|----------|---------|\")\n",
        "    print(f\"| API Recall | {recall_b:.4f}   | {recall_r:.4f}   |\")\n",
        "    print(f\"| ChrF       | {chrf_b:.2f}    | {chrf_r:.2f}    |\")\n",
        "    print(f\"| CodeBLEU   | {cbleu_b:.2f}    | {cbleu_r:.2f}    |\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"recall\":   (recall_b,   recall_r),\n",
        "        \"chrf\":     (chrf_b,     chrf_r),\n",
        "        \"codebleu\": (cbleu_b,    cbleu_r),\n",
        "    }\n",
        "\n",
        "\n",
        "metrics = evaluate(baseline_output, rag_output, references, reference_apis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUw1sKpXe63X"
      },
      "source": [
        "### Prompt 1\n",
        "| Metrica    | Baseline |   RAG   |\n",
        "|------------|----------|---------|\n",
        "| API Recall | 0.0000   | 0.0000   |\n",
        "| ChrF       | 17.05    | 53.43    |\n",
        "| CodeBLEU   | 0.11    | 0.43    |\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### Prompt 2\n",
        "| Metrica    | Baseline |   RAG   |\n",
        "|------------|----------|---------|\n",
        "| API Recall | 0.0000   | 0.0000   |\n",
        "| ChrF       | 25.75    | 53.43    |\n",
        "| CodeBLEU   | 0.21    | 0.43    |\n",
        "\n",
        "---\n",
        "\n",
        "### Prompt 3\n",
        "| Metrica    | Baseline |   RAG   |\n",
        "|------------|----------|---------|\n",
        "| API Recall | 0.0000   | 0.0000   |\n",
        "| ChrF       | 23.09    | 53.15    |\n",
        "| CodeBLEU   | 0.16    | 0.43    |\n",
        "\n",
        "---\n",
        "\n",
        "### Prompt 4 (corretto API)\n",
        "| Metrica    | Baseline |   RAG   |\n",
        "|------------|----------|---------|\n",
        "| API Recall | 0.0228   | 0.7427   |\n",
        "| ChrF       | 17.32    | 53.15    |\n",
        "| CodeBLEU   | 0.13    | 0.43    |\n",
        "\n",
        "---\n",
        "\n",
        "### Prompt 5\n",
        "| Metrica    | Baseline |   RAG   |\n",
        "|------------|----------|---------|\n",
        "| API Recall | 0.0265   | 0.6082   |\n",
        "| ChrF       | 21.14    | 49.75    |\n",
        "| CodeBLEU   | 0.16    | 0.41    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd-7Eni7G4vB"
      },
      "source": [
        "---\n",
        "### Prompt 5 - REDO\n",
        "| Metrica    | Baseline |   RAG   |\n",
        "|------------|----------|---------|\n",
        "| API Recall | 0.0121   | 0.1393   |\n",
        "| ChrF       | 9.78    | 11.26    |\n",
        "| CodeBLEU   | 0.13    | 0.14    |\n",
        "\n",
        "\n",
        "---\n",
        "### Prompt 6\n",
        "| Metrica    | Baseline |   RAG   |\n",
        "|------------|----------|---------|\n",
        "| API Recall | 0.0000   | 0.0861   |\n",
        "| ChrF       | 9.38    | 11.52    |\n",
        "| CodeBLEU   | 0.13    | 0.14    |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzpG7Hxk7ox6"
      },
      "outputs": [],
      "source": [
        "def count_valid(fn, preds, refs):\n",
        "    \"\"\"Count how many (pred, ref) pairs return a non‚ÄêNone metric.\"\"\"\n",
        "    return sum(1 for p, r in zip(preds, refs) if fn(p, r) is not None)\n",
        "\n",
        "# For API‚ÄêRecall we never return None, so it‚Äôs simply the full length:\n",
        "n_api = len(baseline_outputs)\n",
        "\n",
        "# For ChrF & CodeBLEU we drop any None‚Äôs:\n",
        "n_chrf     = count_valid(calculate_chrf,     baseline_outputs, references)\n",
        "n_codebleu = count_valid(calculate_codebleu, baseline_outputs, references)\n",
        "\n",
        "print(f\"API Recall was computed on {n_api} samples\")\n",
        "print(f\"ChrF       was computed on {n_chrf} samples\")\n",
        "print(f\"CodeBLEU   was computed on {n_codebleu} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmD0bqjg7rI0"
      },
      "outputs": [],
      "source": [
        "n_chrf_rag     = count_valid(calculate_chrf,     rag_outputs, references)\n",
        "n_codebleu_rag = count_valid(calculate_codebleu, rag_outputs, references)\n",
        "print(f\"(RAG) ChrF       on {n_chrf_rag} samples\")\n",
        "print(f\"(RAG) CodeBLEU   on {n_codebleu_rag} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtIyq0dr_I5F"
      },
      "source": [
        "## Section 10 ¬∑ Example result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7Cc5Z8AHgE2"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Display a sample: task + reference + baseline + RAG side by side\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from IPython.display import display, Markdown\n",
        "import textwrap\n",
        "\n",
        "EXAMPLE_INDEX = 7  # Change this to any index within your dataset size\n",
        "\n",
        "task       = references[EXAMPLE_INDEX]            # Gold reference code (cleaned)\n",
        "baseline   = baseline_outputs[EXAMPLE_INDEX]      # Generated from instruction only\n",
        "rag        = rag_outputs[EXAMPLE_INDEX]           # Generated with RAG prompt\n",
        "instruction = lca_dataset_split[EXAMPLE_INDEX]['instruction']  # Original task (English)\n",
        "\n",
        "def print_block(title, content):\n",
        "    print(f\"{title}\")\n",
        "    print(\"-\" * 10)\n",
        "    print(textwrap.dedent(content).strip())\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjF1QZW4Hr-X"
      },
      "outputs": [],
      "source": [
        "print(\"Instruction\")\n",
        "print(\"=\" * 40)\n",
        "print(textwrap.dedent(instruction).strip())\n",
        "print(\"=\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOih6Zt-H0X8"
      },
      "outputs": [],
      "source": [
        "print(\"‚úÖ Gold Reference\")\n",
        "print(\"=\" * 40)\n",
        "print(textwrap.dedent(task).strip())\n",
        "print(\"=\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gcQYsQiH9jz"
      },
      "outputs": [],
      "source": [
        "print(\"Baseline Output\")\n",
        "print(\"=\" * 40)\n",
        "print(textwrap.dedent(generated_code_baseline).strip())\n",
        "print(\"=\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CZr1Qr8IEdj"
      },
      "outputs": [],
      "source": [
        "print(\"RAG Output\")\n",
        "print(\"=\" * 40)\n",
        "print(textwrap.dedent(rag).strip())\n",
        "print(\"=\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFZV-uWWreSf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKEI7Owk_lJV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_qYnqso_lGj"
      },
      "outputs": [],
      "source": [
        "example_idx = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbwXx1vB_ol4"
      },
      "outputs": [],
      "source": [
        "print(\"Baseline Output\")\n",
        "print(\"=\"*40)\n",
        "print(textwrap.dedent(baseline_outputs[example_idx]).strip())\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQGvwqI68lB4"
      },
      "outputs": [],
      "source": [
        "print(\"RAG Output\")\n",
        "print(\"=\"*40)\n",
        "print(textwrap.dedent(rag_outputs[example_idx]).strip())\n",
        "print(\"=\"*40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkItAQxWQnAG"
      },
      "outputs": [],
      "source": [
        "print(generate_code(build_rag_prompt(\"reverse a string\", \"def foo(): pass\")))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f238561a83fb43309aa41a9f7a342f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea0c3bae1a0d4bab8f0aacae449a8fa7",
              "IPY_MODEL_790a62f5c0214ff391756d1ea48cc5c2",
              "IPY_MODEL_2929f4f12cce439a91a95ce34f291120"
            ],
            "layout": "IPY_MODEL_b48d3d15e4054c328106cb7363c2371b"
          }
        },
        "ea0c3bae1a0d4bab8f0aacae449a8fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93319521892044faade2d6a84e89ccf3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0fa93830eaac46ce9a30a92afd305ea2",
            "value": "README.md:‚Äá100%"
          }
        },
        "790a62f5c0214ff391756d1ea48cc5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7483d4d774a4135b2ac31f9a747a648",
            "max": 5212,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1f3049d932d4619952ecc16240ae2f7",
            "value": 5212
          }
        },
        "2929f4f12cce439a91a95ce34f291120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a07014311d640eaa6d5b75404927057",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_48fe48714a2b4f4fa35b8997e43b1cde",
            "value": "‚Äá5.21k/5.21k‚Äá[00:00&lt;00:00,‚Äá306kB/s]"
          }
        },
        "b48d3d15e4054c328106cb7363c2371b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93319521892044faade2d6a84e89ccf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fa93830eaac46ce9a30a92afd305ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7483d4d774a4135b2ac31f9a747a648": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1f3049d932d4619952ecc16240ae2f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a07014311d640eaa6d5b75404927057": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48fe48714a2b4f4fa35b8997e43b1cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48590beb1910415c8c28762ab3cc4e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68ea6c0d789742ccbe8f805b506ea3b2",
              "IPY_MODEL_676694761ee74fa0af5ca4785e0ac4d9",
              "IPY_MODEL_476ce1405a5b4c1f9c963e3a15b9d727"
            ],
            "layout": "IPY_MODEL_f3c0ee3e7bcb419ab1c0370ef2701810"
          }
        },
        "68ea6c0d789742ccbe8f805b506ea3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79ab87e49c054d42a3473a94e1ad75c5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e19042e1d748427ea260f37950409968",
            "value": "(‚Ä¶)-00000-of-00001-518ed46ecbe35ff9.parquet:‚Äá100%"
          }
        },
        "676694761ee74fa0af5ca4785e0ac4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e0d32673c644d587dbc819174a188b",
            "max": 4577126,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b213024e186412f83f46a2c2e70ae9d",
            "value": 4577126
          }
        },
        "476ce1405a5b4c1f9c963e3a15b9d727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5366ff2cab4547b7a3cda93911ad8d30",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4f50bedbc88445b5bba961093bbf8b7d",
            "value": "‚Äá4.58M/4.58M‚Äá[00:00&lt;00:00,‚Äá17.4MB/s]"
          }
        },
        "f3c0ee3e7bcb419ab1c0370ef2701810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79ab87e49c054d42a3473a94e1ad75c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e19042e1d748427ea260f37950409968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23e0d32673c644d587dbc819174a188b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b213024e186412f83f46a2c2e70ae9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5366ff2cab4547b7a3cda93911ad8d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f50bedbc88445b5bba961093bbf8b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea2e32f360034eedbdff4a57c09fffda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_594ecb2358da46b0a927efe8684378ec",
              "IPY_MODEL_a5f4a49e240a49f0b445bef32c28cec6",
              "IPY_MODEL_f4549bea47e1496bb2245214f3d6d6c6"
            ],
            "layout": "IPY_MODEL_16534dac29f642208aab35f431bb433a"
          }
        },
        "594ecb2358da46b0a927efe8684378ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed124a1ee1814b83aab55341338559c9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6f8a438dc1c94f2795a4ae627971d671",
            "value": "Generating‚Äátest‚Äásplit:‚Äá100%"
          }
        },
        "a5f4a49e240a49f0b445bef32c28cec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fbe6fa881a74fde8c390130277a5608",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5c90620cd1f43299a86eea4d785f345",
            "value": 150
          }
        },
        "f4549bea47e1496bb2245214f3d6d6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcc3614585da425fa7a6041344a58219",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1b89df419a90487b9dfecf7132e55b6f",
            "value": "‚Äá150/150‚Äá[00:00&lt;00:00,‚Äá484.30‚Äáexamples/s]"
          }
        },
        "16534dac29f642208aab35f431bb433a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed124a1ee1814b83aab55341338559c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8a438dc1c94f2795a4ae627971d671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fbe6fa881a74fde8c390130277a5608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c90620cd1f43299a86eea4d785f345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcc3614585da425fa7a6041344a58219": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b89df419a90487b9dfecf7132e55b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9405b8edd4e3406c8ce600f61f0bad4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32ad553f953149ff84b2d87709f4ce6a",
              "IPY_MODEL_5ee8ddd9c3b44cefb5090015c55babe8",
              "IPY_MODEL_865af25810034b9586f2a075e786e0d6"
            ],
            "layout": "IPY_MODEL_edd1e447740043a3bd47bd96f06b9c56"
          }
        },
        "32ad553f953149ff84b2d87709f4ce6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7394dc71f5ce4b678caaa61d2bd62076",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8b2d6f092c334eab92207287cca3b8e8",
            "value": "Filter:‚Äá100%"
          }
        },
        "5ee8ddd9c3b44cefb5090015c55babe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09c91f6ec60f4447b5e57df9f7ac19af",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_463fa7577bb849c88c79209cdcc236fe",
            "value": 150
          }
        },
        "865af25810034b9586f2a075e786e0d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb16151345764a88b3e3779c34aa2690",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bc9929028e614efa80618181064a1c40",
            "value": "‚Äá150/150‚Äá[00:00&lt;00:00,‚Äá182.06‚Äáexamples/s]"
          }
        },
        "edd1e447740043a3bd47bd96f06b9c56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7394dc71f5ce4b678caaa61d2bd62076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b2d6f092c334eab92207287cca3b8e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09c91f6ec60f4447b5e57df9f7ac19af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "463fa7577bb849c88c79209cdcc236fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb16151345764a88b3e3779c34aa2690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc9929028e614efa80618181064a1c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3af433502b3d4364b0e5b282e1c7eb05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab2cc489ee8544db9f3e4c342df9ead9",
              "IPY_MODEL_cf115fd3142642bfaa83d518765fada8",
              "IPY_MODEL_0935704267754e8e9db95b9a35506993"
            ],
            "layout": "IPY_MODEL_c18f0639ab844a9893b11bb44d1716b7"
          }
        },
        "ab2cc489ee8544db9f3e4c342df9ead9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45b4c387a5964df5ac286ea6a6e84a31",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_272b44daf4a2431eadecd5d694598898",
            "value": "‚öôÔ∏è‚ÄáEvaluating‚ÄáSamples:‚Äá100%"
          }
        },
        "cf115fd3142642bfaa83d518765fada8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b7e34009a2d40129af54451bc51d15e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05a964bdd43f48648632add49ba12988",
            "value": 3
          }
        },
        "0935704267754e8e9db95b9a35506993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d68ba320661b4b0495f14b3e9d98317d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a8e04eb2757e4368b02cee8b932704c0",
            "value": "‚Äá3/3‚Äá[02:59&lt;00:00,‚Äá58.10s/it]"
          }
        },
        "c18f0639ab844a9893b11bb44d1716b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45b4c387a5964df5ac286ea6a6e84a31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "272b44daf4a2431eadecd5d694598898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b7e34009a2d40129af54451bc51d15e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05a964bdd43f48648632add49ba12988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d68ba320661b4b0495f14b3e9d98317d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8e04eb2757e4368b02cee8b932704c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}