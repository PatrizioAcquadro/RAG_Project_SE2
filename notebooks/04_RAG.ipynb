{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNpkTgRYJckA"
      },
      "source": [
        "## Section 1: Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6JT4FlEJSQK",
        "outputId": "b87ffee7-4764-478e-b640-f0d1cb1abbbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted successfully.\n"
          ]
        }
      ],
      "source": [
        "# Import the 'drive' module from google.colab library.\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive at the '/content/drive' path in the Colab filesystem.\n",
        "# Requires user authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Confirm successful mounting.\n",
        "print(\"Drive mounted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNe6W1jUJo2g",
        "outputId": "991b88ca-7b0a-42f8-effb-87aaab15791c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing libraries...\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank-bm25) (2.0.2)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [datasets]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Libraries installed (or updated) successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing libraries...\")\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# -q ensures minimal output (quiet installation)\n",
        "!pip install rank-bm25\n",
        "# update pip\n",
        "!pip install --upgrade pip -q\n",
        "\n",
        "# install bm25 (worked already, but no harm)\n",
        "!pip install -q rank-bm25\n",
        "\n",
        "!pip install -q \\\n",
        "  \"transformers\" \\\n",
        "  \"datasets\" \\\n",
        "  \"torch\" \\\n",
        "  \"accelerate\" \\\n",
        "  \"bitsandbytes\" \\\n",
        "  \"huggingface_hub\" \\\n",
        "  \"sacrebleu>=2.0.0\" \\\n",
        "  \"codebleu\" \\\n",
        "  \"tree-sitter-python\"\n",
        "  #\"fsspec==2024.12.0\" \\\n",
        "\n",
        "# üöÄ upgrade to a good combination ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "!pip install -qU \"datasets>=2.19.0\" \"fsspec>=2024.3.0\"  \\\n",
        "                 \"huggingface_hub>=0.22.2\"  \"aiohttp\"\n",
        "\n",
        "\n",
        "import os\n",
        "import time # for the delay before nvidia-smi\n",
        "import warnings # for non-critical warnings\n",
        "import shutil\n",
        "import tarfile # for .tar archives\n",
        "import ast\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import textwrap # for snipper preview\n",
        "import traceback\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# from typing import List, Optional # For type hinting (optional)\n",
        "# from transformers import PreTrainedTokenizerBase # For type hinting (optional)\n",
        "\n",
        "from datasets import load_dataset, DownloadMode\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub.utils import EntryNotFoundError\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm  # for progress bars (optional)\n",
        "    USE_TQDM = True\n",
        "except ImportError:\n",
        "    USE_TQDM = False\n",
        "    print(\"Library 'tqdm' not found, progress bar will not be shown.\")\n",
        "    print(\"You can install it with: !pip install -q tqdm\")\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "print(\"\\nLibraries installed (or updated) successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "45yJQWmheb6N"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Prompt templates - version 1\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def build_baseline_prompt(instruction: str) -> str:\n",
        "    \"\"\"\n",
        "    Baseline: only the task, then a '### Code:' marker to start generation.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "### Task:\n",
        "{instruction.strip()}\n",
        "\n",
        "### Code:\n",
        "\"\"\"\n",
        "\n",
        "def build_rag_prompt(instruction: str, retrieved: str) -> str:\n",
        "    \"\"\"\n",
        "    RAG: first show retrieved examples, then the task, then '### Code:'.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "### Retrieved Examples:\n",
        "{retrieved.strip()}\n",
        "\n",
        "### Task:\n",
        "{instruction.strip()}\n",
        "\n",
        "### Code:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "fCJ9cVHFez3U"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Prompt templates - version 2\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def build_baseline_prompt(instruction: str) -> str:\n",
        "    \"\"\"\n",
        "    Baseline: very explicit, with sections for clarity.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "### Library:\n",
        "You will use the `seedemu` Python library to build a network emulation.\n",
        "\n",
        "### Task Description:\n",
        "{instruction.strip()}\n",
        "\n",
        "### Requirements:\n",
        "1. Import only from `seedemu` (layers, services, core, compiler).\n",
        "2. Create objects in this order: Emulator ‚Üí Layers ‚Üí Services ‚Üí Bindings ‚Üí Dump.\n",
        "3. Use clear variable names (e.g. `base`, `routing`, `ebgp`, `sim`).\n",
        "4. Target Python 3.8+ syntax.\n",
        "\n",
        "### Output Format:\n",
        "- Provide only valid Python code.\n",
        "- No comments, no extra text.\n",
        "- Start at the first line of code (do not repeat the task).\n",
        "\n",
        "### Code:\n",
        "\"\"\"\n",
        "\n",
        "def build_rag_prompt(instruction: str, retrieved: str) -> str:\n",
        "    \"\"\"\n",
        "    RAG: include retrieved examples plus the detailed task template.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "### Retrieved Examples:\n",
        "{retrieved.strip()}\n",
        "\n",
        "### Library:\n",
        "Use the `seedemu` Python library.\n",
        "\n",
        "### Task Description:\n",
        "{instruction.strip()}\n",
        "\n",
        "### Requirements:\n",
        "1. Imports: `seedemu.layers`, `seedemu.services`, `seedemu.core`, `seedemu.compiler`.\n",
        "2. Instantiate Emulator, then Base, Routing, eBGP layers in order.\n",
        "3. Install the domain name caching service on specified hosts.\n",
        "4. Add private eBGP peerings between ASes.\n",
        "5. Finally, dump the emulator state to `base-component.bin`.\n",
        "\n",
        "### Output Format:\n",
        "- Return **only** runnable Python code.\n",
        "- No comments or markdown.\n",
        "- Do not echo the instructions.\n",
        "\n",
        "### Code:\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Prompt templates - version 3\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def build_baseline_prompt(instruction: str) -> str:\n",
        "    \"\"\"\n",
        "    Baseline: given only the instruction, ask the model to produce:\n",
        "      1. A clear function signature with type hints\n",
        "      2. A concise docstring explaining behavior, inputs, and outputs\n",
        "      3. The implementation, following PEP8\n",
        "      4. At least one simple unit test demonstrating correct usage\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "You are a senior Python engineer.  Fulfill the following task by writing production-ready code.\n",
        "\n",
        "**Task**:\n",
        "{instruction.strip()}\n",
        "\n",
        "**Requirements**:\n",
        "- Python 3, include type hints\n",
        "- One well-formed function or class with a descriptive name\n",
        "- A docstring (inputs, outputs, edge cases)\n",
        "- PEP8 style (4-space indent, snake_case)\n",
        "- At least one unit test using `assert` or `unittest`\n",
        "\n",
        "**Implementation**:\n",
        "```python\n",
        "\"\"\"\n",
        "\n",
        "def build_rag_prompt(instruction: str, retrieved: str) -> str:\n",
        "    \"\"\"\n",
        "    RAG: first show retrieved examples for inspiration, then the same structured prompt:\n",
        "      instruction, requirements, and a code block marker.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\\\n",
        "You are a senior Python engineer.  Use the retrieved examples to guide your implementation.\n",
        "\n",
        "**Retrieved Examples**:\n",
        "{retrieved.strip()}\n",
        "\n",
        "**Task**:\n",
        "{instruction.strip()}\n",
        "\n",
        "**Requirements**:\n",
        "- Python 3 with type hints\n",
        "- Clean function or class design with a docstring\n",
        "- Adhere to PEP8 conventions\n",
        "- Include at least one unit test\n",
        "\n",
        "**Implementation**:\n",
        "```python\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "kQm1GC4LPRg9"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqbp1sqaJzkH"
      },
      "source": [
        "## Section 2: LLM and Tokenizer Loading with 4-bit Quantization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEZfMT3QJxJU",
        "outputId": "0f53f222-5869-40f2-d899-cf940724b3eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting trust_remote_code=False for deepseek-ai/deepseek-coder-1.3b-base\n"
          ]
        }
      ],
      "source": [
        "# check that there is only one selected model\n",
        "\n",
        "# --- Gemma Series (Google) ---\n",
        "# model_name = \"google/codegemma-7b\"\n",
        "# model_name = \"google/codegemma-7b-it\"\n",
        "\n",
        "# --- Qwen Series (Alibaba) ---\n",
        "# model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "# model_name = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
        "# model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
        "\n",
        "# --- Deepseek Coder Series (Deepseek AI) ---\n",
        "# model_name = \"deepseek-ai/deepseek-coder-7b-instruct-v1.5\"\n",
        "#model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "\n",
        "# --- Code Llama Series (Meta) ---\n",
        "# model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "\n",
        "# --- Phi Series (Microsoft) ---\n",
        "# model_name = \"microsoft/Phi-4-mini-instruct\"\n",
        "# model_name = \"microsoft/Phi-4-multimodal-instruct\"\n",
        "\n",
        "TRUST_REMOTE_CODE_MODELS = [\n",
        "    \"microsoft/Phi-\",\n",
        "    \"Qwen/\",\n",
        "]\n",
        "trust_code = any(model_name.startswith(prefix) for prefix in TRUST_REMOTE_CODE_MODELS)\n",
        "\n",
        "print(f\"Setting trust_remote_code={trust_code} for {model_name}\")\n",
        "if trust_code:\n",
        "    print(\"WARNING: trust_remote_code=True will execute Python code from the model's Hugging Face repo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "MV8HRJKiL4pl"
      },
      "outputs": [],
      "source": [
        "# 4-bit NF4 quantisation\n",
        "QUANT_CFG = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        if (torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n",
        "        else torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "6j-ATB8rL6pE"
      },
      "outputs": [],
      "source": [
        "# == 5.  Where the cached copy will live on your Drive ===========\n",
        "CACHE_ROOT = \"/content/drive/MyDrive/llm_cache\"\n",
        "CACHE_DIR  = os.path.join(\n",
        "    CACHE_ROOT,\n",
        "    model_name.replace(\"/\", \"_\") + \"_4bit_nf4\",\n",
        ")\n",
        "\n",
        "META_FILE  = os.path.join(CACHE_DIR, \"metadata.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "sXahHcIpMAHf"
      },
      "outputs": [],
      "source": [
        "# == 5.  Build the 4‚Äêbit config (for GPU only) ===================\n",
        "# (use bfloat16 on bf16‚Äêcapable GPUs, else float16)\n",
        "compute_dtype = (\n",
        "    torch.bfloat16\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else torch.float16\n",
        ")\n",
        "\n",
        "QUANT_CFG = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "def _qcfg_to_dict(cfg):\n",
        "    return {\n",
        "        \"load_in_4bit\": cfg.load_in_4bit,\n",
        "        \"bnb_4bit_quant_type\": cfg.bnb_4bit_quant_type,\n",
        "        \"bnb_4bit_compute_dtype\": str(cfg.bnb_4bit_compute_dtype),\n",
        "        \"bnb_4bit_use_double_quant\": cfg.bnb_4bit_use_double_quant,\n",
        "    }\n",
        "\n",
        "REQ_META = {\n",
        "    \"model_name\": model_name,\n",
        "    \"quant_cfg\":  _qcfg_to_dict(QUANT_CFG),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-seMg9q8MKYR",
        "outputId": "6b4f7f2f-353b-4f1c-c6f8-39a576e3e5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° Cache metadata match: True\n",
            "‚ö° Loading from Drive cache‚Ä¶\n",
            "üéâ Model & tokenizer ready!\n"
          ]
        }
      ],
      "source": [
        "# == 6.  Check for existing cache & metadata ====================\n",
        "use_cache = False\n",
        "if os.path.isfile(META_FILE):\n",
        "    try:\n",
        "        saved = json.load(open(META_FILE))\n",
        "        use_cache = saved == REQ_META\n",
        "        print(\"‚ö° Cache metadata match:\", use_cache)\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è  Could not parse metadata.json; ignoring cache.\")\n",
        "\n",
        "# == 7.  Load tokenizer & model (fast or slow path) ==============\n",
        "trust_code = model_name.startswith((\"microsoft/Phi-\", \"Qwen/\"))\n",
        "\n",
        "try:\n",
        "    if use_cache:\n",
        "        print(\"‚ö° Loading from Drive cache‚Ä¶\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(CACHE_DIR, local_files_only=True, trust_remote_code=trust_code)\n",
        "        model     = AutoModelForCausalLM.from_pretrained(\n",
        "            CACHE_DIR,\n",
        "            device_map=\"auto\",\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=trust_code,\n",
        "        )\n",
        "    else:\n",
        "        # decide whether we *can* do 4-bit quant:\n",
        "        do_4bit = torch.cuda.is_available()\n",
        "        print(f\"‚è≥ No valid cache. CUDA available? {do_4bit}\")\n",
        "        print(f\"‚è≥ {'Quantising 4-bit‚Ä¶' if do_4bit else 'Loading fp16‚Ä¶'} this will happen once\")\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ tokenizer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_code)\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        if do_4bit:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                quantization_config=QUANT_CFG,\n",
        "                device_map=\"auto\",\n",
        "                trust_remote_code=trust_code,\n",
        "                low_cpu_mem_usage=True,\n",
        "            )\n",
        "        else:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=compute_dtype,\n",
        "                device_map=\"auto\",\n",
        "                trust_remote_code=trust_code,\n",
        "            )\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ Save cache for next time ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        print(\"üíæ Saving to Drive cache‚Ä¶\")\n",
        "        os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "        tokenizer.save_pretrained(CACHE_DIR)\n",
        "        model.save_pretrained(CACHE_DIR)\n",
        "        with open(META_FILE, \"w\") as f:\n",
        "            json.dump(REQ_META, f)\n",
        "        print(\"‚úÖ Cache written at\", CACHE_DIR)\n",
        "\n",
        "    # ensure model.pad_token_id\n",
        "    if getattr(model, \"config\", None) and model.config.pad_token_id is None:\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    print(\"üéâ Model & tokenizer ready!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error loading model/tokenizer:\")\n",
        "    traceback.print_exc()\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0G8BVCNQFTg"
      },
      "source": [
        "## Section 3: Dataset Preparation and Validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW9aGepXMMHf",
        "outputId": "012a83f3-51b8-44e9-e491-65ba1b832647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive Directory available: /content/drive/MyDrive/RAG_Project/\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Configuration of Google Drive directory ---\n",
        "\n",
        "drive_save_path = '/content/drive/MyDrive/RAG_Project/' # to store results and outputs\n",
        "# check that the directory exists!\n",
        "\n",
        "try:\n",
        "    os.makedirs(drive_save_path, exist_ok=True)\n",
        "    print(f\"Google Drive Directory available: {drive_save_path}\")\n",
        "except OSError as e:\n",
        "    print(f\"Warning: can not create or verify the existence of the directory: {drive_save_path}. Details: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "yfD2YFUtQRdQ"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade --quiet datasets==2.16.0 fsspec==2023.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FphZD_qGQTVT",
        "outputId": "c9fd3a89-0eff-4740-b75f-a4258e16041a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ñ∂Ô∏è  Loading dataset 'JetBrains-Research/lca-library-based-code-generation' (split='test')‚Ä¶\n",
            "   (uses HF cache on Colab VM ‚Äî do NOT point cache_dir at Drive)\n",
            "‚úÖ Dataset loaded ‚Äî now filtering to repo_name == 'seed-emulator'\n",
            "‚úÖ Filtered: 13 examples in 'seed-emulator'\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, DownloadMode\n",
        "\n",
        "dataset_name = \"JetBrains-Research/lca-library-based-code-generation\"\n",
        "data_split   = \"test\"\n",
        "\n",
        "print(f\"\\n‚ñ∂Ô∏è  Loading dataset '{dataset_name}' (split='{data_split}')‚Ä¶\")\n",
        "print(\"   (uses HF cache on Colab VM ‚Äî do NOT point cache_dir at Drive)\")\n",
        "\n",
        "try:\n",
        "    lca_dataset_split = load_dataset(\n",
        "        dataset_name,\n",
        "        split=data_split,\n",
        "        # download_mode=DownloadMode.FORCE_REDOWNLOAD,  # uncomment to force fresh pull\n",
        "    )\n",
        "    print(\"‚úÖ Dataset loaded ‚Äî now filtering to repo_name == 'seed-emulator'\")\n",
        "    lca_dataset_split = lca_dataset_split.filter(\n",
        "        lambda ex: ex[\"repo_name\"] == \"seed-emulator\"\n",
        "    )\n",
        "    print(f\"‚úÖ Filtered: {len(lca_dataset_split)} examples in 'seed-emulator'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR loading or filtering dataset: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4KHU4SrReZ1"
      },
      "source": [
        "## Section 4: Repository Archive Download & Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmMVWLTnRhMU",
        "outputId": "1d87890c-add6-4666-e3b4-5a53376c77fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Succesfully import exceptions from huggingface_hub.utils.\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Exception Imports ---\n",
        "try:\n",
        "    from huggingface_hub.utils import HfHubHTTPError, RepositoryNotFoundError, EntryNotFoundError\n",
        "    print(\"Succesfully import exceptions from huggingface_hub.utils.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING: can not import exceptions from huggingface_hub.utils, try from .errors\")\n",
        "    try:\n",
        "        from huggingface_hub.errors import HfHubHTTPError, RepositoryNotFoundError, EntryNotFoundError\n",
        "        print(\"Importing exceptions from huggingface_hub.errors completed.\")\n",
        "    except ImportError:\n",
        "        print(\"ERROR: can not import exceptions from huggingface_hub.\")\n",
        "        class HfHubHTTPError(Exception): pass\n",
        "        class RepositoryNotFoundError(Exception): pass\n",
        "        class EntryNotFoundError(Exception): pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387,
          "referenced_widgets": [
            "4824b141d69d438dbf5dfdeba349d747",
            "822f4390832342a1a77c78d3db73fa21",
            "6dbd49315715471e8b6f41b0a33d7d4e",
            "e88fcc96fe6741d7ad9891b446dea18e",
            "9ae23b3cd4cc4951b60eec28a69208fa",
            "f61662cd58704d11a07db37e83689fe2",
            "567a978197874f11a7c3ab7277acfd4d",
            "8e022ab0e2c64bc1bc4a76999718de3f",
            "2dd44bd42e3f44e396eec80ad9374a48",
            "feac936855304fafb392dfc49f595033",
            "510720d518cf40f88c80f5308a810f75"
          ]
        },
        "id": "9ZiuRBeWRi5_",
        "outputId": "302c9d83-1c81-487b-84b9-888f48241944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Download and configuration ---\n",
            "Repo: JetBrains-Research/lca-library-based-code-generation\n",
            "File in repo: repos/seed-labs__seed-emulator.tar.gz\n",
            "Desired destination: /content/seed-labs__seed-emulator.tar.gz\n",
            "\n",
            "Starting download from Hugging Face Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "seed-labs__seed-emulator.tar.gz:   0%|          | 0.00/24.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4824b141d69d438dbf5dfdeba349d747"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed. File saved at: /content/repos/seed-labs__seed-emulator.tar.gz\n",
            "\\Moving 'seed-labs__seed-emulator.tar.gz' to '/content/seed-labs__seed-emulator.tar.gz'...\n",
            "Move completed successfully.\n",
            "Removing empty intermediate directory: /content/repos\n",
            "\n",
            "Final check:\n",
            "[OK] The final archive is ready at: /content/seed-labs__seed-emulator.tar.gz\n",
            "\n",
            "--- End of Download and Preparation ---\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Archive Download ---\n",
        "\n",
        "repo_id = \"JetBrains-Research/lca-library-based-code-generation\"\n",
        "filename_in_repo = \"repos/seed-labs__seed-emulator.tar.gz\"\n",
        "desired_local_archive_path = \"/content/seed-labs__seed-emulator.tar.gz\"\n",
        "download_base_dir = \"/content/\"\n",
        "\n",
        "print(f\"\\n--- Download and configuration ---\")\n",
        "print(f\"Repo: {repo_id}\")\n",
        "print(f\"File in repo: {filename_in_repo}\")\n",
        "print(f\"Desired destination: {desired_local_archive_path}\")\n",
        "\n",
        "actual_downloaded_path = None\n",
        "\n",
        "# actual donwload from Hugging Face\n",
        "try:\n",
        "    print(f\"\\nStarting download from Hugging Face Hub...\")\n",
        "    actual_downloaded_path = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=filename_in_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        local_dir=download_base_dir,\n",
        "        local_dir_use_symlinks=False,\n",
        "    )\n",
        "    print(f\"Download completed. File saved at: {actual_downloaded_path}\")\n",
        "\n",
        "# --- 3. Error Handling ---\n",
        "except RepositoryNotFoundError:\n",
        "    print(f\"\\nERROR: Repository '{repo_id}' not found on Hugging Face Hub.\")\n",
        "    print(\"  Make sure the repository name is correct.\")\n",
        "except EntryNotFoundError:  # Specific file not found in the repo\n",
        "    print(f\"\\nERROR: File/Entry '{filename_in_repo}' not found in the repository '{repo_id}'.\")\n",
        "except HfHubHTTPError as e:  # HTTP errors (including 401, 403, 404 not already caught above)\n",
        "    print(f\"\\nHTTP ERROR during download from Hugging Face Hub: {e}\")\n",
        "    if hasattr(e, 'response') and e.response is not None:\n",
        "        print(f\"  Status Code: {e.response.status_code}\")\n",
        "        if e.response.status_code == 404:\n",
        "            print(f\"  -> The file '{filename_in_repo}' or the repo '{repo_id}' may not exist (Error 404).\")\n",
        "    print(f\"  Please check the repo_id, filename_in_repo, and your internet connection or HF token if necessary.\")\n",
        "except Exception as e:\n",
        "    # Catch other unexpected errors\n",
        "    import traceback\n",
        "    print(f\"\\nUNEXPECTED ERROR during the download:\")\n",
        "    # print(traceback.format_exc())  # Uncomment for full traceback during debugging\n",
        "    print(f\"  Error Type: {type(e).__name__}, Message: {e}\")\n",
        "\n",
        "# --- 4. File Relocation & Cleanup ---\n",
        "archive_ready = False\n",
        "if actual_downloaded_path and os.path.exists(actual_downloaded_path):\n",
        "    if os.path.abspath(actual_downloaded_path) == os.path.abspath(desired_local_archive_path):\n",
        "        print(f\"\\nThe archive is already at the desired final location: {desired_local_archive_path}\")\n",
        "        archive_ready = True\n",
        "    else:\n",
        "        try:\n",
        "            print(f\"\\Moving '{os.path.basename(actual_downloaded_path)}' to '{desired_local_archive_path}'...\")\n",
        "            os.makedirs(os.path.dirname(desired_local_archive_path), exist_ok=True)\n",
        "            shutil.move(actual_downloaded_path, desired_local_archive_path)\n",
        "            print(f\"Move completed successfully.\")\n",
        "            archive_ready = True\n",
        "\n",
        "            # Clean up intermediate directory if empty\n",
        "            download_parent_dir = os.path.dirname(actual_downloaded_path)\n",
        "            if (os.path.exists(download_parent_dir) and\n",
        "                os.path.abspath(download_parent_dir) != os.path.abspath(download_base_dir) and\n",
        "                os.path.abspath(download_parent_dir).startswith(os.path.abspath(download_base_dir)) and\n",
        "                not os.listdir(download_parent_dir)):\n",
        "                try:\n",
        "                    print(f\"Removing empty intermediate directory: {download_parent_dir}\")\n",
        "                    os.rmdir(download_parent_dir)\n",
        "                except OSError as rmdir_e:\n",
        "                    print(f\"  Warining: can not remove {download_parent_dir}. Issue: {rmdir_e}\")\n",
        "\n",
        "        except Exception as move_e:\n",
        "            print(\"\\nERROR during move or cleanup of downloaded file:\")\n",
        "            print(f\"  Error: {move_e}\")\n",
        "            print(f\"  The downloaded file may still be located at: {actual_downloaded_path}\")\n",
        "            archive_ready = False\n",
        "\n",
        "elif not actual_downloaded_path:\n",
        "     print(\"\\nDownload failed. Cannot proceed.\")\n",
        "else:\n",
        "     print(f\"\\nINTERNAL ERROR: Download path ({actual_downloaded_path}) does not exist after the attempt.\")\n",
        "\n",
        "# --- 5. Final Verification ---\n",
        "print(\"\\nFinal check:\")\n",
        "if archive_ready and os.path.exists(desired_local_archive_path):\n",
        "    print(f\"[OK] The final archive is ready at: {desired_local_archive_path}\")\n",
        "else:\n",
        "    print(f\"[ERROR] The final archive was NOT found or prepared correctly at: {desired_local_archive_path}\")\n",
        "\n",
        "print(\"\\n--- End of Download and Preparation ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f83x19j-RlOt"
      },
      "source": [
        "## Section 5: Source Extraction & Knowledge Base Construction\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UexBY3uPRnk-",
        "outputId": "22976b50-dcfb-4aa2-e5ad-e934e6e999ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Extraction archive ---\n",
            "Archive: /content/seed-labs__seed-emulator.tar.gz\n",
            "Destination directory: /content/library_sources/\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Archive Extraction ---\n",
        "# --- 1.1. Configuration ---\n",
        "# Path to the downloaded archive (should already exist from the previous cell)\n",
        "local_archive_path = '/content/seed-labs__seed-emulator.tar.gz'\n",
        "# Base directory where we want to extract the archive contents\n",
        "extract_dir_parent = \"/content/library_sources/\"\n",
        "\n",
        "# This variable will hold the actual path to the main extracted folder.\n",
        "# It will be determined after extraction is complete.\n",
        "final_extracted_code_path = None\n",
        "\n",
        "print(\"--- Extraction archive ---\")\n",
        "print(f\"Archive: {local_archive_path}\")\n",
        "print(f\"Destination directory: {extract_dir_parent}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms7xt_8PRpKi",
        "outputId": "dac3e9c4-6ade-4ab2-b4bf-0874b28e44ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Target extraction directory '/content/library_sources/' is ready.\n",
            "Starting extraction of 'seed-labs__seed-emulator.tar.gz'...\n",
            "Extraction completed successfully.\n",
            "Identified main extracted directory: /content/library_sources/mnt\n"
          ]
        }
      ],
      "source": [
        "# --- 1.2. Safety Check ---\n",
        "if not os.path.exists(local_archive_path):\n",
        "    print(f\"\\n[ERROR] Source archive not found: {local_archive_path}\")\n",
        "    print(\"  Make sure the download cell was run correctly.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    try:\n",
        "        # Optional: clean destionation directory before the execution (if next instruction is not commented)\n",
        "        if os.path.exists(extract_dir_parent): shutil.rmtree(extract_dir_parent)\n",
        "\n",
        "        # Create destination directory\n",
        "        # exist_ok=True avoids errors if already exists\n",
        "        os.makedirs(extract_dir_parent, exist_ok=True)\n",
        "        print(f\"\\nTarget extraction directory '{extract_dir_parent}' is ready.\")\n",
        "\n",
        "        # --- 1.3. Unpacking ---\n",
        "        # extract the archive\n",
        "        print(f\"Starting extraction of '{os.path.basename(local_archive_path)}'...\")\n",
        "        with tarfile.open(local_archive_path, \"r:gz\") as tar:\n",
        "            tar.extractall(path=extract_dir_parent)\n",
        "        print(\"Extraction completed successfully.\")\n",
        "\n",
        "        # --- 1.4. Dynamic Path Resolution ---\n",
        "        # dynamically determine the extracted path\n",
        "        try:\n",
        "            extracted_items = os.listdir(extract_dir_parent)\n",
        "            if len(extracted_items) == 1 and os.path.isdir(os.path.join(extract_dir_parent, extracted_items[0])):\n",
        "                final_extracted_code_path = os.path.join(extract_dir_parent, extracted_items[0])\n",
        "                print(f\"Identified main extracted directory: {final_extracted_code_path}\")\n",
        "            elif len(extracted_items) > 0:\n",
        "                 # look for a folder matching the archive's base name\n",
        "                 archive_basename = os.path.basename(local_archive_path).replace('.tar.gz', '').replace('.tgz', '')\n",
        "                 potential_match = os.path.join(extract_dir_parent, archive_basename)\n",
        "                 if os.path.isdir(potential_match):\n",
        "                     final_extracted_code_path = potential_match\n",
        "                     print(f\"Found potential matching directory: {final_extracted_code_path}\")\n",
        "                 else:\n",
        "                     first_item_path = os.path.join(extract_dir_parent, extracted_items[0])\n",
        "                     if os.path.isdir(first_item_path):\n",
        "                          final_extracted_code_path = first_item_path\n",
        "                          print(f\"WARNING: Multiple items found. Assuming first directory: {final_extracted_code_path}\")\n",
        "                     else:\n",
        "                          print(f\"WARNING: No main directory found in the extraction folder {extract_dir_parent}.\")\n",
        "                          print(f\"  Contents: {extracted_items}\")\n",
        "                          print(f\"  'final_extracted_code_path' might be set manually.\")\n",
        "                          final_extracted_code_path = extract_dir_parent # Fallback: use the parent dir\n",
        "                          print(f\"  Impostato fallback a: {final_extracted_code_path}\")\n",
        "\n",
        "            else:\n",
        "                 print(f\"WARNING: Extraction folder '{extract_dir_parent}' is empty after extraction.\")\n",
        "\n",
        "        except Exception as list_e:\n",
        "             print(f\"Issue while analyzing the extracted data: {list_e}\")\n",
        "\n",
        "    except tarfile.ReadError:\n",
        "        print(f\"\\n[ERROR] Cannot read archive: {local_archive_path}. It may be corrupted.\")\n",
        "    except FileNotFoundError:\n",
        "        # can happen only if local_archive_path is removed\n",
        "        print(f\"\\n[ERROR] Archive file not found during open attempt: {local_archive_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] Unexpected error during preparation or extraction:\")\n",
        "        # print(traceback.format_exc()) # uncomment for debug\n",
        "        print(f\"  Error Type: {type(e).__name__}, Message: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8oSLADbRrRg",
        "outputId": "76329945-f91a-4e51-f93d-12e99e21467f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final check:\n",
            "[OK] The extracted source code path is: /content/library_sources/mnt\n",
            "\n",
            "Partial content of the extracted directory (first 10 entries):\n",
            "  - data\n",
            "\n",
            "--- End of Archive Extraction ---\n"
          ]
        }
      ],
      "source": [
        "# --- 1.5. Final check ---\n",
        "\n",
        "print(\"\\nFinal check:\")\n",
        "if final_extracted_code_path and os.path.isdir(final_extracted_code_path):\n",
        "    print(f\"[OK] The extracted source code path is: {final_extracted_code_path}\")\n",
        "    print(\"\\nPartial content of the extracted directory (first 10 entries):\")\n",
        "    try:\n",
        "        content_list = os.listdir(final_extracted_code_path)\n",
        "        for item in content_list[:10]:\n",
        "            print(f\"  - {item}\")\n",
        "        if len(content_list) > 10:\n",
        "            print(\"  ...\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Errore while listing the content of {final_extracted_code_path}: {e}\")\n",
        "else:\n",
        "    print(f\"[ERROR] Unable to determine or locate the extracted code directory.\")\n",
        "    print(f\"         'final_extracted_code_path' is: {final_extracted_code_path}\")\n",
        "    print(f\"         Make sure the extraction completed successfully.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- End of Archive Extraction ---\")\n",
        "\n",
        "# Make the variable available for subsequent cells (optional but useful)\n",
        "# You may want to rename it to `extracted_code_path` if subsequent cells\n",
        "# use that specific name.\n",
        "# extracted_code_path = final_extracted_code_path\n",
        "# print(f‚Äò\\nVariable ‚Äúextracted_code_path‚Äù set to: {extracted_code_path}‚Äô)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eCYcw-yhB-0",
        "outputId": "068e30e3-b9a1-4160-f3c7-cbacc791898a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [datasets]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2023.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade \\\n",
        "    datasets==2.16.0 \\\n",
        "    fsspec==2023.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Uxg5O72DRtq4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "if 'lca_dataset_split' not in globals() or lca_dataset_split is None:\n",
        "    print(\"‚ÑπÔ∏è  (Re)loading the `test` split of the LCA dataset‚Ä¶\")\n",
        "    lca_dataset_split = load_dataset(\n",
        "        \"JetBrains-Research/lca-library-based-code-generation\",\n",
        "        split=\"test\"\n",
        "    ).filter(lambda ex: ex[\"repo_name\"] == \"seed-emulator\")\n",
        "    print(f\"‚úÖ Loaded {len(lca_dataset_split)} examples from 'seed-emulator'\")\n",
        "\n",
        "\n",
        "# --- 1. Configuration (pre Snippet Extraction Helpers) ---\n",
        "SAMPLE_INDEX = 0       # Index of the dataset sample to process\n",
        "MAX_KB_SIZE = 15000    # Max number of code snippets to include in the KB (to limit RAM)\n",
        "FALLBACK_ENCODING = 'iso-8859-1'  # Encoding to use if UTF-8 fails\n",
        "DRIVE_KB_SAVE_DIR = '/content/drive/MyDrive/RAG_Project/library_kbs'  # Directory to save KBs on Google Drive\n",
        "\n",
        "\n",
        "# Check the existance of the needed variables\n",
        "if 'lca_dataset_split' not in locals() or not lca_dataset_split:\n",
        "    raise NameError(\"CRITICAL ERROR: Variable 'lca_dataset_split' is not defined or is empty. Rerun the dataset loading cell.\")\n",
        "if 'final_extracted_code_path' not in locals() or not final_extracted_code_path:\n",
        "     # Fallback: try to use old name\n",
        "     if 'extracted_code_path' in locals() and extracted_code_path:\n",
        "          warnings.warn(\"Variable 'final_extracted_code_path' not found, using 'extracted_code_path' as fallback.\")\n",
        "          final_extracted_code_path = extracted_code_path\n",
        "     else:\n",
        "          raise NameError(\"CRITICAL ERROR: Variable 'final_extracted_code_path' (or 'extracted_code_path') is not defined. Rerun the archive extraction cell.\")\n",
        "if not os.path.isdir(final_extracted_code_path):\n",
        "     raise FileNotFoundError(f\"CRITICAL ERROR: The extracted code path '{final_extracted_code_path}' does not exist or is not a directory. Check the archive extraction step.\")\n",
        "\n",
        "# Actual source code path from previous cell\n",
        "library_source_dir = final_extracted_code_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "TvAqGmu0RwTc"
      },
      "outputs": [],
      "source": [
        "# --- 2. Snippet Extraction Helpers ---\n",
        "\n",
        "def extract_code_units(py_file_path, fallback_encoding=FALLBACK_ENCODING):\n",
        "    \"\"\"Extracts functions and classes from a Python file as strings, with improved error handling.\"\"\"\n",
        "    units = []\n",
        "    source = None\n",
        "    encoding_used = 'utf-8'\n",
        "    try:\n",
        "        # Attempt to read with UTF-8\n",
        "        with open(py_file_path, 'r', encoding='utf-8') as file:\n",
        "            source = file.read()\n",
        "    except UnicodeDecodeError:\n",
        "        # Fallback to the specified encoding\n",
        "        encoding_used = fallback_encoding\n",
        "        try:\n",
        "            with open(py_file_path, 'r', encoding=fallback_encoding) as file:\n",
        "                source = file.read()\n",
        "            # warnings.warn(f\"Used encoding '{encoding_used}' for {py_file_path}\") # Optional: Log used encoding\n",
        "        except Exception as read_e:\n",
        "            # print(f\"  Error reading file {py_file_path} (even with {encoding_used}): {read_e}\")\n",
        "            return units # Nothing we can do if reading fails\n",
        "    except PermissionError:\n",
        "        # print(f\"  Permission error reading {py_file_path}\")\n",
        "        return units\n",
        "    except Exception as read_e:\n",
        "        # print(f\"  Unexpected error reading {py_file_path}: {read_e}\")\n",
        "        return units\n",
        "\n",
        "    # If reading succeeds, try to parse\n",
        "    if source is not None:\n",
        "        try:\n",
        "            tree = ast.parse(source, filename=py_file_path)\n",
        "            # Check availability of get_source_segment (should be available in Python 3.8+)\n",
        "            can_get_segment = hasattr(ast, 'get_source_segment')\n",
        "\n",
        "            for node in ast.walk(tree):\n",
        "                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
        "                    code_segment = None\n",
        "                    if can_get_segment:\n",
        "                        try:\n",
        "                            code_segment = ast.get_source_segment(source, node)\n",
        "                        except Exception as segment_e:\n",
        "                            # Sometimes the segment can't be extracted for complex nodes or decorators\n",
        "                            # print(f\"  Warning: Error extracting segment ({type(node).__name__}) in {py_file_path}: {segment_e}\")\n",
        "                            pass\n",
        "                    else: # Very simple fallback if get_source_segment is not available\n",
        "                        code_segment = ast.dump(node) # Not ideal, but better than nothing\n",
        "\n",
        "                    if code_segment:\n",
        "                        units.append(code_segment)\n",
        "\n",
        "        except SyntaxError as syn_e:\n",
        "            # Ignore files with Python syntax errors\n",
        "            # print(f\"  Ignored: Syntax error in {py_file_path}: {syn_e}\")\n",
        "            pass\n",
        "        except Exception as parse_e:\n",
        "            # Ignore other parsing errors\n",
        "            # print(f\"  Ignored: AST parsing error in {py_file_path}: {parse_e}\")\n",
        "            pass\n",
        "    return units\n",
        "\n",
        "def build_kb_for_library(source_path, max_kb_size=MAX_KB_SIZE, use_tqdm=USE_TQDM):\n",
        "    \"\"\"Builds the KB (list of snippets) by scanning .py files, with progress and error handling.\"\"\"\n",
        "    if not os.path.isdir(source_path):\n",
        "        print(f\"[ERROR] The provided source path is not a valid directory: {source_path}\")\n",
        "        return []\n",
        "\n",
        "    print(f\"\\nStarting library scan in: {source_path}\")\n",
        "    knowledge_base = []\n",
        "    file_count = 0\n",
        "    processed_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    # Count total .py files for tqdm (if used)\n",
        "    total_py_files = 0\n",
        "    if use_tqdm:\n",
        "        print(\"Counting .py files for progress bar...\")\n",
        "        for _, _, files in os.walk(source_path):\n",
        "            total_py_files += sum(1 for file in files if file.endswith(\".py\"))\n",
        "        print(f\"Found {total_py_files} .py files.\")\n",
        "\n",
        "    # Set up the iterator (with or without tqdm)\n",
        "    walker = os.walk(source_path, topdown=True) # topdown=True for potential dir exclusion\n",
        "    if use_tqdm:\n",
        "        pbar = tqdm(total=total_py_files, desc=\"Extracting Snippets\", unit=\"file\")\n",
        "\n",
        "    try:\n",
        "        for root, dirs, files in walker:\n",
        "            # Optional: Exclude specific directories (e.g., test, docs, build)\n",
        "            # dirs[:] = [d for d in dirs if d not in ['tests', 'test', 'docs', '__pycache__', 'build']]\n",
        "\n",
        "            for file in files:\n",
        "                if file.endswith(\".py\"):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    file_count += 1\n",
        "                    snippets = extract_code_units(file_path)\n",
        "                    if snippets:\n",
        "                        knowledge_base.extend(snippets)\n",
        "                        processed_count += 1\n",
        "                    else:\n",
        "                        skipped_count += 1 # .py file read but no snippet extracted (error or empty)\n",
        "\n",
        "                    if use_tqdm:\n",
        "                        pbar.update(1)\n",
        "                    elif file_count % 200 == 0: # Print progress less frequently without tqdm\n",
        "                        print(f\"  Processed {file_count} files...\")\n",
        "\n",
        "    except PermissionError as perm_e:\n",
        "        print(f\"\\n[ERROR] Permission error during scan of {source_path}: {perm_e}\")\n",
        "        print(\"  You may need to adjust permissions or run as a different user.\")\n",
        "    except Exception as walk_e:\n",
        "        print(f\"\\n[ERROR] Unexpected error during scan: {walk_e}\")\n",
        "    finally:\n",
        "        if use_tqdm:\n",
        "            pbar.close()\n",
        "\n",
        "    print(f\"\\nScan completed.\")\n",
        "    print(f\"  Total .py files encountered: {file_count}\")\n",
        "    print(f\"  .py files processed with snippets: {processed_count}\")\n",
        "    print(f\"  .py files skipped/with errors: {skipped_count}\")\n",
        "    print(f\"  Total snippets extracted (before sampling): {len(knowledge_base)}\")\n",
        "\n",
        "    # Sampling if the KB is too large\n",
        "    if len(knowledge_base) > max_kb_size:\n",
        "        print(f\"\\nWARNING: KB too large ({len(knowledge_base)} snippets).\")\n",
        "        print(f\"  Random sampling to keep a maximum of {max_kb_size} snippets.\")\n",
        "        knowledge_base = random.sample(knowledge_base, max_kb_size)\n",
        "        print(f\"  KB size after sampling: {len(knowledge_base)}\")\n",
        "    elif len(knowledge_base) == 0:\n",
        "        print(\"\\nWARNING: No snippet extracted from the library.\")\n",
        "        print(f\"  Check that '{source_path}' contains valid and readable .py files.\")\n",
        "\n",
        "    return knowledge_base\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486,
          "referenced_widgets": [
            "96c0ebbad6154d0698fff23626d10c42",
            "c9de4056d801412e849d4e3b8010ad14",
            "37fe771ad17746afb0fa82639bd52a3f",
            "7e18215361d74b288b625bc0edb7b102",
            "6ed7e78b71184b5ea01839b9a8536347",
            "57575822faf34ab2a14cb1ba571eee3e",
            "dc339416ef9d46cf91517094bac19a19",
            "fb8a78c6ac4f4cd9b4fd426bff7c72af",
            "dd08f9d98d3442f6a2e0ece9723f1d65",
            "a584aa2b9791470391632a3cd1f83d58",
            "6a073cf121d843bb8701f59e9c39b752"
          ]
        },
        "id": "A4tnkl6BRx6L",
        "outputId": "a020dfaa-89a7-4ca1-fae3-957b3ec91fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "--- Knowledge Base (KB) Creation ---\n",
            "========================================\n",
            "Processing Sample 0: Library 'seed-labs__seed-emulator'\n",
            "Source code path: /content/library_sources/mnt\n",
            "\n",
            "Starting library scan in: /content/library_sources/mnt\n",
            "Counting .py files for progress bar...\n",
            "Found 136 .py files.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting Snippets:   0%|          | 0/136 [00:00<?, ?file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96c0ebbad6154d0698fff23626d10c42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scan completed.\n",
            "  Total .py files encountered: 136\n",
            "  .py files processed with snippets: 99\n",
            "  .py files skipped/with errors: 37\n",
            "  Total snippets extracted (before sampling): 1196\n",
            "\n",
            "Attempting to save KB (1196 snippets) to: /content/drive/MyDrive/RAG_Project/library_kbs/kb_seed-labs__seed-emulator_sample_0.json\n",
            "[OK] KB successfully saved.\n",
            "\n",
            "--- KB for seed-labs__seed-emulator Ready (1196 snippets) ---\n",
            "\n",
            "--- End of KB Creation ---\n"
          ]
        }
      ],
      "source": [
        "# --- 3. KB Creation & Persistence ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"--- Knowledge Base (KB) Creation ---\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "current_kb = []  # Initialize KB as empty\n",
        "\n",
        "try:\n",
        "    # Retrieve info from the loaded dataset\n",
        "    sample = lca_dataset_split[SAMPLE_INDEX]\n",
        "    repo_full_name = sample.get('repo_full_name')\n",
        "\n",
        "    if not repo_full_name:\n",
        "        print(f\"[ERROR] 'repo_full_name' not found in dataset sample {SAMPLE_INDEX}.\")\n",
        "    else:\n",
        "        print(f\"Processing Sample {SAMPLE_INDEX}: Library '{repo_full_name}'\")\n",
        "        print(f\"Source code path: {library_source_dir}\")\n",
        "\n",
        "        # Build the KB\n",
        "        current_kb = build_kb_for_library(library_source_dir)  # Use the improved function\n",
        "\n",
        "        # Save the KB to Drive if it's not empty\n",
        "        if current_kb:\n",
        "            # Create the save directory on Drive if it doesn't exist\n",
        "            try:\n",
        "                os.makedirs(DRIVE_KB_SAVE_DIR, exist_ok=True)\n",
        "            except OSError as drive_err:\n",
        "                print(f\"\\n[ERROR] Unable to create save directory on Drive: {DRIVE_KB_SAVE_DIR}\")\n",
        "                print(f\"  Error: {drive_err}\")\n",
        "                print(\"  KB save skipped.\")\n",
        "                # You might choose to exit or continue without saving\n",
        "                # raise drive_err  # Uncomment to stop execution\n",
        "\n",
        "            # Build the full path for the KB file\n",
        "            # Clean the repo name to avoid problematic characters in filenames\n",
        "            safe_repo_name = repo_full_name.replace('/', '__')  # Replace / with __\n",
        "            kb_filename = f\"kb_{safe_repo_name}_sample_{SAMPLE_INDEX}.json\"\n",
        "            kb_full_path = os.path.join(DRIVE_KB_SAVE_DIR, kb_filename)\n",
        "\n",
        "            print(f\"\\nAttempting to save KB ({len(current_kb)} snippets) to: {kb_full_path}\")\n",
        "            try:\n",
        "                with open(kb_full_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(current_kb, f, indent=2, ensure_ascii=False)\n",
        "                print(f\"[OK] KB successfully saved.\")\n",
        "            except OSError as save_err:\n",
        "                print(f\"\\n[ERROR] Unable to write KB file to Drive: {kb_full_path}\")\n",
        "                print(f\"  Error: {save_err}. Check write permissions on Drive.\")\n",
        "            except Exception as json_err:\n",
        "                print(f\"\\n[ERROR] Error during JSON serialization of the KB: {json_err}\")\n",
        "        else:\n",
        "            print(\"\\nKB is empty, no file saved.\")\n",
        "\n",
        "except IndexError:\n",
        "    print(f\"[ERROR] Index {SAMPLE_INDEX} out of bounds for 'lca_dataset_split' (size: {len(lca_dataset_split)}).\")\n",
        "except Exception as main_e:\n",
        "    import traceback\n",
        "    print(f\"\\n[ERROR] Unexpected error in main script:\")\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "# --- 4. Final Check ---\n",
        "if current_kb:\n",
        "    print(f\"\\n--- KB for {repo_full_name} Ready ({len(current_kb)} snippets) ---\")\n",
        "else:\n",
        "    print(f\"\\n--- KB not created or empty ---\")\n",
        "\n",
        "print(\"\\n--- End of KB Creation ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGMR7ZvsRz9p"
      },
      "source": [
        "## Section 6: BM25 Retrieval & Prompt Assembly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f20m3cZmR2bM",
        "outputId": "1e085178-5813-4ad0-cabf-88f89059a2b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Retrieval with BM25 ---\n",
            "Using in-memory KB ('current_kb').\n",
            "\n",
            "--- Running BM25 for Sample 0 (Library: seed-labs__seed-emulator) ---\n",
            "Instruction (Query): Generate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n",
            "\n",
            "The base layer should create multiple autonomous systems an...\n",
            "\n",
            "Tokenizing Knowledge Base...\n",
            "Tokenized KB (1196 valid documents).\n",
            "Creating BM25 index (k1=1.5, b=0.75)...\n",
            "BM25 index created.\n",
            "Tokenizing instruction (query)...\n",
            "Retrieving top 5 relevant snippets...\n",
            "\n",
            "--- Top 5 Snippets Retrieved (BM25) ---\n",
            "\n",
            "--- Snippet 1 (BM25 Rank 1) ---\n",
            "def makeStubAs(emu: Emulator, base: Base, asn: int, exchange: int, services: ... (total length: 895 characters)\n",
            "\n",
            "--- Snippet 2 (BM25 Rank 2) ---\n",
            "def __init__( self, onAsConflict: Callable[[AutonomousSystem, AutonomousSystem], ... (total length: 1088 characters)\n",
            "\n",
            "--- Snippet 3 (BM25 Rank 3) ---\n",
            "def _doInstall(self, node: Node, server: Server): assert False, 'CymruIpOriginService ... (total length: 243 characters)\n",
            "\n",
            "--- Snippet 4 (BM25 Rank 4) ---\n",
            "def install(self, vnode: str) -> Server: assert False, 'ReverseDomainNameService is ... (total length: 240 characters)\n",
            "\n",
            "--- Snippet 5 (BM25 Rank 5) ---\n",
            "def getInternetExchangeMembers(self, id: int) -> Dict[int, str]: \"\"\"! @brief Get ... (total length: 484 characters)\n",
            "\n",
            "--- [OK] Retrieved 5 BM25 snippets ---\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Configuration ---\n",
        "SAMPLE_INDEX = 0      # Index of the sample to process (same as the KB cells)\n",
        "TOP_K_SNIPPETS = 5    # Number of snippets to retrieve with BM25\n",
        "BM25_K1 = 1.5         # BM25 parameter (common default, controls TF saturation)\n",
        "BM25_B = 0.75         # BM25 parameter (common default, controls document length)\n",
        "DRIVE_KB_SAVE_DIR = '/content/drive/MyDrive/RAG_Project/library_kbs' # KB folder on Drive\n",
        "\n",
        "# --- 2. Tokenizer Helper ---\n",
        "def simple_code_tokenizer(text):\n",
        "    \"\"\"\n",
        "    Simple tokenizer optimized for code snippets:\n",
        "    - lowercase\n",
        "    - split on spaces and common punctuation (keeping underscores)\n",
        "    - optionally removes very short tokens\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):  # Handles non-string input\n",
        "        return []\n",
        "    text = text.lower()\n",
        "    # Replace non-alphanumeric or underscore characters with space\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # Split on multiple spaces\n",
        "    tokens = text.split()\n",
        "    # Optional: remove very short tokens (e.g., length 1), they might be noise\n",
        "    # tokens = [token for token in tokens if len(token) > 1]\n",
        "    return tokens\n",
        "\n",
        "# --- 3. KB Loading ---\n",
        "\n",
        "print(\"--- Retrieval with BM25 ---\")\n",
        "kb_data = None  # Initialize KB\n",
        "\n",
        "# Check required variables from previous cells\n",
        "if 'lca_dataset_split' not in locals() or not lca_dataset_split:\n",
        "    raise NameError(\"CRITICAL ERROR: 'lca_dataset_split' not defined or empty. Re-run the dataset loading cell.\")\n",
        "\n",
        "# Try using KB already in memory ('current_kb' from the previous cell)\n",
        "# Check that it exists, is a list, and is not empty\n",
        "if 'current_kb' in locals() and isinstance(current_kb, list) and current_kb:\n",
        "    print(\"Using in-memory KB ('current_kb').\")\n",
        "    kb_data = current_kb\n",
        "else:\n",
        "    # If current_kb is not valid, try loading from Drive\n",
        "    print(\"\\n'current_kb' not available or empty in memory.\")\n",
        "    try:\n",
        "        # Determine KB file name (requires repo_full_name)\n",
        "        sample = lca_dataset_split[SAMPLE_INDEX]\n",
        "        repo_full_name_for_kb = sample.get('repo_full_name')\n",
        "        if not repo_full_name_for_kb:\n",
        "            print(f\"[ERROR] 'repo_full_name' not found in sample {SAMPLE_INDEX} to load KB.\")\n",
        "        else:\n",
        "            # Clean repo name and build path\n",
        "            safe_repo_name = repo_full_name_for_kb.replace('/', '__')\n",
        "            kb_filename = f\"kb_{safe_repo_name}_sample_{SAMPLE_INDEX}.json\"\n",
        "            kb_full_path = os.path.join(DRIVE_KB_SAVE_DIR, kb_filename)\n",
        "\n",
        "            if os.path.exists(kb_full_path):\n",
        "                print(f\"Attempting to load KB from Drive: {kb_full_path}\")\n",
        "                with open(kb_full_path, 'r', encoding='utf-8') as f:\n",
        "                    kb_data = json.load(f)\n",
        "                # Additional check: is the loaded file a non-empty list?\n",
        "                if isinstance(kb_data, list) and kb_data:\n",
        "                    print(f\"KB for '{repo_full_name_for_kb}' loaded from Drive ({len(kb_data)} snippets).\")\n",
        "                else:\n",
        "                    print(f\"[ERROR] KB file loaded from '{kb_full_path}' is not a valid list or is empty.\")\n",
        "                    kb_data = None  # Reset if content is invalid\n",
        "            else:\n",
        "                print(f\"[ERROR] KB file not found at: {kb_full_path}\")\n",
        "\n",
        "    except IndexError:\n",
        "        print(f\"[ERROR] Invalid index {SAMPLE_INDEX} for 'lca_dataset_split' when retrieving repo name.\")\n",
        "    except FileNotFoundError:  # If DRIVE_KB_SAVE_DIR does not exist\n",
        "        print(f\"[ERROR] KB directory on Drive not found: {DRIVE_KB_SAVE_DIR}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Unexpected error while loading KB from Drive: {e}\")\n",
        "        kb_data = None  # Ensure None in case of error\n",
        "\n",
        "# If kb_data is still not loaded, exit with a clear error\n",
        "if not kb_data:\n",
        "    raise RuntimeError(\"CRITICAL ERROR: Unable to obtain Knowledge Base (KB) data, neither from memory nor Drive. \"\n",
        "                       \"Run Step 2.B cell first to create/save the KB.\")\n",
        "\n",
        "# --- 4. BM25 Indexing & Retrieval ---\n",
        "retrieved_snippets_bm25 = []  # Initialize results list\n",
        "\n",
        "try:\n",
        "    # Extract instruction and repo name (reuse sample if previously loaded)\n",
        "    if 'sample' not in locals() or sample is None:  # Load sample if not already loaded\n",
        "        sample = lca_dataset_split[SAMPLE_INDEX]\n",
        "    instruction = sample.get('instruction')\n",
        "    repo_full_name = sample.get('repo_full_name', 'N/A')  # Use N/A if missing\n",
        "\n",
        "    if not instruction:\n",
        "        print(\"[ERROR] Instruction (query) not found in the sample.\")\n",
        "    else:\n",
        "        print(f\"\\n--- Running BM25 for Sample {SAMPLE_INDEX} (Library: {repo_full_name}) ---\")\n",
        "        print(f\"Instruction (Query): {instruction[:250]}...\")  # Show a bit more of the query\n",
        "\n",
        "        # 4.1 Tokenize the Knowledge Base (ensure snippets are strings)\n",
        "        print(\"\\nTokenizing Knowledge Base...\")\n",
        "        valid_kb_docs = [doc for doc in kb_data if isinstance(doc, str) and doc.strip()]\n",
        "        if len(valid_kb_docs) < len(kb_data):\n",
        "            print(f\"  Warning: {len(kb_data) - len(valid_kb_docs)} invalid snippets (non-strings/empty) ignored.\")\n",
        "\n",
        "        if not valid_kb_docs:\n",
        "            print(\"[ERROR] No valid snippets found in the KB after cleaning.\")\n",
        "        else:\n",
        "            tokenized_kb = [simple_code_tokenizer(doc) for doc in valid_kb_docs]\n",
        "            # Remove any empty lists resulting from tokenization\n",
        "            tokenized_kb_filtered = [tokens for tokens in tokenized_kb if tokens]\n",
        "            if not tokenized_kb_filtered:\n",
        "                print(\"[ERROR] Tokenized KB is empty after removing empty tokens.\")\n",
        "            else:\n",
        "                original_indices = [i for i, tokens in enumerate(tokenized_kb) if tokens]  # Original indices of valid docs\n",
        "                print(f\"Tokenized KB ({len(tokenized_kb_filtered)} valid documents).\")\n",
        "\n",
        "                # 4.2 Create the BM25 index with configured parameters\n",
        "                print(f\"Creating BM25 index (k1={BM25_K1}, b={BM25_B})...\")\n",
        "                bm25 = BM25Okapi(tokenized_kb_filtered, k1=BM25_K1, b=BM25_B)\n",
        "                print(\"BM25 index created.\")\n",
        "\n",
        "                # 4.3 Tokenize the instruction (query)\n",
        "                print(\"Tokenizing instruction (query)...\")\n",
        "                tokenized_query = simple_code_tokenizer(instruction)\n",
        "                if not tokenized_query:\n",
        "                    print(\"[ERROR] Tokenized query is empty.\")\n",
        "                else:\n",
        "                    # 4.4 Perform the retrieval\n",
        "                    print(f\"Retrieving top {TOP_K_SNIPPETS} relevant snippets...\")\n",
        "                    scores = bm25.get_scores(tokenized_query)\n",
        "                    top_n_filtered_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:TOP_K_SNIPPETS]\n",
        "                    retrieved_snippets_bm25 = [\n",
        "                        valid_kb_docs[original_indices[i]] for i in top_n_filtered_indices if i < len(original_indices)\n",
        "                    ]\n",
        "\n",
        "                    print(f\"\\n--- Top {len(retrieved_snippets_bm25)} Snippets Retrieved (BM25) ---\")\n",
        "                    if retrieved_snippets_bm25:\n",
        "                        for i, snippet in enumerate(retrieved_snippets_bm25):\n",
        "                            print(f\"\\n--- Snippet {i+1} (BM25 Rank {i+1}) ---\")\n",
        "                            snippet_preview = textwrap.shorten(\n",
        "                                snippet.strip(),\n",
        "                                width=120,\n",
        "                                placeholder=f\" ... (total length: {len(snippet)} characters)\"\n",
        "                            )\n",
        "                            print(snippet_preview)\n",
        "                    else:\n",
        "                        print(\"No snippets retrieved.\")\n",
        "\n",
        "except IndexError:\n",
        "    print(f\"[ERROR] Invalid index {SAMPLE_INDEX} for 'lca_dataset_split'.\")\n",
        "except Exception as main_e:\n",
        "    import traceback\n",
        "    print(f\"\\n[ERROR] Unexpected error in BM25 main script:\")\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "if retrieved_snippets_bm25:\n",
        "    print(f\"\\n--- [OK] Retrieved {len(retrieved_snippets_bm25)} BM25 snippets ---\")\n",
        "    # The variable 'retrieved_snippets_bm25' contains the list of strings\n",
        "else:\n",
        "    print(f\"\\n--- [WARNING/ERROR] No snippets retrieved from BM25 ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHHG4wb4R4K2",
        "outputId": "72204762-81fa-4d30-f63b-213644737c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "--- Step 4: Creating RAG Prompt (SIF) ---\n",
            "========================================\n",
            "Creating SIF prompt with max 3500 tokens...\n",
            "Token calculation: Total max=3500, Instruction=189, Base template=92\n",
            "Available tokens for snippets (approx): 3169\n",
            "\n",
            "Prompt SIF created.\n",
            "  Snippets included: 5 / 5\n",
            "  Estimated length (content only): 1152 tokens (Limit set: 3500)\n",
            "\n",
            "--- Preview of Final SIF Prompt (start) ---\n",
            "SYSTEM: You are an expert Python programmer. Generate Python code based ONLY on the user's instruction, using the provided library code snippets for context and correct API usage. Adapt snippets as needed; do not copy them verbatim unless requested. USER: ### Context: Relevant Code Snippets from Library # --- Snippet 1 --- ```python def makeStubAs(emu: Emulator, base: Base, asn: int, exchange: int, services: List[Service]): \"\"\"! @brief create a new stub AS. @param emu reference to the Emulator object. @param base reference to the base layer. @param asn ASN for the newly created AS. @param exchange IXP ID for new newly created AS to join. @param list of instances of Service to install on hosts. One host will be created for each. \"\"\" # Create AS and internal network stub_as = base.createAutonomousSystem(asn) stub_as.createNetwork('net0') # Create a BGP router # Attach the router to both the internal and external networks router = stub_as.createRouter('router0') router.joinNetwork('net0') router.joinNetwork('ix{}'.format(exchange)) # Create a host node for each specified service createHostsOnNetwork(emu, stub_as, 'net0', services) ``` # --- Snippet 2 --- ```python def __init__( self, onAsConflict: Callable[[AutonomousSystem, AutonomousSystem], AutonomousSystem] = lambda asA, asB: asA, onIxConflict: Callable[[InternetExchange, InternetExchange], InternetExchange] = lambda ixA, ixB: ixA): \"\"\"! @brief DefaultBaseMerger constructor. @param onAsConflict AS conflict [...]\n",
            "```python\n",
            "\n",
            "\n",
            "--- End of SIF Prompt Creation ---\n"
          ]
        }
      ],
      "source": [
        "# --- 5. SIF Prompt Creation ---\n",
        "\n",
        "# --- Constants and Configurations (Optional but good practice) ---\n",
        "# Conservative estimate of tokens for the fixed prompt structure\n",
        "# (You can calculate it more precisely later with your tokenizer)\n",
        "# Safety margin to avoid hitting the limit exactly\n",
        "PROMPT_TEMPLATE_BASE_TOKENS = 100\n",
        "TOKEN_LIMIT_MARGIN = 50\n",
        "\n",
        "def create_sif_prompt(\n",
        "    instruction: str,                          # Original instruction\n",
        "    retrieved_snippets: list[str],            # List of retrieved snippets (from BM25 or similar)\n",
        "    tokenizer,                                # Loaded Hugging Face tokenizer instance\n",
        "    max_prompt_tokens: int = 3500,            # Maximum tokens for the entire prompt\n",
        "    # model_max_length: Optional[int] = None  # Optional: Model max length (if different)\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Creates a SIF (Snippet Integration Format) prompt optimized for an LLM.\n",
        "\n",
        "    Integrates retrieved snippets as context for code generation based on the given instruction,\n",
        "    handling tokenization and truncation.\n",
        "\n",
        "    Args:\n",
        "        instruction: The user's instruction.\n",
        "        retrieved_snippets: List of strings containing the retrieved code snippets.\n",
        "        tokenizer: The initialized Hugging Face tokenizer instance.\n",
        "        max_prompt_tokens: The approximate maximum tokens allowed for the final prompt.\n",
        "                           (Considers the LLM's context window minus the tokens for the response).\n",
        "        # model_max_length: Optional: The model's absolute max length, if known and different\n",
        "        #                   from tokenizer.model_max_length.\n",
        "\n",
        "    Returns:\n",
        "        The formatted prompt string ready to be passed to the LLM.\n",
        "        Returns an empty string if the instruction is missing.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If tokenizer is not provided or is invalid.\n",
        "        ValueError: If max_prompt_tokens is not a positive integer.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(instruction, str) or not instruction.strip():\n",
        "        warnings.warn(\"Missing or empty instruction; returning an empty prompt.\")\n",
        "        return \"\"\n",
        "    if tokenizer is None or not hasattr(tokenizer, 'encode'):\n",
        "        raise TypeError(\"A valid Hugging Face tokenizer is required for create_sif_prompt.\")\n",
        "    if not isinstance(max_prompt_tokens, int) or max_prompt_tokens <= 0:\n",
        "        raise ValueError(\"max_prompt_tokens must be a positive integer.\")\n",
        "\n",
        "    # Determine the effective context limit of the model, if available\n",
        "    effective_model_max_length = getattr(tokenizer, 'model_max_length', None)\n",
        "    if effective_model_max_length and max_prompt_tokens > effective_model_max_length:\n",
        "        warnings.warn(\n",
        "            f\"max_prompt_tokens ({max_prompt_tokens}) exceeds the model's maximum length\"\n",
        "            f\" ({effective_model_max_length}). The model limit will take precedence\"\n",
        "        )\n",
        "\n",
        "    # --- Improved Prompt Template ---\n",
        "    prompt_template = \"\"\"SYSTEM: You are an expert Python programmer. Generate Python code based ONLY on the user's instruction, using the provided library code snippets for context and correct API usage. Adapt snippets as needed; do not copy them verbatim unless requested.\n",
        "\n",
        "USER:\n",
        "### Context: Relevant Code Snippets from Library\n",
        "\n",
        "{snippets_section}\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "ASSISTANT:\n",
        "```python\n",
        "\"\"\"\n",
        "\n",
        "    # --- End of Template ---\n",
        "\n",
        "    # --- Calculating Available Space for Snippets ---\n",
        "    # Tokenize instruction and base template to know how much space remains\n",
        "    # Use add_special_tokens=False to count only content tokens\n",
        "    instruction_tokens = len(tokenizer(instruction, add_special_tokens=False).input_ids)\n",
        "    template_base_formatted = prompt_template.format(snippets_section=\"\", instruction=\"\")\n",
        "    template_base_tokens = len(tokenizer(template_base_formatted, add_special_tokens=False).input_ids)\n",
        "\n",
        "    available_tokens_for_snippets = max(\n",
        "        0,\n",
        "        max_prompt_tokens\n",
        "        - instruction_tokens\n",
        "        - template_base_tokens\n",
        "        - TOKEN_LIMIT_MARGIN\n",
        "    )\n",
        "    print(f\"Token calculation: Total max={max_prompt_tokens}, Instruction={instruction_tokens}, Base template={template_base_tokens}\")\n",
        "    print(f\"Available tokens for snippets (approx): {available_tokens_for_snippets}\")\n",
        "\n",
        "    # --- Constructing Snippet Section with Token Checks ---\n",
        "    snippets_text_parts = []\n",
        "    accumulated_snippet_tokens = 0\n",
        "    snippets_included_count = 0\n",
        "\n",
        "    if not retrieved_snippets:\n",
        "        warnings.warn(\"No snippets provided to create_sif_prompt.\")\n",
        "\n",
        "    for i, snippet in enumerate(retrieved_snippets):\n",
        "        if not isinstance(snippet, str) or not snippet.strip():\n",
        "            continue\n",
        "\n",
        "        snippet_header = f\"# --- Snippet {i+1} ---\\n\"\n",
        "        snippet_content = snippet.strip().strip('`')\n",
        "        if not snippet_content:\n",
        "            continue\n",
        "        snippet_formatted = f\"```python\\n{snippet_content}\\n```\\n\\n\"\n",
        "\n",
        "        # Estimate tokens for this snippet (header + formatted code)\n",
        "        current_snippet_section_tokens = len(\n",
        "            tokenizer(snippet_header + snippet_formatted, add_special_tokens=False).input_ids\n",
        "        )\n",
        "\n",
        "        # Check if adding this snippet exceeds available space\n",
        "        if accumulated_snippet_tokens + current_snippet_section_tokens > available_tokens_for_snippets:\n",
        "            print(\n",
        "                f\"INFO: Token limit for snippets ({available_tokens_for_snippets}) reached. \"\n",
        "                f\"Snippet {i+1} and subsequent ones skipped.\"\n",
        "            )\n",
        "            break\n",
        "\n",
        "        # Add snippet to the prompt\n",
        "        snippets_text_parts.append(snippet_header)\n",
        "        snippets_text_parts.append(snippet_formatted)\n",
        "        accumulated_snippet_tokens += current_snippet_section_tokens\n",
        "        snippets_included_count += 1\n",
        "\n",
        "    # Assemble final snippet section\n",
        "    if snippets_included_count > 0:\n",
        "        snippets_section_content = \"\".join(snippets_text_parts).strip()\n",
        "    else:\n",
        "        snippets_section_content = \"# (No relevant snippets provided or all exceeded token limit)\"\n",
        "\n",
        "    # --- Composing Final Prompt ---\n",
        "    final_prompt = prompt_template.format(\n",
        "        snippets_section=snippets_section_content,\n",
        "        instruction=instruction\n",
        "    )\n",
        "\n",
        "    # --- Final Length Check (Optional but Useful) ---\n",
        "    final_token_count = len(\n",
        "        tokenizer(final_prompt, add_special_tokens=False).input_ids\n",
        "    )\n",
        "    print(f\"\\nPrompt SIF created.\")\n",
        "    print(f\"  Snippets included: {snippets_included_count} / {len(retrieved_snippets)}\")\n",
        "    print(f\"  Estimated length (content only): {final_token_count} tokens (Limit set: {max_prompt_tokens})\")\n",
        "\n",
        "    if effective_model_max_length and final_token_count > effective_model_max_length:\n",
        "        warnings.warn(\n",
        "            f\"The final prompt ({final_token_count} tokens) EXCEEDS the model's maximum length\"\n",
        "            f\" ({effective_model_max_length}). It may be truncated or cause errors.\"\n",
        "        )\n",
        "    elif final_token_count > max_prompt_tokens:\n",
        "        warnings.warn(\n",
        "            f\"The final prompt ({final_token_count} tokens) EXCEEDS the 'max_prompt_tokens' limit\"\n",
        "            f\" ({max_prompt_tokens}). The token estimate may be inaccurate.\"\n",
        "        )\n",
        "\n",
        "    return final_prompt\n",
        "\n",
        "# --- Example Usage (Modified to use correct variable) ---\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"--- Step 4: Creating RAG Prompt (SIF) ---\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "sif_prompt_final = None\n",
        "\n",
        "if ('instruction' in locals() and instruction and\n",
        "    'retrieved_snippets_bm25' in locals() and isinstance(retrieved_snippets_bm25, list) and\n",
        "    'tokenizer' in locals() and tokenizer):\n",
        "\n",
        "    prompt_token_limit = 3500\n",
        "    print(f\"Creating SIF prompt with max {prompt_token_limit} tokens...\")\n",
        "    sif_prompt_final = create_sif_prompt(\n",
        "        instruction=instruction,\n",
        "        retrieved_snippets=retrieved_snippets_bm25,\n",
        "        tokenizer=tokenizer,\n",
        "        max_prompt_tokens=prompt_token_limit\n",
        "    )\n",
        "\n",
        "    if sif_prompt_final:\n",
        "        print(\"\\n--- Preview of Final SIF Prompt (start) ---\")\n",
        "        # Usa textwrap.shorten per la preview\n",
        "        print(textwrap.shorten(sif_prompt_final, width=1500, placeholder=\" [...]\\n```python\\n\")) # show the beginning\n",
        "    else:\n",
        "         print(\"[ERROR] Failed to create the SIF prompt (returned empty).\")\n",
        "\n",
        "else:\n",
        "    missing_vars = []\n",
        "    if 'instruction' not in locals() or not instruction: missing_vars.append(\"'instruction'\")\n",
        "    if 'retrieved_snippets_bm25' not in locals() or not isinstance(retrieved_snippets_bm25, list): missing_vars.append(\"'retrieved_snippets_bm25' (BM25 list)\")\n",
        "    if 'tokenizer' not in locals() or not tokenizer: missing_vars.append(\"'tokenizer'\")\n",
        "    print(f\"[ERROR] Cannot create SIF prompt. Missing or invalid variables: {', '.join(missing_vars)}.\")\n",
        "    print(\"         Ensure the previous cells (dataset loading, BM25, tokenizer load) ran correctly.\")\n",
        "\n",
        "print(\"\\n--- End of SIF Prompt Creation ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3RCwduHR6iR"
      },
      "source": [
        "## Section 7 ¬∑ RAG Code Generation and Output Processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqOSW9JBR6Fc",
        "outputId": "0c1b6a18-678b-4f33-a64f-7fe1c8adecb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping sequence: '\n",
            "```\n",
            "' -> IDs: [185, 10252, 185]\n",
            "Stop on EOS (32014): True\n",
            "Custom StoppingCriteria created\n",
            "\n",
            "========================================\n",
            "--- Step 5: RAG Code Generation ---\n",
            "========================================\n",
            "SIF prompt received (length: 4339 chars).\n",
            "Generation parameters:\n",
            "  max_new_tokens=1024, temperature=0.6\n",
            "  top_p=0.95, top_k=50\n",
            "  repetition_penalty=1.1\n",
            "  do_sample=True\n",
            "  Stopping Criteria: Active\n",
            "\n",
            "Tokenizing SIF prompt...\n",
            "Tokenized input length: 1153 tokens.\n",
            "Starting code generation...\n",
            "Generation completed in 63.54 seconds.\n",
            "\n",
            "--- Generated Code (Raw) ---\n",
            "import seedemu\n",
            "from seedemu import *\n",
            "\n",
            "ASN_BASE = 64512\n",
            "EXCHANGE_BASE = 9876\n",
            "\n",
            "EMPTY_DOMAINNAME_SERVICE_ID = 0\n",
            "REVERSE_DOMAINNAME_SERVICE_ID = 1\n",
            "IPV4_ORIGIN_SERVICE_ID = 2\n",
            "DELEGATION_ZONE_SERVICE_ID = 3\n",
            "DNS_SERVER_PORT = 53\n",
            "BACKUP_DNS_SERVER_PORT = 53\n",
            "ROUTER_ADDR = '10.1.1.1/24'\n",
            "NETWORK_SIZE = 100\n",
            "MAX_HOSTS = 50\n",
            "\n",
            "class StubAsException(Exception):\n",
            "    pass\n",
            "\n",
            "class AsNotExistsException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class IxNotFoundException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostAlreadyJoinedException(S...\n",
            "\n",
            "No ``` block found, taking the full output.\n",
            "\n",
            "--- Generated Code (Clean) ---\n",
            "import seedemu\n",
            "from seedemu import *\n",
            "\n",
            "ASN_BASE = 64512\n",
            "EXCHANGE_BASE = 9876\n",
            "\n",
            "EMPTY_DOMAINNAME_SERVICE_ID = 0\n",
            "REVERSE_DOMAINNAME_SERVICE_ID = 1\n",
            "IPV4_ORIGIN_SERVICE_ID = 2\n",
            "DELEGATION_ZONE_SERVICE_ID = 3\n",
            "DNS_SERVER_PORT = 53\n",
            "BACKUP_DNS_SERVER_PORT = 53\n",
            "ROUTER_ADDR = '10.1.1.1/24'\n",
            "NETWORK_SIZE = 100\n",
            "MAX_HOSTS = 50\n",
            "\n",
            "class StubAsException(Exception):\n",
            "    pass\n",
            "\n",
            "class AsNotExistsException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class IxNotFoundException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostAlreadyJoinedException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostDoesntHaveIXException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostDoesntHaveAnyNetworksException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostIsInAnotherAsException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostHasNoJoinableNetworksException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddToASException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantCreateNetworkException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantJoinNetworkException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantJoinIXException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantCreateRouterException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantCreatePeeringException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeeringException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantCreateServerException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddServerException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddBindingException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantCreateRoutingInstanceException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantCreatePeerException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantCreatePeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeersToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeeringsToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantCreatePeeringGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeeringGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeerGroupsToPeeringGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeeringsToPeeringGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeerAndPeerGroupToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeerOrPeerGroupToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeerWithSameDestinationToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeerFromOtherAsToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddPeerOrPeerGroupToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantCreateBgpPeerException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddBgpPeerToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddBgpPeerGroupToPeeringGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddBgpPeerGroupToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddBgpPeerFromOtherAsToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddBgpPeerOrPeerGroupToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddBgpPeerOrPeerGroupToPeeringGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddBgpPeerAndPeerGroupToPeerGroupException(StubAsException):\n",
            "    pass\n",
            "\n",
            "class HostCantAddBgpPeerWithSameDestinationTo\n",
            "\n",
            "--- RAG code generation completed ---\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList, LogitsProcessor, LogitsProcessorList\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "# --- 1. Generation Setup ---\n",
        "# --- 1.1. Configuration Parameters ---\n",
        "MAX_NEW_TOKENS = 1024      # Max tokens to generate for the response\n",
        "TEMPERATURE = 0.6          # Recommended value for R1-Distill (0.5-0.7). Lower = more deterministic\n",
        "TOP_P = 0.95               # Nucleus sampling (considers only tokens whose cumulative probability > top_p)\n",
        "TOP_K = 50                 # Top-k sampling (considers only the top k most probable tokens)\n",
        "REPETITION_PENALTY = 1.1   # Slightly penalize already generated tokens (e.g., 1.1-1.2) to reduce repetition\n",
        "DO_SAMPLE = True           # Enable sampling (True to use temp/top_p/top_k, False for greedy/deterministic)\n",
        "STOP_ON_EOS = True         # Stop generation if the EOS token is generated\n",
        "STOP_ON_CODE_END = True    # Attempt to stop after the end of a code block (e.g. ```)\n",
        "\n",
        "# --- 1.2. Advanced Stopping Criteria (Optional but Recommended) ---\n",
        "# Combines EOS stop and, optionally, code block ending\n",
        "\n",
        "class EosAndCodeStopCriteria(StoppingCriteria):\n",
        "    def __init__(self, tokenizer, stop_on_eos=True, stop_sequence=\"\\n```\\n\"):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.stop_on_eos = stop_on_eos\n",
        "        self.stop_sequence = stop_sequence\n",
        "        self.stop_sequence_ids = self.tokenizer.encode(stop_sequence, add_special_tokens=False)\n",
        "        # Remove any unwanted leading/trailing tokens from the stop sequence\n",
        "        # (e.g., if encode adds BOS) - may require tokenizer-specific debugging\n",
        "        print(f\"Stopping sequence: '{self.stop_sequence}' -> IDs: {self.stop_sequence_ids}\")\n",
        "        print(f\"Stop on EOS ({self.tokenizer.eos_token_id}): {self.stop_on_eos}\")\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        # 1. Check EOS\n",
        "        if self.stop_on_eos and (input_ids[0, -1] == self.tokenizer.eos_token_id):\n",
        "            print(\"Stopping criteria: EOS token detected.\")\n",
        "            return True\n",
        "\n",
        "        # 2. Check the stop sequence (e.g., \\n```\\n)\n",
        "        if self.stop_sequence_ids:\n",
        "             # Check if the last N tokens match the stop sequence\n",
        "             len_stop_seq = len(self.stop_sequence_ids)\n",
        "             if input_ids.shape[1] >= len_stop_seq:\n",
        "                  last_tokens = input_ids[0, -len_stop_seq:]\n",
        "                  if torch.equal(last_tokens, torch.tensor(self.stop_sequence_ids).to(last_tokens.device)):\n",
        "                      print(f\"Stopping criteria: Stop sequence '{self.stop_sequence}' detected.\")\n",
        "                      return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria_list = None\n",
        "if STOP_ON_EOS or STOP_ON_CODE_END:\n",
        "     try:\n",
        "         custom_stopper = EosAndCodeStopCriteria(\n",
        "             tokenizer,\n",
        "             stop_on_eos=STOP_ON_EOS,\n",
        "             stop_sequence=\"\\n```\\n\" if STOP_ON_CODE_END else None # Use \\n```\\n as the code stop sequence\n",
        "         )\n",
        "         stopping_criteria_list = StoppingCriteriaList([custom_stopper])\n",
        "         print(\"Custom StoppingCriteria created\")\n",
        "     except Exception as e:\n",
        "          print(f\"WARNING: Unable to create custom StoppingCriteria: {e}\")\n",
        "\n",
        "# --- 1.3. (Optional) Forced Decoder IDs to start with <think> ---\n",
        "# According to R1-Distill recommendations. Basic implementation:\n",
        "# think_token_sequence = tokenizer.encode(\"<think>\\n\", add_special_tokens=False)\n",
        "# force_think_processor = LogitsProcessorList([\n",
        "#     ForcedBOSTokenLogitsProcessor(think_token_sequence[0]),  # Force the first token\n",
        "#     ForcedEOSTokenLogitsProcessor(max_length=MAX_NEW_TOKENS + len(think_token_sequence), eos_token_id=think_token_sequence[1:])  # Force the rest if necessary\n",
        "# ])\n",
        "# This part is complex and may require tokenizer-specific adjustments.\n",
        "# For now we omit it and rely on manually adding it to the prompt if needed.\n",
        "\n",
        "# --- 2. Input & Generation ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"--- Step 5: RAG Code Generation ---\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "generated_code_rag = None # Initialize output\n",
        "\n",
        "# Check dependencies\n",
        "if 'sif_prompt_final' in locals() and sif_prompt_final and \\\n",
        "   'model' in locals() and model and \\\n",
        "   'tokenizer' in locals() and tokenizer:\n",
        "\n",
        "    print(f\"SIF prompt received (length: {len(sif_prompt_final)} chars).\")\n",
        "    print(\"Generation parameters:\")\n",
        "    print(f\"  max_new_tokens={MAX_NEW_TOKENS}, temperature={TEMPERATURE if DO_SAMPLE else 'N/A (Greedy)'}\")\n",
        "    print(f\"  top_p={TOP_P if DO_SAMPLE else 'N/A'}, top_k={TOP_K if DO_SAMPLE else 'N/A'}\")\n",
        "    print(f\"  repetition_penalty={REPETITION_PENALTY}\")\n",
        "    print(f\"  do_sample={DO_SAMPLE}\")\n",
        "    print(f\"  Stopping Criteria: {'Active' if stopping_criteria_list else 'Inactive'}\")\n",
        "\n",
        "    try:\n",
        "        # --- Tokenization ---\n",
        "        print(\"\\nTokenizing SIF prompt...\")\n",
        "        # No need to truncate here if create_sif_prompt already handled limits\n",
        "        # max_length = tokenizer.model_max_length  # Model maximum length\n",
        "        inputs = tokenizer(\n",
        "            sif_prompt_final,\n",
        "            return_tensors=\"pt\",\n",
        "            # truncation=True,  # Enable only if strictly necessary\n",
        "            # max_length=max_length - MAX_NEW_TOKENS  # Leave room for generation\n",
        "        ).to(model.device)  # Move to GPU\n",
        "\n",
        "        input_length = inputs['input_ids'].shape[1]\n",
        "        print(f\"Tokenized input length: {input_length} tokens.\")\n",
        "\n",
        "        # --- Generation ---\n",
        "        print(\"Starting code generation...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        generation_args = {\n",
        "            \"input_ids\": inputs['input_ids'],\n",
        "            \"attention_mask\": inputs['attention_mask'],\n",
        "            \"max_new_tokens\": MAX_NEW_TOKENS,\n",
        "            \"pad_token_id\": tokenizer.eos_token_id,\n",
        "            \"repetition_penalty\": REPETITION_PENALTY,\n",
        "            \"stopping_criteria\": stopping_criteria_list  # Can be None\n",
        "        }\n",
        "        if DO_SAMPLE:\n",
        "            generation_args.update({\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"top_p\": TOP_P,\n",
        "                \"top_k\": TOP_K,\n",
        "                \"do_sample\": True,\n",
        "            })\n",
        "        else:\n",
        "            # Greedy (deterministic) generation\n",
        "            generation_args[\"do_sample\"] = False\n",
        "            # temperature, top_p, top_k are not used\n",
        "\n",
        "        with torch.no_grad():  # Essential for inference\n",
        "            # outputs = model.generate(**inputs, ...)  # Alternate way\n",
        "            outputs = model.generate(**generation_args)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Generation completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "        # --- Decode and Clean Output ---\n",
        "        # Decode only the NEW generated tokens\n",
        "        output_tokens = outputs[0, input_length:]\n",
        "        generated_code_rag_full = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "        print(\"\\n--- Generated Code (Raw) ---\")\n",
        "        print(generated_code_rag_full[:500] + \"...\" if len(generated_code_rag_full) > 500 else generated_code_rag_full)\n",
        "\n",
        "        # --- Specific Cleanup for Code Blocks ---\n",
        "        # Look for the content inside the first ```python ... ``` block\n",
        "        # This is more robust than splitting only on ```\n",
        "        code_block_match = re.search(r'```python\\n(.*?)(?:\\n```|\\Z)', generated_code_rag_full, re.DOTALL)\n",
        "        if code_block_match:\n",
        "            generated_code_rag = code_block_match.group(1).strip()\n",
        "            print(\"\\nExtracted code from the ```python ... ``` block.\")\n",
        "        else:\n",
        "            # Fallback: if it does not find ```python, take everything before a closing ```\n",
        "            # or simply take the whole output if there are no backticks.\n",
        "            if \"\\n```\" in generated_code_rag_full:  # Look for \\n``` to avoid inline matches\n",
        "                generated_code_rag = generated_code_rag_full.split(\"\\n```\")[0].strip()\n",
        "                print(\"\\n```python block not found, taking output before ```.\" )\n",
        "            else:\n",
        "                generated_code_rag = generated_code_rag_full.strip()\n",
        "                print(\"\\nNo ``` block found, taking the full output.\")\n",
        "\n",
        "        print(\"\\n--- Generated Code (Clean) ---\")\n",
        "        print(generated_code_rag)\n",
        "\n",
        "    # --- 3. Error Handling ---\n",
        "    except torch.cuda.OutOfMemoryError as e:\n",
        "        print(f\"\\n[ERROR] Out Of Memory (OOM) during GENERATION!\")\n",
        "        print(\"  The prompt plus the generated output may exceed VRAM.\")\n",
        "        print(\"  Try reducing 'max_prompt_tokens' in create_sif_prompt or 'MAX_NEW_TOKENS' here.\")\n",
        "        generated_code_rag = None\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"\\n[ERROR] Unexpected error during RAG generation:\")\n",
        "        print(traceback.format_exc())\n",
        "        generated_code_rag = None\n",
        "\n",
        "else:\n",
        "    missing = []\n",
        "    if 'sif_prompt_final' not in locals() or not sif_prompt_final: missing.append(\"'sif_prompt_final'\")\n",
        "    if 'model' not in locals() or not model: missing.append(\"'model'\")\n",
        "    if 'tokenizer' not in locals() or not tokenizer: missing.append(\"'tokenizer'\")\n",
        "    print(f\"[ERROR] Unable to perform generation. Missing or invalid variables: {', '.join(missing)}.\")\n",
        "    print(\"         Make sure the previous cells have been executed correctly.\")\n",
        "    generated_code_rag = None\n",
        "\n",
        "# --- 4. Final Verification ---\n",
        "if generated_code_rag:\n",
        "    print(\"\\n--- RAG code generation completed ---\")\n",
        "    # The variable 'generated_code_rag' contains the cleaned code\n",
        "else:\n",
        "    print(\"\\n--- [ERROR] RAG code generation failed or was not executed ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0TPBQavR-OM"
      },
      "source": [
        "## Section 8 ¬∑ Baseline Generation and RAG Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWWgseLgR_Xd",
        "outputId": "1108a84a-7483-4387-dc25-952168f2ddb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "--- Step 6.A: Baseline Generation (LLM-only) ---\n",
            "========================================\n",
            "NOTE: This cell expects that 'instruction', 'model', 'tokenizer'\n",
            "      and the generation parameters (MAX_NEW_TOKENS, etc.) have\n",
            "      been defined in the previous cells (including Step 5).\n",
            "\n",
            "All required variables were found.\n",
            "\n",
            "Baseline Prompt (start):\n",
            "USER: ### Instruction: Generate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. The base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. The domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. The eBGP layer should add private peerings between different autonomous systems. Finally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file. ASSISTANT: ```python\n",
            "\n",
            "Using the SAME parameters inherited from the RAG generation:\n",
            "  max_new_tokens=1024, temperature=0.6\n",
            "  top_p=0.95, top_k=50\n",
            "  repetition_penalty=1.1\n",
            "  do_sample=True\n",
            "  Stopping Criteria: Active\n",
            "\n",
            "Tokenized input length: 208 tokens.\n",
            "Starting Baseline generation...\n",
            "Stopping criteria: Stop sequence '\n",
            "```\n",
            "' detected.\n",
            "Baseline generation completed in 34.46 seconds.\n",
            "\n",
            "--- Baseline Generated Code (Raw) ---\n",
            "import os\n",
            "import random\n",
            "from seedemu import *\n",
            "\n",
            "# Create an emulator object\n",
            "my_emulator = Emulator(100)\n",
            "print(\"Emulator created.\")\n",
            "\n",
            "## Generates Autonomous System\n",
            "def generate_AS():\n",
            "    \"\"\"Create an autonomous system.\"\"\"\n",
            "    AS = my_emulator.create_autonomous_system()\n",
            "    \n",
            "    # Adds the host into the autonomous system\n",
            "    AS.add_host(random.randint(1024,65535))\n",
            "     \n",
            "    # Adds a router into the autonomous system\n",
            "    router = AS.add_router()\n",
            "  \n",
            "    # Creates the route from the router to the auto...\n",
            "\n",
            "```python block not found, took output before ```.\n",
            "\n",
            "--- Generated Code (Baseline LLM-only - Clean) ---\n",
            "import os\n",
            "import random\n",
            "from seedemu import *\n",
            "\n",
            "# Create an emulator object\n",
            "my_emulator = Emulator(100)\n",
            "print(\"Emulator created.\")\n",
            "\n",
            "## Generates Autonomous System\n",
            "def generate_AS():\n",
            "    \"\"\"Create an autonomous system.\"\"\"\n",
            "    AS = my_emulator.create_autonomous_system()\n",
            "    \n",
            "    # Adds the host into the autonomous system\n",
            "    AS.add_host(random.randint(1024,65535))\n",
            "     \n",
            "    # Adds a router into the autonomous system\n",
            "    router = AS.add_router()\n",
            "  \n",
            "    # Creates the route from the router to the autonomous system\n",
            "    AS.add_route(network=\"192.168.1.\"+str(random.randint(1024,65535)), prefixlen=24, gateway=router._id)\n",
            "      \n",
            "    # Binds the autonomous system with the internet exchange\n",
            "    AS.bind_internet_exchange('inet')\n",
            "       \n",
            "    return AS\n",
            "\n",
            "## GENERATE HOSTS\n",
            "for i in range(10):\n",
            "    ## Call function to generate an autonomous system\n",
            "    AS = generate_AS()\n",
            "        \n",
            "    print(\"\\nAutonomous system \" + str(i + 1) + \"\\n\")\n",
            "           \n",
            "    # Print out information about the autonomous system\n",
            "    print (\"Host ID:\\t\", AS[0]._id)\n",
            "    print (\"Router ID:\\t\", AS[-1].get_ID())\n",
            "    print (\"Address:\\t\", AS[0][0])\n",
            "    print (\"IP Address:\\t\", AS[0][1])\n",
            "    print (\"Prefix Length:\\t\", AS[0][2], \"\\n\")\n",
            "         \n",
            "    # Iterate through the hosts in the autonomous system\n",
            "    for host in AS[:-1]:\n",
            "        print (host.__str__(), '\\n')\n",
            "            \n",
            "## GENERATE ROUTING TABLES\n",
            "for i in range(10):\n",
            "    ## Call function to generate an autonomous system\n",
            "    AS = generate_AS()\n",
            "        \n",
            "    print(\"\\nAutonomous system \" + str(i + 1) + \"\\n\")\n",
            "           \n",
            "    # Print out information about the autonomous system\n",
            "    print (\"Host ID:\\t\", AS[0]._id)\n",
            "    print (\"Router ID:\\t\", AS[-1].get_ID())\n",
            "    print (\"Address:\\t\", AS[0][0])\n",
            "    print (\"IP Address:\\t\", AS[0][1])\n",
            "    print (\"Prefix Length:\\t\", AS[0][2], \"\\n\")\n",
            "         \n",
            "    # Iterate through the hosts in the autonomous system\n",
            "    for host in AS[:-1]:\n",
            "        print (host.__str__(), '\\n')\n",
            "          \n",
            "if __name__ == '__main__':\n",
            "    main()\n",
            "\n",
            "--- Baseline code generation completed ---\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "import re       # Required for regex cleanup\n",
        "import textwrap # For prompt preview\n",
        "import warnings # To handle warnings\n",
        "\n",
        "print(\"\\n\" + \"=\" * 40)\n",
        "print(\"--- Step 6.A: Baseline Generation (LLM-only) ---\")\n",
        "print(\"=\" * 40)\n",
        "print(\"NOTE: This cell expects that 'instruction', 'model', 'tokenizer'\")\n",
        "print(\"      and the generation parameters (MAX_NEW_TOKENS, etc.) have\")\n",
        "print(\"      been defined in the previous cells (including Step 5).\")\n",
        "\n",
        "generated_code_baseline = None  # Initialize output\n",
        "\n",
        "# --- 1. Robust Dependency Check ---\n",
        "# Verify all necessary variables inherited from the previous execution\n",
        "required_vars = [\n",
        "    'instruction', 'model', 'tokenizer',\n",
        "    'MAX_NEW_TOKENS', 'TEMPERATURE', 'TOP_P',\n",
        "    'TOP_K', 'REPETITION_PENALTY', 'DO_SAMPLE'\n",
        "]\n",
        "missing_vars = []\n",
        "invalid_vars = []\n",
        "\n",
        "for var_name in required_vars:\n",
        "    if var_name not in locals():\n",
        "        missing_vars.append(f\"'{var_name}'\")\n",
        "    # Also check that they are not None or empty (where applicable)\n",
        "    elif var_name in ['instruction', 'model', 'tokenizer'] and not locals()[var_name]:\n",
        "        invalid_vars.append(f\"'{var_name}' (is None or empty)\")\n",
        "\n",
        "# Also verify the stopping criteria (optional, but if it exists it must be used)\n",
        "# If it doesn't exist from the previous cell, it will be set to None later\n",
        "stopping_criteria_to_use = locals().get('stopping_criteria_list', None)\n",
        "\n",
        "# --- 2. Proceed only if all dependencies are OK ---\n",
        "if not missing_vars and not invalid_vars:\n",
        "\n",
        "    print(\"\\nAll required variables were found.\")\n",
        "\n",
        "    # --- 3. Baseline Prompt Creation ---\n",
        "    # Use the same prompt structure for consistency (even if simple)\n",
        "    baseline_prompt = f\"\"\"USER:\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "ASSISTANT:\n",
        "```python\n",
        "\"\"\"\n",
        "    # Do not print the entire prompt if it is very long\n",
        "    print(\"\\nBaseline Prompt (start):\")\n",
        "    print(textwrap.shorten(baseline_prompt, width=1200, placeholder=\"...```python\\n\"))\n",
        "\n",
        "    # --- 4. Code Generation ---\n",
        "    print(f\"\\nUsing the SAME parameters inherited from the RAG generation:\")\n",
        "    print(f\"  max_new_tokens={MAX_NEW_TOKENS}, temperature={TEMPERATURE if DO_SAMPLE else 'N/A (Greedy)'}\")\n",
        "    print(f\"  top_p={TOP_P if DO_SAMPLE else 'N/A'}, top_k={TOP_K if DO_SAMPLE else 'N/A'}\")\n",
        "    print(f\"  repetition_penalty={REPETITION_PENALTY}\")\n",
        "    print(f\"  do_sample={DO_SAMPLE}\")\n",
        "    print(f\"  Stopping Criteria: {'Active' if stopping_criteria_to_use else 'Inactive'}\")\n",
        "\n",
        "    try:\n",
        "        # --- Tokenization ---\n",
        "        inputs_base = tokenizer(baseline_prompt, return_tensors=\"pt\").to(model.device)\n",
        "        input_length_base = inputs_base['input_ids'].shape[1]\n",
        "        print(f\"\\nTokenized input length: {input_length_base} tokens.\")\n",
        "\n",
        "        # --- model.generate call (Same as RAG except for the input) ---\n",
        "        print(\"Starting Baseline generation...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        generation_args_base = {\n",
        "            \"input_ids\": inputs_base['input_ids'],\n",
        "            \"attention_mask\": inputs_base['attention_mask'],\n",
        "            \"max_new_tokens\": MAX_NEW_TOKENS,\n",
        "            \"pad_token_id\": tokenizer.eos_token_id,\n",
        "            \"repetition_penalty\": REPETITION_PENALTY,\n",
        "            \"stopping_criteria\": stopping_criteria_to_use  # Use the same one from RAG (can be None)\n",
        "        }\n",
        "        if DO_SAMPLE:\n",
        "            generation_args_base.update({\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"top_p\": TOP_P,\n",
        "                \"top_k\": TOP_K,\n",
        "                \"do_sample\": True,\n",
        "            })\n",
        "        else:\n",
        "            generation_args_base[\"do_sample\"] = False\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_base = model.generate(**generation_args_base)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Baseline generation completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "        # --- Decode and Clean (Same logic as RAG) ---\n",
        "        output_tokens_base = outputs_base[0, input_length_base:]\n",
        "        generated_code_baseline_full = tokenizer.decode(output_tokens_base, skip_special_tokens=True)\n",
        "\n",
        "        print(\"\\n--- Baseline Generated Code (Raw) ---\")\n",
        "        print(generated_code_baseline_full[:500] + \"...\" if len(generated_code_baseline_full) > 500 else generated_code_baseline_full)\n",
        "\n",
        "        # Cleanup with Regex (identical to RAG)\n",
        "        code_block_match_base = re.search(r'```python\\n(.*?)(?:\\n```|\\Z)', generated_code_baseline_full, re.DOTALL)\n",
        "        if code_block_match_base:\n",
        "            generated_code_baseline = code_block_match_base.group(1).strip()\n",
        "            print(\"\\nExtracted code from the ```python block.\")\n",
        "        else:\n",
        "            if \"\\n```\" in generated_code_baseline_full:\n",
        "                generated_code_baseline = generated_code_baseline_full.split(\"\\n```\")[0].strip()\n",
        "                print(\"\\n```python block not found, took output before ```.\")\n",
        "            else:\n",
        "                generated_code_baseline = generated_code_baseline_full.strip()\n",
        "                print(\"\\nNo ``` block found, taking the full output.\")\n",
        "\n",
        "        print(\"\\n--- Generated Code (Baseline LLM-only - Clean) ---\")\n",
        "        print(generated_code_baseline or \"[Empty generation]\")\n",
        "\n",
        "    except torch.cuda.OutOfMemoryError as e:\n",
        "        print(f\"\\n[ERROR] Out Of Memory (OOM) during BASELINE GENERATION!\")\n",
        "        print(\"  Try reducing 'MAX_NEW_TOKENS'.\")\n",
        "        generated_code_baseline = None  # Ensure None in case of error\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"\\n[ERROR] Unexpected error during Baseline generation:\")\n",
        "        print(traceback.format_exc())\n",
        "        generated_code_baseline = None  # Ensure None in case of error\n",
        "\n",
        "else:\n",
        "    # Print detailed error message\n",
        "    print(\"\\n[ERROR] Unable to perform Baseline generation.\")\n",
        "    error_msg = \"         Issue detected with:\"\n",
        "    if missing_vars:\n",
        "        error_msg += f\" Missing variables: {', '.join(missing_vars)}.\"\n",
        "    if invalid_vars:\n",
        "        error_msg += f\" Invalid variables (None/empty): {', '.join(invalid_vars)}.\"\n",
        "    print(error_msg)\n",
        "    print(\"         Make sure ALL previous cells (data/model loading, RAG generation) executed successfully.\")\n",
        "\n",
        "# --- 5. Final Verification ---\n",
        "if generated_code_baseline is not None:\n",
        "    print(\"\\n--- Baseline code generation completed ---\")\n",
        "else:\n",
        "    print(\"\\n--- [ERROR] Baseline code generation failed or was not executed ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLbYVy06SGS5"
      },
      "source": [
        "## Section 9 ¬∑ Metrics Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "6d05377810e04802811b90a10711ec1d",
            "d37b121195154e72afb28e28006ee1be",
            "574d953647334a57b23bd9b5b4907ddc",
            "01a75d803732460d812c391d29dde41e",
            "ed2c6e8df6be41ad9750a2cf3d8ec181",
            "b98c2affb9934762a2fd7ddb0c1cafeb",
            "4194dbc0198943ce8f5aa2794e38658c",
            "9262af1f37564099935068959af83a58",
            "e7257ba96ce6488f964dcdfecf2c9e56",
            "76a5797957124fa4ad3865c6978fd4b9",
            "64c3226a270547d0a401512216136a73"
          ]
        },
        "id": "dubtt5LHdYHJ",
        "outputId": "c5bb6616-99e4-4c99-84fb-0e243fc8fa9e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚è≥ generating:   0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d05377810e04802811b90a10711ec1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Built lists for 13 examples.\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "#  Build baseline_outputs, rag_outputs, references, reference_apis\n",
        "# ================================================================\n",
        "from rank_bm25 import BM25Okapi\n",
        "from tqdm.auto import tqdm\n",
        "import torch, textwrap\n",
        "\n",
        "# ----------------- 1. pick how many examples to run -------------\n",
        "NUM_EXAMPLES = len(lca_dataset_split)    # e.g. 10 for a quick test\n",
        "\n",
        "# ----------------- 2. BM25 over clean_reference -----------------\n",
        "corpus_texts  = [ex[\"clean_reference\"] for ex in lca_dataset_split]\n",
        "bm25          = BM25Okapi([t.split() for t in corpus_texts])\n",
        "\n",
        "# ----------------- 3. helper: deterministic generation ----------\n",
        "def generate_code(prompt, max_new_tokens=256):\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=2048,              # safety for long RAG prompts\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,          # greedy for reproducibility\n",
        "            temperature=0.0,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# ----------------- 4. main loop --------------------------------\n",
        "baseline_outputs, rag_outputs = [], []\n",
        "references, reference_apis    = [], []\n",
        "\n",
        "for idx, ex in enumerate(tqdm(lca_dataset_split.select(range(NUM_EXAMPLES)),\n",
        "                              desc=\"‚è≥ generating\")):\n",
        "    instr = ex[\"instruction\"]\n",
        "\n",
        "    # --- baseline ---\n",
        "    b_prompt = build_baseline_prompt(instr)\n",
        "    baseline_outputs.append(generate_code(b_prompt))\n",
        "\n",
        "    # --- retrieve top-k snippets (BM25) ---\n",
        "    query       = instr.split()\n",
        "    top_indices = bm25.get_top_n(query, list(range(len(corpus_texts))), n=5)\n",
        "    retrieved   = \"\\n\\n\".join(corpus_texts[i] for i in top_indices)\n",
        "\n",
        "    # --- RAG ---\n",
        "    r_prompt = build_rag_prompt(instr, retrieved)\n",
        "    rag_outputs.append(generate_code(r_prompt))\n",
        "\n",
        "    # --- store references & APIs as before ---\n",
        "    references.append(ex[\"clean_reference\"])\n",
        "    reference_apis.append(ex[\"unique_apis\"])\n",
        "\n",
        "# ----------------- 5. sanity check ------------------------------\n",
        "assert len({len(baseline_outputs), len(rag_outputs),\n",
        "            len(references), len(reference_apis)}) == 1, \"length mismatch!\"\n",
        "\n",
        "print(f\"\\n‚úÖ Built lists for {len(baseline_outputs)} examples.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# ================================================================\n",
        "#  Metrics helpers ‚Äì with CodeBLEU support & automatic key-detection\n",
        "# ================================================================\n",
        "import importlib, warnings, re\n",
        "from importlib.metadata import version as _get_version, PackageNotFoundError\n",
        "import numpy as np\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ sacrebleu ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "import sacrebleu\n",
        "print(\"‚úÖ sacrebleu\", sacrebleu.__version__)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ codebleu ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "try:\n",
        "    from codebleu import calc_codebleu\n",
        "    try:\n",
        "        cb_ver = _get_version(\"codebleu\")\n",
        "    except PackageNotFoundError:\n",
        "        cb_ver = \"n/a\"\n",
        "    print(\"‚úÖ codebleu\", cb_ver)\n",
        "    _HAS_CODEBLEU = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  codebleu import failed ‚Äî CodeBLEU will be skipped.\")\n",
        "    _HAS_CODEBLEU = False\n",
        "\n",
        "!pip install -q --upgrade tree_sitter tree_sitter_python\n",
        "!pip install -q git+https://github.com/k4black/codebleu.git\n",
        "# run this in a fresh cell *before* any CodeBLEU import\n",
        "!pip install -q --upgrade \"tree_sitter<0.23\" \"tree_sitter_python<0.23\"\n",
        "\"\"\"\n",
        "\n",
        "# ‚îÄ‚îÄ FIRST cell in the notebook ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "!pip uninstall -yq codebleu            # throw away 0.7.0\n",
        "!pip install -q --upgrade tree_sitter tree_sitter_python  # stays at 0.24+\n",
        "!pip install -q git+https://github.com/k4black/codebleu.git  # 0.7.1-dev\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOt-AJ_mHOWT",
        "outputId": "15fcd735-a5b4-442f-d5e0-4d3db2644a0b"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for codebleu (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [codebleu]\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL8_lv8BzkjL",
        "outputId": "00a2a57a-4016-4b79-dc36-11e4027d8ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now running CodeBLEU 0.7.1\n",
            "üîç CodeBLEU raw result keys: ['codebleu', 'ngram_match_score', 'weighted_ngram_match_score', 'syntax_match_score', 'dataflow_match_score']\n",
            "\n",
            "--- Risultati Metriche Automatiche ---\n",
            "| Metrica    | Baseline |   RAG   |\n",
            "|------------|----------|---------|\n",
            "| API Recall | 0.0000   | 0.0000   |\n",
            "| ChrF       | 23.09    | 53.15    |\n",
            "| CodeBLEU   | 0.16    | 0.43    |\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from codebleu import calc_codebleu\n",
        "import sacrebleu\n",
        "import importlib.metadata as md\n",
        "print(\"Now running CodeBLEU\", md.version(\"codebleu\"))\n",
        "_HAS_CODEBLEU = True\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "#  Composite key auto-detector (runs once on your first example)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "_codebleu_key = None\n",
        "def _detect_codebleu_key(sample_pred, sample_ref):\n",
        "    global _codebleu_key\n",
        "    if not _HAS_CODEBLEU:\n",
        "        return None\n",
        "    res = calc_codebleu(\n",
        "        references=[[sample_ref]],\n",
        "        predictions=[sample_pred],\n",
        "        lang=\"python\",\n",
        "        weights=(0.25,0.25,0.25,0.25)\n",
        "    )\n",
        "    print(\"üîç CodeBLEU raw result keys:\", list(res.keys()))\n",
        "    # pick the first key containing ‚Äúcodebleu‚Äù (case-insensitive)\n",
        "    for k in res:\n",
        "        if \"codebleu\" in k.lower():\n",
        "            _codebleu_key = k\n",
        "            break\n",
        "    return _codebleu_key\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "#  Metric functions\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def calculate_chrf(pred, ref):\n",
        "    if not (isinstance(pred, str) and isinstance(ref, str)):\n",
        "        return None\n",
        "    if not pred or not ref:\n",
        "        return 0.0\n",
        "    return sacrebleu.corpus_chrf([pred], [[ref]]).score\n",
        "\n",
        "def calculate_codebleu(pred, ref, lang=\"python\", weights=(0.25,0.25,0.25,0.25)):\n",
        "    if not _HAS_CODEBLEU:\n",
        "        return None\n",
        "    if not (isinstance(pred, str) and isinstance(ref, str)):\n",
        "        return None\n",
        "\n",
        "    # detect key on first call\n",
        "    global _codebleu_key\n",
        "    if _codebleu_key is None:\n",
        "        _detect_codebleu_key(pred, ref)\n",
        "        if _codebleu_key is None:\n",
        "            warnings.warn(\"Could not find a CodeBLEU key in the result; returning 0.0\")\n",
        "            return 0.0\n",
        "\n",
        "    try:\n",
        "        res = calc_codebleu(\n",
        "            references=[[ref]],\n",
        "            predictions=[pred],\n",
        "            lang=lang,\n",
        "            weights=weights\n",
        "        )\n",
        "        return float(res.get(_codebleu_key, 0.0))\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"CodeBLEU failed for one example: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_api_recall(gen, ref_apis):\n",
        "    if not isinstance(gen, str) or not isinstance(ref_apis, list):\n",
        "        return 0.0\n",
        "    valid = [api for api in ref_apis if isinstance(api, str) and api.strip()]\n",
        "    if not gen or not valid:\n",
        "        return 0.0\n",
        "    hits = sum(bool(re.search(rf\"\\b{re.escape(api)}\\b\", gen)) for api in valid)\n",
        "    return hits / len(valid)\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "#  Evaluation driver (baseline vs RAG)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def evaluate(baseline_preds, rag_preds, refs, ref_api_lists):\n",
        "    assert len(baseline_preds) == len(rag_preds) == len(refs) == len(ref_api_lists), \\\n",
        "        \"All four lists must have the same length!\"\n",
        "\n",
        "    def _mean(fn, preds):\n",
        "        vals = [fn(p, r) for p, r in zip(preds, refs)]\n",
        "        vals = [v for v in vals if v is not None]\n",
        "        return np.mean(vals) if vals else 0.0\n",
        "\n",
        "    recall_b = _mean(calculate_api_recall, baseline_preds)\n",
        "    recall_r = _mean(calculate_api_recall, rag_preds)\n",
        "    chrf_b   = _mean(calculate_chrf,           baseline_preds)\n",
        "    chrf_r   = _mean(calculate_chrf,           rag_preds)\n",
        "    cbleu_b  = _mean(calculate_codebleu,       baseline_preds)\n",
        "    cbleu_r  = _mean(calculate_codebleu,       rag_preds)\n",
        "\n",
        "    print(\"\\n--- Risultati Metriche Automatiche ---\")\n",
        "    print(f\"| Metrica    | Baseline |   RAG   |\")\n",
        "    print(f\"|------------|----------|---------|\")\n",
        "    print(f\"| API Recall | {recall_b:.4f}   | {recall_r:.4f}   |\")\n",
        "    print(f\"| ChrF       | {chrf_b:.2f}    | {chrf_r:.2f}    |\")\n",
        "    print(f\"| CodeBLEU   | {cbleu_b:.2f}    | {cbleu_r:.2f}    |\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    return {\n",
        "        \"recall\":   (recall_b,   recall_r),\n",
        "        \"chrf\":     (chrf_b,     chrf_r),\n",
        "        \"codebleu\": (cbleu_b,    cbleu_r),\n",
        "    }\n",
        "metrics = evaluate(baseline_outputs, rag_outputs, references, reference_apis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUw1sKpXe63X"
      },
      "source": [
        "### Prompt 1\n",
        "| Metrica    | Baseline |   RAG   |\n",
        "|------------|----------|---------|\n",
        "| API Recall | 0.0000   | 0.0000   |\n",
        "| ChrF       | 17.05    | 53.43    |\n",
        "| CodeBLEU   | 0.11    | 0.43    |\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### Prompt 2\n",
        "| Metrica    | Baseline |   RAG   |\n",
        "|------------|----------|---------|\n",
        "| API Recall | 0.0000   | 0.0000   |\n",
        "| ChrF       | 25.75    | 53.43    |\n",
        "| CodeBLEU   | 0.21    | 0.43    |\n",
        "\n",
        "---\n",
        "\n",
        "### Prompt 3\n",
        "| Metrica    | Baseline |   RAG   |\n",
        "|------------|----------|---------|\n",
        "| API Recall | 0.0000   | 0.0000   |\n",
        "| ChrF       | 23.09    | 53.15    |\n",
        "| CodeBLEU   | 0.16    | 0.43    |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzpG7Hxk7ox6",
        "outputId": "5fc08739-80f6-4530-cdae-cbb09095cd6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Recall was computed on 13 samples\n",
            "ChrF       was computed on 13 samples\n",
            "CodeBLEU   was computed on 13 samples\n"
          ]
        }
      ],
      "source": [
        "def count_valid(fn, preds, refs):\n",
        "    \"\"\"Count how many (pred, ref) pairs return a non‚ÄêNone metric.\"\"\"\n",
        "    return sum(1 for p, r in zip(preds, refs) if fn(p, r) is not None)\n",
        "\n",
        "# For API‚ÄêRecall we never return None, so it‚Äôs simply the full length:\n",
        "n_api = len(baseline_outputs)\n",
        "\n",
        "# For ChrF & CodeBLEU we drop any None‚Äôs:\n",
        "n_chrf     = count_valid(calculate_chrf,     baseline_outputs, references)\n",
        "n_codebleu = count_valid(calculate_codebleu, baseline_outputs, references)\n",
        "\n",
        "print(f\"API Recall was computed on {n_api} samples\")\n",
        "print(f\"ChrF       was computed on {n_chrf} samples\")\n",
        "print(f\"CodeBLEU   was computed on {n_codebleu} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmD0bqjg7rI0",
        "outputId": "593d1f3c-8bae-49db-c646-ea6081019be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(RAG) ChrF       on 13 samples\n",
            "(RAG) CodeBLEU   on 13 samples\n"
          ]
        }
      ],
      "source": [
        "n_chrf_rag     = count_valid(calculate_chrf,     rag_outputs, references)\n",
        "n_codebleu_rag = count_valid(calculate_codebleu, rag_outputs, references)\n",
        "print(f\"(RAG) ChrF       on {n_chrf_rag} samples\")\n",
        "print(f\"(RAG) CodeBLEU   on {n_codebleu_rag} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtIyq0dr_I5F"
      },
      "source": [
        "## Section 10 ¬∑ Example result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "a7Cc5Z8AHgE2"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Display a sample: task + reference + baseline + RAG side by side\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from IPython.display import display, Markdown\n",
        "import textwrap\n",
        "\n",
        "EXAMPLE_INDEX = 0  # Change this to any index within your dataset size\n",
        "\n",
        "task       = references[EXAMPLE_INDEX]            # Gold reference code (cleaned)\n",
        "baseline   = baseline_outputs[EXAMPLE_INDEX]      # Generated from instruction only\n",
        "rag        = rag_outputs[EXAMPLE_INDEX]           # Generated with RAG prompt\n",
        "instruction = lca_dataset_split[EXAMPLE_INDEX]['instruction']  # Original task (English)\n",
        "\n",
        "def print_block(title, content):\n",
        "    print(f\"{title}\")\n",
        "    print(\"-\" * 10)\n",
        "    print(textwrap.dedent(content).strip())\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjF1QZW4Hr-X",
        "outputId": "a874e1d1-4616-485c-b6c7-b49cf5eb0ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction\n",
            "========================================\n",
            "Generate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n",
            "\n",
            "The base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. \n",
            "\n",
            "The domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. \n",
            "\n",
            "The eBGP layer should add private peerings between different autonomous systems. \n",
            "\n",
            "Finally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file.\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Instruction\")\n",
        "print(\"=\" * 40)\n",
        "print(textwrap.dedent(instruction).strip())\n",
        "print(\"=\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOih6Zt-H0X8",
        "outputId": "ef3b1d42-81fc-45e1-df3f-5f691eaa5559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gold Reference\n",
            "========================================\n",
            "from seedemu.layers import Base, Routing, Ebgp, PeerRelationship, Ibgp, Ospf\n",
            "from seedemu.compiler import Docker\n",
            "from seedemu.services import DomainNameCachingService\n",
            "from seedemu.core import Emulator, Binding, Filter, Node\n",
            "from typing import List\n",
            "\n",
            "sim = Emulator()\n",
            "\n",
            "base = Base()\n",
            "routing = Routing()\n",
            "ebgp = Ebgp()\n",
            "ibgp = Ibgp()\n",
            "ospf = Ospf()\n",
            "ldns = DomainNameCachingService()\n",
            "\n",
            "def make_stub_as(asn: int, exchange: str):\n",
            "    stub_as = base.createAutonomousSystem(asn)\n",
            "    host = stub_as.createHost('host0')\n",
            "    host1 = stub_as.createHost('host1')\n",
            "    host2 = stub_as.createHost('host2')\n",
            "    host3 = stub_as.createHost('host3')\n",
            "    host4 = stub_as.createHost('host4')\n",
            "    host5 = stub_as.createHost('host5')\n",
            "    ldns_host = stub_as.createHost('ldns') \n",
            "\n",
            "    router = stub_as.createRouter('router0')\n",
            "    net = stub_as.createNetwork('net0')\n",
            "\n",
            "    router.joinNetwork('net0')\n",
            "    host.joinNetwork('net0')\n",
            "    host1.joinNetwork('net0')\n",
            "    host2.joinNetwork('net0')\n",
            "    host3.joinNetwork('net0')\n",
            "    host4.joinNetwork('net0')\n",
            "    host5.joinNetwork('net0')\n",
            "    ldns_host.joinNetwork('net0')\n",
            "\n",
            "    router.joinNetwork(exchange)\n",
            "\n",
            "ldns.install('local-dns-150').setConfigureResolvconf(True)\n",
            "ldns.install('local-dns-151').setConfigureResolvconf(True)\n",
            "ldns.install('local-dns-152').setConfigureResolvconf(True)\n",
            "ldns.install('local-dns-153').setConfigureResolvconf(True)\n",
            "ldns.install('local-dns-154').setConfigureResolvconf(True)\n",
            "ldns.install('local-dns-160').setConfigureResolvconf(True)\n",
            "ldns.install('local-dns-161').setConfigureResolvconf(True)\n",
            "\n",
            "sim.addBinding(Binding('local-dns-150', filter = Filter(asn=150, nodeName=\"ldns\")))\n",
            "sim.addBinding(Binding('local-dns-151', filter = Filter(asn=151, nodeName=\"ldns\")))\n",
            "sim.addBinding(Binding('local-dns-152', filter = Filter(asn=152, nodeName=\"ldns\")))\n",
            "sim.addBinding(Binding('local-dns-153', filter = Filter(asn=153, nodeName=\"ldns\")))\n",
            "sim.addBinding(Binding('local-dns-154', filter = Filter(asn=154, nodeName=\"ldns\")))\n",
            "sim.addBinding(Binding('local-dns-160', filter = Filter(asn=160, nodeName=\"ldns\")))\n",
            "sim.addBinding(Binding('local-dns-161', filter = Filter(asn=161, nodeName=\"ldns\")))\n",
            "\n",
            "base.createInternetExchange(100)\n",
            "base.createInternetExchange(101)\n",
            "base.createInternetExchange(102)\n",
            "\n",
            "make_stub_as(150, 'ix100')\n",
            "make_stub_as(151, 'ix100')\n",
            "\n",
            "make_stub_as(152, 'ix101')\n",
            "make_stub_as(153, 'ix101')\n",
            "make_stub_as(154, 'ix101')\n",
            "\n",
            "make_stub_as(160, 'ix102')\n",
            "make_stub_as(161, 'ix102')\n",
            "\n",
            "as2 = base.createAutonomousSystem(2)\n",
            "\n",
            "as2_100 = as2.createRouter('r0')\n",
            "as2_101 = as2.createRouter('r1')\n",
            "as2_102 = as2.createRouter('r2')\n",
            "\n",
            "as2_100.joinNetwork('ix100')\n",
            "as2_101.joinNetwork('ix101')\n",
            "as2_102.joinNetwork('ix102')\n",
            "\n",
            "as2_net_100_101 = as2.createNetwork('n01')\n",
            "as2_net_101_102 = as2.createNetwork('n12')\n",
            "as2_net_102_100 = as2.createNetwork('n20')\n",
            "\n",
            "as2_100.joinNetwork('n01')\n",
            "as2_101.joinNetwork('n01')\n",
            "\n",
            "as2_101.joinNetwork('n12')\n",
            "as2_102.joinNetwork('n12')\n",
            "\n",
            "as2_102.joinNetwork('n20')\n",
            "as2_100.joinNetwork('n20')\n",
            "\n",
            "as3 = base.createAutonomousSystem(3)\n",
            "\n",
            "as3_101 = as3.createRouter('r1')\n",
            "as3_102 = as3.createRouter('r2')\n",
            "\n",
            "as3_101.joinNetwork('ix101')\n",
            "as3_102.joinNetwork('ix102')\n",
            "\n",
            "as3_net_101_102 = as3.createNetwork('n12')\n",
            "\n",
            "as3_101.joinNetwork('n12')\n",
            "as3_102.joinNetwork('n12')\n",
            "\n",
            "ebgp.addPrivatePeering(100, 2, 150, PeerRelationship.Provider)\n",
            "ebgp.addPrivatePeering(100, 150, 151, PeerRelationship.Provider)\n",
            "\n",
            "ebgp.addPrivatePeering(101, 2, 3, PeerRelationship.Provider)\n",
            "ebgp.addPrivatePeering(101, 2, 152, PeerRelationship.Provider)\n",
            "ebgp.addPrivatePeering(101, 3, 152, PeerRelationship.Provider)\n",
            "ebgp.addPrivatePeering(101, 2, 153, PeerRelationship.Provider)\n",
            "ebgp.addPrivatePeering(101, 3, 153, PeerRelationship.Provider)\n",
            "ebgp.addPrivatePeering(101, 2, 154, PeerRelationship.Provider)\n",
            "ebgp.addPrivatePeering(101, 3, 154, PeerRelationship.Provider)\n",
            "\n",
            "ebgp.addPrivatePeering(102, 2, 160, PeerRelationship.Provider)\n",
            "ebgp.addPrivatePeering(102, 3, 160, PeerRelationship.Provider)\n",
            "ebgp.addPrivatePeering(102, 3, 161, PeerRelationship.Provider)\n",
            "\n",
            "sim.addLayer(base)\n",
            "sim.addLayer(routing)\n",
            "sim.addLayer(ebgp)\n",
            "sim.addLayer(ibgp)\n",
            "sim.addLayer(ospf)\n",
            "sim.addLayer(ldns)\n",
            "\n",
            "sim.dump('base-component.bin')\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "print(\"‚úÖ Gold Reference\")\n",
        "print(\"=\" * 40)\n",
        "print(textwrap.dedent(task).strip())\n",
        "print(\"=\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gcQYsQiH9jz",
        "outputId": "c6d3eeec-f219-4f3c-c9e3-c442bd50117e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Output\n",
            "========================================\n",
            "You are a senior Python engineer.  Fulfill the following task by writing production-ready code.\n",
            "\n",
            "**Task**:\n",
            "Generate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n",
            "\n",
            "The base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. \n",
            "\n",
            "The domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. \n",
            "\n",
            "The eBGP layer should add private peerings between different autonomous systems. \n",
            "\n",
            "Finally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file.\n",
            "\n",
            "**Requirements**:\n",
            "- Python 3, include type hints\n",
            "- One well-formed function or class with a descriptive name\n",
            "- A docstring (inputs, outputs, edge cases)\n",
            "- PEP8 style (4-space indent, snake_case)\n",
            "- At least one unit test using `assert` or `unittest`\n",
            "\n",
            "**Implementation**:\n",
            "```python\n",
            "import seedemu\n",
            "\n",
            "def main():\n",
            "    emu = seedemu.Emulator()\n",
            "    emu.add_layer(seedemu.BaseLayer())\n",
            "    emu.add_layer(seedemu.RoutingLayer())\n",
            "    emu.add_layer(seedemu.EBGPLayer())\n",
            "    emu.add_layer(seedemu.DNSLayer())\n",
            "    emu.dump_to_file(\"emu.bin\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "```\n",
            "\n",
            "**Output**:\n",
            "```\n",
            "$ python3 emu.py\n",
            "```\n",
            "\n",
            "**Expected output**:\n",
            "```\n",
            "$ python3 emu.py\n",
            "```\n",
            "\n",
            "**Notes**:\n",
            "- The emulator should be able to handle multiple autonomous systems and internet exchanges.\n",
            "- The emulator should be able to handle multiple hosts and routers within each autonomous system.\n",
            "- The emulator should be able to handle multiple bindings for each host and router.\n",
            "- The emulator should be able to handle multiple private peerings between different autonomous systems.\n",
            "- The emulator should be able\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Baseline Output\")\n",
        "print(\"=\" * 40)\n",
        "print(textwrap.dedent(baseline).strip())\n",
        "print(\"=\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CZr1Qr8IEdj",
        "outputId": "d0da7ac9-2302-4698-9069-031ac1bd6c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Output\n",
            "========================================\n",
            "You are a senior Python engineer.  Use the retrieved examples to guide your implementation.\n",
            "\n",
            "**Retrieved Examples**:\n",
            "from seedemu import *\n",
            "\n",
            "hosts_per_stub_as = 3\n",
            "emu = Makers.makeEmulatorBaseWith10StubASAndHosts(hosts_per_stub_as = hosts_per_stub_as)\n",
            "\n",
            "eth = EthereumService()\n",
            "\n",
            "blockchain = eth.createBlockchain(chainName=\"pos\", consensus=ConsensusMechanism.POS)\n",
            "\n",
            "blockchain.setTerminalTotalDifficulty(30)\n",
            "\n",
            "asns = [150, 151, 152, 153, 154, 160, 161, 162, 163, 164]\n",
            "\n",
            "i = 1\n",
            "for asn in asns:\n",
            "    for id in range(hosts_per_stub_as):        \n",
            "\n",
            "        e:EthereumServer = blockchain.createNode(\"eth{}\".format(i))   \n",
            "\n",
            "        e.appendClassName('Ethereum-POS-{}'.format(i))\n",
            "\n",
            "        e.enableGethHttp()\n",
            "\n",
            "        if asn == 150 and id == 0:\n",
            "                e.setBeaconSetupNode()\n",
            "\n",
            "        if asn == 150 and id == 1:\n",
            "                e.setBootNode(True)\n",
            "\n",
            "        if asn in [151]:\n",
            "            if id == 0:\n",
            "                e.enablePOSValidatorAtRunning()\n",
            "            if id == 1:\n",
            "                e.enablePOSValidatorAtRunning(is_manual=True)\n",
            "\n",
            "        if asn in [152,153,154,160,161,162,163,164]:\n",
            "            e.enablePOSValidatorAtGenesis()\n",
            "            e.startMiner()\n",
            "\n",
            "        if e.isBeaconSetupNode():\n",
            "            emu.getVirtualNode('eth{}'.format(i)).setDisplayName('Ethereum-BeaconSetup')\n",
            "        else:\n",
            "            emu.getVirtualNode('eth{}'.format(i)).setDisplayName('Ethereum-POS-{}'.format(i))\n",
            "\n",
            "        emu.addBinding(Binding('eth{}'.format(i), filter=Filter(asn=asn, nodeName='host_{}'.format(id))))\n",
            "\n",
            "        i = i+1\n",
            "\n",
            "emu.addLayer(eth)\n",
            "\n",
            "emu.render()\n",
            "\n",
            "docker = Docker(internetMapEnabled=True, etherViewEnabled=True)\n",
            "\n",
            "emu.compile(docker, './output', override = True)\n",
            "\n",
            "from seedemu.layers import Base, Routing, Ebgp\n",
            "from seedemu.services import WebService\n",
            "from seedemu.compiler import Docker\n",
            "from seedemu.core import Emulator, Binding, Filter\n",
            "\n",
            "emu = Emulator()\n",
            "base = Base()\n",
            "routing = Routing()\n",
            "ebgp = Ebgp()\n",
            "web = WebService()\n",
            "\n",
            "ix100 = base.createInternetExchange(100)\n",
            "\n",
            "ix100_net = ix100.getPeeringLan()\n",
            "ix100_net.setDisplayName('Seattle Internet Exchange')\n",
            "ix100_net.setDescription('The largest IX in Seattle.')\n",
            "\n",
            "as150 = base.createAutonomousSystem(150)\n",
            "\n",
            "net150 = as150.createNetwork('net0')\n",
            "\n",
            "net150.setDisplayName('AS150 Backbone')\n",
            "net150.setDescription('This is the main network of AS150.')\n",
            "\n",
            "as150_router = as150.createRouter('router0')\n",
            "as150_router.joinNetwork('net0')\n",
            "as150_router.joinNetwork('ix100')\n",
            "\n",
            "as150_router.setDisplayName('AS150 Core Router')\n",
            "as150_router.setDescription('The core router of AS150.')\n",
            "\n",
            "as150.createHost('web').joinNetwork('net0')\n",
            "\n",
            "web.install('web150')\n",
            "\n",
            "emu.addBinding(Binding('web150', filter = Filter(nodeName = 'web', asn = 150)))\n",
            "\n",
            "as151 = base.createAutonomousSystem(151)\n",
            "as151.createNetwork('net0')\n",
            "\n",
            "as151.createHost('web').joinNetwork('net0')\n",
            "web.install('web151')\n",
            "emu.addBinding(Binding('web151', filter = Filter(nodeName = 'web', asn = 151)))\n",
            "\n",
            "as151_router = as151.createRouter('router0')\n",
            "as151_router.joinNetwork('net0')\n",
            "as151_router.joinNetwork('ix100')\n",
            "\n",
            "as152 = base.createAutonomousSystem(152)\n",
            "as152.createNetwork('net0')\n",
            "\n",
            "as152.createHost('web').joinNetwork('net0')\n",
            "web.install('web152')\n",
            "emu.addBinding(Binding('web152', filter = Filter(nodeName = 'web', asn = 152)))\n",
            "\n",
            "as152_router = as152.createRouter('router0')\n",
            "as152_router.joinNetwork('net0')\n",
            "as152_router.joinNetwork('ix100')\n",
            "\n",
            "ebgp.addRsPeer(100, 150)\n",
            "ebgp.addRsPeer(100, 151)\n",
            "ebgp.addRsPeer(100, 152)\n",
            "\n",
            "emu.addLayer(base)\n",
            "emu.addLayer(routing)\n",
            "emu.addLayer(ebgp)\n",
            "emu.addLayer(web)\n",
            "\n",
            "emu.render()\n",
            "\n",
            "emu.compile(Docker(internetMapEnabled = True), './output')\n",
            "\n",
            "\n",
            "from seedemu.layers import Base, Routing, Ebgp, Ibgp, Ospf, PeerRelationship, Dnssec\n",
            "from seedemu.services import WebService, DomainNameService, DomainNameCachingService\n",
            "from seedemu.services import CymruIpOriginService, ReverseDomainNameService, BgpLookingGlassService\n",
            "from seedemu.compiler import Docker, Graphviz\n",
            "from seedemu.hooks import ResolvConfHook\n",
            "from seedemu.core import Emulator, Service, Binding, Filter\n",
            "from seedemu.layers import Router\n",
            "from seedemu.raps import OpenVpnRemoteAccessProvider\n",
            "from seedemu.utilities import Makers\n",
            "\n",
            "from typing import List, Tuple, Dict\n",
            "\n",
            "emu     = Emulator()\n",
            "base    = Base()\n",
            "routing = Routing()\n",
            "ebgp    = Ebgp()\n",
            "ibgp    = Ibgp()\n",
            "ospf    = Ospf()\n",
            "web     = WebService()\n",
            "ovpn    = OpenVpnRemoteAccessProvider()\n",
            "\n",
            "ix100 = base.createInternetExchange(100)\n",
            "ix101 = base.createInternetExchange(101)\n",
            "ix102 = base.createInternetExchange(102)\n",
            "ix103 = base.createInternetExchange(103)\n",
            "ix104 = base.createInternetExchange(104)\n",
            "ix105 = base.createInternetExchange(105)\n",
            "\n",
            "ix100.getPeeringLan().setDisplayName('NYC-100')\n",
            "ix101.getPeeringLan().setDisplayName('San Jose-101')\n",
            "ix102.getPeeringLan().setDisplayName('Chicago-102')\n",
            "ix103.getPeeringLan().setDisplayName('Miami-103')\n",
            "ix104.getPeeringLan().setDisplayName('Boston-104')\n",
            "ix105.getPeeringLan().setDisplayName('Huston-105')\n",
            "\n",
            "Makers.makeTransitAs(base, 2, [100, 101, 102, 105], \n",
            "       [(100, 101), (101, 102), (100, 105)] \n",
            ")\n",
            "\n",
            "Makers.makeTransitAs(base, 3, [100, 103, 104, 105], \n",
            "       [(100, 103), (100, 105), (103, 104)] \n",
            ")\n",
            "\n",
            "Makers.makeTransitAs(base, 4, [100, 102, 104, 105], \n",
            "       [(100, 102), (100, 104), (102, 104)] \n",
            ")\n",
            "\n",
            "Makers.makeTransitAs(base, 5, [100, 101, 103, 104, 105], \n",
            "       [(100, 101), (100, 103), (100, 104), (101, 103), (101, 104), (103, 104)] \n",
            ")\n",
            "\n",
            "Makers.makeTransitAs(base, 6, [100, 101, 102, 103, 104, 105], \n",
            "       [(100, 101), (100, 102\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "print(\"RAG Output\")\n",
        "print(\"=\" * 40)\n",
        "print(textwrap.dedent(rag).strip())\n",
        "print(\"=\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AvxDu9lcz5Dq"
      },
      "execution_count": 107,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4824b141d69d438dbf5dfdeba349d747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_822f4390832342a1a77c78d3db73fa21",
              "IPY_MODEL_6dbd49315715471e8b6f41b0a33d7d4e",
              "IPY_MODEL_e88fcc96fe6741d7ad9891b446dea18e"
            ],
            "layout": "IPY_MODEL_9ae23b3cd4cc4951b60eec28a69208fa"
          }
        },
        "822f4390832342a1a77c78d3db73fa21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f61662cd58704d11a07db37e83689fe2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_567a978197874f11a7c3ab7277acfd4d",
            "value": "seed-labs__seed-emulator.tar.gz:‚Äá100%"
          }
        },
        "6dbd49315715471e8b6f41b0a33d7d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e022ab0e2c64bc1bc4a76999718de3f",
            "max": 23957580,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2dd44bd42e3f44e396eec80ad9374a48",
            "value": 23957580
          }
        },
        "e88fcc96fe6741d7ad9891b446dea18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feac936855304fafb392dfc49f595033",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_510720d518cf40f88c80f5308a810f75",
            "value": "‚Äá24.0M/24.0M‚Äá[00:00&lt;00:00,‚Äá72.6MB/s]"
          }
        },
        "9ae23b3cd4cc4951b60eec28a69208fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f61662cd58704d11a07db37e83689fe2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "567a978197874f11a7c3ab7277acfd4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e022ab0e2c64bc1bc4a76999718de3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dd44bd42e3f44e396eec80ad9374a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "feac936855304fafb392dfc49f595033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "510720d518cf40f88c80f5308a810f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96c0ebbad6154d0698fff23626d10c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9de4056d801412e849d4e3b8010ad14",
              "IPY_MODEL_37fe771ad17746afb0fa82639bd52a3f",
              "IPY_MODEL_7e18215361d74b288b625bc0edb7b102"
            ],
            "layout": "IPY_MODEL_6ed7e78b71184b5ea01839b9a8536347"
          }
        },
        "c9de4056d801412e849d4e3b8010ad14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57575822faf34ab2a14cb1ba571eee3e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dc339416ef9d46cf91517094bac19a19",
            "value": "Extracting‚ÄáSnippets:‚Äá100%"
          }
        },
        "37fe771ad17746afb0fa82639bd52a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb8a78c6ac4f4cd9b4fd426bff7c72af",
            "max": 136,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd08f9d98d3442f6a2e0ece9723f1d65",
            "value": 136
          }
        },
        "7e18215361d74b288b625bc0edb7b102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a584aa2b9791470391632a3cd1f83d58",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6a073cf121d843bb8701f59e9c39b752",
            "value": "‚Äá136/136‚Äá[00:06&lt;00:00,‚Äá‚Äá8.80file/s]"
          }
        },
        "6ed7e78b71184b5ea01839b9a8536347": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57575822faf34ab2a14cb1ba571eee3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc339416ef9d46cf91517094bac19a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb8a78c6ac4f4cd9b4fd426bff7c72af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd08f9d98d3442f6a2e0ece9723f1d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a584aa2b9791470391632a3cd1f83d58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a073cf121d843bb8701f59e9c39b752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d05377810e04802811b90a10711ec1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d37b121195154e72afb28e28006ee1be",
              "IPY_MODEL_574d953647334a57b23bd9b5b4907ddc",
              "IPY_MODEL_01a75d803732460d812c391d29dde41e"
            ],
            "layout": "IPY_MODEL_ed2c6e8df6be41ad9750a2cf3d8ec181"
          }
        },
        "d37b121195154e72afb28e28006ee1be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b98c2affb9934762a2fd7ddb0c1cafeb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4194dbc0198943ce8f5aa2794e38658c",
            "value": "‚è≥‚Äágenerating:‚Äá100%"
          }
        },
        "574d953647334a57b23bd9b5b4907ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9262af1f37564099935068959af83a58",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7257ba96ce6488f964dcdfecf2c9e56",
            "value": 13
          }
        },
        "01a75d803732460d812c391d29dde41e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76a5797957124fa4ad3865c6978fd4b9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_64c3226a270547d0a401512216136a73",
            "value": "‚Äá13/13‚Äá[06:01&lt;00:00,‚Äá27.30s/it]"
          }
        },
        "ed2c6e8df6be41ad9750a2cf3d8ec181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b98c2affb9934762a2fd7ddb0c1cafeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4194dbc0198943ce8f5aa2794e38658c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9262af1f37564099935068959af83a58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7257ba96ce6488f964dcdfecf2c9e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76a5797957124fa4ad3865c6978fd4b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64c3226a270547d0a401512216136a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}