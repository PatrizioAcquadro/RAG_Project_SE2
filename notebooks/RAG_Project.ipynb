{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d8a2413391d44e109184c188e4a15a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4da4040644474b91822dc4726155214d",
              "IPY_MODEL_bbb5f19adefc4e3f94ae7c84e463ef55",
              "IPY_MODEL_9e2b091b983e4367a7272e9069a3215c"
            ],
            "layout": "IPY_MODEL_a383bae6e426452e8b57e3baa49879ab"
          }
        },
        "4da4040644474b91822dc4726155214d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cebf5fa12f34d818d7ba518f3602044",
            "placeholder": "​",
            "style": "IPY_MODEL_4d1b8e433ef74cbbad6476047ea229fa",
            "value": "seed-labs__seed-emulator.tar.gz: 100%"
          }
        },
        "bbb5f19adefc4e3f94ae7c84e463ef55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b70b81d4e5534a20b9dc94513340af79",
            "max": 23957580,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_499f62574e254dd3bc155f02c56af60f",
            "value": 23957580
          }
        },
        "9e2b091b983e4367a7272e9069a3215c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4921f1925878492c9e776825bb7031cf",
            "placeholder": "​",
            "style": "IPY_MODEL_77d917ce675c4d43b2b3c2ca4e7255ad",
            "value": " 24.0M/24.0M [00:00&lt;00:00, 46.9MB/s]"
          }
        },
        "a383bae6e426452e8b57e3baa49879ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cebf5fa12f34d818d7ba518f3602044": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d1b8e433ef74cbbad6476047ea229fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b70b81d4e5534a20b9dc94513340af79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "499f62574e254dd3bc155f02c56af60f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4921f1925878492c9e776825bb7031cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d917ce675c4d43b2b3c2ca4e7255ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8aa2e7f4020546cb9a65539f179aff85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba0638061e4747dbb4b94c3dcb64b018",
              "IPY_MODEL_98908a3ac174498bb92773964c9e8709",
              "IPY_MODEL_0b78173b54174306a1fc718af16fbda0"
            ],
            "layout": "IPY_MODEL_e999e5a196794931b14c44e0166e875c"
          }
        },
        "ba0638061e4747dbb4b94c3dcb64b018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fff6f7f6848940fd848d9ad10df46a91",
            "placeholder": "​",
            "style": "IPY_MODEL_6174bbcf917045bfaf1f06e0501fc07f",
            "value": "Extracting Snippets: 100%"
          }
        },
        "98908a3ac174498bb92773964c9e8709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50ff854341f94fff88f4d0be7efd4fc6",
            "max": 136,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34a208afc77e4f9f991a0867a1edabf8",
            "value": 136
          }
        },
        "0b78173b54174306a1fc718af16fbda0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7909630aabcb42af95d90c0a84bf8b2f",
            "placeholder": "​",
            "style": "IPY_MODEL_b24ca3fca3e1411087165fc8694cbdbb",
            "value": " 136/136 [00:02&lt;00:00, 68.84file/s]"
          }
        },
        "e999e5a196794931b14c44e0166e875c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fff6f7f6848940fd848d9ad10df46a91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6174bbcf917045bfaf1f06e0501fc07f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50ff854341f94fff88f4d0be7efd4fc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34a208afc77e4f9f991a0867a1edabf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7909630aabcb42af95d90c0a84bf8b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b24ca3fca3e1411087165fc8694cbdbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b65132b35c5c4e60bca6772786c6b213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45de9a7ec0964f4eae38d335846c41a4",
              "IPY_MODEL_0ddacbe15c4e438ba52cb9788c167120",
              "IPY_MODEL_bfe0de564ff640fead8e8e1ede427bc6"
            ],
            "layout": "IPY_MODEL_010ec30ce0494803bd910372b38f70a7"
          }
        },
        "45de9a7ec0964f4eae38d335846c41a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_614a773a599b468998ac80c563f542d2",
            "placeholder": "​",
            "style": "IPY_MODEL_53d8a7917a2948d892a512dda0dad25d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "0ddacbe15c4e438ba52cb9788c167120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95b96d93748e49dba6198d9bd2383a91",
            "max": 793,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f20e7d8a2fa94bc9a2dc006c59935d02",
            "value": 793
          }
        },
        "bfe0de564ff640fead8e8e1ede427bc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32c3cc88e6d54c47b570f1cb6ad2ba1d",
            "placeholder": "​",
            "style": "IPY_MODEL_5b60b5a7dc1a4370998b90efa5c750de",
            "value": " 793/793 [00:00&lt;00:00, 20.9kB/s]"
          }
        },
        "010ec30ce0494803bd910372b38f70a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "614a773a599b468998ac80c563f542d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53d8a7917a2948d892a512dda0dad25d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95b96d93748e49dba6198d9bd2383a91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f20e7d8a2fa94bc9a2dc006c59935d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32c3cc88e6d54c47b570f1cb6ad2ba1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b60b5a7dc1a4370998b90efa5c750de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07fef6eb899f48f4a008ed4b93567fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ac750cd48f94a8cbb5747436559c485",
              "IPY_MODEL_2be86e6d84a24afe9d5e8cd273d4c187",
              "IPY_MODEL_e52e10e4e75749a499442843ae131bf0"
            ],
            "layout": "IPY_MODEL_d67e2447dacd44a9818a23f991205979"
          }
        },
        "0ac750cd48f94a8cbb5747436559c485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cc1e7689cff4c6eb3b7a562d1fb3a43",
            "placeholder": "​",
            "style": "IPY_MODEL_608f53317e5d4ac2a975c72a6f01dec4",
            "value": "tokenizer.json: 100%"
          }
        },
        "2be86e6d84a24afe9d5e8cd273d4c187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adbefc240a1049f5b48e63577793b20b",
            "max": 1367962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88bc8087b7c64e7ba19aa3f8644c0121",
            "value": 1367962
          }
        },
        "e52e10e4e75749a499442843ae131bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_674f4e8007cd45879327d666141cceb2",
            "placeholder": "​",
            "style": "IPY_MODEL_ddd7af1f1b83449782284f8f751aeea8",
            "value": " 1.37M/1.37M [00:00&lt;00:00, 10.4MB/s]"
          }
        },
        "d67e2447dacd44a9818a23f991205979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc1e7689cff4c6eb3b7a562d1fb3a43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608f53317e5d4ac2a975c72a6f01dec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adbefc240a1049f5b48e63577793b20b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88bc8087b7c64e7ba19aa3f8644c0121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "674f4e8007cd45879327d666141cceb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddd7af1f1b83449782284f8f751aeea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8af5d87383214ec08a239913e9fd08af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c0a5a6080834147aa8df4fcfef02287",
              "IPY_MODEL_2bb9e3dc577f44c986b665c0dfd9974c",
              "IPY_MODEL_5d81744741d2460e9d545c8633305e96"
            ],
            "layout": "IPY_MODEL_0db05a96bd274ea6b2e21fe528eb4c96"
          }
        },
        "5c0a5a6080834147aa8df4fcfef02287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32b6e2df722b49b098f0f2e7ebcaa1e2",
            "placeholder": "​",
            "style": "IPY_MODEL_77642ebaa035478a9485ba3729ec1ab6",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "2bb9e3dc577f44c986b665c0dfd9974c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce4b65530f344d45a3e8b20a03407367",
            "max": 482,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f92e511f6d93453ab486eefcb34b91aa",
            "value": 482
          }
        },
        "5d81744741d2460e9d545c8633305e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36ab4f7d130c46ea8e71e25ce4308cdb",
            "placeholder": "​",
            "style": "IPY_MODEL_d0eeaa7cf5004319a11f9bd7ab691b0c",
            "value": " 482/482 [00:00&lt;00:00, 12.3kB/s]"
          }
        },
        "0db05a96bd274ea6b2e21fe528eb4c96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32b6e2df722b49b098f0f2e7ebcaa1e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77642ebaa035478a9485ba3729ec1ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce4b65530f344d45a3e8b20a03407367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f92e511f6d93453ab486eefcb34b91aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36ab4f7d130c46ea8e71e25ce4308cdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0eeaa7cf5004319a11f9bd7ab691b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99545a3a876b46849d4b12e423a02c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b028bbad627d4ef3a7d0ce9526c0e70c",
              "IPY_MODEL_16bad7ee1e3943db877f1b28aed3349d",
              "IPY_MODEL_7d3c7dd9b0414c35ade29ee494db462f"
            ],
            "layout": "IPY_MODEL_25bd71decdec4c968d871c5e1addccbf"
          }
        },
        "b028bbad627d4ef3a7d0ce9526c0e70c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81b43653571d4f1eb0227875a44e2d3e",
            "placeholder": "​",
            "style": "IPY_MODEL_b3fcba7e35d441ab8d61003eac2a2609",
            "value": "config.json: 100%"
          }
        },
        "16bad7ee1e3943db877f1b28aed3349d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d884adbd4cd4582a5e2b77d22cafeda",
            "max": 631,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c311aba998442fb83f9929c910ddf89",
            "value": 631
          }
        },
        "7d3c7dd9b0414c35ade29ee494db462f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_803c80a7d13241e195e7e2df90a1e46a",
            "placeholder": "​",
            "style": "IPY_MODEL_e5a04ac1f0bd49e39138ce9770956733",
            "value": " 631/631 [00:00&lt;00:00, 57.6kB/s]"
          }
        },
        "25bd71decdec4c968d871c5e1addccbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81b43653571d4f1eb0227875a44e2d3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3fcba7e35d441ab8d61003eac2a2609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d884adbd4cd4582a5e2b77d22cafeda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c311aba998442fb83f9929c910ddf89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "803c80a7d13241e195e7e2df90a1e46a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a04ac1f0bd49e39138ce9770956733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3acb751bade4cb2b798a31556e6af5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c3b7883f68b4ca79703e8c6e40abfc2",
              "IPY_MODEL_48a92d66e6cb45f18decdd2e7d8b7af9",
              "IPY_MODEL_33d8343a0d03401ca01c857ab64ff54b"
            ],
            "layout": "IPY_MODEL_48c4bdd6310a46efa53be6ce551d8445"
          }
        },
        "4c3b7883f68b4ca79703e8c6e40abfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6dcc2d472aa4957ab61a8793b9af395",
            "placeholder": "​",
            "style": "IPY_MODEL_96449a24c237466db6486339d2822d1b",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "48a92d66e6cb45f18decdd2e7d8b7af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d120a68986634d8b997dcbc0c2c30ae2",
            "max": 2693014838,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb26b7a1d72241dea7792724aea898db",
            "value": 2693014838
          }
        },
        "33d8343a0d03401ca01c857ab64ff54b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_316f40ef94d940cdaab9a06bd5d260b9",
            "placeholder": "​",
            "style": "IPY_MODEL_497cc89edd7b40c3a7dc9e6d2e4ef42f",
            "value": " 2.69G/2.69G [00:11&lt;00:00, 230MB/s]"
          }
        },
        "48c4bdd6310a46efa53be6ce551d8445": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6dcc2d472aa4957ab61a8793b9af395": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96449a24c237466db6486339d2822d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d120a68986634d8b997dcbc0c2c30ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb26b7a1d72241dea7792724aea898db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "316f40ef94d940cdaab9a06bd5d260b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "497cc89edd7b40c3a7dc9e6d2e4ef42f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdccfbb69d89413f911cd966a0748d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_061f974c5ea84079a1ee5c28e4e44d58",
              "IPY_MODEL_9fbc08862df943dfad9001e374ccc614",
              "IPY_MODEL_eec3dd9e9c904d8e9ec8ce14d2bb61b6"
            ],
            "layout": "IPY_MODEL_68a63741056f4e4db7924bcf0883a2c3"
          }
        },
        "061f974c5ea84079a1ee5c28e4e44d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee1a94537bb640a1a4d28ec1beefef2d",
            "placeholder": "​",
            "style": "IPY_MODEL_520db47e15b14bb794425607be0b84c0",
            "value": "model.safetensors: 100%"
          }
        },
        "9fbc08862df943dfad9001e374ccc614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_590912a3ec5241ffa258cae5352a8083",
            "max": 2692969128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d51d51b6d9b445cb9c470db94af26ce2",
            "value": 2692969128
          }
        },
        "eec3dd9e9c904d8e9ec8ce14d2bb61b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0c1096475334d7abaae829ca6d75805",
            "placeholder": "​",
            "style": "IPY_MODEL_f597bc9d28f24659b4e2e97adeda1499",
            "value": " 2.69G/2.69G [00:19&lt;00:00, 247MB/s]"
          }
        },
        "68a63741056f4e4db7924bcf0883a2c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee1a94537bb640a1a4d28ec1beefef2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "520db47e15b14bb794425607be0b84c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "590912a3ec5241ffa258cae5352a8083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d51d51b6d9b445cb9c470db94af26ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0c1096475334d7abaae829ca6d75805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f597bc9d28f24659b4e2e97adeda1499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82ff1618bb274f9ab5b29ed53eea4e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f98be5e3cb624fa2b0f71ca40f5b7719",
              "IPY_MODEL_69a35979672948dab37ab105f787b350",
              "IPY_MODEL_b89677a9d1424961b98f7750c404f8eb"
            ],
            "layout": "IPY_MODEL_885bd1f13d8040448928a3424a63c268"
          }
        },
        "f98be5e3cb624fa2b0f71ca40f5b7719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b13cfb8bc90c4dc8aabb06239bffec7c",
            "placeholder": "​",
            "style": "IPY_MODEL_e46021dcc9474a9bb8068f51f87bcbda",
            "value": "generation_config.json: 100%"
          }
        },
        "69a35979672948dab37ab105f787b350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_708f0c78f27d480f8dcdbb3188f6b881",
            "max": 119,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8f15df1528142a98c9afdcbbd0276f9",
            "value": 119
          }
        },
        "b89677a9d1424961b98f7750c404f8eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6a5326a18b54b11a2dbf7299055829b",
            "placeholder": "​",
            "style": "IPY_MODEL_a0269ed8ae47429ca3ccc96cb5d14302",
            "value": " 119/119 [00:00&lt;00:00, 4.83kB/s]"
          }
        },
        "885bd1f13d8040448928a3424a63c268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b13cfb8bc90c4dc8aabb06239bffec7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e46021dcc9474a9bb8068f51f87bcbda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "708f0c78f27d480f8dcdbb3188f6b881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8f15df1528142a98c9afdcbbd0276f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6a5326a18b54b11a2dbf7299055829b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0269ed8ae47429ca3ccc96cb5d14302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91760f94964a42028e41c959dd98f464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8f5b23d9de2471ba212544419bca194",
              "IPY_MODEL_932a593e38094907a4b8d2f83440f195",
              "IPY_MODEL_66c04e29039247eaa7154979a7040226"
            ],
            "layout": "IPY_MODEL_67e185b505544f598cc5d5ee3310e4ec"
          }
        },
        "c8f5b23d9de2471ba212544419bca194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e592da5973b44698343ad2970fb19a1",
            "placeholder": "​",
            "style": "IPY_MODEL_774591b24b2a43dba5603ad67e9686ec",
            "value": "README.md: 100%"
          }
        },
        "932a593e38094907a4b8d2f83440f195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f046f1d079a24722aa99d51938a377f4",
            "max": 5212,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_796b3e6517074b3d92d10dce44a42e3c",
            "value": 5212
          }
        },
        "66c04e29039247eaa7154979a7040226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5b14d9f4fc24d9e95b0b7d9cc01b166",
            "placeholder": "​",
            "style": "IPY_MODEL_9127c95b49a345b78bb9843cc14da257",
            "value": " 5.21k/5.21k [00:00&lt;00:00, 287kB/s]"
          }
        },
        "67e185b505544f598cc5d5ee3310e4ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e592da5973b44698343ad2970fb19a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "774591b24b2a43dba5603ad67e9686ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f046f1d079a24722aa99d51938a377f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "796b3e6517074b3d92d10dce44a42e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5b14d9f4fc24d9e95b0b7d9cc01b166": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9127c95b49a345b78bb9843cc14da257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31a8f8bd477241d0b67885ada835648a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab94c3b5204f408fb585e9c40b96af6a",
              "IPY_MODEL_ceb49e9f4f3e490697aadaba1e7a5fc5",
              "IPY_MODEL_567a8effd7f0402b9cbec4f21404b165"
            ],
            "layout": "IPY_MODEL_cd10f5c9537e40ba9b96c5d12233cc01"
          }
        },
        "ab94c3b5204f408fb585e9c40b96af6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95d70551a54c43009bcda2931364f823",
            "placeholder": "​",
            "style": "IPY_MODEL_1783b6d6e2634f3e99276876c7bae180",
            "value": "(…)-00000-of-00001-518ed46ecbe35ff9.parquet: 100%"
          }
        },
        "ceb49e9f4f3e490697aadaba1e7a5fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_632253fd43c24d249ff9f83bb9051898",
            "max": 4577126,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_492079920e9e43b983825e2e1a41df69",
            "value": 4577126
          }
        },
        "567a8effd7f0402b9cbec4f21404b165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4560dc3f81d44e4c891133b62a144810",
            "placeholder": "​",
            "style": "IPY_MODEL_3f257f3efc7744f7b6577e1fd405dbd9",
            "value": " 4.58M/4.58M [00:00&lt;00:00, 19.0MB/s]"
          }
        },
        "cd10f5c9537e40ba9b96c5d12233cc01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95d70551a54c43009bcda2931364f823": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1783b6d6e2634f3e99276876c7bae180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "632253fd43c24d249ff9f83bb9051898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "492079920e9e43b983825e2e1a41df69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4560dc3f81d44e4c891133b62a144810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f257f3efc7744f7b6577e1fd405dbd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f9cc57a886847abbfdc6c9da56ecdcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5cfb4d79d4644daca79380d551914946",
              "IPY_MODEL_e87cb4112f8b46829623fc8b3e6cd380",
              "IPY_MODEL_0a7cdc8561024c468f2d8e07e831fbf0"
            ],
            "layout": "IPY_MODEL_5ea1cb1133da4458ae7f2f6f0aa4e71a"
          }
        },
        "5cfb4d79d4644daca79380d551914946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60fa54ef0237416b91d8dd99c490f686",
            "placeholder": "​",
            "style": "IPY_MODEL_b4ce812c26fa4ad89ccb3e3bd4d7e764",
            "value": "Generating test split: 100%"
          }
        },
        "e87cb4112f8b46829623fc8b3e6cd380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a69af8d1f3f84b89b25cd14153e9a6d4",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7aad4098b4b487085633284d8f35bc3",
            "value": 150
          }
        },
        "0a7cdc8561024c468f2d8e07e831fbf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_365d4da2d580402a9e4b3cccb651ac2e",
            "placeholder": "​",
            "style": "IPY_MODEL_6d19fee5a9664ecd9214989039e234a2",
            "value": " 150/150 [00:00&lt;00:00, 565.17 examples/s]"
          }
        },
        "5ea1cb1133da4458ae7f2f6f0aa4e71a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60fa54ef0237416b91d8dd99c490f686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4ce812c26fa4ad89ccb3e3bd4d7e764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a69af8d1f3f84b89b25cd14153e9a6d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7aad4098b4b487085633284d8f35bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "365d4da2d580402a9e4b3cccb651ac2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d19fee5a9664ecd9214989039e234a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Overview – Retrieval‑Augmented Code Generation\n"
      ],
      "metadata": {
        "id": "SEfVOKdbeJFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Inspiration**  \n",
        "> This notebook re-implements the three-stage **Retrieval-Augmented Generation (RAG)** workflow proposed in  \n",
        "> *“An Empirical Study of Retrieval-Augmented Code Generation: Challenges and Opportunities”* (Yang et al., 2025).  \n",
        "> By feeding an LLM with code snippets retrieved on-the-fly from a project-specific knowledge base, it narrows the\n",
        "> semantic gap between natural-language instructions and the final source code.\n",
        "\n",
        "\n",
        "\n",
        "Three-Phase Design (exactly as in the paper)\n",
        "\n",
        "| Phase | One-line description (what the notebook does) | Paper counterpart |\n",
        "|-------|-----------------------------------------------|-------------------|\n",
        "| **Retrieval** | Build a snippet KB from repo sources → index with **BM25** → fetch Top-*k* snippets for each user query. | Comparative retriever study; BM25 emerges as the simplest & strongest. |\n",
        "| **Fusion** | Pack snippets + instruction into a **Snippet-Integration Format** (SIF); other fusion modes (Sample Expansion, Vectorised Decoding, Sketch Filling) are optional. | Evaluation of four fusion strategies; SIF ≅ *Sequential Integration Fusion* (best trade-off). |\n",
        "| **Generation** | Run a 4-bit quantised LLM (DeepSeek R1 / CodeGen / UniXcoder / CodeT5) with shared decoding settings and a custom stop-rule (EOS or closing ``` block). | Measure how retrieved context boosts vanilla pre-trained models. |\n",
        "\n",
        "---\n",
        "\n",
        "Walk-Through of Notebook Sections\n",
        "\n",
        "| # | Colab section | What happens in **one sentence** |\n",
        "|---|---------------|----------------------------------|\n",
        "| 0 | **Google Drive Mounting** | Mounts Drive to save models, KBs and results persistently. |\n",
        "| 1 | **Environment Setup** | Installs required libraries (transformers, datasets, BM25, CodeBLEU, etc.). |\n",
        "| 2 | **LLM + Tokenizer Loading (4-bit)** | Downloads and quantises the chosen model to fit GPU memory. |\n",
        "| 3 | **Dataset Preparation & Validation** | Loads the LCA test split and prints sanity-check stats. |\n",
        "| 4 | **Repository Download & Prep** | Retrieves the library archive from HF Hub and extracts it in /content. |\n",
        "| 5 | **Source Extraction → KB Build** | Parses every .py file, extracts functions/classes, stores ≤ 15 k snippets. |\n",
        "| 6 | **BM25 Retrieval & Prompt Assembly** | Tokenises KB + query, ranks snippets, and assembles the SIF prompt. |\n",
        "| 7 | **RAG Code Generation & Post-processing** | Feeds the SIF prompt to the LLM, decodes output, cleans first ```python``` block. |\n",
        "| 8 | **Baseline Generation & Comparison** | Generates code with the *same* LLM but **without** snippets; stores both outputs. |\n",
        "| 9 | **Metric Calculation** | Computes BLEU, CodeBLEU, ChrF, API-Recall, Edit-Distance; logs wins / losses. |\n",
        "\n",
        " Alignment with the Paper\n",
        "\n",
        "* **BM25** is used as default retriever – matching the empirical winner reported by the authors.  \n",
        "* **Sequential Integration Fusion** (SIF) is the primary fusion baseline.  \n",
        "* **Metrics** (BLEU, CodeBLEU, ChrF, structural & API measures) mirror those in the study.  \n",
        "* **Failure logging** reproduces the paper’s “Direction 2” analysis of RAG short-comings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R0Ln2u30tAxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive Mounting\n",
        "\n"
      ],
      "metadata": {
        "id": "QYvqh5V9lAah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell connects the Google Colab runtime to the user's *Google Drive*, establishing **persistent storage**.\n",
        "\n",
        "The primary **rationale** is to enable saving and loading of critical project components like *models*, *datasets*, *indexes*, and *results*, ensuring work continuity."
      ],
      "metadata": {
        "id": "wA3zS0L1mz6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'drive' module from google.colab library.\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive at the '/content/drive' path in the Colab filesystem.\n",
        "# Requires user authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Confirm successful mounting.\n",
        "print(\"Drive mounted successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezaU5jlJlAJ3",
        "outputId": "081e6297-bbee-4566-96e6-d1a634545a53"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Environment Setup\n",
        "\n",
        "This section establishes the foundational Python environment for our Retrieval-Augmented Generation (RAG) project.\n",
        "Key activities include:\n",
        "1.  **Dependency Installation**: Securely installing all required external libraries.\n",
        "2.  **Module Imports**: Loading necessary Python modules and classes.\n",
        "3.  **Parser Configuration**: Setting up the `tree-sitter` parser for Python, crucial for advanced code evaluation."
      ],
      "metadata": {
        "id": "EYTInqm1sA83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Installing Core Libraries\n",
        "\n",
        "We install dependencies using `pip`. Version pinning is applied to critical packages like `codebleu` and its `tree-sitter` dependencies to ensure consistent behavior and reproducibility across environments.\n",
        "\n",
        "**Key Libraries & Purpose:**\n",
        "*   **`transformers`, `datasets`, `huggingface_hub`**: Hugging Face ecosystem for models, datasets, and hub interaction.\n",
        "*   **`torch`, `accelerate`, `bitsandbytes`**: PyTorch framework and tools for efficient model execution, including 4-bit quantization.\n",
        "*   **`rank_bm25`**: BM25 algorithm for lexical retrieval.\n",
        "*   **`sacrebleu`, `codebleu`, `tree-sitter`, `tree-sitter-languages`**: Code evaluation metrics and their parsing dependencies.\n",
        "*   **`fsspec`**: Filesystem Abstraction (managed by pip's resolver unless conflicts arise)."
      ],
      "metadata": {
        "id": "lRxmzqq8m5AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Installing fundamental libraries (attempt 1.1 - updating transformers and hf_hub)...\")\n",
        "!pip install -U --no-cache-dir \\\n",
        "    transformers \\\n",
        "    datasets \\\n",
        "    fsspec \\\n",
        "    huggingface_hub \\\n",
        "    accelerate \\\n",
        "    bitsandbytes\n",
        "\n",
        "!pip install --no-cache-dir \\\n",
        "    torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip install --no-cache-dir \\\n",
        "    rank_bm25 \\\n",
        "    sacrebleu \\\n",
        "    tree-sitter-python \\\n",
        "    tree-sitter-languages \\\n",
        "    codebleu\n",
        "\n",
        "print(\"Fundamental libraries (attempt 1.1) installation attempt finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKb0AONqgKiI",
        "outputId": "4f1e6bcb-77bb-4aaa-a3df-114c68de60d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing fundamental libraries (attempt 1.1 - updating transformers and hf_hub)...\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m392.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m341.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m275.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m197.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m278.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m275.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m401.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m164.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m185.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m179.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m228.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m209.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, datasets, bitsandbytes, accelerate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.6.0\n",
            "    Uninstalling accelerate-1.6.0:\n",
            "      Successfully uninstalled accelerate-1.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.7.0 bitsandbytes-0.45.5 datasets-3.6.0 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tree-sitter-python\n",
            "  Downloading tree_sitter_python-0.23.6-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tree-sitter-languages\n",
            "  Downloading tree_sitter_languages-1.10.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting codebleu\n",
            "  Downloading codebleu-0.7.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (2.0.2)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Collecting tree-sitter (from tree-sitter-languages)\n",
            "  Downloading tree_sitter-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
            "  Downloading tree_sitter-0.22.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.11/dist-packages (from codebleu) (75.2.0)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_python-0.23.6-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m318.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_languages-1.10.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading codebleu-0.7.0-py3-none-any.whl (31 kB)\n",
            "Downloading tree_sitter-0.22.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (544 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.2/544.2 kB\u001b[0m \u001b[31m128.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: tree-sitter-python, tree-sitter, rank_bm25, portalocker, colorama, tree-sitter-languages, sacrebleu, codebleu\n",
            "Successfully installed codebleu-0.7.0 colorama-0.4.6 portalocker-3.1.1 rank_bm25-0.2.2 sacrebleu-2.5.1 tree-sitter-0.22.3 tree-sitter-languages-1.10.2 tree-sitter-python-0.23.6\n",
            "Fundamental libraries (attempt 1.1) installation attempt finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Importing Python Modules\n",
        "\n",
        "We import standard Python libraries and specific modules from the newly installed packages.\n",
        "\n",
        "**Module Groups & Purpose:**\n",
        "*   **Standard Library**: `os`, `sys`, `time`, `warnings`, `json`, `re`, etc., for general utility.\n",
        "*   **Type Hinting**: From `typing` for improved code readability and static analysis.\n",
        "*   **PyTorch**: `torch` for tensor operations.\n",
        "*   **Hugging Face `transformers`**:\n",
        "    *   `AutoTokenizer`, `AutoModelForCausalLM`: For loading models and tokenizers.\n",
        "    *   `BitsAndBytesConfig`: For quantization configuration.\n",
        "    *   `PreTrainedTokenizer`, `PreTrainedTokenizerFast`: For tokenizer type hints.\n",
        "    *   `StoppingCriteria`, `StoppingCriteriaList`: For custom generation stopping conditions.\n",
        "*   **Hugging Face `datasets`**: `load_dataset`, `Dataset` (class for type hint), `DownloadMode`.\n",
        "*   **Hugging Face `huggingface_hub`**: `hf_hub_download`, `EntryNotFoundError`.\n",
        "*   **Retrieval**: `BM25Okapi` from `rank_bm25`.\n",
        "*   **Progress Visualization**: `tqdm` (optional, with graceful fallback).\n",
        "*   **Evaluation Metrics**: `sacrebleu` and `calc_codebleu` (imports are failure-tolerant)."
      ],
      "metadata": {
        "id": "u_4YQmrZdqNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "import json\n",
        "import re\n",
        "import shutil\n",
        "import tarfile\n",
        "import ast\n",
        "import random\n",
        "import textwrap\n",
        "\n",
        "from typing import List, Optional, Dict, Any, Tuple, Union\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    PreTrainedTokenizer,\n",
        "    PreTrainedTokenizerFast,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList\n",
        ")\n",
        "from datasets import load_dataset, Dataset, DownloadMode\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub.utils import EntryNotFoundError as HfEntryNotFoundError # Alias for clarity\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "    USE_TQDM = True\n",
        "except ImportError:\n",
        "    USE_TQDM = False\n",
        "    print(\"Warning: 'tqdm' library not found. Progress bars will not be shown.\")\n",
        "    print(\"You can install it with: !pip install -q tqdm\")\n",
        "\n",
        "# Filter out less critical warnings for a cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"accelerate\")"
      ],
      "metadata": {
        "id": "JVNwBiOaRggx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: LLM and Tokenizer Loading with 4-bit Quantization\n",
        "\n",
        "This section focuses on loading and validating the dataset that will serve as the foundation for our Retrieval-Augmented Generation task. It involves specifying the dataset source, loading a particular split, performing integrity checks, and previewing its structure to ensure it's suitable for subsequent processing, including knowledge base construction and evaluation."
      ],
      "metadata": {
        "id": "qOrG-Nkrxwjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script orchestrates the loading of a specific LLM and its corresponding tokenizer, incorporating **4-bit quantization** to manage memory usage effectively.\n",
        "\n",
        "1.  **Model Selection:** A `model_name` variable is defined, selecting the specific pre-trained model to be used (CodeGemma, Qwen, Deepseek, Code Llama, Phi) by simply removing the  comment. The choice impacts the model's architecture, size, and capabilities.\n",
        "\n",
        "2.  **`trust_remote_code` Configuration:** Determines if the selected model requires executing custom code hosted on the Hugging Face Hub (`trust_remote_code=True`). This is automatically set based on known model prefixes (e.g., `Qwen/`, `microsoft/Phi-`) for security and functionality. *A warning is issued if set to `True`*.\n",
        "\n",
        "3.  **Quantization Setup (`BitsAndBytesConfig`):**\n",
        "    *   **Goal:** Reduce the model's memory footprint, enabling larger models to run on the available hardware.\n",
        "    *   **Method:** Configures **4-bit NF4 quantization** using `BitsAndBytesConfig`.\n",
        "    *   **Compute Type:** Dynamically selects the compute data type (`torch.bfloat16` if supported, otherwise `torch.float16`) for optimal performance during quantized operations.\n",
        "    *   **Details:** Uses `load_in_4bit=True`, `bnb_4bit_quant_type=\"nf4\"` (a common and effective quantization type), and `bnb_4bit_use_double_quant=True` for further memory savings.\n",
        "\n",
        "4.  **Tokenizer and Model Loading:**\n",
        "    *   Loads the appropriate `tokenizer` using `AutoTokenizer.from_pretrained`, passing the `trust_remote_code` flag. It includes a *crucial check* to set `tokenizer.pad_token_id` to `tokenizer.eos_token_id` if it's missing, a common requirement for causal LMs during batch processing or generation.\n",
        "    *   Loads the `model` using `AutoModelForCausalLM.from_pretrained`.\n",
        "        *   Applies the `quantization_config`.\n",
        "        *   Uses `device_map=\"auto\"` to automatically distribute model layers across available devices (GPU/CPU), essential for large models.\n",
        "        *   Sets `low_cpu_mem_usage=True` to minimize RAM usage during model loading.\n",
        "        *   Updates `model.config.pad_token_id` to match the tokenizer's setting if necessary.\n",
        "\n",
        "5.  **Error Handling:** Robust `try...except` blocks catch common loading issues (e.g., `ImportError`, network/model `OSError`, GPU `OutOfMemoryError`, `ValueError` related to quantization support) providing informative error messages.\n",
        "\n",
        "6.  **Verification:** If loading succeeds, it prints key details about the loaded model (name, estimated parameters, quantization settings) and tokenizer (class, vocab size, pad token ID). It concludes by running `nvidia-smi` to show the actual GPU memory consumption post-loading. If loading fails, a clear failure message is displayed."
      ],
      "metadata": {
        "id": "1LfVVpKznAZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that there is only one selected model\n",
        "\n",
        "# --- Gemma Series (Google) ---\n",
        "# model_name = \"google/codegemma-7b\"\n",
        "# model_name = \"google/codegemma-7b-it\"\n",
        "\n",
        "# --- Qwen Series (Alibaba) ---\n",
        "# model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "# model_name = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
        "# model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
        "\n",
        "# --- Deepseek Coder Series (Deepseek AI) ---\n",
        "# model_name = \"deepseek-ai/deepseek-coder-7b-instruct-v1.5\"\n",
        "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "\n",
        "# --- Code Llama Series (Meta) ---\n",
        "# model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "\n",
        "# --- Phi Series (Microsoft) ---\n",
        "# model_name = \"microsoft/Phi-4-mini-instruct\"\n",
        "# model_name = \"microsoft/Phi-4-multimodal-instruct\"\n",
        "\n",
        "print(f\"Selected model: {model_name}\")"
      ],
      "metadata": {
        "id": "k4O5pJ8ji-qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4d0478-48a3-4a56-b9d4-a571db279564"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected model: deepseek-ai/deepseek-coder-1.3b-base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Models/prefixes generally requiring trust_remote_code=True\n",
        "TRUST_REMOTE_CODE_MODELS = [\n",
        "    \"microsoft/Phi-\",\n",
        "    \"Qwen/\",\n",
        "]\n",
        "\n",
        "# Default to False, enable only if the model_name matches a prefix in the list\n",
        "trust_code = any(model_name.startswith(prefix) for prefix in TRUST_REMOTE_CODE_MODELS)\n",
        "\n",
        "# for deepseek-ai/DeepSeek-R1..., trust_code will be False (correct)\n",
        "print(f\"Setting trust_remote_code={trust_code} for {model_name}\")"
      ],
      "metadata": {
        "id": "oHqyjHXajE74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2fbda4-2196-4a4f-ecb5-01190aac6563"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting trust_remote_code=False for deepseek-ai/deepseek-coder-1.3b-base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.is_bf16_supported():\n",
        "        compute_dtype = torch.bfloat16\n",
        "        print(\"GPU supports bfloat16.\")\n",
        "    else:\n",
        "        compute_dtype = torch.float16\n",
        "        print(\"GPU does not supports bfloat16: use float16.\")\n",
        "else:\n",
        "    compute_dtype = torch.float16\n",
        "    print(\"CUDA not available: use float16.\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # use 4-bit precision\n",
        "    bnb_4bit_quant_type=\"nf4\", # use Normalized Float 4 (NF4)\n",
        "    bnb_4bit_compute_dtype=compute_dtype, #either bfloat16 or float16\n",
        "    bnb_4bit_use_double_quant=True, # double quantization\n",
        ")\n",
        "print(\"Create quantization\")"
      ],
      "metadata": {
        "id": "9Umsjmc3jLBY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e7cad8-78f3-473e-f868-92f4594b2cb2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU supports bfloat16.\n",
            "Create quantization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Stage 1: Load Tokenizer ---\n",
        "tokenizer = None\n",
        "\n",
        "try:\n",
        "    print(f\"\\nLoading tokenizer for model: {model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=trust_code  # From cell 2.2\n",
        "    )\n",
        "    print(\"Tokenizer loading successful.\")\n",
        "\n",
        "    # Standard pad_token configuration\n",
        "    if tokenizer.eos_token_id is None:\n",
        "        warnings.warn(\n",
        "            f\"CRITICAL: Tokenizer for {model_name} has no eos_token_id. \"\n",
        "            \"This is highly unusual and may lead to issues with padding or generation.\"\n",
        "        )\n",
        "\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        if tokenizer.eos_token_id is not None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "            warnings.warn(\n",
        "                f\"Tokenizer for {model_name} lacked a pad_token_id. \"\n",
        "                f\"Set to eos_token_id: {tokenizer.eos_token_id}.\"\n",
        "            )\n",
        "        else: # Should be caught by the warning above, but good to have a fallback.\n",
        "             raise ValueError(f\"Tokenizer for {model_name} has neither pad_token_id nor eos_token_id. Cannot proceed.\")\n",
        "\n",
        "\n",
        "    if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        warnings.warn(\n",
        "            f\"Tokenizer for {model_name} lacked a pad_token string. \"\n",
        "            f\"Set to eos_token string: '{tokenizer.eos_token}'.\"\n",
        "        )\n",
        "\n",
        "except HfEntryNotFoundError:\n",
        "    print(f\"ERROR: Tokenizer for '{model_name}' not found on Hugging Face Hub. Check model name.\")\n",
        "    tokenizer = None\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Unexpected error loading tokenizer for {model_name}:\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    tokenizer = None"
      ],
      "metadata": {
        "id": "Y3ZnMMcRA_J6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289,
          "referenced_widgets": [
            "b65132b35c5c4e60bca6772786c6b213",
            "45de9a7ec0964f4eae38d335846c41a4",
            "0ddacbe15c4e438ba52cb9788c167120",
            "bfe0de564ff640fead8e8e1ede427bc6",
            "010ec30ce0494803bd910372b38f70a7",
            "614a773a599b468998ac80c563f542d2",
            "53d8a7917a2948d892a512dda0dad25d",
            "95b96d93748e49dba6198d9bd2383a91",
            "f20e7d8a2fa94bc9a2dc006c59935d02",
            "32c3cc88e6d54c47b570f1cb6ad2ba1d",
            "5b60b5a7dc1a4370998b90efa5c750de",
            "07fef6eb899f48f4a008ed4b93567fec",
            "0ac750cd48f94a8cbb5747436559c485",
            "2be86e6d84a24afe9d5e8cd273d4c187",
            "e52e10e4e75749a499442843ae131bf0",
            "d67e2447dacd44a9818a23f991205979",
            "3cc1e7689cff4c6eb3b7a562d1fb3a43",
            "608f53317e5d4ac2a975c72a6f01dec4",
            "adbefc240a1049f5b48e63577793b20b",
            "88bc8087b7c64e7ba19aa3f8644c0121",
            "674f4e8007cd45879327d666141cceb2",
            "ddd7af1f1b83449782284f8f751aeea8",
            "8af5d87383214ec08a239913e9fd08af",
            "5c0a5a6080834147aa8df4fcfef02287",
            "2bb9e3dc577f44c986b665c0dfd9974c",
            "5d81744741d2460e9d545c8633305e96",
            "0db05a96bd274ea6b2e21fe528eb4c96",
            "32b6e2df722b49b098f0f2e7ebcaa1e2",
            "77642ebaa035478a9485ba3729ec1ab6",
            "ce4b65530f344d45a3e8b20a03407367",
            "f92e511f6d93453ab486eefcb34b91aa",
            "36ab4f7d130c46ea8e71e25ce4308cdb",
            "d0eeaa7cf5004319a11f9bd7ab691b0c"
          ]
        },
        "outputId": "f77ac666-a5bb-4ee0-a709-699058f8bd97"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading tokenizer for model: deepseek-ai/deepseek-coder-1.3b-base...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/793 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b65132b35c5c4e60bca6772786c6b213"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07fef6eb899f48f4a008ed4b93567fec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8af5d87383214ec08a239913e9fd08af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loading successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Stage 2: Load Model ---\n",
        "model = None\n",
        "\n",
        "if tokenizer:\n",
        "    try:\n",
        "        print(f\"\\nLoading quantized model: {model_name} (4-bit)...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=quantization_config, #From cell 2.3\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=trust_code, # From cell 2.2\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "\n",
        "        if hasattr(model, 'config') and model.config.pad_token_id is None and tokenizer.pad_token_id is not None:\n",
        "            model.config.pad_token_id = tokenizer.pad_token_id\n",
        "            print(f\"Synced model.config.pad_token_id with tokenizer: {tokenizer.pad_token_id}\")\n",
        "\n",
        "        print(f\"Model '{model_name}' loaded successfully with 4-bit quantization.\")\n",
        "\n",
        "    except HfEntryNotFoundError:\n",
        "        print(f\"ERROR: Model weights/config for '{model_name}' not found on Hugging Face Hub.\")\n",
        "        model = None\n",
        "    except torch.cuda.OutOfMemoryError:\n",
        "        print(f\"ERROR: CUDA OOM while loading model '{model_name}'. Model too large for GPU VRAM.\")\n",
        "        model = None\n",
        "    except ValueError as e:\n",
        "        print(f\"ERROR (ValueError) loading model '{model_name}': {e}\")\n",
        "        if \"bitsandbytes\" in str(e).lower() or \"NF4\" in str(e):\n",
        "            print(\"  This might be related to bitsandbytes setup or NF4 incompatibility.\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        model = None\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Unexpected error loading model {model_name}:\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        model = None\n",
        "else:\n",
        "    print(f\"\\nSkipping model loading for {model_name} due to tokenizer loading failure.\")\n",
        "    model = None\n",
        "\n",
        "# --- Final Status Update ---\n",
        "if tokenizer and model:\n",
        "    print(f\"\\nSUCCESS: '{model_name}' is loaded.\")\n",
        "else:\n",
        "    print(f\"\\nCRITICAL FAILURE: Could not load '{model_name}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "99545a3a876b46849d4b12e423a02c84",
            "b028bbad627d4ef3a7d0ce9526c0e70c",
            "16bad7ee1e3943db877f1b28aed3349d",
            "7d3c7dd9b0414c35ade29ee494db462f",
            "25bd71decdec4c968d871c5e1addccbf",
            "81b43653571d4f1eb0227875a44e2d3e",
            "b3fcba7e35d441ab8d61003eac2a2609",
            "7d884adbd4cd4582a5e2b77d22cafeda",
            "4c311aba998442fb83f9929c910ddf89",
            "803c80a7d13241e195e7e2df90a1e46a",
            "e5a04ac1f0bd49e39138ce9770956733",
            "f3acb751bade4cb2b798a31556e6af5c",
            "4c3b7883f68b4ca79703e8c6e40abfc2",
            "48a92d66e6cb45f18decdd2e7d8b7af9",
            "33d8343a0d03401ca01c857ab64ff54b",
            "48c4bdd6310a46efa53be6ce551d8445",
            "c6dcc2d472aa4957ab61a8793b9af395",
            "96449a24c237466db6486339d2822d1b",
            "d120a68986634d8b997dcbc0c2c30ae2",
            "eb26b7a1d72241dea7792724aea898db",
            "316f40ef94d940cdaab9a06bd5d260b9",
            "497cc89edd7b40c3a7dc9e6d2e4ef42f",
            "bdccfbb69d89413f911cd966a0748d18",
            "061f974c5ea84079a1ee5c28e4e44d58",
            "9fbc08862df943dfad9001e374ccc614",
            "eec3dd9e9c904d8e9ec8ce14d2bb61b6",
            "68a63741056f4e4db7924bcf0883a2c3",
            "ee1a94537bb640a1a4d28ec1beefef2d",
            "520db47e15b14bb794425607be0b84c0",
            "590912a3ec5241ffa258cae5352a8083",
            "d51d51b6d9b445cb9c470db94af26ce2",
            "d0c1096475334d7abaae829ca6d75805",
            "f597bc9d28f24659b4e2e97adeda1499",
            "82ff1618bb274f9ab5b29ed53eea4e9d",
            "f98be5e3cb624fa2b0f71ca40f5b7719",
            "69a35979672948dab37ab105f787b350",
            "b89677a9d1424961b98f7750c404f8eb",
            "885bd1f13d8040448928a3424a63c268",
            "b13cfb8bc90c4dc8aabb06239bffec7c",
            "e46021dcc9474a9bb8068f51f87bcbda",
            "708f0c78f27d480f8dcdbb3188f6b881",
            "e8f15df1528142a98c9afdcbbd0276f9",
            "b6a5326a18b54b11a2dbf7299055829b",
            "a0269ed8ae47429ca3ccc96cb5d14302"
          ]
        },
        "id": "qBxJOk4bANe7",
        "outputId": "2c2aa748-ad55-4e74-e5d1-126afab0eb98"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading quantized model: deepseek-ai/deepseek-coder-1.3b-base (4-bit)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99545a3a876b46849d4b12e423a02c84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3acb751bade4cb2b798a31556e6af5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdccfbb69d89413f911cd966a0748d18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82ff1618bb274f9ab5b29ed53eea4e9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synced model.config.pad_token_id with tokenizer: 32014\n",
            "Model 'deepseek-ai/deepseek-coder-1.3b-base' loaded successfully with 4-bit quantization.\n",
            "\n",
            "SUCCESS: 'deepseek-ai/deepseek-coder-1.3b-base' is loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Dataset Preparation and Validation\n",
        "\n"
      ],
      "metadata": {
        "id": "hybDs-6NsH2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section orchestrates the ingestion and initial vetting of our target dataset to ensure it's ready for downstream RAG processing. It covers four main steps:\n",
        "\n",
        "1. **Google Drive Directory Setup**  \n",
        "   Defines and verifies a persistent storage path (`drive_save_path`) on Google Drive—creating it if needed—to hold all dataset artifacts, intermediate files, and future outputs.\n",
        "\n",
        "2. **Dataset Loading**  \n",
        "   Uses Hugging Face's `load_dataset` to fetch the specified split (`test`) of the `JetBrains-Research/lca-library-based-code-generation` dataset into Colab's local cache for speed and stability (avoiding Drive as a cache directory).\n",
        "\n",
        "3. **Inspection & Error Handling**  \n",
        "   - **Entry Count & Size Estimate:** Reports the number of records and the approximate memory/cache footprint.  \n",
        "   - **Structure Preview:** Prints available columns and a snippet of the first example (repository name, instruction, reference code, and top APIs).  \n",
        "   - **Robust Exceptions:** Catches issues like missing dataset, connection failures, or other unexpected errors, offering troubleshooting tips.\n",
        "\n",
        "4. **Final Verification & Advisory**  \n",
        "   Prints a clear success/failure banner and reminds you that the actual library source code (the “repos”) must be downloaded separately if you plan to build your knowledge base.\n"
      ],
      "metadata": {
        "id": "U0ukqCaVnHBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.1 GDrive Directory Setup ---\n",
        "drive_save_path = '/content/drive/MyDrive/RAG_Project/' # To store results/outputs\n",
        "\n",
        "# Check that the directory exists; If not, we create it\n",
        "try:\n",
        "    os.makedirs(drive_save_path, exist_ok=True)\n",
        "    print(f\"Google Drive Directory available: {drive_save_path}\")\n",
        "except OSError as e:\n",
        "    print(f\"Warning: can not create or verify the existence of the directory: {drive_save_path}. Details: {e}\")"
      ],
      "metadata": {
        "id": "6afhPXWpw7KP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4479ddd4-396e-4f99-bee5-79c8c45d4f1d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive Directory available: /content/drive/MyDrive/RAG_Project/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.2 Loading of the dataset ---\n",
        "from datasets import load_dataset, DownloadMode\n",
        "\n",
        "# Configuration for dataset loading\n",
        "dataset_name = \"JetBrains-Research/lca-library-based-code-generation\"\n",
        "data_split = \"test\"  # As per dataset documentation for evaluation\n",
        "\n",
        "lca_dataset_split = None  # Initialize variable to store the loaded dataset\n",
        "\n",
        "print(f\"\\nAttempting to load dataset: '{dataset_name}' (split: '{data_split}').\")\n",
        "print(\"Note: Hugging Face Datasets library will use local Colab cache for optimal performance.\")\n",
        "\n",
        "\n",
        "try:\n",
        "    # Dataset Loading Attempt\n",
        "    lca_dataset_split = load_dataset(\n",
        "        dataset_name,\n",
        "        split=data_split,\n",
        "        download_mode=DownloadMode.FORCE_REDOWNLOAD,\n",
        "    )\n",
        "    print(\"\\nThe dataset was successfully loaded!\")\n",
        "\n",
        "except ConnectionError as e:\n",
        "    print(f\"\\nERROR: Connection error while loading dataset. Please check your internet connection. Details: {e}\")\n",
        "    lca_dataset_split = None # Ensure it's None on failure\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: An unexpected error occurred while loading the dataset: {e}\")     # Catch other potential errors during dataset loading.\n",
        "    lca_dataset_split = None # Ensure it's None on failure\n"
      ],
      "metadata": {
        "id": "OX6IjRiJxJNx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "91760f94964a42028e41c959dd98f464",
            "c8f5b23d9de2471ba212544419bca194",
            "932a593e38094907a4b8d2f83440f195",
            "66c04e29039247eaa7154979a7040226",
            "67e185b505544f598cc5d5ee3310e4ec",
            "4e592da5973b44698343ad2970fb19a1",
            "774591b24b2a43dba5603ad67e9686ec",
            "f046f1d079a24722aa99d51938a377f4",
            "796b3e6517074b3d92d10dce44a42e3c",
            "f5b14d9f4fc24d9e95b0b7d9cc01b166",
            "9127c95b49a345b78bb9843cc14da257",
            "31a8f8bd477241d0b67885ada835648a",
            "ab94c3b5204f408fb585e9c40b96af6a",
            "ceb49e9f4f3e490697aadaba1e7a5fc5",
            "567a8effd7f0402b9cbec4f21404b165",
            "cd10f5c9537e40ba9b96c5d12233cc01",
            "95d70551a54c43009bcda2931364f823",
            "1783b6d6e2634f3e99276876c7bae180",
            "632253fd43c24d249ff9f83bb9051898",
            "492079920e9e43b983825e2e1a41df69",
            "4560dc3f81d44e4c891133b62a144810",
            "3f257f3efc7744f7b6577e1fd405dbd9",
            "5f9cc57a886847abbfdc6c9da56ecdcb",
            "5cfb4d79d4644daca79380d551914946",
            "e87cb4112f8b46829623fc8b3e6cd380",
            "0a7cdc8561024c468f2d8e07e831fbf0",
            "5ea1cb1133da4458ae7f2f6f0aa4e71a",
            "60fa54ef0237416b91d8dd99c490f686",
            "b4ce812c26fa4ad89ccb3e3bd4d7e764",
            "a69af8d1f3f84b89b25cd14153e9a6d4",
            "a7aad4098b4b487085633284d8f35bc3",
            "365d4da2d580402a9e4b3cccb651ac2e",
            "6d19fee5a9664ecd9214989039e234a2"
          ]
        },
        "outputId": "898bae92-d266-4187-f403-58bca7d0942b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attempting to load dataset: 'JetBrains-Research/lca-library-based-code-generation' (split: 'test').\n",
            "Note: Hugging Face Datasets library will use local Colab cache for optimal performance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/5.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91760f94964a42028e41c959dd98f464"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)-00000-of-00001-518ed46ecbe35ff9.parquet:   0%|          | 0.00/4.58M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31a8f8bd477241d0b67885ada835648a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/150 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f9cc57a886847abbfdc6c9da56ecdcb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The dataset was successfully loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap # For cleaner text truncation\n",
        "\n",
        "print(\"\\n--- 3.3 Dataset Inspection and Validation ---\")\n",
        "\n",
        "if lca_dataset_split is not None:\n",
        "    print(f\"\\nDataset '{dataset_name}' (split: '{data_split}') appears to be loaded.\")\n",
        "    print(f\"Number of entries: {len(lca_dataset_split)}\")\n",
        "\n",
        "    # Attempt to get and display dataset size\n",
        "    try:\n",
        "        # dataset.info.size_in_bytes is another way, or dataset.size_in_bytes\n",
        "        dataset_size_bytes = lca_dataset_split.size_in_bytes\n",
        "        if dataset_size_bytes is not None: # Check if the attribute exists and is not None\n",
        "            print(f\"Estimated dataset size (RAM/cache): {dataset_size_bytes / (1024**2):.2f} MB\")\n",
        "        else:\n",
        "            print(\"Dataset size information (size_in_bytes) is None or not available.\")\n",
        "    except AttributeError:\n",
        "        print(\"Info: The 'size_in_bytes' attribute is not available for this dataset object.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Info: Could not retrieve dataset size. Error: {e}\")\n",
        "\n",
        "    # Display dataset features (columns)\n",
        "    if hasattr(lca_dataset_split, 'features'):\n",
        "        print(\"\\nAvailable columns (features) in the dataset:\")\n",
        "        print(list(lca_dataset_split.features.keys()))\n",
        "    else:\n",
        "        print(\"\\nWarning: Dataset features (column names) could not be retrieved.\")\n",
        "\n",
        "    # Preview the first example if the dataset is not empty\n",
        "    if len(lca_dataset_split) > 0:\n",
        "        print(\"\\nPreview of the first example's content:\")\n",
        "        first_example = lca_dataset_split[0]\n",
        "\n",
        "        repo_name = first_example.get('repo_full_name', 'N/A')\n",
        "        instruction_text = first_example.get('instruction', 'N/A')\n",
        "        reference_text = first_example.get('reference', 'N/A')\n",
        "        apis_list = first_example.get('unique_apis', [])\n",
        "\n",
        "        print(f\"  Repository: {repo_name}\")\n",
        "        print(f\"  Instruction: {textwrap.shorten(instruction_text, width=100, placeholder='...')}\")\n",
        "        print(f\"  Reference Code: {textwrap.shorten(reference_text, width=100, placeholder='...')}\")\n",
        "        print(f\"  Unique APIs (first 5): {apis_list[:5]}{'...' if len(apis_list) > 5 else ''}\")\n",
        "    else:\n",
        "        print(\"\\nDataset is loaded but contains no entries.\")"
      ],
      "metadata": {
        "id": "ub_mCE-N0zh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8458e454-cd7f-4a77-c623-1331adf64502"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 3.3 Dataset Inspection and Validation ---\n",
            "\n",
            "Dataset 'JetBrains-Research/lca-library-based-code-generation' (split: 'test') appears to be loaded.\n",
            "Number of entries: 150\n",
            "Estimated dataset size (RAM/cache): 12.62 MB\n",
            "\n",
            "Available columns (features) in the dataset:\n",
            "['repo_full_name', 'repo_name', 'repo_owner', 'instruction', 'reference', 'clean_reference', 'path_to_reference_file', 'path_to_examples_folder', 'n_unique_apis', 'unique_apis', 'project_defined_elements', 'api_calls', 'internal_apis']\n",
            "\n",
            "Preview of the first example's content:\n",
            "  Repository: seed-labs__seed-emulator\n",
            "  Instruction: Generate code that creates an emulation using the seedemu library. The emulation should include...\n",
            "  Reference Code: #!/usr/bin/env python # encoding: utf-8 # __author__ = 'Demon' from seedemu.layers import Base,...\n",
            "  Unique APIs (first 5): ['DomainNameCachingService', 'addLayer', 'addPrivatePeering', 'Ospf', 'createHost']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4: Repository Archive Download & Preparation\n",
        "\n"
      ],
      "metadata": {
        "id": "Smf_hCzO6FRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section automates fetching a specific archived snapshot from our dataset’s Hugging Face repository and placing it into the working directory with robust error handling. It comprises five key phases:\n",
        "\n",
        "1. **Exception Imports**  \n",
        "   Attempts to import `HfHubHTTPError`, `RepositoryNotFoundError`, and `EntryNotFoundError` from the most up‑to‑date `huggingface_hub.utils`; falls back to `.errors`, and finally defines dummy exception classes if neither is available. This ensures our download logic can catch and respond to all common HF Hub issues.\n",
        "\n",
        "2. **Archive Download**  \n",
        "   Uses `hf_hub_download` to pull `repos/seed-labs__seed-emulator.tar.gz` from the `JetBrains-Research/lca-library-based-code-generation` dataset into a temporary local cache. Prints progress messages to keep you informed.\n",
        "\n",
        "3. **Error Handling**  \n",
        "   - **RepositoryNotFoundError** if the repo ID is invalid  \n",
        "   - **EntryNotFoundError** if the specified file path doesn’t exist in the repo  \n",
        "   - **HfHubHTTPError** for other HTTP failures (401/403/404, etc.)  \n",
        "   - **Generic Exceptions** for any unforeseen issues, with a concise summary of the error type and message\n",
        "\n",
        "4. **File Relocation & Cleanup**  \n",
        "   If the download succeeds, the script checks whether the file is already at the desired target (`/content/seed-labs__seed-emulator.tar.gz`). If not, it moves the archive there, creates any missing directories, and then removes any now‑empty intermediate folders to keep the workspace tidy.\n",
        "\n",
        "5. **Final Verification**  \n",
        "   Prints a clear `[OK]` banner if the archive is present at the target path—or an `[ERROR]` banner otherwise—so you can be confident whether to proceed with the next extraction or processing steps.\n"
      ],
      "metadata": {
        "id": "K0BCKBpnnQ7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Exception Imports ---\n",
        "try:\n",
        "    from huggingface_hub.utils import HfHubHTTPError, RepositoryNotFoundError, EntryNotFoundError\n",
        "    print(\"Succesfully import exceptions from huggingface_hub.utils.\")\n",
        "except ImportError:\n",
        "    print(\"WARNING: can not import exceptions from huggingface_hub.utils, try from .errors\")\n",
        "    try:\n",
        "        from huggingface_hub.errors import HfHubHTTPError, RepositoryNotFoundError, EntryNotFoundError\n",
        "        print(\"Importing exceptions from huggingface_hub.errors completed.\")\n",
        "    except ImportError:\n",
        "        print(\"ERROR: can not import exceptions from huggingface_hub.\")\n",
        "        class HfHubHTTPError(Exception): pass\n",
        "        class RepositoryNotFoundError(Exception): pass\n",
        "        class EntryNotFoundError(Exception): pass"
      ],
      "metadata": {
        "id": "pOrW3AZ16VCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d5b2f2-0b5a-4426-93ee-9ecb0d2e85c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Succesfully import exceptions from huggingface_hub.utils.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Archive Download ---\n",
        "\n",
        "repo_id = \"JetBrains-Research/lca-library-based-code-generation\"\n",
        "filename_in_repo = \"repos/seed-labs__seed-emulator.tar.gz\"\n",
        "desired_local_archive_path = \"/content/seed-labs__seed-emulator.tar.gz\"\n",
        "download_base_dir = \"/content/\"\n",
        "\n",
        "print(f\"\\n--- Download and configuration ---\")\n",
        "print(f\"Repo: {repo_id}\")\n",
        "print(f\"File in repo: {filename_in_repo}\")\n",
        "print(f\"Desired destination: {desired_local_archive_path}\")\n",
        "\n",
        "actual_downloaded_path = None\n",
        "\n",
        "# actual donwload from Hugging Face\n",
        "try:\n",
        "    print(f\"\\nStarting download from Hugging Face Hub...\")\n",
        "    actual_downloaded_path = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=filename_in_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        local_dir=download_base_dir,\n",
        "        local_dir_use_symlinks=False,\n",
        "    )\n",
        "    print(f\"Download completed. File saved at: {actual_downloaded_path}\")\n",
        "\n",
        "# --- 3. Error Handling ---\n",
        "except RepositoryNotFoundError:\n",
        "    print(f\"\\nERROR: Repository '{repo_id}' not found on Hugging Face Hub.\")\n",
        "    print(\"  Make sure the repository name is correct.\")\n",
        "except EntryNotFoundError:  # Specific file not found in the repo\n",
        "    print(f\"\\nERROR: File/Entry '{filename_in_repo}' not found in the repository '{repo_id}'.\")\n",
        "except HfHubHTTPError as e:  # HTTP errors (including 401, 403, 404 not already caught above)\n",
        "    print(f\"\\nHTTP ERROR during download from Hugging Face Hub: {e}\")\n",
        "    if hasattr(e, 'response') and e.response is not None:\n",
        "        print(f\"  Status Code: {e.response.status_code}\")\n",
        "        if e.response.status_code == 404:\n",
        "            print(f\"  -> The file '{filename_in_repo}' or the repo '{repo_id}' may not exist (Error 404).\")\n",
        "    print(f\"  Please check the repo_id, filename_in_repo, and your internet connection or HF token if necessary.\")\n",
        "except Exception as e:\n",
        "    # Catch other unexpected errors\n",
        "    import traceback\n",
        "    print(f\"\\nUNEXPECTED ERROR during the download:\")\n",
        "    # print(traceback.format_exc())  # Uncomment for full traceback during debugging\n",
        "    print(f\"  Error Type: {type(e).__name__}, Message: {e}\")\n",
        "\n",
        "# --- 4. File Relocation & Cleanup ---\n",
        "archive_ready = False\n",
        "if actual_downloaded_path and os.path.exists(actual_downloaded_path):\n",
        "    if os.path.abspath(actual_downloaded_path) == os.path.abspath(desired_local_archive_path):\n",
        "        print(f\"\\nThe archive is already at the desired final location: {desired_local_archive_path}\")\n",
        "        archive_ready = True\n",
        "    else:\n",
        "        try:\n",
        "            print(f\"\\Moving '{os.path.basename(actual_downloaded_path)}' to '{desired_local_archive_path}'...\")\n",
        "            os.makedirs(os.path.dirname(desired_local_archive_path), exist_ok=True)\n",
        "            shutil.move(actual_downloaded_path, desired_local_archive_path)\n",
        "            print(f\"Move completed successfully.\")\n",
        "            archive_ready = True\n",
        "\n",
        "            # Clean up intermediate directory if empty\n",
        "            download_parent_dir = os.path.dirname(actual_downloaded_path)\n",
        "            if (os.path.exists(download_parent_dir) and\n",
        "                os.path.abspath(download_parent_dir) != os.path.abspath(download_base_dir) and\n",
        "                os.path.abspath(download_parent_dir).startswith(os.path.abspath(download_base_dir)) and\n",
        "                not os.listdir(download_parent_dir)):\n",
        "                try:\n",
        "                    print(f\"Removing empty intermediate directory: {download_parent_dir}\")\n",
        "                    os.rmdir(download_parent_dir)\n",
        "                except OSError as rmdir_e:\n",
        "                    print(f\"  Warining: can not remove {download_parent_dir}. Issue: {rmdir_e}\")\n",
        "\n",
        "        except Exception as move_e:\n",
        "            print(\"\\nERROR during move or cleanup of downloaded file:\")\n",
        "            print(f\"  Error: {move_e}\")\n",
        "            print(f\"  The downloaded file may still be located at: {actual_downloaded_path}\")\n",
        "            archive_ready = False\n",
        "\n",
        "elif not actual_downloaded_path:\n",
        "     print(\"\\nDownload failed. Cannot proceed.\")\n",
        "else:\n",
        "     print(f\"\\nINTERNAL ERROR: Download path ({actual_downloaded_path}) does not exist after the attempt.\")\n",
        "\n",
        "# --- 5. Final Verification ---\n",
        "print(\"\\nFinal check:\")\n",
        "if archive_ready and os.path.exists(desired_local_archive_path):\n",
        "    print(f\"[OK] The final archive is ready at: {desired_local_archive_path}\")\n",
        "else:\n",
        "    print(f\"[ERROR] The final archive was NOT found or prepared correctly at: {desired_local_archive_path}\")\n",
        "\n",
        "print(\"\\n--- End of Download and Preparation ---\")"
      ],
      "metadata": {
        "id": "NRFnS_H-NO0F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399,
          "referenced_widgets": [
            "d8a2413391d44e109184c188e4a15a91",
            "4da4040644474b91822dc4726155214d",
            "bbb5f19adefc4e3f94ae7c84e463ef55",
            "9e2b091b983e4367a7272e9069a3215c",
            "a383bae6e426452e8b57e3baa49879ab",
            "8cebf5fa12f34d818d7ba518f3602044",
            "4d1b8e433ef74cbbad6476047ea229fa",
            "b70b81d4e5534a20b9dc94513340af79",
            "499f62574e254dd3bc155f02c56af60f",
            "4921f1925878492c9e776825bb7031cf",
            "77d917ce675c4d43b2b3c2ca4e7255ad"
          ]
        },
        "outputId": "ab692d09-e13a-4d30-9c1a-233967e75746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Download and configuration ---\n",
            "Repo: JetBrains-Research/lca-library-based-code-generation\n",
            "File in repo: repos/seed-labs__seed-emulator.tar.gz\n",
            "Desired destination: /content/seed-labs__seed-emulator.tar.gz\n",
            "\n",
            "Starting download from Hugging Face Hub...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "seed-labs__seed-emulator.tar.gz:   0%|          | 0.00/24.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8a2413391d44e109184c188e4a15a91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed. File saved at: /content/repos/seed-labs__seed-emulator.tar.gz\n",
            "\\Moving 'seed-labs__seed-emulator.tar.gz' to '/content/seed-labs__seed-emulator.tar.gz'...\n",
            "Move completed successfully.\n",
            "Removing empty intermediate directory: /content/repos\n",
            "\n",
            "Final check:\n",
            "[OK] The final archive is ready at: /content/seed-labs__seed-emulator.tar.gz\n",
            "\n",
            "--- End of Download and Preparation ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 5: Source Extraction & Knowledge Base Construction\n",
        "\n"
      ],
      "metadata": {
        "id": "9RBajwXBaSP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section handles unpacking the downloaded emulator archive and building a searchable code-snippet knowledge base (KB) from the extracted Python sources. It consists of three major parts:\n",
        "\n",
        "1. **Archive Extraction**  \n",
        "   - **Configuration:** Points to the local archive (`local_archive_path`) and defines the parent directory for extraction (`extract_dir_parent`).  \n",
        "   - **Safety Checks:** Verifies the archive exists, cleans any old extraction folder, then creates the target directory.  \n",
        "   - **Unpacking:** Uses Python's `tarfile` to extract all contents into `extract_dir_parent`.  \n",
        "   - **Dynamic Path Resolution:** Scans the extraction folder to identify the main code directory—handling single‐folder archives, multiple items, or unexpected layouts—falling back to the parent directory if needed.\n",
        "\n",
        "2. **Snippet Extraction Helpers**  \n",
        "   - **`extract_code_units(...)`:** Reads each `.py` file (with UTF-8 and fallback encoding), parses its AST, and collects the full source text of every function, async function, and class definition, gracefully ignoring syntax or permission errors.  \n",
        "   - **`build_kb_for_library(...)`:** Recursively walks the extracted source tree, applies `extract_code_units` to every Python file (with an optional `tqdm` progress bar), and accumulates all code units. If the total exceeds `MAX_KB_SIZE`, it randomly samples down to limit memory use.\n",
        "\n",
        "3. **KB Creation & Persistence**  \n",
        "   - **Sample Selection:** Retrieves the `repo_full_name` from the chosen dataset entry (`SAMPLE_INDEX`) to label the KB.  \n",
        "   - **KB Assembly:** Invokes `build_kb_for_library` on the extracted code path, reporting how many files and snippets were processed or skipped.  \n",
        "   - **Drive Saving:** Creates (if necessary) a `library_kbs` folder on Google Drive, serializes the final snippet list to JSON, and writes it as `kb_<repo_name>_sample_<i>.json`.  \n",
        "   - **Final Check:** Prints a success banner with the total snippet count or an error if the KB is empty or failed to save.\n",
        "\n",
        "By the end of this cell you'll have both the raw source files available under `extract_dir_parent` and a curated, size-limited KB of Python code snippets stored permanently in your Drive for use in retrieval-augmented generation.  \n"
      ],
      "metadata": {
        "id": "KADqJusenacQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Archive Extraction ---\n",
        "# --- 1.1. Configuration ---\n",
        "# Path to the downloaded archive (should already exist from the previous cell)\n",
        "local_archive_path = '/content/seed-labs__seed-emulator.tar.gz'\n",
        "# Base directory where we want to extract the archive contents\n",
        "extract_dir_parent = \"/content/library_sources/\"\n",
        "\n",
        "# This variable will hold the actual path to the main extracted folder.\n",
        "# It will be determined after extraction is complete.\n",
        "final_extracted_code_path = None\n",
        "\n",
        "print(\"--- Extraction archive ---\")\n",
        "print(f\"Archive: {local_archive_path}\")\n",
        "print(f\"Destination directory: {extract_dir_parent}\")"
      ],
      "metadata": {
        "id": "llFrYgnkail7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec35d3e-8a99-4e90-f2dd-60e39229ba40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Extraction archive ---\n",
            "Archive: /content/seed-labs__seed-emulator.tar.gz\n",
            "Destination directory: /content/library_sources/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1.2. Safety Check ---\n",
        "if not os.path.exists(local_archive_path):\n",
        "    print(f\"\\n[ERROR] Source archive not found: {local_archive_path}\")\n",
        "    print(\"  Make sure the download cell was run correctly.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    try:\n",
        "        # Optional: clean destionation directory before the execution (if next instruction is not commented)\n",
        "        if os.path.exists(extract_dir_parent): shutil.rmtree(extract_dir_parent)\n",
        "\n",
        "        # Create destination directory\n",
        "        # exist_ok=True avoids errors if already exists\n",
        "        os.makedirs(extract_dir_parent, exist_ok=True)\n",
        "        print(f\"\\nTarget extraction directory '{extract_dir_parent}' is ready.\")\n",
        "\n",
        "        # --- 1.3. Unpacking ---\n",
        "        # extract the archive\n",
        "        print(f\"Starting extraction of '{os.path.basename(local_archive_path)}'...\")\n",
        "        with tarfile.open(local_archive_path, \"r:gz\") as tar:\n",
        "            tar.extractall(path=extract_dir_parent)\n",
        "        print(\"Extraction completed successfully.\")\n",
        "\n",
        "        # --- 1.4. Dynamic Path Resolution ---\n",
        "        # dynamically determine the extracted path\n",
        "        try:\n",
        "            extracted_items = os.listdir(extract_dir_parent)\n",
        "            if len(extracted_items) == 1 and os.path.isdir(os.path.join(extract_dir_parent, extracted_items[0])):\n",
        "                final_extracted_code_path = os.path.join(extract_dir_parent, extracted_items[0])\n",
        "                print(f\"Identified main extracted directory: {final_extracted_code_path}\")\n",
        "            elif len(extracted_items) > 0:\n",
        "                 # look for a folder matching the archive's base name\n",
        "                 archive_basename = os.path.basename(local_archive_path).replace('.tar.gz', '').replace('.tgz', '')\n",
        "                 potential_match = os.path.join(extract_dir_parent, archive_basename)\n",
        "                 if os.path.isdir(potential_match):\n",
        "                     final_extracted_code_path = potential_match\n",
        "                     print(f\"Found potential matching directory: {final_extracted_code_path}\")\n",
        "                 else:\n",
        "                     first_item_path = os.path.join(extract_dir_parent, extracted_items[0])\n",
        "                     if os.path.isdir(first_item_path):\n",
        "                          final_extracted_code_path = first_item_path\n",
        "                          print(f\"WARNING: Multiple items found. Assuming first directory: {final_extracted_code_path}\")\n",
        "                     else:\n",
        "                          print(f\"WARNING: No main directory found in the extraction folder {extract_dir_parent}.\")\n",
        "                          print(f\"  Contents: {extracted_items}\")\n",
        "                          print(f\"  'final_extracted_code_path' might be set manually.\")\n",
        "                          final_extracted_code_path = extract_dir_parent # Fallback: use the parent dir\n",
        "                          print(f\"  Impostato fallback a: {final_extracted_code_path}\")\n",
        "\n",
        "            else:\n",
        "                 print(f\"WARNING: Extraction folder '{extract_dir_parent}' is empty after extraction.\")\n",
        "\n",
        "        except Exception as list_e:\n",
        "             print(f\"Issue while analyzing the extracted data: {list_e}\")\n",
        "\n",
        "    except tarfile.ReadError:\n",
        "        print(f\"\\n[ERROR] Cannot read archive: {local_archive_path}. It may be corrupted.\")\n",
        "    except FileNotFoundError:\n",
        "        # can happen only if local_archive_path is removed\n",
        "        print(f\"\\n[ERROR] Archive file not found during open attempt: {local_archive_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] Unexpected error during preparation or extraction:\")\n",
        "        # print(traceback.format_exc()) # uncomment for debug\n",
        "        print(f\"  Error Type: {type(e).__name__}, Message: {e}\")\n"
      ],
      "metadata": {
        "id": "g_SzTEJ0bZT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747f146f-2551-40ff-a9f5-9454b6b49862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Target extraction directory '/content/library_sources/' is ready.\n",
            "Starting extraction of 'seed-labs__seed-emulator.tar.gz'...\n",
            "Extraction completed successfully.\n",
            "Identified main extracted directory: /content/library_sources/mnt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1.5. Final check ---\n",
        "\n",
        "print(\"\\nFinal check:\")\n",
        "if final_extracted_code_path and os.path.isdir(final_extracted_code_path):\n",
        "    print(f\"[OK] The extracted source code path is: {final_extracted_code_path}\")\n",
        "    print(\"\\nPartial content of the extracted directory (first 10 entries):\")\n",
        "    try:\n",
        "        content_list = os.listdir(final_extracted_code_path)\n",
        "        for item in content_list[:10]:\n",
        "            print(f\"  - {item}\")\n",
        "        if len(content_list) > 10:\n",
        "            print(\"  ...\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Errore while listing the content of {final_extracted_code_path}: {e}\")\n",
        "else:\n",
        "    print(f\"[ERROR] Unable to determine or locate the extracted code directory.\")\n",
        "    print(f\"         'final_extracted_code_path' is: {final_extracted_code_path}\")\n",
        "    print(f\"         Make sure the extraction completed successfully.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- End of Archive Extraction ---\")\n",
        "\n",
        "# Make the variable available for subsequent cells (optional but useful)\n",
        "# You may want to rename it to `extracted_code_path` if subsequent cells\n",
        "# use that specific name.\n",
        "# extracted_code_path = final_extracted_code_path\n",
        "# print(f‘\\nVariable “extracted_code_path” set to: {extracted_code_path}’)"
      ],
      "metadata": {
        "id": "kam_QeN5NX-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6e674a3-c8ea-4ca2-cdde-f14d2f6fe54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final check:\n",
            "[OK] The extracted source code path is: /content/library_sources/mnt\n",
            "\n",
            "Partial content of the extracted directory (first 10 entries):\n",
            "  - data\n",
            "\n",
            "--- End of Archive Extraction ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Configuration (pre Snippet Extraction Helpers) ---\n",
        "SAMPLE_INDEX = 0       # Index of the dataset sample to process\n",
        "MAX_KB_SIZE = 15000    # Max number of code snippets to include in the KB (to limit RAM)\n",
        "FALLBACK_ENCODING = 'iso-8859-1'  # Encoding to use if UTF-8 fails\n",
        "DRIVE_KB_SAVE_DIR = '/content/drive/MyDrive/RAG_Project/library_kbs'  # Directory to save KBs on Google Drive\n",
        "\n",
        "\n",
        "# Check the existance of the needed variables\n",
        "if 'lca_dataset_split' not in locals() or not lca_dataset_split:\n",
        "    raise NameError(\"CRITICAL ERROR: Variable 'lca_dataset_split' is not defined or is empty. Rerun the dataset loading cell.\")\n",
        "if 'final_extracted_code_path' not in locals() or not final_extracted_code_path:\n",
        "     # Fallback: try to use old name\n",
        "     if 'extracted_code_path' in locals() and extracted_code_path:\n",
        "          warnings.warn(\"Variable 'final_extracted_code_path' not found, using 'extracted_code_path' as fallback.\")\n",
        "          final_extracted_code_path = extracted_code_path\n",
        "     else:\n",
        "          raise NameError(\"CRITICAL ERROR: Variable 'final_extracted_code_path' (or 'extracted_code_path') is not defined. Rerun the archive extraction cell.\")\n",
        "if not os.path.isdir(final_extracted_code_path):\n",
        "     raise FileNotFoundError(f\"CRITICAL ERROR: The extracted code path '{final_extracted_code_path}' does not exist or is not a directory. Check the archive extraction step.\")\n",
        "\n",
        "# Actual source code path from previous cell\n",
        "library_source_dir = final_extracted_code_path\n"
      ],
      "metadata": {
        "id": "7rPF3q96fgxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Snippet Extraction Helpers ---\n",
        "\n",
        "def extract_code_units(py_file_path, fallback_encoding=FALLBACK_ENCODING):\n",
        "    \"\"\"Extracts functions and classes from a Python file as strings, with improved error handling.\"\"\"\n",
        "    units = []\n",
        "    source = None\n",
        "    encoding_used = 'utf-8'\n",
        "    try:\n",
        "        # Attempt to read with UTF-8\n",
        "        with open(py_file_path, 'r', encoding='utf-8') as file:\n",
        "            source = file.read()\n",
        "    except UnicodeDecodeError:\n",
        "        # Fallback to the specified encoding\n",
        "        encoding_used = fallback_encoding\n",
        "        try:\n",
        "            with open(py_file_path, 'r', encoding=fallback_encoding) as file:\n",
        "                source = file.read()\n",
        "            # warnings.warn(f\"Used encoding '{encoding_used}' for {py_file_path}\") # Optional: Log used encoding\n",
        "        except Exception as read_e:\n",
        "            # print(f\"  Error reading file {py_file_path} (even with {encoding_used}): {read_e}\")\n",
        "            return units # Nothing we can do if reading fails\n",
        "    except PermissionError:\n",
        "        # print(f\"  Permission error reading {py_file_path}\")\n",
        "        return units\n",
        "    except Exception as read_e:\n",
        "        # print(f\"  Unexpected error reading {py_file_path}: {read_e}\")\n",
        "        return units\n",
        "\n",
        "    # If reading succeeds, try to parse\n",
        "    if source is not None:\n",
        "        try:\n",
        "            tree = ast.parse(source, filename=py_file_path)\n",
        "            # Check availability of get_source_segment (should be available in Python 3.8+)\n",
        "            can_get_segment = hasattr(ast, 'get_source_segment')\n",
        "\n",
        "            for node in ast.walk(tree):\n",
        "                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
        "                    code_segment = None\n",
        "                    if can_get_segment:\n",
        "                        try:\n",
        "                            code_segment = ast.get_source_segment(source, node)\n",
        "                        except Exception as segment_e:\n",
        "                            # Sometimes the segment can't be extracted for complex nodes or decorators\n",
        "                            # print(f\"  Warning: Error extracting segment ({type(node).__name__}) in {py_file_path}: {segment_e}\")\n",
        "                            pass\n",
        "                    else: # Very simple fallback if get_source_segment is not available\n",
        "                        code_segment = ast.dump(node) # Not ideal, but better than nothing\n",
        "\n",
        "                    if code_segment:\n",
        "                        units.append(code_segment)\n",
        "\n",
        "        except SyntaxError as syn_e:\n",
        "            # Ignore files with Python syntax errors\n",
        "            # print(f\"  Ignored: Syntax error in {py_file_path}: {syn_e}\")\n",
        "            pass\n",
        "        except Exception as parse_e:\n",
        "            # Ignore other parsing errors\n",
        "            # print(f\"  Ignored: AST parsing error in {py_file_path}: {parse_e}\")\n",
        "            pass\n",
        "    return units\n",
        "\n",
        "def build_kb_for_library(source_path, max_kb_size=MAX_KB_SIZE, use_tqdm=USE_TQDM):\n",
        "    \"\"\"Builds the KB (list of snippets) by scanning .py files, with progress and error handling.\"\"\"\n",
        "    if not os.path.isdir(source_path):\n",
        "        print(f\"[ERROR] The provided source path is not a valid directory: {source_path}\")\n",
        "        return []\n",
        "\n",
        "    print(f\"\\nStarting library scan in: {source_path}\")\n",
        "    knowledge_base = []\n",
        "    file_count = 0\n",
        "    processed_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    # Count total .py files for tqdm (if used)\n",
        "    total_py_files = 0\n",
        "    if use_tqdm:\n",
        "        print(\"Counting .py files for progress bar...\")\n",
        "        for _, _, files in os.walk(source_path):\n",
        "            total_py_files += sum(1 for file in files if file.endswith(\".py\"))\n",
        "        print(f\"Found {total_py_files} .py files.\")\n",
        "\n",
        "    # Set up the iterator (with or without tqdm)\n",
        "    walker = os.walk(source_path, topdown=True) # topdown=True for potential dir exclusion\n",
        "    if use_tqdm:\n",
        "        pbar = tqdm(total=total_py_files, desc=\"Extracting Snippets\", unit=\"file\")\n",
        "\n",
        "    try:\n",
        "        for root, dirs, files in walker:\n",
        "            # Optional: Exclude specific directories (e.g., test, docs, build)\n",
        "            # dirs[:] = [d for d in dirs if d not in ['tests', 'test', 'docs', '__pycache__', 'build']]\n",
        "\n",
        "            for file in files:\n",
        "                if file.endswith(\".py\"):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    file_count += 1\n",
        "                    snippets = extract_code_units(file_path)\n",
        "                    if snippets:\n",
        "                        knowledge_base.extend(snippets)\n",
        "                        processed_count += 1\n",
        "                    else:\n",
        "                        skipped_count += 1 # .py file read but no snippet extracted (error or empty)\n",
        "\n",
        "                    if use_tqdm:\n",
        "                        pbar.update(1)\n",
        "                    elif file_count % 200 == 0: # Print progress less frequently without tqdm\n",
        "                        print(f\"  Processed {file_count} files...\")\n",
        "\n",
        "    except PermissionError as perm_e:\n",
        "        print(f\"\\n[ERROR] Permission error during scan of {source_path}: {perm_e}\")\n",
        "        print(\"  You may need to adjust permissions or run as a different user.\")\n",
        "    except Exception as walk_e:\n",
        "        print(f\"\\n[ERROR] Unexpected error during scan: {walk_e}\")\n",
        "    finally:\n",
        "        if use_tqdm:\n",
        "            pbar.close()\n",
        "\n",
        "    print(f\"\\nScan completed.\")\n",
        "    print(f\"  Total .py files encountered: {file_count}\")\n",
        "    print(f\"  .py files processed with snippets: {processed_count}\")\n",
        "    print(f\"  .py files skipped/with errors: {skipped_count}\")\n",
        "    print(f\"  Total snippets extracted (before sampling): {len(knowledge_base)}\")\n",
        "\n",
        "    # Sampling if the KB is too large\n",
        "    if len(knowledge_base) > max_kb_size:\n",
        "        print(f\"\\nWARNING: KB too large ({len(knowledge_base)} snippets).\")\n",
        "        print(f\"  Random sampling to keep a maximum of {max_kb_size} snippets.\")\n",
        "        knowledge_base = random.sample(knowledge_base, max_kb_size)\n",
        "        print(f\"  KB size after sampling: {len(knowledge_base)}\")\n",
        "    elif len(knowledge_base) == 0:\n",
        "        print(\"\\nWARNING: No snippet extracted from the library.\")\n",
        "        print(f\"  Check that '{source_path}' contains valid and readable .py files.\")\n",
        "\n",
        "    return knowledge_base"
      ],
      "metadata": {
        "id": "XOErR5V2ObtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. KB Creation & Persistence ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"--- Knowledge Base (KB) Creation ---\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "current_kb = []  # Initialize KB as empty\n",
        "\n",
        "try:\n",
        "    # Retrieve info from the loaded dataset\n",
        "    sample = lca_dataset_split[SAMPLE_INDEX]\n",
        "    repo_full_name = sample.get('repo_full_name')\n",
        "\n",
        "    if not repo_full_name:\n",
        "        print(f\"[ERROR] 'repo_full_name' not found in dataset sample {SAMPLE_INDEX}.\")\n",
        "    else:\n",
        "        print(f\"Processing Sample {SAMPLE_INDEX}: Library '{repo_full_name}'\")\n",
        "        print(f\"Source code path: {library_source_dir}\")\n",
        "\n",
        "        # Build the KB\n",
        "        current_kb = build_kb_for_library(library_source_dir)  # Use the improved function\n",
        "\n",
        "        # Save the KB to Drive if it's not empty\n",
        "        if current_kb:\n",
        "            # Create the save directory on Drive if it doesn't exist\n",
        "            try:\n",
        "                os.makedirs(DRIVE_KB_SAVE_DIR, exist_ok=True)\n",
        "            except OSError as drive_err:\n",
        "                print(f\"\\n[ERROR] Unable to create save directory on Drive: {DRIVE_KB_SAVE_DIR}\")\n",
        "                print(f\"  Error: {drive_err}\")\n",
        "                print(\"  KB save skipped.\")\n",
        "                # You might choose to exit or continue without saving\n",
        "                # raise drive_err  # Uncomment to stop execution\n",
        "\n",
        "            # Build the full path for the KB file\n",
        "            # Clean the repo name to avoid problematic characters in filenames\n",
        "            safe_repo_name = repo_full_name.replace('/', '__')  # Replace / with __\n",
        "            kb_filename = f\"kb_{safe_repo_name}_sample_{SAMPLE_INDEX}.json\"\n",
        "            kb_full_path = os.path.join(DRIVE_KB_SAVE_DIR, kb_filename)\n",
        "\n",
        "            print(f\"\\nAttempting to save KB ({len(current_kb)} snippets) to: {kb_full_path}\")\n",
        "            try:\n",
        "                with open(kb_full_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(current_kb, f, indent=2, ensure_ascii=False)\n",
        "                print(f\"[OK] KB successfully saved.\")\n",
        "            except OSError as save_err:\n",
        "                print(f\"\\n[ERROR] Unable to write KB file to Drive: {kb_full_path}\")\n",
        "                print(f\"  Error: {save_err}. Check write permissions on Drive.\")\n",
        "            except Exception as json_err:\n",
        "                print(f\"\\n[ERROR] Error during JSON serialization of the KB: {json_err}\")\n",
        "        else:\n",
        "            print(\"\\nKB is empty, no file saved.\")\n",
        "\n",
        "except IndexError:\n",
        "    print(f\"[ERROR] Index {SAMPLE_INDEX} out of bounds for 'lca_dataset_split' (size: {len(lca_dataset_split)}).\")\n",
        "except Exception as main_e:\n",
        "    import traceback\n",
        "    print(f\"\\n[ERROR] Unexpected error in main script:\")\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "# --- 4. Final Check ---\n",
        "if current_kb:\n",
        "    print(f\"\\n--- KB for {repo_full_name} Ready ({len(current_kb)} snippets) ---\")\n",
        "else:\n",
        "    print(f\"\\n--- KB not created or empty ---\")\n",
        "\n",
        "print(\"\\n--- End of KB Creation ---\")\n"
      ],
      "metadata": {
        "id": "W-0g5KBJhE5s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448,
          "referenced_widgets": [
            "8aa2e7f4020546cb9a65539f179aff85",
            "ba0638061e4747dbb4b94c3dcb64b018",
            "98908a3ac174498bb92773964c9e8709",
            "0b78173b54174306a1fc718af16fbda0",
            "e999e5a196794931b14c44e0166e875c",
            "fff6f7f6848940fd848d9ad10df46a91",
            "6174bbcf917045bfaf1f06e0501fc07f",
            "50ff854341f94fff88f4d0be7efd4fc6",
            "34a208afc77e4f9f991a0867a1edabf8",
            "7909630aabcb42af95d90c0a84bf8b2f",
            "b24ca3fca3e1411087165fc8694cbdbb"
          ]
        },
        "outputId": "c75a3c68-085c-43d1-d45e-cf71dcfca3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "--- Knowledge Base (KB) Creation ---\n",
            "========================================\n",
            "Processing Sample 0: Library 'seed-labs__seed-emulator'\n",
            "Source code path: /content/library_sources/mnt\n",
            "\n",
            "Starting library scan in: /content/library_sources/mnt\n",
            "Counting .py files for progress bar...\n",
            "Found 136 .py files.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting Snippets:   0%|          | 0/136 [00:00<?, ?file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8aa2e7f4020546cb9a65539f179aff85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scan completed.\n",
            "  Total .py files encountered: 136\n",
            "  .py files processed with snippets: 99\n",
            "  .py files skipped/with errors: 37\n",
            "  Total snippets extracted (before sampling): 1196\n",
            "\n",
            "Attempting to save KB (1196 snippets) to: /content/drive/MyDrive/RAG_Project/library_kbs/kb_seed-labs__seed-emulator_sample_0.json\n",
            "[OK] KB successfully saved.\n",
            "\n",
            "--- KB for seed-labs__seed-emulator Ready (1196 snippets) ---\n",
            "\n",
            "--- End of KB Creation ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 6: BM25 Retrieval & Prompt Assembly\n",
        "\n"
      ],
      "metadata": {
        "id": "RdReUhffnXGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section implements the core retrieval‑augmented generation workflow: it loads or reuses the previously built knowledge base (KB), runs a BM25 search to find the most relevant code snippets for the current instruction, and then assembles those snippets into a Snippet‑Integration‑Format (SIF) prompt ready for the LLM.\n",
        "\n",
        "1. **Configuration**  \n",
        "   Sets retrieval parameters (`TOP_K_SNIPPETS`, BM25’s `k1` and `b`) and locations (`DRIVE_KB_SAVE_DIR`) to control how many and which snippets to fetch.\n",
        "\n",
        "2. **Tokenizer Helper**  \n",
        "   Defines a simple code‑aware tokenizer (`simple_code_tokenizer`) that normalizes text, splits on punctuation/whitespace, and filters out noise—preparing both snippets and the user instruction for BM25.\n",
        "\n",
        "3. **KB Loading**  \n",
        "   Attempts to use the in‑memory `current_kb` list; if unavailable, loads the JSON file from Drive. Validates that the KB is a non‑empty list of strings before proceeding.\n",
        "\n",
        "4. **BM25 Indexing & Retrieval**  \n",
        "   - **Tokenization:** Converts all valid KB snippets into token lists.  \n",
        "   - **Index Construction:** Builds a BM25 index with the specified parameters.  \n",
        "   - **Querying:** Tokenizes the instruction, scores every snippet, and selects the top K matches.  \n",
        "   - **Preview:** Prints a shortened preview of each retrieved snippet for quick inspection.\n",
        "\n",
        "5. **SIF Prompt Creation**  \n",
        "   Defines `create_sif_prompt()`, which:  \n",
        "   - Calculates available token budget given the model’s context window.  \n",
        "   - Iteratively incorporates retrieved snippets (wrapped in fenced Python blocks), stopping when the token budget is reached.  \n",
        "   - Embeds the final instruction at the end, producing a single string ready to send to the model.  \n",
        "   - Reports estimated token usage, warns if limits are exceeded, and shows a truncated preview of the assembled prompt.\n",
        "\n",
        "By the end of this cell, you will have a ranked set of relevant code snippets and a fully formatted, token‑aware prompt that leverages those snippets to guide the LLM’s code generation.  \n"
      ],
      "metadata": {
        "id": "Scd5BYS-noOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Configuration ---\n",
        "SAMPLE_INDEX = 0      # Index of the sample to process (same as the KB cells)\n",
        "TOP_K_SNIPPETS = 5    # Number of snippets to retrieve with BM25\n",
        "BM25_K1 = 1.5         # BM25 parameter (common default, controls TF saturation)\n",
        "BM25_B = 0.75         # BM25 parameter (common default, controls document length)\n",
        "DRIVE_KB_SAVE_DIR = '/content/drive/MyDrive/RAG_Project/library_kbs' # KB folder on Drive\n",
        "\n",
        "# --- 2. Tokenizer Helper ---\n",
        "def simple_code_tokenizer(text):\n",
        "    \"\"\"\n",
        "    Simple tokenizer optimized for code snippets:\n",
        "    - lowercase\n",
        "    - split on spaces and common punctuation (keeping underscores)\n",
        "    - optionally removes very short tokens\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):  # Handles non-string input\n",
        "        return []\n",
        "    text = text.lower()\n",
        "    # Replace non-alphanumeric or underscore characters with space\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # Split on multiple spaces\n",
        "    tokens = text.split()\n",
        "    # Optional: remove very short tokens (e.g., length 1), they might be noise\n",
        "    # tokens = [token for token in tokens if len(token) > 1]\n",
        "    return tokens\n",
        "\n",
        "# --- 3. KB Loading ---\n",
        "\n",
        "print(\"--- Retrieval with BM25 ---\")\n",
        "kb_data = None  # Initialize KB\n",
        "\n",
        "# Check required variables from previous cells\n",
        "if 'lca_dataset_split' not in locals() or not lca_dataset_split:\n",
        "    raise NameError(\"CRITICAL ERROR: 'lca_dataset_split' not defined or empty. Re-run the dataset loading cell.\")\n",
        "\n",
        "# Try using KB already in memory ('current_kb' from the previous cell)\n",
        "# Check that it exists, is a list, and is not empty\n",
        "if 'current_kb' in locals() and isinstance(current_kb, list) and current_kb:\n",
        "    print(\"Using in-memory KB ('current_kb').\")\n",
        "    kb_data = current_kb\n",
        "else:\n",
        "    # If current_kb is not valid, try loading from Drive\n",
        "    print(\"\\n'current_kb' not available or empty in memory.\")\n",
        "    try:\n",
        "        # Determine KB file name (requires repo_full_name)\n",
        "        sample = lca_dataset_split[SAMPLE_INDEX]\n",
        "        repo_full_name_for_kb = sample.get('repo_full_name')\n",
        "        if not repo_full_name_for_kb:\n",
        "            print(f\"[ERROR] 'repo_full_name' not found in sample {SAMPLE_INDEX} to load KB.\")\n",
        "        else:\n",
        "            # Clean repo name and build path\n",
        "            safe_repo_name = repo_full_name_for_kb.replace('/', '__')\n",
        "            kb_filename = f\"kb_{safe_repo_name}_sample_{SAMPLE_INDEX}.json\"\n",
        "            kb_full_path = os.path.join(DRIVE_KB_SAVE_DIR, kb_filename)\n",
        "\n",
        "            if os.path.exists(kb_full_path):\n",
        "                print(f\"Attempting to load KB from Drive: {kb_full_path}\")\n",
        "                with open(kb_full_path, 'r', encoding='utf-8') as f:\n",
        "                    kb_data = json.load(f)\n",
        "                # Additional check: is the loaded file a non-empty list?\n",
        "                if isinstance(kb_data, list) and kb_data:\n",
        "                    print(f\"KB for '{repo_full_name_for_kb}' loaded from Drive ({len(kb_data)} snippets).\")\n",
        "                else:\n",
        "                    print(f\"[ERROR] KB file loaded from '{kb_full_path}' is not a valid list or is empty.\")\n",
        "                    kb_data = None  # Reset if content is invalid\n",
        "            else:\n",
        "                print(f\"[ERROR] KB file not found at: {kb_full_path}\")\n",
        "\n",
        "    except IndexError:\n",
        "        print(f\"[ERROR] Invalid index {SAMPLE_INDEX} for 'lca_dataset_split' when retrieving repo name.\")\n",
        "    except FileNotFoundError:  # If DRIVE_KB_SAVE_DIR does not exist\n",
        "        print(f\"[ERROR] KB directory on Drive not found: {DRIVE_KB_SAVE_DIR}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Unexpected error while loading KB from Drive: {e}\")\n",
        "        kb_data = None  # Ensure None in case of error\n",
        "\n",
        "# If kb_data is still not loaded, exit with a clear error\n",
        "if not kb_data:\n",
        "    raise RuntimeError(\"CRITICAL ERROR: Unable to obtain Knowledge Base (KB) data, neither from memory nor Drive. \"\n",
        "                       \"Run Step 2.B cell first to create/save the KB.\")\n",
        "\n",
        "# --- 4. BM25 Indexing & Retrieval ---\n",
        "retrieved_snippets_bm25 = []  # Initialize results list\n",
        "\n",
        "try:\n",
        "    # Extract instruction and repo name (reuse sample if previously loaded)\n",
        "    if 'sample' not in locals() or sample is None:  # Load sample if not already loaded\n",
        "        sample = lca_dataset_split[SAMPLE_INDEX]\n",
        "    instruction = sample.get('instruction')\n",
        "    repo_full_name = sample.get('repo_full_name', 'N/A')  # Use N/A if missing\n",
        "\n",
        "    if not instruction:\n",
        "        print(\"[ERROR] Instruction (query) not found in the sample.\")\n",
        "    else:\n",
        "        print(f\"\\n--- Running BM25 for Sample {SAMPLE_INDEX} (Library: {repo_full_name}) ---\")\n",
        "        print(f\"Instruction (Query): {instruction[:250]}...\")  # Show a bit more of the query\n",
        "\n",
        "        # 4.1 Tokenize the Knowledge Base (ensure snippets are strings)\n",
        "        print(\"\\nTokenizing Knowledge Base...\")\n",
        "        valid_kb_docs = [doc for doc in kb_data if isinstance(doc, str) and doc.strip()]\n",
        "        if len(valid_kb_docs) < len(kb_data):\n",
        "            print(f\"  Warning: {len(kb_data) - len(valid_kb_docs)} invalid snippets (non-strings/empty) ignored.\")\n",
        "\n",
        "        if not valid_kb_docs:\n",
        "            print(\"[ERROR] No valid snippets found in the KB after cleaning.\")\n",
        "        else:\n",
        "            tokenized_kb = [simple_code_tokenizer(doc) for doc in valid_kb_docs]\n",
        "            # Remove any empty lists resulting from tokenization\n",
        "            tokenized_kb_filtered = [tokens for tokens in tokenized_kb if tokens]\n",
        "            if not tokenized_kb_filtered:\n",
        "                print(\"[ERROR] Tokenized KB is empty after removing empty tokens.\")\n",
        "            else:\n",
        "                original_indices = [i for i, tokens in enumerate(tokenized_kb) if tokens]  # Original indices of valid docs\n",
        "                print(f\"Tokenized KB ({len(tokenized_kb_filtered)} valid documents).\")\n",
        "\n",
        "                # 4.2 Create the BM25 index with configured parameters\n",
        "                print(f\"Creating BM25 index (k1={BM25_K1}, b={BM25_B})...\")\n",
        "                bm25 = BM25Okapi(tokenized_kb_filtered, k1=BM25_K1, b=BM25_B)\n",
        "                print(\"BM25 index created.\")\n",
        "\n",
        "                # 4.3 Tokenize the instruction (query)\n",
        "                print(\"Tokenizing instruction (query)...\")\n",
        "                tokenized_query = simple_code_tokenizer(instruction)\n",
        "                if not tokenized_query:\n",
        "                    print(\"[ERROR] Tokenized query is empty.\")\n",
        "                else:\n",
        "                    # 4.4 Perform the retrieval\n",
        "                    print(f\"Retrieving top {TOP_K_SNIPPETS} relevant snippets...\")\n",
        "                    scores = bm25.get_scores(tokenized_query)\n",
        "                    top_n_filtered_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:TOP_K_SNIPPETS]\n",
        "                    retrieved_snippets_bm25 = [\n",
        "                        valid_kb_docs[original_indices[i]] for i in top_n_filtered_indices if i < len(original_indices)\n",
        "                    ]\n",
        "\n",
        "                    print(f\"\\n--- Top {len(retrieved_snippets_bm25)} Snippets Retrieved (BM25) ---\")\n",
        "                    if retrieved_snippets_bm25:\n",
        "                        for i, snippet in enumerate(retrieved_snippets_bm25):\n",
        "                            print(f\"\\n--- Snippet {i+1} (BM25 Rank {i+1}) ---\")\n",
        "                            snippet_preview = textwrap.shorten(\n",
        "                                snippet.strip(),\n",
        "                                width=120,\n",
        "                                placeholder=f\" ... (total length: {len(snippet)} characters)\"\n",
        "                            )\n",
        "                            print(snippet_preview)\n",
        "                    else:\n",
        "                        print(\"No snippets retrieved.\")\n",
        "\n",
        "except IndexError:\n",
        "    print(f\"[ERROR] Invalid index {SAMPLE_INDEX} for 'lca_dataset_split'.\")\n",
        "except Exception as main_e:\n",
        "    import traceback\n",
        "    print(f\"\\n[ERROR] Unexpected error in BM25 main script:\")\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "if retrieved_snippets_bm25:\n",
        "    print(f\"\\n--- [OK] Retrieved {len(retrieved_snippets_bm25)} BM25 snippets ---\")\n",
        "    # The variable 'retrieved_snippets_bm25' contains the list of strings\n",
        "else:\n",
        "    print(f\"\\n--- [WARNING/ERROR] No snippets retrieved from BM25 ---\")"
      ],
      "metadata": {
        "id": "2OiiDW95RlXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc79c58b-a751-454a-9ccc-83e80879e8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Retrieval with BM25 ---\n",
            "Using in-memory KB ('current_kb').\n",
            "\n",
            "--- Running BM25 for Sample 0 (Library: seed-labs__seed-emulator) ---\n",
            "Instruction (Query): Generate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n",
            "\n",
            "The base layer should create multiple autonomous systems an...\n",
            "\n",
            "Tokenizing Knowledge Base...\n",
            "Tokenized KB (1196 valid documents).\n",
            "Creating BM25 index (k1=1.5, b=0.75)...\n",
            "BM25 index created.\n",
            "Tokenizing instruction (query)...\n",
            "Retrieving top 5 relevant snippets...\n",
            "\n",
            "--- Top 5 Snippets Retrieved (BM25) ---\n",
            "\n",
            "--- Snippet 1 (BM25 Rank 1) ---\n",
            "def makeStubAs(emu: Emulator, base: Base, asn: int, exchange: int, services: ... (total length: 895 characters)\n",
            "\n",
            "--- Snippet 2 (BM25 Rank 2) ---\n",
            "def __init__( self, onAsConflict: Callable[[AutonomousSystem, AutonomousSystem], ... (total length: 1088 characters)\n",
            "\n",
            "--- Snippet 3 (BM25 Rank 3) ---\n",
            "def install(self, vnode: str) -> Server: assert False, 'ReverseDomainNameService is ... (total length: 240 characters)\n",
            "\n",
            "--- Snippet 4 (BM25 Rank 4) ---\n",
            "def _doInstall(self, node: Node, server: Server): assert False, 'CymruIpOriginService ... (total length: 243 characters)\n",
            "\n",
            "--- Snippet 5 (BM25 Rank 5) ---\n",
            "def getInternetExchangeMembers(self, id: int) -> Dict[int, str]: \"\"\"! @brief Get ... (total length: 484 characters)\n",
            "\n",
            "--- [OK] Retrieved 5 BM25 snippets ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. SIF Prompt Creation ---\n",
        "\n",
        "# --- Constants and Configurations (Optional but good practice) ---\n",
        "# Conservative estimate of tokens for the fixed prompt structure\n",
        "# (You can calculate it more precisely later with your tokenizer)\n",
        "# Safety margin to avoid hitting the limit exactly\n",
        "PROMPT_TEMPLATE_BASE_TOKENS = 100\n",
        "TOKEN_LIMIT_MARGIN = 50\n",
        "\n",
        "def create_sif_prompt(\n",
        "    instruction: str,                          # Original instruction\n",
        "    retrieved_snippets: list[str],            # List of retrieved snippets (from BM25 or similar)\n",
        "    tokenizer,                                # Loaded Hugging Face tokenizer instance\n",
        "    max_prompt_tokens: int = 3500,            # Maximum tokens for the entire prompt\n",
        "    # model_max_length: Optional[int] = None  # Optional: Model max length (if different)\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Creates a SIF (Snippet Integration Format) prompt optimized for an LLM.\n",
        "\n",
        "    Integrates retrieved snippets as context for code generation based on the given instruction,\n",
        "    handling tokenization and truncation.\n",
        "\n",
        "    Args:\n",
        "        instruction: The user's instruction.\n",
        "        retrieved_snippets: List of strings containing the retrieved code snippets.\n",
        "        tokenizer: The initialized Hugging Face tokenizer instance.\n",
        "        max_prompt_tokens: The approximate maximum tokens allowed for the final prompt.\n",
        "                           (Considers the LLM's context window minus the tokens for the response).\n",
        "        # model_max_length: Optional: The model's absolute max length, if known and different\n",
        "        #                   from tokenizer.model_max_length.\n",
        "\n",
        "    Returns:\n",
        "        The formatted prompt string ready to be passed to the LLM.\n",
        "        Returns an empty string if the instruction is missing.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If tokenizer is not provided or is invalid.\n",
        "        ValueError: If max_prompt_tokens is not a positive integer.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(instruction, str) or not instruction.strip():\n",
        "        warnings.warn(\"Missing or empty instruction; returning an empty prompt.\")\n",
        "        return \"\"\n",
        "    if tokenizer is None or not hasattr(tokenizer, 'encode'):\n",
        "        raise TypeError(\"A valid Hugging Face tokenizer is required for create_sif_prompt.\")\n",
        "    if not isinstance(max_prompt_tokens, int) or max_prompt_tokens <= 0:\n",
        "        raise ValueError(\"max_prompt_tokens must be a positive integer.\")\n",
        "\n",
        "    # Determine the effective context limit of the model, if available\n",
        "    effective_model_max_length = getattr(tokenizer, 'model_max_length', None)\n",
        "    if effective_model_max_length and max_prompt_tokens > effective_model_max_length:\n",
        "        warnings.warn(\n",
        "            f\"max_prompt_tokens ({max_prompt_tokens}) exceeds the model's maximum length\"\n",
        "            f\" ({effective_model_max_length}). The model limit will take precedence\"\n",
        "        )\n",
        "\n",
        "    # --- Improved Prompt Template ---\n",
        "    prompt_template = \"\"\"SYSTEM: You are an expert Python programmer. Generate Python code based ONLY on the user's instruction, using the provided library code snippets for context and correct API usage. Adapt snippets as needed; do not copy them verbatim unless requested.\n",
        "\n",
        "USER:\n",
        "### Context: Relevant Code Snippets from Library\n",
        "\n",
        "{snippets_section}\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "ASSISTANT:\n",
        "```python\n",
        "\"\"\"\n",
        "\n",
        "    # --- End of Template ---\n",
        "\n",
        "    # --- Calculating Available Space for Snippets ---\n",
        "    # Tokenize instruction and base template to know how much space remains\n",
        "    # Use add_special_tokens=False to count only content tokens\n",
        "    instruction_tokens = len(tokenizer(instruction, add_special_tokens=False).input_ids)\n",
        "    template_base_formatted = prompt_template.format(snippets_section=\"\", instruction=\"\")\n",
        "    template_base_tokens = len(tokenizer(template_base_formatted, add_special_tokens=False).input_ids)\n",
        "\n",
        "    available_tokens_for_snippets = max(\n",
        "        0,\n",
        "        max_prompt_tokens\n",
        "        - instruction_tokens\n",
        "        - template_base_tokens\n",
        "        - TOKEN_LIMIT_MARGIN\n",
        "    )\n",
        "    print(f\"Token calculation: Total max={max_prompt_tokens}, Instruction={instruction_tokens}, Base template={template_base_tokens}\")\n",
        "    print(f\"Available tokens for snippets (approx): {available_tokens_for_snippets}\")\n",
        "\n",
        "    # --- Constructing Snippet Section with Token Checks ---\n",
        "    snippets_text_parts = []\n",
        "    accumulated_snippet_tokens = 0\n",
        "    snippets_included_count = 0\n",
        "\n",
        "    if not retrieved_snippets:\n",
        "        warnings.warn(\"No snippets provided to create_sif_prompt.\")\n",
        "\n",
        "    for i, snippet in enumerate(retrieved_snippets):\n",
        "        if not isinstance(snippet, str) or not snippet.strip():\n",
        "            continue\n",
        "\n",
        "        snippet_header = f\"# --- Snippet {i+1} ---\\n\"\n",
        "        snippet_content = snippet.strip().strip('`')\n",
        "        if not snippet_content:\n",
        "            continue\n",
        "        snippet_formatted = f\"```python\\n{snippet_content}\\n```\\n\\n\"\n",
        "\n",
        "        # Estimate tokens for this snippet (header + formatted code)\n",
        "        current_snippet_section_tokens = len(\n",
        "            tokenizer(snippet_header + snippet_formatted, add_special_tokens=False).input_ids\n",
        "        )\n",
        "\n",
        "        # Check if adding this snippet exceeds available space\n",
        "        if accumulated_snippet_tokens + current_snippet_section_tokens > available_tokens_for_snippets:\n",
        "            print(\n",
        "                f\"INFO: Token limit for snippets ({available_tokens_for_snippets}) reached. \"\n",
        "                f\"Snippet {i+1} and subsequent ones skipped.\"\n",
        "            )\n",
        "            break\n",
        "\n",
        "        # Add snippet to the prompt\n",
        "        snippets_text_parts.append(snippet_header)\n",
        "        snippets_text_parts.append(snippet_formatted)\n",
        "        accumulated_snippet_tokens += current_snippet_section_tokens\n",
        "        snippets_included_count += 1\n",
        "\n",
        "    # Assemble final snippet section\n",
        "    if snippets_included_count > 0:\n",
        "        snippets_section_content = \"\".join(snippets_text_parts).strip()\n",
        "    else:\n",
        "        snippets_section_content = \"# (No relevant snippets provided or all exceeded token limit)\"\n",
        "\n",
        "    # --- Composing Final Prompt ---\n",
        "    final_prompt = prompt_template.format(\n",
        "        snippets_section=snippets_section_content,\n",
        "        instruction=instruction\n",
        "    )\n",
        "\n",
        "    # --- Final Length Check (Optional but Useful) ---\n",
        "    final_token_count = len(\n",
        "        tokenizer(final_prompt, add_special_tokens=False).input_ids\n",
        "    )\n",
        "    print(f\"\\nPrompt SIF created.\")\n",
        "    print(f\"  Snippets included: {snippets_included_count} / {len(retrieved_snippets)}\")\n",
        "    print(f\"  Estimated length (content only): {final_token_count} tokens (Limit set: {max_prompt_tokens})\")\n",
        "\n",
        "    if effective_model_max_length and final_token_count > effective_model_max_length:\n",
        "        warnings.warn(\n",
        "            f\"The final prompt ({final_token_count} tokens) EXCEEDS the model's maximum length\"\n",
        "            f\" ({effective_model_max_length}). It may be truncated or cause errors.\"\n",
        "        )\n",
        "    elif final_token_count > max_prompt_tokens:\n",
        "        warnings.warn(\n",
        "            f\"The final prompt ({final_token_count} tokens) EXCEEDS the 'max_prompt_tokens' limit\"\n",
        "            f\" ({max_prompt_tokens}). The token estimate may be inaccurate.\"\n",
        "        )\n",
        "\n",
        "    return final_prompt\n",
        "\n",
        "# --- Example Usage (Modified to use correct variable) ---\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"--- Step 4: Creating RAG Prompt (SIF) ---\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "sif_prompt_final = None\n",
        "\n",
        "if ('instruction' in locals() and instruction and\n",
        "    'retrieved_snippets_bm25' in locals() and isinstance(retrieved_snippets_bm25, list) and\n",
        "    'tokenizer' in locals() and tokenizer):\n",
        "\n",
        "    prompt_token_limit = 3500\n",
        "    print(f\"Creating SIF prompt with max {prompt_token_limit} tokens...\")\n",
        "    sif_prompt_final = create_sif_prompt(\n",
        "        instruction=instruction,\n",
        "        retrieved_snippets=retrieved_snippets_bm25,\n",
        "        tokenizer=tokenizer,\n",
        "        max_prompt_tokens=prompt_token_limit\n",
        "    )\n",
        "\n",
        "    if sif_prompt_final:\n",
        "        print(\"\\n--- Preview of Final SIF Prompt (start) ---\")\n",
        "        # Usa textwrap.shorten per la preview\n",
        "        print(textwrap.shorten(sif_prompt_final, width=1500, placeholder=\" [...]\\n```python\\n\")) # show the beginning\n",
        "    else:\n",
        "         print(\"[ERROR] Failed to create the SIF prompt (returned empty).\")\n",
        "\n",
        "else:\n",
        "    missing_vars = []\n",
        "    if 'instruction' not in locals() or not instruction: missing_vars.append(\"'instruction'\")\n",
        "    if 'retrieved_snippets_bm25' not in locals() or not isinstance(retrieved_snippets_bm25, list): missing_vars.append(\"'retrieved_snippets_bm25' (BM25 list)\")\n",
        "    if 'tokenizer' not in locals() or not tokenizer: missing_vars.append(\"'tokenizer'\")\n",
        "    print(f\"[ERROR] Cannot create SIF prompt. Missing or invalid variables: {', '.join(missing_vars)}.\")\n",
        "    print(\"         Ensure the previous cells (dataset loading, BM25, tokenizer load) ran correctly.\")\n",
        "\n",
        "print(\"\\n--- End of SIF Prompt Creation ---\")"
      ],
      "metadata": {
        "id": "yg6MuIrrSaf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c04d0b-57b7-4b0e-96f8-97cb12ad65cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "--- Step 4: Creating RAG Prompt (SIF) ---\n",
            "========================================\n",
            "Creating SIF prompt with max 3500 tokens...\n",
            "Token calculation: Total max=3500, Instruction=158, Base template=69\n",
            "Available tokens for snippets (approx): 3223\n",
            "\n",
            "Prompt SIF created.\n",
            "  Snippets included: 5 / 5\n",
            "  Estimated length (content only): 959 tokens (Limit set: 3500)\n",
            "\n",
            "--- Preview of Final SIF Prompt (start) ---\n",
            "SYSTEM: You are an expert Python programmer. Generate Python code based ONLY on the user's instruction, using the provided library code snippets for context and correct API usage. Adapt snippets as needed; do not copy them verbatim unless requested. USER: ### Context: Relevant Code Snippets from Library # --- Snippet 1 --- ```python def makeStubAs(emu: Emulator, base: Base, asn: int, exchange: int, services: List[Service]): \"\"\"! @brief create a new stub AS. @param emu reference to the Emulator object. @param base reference to the base layer. @param asn ASN for the newly created AS. @param exchange IXP ID for new newly created AS to join. @param list of instances of Service to install on hosts. One host will be created for each. \"\"\" # Create AS and internal network stub_as = base.createAutonomousSystem(asn) stub_as.createNetwork('net0') # Create a BGP router # Attach the router to both the internal and external networks router = stub_as.createRouter('router0') router.joinNetwork('net0') router.joinNetwork('ix{}'.format(exchange)) # Create a host node for each specified service createHostsOnNetwork(emu, stub_as, 'net0', services) ``` # --- Snippet 2 --- ```python def __init__( self, onAsConflict: Callable[[AutonomousSystem, AutonomousSystem], AutonomousSystem] = lambda asA, asB: asA, onIxConflict: Callable[[InternetExchange, InternetExchange], InternetExchange] = lambda ixA, ixB: ixA): \"\"\"! @brief DefaultBaseMerger constructor. @param onAsConflict AS conflict [...]\n",
            "```python\n",
            "\n",
            "\n",
            "--- End of SIF Prompt Creation ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 7 · RAG Code Generation and Output Processing\n",
        "\n"
      ],
      "metadata": {
        "id": "51QOGcjsmXaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the point where the **Retrieval-Augmented Generation (RAG)** pipeline actually **creates new code** from the prompt assembled in the previous steps.\n",
        "\n",
        "\n",
        "\n",
        " 1 · Generation Setup\n",
        "* **Configuration parameters**  \n",
        "  * `MAX_NEW_TOKENS` – upper bound on tokens the model may produce.  \n",
        "  * `TEMPERATURE` – randomness / creativity; lower → more deterministic.  \n",
        "  * `TOP_P`, `TOP_K` – nucleus & top-k sampling thresholds.  \n",
        "  * `REPETITION_PENALTY` – discourages verbatim repetition.  \n",
        "  * `STOP_ON_EOS`, `STOP_ON_CODE_END` – early-stop on `<eos>` or when the model closes a ``` code block.\n",
        "* **Advanced stopping criterion**  \n",
        "  A custom `EosAndCodeStopCriteria` halts decoding as soon as either condition is met, preventing endless or irrelevant output.\n",
        "* **(Optional) Forced decoder IDs**  \n",
        "  A commented stub shows how you could force the model to start with a token such as `<think>` to steer generation, but it is **disabled by default**.\n",
        "\n",
        "\n",
        "2 · Input & Generation\n",
        "1. **Dependency check** – verifies that `sif_prompt_final`, `model`, and `tokenizer` are present; otherwise the cell aborts with a clear error.\n",
        "2. **Tokenisation** – the prompt is converted into IDs the model understands and moved to the correct device (GPU/CPU).\n",
        "3. **Code generation** – `model.generate()` is called with the chosen decoding hyper-parameters and (optionally) the custom stopping list.\n",
        "4. **Decoding & cleanup**  \n",
        "   * Newly generated tokens are decoded back to text.  \n",
        "   * A regex extracts the first ```python …``` block (or a fallback slice) so only the **relevant code** is returned.\n",
        "\n",
        "\n",
        " 3 · Error Handling\n",
        "* **Out-of-memory (OOM)** – catches `torch.cuda.OutOfMemoryError`, prints advice on shrinking prompt or `MAX_NEW_TOKENS`.\n",
        "* **Unexpected exceptions** – a generic `try/except` prints the full traceback for rapid debugging.\n",
        "\n",
        "\n",
        "4 · Final Verification\n",
        "* If `generated_code_rag` **contains code**, the cell announces success.  \n",
        "* Otherwise it flags a failure, pointing back to missing dependencies or runtime errors.\n",
        "\n",
        "\n",
        "**In short:** this section feeds the prepared RAG prompt to the language model, retrieves the fresh code it produces, sanitises the output, and robustly reports any issues encountered along the way.\n"
      ],
      "metadata": {
        "id": "_pB9wPXLpOMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList, LogitsProcessor, LogitsProcessorList\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "# --- 1. Generation Setup ---\n",
        "# --- 1.1. Configuration Parameters ---\n",
        "MAX_NEW_TOKENS = 1024      # Max tokens to generate for the response\n",
        "TEMPERATURE = 0.6          # Recommended value for R1-Distill (0.5-0.7). Lower = more deterministic\n",
        "TOP_P = 0.95               # Nucleus sampling (considers only tokens whose cumulative probability > top_p)\n",
        "TOP_K = 50                 # Top-k sampling (considers only the top k most probable tokens)\n",
        "REPETITION_PENALTY = 1.1   # Slightly penalize already generated tokens (e.g., 1.1-1.2) to reduce repetition\n",
        "DO_SAMPLE = True           # Enable sampling (True to use temp/top_p/top_k, False for greedy/deterministic)\n",
        "STOP_ON_EOS = True         # Stop generation if the EOS token is generated\n",
        "STOP_ON_CODE_END = True    # Attempt to stop after the end of a code block (e.g. ```)\n",
        "\n",
        "# --- 1.2. Advanced Stopping Criteria (Optional but Recommended) ---\n",
        "# Combines EOS stop and, optionally, code block ending\n",
        "\n",
        "class EosAndCodeStopCriteria(StoppingCriteria):\n",
        "    def __init__(self, tokenizer, stop_on_eos=True, stop_sequence=\"\\n```\\n\"):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.stop_on_eos = stop_on_eos\n",
        "        self.stop_sequence = stop_sequence\n",
        "        self.stop_sequence_ids = self.tokenizer.encode(stop_sequence, add_special_tokens=False)\n",
        "        # Remove any unwanted leading/trailing tokens from the stop sequence\n",
        "        # (e.g., if encode adds BOS) - may require tokenizer-specific debugging\n",
        "        print(f\"Stopping sequence: '{self.stop_sequence}' -> IDs: {self.stop_sequence_ids}\")\n",
        "        print(f\"Stop on EOS ({self.tokenizer.eos_token_id}): {self.stop_on_eos}\")\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        # 1. Check EOS\n",
        "        if self.stop_on_eos and (input_ids[0, -1] == self.tokenizer.eos_token_id):\n",
        "            print(\"Stopping criteria: EOS token detected.\")\n",
        "            return True\n",
        "\n",
        "        # 2. Check the stop sequence (e.g., \\n```\\n)\n",
        "        if self.stop_sequence_ids:\n",
        "             # Check if the last N tokens match the stop sequence\n",
        "             len_stop_seq = len(self.stop_sequence_ids)\n",
        "             if input_ids.shape[1] >= len_stop_seq:\n",
        "                  last_tokens = input_ids[0, -len_stop_seq:]\n",
        "                  if torch.equal(last_tokens, torch.tensor(self.stop_sequence_ids).to(last_tokens.device)):\n",
        "                      print(f\"Stopping criteria: Stop sequence '{self.stop_sequence}' detected.\")\n",
        "                      return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria_list = None\n",
        "if STOP_ON_EOS or STOP_ON_CODE_END:\n",
        "     try:\n",
        "         custom_stopper = EosAndCodeStopCriteria(\n",
        "             tokenizer,\n",
        "             stop_on_eos=STOP_ON_EOS,\n",
        "             stop_sequence=\"\\n```\\n\" if STOP_ON_CODE_END else None # Use \\n```\\n as the code stop sequence\n",
        "         )\n",
        "         stopping_criteria_list = StoppingCriteriaList([custom_stopper])\n",
        "         print(\"Custom StoppingCriteria created\")\n",
        "     except Exception as e:\n",
        "          print(f\"WARNING: Unable to create custom StoppingCriteria: {e}\")\n",
        "\n",
        "# --- 1.3. (Optional) Forced Decoder IDs to start with <think> ---\n",
        "# According to R1-Distill recommendations. Basic implementation:\n",
        "# think_token_sequence = tokenizer.encode(\"<think>\\n\", add_special_tokens=False)\n",
        "# force_think_processor = LogitsProcessorList([\n",
        "#     ForcedBOSTokenLogitsProcessor(think_token_sequence[0]),  # Force the first token\n",
        "#     ForcedEOSTokenLogitsProcessor(max_length=MAX_NEW_TOKENS + len(think_token_sequence), eos_token_id=think_token_sequence[1:])  # Force the rest if necessary\n",
        "# ])\n",
        "# This part is complex and may require tokenizer-specific adjustments.\n",
        "# For now we omit it and rely on manually adding it to the prompt if needed.\n",
        "\n",
        "# --- 2. Input & Generation ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"--- Step 5: RAG Code Generation ---\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "generated_code_rag = None # Initialize output\n",
        "\n",
        "# Check dependencies\n",
        "if 'sif_prompt_final' in locals() and sif_prompt_final and \\\n",
        "   'model' in locals() and model and \\\n",
        "   'tokenizer' in locals() and tokenizer:\n",
        "\n",
        "    print(f\"SIF prompt received (length: {len(sif_prompt_final)} chars).\")\n",
        "    print(\"Generation parameters:\")\n",
        "    print(f\"  max_new_tokens={MAX_NEW_TOKENS}, temperature={TEMPERATURE if DO_SAMPLE else 'N/A (Greedy)'}\")\n",
        "    print(f\"  top_p={TOP_P if DO_SAMPLE else 'N/A'}, top_k={TOP_K if DO_SAMPLE else 'N/A'}\")\n",
        "    print(f\"  repetition_penalty={REPETITION_PENALTY}\")\n",
        "    print(f\"  do_sample={DO_SAMPLE}\")\n",
        "    print(f\"  Stopping Criteria: {'Active' if stopping_criteria_list else 'Inactive'}\")\n",
        "\n",
        "    try:\n",
        "        # --- Tokenization ---\n",
        "        print(\"\\nTokenizing SIF prompt...\")\n",
        "        # No need to truncate here if create_sif_prompt already handled limits\n",
        "        # max_length = tokenizer.model_max_length  # Model maximum length\n",
        "        inputs = tokenizer(\n",
        "            sif_prompt_final,\n",
        "            return_tensors=\"pt\",\n",
        "            # truncation=True,  # Enable only if strictly necessary\n",
        "            # max_length=max_length - MAX_NEW_TOKENS  # Leave room for generation\n",
        "        ).to(model.device)  # Move to GPU\n",
        "\n",
        "        input_length = inputs['input_ids'].shape[1]\n",
        "        print(f\"Tokenized input length: {input_length} tokens.\")\n",
        "\n",
        "        # --- Generation ---\n",
        "        print(\"Starting code generation...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        generation_args = {\n",
        "            \"input_ids\": inputs['input_ids'],\n",
        "            \"attention_mask\": inputs['attention_mask'],\n",
        "            \"max_new_tokens\": MAX_NEW_TOKENS,\n",
        "            \"pad_token_id\": tokenizer.eos_token_id,\n",
        "            \"repetition_penalty\": REPETITION_PENALTY,\n",
        "            \"stopping_criteria\": stopping_criteria_list  # Can be None\n",
        "        }\n",
        "        if DO_SAMPLE:\n",
        "            generation_args.update({\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"top_p\": TOP_P,\n",
        "                \"top_k\": TOP_K,\n",
        "                \"do_sample\": True,\n",
        "            })\n",
        "        else:\n",
        "            # Greedy (deterministic) generation\n",
        "            generation_args[\"do_sample\"] = False\n",
        "            # temperature, top_p, top_k are not used\n",
        "\n",
        "        with torch.no_grad():  # Essential for inference\n",
        "            # outputs = model.generate(**inputs, ...)  # Alternate way\n",
        "            outputs = model.generate(**generation_args)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Generation completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "        # --- Decode and Clean Output ---\n",
        "        # Decode only the NEW generated tokens\n",
        "        output_tokens = outputs[0, input_length:]\n",
        "        generated_code_rag_full = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "        print(\"\\n--- Generated Code (Raw) ---\")\n",
        "        print(generated_code_rag_full[:500] + \"...\" if len(generated_code_rag_full) > 500 else generated_code_rag_full)\n",
        "\n",
        "        # --- Specific Cleanup for Code Blocks ---\n",
        "        # Look for the content inside the first ```python ... ``` block\n",
        "        # This is more robust than splitting only on ```\n",
        "        code_block_match = re.search(r'```python\\n(.*?)(?:\\n```|\\Z)', generated_code_rag_full, re.DOTALL)\n",
        "        if code_block_match:\n",
        "            generated_code_rag = code_block_match.group(1).strip()\n",
        "            print(\"\\nExtracted code from the ```python ... ``` block.\")\n",
        "        else:\n",
        "            # Fallback: if it does not find ```python, take everything before a closing ```\n",
        "            # or simply take the whole output if there are no backticks.\n",
        "            if \"\\n```\" in generated_code_rag_full:  # Look for \\n``` to avoid inline matches\n",
        "                generated_code_rag = generated_code_rag_full.split(\"\\n```\")[0].strip()\n",
        "                print(\"\\n```python block not found, taking output before ```.\" )\n",
        "            else:\n",
        "                generated_code_rag = generated_code_rag_full.strip()\n",
        "                print(\"\\nNo ``` block found, taking the full output.\")\n",
        "\n",
        "        print(\"\\n--- Generated Code (Clean) ---\")\n",
        "        print(generated_code_rag)\n",
        "\n",
        "    # --- 3. Error Handling ---\n",
        "    except torch.cuda.OutOfMemoryError as e:\n",
        "        print(f\"\\n[ERROR] Out Of Memory (OOM) during GENERATION!\")\n",
        "        print(\"  The prompt plus the generated output may exceed VRAM.\")\n",
        "        print(\"  Try reducing 'max_prompt_tokens' in create_sif_prompt or 'MAX_NEW_TOKENS' here.\")\n",
        "        generated_code_rag = None\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"\\n[ERROR] Unexpected error during RAG generation:\")\n",
        "        print(traceback.format_exc())\n",
        "        generated_code_rag = None\n",
        "\n",
        "else:\n",
        "    missing = []\n",
        "    if 'sif_prompt_final' not in locals() or not sif_prompt_final: missing.append(\"'sif_prompt_final'\")\n",
        "    if 'model' not in locals() or not model: missing.append(\"'model'\")\n",
        "    if 'tokenizer' not in locals() or not tokenizer: missing.append(\"'tokenizer'\")\n",
        "    print(f\"[ERROR] Unable to perform generation. Missing or invalid variables: {', '.join(missing)}.\")\n",
        "    print(\"         Make sure the previous cells have been executed correctly.\")\n",
        "    generated_code_rag = None\n",
        "\n",
        "# --- 4. Final Verification ---\n",
        "if generated_code_rag:\n",
        "    print(\"\\n--- RAG code generation completed ---\")\n",
        "    # The variable 'generated_code_rag' contains the cleaned code\n",
        "else:\n",
        "    print(\"\\n--- [ERROR] RAG code generation failed or was not executed ---\")\n"
      ],
      "metadata": {
        "id": "Cr_UQchYSl74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "550b8cd2-8e72-4221-f039-0c5cb8c8894d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping sequence: '\n",
            "```\n",
            "' -> IDs: [198, 13874, 3989]\n",
            "Stop on EOS (151643): True\n",
            "Custom StoppingCriteria created\n",
            "\n",
            "========================================\n",
            "--- Step 5: RAG Code Generation ---\n",
            "========================================\n",
            "[ERROR] Unable to perform generation. Missing or invalid variables: 'model'.\n",
            "         Make sure the previous cells have been executed correctly.\n",
            "\n",
            "--- [ERROR] RAG code generation failed or was not executed ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 8 · Baseline Generation and RAG Comparison\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fmy1SP-zpWdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this stage we generate a **baseline**—code produced by the LLM **without any retrieval augmentation**—and\n",
        "contrast it with the RAG output obtained earlier.  \n",
        "The goal is to isolate the model’s native ability, establish a reference point, and quantify the gains introduced by\n",
        "the Retrieval-Augmented Generation pipeline.\n",
        "\n",
        "\n",
        "Procedure\n",
        "\n",
        "1. **Baseline Code Generation**  \n",
        "   1. A minimal prompt is built that contains only the user instruction.  \n",
        "   2. The prompt is tokenised and passed to the LLM with the same decoding parameters used for RAG.  \n",
        "   3. The raw output is decoded and cleaned, stripping headers, back-ticks, or other artefacts so that only executable code remains.\n",
        "\n",
        "2. **Side-by-side Evaluation**  \n",
        "   * The baseline output is compared to the RAG output generated in Section 7.  \n",
        "   * The same evaluation metrics are applied to both versions.  \n",
        "   * A comparative report highlights improvements or regressions in quality, accuracy, and completeness.\n",
        "\n",
        "Evaluation Metrics\n",
        "\n",
        "| Metric | Purpose |\n",
        "|--------|---------|\n",
        "| **BLEU** | Token-level similarity to the reference implementation. |\n",
        "| **CodeBLEU** | Code-aware score that accounts for syntax, data-flow and API usage. |\n",
        "| **Accuracy** | Functional correctness (e.g. pass/fail on test cases). |\n",
        "| **Completeness** | Whether all requested features from the instruction are implemented. |\n",
        "\n",
        "Expected Outcome\n",
        "\n",
        "We anticipate that incorporating RAG will **increase quality and accuracy** versus the standalone\n",
        "LLM.  \n",
        "Quantifying these deltas allows us to assess how effective retrieval is for the specific dataset and model used.\n",
        "Analysis & Conclusions\n",
        "\n",
        "The results table will be analysed to identify:\n",
        "\n",
        "* Scenarios where the baseline already excels (little room for RAG improvement).  \n",
        "* Cases where RAG corrects or enriches the baseline solution.  \n",
        "* Remaining weaknesses—e.g. when poor retrieval hurts generation—which inform future improvements.\n",
        "\n",
        "Notes & Caveats\n",
        "\n",
        "* This section assumes that the LLM, tokenizer, dataset, and decoding parameters were successfully initialised in earlier cells.  \n",
        "* Metric scores will vary with the dataset, the LLM architecture, and hyper-parameters chosen.  \n",
        "* Always interpret the numbers in the context of your project requirements and evaluation budget."
      ],
      "metadata": {
        "id": "RDosqCNGreTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import re       # Required for regex cleanup\n",
        "import textwrap # For prompt preview\n",
        "import warnings # To handle warnings\n",
        "\n",
        "print(\"\\n\" + \"=\" * 40)\n",
        "print(\"--- Step 6.A: Baseline Generation (LLM-only) ---\")\n",
        "print(\"=\" * 40)\n",
        "print(\"NOTE: This cell expects that 'instruction', 'model', 'tokenizer'\")\n",
        "print(\"      and the generation parameters (MAX_NEW_TOKENS, etc.) have\")\n",
        "print(\"      been defined in the previous cells (including Step 5).\")\n",
        "\n",
        "generated_code_baseline = None  # Initialize output\n",
        "\n",
        "# --- 1. Robust Dependency Check ---\n",
        "# Verify all necessary variables inherited from the previous execution\n",
        "required_vars = [\n",
        "    'instruction', 'model', 'tokenizer',\n",
        "    'MAX_NEW_TOKENS', 'TEMPERATURE', 'TOP_P',\n",
        "    'TOP_K', 'REPETITION_PENALTY', 'DO_SAMPLE'\n",
        "]\n",
        "missing_vars = []\n",
        "invalid_vars = []\n",
        "\n",
        "for var_name in required_vars:\n",
        "    if var_name not in locals():\n",
        "        missing_vars.append(f\"'{var_name}'\")\n",
        "    # Also check that they are not None or empty (where applicable)\n",
        "    elif var_name in ['instruction', 'model', 'tokenizer'] and not locals()[var_name]:\n",
        "        invalid_vars.append(f\"'{var_name}' (is None or empty)\")\n",
        "\n",
        "# Also verify the stopping criteria (optional, but if it exists it must be used)\n",
        "# If it doesn't exist from the previous cell, it will be set to None later\n",
        "stopping_criteria_to_use = locals().get('stopping_criteria_list', None)\n",
        "\n",
        "# --- 2. Proceed only if all dependencies are OK ---\n",
        "if not missing_vars and not invalid_vars:\n",
        "\n",
        "    print(\"\\nAll required variables were found.\")\n",
        "\n",
        "    # --- 3. Baseline Prompt Creation ---\n",
        "    # Use the same prompt structure for consistency (even if simple)\n",
        "    baseline_prompt = f\"\"\"USER:\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "ASSISTANT:\n",
        "```python\n",
        "\"\"\"\n",
        "    # Do not print the entire prompt if it is very long\n",
        "    print(\"\\nBaseline Prompt (start):\")\n",
        "    print(textwrap.shorten(baseline_prompt, width=1200, placeholder=\"...```python\\n\"))\n",
        "\n",
        "    # --- 4. Code Generation ---\n",
        "    print(f\"\\nUsing the SAME parameters inherited from the RAG generation:\")\n",
        "    print(f\"  max_new_tokens={MAX_NEW_TOKENS}, temperature={TEMPERATURE if DO_SAMPLE else 'N/A (Greedy)'}\")\n",
        "    print(f\"  top_p={TOP_P if DO_SAMPLE else 'N/A'}, top_k={TOP_K if DO_SAMPLE else 'N/A'}\")\n",
        "    print(f\"  repetition_penalty={REPETITION_PENALTY}\")\n",
        "    print(f\"  do_sample={DO_SAMPLE}\")\n",
        "    print(f\"  Stopping Criteria: {'Active' if stopping_criteria_to_use else 'Inactive'}\")\n",
        "\n",
        "    try:\n",
        "        # --- Tokenization ---\n",
        "        inputs_base = tokenizer(baseline_prompt, return_tensors=\"pt\").to(model.device)\n",
        "        input_length_base = inputs_base['input_ids'].shape[1]\n",
        "        print(f\"\\nTokenized input length: {input_length_base} tokens.\")\n",
        "\n",
        "        # --- model.generate call (Same as RAG except for the input) ---\n",
        "        print(\"Starting Baseline generation...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        generation_args_base = {\n",
        "            \"input_ids\": inputs_base['input_ids'],\n",
        "            \"attention_mask\": inputs_base['attention_mask'],\n",
        "            \"max_new_tokens\": MAX_NEW_TOKENS,\n",
        "            \"pad_token_id\": tokenizer.eos_token_id,\n",
        "            \"repetition_penalty\": REPETITION_PENALTY,\n",
        "            \"stopping_criteria\": stopping_criteria_to_use  # Use the same one from RAG (can be None)\n",
        "        }\n",
        "        if DO_SAMPLE:\n",
        "            generation_args_base.update({\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"top_p\": TOP_P,\n",
        "                \"top_k\": TOP_K,\n",
        "                \"do_sample\": True,\n",
        "            })\n",
        "        else:\n",
        "            generation_args_base[\"do_sample\"] = False\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_base = model.generate(**generation_args_base)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Baseline generation completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "        # --- Decode and Clean (Same logic as RAG) ---\n",
        "        output_tokens_base = outputs_base[0, input_length_base:]\n",
        "        generated_code_baseline_full = tokenizer.decode(output_tokens_base, skip_special_tokens=True)\n",
        "\n",
        "        print(\"\\n--- Baseline Generated Code (Raw) ---\")\n",
        "        print(generated_code_baseline_full[:500] + \"...\" if len(generated_code_baseline_full) > 500 else generated_code_baseline_full)\n",
        "\n",
        "        # Cleanup with Regex (identical to RAG)\n",
        "        code_block_match_base = re.search(r'```python\\n(.*?)(?:\\n```|\\Z)', generated_code_baseline_full, re.DOTALL)\n",
        "        if code_block_match_base:\n",
        "            generated_code_baseline = code_block_match_base.group(1).strip()\n",
        "            print(\"\\nExtracted code from the ```python block.\")\n",
        "        else:\n",
        "            if \"\\n```\" in generated_code_baseline_full:\n",
        "                generated_code_baseline = generated_code_baseline_full.split(\"\\n```\")[0].strip()\n",
        "                print(\"\\n```python block not found, took output before ```.\")\n",
        "            else:\n",
        "                generated_code_baseline = generated_code_baseline_full.strip()\n",
        "                print(\"\\nNo ``` block found, taking the full output.\")\n",
        "\n",
        "        print(\"\\n--- Generated Code (Baseline LLM-only - Clean) ---\")\n",
        "        print(generated_code_baseline or \"[Empty generation]\")\n",
        "\n",
        "    except torch.cuda.OutOfMemoryError as e:\n",
        "        print(f\"\\n[ERROR] Out Of Memory (OOM) during BASELINE GENERATION!\")\n",
        "        print(\"  Try reducing 'MAX_NEW_TOKENS'.\")\n",
        "        generated_code_baseline = None  # Ensure None in case of error\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"\\n[ERROR] Unexpected error during Baseline generation:\")\n",
        "        print(traceback.format_exc())\n",
        "        generated_code_baseline = None  # Ensure None in case of error\n",
        "\n",
        "else:\n",
        "    # Print detailed error message\n",
        "    print(\"\\n[ERROR] Unable to perform Baseline generation.\")\n",
        "    error_msg = \"         Issue detected with:\"\n",
        "    if missing_vars:\n",
        "        error_msg += f\" Missing variables: {', '.join(missing_vars)}.\"\n",
        "    if invalid_vars:\n",
        "        error_msg += f\" Invalid variables (None/empty): {', '.join(invalid_vars)}.\"\n",
        "    print(error_msg)\n",
        "    print(\"         Make sure ALL previous cells (data/model loading, RAG generation) executed successfully.\")\n",
        "\n",
        "# --- 5. Final Verification ---\n",
        "if generated_code_baseline is not None:\n",
        "    print(\"\\n--- Baseline code generation completed ---\")\n",
        "else:\n",
        "    print(\"\\n--- [ERROR] Baseline code generation failed or was not executed ---\")\n"
      ],
      "metadata": {
        "id": "Dl8oSGsPS4Cv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0fa064-fff4-4847-ca77-49a9209cdf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "--- Step 6.A: Baseline Generation (LLM-only) ---\n",
            "========================================\n",
            "NOTE: This cell expects that 'instruction', 'model', 'tokenizer'\n",
            "      and the generation parameters (MAX_NEW_TOKENS, etc.) have\n",
            "      been defined in the previous cells (including Step 5).\n",
            "\n",
            "[ERROR] Unable to perform Baseline generation.\n",
            "         Issue detected with: Invalid variables (None/empty): 'model' (is None or empty).\n",
            "         Make sure ALL previous cells (data/model loading, RAG generation) executed successfully.\n",
            "\n",
            "--- [ERROR] Baseline code generation failed or was not executed ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import torch # Assicurati sia importato\n",
        "\n",
        "# --- Setup Metriche ---\n",
        "# --- Funzione ChrF ---\n",
        "try:\n",
        "    import sacrebleu\n",
        "    print(f\"Sacrebleu versione: {getattr(sacrebleu, '__version__', 'N/A')}\") # Stampa versione per debug\n",
        "\n",
        "    def calculate_chrf(prediction, reference):\n",
        "        \"\"\"Calcola ChrF (o ChrF++) usando sacrebleu, con gestione errori e tipi.\"\"\"\n",
        "        # Validazione Input\n",
        "        if not isinstance(prediction, str) or not isinstance(reference, str):\n",
        "             print(\"Errore ChrF: Predizione o riferimento non sono stringhe.\")\n",
        "             return None # O 0.0 se preferisci\n",
        "        if not prediction or not reference:\n",
        "             # Restituiamo 0.0 se uno è vuoto, come da comportamento originale\n",
        "             # Ma potresti preferire None se vuoi distinguere zero score da input vuoto\n",
        "             return 0.0\n",
        "\n",
        "        try:\n",
        "            # Chiamata corretta per sacrebleu >= 2.0.0\n",
        "            score = sacrebleu.corpus_chrf([prediction], [[reference]]).score\n",
        "            return score\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(f\"--- ERRORE durante il calcolo di ChrF ---\")\n",
        "            # Stampa più dettagli per il debug\n",
        "            pred_type = type(prediction).__name__\n",
        "            ref_type = type(reference).__name__\n",
        "            pred_preview = str(prediction)[:100] + '...' if prediction else 'None'\n",
        "            ref_preview = str(reference)[:100] + '...' if reference else 'None'\n",
        "            print(f\"  Errore: {e}\")\n",
        "            # print(traceback.format_exc()) # Decommenta per stack trace completo se necessario\n",
        "            print(f\"  Predizione (tipo {pred_type}): {pred_preview}\")\n",
        "            print(f\"  Riferimento (tipo {ref_type}): {ref_preview}\")\n",
        "            return None # Restituisce None per indicare fallimento nel calcolo\n",
        "except ImportError:\n",
        "    print(\"WARNING: Libreria 'sacrebleu' non trovata. ChrF non sarà calcolato.\")\n",
        "    # Restituisce None per coerenza\n",
        "    calculate_chrf = lambda p, r: None\n",
        "\n",
        "# --- Funzione CodeBLEU ---\n",
        "try:\n",
        "    from codebleu import calc_codebleu, __version__ as codebleu_version\n",
        "    print(f\"CodeBLEU versione: {codebleu_version}\") # Stampa versione per debug\n",
        "\n",
        "    def calculate_codebleu(prediction, reference, lang=\"python\", weights=(0.25, 0.25, 0.25, 0.25)):\n",
        "        \"\"\"Calcola CodeBLEU con gestione errori, validazione input e correzione argomenti.\"\"\"\n",
        "        # Validazione Input\n",
        "        if not isinstance(prediction, str) or not isinstance(reference, str):\n",
        "            print(\"Errore CodeBLEU: Predizione o riferimento non sono stringhe.\")\n",
        "            return None\n",
        "        # Gestione input vuoti (CodeBLEU potrebbe dare errori o risultati 0)\n",
        "        if not prediction:\n",
        "            print(\"Warning CodeBLEU: Predizione vuota.\")\n",
        "            # Potrebbe avere senso restituire 0.0 o un valore specifico, ma None indica fallimento/non calcolato\n",
        "            # return 0.0\n",
        "        if not reference:\n",
        "            print(\"Warning CodeBLEU: Riferimento vuoto.\")\n",
        "            # return 0.0\n",
        "\n",
        "        try:\n",
        "            # !!! CORREZIONE CRITICA: Ordine e formato argomenti !!!\n",
        "            # references deve essere lista di liste, predictions lista semplice\n",
        "            result_dict = calc_codebleu(\n",
        "                references=[[reference]], # Doppio array per i riferimenti\n",
        "                predictions=[prediction],   # Array singolo per le predizioni\n",
        "                lang=lang,\n",
        "                weights=weights\n",
        "            )\n",
        "            # Estrai il punteggio composito 'codebleu'\n",
        "            # Aggiungi controllo se la chiave esiste nel dizionario risultato\n",
        "            return result_dict.get('codebleu', None) # Restituisce None se 'codebleu' non è presente\n",
        "\n",
        "        except TypeError as e:\n",
        "            # Errore comune se tree-sitter non è configurato correttamente\n",
        "            print(f\"--- ERRORE CodeBLEU (TypeError): {e} ---\")\n",
        "            print(\"Questo spesso indica un problema irrisolto con la configurazione di tree-sitter.\")\n",
        "            print(f\"  Verifica che la cella di configurazione/test di tree-sitter completi senza errori.\")\n",
        "            pred_preview = str(prediction)[:100] + '...' if prediction else 'None'\n",
        "            ref_preview = str(reference)[:100] + '...' if reference else 'None'\n",
        "            print(f\"  Predizione (tipo {type(prediction).__name__}): {pred_preview}\")\n",
        "            print(f\"  Riferimento (tipo {type(reference).__name__}): {ref_preview}\")\n",
        "            return None # Restituisce None per indicare fallimento\n",
        "        except Exception as e:\n",
        "            # Cattura altri possibili errori\n",
        "            import traceback\n",
        "            print(f\"--- ERRORE INASPETTATO durante il calcolo di CodeBLEU ---\")\n",
        "            print(traceback.format_exc()) # Stampa lo stack trace completo per debug\n",
        "            print(\"-----------------------------------------------------\")\n",
        "            pred_preview = str(prediction)[:100] + '...' if prediction else 'None'\n",
        "            ref_preview = str(reference)[:100] + '...' if reference else 'None'\n",
        "            print(f\"  Predizione (tipo {type(prediction).__name__}): {pred_preview}\")\n",
        "            print(f\"  Riferimento (tipo {type(reference).__name__}): {ref_preview}\")\n",
        "            return None # Restituisce None per indicare fallimento\n",
        "\n",
        "except ImportError:\n",
        "    print(\"WARNING: Libreria 'codebleu' non trovata. CodeBLEU non sarà calcolato.\")\n",
        "    # Restituisce None per coerenza\n",
        "    calculate_codebleu = lambda p, r, lang=\"python\": None\n",
        "\n",
        "# --- Funzione API Recall (con controlli tipo aggiunti) ---\n",
        "def calculate_api_recall(generated_code, reference_apis):\n",
        "    \"\"\"Calcola la recall delle API con validazione tipi.\"\"\"\n",
        "    # Validazione Input\n",
        "    if not isinstance(generated_code, str):\n",
        "        print(\"Errore API Recall: 'generated_code' non è una stringa.\")\n",
        "        return 0.0 # O None se preferisci\n",
        "    if not isinstance(reference_apis, list):\n",
        "        print(\"Errore API Recall: 'reference_apis' non è una lista.\")\n",
        "        return 0.0 # O None\n",
        "    if not generated_code or not reference_apis: # Se uno è vuoto, recall è 0\n",
        "        return 0.0\n",
        "\n",
        "    present_apis = 0\n",
        "    valid_ref_apis_count = 0 # Contiamo solo le API valide nel riferimento\n",
        "    for api_call in reference_apis:\n",
        "        # Validazione tipo elemento lista\n",
        "        if not isinstance(api_call, str):\n",
        "            # print(f\"Warning API Recall: Trovato elemento non-stringa nella lista API di riferimento: {api_call}\")\n",
        "            continue # Salta questo elemento non valido\n",
        "        if not api_call.strip(): # Salta stringhe vuote o solo spazi\n",
        "             continue\n",
        "\n",
        "        valid_ref_apis_count += 1 # Incrementa solo per API di riferimento valide\n",
        "        try:\n",
        "            # Usa word boundary per matchare nomi interi\n",
        "            pattern = r'\\b' + re.escape(api_call) + r'\\b'\n",
        "            if re.search(pattern, generated_code):\n",
        "                present_apis += 1\n",
        "        except re.error as e:\n",
        "             print(f\"Warning API Recall: Ignorato pattern regex non valido per API '{api_call}'. Errore: {e}\")\n",
        "             pass # Ignora pattern regex non validi\n",
        "\n",
        "    # Calcola recall basandosi sul numero di API *valide* nel riferimento\n",
        "    recall = present_apis / valid_ref_apis_count if valid_ref_apis_count > 0 else 0.0\n",
        "    return recall\n",
        "\n",
        "print(\"\\nFunzioni per le metriche (calculate_chrf, calculate_codebleu, calculate_api_recall) definite/aggiornate.\")"
      ],
      "metadata": {
        "id": "ieOEFNzdAu2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b2392d-bb26-481f-f891-8f8d2fae84cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sacrebleu versione: 2.5.1\n",
            "WARNING: Libreria 'codebleu' non trovata. CodeBLEU non sarà calcolato.\n",
            "\n",
            "Funzioni per le metriche (calculate_chrf, calculate_codebleu, calculate_api_recall) definite/aggiornate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 9 · Metrics Results\n"
      ],
      "metadata": {
        "id": "78ze6zTCrroc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 6.B: Calcolo Metriche di Confronto\n",
        "\n",
        "import textwrap\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Assicurati che le funzioni delle metriche siano definite (da inizio Passo 6 o sopra)\n",
        "# Esempio: calculate_api_recall, calculate_chrf, calculate_codebleu\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"--- Passo 6.B: Calcolo Metriche di Confronto ---\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Dizionario per conservare le metriche calcolate\n",
        "metrics = {\"api_recall\": {}, \"chrf\": {}, \"codebleu\": {}}\n",
        "calculation_possible = False # Flag per sapere se possiamo calcolare\n",
        "\n",
        "# Verifica che gli output generati e il dataset siano disponibili\n",
        "if ('lca_dataset_split' in locals() and lca_dataset_split and\n",
        "    'generated_code_rag' in locals() and generated_code_rag is not None and\n",
        "    'generated_code_baseline' in locals() and generated_code_baseline is not None):\n",
        "\n",
        "    sample_index = 0 # L'indice dell'esempio che stiamo valutando\n",
        "    sample = lca_dataset_split[sample_index]\n",
        "    reference_code = sample.get('reference') # Codice di riferimento\n",
        "    reference_apis = sample.get('unique_apis') # Lista API di riferimento\n",
        "\n",
        "    print(f\"Confronto per Sample {sample_index}...\")\n",
        "\n",
        "    if not reference_code:\n",
        "        print(\"ATTENZIONE: Codice di riferimento non trovato nel dataset. Impossibile calcolare ChrF e CodeBLEU.\")\n",
        "    if not reference_apis:\n",
        "        print(\"ATTENZIONE: Lista API di riferimento non trovata nel dataset. Impossibile calcolare API Recall.\")\n",
        "\n",
        "    calculation_possible = True # Possiamo provare a calcolare qualcosa\n",
        "\n",
        "else:\n",
        "    print(\"ERRORE: Impossibile calcolare metriche.\")\n",
        "    print(\"Verifica che 'lca_dataset_split', 'generated_code_rag', 'generated_code_baseline' siano definiti.\")\n",
        "\n",
        "# Calcola le metriche solo se possibile\n",
        "if calculation_possible:\n",
        "\n",
        "    # --- API Recall ---\n",
        "    print(\"\\nCalcolo API Recall...\")\n",
        "    if reference_apis:\n",
        "        metrics[\"api_recall\"][\"baseline\"] = calculate_api_recall(generated_code_baseline, reference_apis)\n",
        "        metrics[\"api_recall\"][\"rag\"] = calculate_api_recall(generated_code_rag, reference_apis)\n",
        "    else:\n",
        "         metrics[\"api_recall\"][\"baseline\"] = None\n",
        "         metrics[\"api_recall\"][\"rag\"] = None\n",
        "\n",
        "    # --- ChrF ---\n",
        "    print(\"Calcolo ChrF...\")\n",
        "    if reference_code and 'calculate_chrf' in locals():\n",
        "        metrics[\"chrf\"][\"baseline\"] = calculate_chrf(generated_code_baseline, reference_code)\n",
        "        metrics[\"chrf\"][\"rag\"] = calculate_chrf(generated_code_rag, reference_code)\n",
        "    else:\n",
        "        metrics[\"chrf\"][\"baseline\"] = None\n",
        "        metrics[\"chrf\"][\"rag\"] = None\n",
        "\n",
        "    # --- CodeBLEU ---\n",
        "    print(\"Calcolo CodeBLEU...\")\n",
        "    if reference_code and 'calculate_codebleu' in locals():\n",
        "        # Assicurati che la predizione non sia vuota, potrebbe causare errori in codebleu\n",
        "        pred_baseline = generated_code_baseline if generated_code_baseline else \"\"\n",
        "        pred_rag = generated_code_rag if generated_code_rag else \"\"\n",
        "        metrics[\"codebleu\"][\"baseline\"] = calculate_codebleu(pred_baseline, reference_code)\n",
        "        metrics[\"codebleu\"][\"rag\"] = calculate_codebleu(pred_rag, reference_code)\n",
        "    else:\n",
        "        metrics[\"codebleu\"][\"baseline\"] = None\n",
        "        metrics[\"codebleu\"][\"rag\"] = None\n",
        "\n",
        "    # --- Stampa Risultati Metriche ---\n",
        "    print(\"\\n--- Risultati Metriche Automatiche ---\")\n",
        "    print(f\"| Metrica         | Baseline        | RAG             |\")\n",
        "    print(f\"|-----------------|-----------------|-----------------|\")\n",
        "    api_ref_count = len(reference_apis) if reference_apis else 0\n",
        "    print(f\"| API Recall      | {metrics['api_recall'].get('baseline', 'N/A'):<15.4f} | {metrics['api_recall'].get('rag', 'N/A'):<15.4f} | (Ref APIs: {api_ref_count})\")\n",
        "    print(f\"| ChrF            | {metrics['chrf'].get('baseline', 'N/A'):<15} | {metrics['chrf'].get('rag', 'N/A'):<15} |\")\n",
        "    # Gestisci None per CodeBLEU se il calcolo fallisce\n",
        "    cb_baseline_str = f\"{metrics['codebleu'].get('baseline'):.4f}\" if isinstance(metrics['codebleu'].get('baseline'), float) else str(metrics['codebleu'].get('baseline', 'N/A'))\n",
        "    cb_rag_str = f\"{metrics['codebleu'].get('rag'):.4f}\" if isinstance(metrics['codebleu'].get('rag'), float) else str(metrics['codebleu'].get('rag', 'N/A'))\n",
        "    print(f\"| CodeBLEU        | {cb_baseline_str:<15} | {cb_rag_str:<15} |\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "else:\n",
        "     print(\"Calcolo metriche saltato a causa di dati mancanti.\")"
      ],
      "metadata": {
        "id": "gHI85gPL-tQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea44804-4de2-43ef-c237-803f5eb82c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "--- Passo 6.B: Calcolo Metriche di Confronto ---\n",
            "==============================\n",
            "ERRORE: Impossibile calcolare metriche.\n",
            "Verifica che 'lca_dataset_split', 'generated_code_rag', 'generated_code_baseline' siano definiti.\n",
            "Calcolo metriche saltato a causa di dati mancanti.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 6.C: Analisi Direzione 2 (Fallimenti RAG) e Salvataggio\n",
        "\n",
        "import textwrap\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"--- Passo 6.C: Analisi Direzione 2 e Salvataggio ---\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Verifica che le metriche e gli altri dati siano disponibili\n",
        "analysis_possible = (\n",
        "    'metrics' in locals() and metrics and # Verifica che il dizionario metriche esista\n",
        "    'lca_dataset_split' in locals() and lca_dataset_split and\n",
        "    'generated_code_rag' in locals() and generated_code_rag is not None and\n",
        "    'generated_code_baseline' in locals() and generated_code_baseline is not None and\n",
        "    'instruction' in locals() and instruction and\n",
        "    'retrieved_snippets' in locals()\n",
        ")\n",
        "\n",
        "if analysis_possible:\n",
        "    sample_index = 0 # Indice dell'esempio\n",
        "    sample = lca_dataset_split[sample_index]\n",
        "    repo_full_name = sample.get('repo_full_name')\n",
        "\n",
        "    # --- 1. Analisi Fallimenti RAG (Direzione 2) ---\n",
        "    print(\"\\n--- Analisi Performance RAG (Direzione 2) ---\")\n",
        "    rag_failed = False\n",
        "    failure_reasons = []\n",
        "\n",
        "    # Logica per determinare se RAG ha fallito (basata sulle metriche di 6.B)\n",
        "    # Confrontiamo se le metriche sono numeriche e RAG è peggiore\n",
        "    try:\n",
        "        # API Recall\n",
        "        recall_rag = metrics.get(\"api_recall\", {}).get(\"rag\")\n",
        "        recall_baseline = metrics.get(\"api_recall\", {}).get(\"baseline\")\n",
        "        if isinstance(recall_rag, (int, float)) and isinstance(recall_baseline, (int, float)):\n",
        "            if recall_rag < recall_baseline:\n",
        "                rag_failed = True\n",
        "                failure_reasons.append(\"API Recall RAG < Baseline\")\n",
        "\n",
        "        # CodeBLEU (solo se non già fallito e metriche disponibili/valide)\n",
        "        if not rag_failed:\n",
        "             codebleu_rag = metrics.get(\"codebleu\", {}).get(\"rag\")\n",
        "             codebleu_baseline = metrics.get(\"codebleu\", {}).get(\"baseline\")\n",
        "             # CodeBLEU ritorna None se fallisce, controlla sia float\n",
        "             if isinstance(codebleu_rag, float) and isinstance(codebleu_baseline, float):\n",
        "                  if codebleu_rag < codebleu_baseline:\n",
        "                       rag_failed = True\n",
        "                       failure_reasons.append(\"CodeBLEU RAG < Baseline\")\n",
        "\n",
        "        # ChrF (meno indicativo, ma puoi aggiungerlo se vuoi)\n",
        "        # if not rag_failed:\n",
        "        #      chrf_rag = metrics.get(\"chrf\", {}).get(\"rag\")\n",
        "        #      chrf_baseline = metrics.get(\"chrf\", {}).get(\"baseline\")\n",
        "        #      if isinstance(chrf_rag, (int, float)) and isinstance(chrf_baseline, (int, float)):\n",
        "        #           if chrf_rag < chrf_baseline - 5: # Es. se ChrF è peggiore di 5 punti\n",
        "        #                rag_failed = True\n",
        "        #                failure_reasons.append(\"ChrF RAG < Baseline (significativamente)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"- Errore durante il confronto metriche per analisi fallimenti: {e}\")\n",
        "\n",
        "    # Se RAG è considerato fallito, guida l'analisi\n",
        "    if rag_failed:\n",
        "        print(f\"\\n**ANALISI RICHIESTA: RAG sembra aver performato peggio della Baseline per Sample {sample_index}.**\")\n",
        "        print(f\"  Motivo/i rilevato/i: {', '.join(failure_reasons)}\")\n",
        "\n",
        "        print(\"\\n  **1. Esamina gli Snippet Recuperati (stampati qui sotto):**\")\n",
        "        if retrieved_snippets:\n",
        "            for i, snippet in enumerate(retrieved_snippets):\n",
        "                print(f\"\\n  --- Snippet {i+1} ---\")\n",
        "                snippet_preview = '\\n'.join(snippet.splitlines()[:10]) # Mostra prime 10 righe\n",
        "                if len(snippet_preview) > 400: snippet_preview = snippet_preview[:400] + \"...\"\n",
        "                elif len(snippet) > len(snippet_preview): snippet_preview += \"\\n...\"\n",
        "                print(textwrap.indent(snippet_preview, '    ')) # Indenta per chiarezza\n",
        "        else:\n",
        "            print(\"    (Nessuno snippet recuperato)\")\n",
        "\n",
        "        print(\"\\n  **2. Punti Chiave da Analizzare:**\")\n",
        "        print(\"     - Qualità Retrieval (BM25): Gli snippet erano realmente utili/rilevanti per l'istruzione?\")\n",
        "        print(\"     - Correttezza Snippet: Contenevano errori o erano fuorvianti?\")\n",
        "        print(\"     - Integrazione LLM: Il modello RAG ha ignorato/usato male gli snippet?\")\n",
        "        print(\"     - Causa Probabile: Il problema è nel retriever (BM25) o nel generatore (LLM)?\")\n",
        "\n",
        "        # Log del caso di fallimento per analisi futura\n",
        "        failure_log_path = os.path.join(drive_save_path, 'rag_failures.jsonl')\n",
        "        failure_analysis_data = {\n",
        "             \"sample_index\": sample_index,\n",
        "             \"repo_full_name\": repo_full_name,\n",
        "             \"failure_reasons\": failure_reasons,\n",
        "             \"instruction\": instruction,\n",
        "             \"retrieved_snippets\": retrieved_snippets, # Salva gli snippet completi nel log\n",
        "             \"generated_code_rag\": generated_code_rag,\n",
        "             \"generated_code_baseline\": generated_code_baseline,\n",
        "             \"metrics\": metrics # Salva tutte le metriche calcolate\n",
        "        }\n",
        "        try:\n",
        "             with open(failure_log_path, 'a', encoding='utf-8') as f: # Modalità Append (aggiunge)\n",
        "                  f.write(json.dumps(failure_analysis_data) + '\\n') # Scrivi come JSON per riga\n",
        "             print(f\"\\n  --> Caso di fallimento RAG loggato in: {failure_log_path}\")\n",
        "        except Exception as e:\n",
        "             print(f\"\\n  ERRORE durante il logging del caso di fallimento: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nAnalisi Performance RAG: Nessun chiaro segnale di fallimento rispetto alla Baseline basato sulle metriche attuali.\")\n",
        "\n",
        "    # --- 2. Salvataggio Risultati Complessivi del Sample ---\n",
        "    print(\"\\n--- Salvataggio Risultati Complessivi ---\")\n",
        "    results_save_path = os.path.join(drive_save_path, 'results')\n",
        "    os.makedirs(results_save_path, exist_ok=True)\n",
        "    result_filename = f\"result_sample_{sample_index}_{repo_full_name}.json\" # Nome file più descrittivo\n",
        "    result_full_path = os.path.join(results_save_path, result_filename)\n",
        "\n",
        "    # Prepara i dati da salvare (con anteprime per ridurre dimensione)\n",
        "    reference_code = sample.get('reference') # Recupera di nuovo se necessario\n",
        "    result_data = {\n",
        "        \"sample_index\": sample_index,\n",
        "        \"repo_full_name\": repo_full_name,\n",
        "        \"instruction\": instruction,\n",
        "        # Salva solo anteprime di snippet e riferimento nel risultato principale\n",
        "        \"retrieved_snippets_preview\": [s[:300]+\"...\" for s in retrieved_snippets] if retrieved_snippets else [],\n",
        "        \"generated_code_baseline\": generated_code_baseline,\n",
        "        \"generated_code_rag\": generated_code_rag,\n",
        "        \"reference_code_preview\": textwrap.shorten(reference_code or \"N/A\", width=600, placeholder=\"...\"),\n",
        "        \"metrics\": metrics, # Include il dizionario completo delle metriche\n",
        "        \"rag_failed_analysis_triggered\": rag_failed\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with open(result_full_path, 'w', encoding='utf-8') as f:\n",
        "             # Usa default=str per gestire tipi non serializzabili (es. None)\n",
        "            json.dump(result_data, f, indent=2, default=str)\n",
        "        print(f\"Risultati dettagliati per Sample {sample_index} salvati in: {result_full_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERRORE durante il salvataggio dei risultati dettagliati: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(\"--- Analisi/Salvataggio Saltati ---\")\n",
        "    print(\"Impossibile procedere. Dati mancanti dai passi precedenti (metriche, output generati, ecc.).\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "print(\"\\n--- Fine Passo 6.C ---\")"
      ],
      "metadata": {
        "id": "AJWoCcZ9_FT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d655c3-e5ef-4b5a-9734-cb0fc9623622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "--- Passo 6.C: Analisi Direzione 2 e Salvataggio ---\n",
            "==============================\n",
            "\n",
            "==============================\n",
            "--- Analisi/Salvataggio Saltati ---\n",
            "Impossibile procedere. Dati mancanti dai passi precedenti (metriche, output generati, ecc.).\n",
            "==============================\n",
            "\n",
            "--- Fine Passo 6.C ---\n"
          ]
        }
      ]
    }
  ]
}