[
  "def _get_version():\n    try:\n        cmd = [\"git\", \"rev-parse\", \"HEAD\"]\n        sha = subprocess.check_output(cmd, cwd=str(ROOT_DIR)).decode(\"ascii\").strip()\n    except Exception:\n        sha = None\n\n    if \"BUILD_VERSION\" in os.environ:\n        version = os.environ[\"BUILD_VERSION\"]\n    else:\n        with open(os.path.join(ROOT_DIR, \"version.txt\"), \"r\") as f:\n            version = f.readline().strip()\n        if sha is not None:\n            version += \"+\" + sha[:7]\n\n    if sha is None:\n        sha = \"Unknown\"\n    return version, sha",
  "def _export_version(version, sha):\n    version_path = ROOT_DIR / \"torchrec\" / \"version.py\"\n    with open(version_path, \"w\") as fileobj:\n        fileobj.write(\"__version__ = '{}'\\n\".format(version))\n        fileobj.write(\"git_version = {}\\n\".format(repr(sha)))",
  "def get_channel():\n    # Channel typically takes on the following values:\n    # - NIGHTLY: for nightly published binaries\n    # - TEST: for binaries build from release candidate branches\n    return os.getenv(\"CHANNEL\")",
  "def get_cu_version():\n    return os.getenv(\"CU_VERSION\", \"cpu\")",
  "def parse_args(argv: List[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"torchrec setup\")\n    return parser.parse_known_args(argv)",
  "def main(argv: List[str]) -> None:\n    args, unknown = parse_args(argv)\n\n    # Set up package version\n    channel = get_channel()\n\n    with open(\n        os.path.join(os.path.dirname(__file__), \"README.MD\"), encoding=\"utf8\"\n    ) as f:\n        readme = f.read()\n    with open(\n        os.path.join(os.path.dirname(__file__), \"install-requirements.txt\"),\n        encoding=\"utf8\",\n    ) as f:\n        reqs = f.read()\n        install_requires = reqs.strip().split(\"\\n\")\n\n    version, sha = _get_version()\n    _export_version(version, sha)\n\n    if channel != \"nightly\":\n        if \"fbgemm-gpu-nightly\" in install_requires:\n            install_requires.remove(\"fbgemm-gpu-nightly\")\n        install_requires.append(\"fbgemm-gpu\")\n\n    cu_version = get_cu_version()\n    if cu_version == \"cpu\":\n        if \"fbgemm-gpu-nightly\" in install_requires:\n            install_requires.remove(\"fbgemm-gpu-nightly\")\n            install_requires.append(\"fbgemm-gpu-nightly-cpu\")\n        if \"fbgemm-gpu\" in install_requires:\n            install_requires.remove(\"fbgemm-gpu\")\n            install_requires.append(\"fbgemm-gpu-cpu\")\n\n    print(f\"-- torchrec building version: {version} CU Version: {cu_version}\")\n\n    packages = find_packages(\n        exclude=(\n            \"*tests\",\n            \"*test\",\n            \"examples\",\n            \"*examples.*\",\n            \"*benchmarks\",\n            \"*build\",\n            \"*rfc\",\n        )\n    )\n    sys.argv = [sys.argv[0]] + unknown\n\n    setup(\n        # Metadata\n        name=\"torchrec\",\n        version=version,\n        author=\"TorchRec Team\",\n        author_email=\"packages@pytorch.org\",\n        description=\"Pytorch domain library for recommendation systems\",\n        long_description=readme,\n        long_description_content_type=\"text/markdown\",\n        url=\"https://github.com/pytorch/torchrec\",\n        license=\"BSD-3\",\n        keywords=[\"pytorch\", \"recommendation systems\", \"sharding\"],\n        python_requires=\">=3.7\",\n        install_requires=install_requires,\n        packages=packages,\n        zip_safe=False,\n        # PyPI package information.\n        classifiers=[\n            \"Development Status :: 4 - Beta\",\n            \"Intended Audience :: Developers\",\n            \"Intended Audience :: Science/Research\",\n            \"License :: OSI Approved :: BSD License\",\n            \"Programming Language :: Python :: 3\",\n            \"Programming Language :: Python :: 3.7\",\n            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        ],\n    )",
  "class RandomIterator(Iterator):\n    def __init__(\n        self, batch_size: int, num_dense: int, num_sparse: int, num_embeddings: int\n    ) -> None:\n        self.batch_size = batch_size\n        self.num_dense = num_dense\n        self.num_sparse = num_sparse\n        self.sparse_keys = [f\"feature{id}\" for id in range(self.num_sparse)]\n        self.num_embeddings = num_embeddings\n        self.num_ids_per_feature = 3\n        self.num_ids_to_generate = (\n            self.num_sparse * self.num_ids_per_feature * self.batch_size\n        )\n\n    def __next__(self) -> (torch.Tensor, KeyedJaggedTensor, torch.Tensor):\n        float_features = torch.randn(\n            self.batch_size,\n            self.num_dense,\n        )\n        labels = torch.randint(\n            low=0,\n            high=2,\n            size=(self.batch_size,),\n        )\n        sparse_ids = torch.randint(\n            high=self.num_sparse,\n            size=(self.num_ids_to_generate,),\n        )\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n            keys=self.sparse_keys,\n            values=sparse_ids,\n            offsets=torch.tensor(\n                list(range(0, self.num_ids_to_generate + 1, self.num_ids_per_feature)),\n                dtype=torch.int32,\n            ),\n        )\n        return (float_features, sparse_features, labels)",
  "def parse_args(argv: List[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"TorchRec test installation\")\n    parser.add_argument(\"--cpu_only\", action=\"store_true\")\n    return parser.parse_args(argv)",
  "def main(argv: List[str]) -> None:\n    args = parse_args(argv)\n\n    batch_size = 1024\n    num_dense = 1000\n    num_sparse = 20\n    num_embeddings = 1000000\n\n    configs = [\n        EmbeddingBagConfig(\n            name=f\"table{id}\",\n            embedding_dim=64,\n            num_embeddings=num_embeddings,\n            feature_names=[f\"feature{id}\"],\n        )\n        for id in range(num_sparse)\n    ]\n\n    rank = int(os.environ[\"LOCAL_RANK\"])\n    if not args.cpu_only and torch.cuda.is_available():\n        device = torch.device(f\"cuda:{rank}\")\n        backend = \"nccl\"\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\"cpu\")\n        backend = \"gloo\"\n        print(\n            \"\\033[92m\"\n            + f\"WARNING: Running in CPU mode. Cuda is available: {torch.cuda.is_available()}. CPU only: {args.cpu_only}\"\n        )\n\n    dist.init_process_group(backend=backend)\n\n    model = DLRM(\n        embedding_bag_collection=EmbeddingBagCollection(\n            tables=configs, device=torch.device(\"meta\")\n        ),\n        dense_in_features=num_dense,\n        dense_arch_layer_sizes=[500, 64],\n        over_arch_layer_sizes=[32, 16, 1],\n        dense_device=device,\n    )\n    model = DistributedModelParallel(\n        module=model,\n        device=device,\n    )\n    optimizer = KeyedOptimizerWrapper(\n        dict(in_backward_optimizer_filter(model.named_parameters())),\n        lambda params: torch.optim.SGD(params, lr=0.01),\n    )\n\n    random_iterator = RandomIterator(batch_size, num_dense, num_sparse, num_embeddings)\n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    for _ in range(10):\n        (dense_features, sparse_features, labels) = next(random_iterator)\n        dense_features = dense_features.to(device)\n        sparse_features = sparse_features.to(device)\n        output = model(dense_features, sparse_features)\n        labels = labels.to(device)\n        loss = loss_fn(output.squeeze(), labels.float())\n        torch.sum(loss, dim=0).backward()\n        optimizer.zero_grad()\n        optimizer.step()\n\n    print(\n        \"\\033[92m\" + \"Successfully ran a few epochs for DLRM. Installation looks good!\"\n    )",
  "def __init__(\n        self, batch_size: int, num_dense: int, num_sparse: int, num_embeddings: int\n    ) -> None:\n        self.batch_size = batch_size\n        self.num_dense = num_dense\n        self.num_sparse = num_sparse\n        self.sparse_keys = [f\"feature{id}\" for id in range(self.num_sparse)]\n        self.num_embeddings = num_embeddings\n        self.num_ids_per_feature = 3\n        self.num_ids_to_generate = (\n            self.num_sparse * self.num_ids_per_feature * self.batch_size\n        )",
  "def __next__(self) -> (torch.Tensor, KeyedJaggedTensor, torch.Tensor):\n        float_features = torch.randn(\n            self.batch_size,\n            self.num_dense,\n        )\n        labels = torch.randint(\n            low=0,\n            high=2,\n            size=(self.batch_size,),\n        )\n        sparse_ids = torch.randint(\n            high=self.num_sparse,\n            size=(self.num_ids_to_generate,),\n        )\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n            keys=self.sparse_keys,\n            values=sparse_ids,\n            offsets=torch.tensor(\n                list(range(0, self.num_ids_to_generate + 1, self.num_ids_per_feature)),\n                dtype=torch.int32,\n            ),\n        )\n        return (float_features, sparse_features, labels)",
  "def get_random_dataset(\n    batch_size: int,\n    num_batches: int,\n    num_dense_features: int,\n    embedding_bag_configs: List[EmbeddingBagConfig],\n    pooling_factors: Optional[Dict[str, int]] = None,\n) -> IterableDataset[Batch]:\n\n    if pooling_factors is None:\n        pooling_factors = {}\n\n    keys = []\n    ids_per_features = []\n    hash_sizes = []\n\n    for table in embedding_bag_configs:\n        for feature_name in table.feature_names:\n            keys.append(feature_name)\n            # guess a pooling factor here\n            ids_per_features.append(pooling_factors.get(feature_name, 64))\n            hash_sizes.append(table.num_embeddings)\n\n    return RandomRecDataset(\n        keys=keys,\n        batch_size=batch_size,\n        hash_sizes=hash_sizes,\n        ids_per_features=ids_per_features,\n        num_dense=num_dense_features,\n        num_batches=num_batches,\n    )",
  "def train_one_epoch(\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    dataset: IterableDataset[Batch],\n    device: torch.device,\n) -> float:\n\n    start_time = time.perf_counter()\n\n    for data in dataset:\n        sparse_features = data.sparse_features.to(device)\n\n        pooled_embeddings = model(sparse_features)\n        optimizer.zero_grad()\n\n        vals = []\n        for _name, param in pooled_embeddings.to_dict().items():\n            vals.append(param)\n        torch.cat(vals, dim=1).sum().backward()\n        optimizer.step()\n\n    end_time = time.perf_counter()\n\n    return end_time - start_time",
  "def train_one_epoch_fused_optimizer(\n    model: torch.nn.Module,\n    dataset: IterableDataset[Batch],\n    device: torch.device,\n) -> float:\n\n    start_time = time.perf_counter()\n\n    for data in dataset:\n        sparse_features = data.sparse_features.to(device)\n        fused_pooled_embeddings = model(sparse_features)\n\n        fused_vals = []\n        for _name, param in fused_pooled_embeddings.to_dict().items():\n            fused_vals.append(param)\n        torch.cat(fused_vals, dim=1).sum().backward()\n\n    end_time = time.perf_counter()\n\n    return end_time - start_time",
  "def train(\n    model: torch.nn.Module,\n    optimizer: Optional[torch.optim.Optimizer],\n    dataset: IterableDataset[Batch],\n    device: torch.device,\n    epochs: int = 100,\n) -> Tuple[float, float]:\n\n    training_time = []\n    for _ in range(epochs):\n        if optimizer:\n            training_time.append(train_one_epoch(model, optimizer, dataset, device))\n        else:\n            training_time.append(\n                train_one_epoch_fused_optimizer(model, dataset, device)\n            )\n\n    return np.mean(training_time), np.std(training_time)",
  "def get_shrunk_dlrm_num_embeddings(reduction_degree: int) -> List[int]:\n    return [\n        num_emb if num_emb < 10000000 else int(num_emb / reduction_degree)\n        for num_emb in DLRM_NUM_EMBEDDINGS_PER_FEATURE\n    ]",
  "def main(argv: List[str]) -> None:\n    args = parse_args(argv)\n\n    if not args.cpu_only and torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    else:\n        device = torch.device(\"cpu\")\n\n    if args.mode == \"ebc_comparison_dlrm\":\n        print(\"Running EBC vs. FusedEBC on DLRM EMB\")\n\n        for reduction_degree in [128, 64, 32]:\n            embedding_bag_configs: List[EmbeddingBagConfig] = [\n                EmbeddingBagConfig(\n                    name=f\"ebc_{idx}\",\n                    embedding_dim=128,\n                    num_embeddings=num_embeddings,\n                    feature_names=[f\"ebc_{idx}_feat_1\"],\n                )\n                for idx, num_embeddings in enumerate(\n                    get_shrunk_dlrm_num_embeddings(reduction_degree)\n                )\n            ]\n            (\n                ebc_time_avg,\n                ebc_time_std,\n                fused_ebc_time_avg,\n                fused_ebc_time_std,\n                speedup,\n            ) = get_ebc_comparison(embedding_bag_configs, device)\n\n            print(f\"when DLRM EMB is reduced by {reduction_degree} times:\")\n            print(f\"ebc_time = {ebc_time_avg} +/- {ebc_time_std} sec\")\n            print(f\"fused_ebc_time = {fused_ebc_time_avg} +/- {fused_ebc_time_std} sec\")\n            print(f\"speedup = {speedup}\")\n\n    elif args.mode == \"fused_ebc_uvm\":\n        print(\"Running DLRM EMB on FusedEBC with UVM/UVM-caching\")\n        embedding_bag_configs: List[EmbeddingBagConfig] = [\n            EmbeddingBagConfig(\n                name=f\"ebc_{idx}\",\n                embedding_dim=128,\n                num_embeddings=num_embeddings,\n                feature_names=[f\"ebc_{idx}_feat_1\"],\n            )\n            for idx, num_embeddings in enumerate(get_shrunk_dlrm_num_embeddings(2))\n        ]\n        fused_ebc_time_avg, fused_ebc_time_std = get_fused_ebc_uvm_time(\n            embedding_bag_configs, device, EmbeddingLocation.MANAGED_CACHING\n        )\n        print(\n            f\"FusedEBC with UVM caching on DLRM: {fused_ebc_time_avg} +/- {fused_ebc_time_std} sec\"\n        )\n\n        embedding_bag_configs: List[EmbeddingBagConfig] = [\n            EmbeddingBagConfig(\n                name=f\"ebc_{idx}\",\n                embedding_dim=128,\n                num_embeddings=num_embeddings,\n                feature_names=[f\"ebc_{idx}_feat_1\"],\n            )\n            for idx, num_embeddings in enumerate(DLRM_NUM_EMBEDDINGS_PER_FEATURE)\n        ]\n        fused_ebc_time_avg, fused_ebc_time_std = get_fused_ebc_uvm_time(\n            embedding_bag_configs, device, EmbeddingLocation.MANAGED\n        )\n        print(\n            f\"FusedEBC with UVM management on DLRM: {fused_ebc_time_avg} plus/minus {fused_ebc_time_std} sec\"\n        )\n\n    elif args.mode == \"ebc_comparison_scaling\":\n        print(\"Running EBC vs. FusedEBC scaling experiment\")\n\n        num_tables_list = [10, 100, 1000]\n        embedding_dim_list = [4, 8, 16, 32, 64, 128]\n        num_embeddings_list = [4, 8, 16, 32, 64, 128, 256, 1024, 2048, 4096, 8192]\n\n        for num_tables in num_tables_list:\n            for num_embeddings in num_embeddings_list:\n                for embedding_dim in embedding_dim_list:\n                    embedding_bag_configs: List[EmbeddingBagConfig] = [\n                        EmbeddingBagConfig(\n                            name=f\"ebc_{idx}\",\n                            embedding_dim=embedding_dim,\n                            num_embeddings=num_embeddings,\n                            feature_names=[f\"ebc_{idx}_feat_1\"],\n                        )\n                        for idx in range(num_tables)\n                    ]\n                    ebc_time, _, fused_ebc_time, _, speedup = get_ebc_comparison(\n                        embedding_bag_configs, device, epochs=3\n                    )\n                    print(\n                        f\"EBC num_tables = {num_tables}, num_embeddings = {num_embeddings}, embedding_dim = {embedding_dim}:\"\n                    )\n                    print(\n                        f\"ebc_time = {ebc_time} sec, fused_ebc_time = {fused_ebc_time} sec, speedup = {speedup}\"\n                    )",
  "def get_fused_ebc_uvm_time(\n    embedding_bag_configs: List[EmbeddingBagConfig],\n    device: torch.device,\n    location: EmbeddingLocation,\n    epochs: int = 100,\n) -> Tuple[float, float]:\n\n    fused_ebc = FusedEmbeddingBagCollection(\n        tables=embedding_bag_configs,\n        optimizer_type=torch.optim.SGD,\n        optimizer_kwargs={\"lr\": 0.02},\n        device=device,\n        location=location,\n    )\n\n    dataset = ebc_benchmarks_utils.get_random_dataset(\n        batch_size=64,\n        num_batches=10,\n        num_dense_features=1024,\n        embedding_bag_configs=embedding_bag_configs,\n    )\n\n    fused_ebc_time_avg, fused_ebc_time_std = ebc_benchmarks_utils.train(\n        model=fused_ebc,\n        optimizer=None,\n        dataset=dataset,\n        device=device,\n        epochs=epochs,\n    )\n\n    return fused_ebc_time_avg, fused_ebc_time_std",
  "def get_ebc_comparison(\n    embedding_bag_configs: List[EmbeddingBagConfig],\n    device: torch.device,\n    epochs: int = 100,\n) -> Tuple[float, float, float, float, float]:\n\n    # Simple EBC module wrapping a list of nn.EmbeddingBag\n    ebc = EmbeddingBagCollection(\n        tables=embedding_bag_configs,\n        device=device,\n    )\n    optimizer = torch.optim.SGD(ebc.parameters(), lr=0.02)\n\n    # EBC with fused optimizer backed by fbgemm SplitTableBatchedEmbeddingBagsCodegen\n    fused_ebc = FusedEmbeddingBagCollection(\n        tables=embedding_bag_configs,\n        optimizer_type=torch.optim.SGD,\n        optimizer_kwargs={\"lr\": 0.02},\n        device=device,\n    )\n\n    dataset = ebc_benchmarks_utils.get_random_dataset(\n        batch_size=64,\n        num_batches=10,\n        num_dense_features=1024,\n        embedding_bag_configs=embedding_bag_configs,\n    )\n\n    ebc_time_avg, ebc_time_std = ebc_benchmarks_utils.train(\n        model=ebc,\n        optimizer=optimizer,\n        dataset=dataset,\n        device=device,\n        epochs=epochs,\n    )\n    fused_ebc_time_avg, fused_ebc_time_std = ebc_benchmarks_utils.train(\n        model=fused_ebc,\n        optimizer=None,\n        dataset=dataset,\n        device=device,\n        epochs=epochs,\n    )\n    speedup = ebc_time_avg / fused_ebc_time_avg\n\n    return ebc_time_avg, ebc_time_std, fused_ebc_time_avg, fused_ebc_time_std, speedup",
  "def parse_args(argv: List[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"TorchRec ebc benchmarks\")\n    parser.add_argument(\n        \"--cpu_only\",\n        action=\"store_true\",\n        default=False,\n        help=\"specify whether to use cpu\",\n    )\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"ebc_comparison_dlrm\",\n        help=\"specify 'ebc_comparison_dlrm', 'ebc_comparison_scaling' or 'fused_ebc_uvm'\",\n    )\n    return parser.parse_args(argv)",
  "class Multistreamable(abc.ABC):\n    \"\"\"\n    Objects implementing this interface are allowed to be transferred\n    from one CUDA stream to another.\n    torch.Tensor and (Keyed)JaggedTensor implement this interface.\n    \"\"\"\n\n    @abc.abstractmethod\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        \"\"\"\n        See https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html\n        \"\"\"\n        ...",
  "class Pipelineable(Multistreamable):\n    \"\"\"\n    This interface contains two methods, one for moving an input across devices,\n    the other one for marking streams that operate the input.\n\n    torch.Tensor implements this interface and we can used it in many applications.\n    Another example is torchrec.(Keyed)JaggedTensor, which we use as the input to\n    torchrec.EmbeddingBagCollection, which in turn is often the first layer of many models.\n    Some models take compound inputs, which should implement this interface.\n    \"\"\"\n\n    @abc.abstractmethod\n    def to(self, device: torch.device, non_blocking: bool) -> \"Pipelineable\":\n        \"\"\"\n        Please be aware that according to https://pytorch.org/docs/stable/generated/torch.Tensor.to.html,\n        `to` might return self or a copy of self.  So please remember to use `to` with the assignment operator,\n        for example, `in = in.to(new_device)`.\n        \"\"\"\n        ...",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        \"\"\"\n        See https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html\n        \"\"\"\n        ...",
  "def to(self, device: torch.device, non_blocking: bool) -> \"Pipelineable\":\n        \"\"\"\n        Please be aware that according to https://pytorch.org/docs/stable/generated/torch.Tensor.to.html,\n        `to` might return self or a copy of self.  So please remember to use `to` with the assignment operator,\n        for example, `in = in.to(new_device)`.\n        \"\"\"\n        ...",
  "class CopyMixIn:\n    @abstractmethod\n    def copy(self, device: torch.device) -> nn.Module:\n        ...",
  "class ModuleCopyMixin(CopyMixIn):\n    \"\"\"\n    A mixin to allow modules to override copy behaviors in DMP.\n    \"\"\"\n\n    def copy(self, device: torch.device) -> nn.Module:\n        # pyre-ignore [16]\n        return self.to(device)",
  "class ModuleNoCopyMixin(CopyMixIn):\n    \"\"\"\n    A mixin to allow modules to override copy behaviors in DMP.\n    \"\"\"\n\n    def copy(self, device: torch.device) -> nn.Module:\n        # pyre-ignore [7]\n        return self",
  "def copy(self, device: torch.device) -> nn.Module:\n        ...",
  "def copy(self, device: torch.device) -> nn.Module:\n        # pyre-ignore [16]\n        return self.to(device)",
  "def copy(self, device: torch.device) -> nn.Module:\n        # pyre-ignore [7]\n        return self",
  "def populate_fx_names(\n    quant_ebc: Union[QuantEmbeddingBagCollection, ShardedQuantEmbeddingBagCollection]\n) -> None:\n    \"\"\"\n    Assigns fx path to non registered lookup modules. This allows the Torchrec tracer to fallback to\n    emb_module._fx_path for table batched embeddings.\n    \"\"\"\n    if isinstance(quant_ebc, QuantEmbeddingBagCollection):\n        for emb_configs, emb_module in zip(\n            quant_ebc._key_to_tables, quant_ebc._emb_modules\n        ):\n            table_names = []\n            for config in emb_configs:\n                table_names.append(config.name)\n            joined_table_names = \",\".join(table_names)\n            emb_module._fx_path = f\"emb_module.{joined_table_names}\"\n    elif isinstance(quant_ebc, ShardedQuantEmbeddingBagCollection):\n        for i, (emb_module, emb_dist_module) in enumerate(\n            zip(quant_ebc._lookups, quant_ebc._output_dists)\n        ):\n            embedding_fx_path = f\"embedding_lookup.sharding_{i}\"\n            emb_module._fx_path = embedding_fx_path\n            emb_dist_module._fx_path = f\"embedding_dist.{i}\"\n            for rank, rank_module in enumerate(emb_module._embedding_lookups_per_rank):\n                rank_fx_path = f\"{embedding_fx_path}.rank_{rank}\"\n                rank_module._fx_path = rank_fx_path\n                for group, group_module in enumerate(rank_module._emb_modules):\n                    group_module._fx_path = f\"{rank_fx_path}.group_{group}\"\n                    group_module._emb_module._fx_path = (\n                        f\"{rank_fx_path}.group_{group}.tbe\"\n                    )",
  "def recursive_populate_fx_names(module: nn.Module) -> None:\n    if isinstance(module, QuantEmbeddingBagCollection) or isinstance(\n        module, ShardedQuantEmbeddingBagCollection\n    ):\n        populate_fx_names(module)\n        return\n    for submodule in module.children():\n        recursive_populate_fx_names(submodule)",
  "def meta_to_cpu_placement(module: torch.nn.Module) -> None:\n    if hasattr(module, \"_dmp_wrapped_module\"):\n        # for placement update of dmp module, we need to fetch .module (read access) and write\n        # to .dmp_wrapped_module (write access)\n        assert type(module) == DistributedModelParallel\n        _meta_to_cpu_placement(module.module, module, \"_dmp_wrapped_module\")\n    else:\n        # shard module case\n        _meta_to_cpu_placement(module, module)",
  "def _meta_to_cpu_placement(\n    module: nn.Module, root_module: nn.Module, name: Optional[str] = None\n) -> None:\n    if (\n        name is not None\n        and isinstance(module, QuantEmbeddingBagCollection)\n        and module.device.type == \"meta\"\n    ):\n        qebc_cpu = QuantEmbeddingBagCollection(\n            tables=module.embedding_bag_configs(),\n            is_weighted=module.is_weighted(),\n            device=torch.device(\"cpu\"),\n            output_dtype=module.output_dtype(),\n            register_tbes=module.register_tbes,\n            row_alignment=module.row_alignment,\n        )\n        setattr(root_module, name, qebc_cpu)\n    elif (\n        name is not None\n        and isinstance(module, QuantEmbeddingCollection)\n        and module.device.type == \"meta\"\n    ):\n        qec_cpu = QuantEmbeddingCollection(\n            tables=module.embedding_configs(),\n            device=torch.device(\"cpu\"),\n            need_indices=module.need_indices(),\n            output_dtype=module.output_dtype(),\n            register_tbes=module.register_tbes,\n            row_alignment=module.row_alignment,\n        )\n        setattr(root_module, name, qec_cpu)\n    else:\n        for name, submodule in module.named_children():\n            _meta_to_cpu_placement(submodule, module, name)",
  "def for_each_module_of_type_do(\n    module: nn.Module,\n    module_types: List[Type[torch.nn.Module]],\n    op: Callable[[torch.nn.Module], None],\n) -> None:\n    for m in module.modules():\n        if any([isinstance(m, t) for t in module_types]):\n            op(m)",
  "def quant_prep_enable_quant_state_dict_split_scale_bias(module: nn.Module) -> None:\n    setattr(module, MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS, True)",
  "def quant_prep_enable_quant_state_dict_split_scale_bias_for_types(\n    module: nn.Module, module_types: List[Type[torch.nn.Module]]\n) -> None:\n    for_each_module_of_type_do(\n        module,\n        module_types,\n        lambda m: setattr(m, MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS, True),\n    )",
  "def quant_prep_enable_register_tbes(\n    module: nn.Module, module_types: List[Type[torch.nn.Module]]\n) -> None:\n    for_each_module_of_type_do(\n        module,\n        module_types,\n        lambda m: setattr(m, MODULE_ATTR_REGISTER_TBES_BOOL, True),\n    )",
  "def quant_prep_customize_row_alignment(\n    module: nn.Module, module_types: List[Type[torch.nn.Module]], row_alignment: int\n) -> None:\n    for_each_module_of_type_do(\n        module,\n        module_types,\n        lambda m: setattr(m, MODULE_ATTR_ROW_ALIGNMENT_INT, row_alignment),\n    )",
  "def quantize_state_dict(\n    module: nn.Module,\n    table_name_to_quantized_weights: Dict[str, Tuple[Tensor, Tensor]],\n    table_name_to_data_type: Dict[str, DataType],\n) -> torch.device:\n    device = torch.device(\"cpu\")\n    for key, tensor in module.state_dict().items():\n        # Extract table name from state dict key.\n        # e.g. ebc.embedding_bags.t1.weight\n        splits = key.split(\".\")\n        assert splits[-1] == \"weight\"\n        table_name = splits[-2]\n        data_type = table_name_to_data_type[table_name]\n        device = tensor.device\n        num_bits = DATA_TYPE_NUM_BITS[data_type]\n        if tensor.is_meta:\n            quant_weight = torch.empty(\n                (tensor.shape[0], (tensor.shape[1] * num_bits) // 8),\n                device=\"meta\",\n                dtype=torch.uint8,\n            )\n            if (\n                data_type == DataType.INT8\n                or data_type == DataType.INT4\n                or data_type == DataType.INT2\n            ):\n                scale_shift = torch.empty(\n                    (tensor.shape[0], 4),\n                    device=\"meta\",\n                    dtype=torch.uint8,\n                )\n            else:\n                scale_shift = None\n        else:\n            if tensor.dtype == torch.float or tensor.dtype == torch.float16:\n                if data_type == DataType.FP16:\n                    if tensor.dtype == torch.float:\n                        tensor = tensor.half()\n                    quant_res = tensor.view(torch.uint8)\n                else:\n                    quant_res = (\n                        torch.ops.fbgemm.FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf(\n                            tensor, num_bits\n                        )\n                    )\n            else:\n                raise Exception(\"Unsupported dtype: {tensor.dtype}\")\n            if (\n                data_type == DataType.INT8\n                or data_type == DataType.INT4\n                or data_type == DataType.INT2\n            ):\n                quant_weight, scale_shift = (\n                    quant_res[:, :-4],\n                    quant_res[:, -4:],\n                )\n            else:\n                quant_weight, scale_shift = quant_res, None\n        table_name_to_quantized_weights[table_name] = (quant_weight, scale_shift)\n    return device",
  "class EmbeddingBagCollection(EmbeddingBagCollectionInterface, ModuleNoCopyMixin):\n    \"\"\"\n    EmbeddingBagCollection represents a collection of pooled embeddings (EmbeddingBags).\n    This EmbeddingBagCollection is quantized for lower precision. It relies on fbgemm quantized ops and provides\n    table batching.\n\n    It processes sparse data in the form of KeyedJaggedTensor\n    with values of the form [F X B X L]\n    F: features (keys)\n    B: batch size\n    L: Length of sparse features (jagged)\n\n    and outputs a KeyedTensor with values of the form [B * (F * D)]\n    where\n    F: features (keys)\n    D: each feature's (key's) embedding dimension\n    B: batch size\n\n    Args:\n        table_name_to_quantized_weights (Dict[str, Tuple[Tensor, Tensor]]): map of tables to quantized weights\n        embedding_configs (List[EmbeddingBagConfig]): list of embedding tables\n        is_weighted: (bool): whether input KeyedJaggedTensor is weighted\n        device: (Optional[torch.device]): default compute device\n\n    Call Args:\n        features: KeyedJaggedTensor,\n\n    Returns:\n        KeyedTensor\n\n    Example::\n\n        table_0 = EmbeddingBagConfig(\n            name=\"t1\", embedding_dim=3, num_embeddings=10, feature_names=[\"f1\"]\n        )\n        table_1 = EmbeddingBagConfig(\n            name=\"t2\", embedding_dim=4, num_embeddings=10, feature_names=[\"f2\"]\n        )\n        ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])\n\n        #        0       1        2  <-- batch\n        # \"f1\"   [0,1] None    [2]\n        # \"f2\"   [3]    [4]    [5,6,7]\n        #  ^\n        # feature\n        features = KeyedJaggedTensor(\n            keys=[\"f1\", \"f2\"],\n            values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n            offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n        )\n\n        ebc.qconfig = torch.quantization.QConfig(\n            activation=torch.quantization.PlaceholderObserver.with_args(\n                dtype=torch.qint8\n            ),\n            weight=torch.quantization.PlaceholderObserver.with_args(dtype=torch.qint8),\n        )\n\n        qebc = QuantEmbeddingBagCollection.from_float(ebc)\n        quantized_embeddings = qebc(features)\n    \"\"\"\n\n    def __init__(\n        self,\n        tables: List[EmbeddingBagConfig],\n        is_weighted: bool,\n        device: torch.device,\n        output_dtype: torch.dtype = torch.float,\n        table_name_to_quantized_weights: Optional[\n            Dict[str, Tuple[Tensor, Tensor]]\n        ] = None,\n        register_tbes: bool = False,\n        quant_state_dict_split_scale_bias: bool = False,\n        row_alignment: int = DEFAULT_ROW_ALIGNMENT,\n    ) -> None:\n        super().__init__()\n        self._is_weighted = is_weighted\n        self._embedding_bag_configs: List[EmbeddingBagConfig] = tables\n        self._key_to_tables: Dict[\n            Tuple[PoolingType, DataType], List[EmbeddingBagConfig]\n        ] = defaultdict(list)\n        self._length_per_key: List[int] = []\n        # Registering in a List instead of ModuleList because we want don't want them to be auto-registered.\n        # Their states will be modified via self.embedding_bags\n        self._emb_modules: List[nn.Module] = []\n        self._output_dtype = output_dtype\n        self._device: torch.device = device\n        self._table_name_to_quantized_weights: Optional[\n            Dict[str, Tuple[Tensor, Tensor]]\n        ] = None\n        self.row_alignment = row_alignment\n\n        table_names = set()\n        for table in self._embedding_bag_configs:\n            if table.name in table_names:\n                raise ValueError(f\"Duplicate table name {table.name}\")\n            table_names.add(table.name)\n            self._length_per_key.extend(\n                [table.embedding_dim] * len(table.feature_names)\n            )\n            key = (table.pooling, table.data_type)\n            self._key_to_tables[key].append(table)\n\n        self._sum_length_per_key: int = sum(self._length_per_key)\n\n        location = (\n            EmbeddingLocation.HOST if device.type == \"cpu\" else EmbeddingLocation.DEVICE\n        )\n\n        for key, emb_configs in self._key_to_tables.items():\n            (pooling, data_type) = key\n            embedding_specs = []\n            weight_lists: Optional[\n                List[Tuple[torch.Tensor, Optional[torch.Tensor]]]\n            ] = ([] if table_name_to_quantized_weights else None)\n            feature_table_map: List[int] = []\n\n            for idx, table in enumerate(emb_configs):\n                embedding_specs.append(\n                    (\n                        table.name,\n                        table.num_embeddings,\n                        table.embedding_dim,\n                        data_type_to_sparse_type(data_type),\n                        location,\n                    )\n                )\n                if table_name_to_quantized_weights:\n                    none_throws(weight_lists).append(\n                        table_name_to_quantized_weights[table.name]\n                    )\n                feature_table_map.extend([idx] * table.num_features())\n\n            emb_module = IntNBitTableBatchedEmbeddingBagsCodegen(\n                embedding_specs=embedding_specs,\n                pooling_mode=pooling_type_to_pooling_mode(pooling),\n                weight_lists=weight_lists,\n                device=device,\n                output_dtype=data_type_to_sparse_type(dtype_to_data_type(output_dtype)),\n                row_alignment=row_alignment,\n                feature_table_map=feature_table_map,\n            )\n            if weight_lists is None:\n                emb_module.initialize_weights()\n            self._emb_modules.append(emb_module)\n\n        self._embedding_names: List[str] = list(\n            itertools.chain(*get_embedding_names_by_table(self._embedding_bag_configs))\n        )\n        # We map over the parameters from FBGEMM backed kernels to the canonical nn.EmbeddingBag\n        # representation. This provides consistency between this class and the EmbeddingBagCollection\n        # nn.Module API calls (state_dict, named_modules, etc)\n        self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n        for (_key, tables), emb_module in zip(\n            self._key_to_tables.items(), self._emb_modules\n        ):\n            for embedding_config, (weight, qscale, qbias) in zip(\n                tables,\n                emb_module.split_embedding_weights_with_scale_bias(\n                    split_scale_bias_mode=2 if quant_state_dict_split_scale_bias else 0\n                ),\n            ):\n                self.embedding_bags[embedding_config.name] = torch.nn.Module()\n                # register as a buffer so it's exposed in state_dict.\n                # TODO: register as param instead of buffer\n                # however, since this is only needed for inference, we do not need to expose it as part of parameters.\n                # Additionally, we cannot expose uint8 weights as parameters due to autograd restrictions.\n                self.embedding_bags[embedding_config.name].register_buffer(\n                    \"weight\", weight\n                )\n                if quant_state_dict_split_scale_bias:\n                    self.embedding_bags[embedding_config.name].register_buffer(\n                        \"weight_qscale\", qscale\n                    )\n                    self.embedding_bags[embedding_config.name].register_buffer(\n                        \"weight_qbias\", qbias\n                    )\n\n        setattr(\n            self,\n            MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS,\n            quant_state_dict_split_scale_bias,\n        )\n        setattr(self, MODULE_ATTR_REGISTER_TBES_BOOL, register_tbes)\n        self.register_tbes = register_tbes\n        if register_tbes:\n            self.tbes: torch.nn.ModuleList = torch.nn.ModuleList(self._emb_modules)\n\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedTensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            KeyedTensor\n        \"\"\"\n\n        feature_dict = features.to_dict()\n        embeddings = []\n\n        # TODO ideally we can accept KJTs with any feature order. However, this will require an order check + permute, which will break torch.script.\n        # Once torchsccript is no longer a requirement, we should revisit this.\n\n        for emb_op, (_key, tables) in zip(\n            self._emb_modules, self._key_to_tables.items()\n        ):\n            indices = []\n            lengths = []\n            offsets = []\n            weights = []\n\n            for table in tables:\n                for feature in table.feature_names:\n                    f = feature_dict[feature]\n                    indices.append(f.values())\n                    lengths.append(f.lengths())\n                    if self._is_weighted:\n                        weights.append(f.weights())\n\n            indices = torch.cat(indices)\n            lengths = torch.cat(lengths)\n\n            offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)\n            if self._is_weighted:\n                weights = torch.cat(weights)\n\n            embeddings.append(\n                # Syntax for FX to generate call_module instead of call_function to keep TBE copied unchanged to fx.GraphModule, can be done only for registered module\n                emb_op(\n                    indices=indices.int(),\n                    offsets=offsets.int(),\n                    per_sample_weights=weights if self._is_weighted else None,\n                )\n                if self.register_tbes\n                else emb_op.forward(\n                    indices=indices.int(),\n                    offsets=offsets.int(),\n                    per_sample_weights=weights if self._is_weighted else None,\n                )\n            )\n\n        embeddings = torch.stack(embeddings).reshape(-1, self._sum_length_per_key)\n\n        return KeyedTensor(\n            keys=self._embedding_names,\n            values=embeddings,\n            length_per_key=self._length_per_key,\n        )\n\n    def _get_name(self) -> str:\n        return \"QuantizedEmbeddingBagCollection\"\n\n    @classmethod\n    def from_float(\n        cls, module: OriginalEmbeddingBagCollection\n    ) -> \"EmbeddingBagCollection\":\n        assert hasattr(\n            module, \"qconfig\"\n        ), \"EmbeddingBagCollection input float module must have qconfig defined\"\n        per_table_weight_dtype = (\n            module.qconfig.per_table_weight_dtype\n            if isinstance(module.qconfig, TrecQuantConfig)\n            else None\n        )\n        data_type = dtype_to_data_type(module.qconfig.weight().dtype)\n        embedding_bag_configs = copy.deepcopy(module.embedding_bag_configs())\n        table_name_to_data_type: Dict[str, DataType] = {}\n        for config in embedding_bag_configs:\n            if (\n                per_table_weight_dtype is not None\n                and config.name in per_table_weight_dtype\n            ):\n                config.data_type = dtype_to_data_type(\n                    per_table_weight_dtype[config.name]\n                )\n            else:\n                config.data_type = data_type\n            table_name_to_data_type[config.name] = config.data_type\n\n        table_name_to_quantized_weights: Dict[str, Tuple[Tensor, Tensor]] = {}\n        device = quantize_state_dict(\n            module, table_name_to_quantized_weights, table_name_to_data_type\n        )\n        return cls(\n            embedding_bag_configs,\n            module.is_weighted(),\n            device=device,\n            output_dtype=module.qconfig.activation().dtype,\n            table_name_to_quantized_weights=table_name_to_quantized_weights,\n            register_tbes=getattr(module, MODULE_ATTR_REGISTER_TBES_BOOL, False),\n            quant_state_dict_split_scale_bias=getattr(\n                module, MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS, False\n            ),\n            row_alignment=getattr(\n                module, MODULE_ATTR_ROW_ALIGNMENT_INT, DEFAULT_ROW_ALIGNMENT\n            ),\n        )\n\n    def embedding_bag_configs(\n        self,\n    ) -> List[EmbeddingBagConfig]:\n        return self._embedding_bag_configs\n\n    def is_weighted(self) -> bool:\n        return self._is_weighted\n\n    def output_dtype(self) -> torch.dtype:\n        return self._output_dtype\n\n    @property\n    def device(self) -> torch.device:\n        return self._device",
  "class EmbeddingCollection(EmbeddingCollectionInterface, ModuleNoCopyMixin):\n    \"\"\"\n    EmbeddingCollection represents a collection of non-pooled embeddings.\n\n    It processes sparse data in the form of `KeyedJaggedTensor` of the form [F X B X L]\n    where:\n\n    * F: features (keys)\n    * B: batch size\n    * L: length of sparse features (variable)\n\n    and outputs `Dict[feature (key), JaggedTensor]`.\n    Each `JaggedTensor` contains values of the form (B * L) X D\n    where:\n\n    * B: batch size\n    * L: length of sparse features (jagged)\n    * D: each feature's (key's) embedding dimension and lengths are of the form L\n\n    Args:\n        tables (List[EmbeddingConfig]): list of embedding tables.\n        device (Optional[torch.device]): default compute device.\n        need_indices (bool): if we need to pass indices to the final lookup result dict\n\n    Example::\n\n        e1_config = EmbeddingConfig(\n            name=\"t1\", embedding_dim=3, num_embeddings=10, feature_names=[\"f1\"]\n        )\n        e2_config = EmbeddingConfig(\n            name=\"t2\", embedding_dim=3, num_embeddings=10, feature_names=[\"f2\"]\n        )\n\n        ec = EmbeddingCollection(tables=[e1_config, e2_config])\n\n        #     0       1        2  <-- batch\n        # 0   [0,1] None    [2]\n        # 1   [3]    [4]    [5,6,7]\n        # ^\n        # feature\n\n        features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f1\", \"f2\"],\n            values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n            offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n        )\n        feature_embeddings = ec(features)\n        print(feature_embeddings['f2'].values())\n        tensor([[-0.2050,  0.5478,  0.6054],\n        [ 0.7352,  0.3210, -3.0399],\n        [ 0.1279, -0.1756, -0.4130],\n        [ 0.7519, -0.4341, -0.0499],\n        [ 0.9329, -1.0697, -0.8095]], grad_fn=<EmbeddingBackward>)\n    \"\"\"\n\n    def __init__(  # noqa C901\n        self,\n        tables: List[EmbeddingConfig],\n        device: torch.device,\n        need_indices: bool = False,\n        output_dtype: torch.dtype = torch.float,\n        table_name_to_quantized_weights: Optional[\n            Dict[str, Tuple[Tensor, Tensor]]\n        ] = None,\n        register_tbes: bool = False,\n        quant_state_dict_split_scale_bias: bool = False,\n        row_alignment: int = DEFAULT_ROW_ALIGNMENT,\n    ) -> None:\n        super().__init__()\n        self._emb_modules: List[IntNBitTableBatchedEmbeddingBagsCodegen] = []\n        self.embeddings: nn.ModuleDict = nn.ModuleDict()\n\n        self._embedding_configs = tables\n        self._embedding_dim: int = -1\n        self._need_indices: bool = need_indices\n        self._output_dtype = output_dtype\n        self._device = device\n        self.row_alignment = row_alignment\n\n        table_names = set()\n        for config in tables:\n            if config.name in table_names:\n                raise ValueError(f\"Duplicate table name {config.name}\")\n            table_names.add(config.name)\n            self._embedding_dim = (\n                config.embedding_dim if self._embedding_dim < 0 else self._embedding_dim\n            )\n            if self._embedding_dim != config.embedding_dim:\n                raise ValueError(\n                    \"All tables in a EmbeddingCollection are required to have same embedding dimension.\"\n                )\n            weight_lists: Optional[\n                List[Tuple[torch.Tensor, Optional[torch.Tensor]]]\n            ] = ([] if table_name_to_quantized_weights else None)\n            if table_name_to_quantized_weights:\n                none_throws(weight_lists).append(\n                    table_name_to_quantized_weights[config.name]\n                )\n            emb_module = IntNBitTableBatchedEmbeddingBagsCodegen(\n                embedding_specs=[\n                    (\n                        config.name,\n                        config.num_embeddings,\n                        config.embedding_dim,\n                        data_type_to_sparse_type(config.data_type),\n                        EmbeddingLocation.HOST\n                        if device.type == \"cpu\"\n                        else EmbeddingLocation.DEVICE,\n                    )\n                ],\n                pooling_mode=PoolingMode.NONE,\n                weight_lists=weight_lists,\n                device=device,\n                output_dtype=data_type_to_sparse_type(dtype_to_data_type(output_dtype)),\n                row_alignment=row_alignment,\n            )\n            if weight_lists is None:\n                emb_module.initialize_weights()\n\n            self._emb_modules.append(emb_module)\n            self.embeddings[config.name] = torch.nn.Module()\n            # register as a buffer so it's exposed in state_dict.\n            # TODO: register as param instead of buffer\n            # however, since this is only needed for inference, we do not need to expose it as part of parameters.\n            # Additionally, we cannot expose uint8 weights as parameters due to autograd restrictions.\n            weights_list = emb_module.split_embedding_weights_with_scale_bias(\n                split_scale_bias_mode=2 if quant_state_dict_split_scale_bias else 0\n            )\n            self.embeddings[config.name].register_buffer(\"weight\", weights_list[0][0])\n            if quant_state_dict_split_scale_bias:\n                self.embeddings[config.name].register_buffer(\n                    \"weight_qscale\", weights_list[0][1]\n                )\n                self.embeddings[config.name].register_buffer(\n                    \"weight_qbias\", weights_list[0][2]\n                )\n\n            if not config.feature_names:\n                config.feature_names = [config.name]\n\n        self._embedding_names_by_table: List[List[str]] = get_embedding_names_by_table(\n            tables\n        )\n        setattr(\n            self,\n            MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS,\n            quant_state_dict_split_scale_bias,\n        )\n        setattr(self, MODULE_ATTR_REGISTER_TBES_BOOL, register_tbes)\n        self.register_tbes = register_tbes\n        if register_tbes:\n            self.tbes: torch.nn.ModuleList = torch.nn.ModuleList(self._emb_modules)\n\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            Dict[str, JaggedTensor]\n        \"\"\"\n\n        feature_embeddings: Dict[str, JaggedTensor] = {}\n        jt_dict: Dict[str, JaggedTensor] = features.to_dict()\n        for config, embedding_names, emb_module in zip(\n            self._embedding_configs,\n            self._embedding_names_by_table,\n            self._emb_modules,\n        ):\n            for feature_name, embedding_name in zip(\n                config.feature_names, embedding_names\n            ):\n                f = jt_dict[feature_name]\n                values = f.values()\n                offsets = f.offsets()\n                # Syntax for FX to generate call_module instead of call_function to keep TBE copied unchanged to fx.GraphModule, can be done only for registered module\n                lookup = (\n                    emb_module(indices=values.int(), offsets=offsets.int())\n                    if self.register_tbes\n                    else emb_module.forward(indices=values.int(), offsets=offsets.int())\n                )\n                feature_embeddings[embedding_name] = JaggedTensor(\n                    values=lookup,\n                    lengths=f.lengths(),\n                    weights=f.values() if self.need_indices else None,\n                )\n        return feature_embeddings\n\n    @classmethod\n    def from_float(cls, module: OriginalEmbeddingCollection) -> \"EmbeddingCollection\":\n        assert hasattr(\n            module, \"qconfig\"\n        ), \"EmbeddingCollection input float module must have qconfig defined\"\n        per_table_weight_dtype = (\n            module.qconfig.per_table_weight_dtype\n            if isinstance(module.qconfig, TrecQuantConfig)\n            else None\n        )\n        data_type = dtype_to_data_type(module.qconfig.weight().dtype)\n        tables = copy.deepcopy(module.embedding_configs())\n        table_name_to_data_type: Dict[str, DataType] = {}\n        for config in tables:\n            if (\n                per_table_weight_dtype is not None\n                and config.name in per_table_weight_dtype\n            ):\n                config.data_type = dtype_to_data_type(\n                    per_table_weight_dtype[config.name]\n                )\n            else:\n                config.data_type = data_type\n            table_name_to_data_type[config.name] = config.data_type\n        table_name_to_quantized_weights: Dict[str, Tuple[Tensor, Tensor]] = {}\n        device = quantize_state_dict(\n            module, table_name_to_quantized_weights, table_name_to_data_type\n        )\n        return cls(\n            tables,\n            device=device,\n            need_indices=module.need_indices(),\n            output_dtype=module.qconfig.activation().dtype,\n            table_name_to_quantized_weights=table_name_to_quantized_weights,\n            register_tbes=getattr(module, MODULE_ATTR_REGISTER_TBES_BOOL, False),\n            quant_state_dict_split_scale_bias=getattr(\n                module, MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS, False\n            ),\n            row_alignment=getattr(\n                module, MODULE_ATTR_ROW_ALIGNMENT_INT, DEFAULT_ROW_ALIGNMENT\n            ),\n        )\n\n    def _get_name(self) -> str:\n        return \"QuantizedEmbeddingCollection\"\n\n    def need_indices(self) -> bool:\n        return self._need_indices\n\n    def embedding_dim(self) -> int:\n        return self._embedding_dim\n\n    def embedding_configs(self) -> List[EmbeddingConfig]:\n        return self._embedding_configs\n\n    def embedding_names_by_table(self) -> List[List[str]]:\n        return self._embedding_names_by_table\n\n    def output_dtype(self) -> torch.dtype:\n        return self._output_dtype\n\n    @property\n    def device(self) -> torch.device:\n        return self._device",
  "def __init__(\n        self,\n        tables: List[EmbeddingBagConfig],\n        is_weighted: bool,\n        device: torch.device,\n        output_dtype: torch.dtype = torch.float,\n        table_name_to_quantized_weights: Optional[\n            Dict[str, Tuple[Tensor, Tensor]]\n        ] = None,\n        register_tbes: bool = False,\n        quant_state_dict_split_scale_bias: bool = False,\n        row_alignment: int = DEFAULT_ROW_ALIGNMENT,\n    ) -> None:\n        super().__init__()\n        self._is_weighted = is_weighted\n        self._embedding_bag_configs: List[EmbeddingBagConfig] = tables\n        self._key_to_tables: Dict[\n            Tuple[PoolingType, DataType], List[EmbeddingBagConfig]\n        ] = defaultdict(list)\n        self._length_per_key: List[int] = []\n        # Registering in a List instead of ModuleList because we want don't want them to be auto-registered.\n        # Their states will be modified via self.embedding_bags\n        self._emb_modules: List[nn.Module] = []\n        self._output_dtype = output_dtype\n        self._device: torch.device = device\n        self._table_name_to_quantized_weights: Optional[\n            Dict[str, Tuple[Tensor, Tensor]]\n        ] = None\n        self.row_alignment = row_alignment\n\n        table_names = set()\n        for table in self._embedding_bag_configs:\n            if table.name in table_names:\n                raise ValueError(f\"Duplicate table name {table.name}\")\n            table_names.add(table.name)\n            self._length_per_key.extend(\n                [table.embedding_dim] * len(table.feature_names)\n            )\n            key = (table.pooling, table.data_type)\n            self._key_to_tables[key].append(table)\n\n        self._sum_length_per_key: int = sum(self._length_per_key)\n\n        location = (\n            EmbeddingLocation.HOST if device.type == \"cpu\" else EmbeddingLocation.DEVICE\n        )\n\n        for key, emb_configs in self._key_to_tables.items():\n            (pooling, data_type) = key\n            embedding_specs = []\n            weight_lists: Optional[\n                List[Tuple[torch.Tensor, Optional[torch.Tensor]]]\n            ] = ([] if table_name_to_quantized_weights else None)\n            feature_table_map: List[int] = []\n\n            for idx, table in enumerate(emb_configs):\n                embedding_specs.append(\n                    (\n                        table.name,\n                        table.num_embeddings,\n                        table.embedding_dim,\n                        data_type_to_sparse_type(data_type),\n                        location,\n                    )\n                )\n                if table_name_to_quantized_weights:\n                    none_throws(weight_lists).append(\n                        table_name_to_quantized_weights[table.name]\n                    )\n                feature_table_map.extend([idx] * table.num_features())\n\n            emb_module = IntNBitTableBatchedEmbeddingBagsCodegen(\n                embedding_specs=embedding_specs,\n                pooling_mode=pooling_type_to_pooling_mode(pooling),\n                weight_lists=weight_lists,\n                device=device,\n                output_dtype=data_type_to_sparse_type(dtype_to_data_type(output_dtype)),\n                row_alignment=row_alignment,\n                feature_table_map=feature_table_map,\n            )\n            if weight_lists is None:\n                emb_module.initialize_weights()\n            self._emb_modules.append(emb_module)\n\n        self._embedding_names: List[str] = list(\n            itertools.chain(*get_embedding_names_by_table(self._embedding_bag_configs))\n        )\n        # We map over the parameters from FBGEMM backed kernels to the canonical nn.EmbeddingBag\n        # representation. This provides consistency between this class and the EmbeddingBagCollection\n        # nn.Module API calls (state_dict, named_modules, etc)\n        self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n        for (_key, tables), emb_module in zip(\n            self._key_to_tables.items(), self._emb_modules\n        ):\n            for embedding_config, (weight, qscale, qbias) in zip(\n                tables,\n                emb_module.split_embedding_weights_with_scale_bias(\n                    split_scale_bias_mode=2 if quant_state_dict_split_scale_bias else 0\n                ),\n            ):\n                self.embedding_bags[embedding_config.name] = torch.nn.Module()\n                # register as a buffer so it's exposed in state_dict.\n                # TODO: register as param instead of buffer\n                # however, since this is only needed for inference, we do not need to expose it as part of parameters.\n                # Additionally, we cannot expose uint8 weights as parameters due to autograd restrictions.\n                self.embedding_bags[embedding_config.name].register_buffer(\n                    \"weight\", weight\n                )\n                if quant_state_dict_split_scale_bias:\n                    self.embedding_bags[embedding_config.name].register_buffer(\n                        \"weight_qscale\", qscale\n                    )\n                    self.embedding_bags[embedding_config.name].register_buffer(\n                        \"weight_qbias\", qbias\n                    )\n\n        setattr(\n            self,\n            MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS,\n            quant_state_dict_split_scale_bias,\n        )\n        setattr(self, MODULE_ATTR_REGISTER_TBES_BOOL, register_tbes)\n        self.register_tbes = register_tbes\n        if register_tbes:\n            self.tbes: torch.nn.ModuleList = torch.nn.ModuleList(self._emb_modules)",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedTensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            KeyedTensor\n        \"\"\"\n\n        feature_dict = features.to_dict()\n        embeddings = []\n\n        # TODO ideally we can accept KJTs with any feature order. However, this will require an order check + permute, which will break torch.script.\n        # Once torchsccript is no longer a requirement, we should revisit this.\n\n        for emb_op, (_key, tables) in zip(\n            self._emb_modules, self._key_to_tables.items()\n        ):\n            indices = []\n            lengths = []\n            offsets = []\n            weights = []\n\n            for table in tables:\n                for feature in table.feature_names:\n                    f = feature_dict[feature]\n                    indices.append(f.values())\n                    lengths.append(f.lengths())\n                    if self._is_weighted:\n                        weights.append(f.weights())\n\n            indices = torch.cat(indices)\n            lengths = torch.cat(lengths)\n\n            offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)\n            if self._is_weighted:\n                weights = torch.cat(weights)\n\n            embeddings.append(\n                # Syntax for FX to generate call_module instead of call_function to keep TBE copied unchanged to fx.GraphModule, can be done only for registered module\n                emb_op(\n                    indices=indices.int(),\n                    offsets=offsets.int(),\n                    per_sample_weights=weights if self._is_weighted else None,\n                )\n                if self.register_tbes\n                else emb_op.forward(\n                    indices=indices.int(),\n                    offsets=offsets.int(),\n                    per_sample_weights=weights if self._is_weighted else None,\n                )\n            )\n\n        embeddings = torch.stack(embeddings).reshape(-1, self._sum_length_per_key)\n\n        return KeyedTensor(\n            keys=self._embedding_names,\n            values=embeddings,\n            length_per_key=self._length_per_key,\n        )",
  "def _get_name(self) -> str:\n        return \"QuantizedEmbeddingBagCollection\"",
  "def from_float(\n        cls, module: OriginalEmbeddingBagCollection\n    ) -> \"EmbeddingBagCollection\":\n        assert hasattr(\n            module, \"qconfig\"\n        ), \"EmbeddingBagCollection input float module must have qconfig defined\"\n        per_table_weight_dtype = (\n            module.qconfig.per_table_weight_dtype\n            if isinstance(module.qconfig, TrecQuantConfig)\n            else None\n        )\n        data_type = dtype_to_data_type(module.qconfig.weight().dtype)\n        embedding_bag_configs = copy.deepcopy(module.embedding_bag_configs())\n        table_name_to_data_type: Dict[str, DataType] = {}\n        for config in embedding_bag_configs:\n            if (\n                per_table_weight_dtype is not None\n                and config.name in per_table_weight_dtype\n            ):\n                config.data_type = dtype_to_data_type(\n                    per_table_weight_dtype[config.name]\n                )\n            else:\n                config.data_type = data_type\n            table_name_to_data_type[config.name] = config.data_type\n\n        table_name_to_quantized_weights: Dict[str, Tuple[Tensor, Tensor]] = {}\n        device = quantize_state_dict(\n            module, table_name_to_quantized_weights, table_name_to_data_type\n        )\n        return cls(\n            embedding_bag_configs,\n            module.is_weighted(),\n            device=device,\n            output_dtype=module.qconfig.activation().dtype,\n            table_name_to_quantized_weights=table_name_to_quantized_weights,\n            register_tbes=getattr(module, MODULE_ATTR_REGISTER_TBES_BOOL, False),\n            quant_state_dict_split_scale_bias=getattr(\n                module, MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS, False\n            ),\n            row_alignment=getattr(\n                module, MODULE_ATTR_ROW_ALIGNMENT_INT, DEFAULT_ROW_ALIGNMENT\n            ),\n        )",
  "def embedding_bag_configs(\n        self,\n    ) -> List[EmbeddingBagConfig]:\n        return self._embedding_bag_configs",
  "def is_weighted(self) -> bool:\n        return self._is_weighted",
  "def output_dtype(self) -> torch.dtype:\n        return self._output_dtype",
  "def device(self) -> torch.device:\n        return self._device",
  "def __init__(  # noqa C901\n        self,\n        tables: List[EmbeddingConfig],\n        device: torch.device,\n        need_indices: bool = False,\n        output_dtype: torch.dtype = torch.float,\n        table_name_to_quantized_weights: Optional[\n            Dict[str, Tuple[Tensor, Tensor]]\n        ] = None,\n        register_tbes: bool = False,\n        quant_state_dict_split_scale_bias: bool = False,\n        row_alignment: int = DEFAULT_ROW_ALIGNMENT,\n    ) -> None:\n        super().__init__()\n        self._emb_modules: List[IntNBitTableBatchedEmbeddingBagsCodegen] = []\n        self.embeddings: nn.ModuleDict = nn.ModuleDict()\n\n        self._embedding_configs = tables\n        self._embedding_dim: int = -1\n        self._need_indices: bool = need_indices\n        self._output_dtype = output_dtype\n        self._device = device\n        self.row_alignment = row_alignment\n\n        table_names = set()\n        for config in tables:\n            if config.name in table_names:\n                raise ValueError(f\"Duplicate table name {config.name}\")\n            table_names.add(config.name)\n            self._embedding_dim = (\n                config.embedding_dim if self._embedding_dim < 0 else self._embedding_dim\n            )\n            if self._embedding_dim != config.embedding_dim:\n                raise ValueError(\n                    \"All tables in a EmbeddingCollection are required to have same embedding dimension.\"\n                )\n            weight_lists: Optional[\n                List[Tuple[torch.Tensor, Optional[torch.Tensor]]]\n            ] = ([] if table_name_to_quantized_weights else None)\n            if table_name_to_quantized_weights:\n                none_throws(weight_lists).append(\n                    table_name_to_quantized_weights[config.name]\n                )\n            emb_module = IntNBitTableBatchedEmbeddingBagsCodegen(\n                embedding_specs=[\n                    (\n                        config.name,\n                        config.num_embeddings,\n                        config.embedding_dim,\n                        data_type_to_sparse_type(config.data_type),\n                        EmbeddingLocation.HOST\n                        if device.type == \"cpu\"\n                        else EmbeddingLocation.DEVICE,\n                    )\n                ],\n                pooling_mode=PoolingMode.NONE,\n                weight_lists=weight_lists,\n                device=device,\n                output_dtype=data_type_to_sparse_type(dtype_to_data_type(output_dtype)),\n                row_alignment=row_alignment,\n            )\n            if weight_lists is None:\n                emb_module.initialize_weights()\n\n            self._emb_modules.append(emb_module)\n            self.embeddings[config.name] = torch.nn.Module()\n            # register as a buffer so it's exposed in state_dict.\n            # TODO: register as param instead of buffer\n            # however, since this is only needed for inference, we do not need to expose it as part of parameters.\n            # Additionally, we cannot expose uint8 weights as parameters due to autograd restrictions.\n            weights_list = emb_module.split_embedding_weights_with_scale_bias(\n                split_scale_bias_mode=2 if quant_state_dict_split_scale_bias else 0\n            )\n            self.embeddings[config.name].register_buffer(\"weight\", weights_list[0][0])\n            if quant_state_dict_split_scale_bias:\n                self.embeddings[config.name].register_buffer(\n                    \"weight_qscale\", weights_list[0][1]\n                )\n                self.embeddings[config.name].register_buffer(\n                    \"weight_qbias\", weights_list[0][2]\n                )\n\n            if not config.feature_names:\n                config.feature_names = [config.name]\n\n        self._embedding_names_by_table: List[List[str]] = get_embedding_names_by_table(\n            tables\n        )\n        setattr(\n            self,\n            MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS,\n            quant_state_dict_split_scale_bias,\n        )\n        setattr(self, MODULE_ATTR_REGISTER_TBES_BOOL, register_tbes)\n        self.register_tbes = register_tbes\n        if register_tbes:\n            self.tbes: torch.nn.ModuleList = torch.nn.ModuleList(self._emb_modules)",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            Dict[str, JaggedTensor]\n        \"\"\"\n\n        feature_embeddings: Dict[str, JaggedTensor] = {}\n        jt_dict: Dict[str, JaggedTensor] = features.to_dict()\n        for config, embedding_names, emb_module in zip(\n            self._embedding_configs,\n            self._embedding_names_by_table,\n            self._emb_modules,\n        ):\n            for feature_name, embedding_name in zip(\n                config.feature_names, embedding_names\n            ):\n                f = jt_dict[feature_name]\n                values = f.values()\n                offsets = f.offsets()\n                # Syntax for FX to generate call_module instead of call_function to keep TBE copied unchanged to fx.GraphModule, can be done only for registered module\n                lookup = (\n                    emb_module(indices=values.int(), offsets=offsets.int())\n                    if self.register_tbes\n                    else emb_module.forward(indices=values.int(), offsets=offsets.int())\n                )\n                feature_embeddings[embedding_name] = JaggedTensor(\n                    values=lookup,\n                    lengths=f.lengths(),\n                    weights=f.values() if self.need_indices else None,\n                )\n        return feature_embeddings",
  "def from_float(cls, module: OriginalEmbeddingCollection) -> \"EmbeddingCollection\":\n        assert hasattr(\n            module, \"qconfig\"\n        ), \"EmbeddingCollection input float module must have qconfig defined\"\n        per_table_weight_dtype = (\n            module.qconfig.per_table_weight_dtype\n            if isinstance(module.qconfig, TrecQuantConfig)\n            else None\n        )\n        data_type = dtype_to_data_type(module.qconfig.weight().dtype)\n        tables = copy.deepcopy(module.embedding_configs())\n        table_name_to_data_type: Dict[str, DataType] = {}\n        for config in tables:\n            if (\n                per_table_weight_dtype is not None\n                and config.name in per_table_weight_dtype\n            ):\n                config.data_type = dtype_to_data_type(\n                    per_table_weight_dtype[config.name]\n                )\n            else:\n                config.data_type = data_type\n            table_name_to_data_type[config.name] = config.data_type\n        table_name_to_quantized_weights: Dict[str, Tuple[Tensor, Tensor]] = {}\n        device = quantize_state_dict(\n            module, table_name_to_quantized_weights, table_name_to_data_type\n        )\n        return cls(\n            tables,\n            device=device,\n            need_indices=module.need_indices(),\n            output_dtype=module.qconfig.activation().dtype,\n            table_name_to_quantized_weights=table_name_to_quantized_weights,\n            register_tbes=getattr(module, MODULE_ATTR_REGISTER_TBES_BOOL, False),\n            quant_state_dict_split_scale_bias=getattr(\n                module, MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS, False\n            ),\n            row_alignment=getattr(\n                module, MODULE_ATTR_ROW_ALIGNMENT_INT, DEFAULT_ROW_ALIGNMENT\n            ),\n        )",
  "def _get_name(self) -> str:\n        return \"QuantizedEmbeddingCollection\"",
  "def need_indices(self) -> bool:\n        return self._need_indices",
  "def embedding_dim(self) -> int:\n        return self._embedding_dim",
  "def embedding_configs(self) -> List[EmbeddingConfig]:\n        return self._embedding_configs",
  "def embedding_names_by_table(self) -> List[List[str]]:\n        return self._embedding_names_by_table",
  "def output_dtype(self) -> torch.dtype:\n        return self._output_dtype",
  "def device(self) -> torch.device:\n        return self._device",
  "class Batch(Pipelineable):\n    dense_features: torch.Tensor\n    sparse_features: KeyedJaggedTensor\n    labels: torch.Tensor\n\n    def to(self, device: torch.device, non_blocking: bool = False) -> \"Batch\":\n        return Batch(\n            dense_features=self.dense_features.to(\n                device=device, non_blocking=non_blocking\n            ),\n            sparse_features=self.sparse_features.to(\n                device=device, non_blocking=non_blocking\n            ),\n            labels=self.labels.to(device=device, non_blocking=non_blocking),\n        )\n\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n        self.dense_features.record_stream(stream)\n        self.sparse_features.record_stream(stream)\n        # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n        self.labels.record_stream(stream)\n\n    def pin_memory(self) -> \"Batch\":\n        return Batch(\n            dense_features=self.dense_features.pin_memory(),\n            sparse_features=self.sparse_features.pin_memory(),\n            labels=self.labels.pin_memory(),\n        )",
  "class _IdxFilter(IterDataPipe):\n    def __init__(\n        self, datapipe: IterDataPipe, filter_fn: Callable[[int], bool]\n    ) -> None:\n        super().__init__()\n        self.datapipe = datapipe\n        self.filter_fn = filter_fn\n\n    # pyre-ignore[3]\n    def __iter__(self) -> Iterator[Any]:\n        for idx, data in enumerate(self.datapipe):\n            if self.filter_fn(idx):\n                yield data",
  "def _default_key_fn(idx: int) -> int:\n    return idx",
  "def train_filter(\n    key_fn: Callable[[int], int],\n    train_perc: float,\n    decimal_places: int,\n    idx: int,\n) -> bool:\n    return (key_fn(idx) % 10**decimal_places) < round(\n        train_perc * 10**decimal_places\n    )",
  "def val_filter(\n    key_fn: Callable[[int], int],\n    train_perc: float,\n    decimal_places: int,\n    idx: int,\n) -> bool:\n    return not train_filter(key_fn, train_perc, decimal_places, idx)",
  "def idx_split_train_val(\n    datapipe: IterDataPipe,\n    train_perc: float,\n    decimal_places: int = 2,\n    key_fn: Callable[[int], int] = _default_key_fn,\n) -> Tuple[IterDataPipe, IterDataPipe]:\n    if not 0.0 < train_perc < 1.0:\n        raise ValueError(\"train_perc must be in range (0.0, 1.0)\")\n    return (\n        _IdxFilter(datapipe, partial(train_filter, key_fn, train_perc, decimal_places)),\n        _IdxFilter(datapipe, partial(val_filter, key_fn, train_perc, decimal_places)),\n    )",
  "class _RandFilter(IterDataPipe):\n    def __init__(\n        self,\n        datapipe: IterDataPipe,\n        filter_fn: Callable[[random.Random], bool],\n        rand_gen: random.Random,\n    ) -> None:\n        super().__init__()\n        self.datapipe = datapipe\n        self.filter_fn = filter_fn\n        self.rand_gen = rand_gen\n        # pyre-ignore[4]\n        self.rand_gen_init_state: Tuple[Any, ...] = rand_gen.getstate()\n\n    # pyre-ignore[3]\n    def __iter__(self) -> Iterator[Any]:\n        self.rand_gen.setstate(self.rand_gen_init_state)\n        for data in self.datapipe:\n            if self.filter_fn(self.rand_gen):\n                yield data",
  "def _rand_train_filter_fn(\n    train_perc: float,\n    rand_gen: random.Random,\n) -> bool:\n    return rand_gen.random() < train_perc",
  "def _rand_val_filter_fn(train_perc: float, rand_gen: random.Random) -> bool:\n    return not _rand_train_filter_fn(train_perc, rand_gen)",
  "def rand_split_train_val(\n    datapipe: IterDataPipe,\n    train_perc: float,\n    random_seed: int = 0,\n) -> Tuple[IterDataPipe, IterDataPipe]:\n    \"\"\"Via uniform random sampling, generates two IterDataPipe instances representing\n    disjoint train and val splits of the given IterDataPipe.\n\n    Args:\n        datapipe (IterDataPipe): datapipe to split.\n        train_perc (float): value in range (0.0, 1.0) specifying target proportion of\n            datapipe samples to include in train split. Note that the actual proportion\n            is not guaranteed to match train_perc exactly.\n        random_seed (int): determines split membership for a given sample\n            and train_perc. Use the same value across calls to generate consistent splits.\n    Example::\n\n        datapipe = criteo_terabyte(\n            (\"/home/datasets/criteo/day_0.tsv\", \"/home/datasets/criteo/day_1.tsv\")\n        )\n        train_datapipe, val_datapipe = rand_split_train_val(datapipe, 0.75)\n        train_batch = next(iter(train_datapipe))\n        val_batch = next(iter(val_datapipe))\n    \"\"\"\n    if not 0.0 < train_perc < 1.0:\n        raise ValueError(\"train_perc must be in range (0.0, 1.0)\")\n\n    return _RandFilter(\n        datapipe, partial(_rand_train_filter_fn, train_perc), random.Random(random_seed)\n    ), _RandFilter(\n        datapipe, partial(_rand_val_filter_fn, train_perc), random.Random(random_seed)\n    )",
  "def safe_cast(val: T, dest_type: Callable[[T], T], default: T) -> T:\n    try:\n        return dest_type(val)\n    except ValueError:\n        return default",
  "class Limit(IterDataPipe):\n    def __init__(self, datapipe: IterDataPipe, limit: int) -> None:\n        super().__init__()\n        self.datapipe = datapipe\n        self.limit = limit\n\n    # pyre-ignore[3]\n    def __iter__(self) -> Iterator[Any]:\n        for idx, data in enumerate(self.datapipe):\n            if idx >= self.limit:\n                break\n            yield data",
  "class ReadLinesFromCSV(IterDataPipe):\n    def __init__(\n        self,\n        datapipe: IterDataPipe[Tuple[str, \"IOBase\"]],\n        skip_first_line: bool = False,\n        # pyre-ignore[2]\n        **kw,\n    ) -> None:\n        super().__init__()\n        self.datapipe = datapipe\n        self.skip_first_line = skip_first_line\n        # pyre-ignore[4]\n        self.kw = kw\n\n    def __iter__(self) -> Iterator[List[str]]:\n        for _, data in self.datapipe:\n            reader = csv.reader(data, **self.kw)\n            if self.skip_first_line:\n                next(reader, None)\n            for line in reader:\n                yield line",
  "class LoadFiles(IterDataPipe[Tuple[str, \"IOBase\"]]):\n    \"\"\"\n    Taken and adapted from torch.utils.data.datapipes.iter.LoadFilesFromDisk\n\n    TODO:\n    Merge this back or replace this with something in core Datapipes lib\n    \"\"\"\n\n    def __init__(\n        self,\n        datapipe: Iterable[str],\n        mode: str = \"b\",\n        length: int = -1,\n        path_manager_key: str = PATH_MANAGER_KEY,\n        # pyre-ignore[2]\n        **open_kw,\n    ) -> None:\n        super().__init__()\n        self.datapipe: Iterable[str] = datapipe\n        self.mode: str = mode\n        if self.mode not in (\"b\", \"t\", \"rb\", \"rt\", \"r\"):\n            raise ValueError(\"Invalid mode {}\".format(mode))\n        # TODO: enforce typing for each instance based on mode, otherwise\n        #       `argument_validation` with this DataPipe may be potentially broken\n        self.length: int = length\n        # pyre-ignore[4]\n        self.open_kw = open_kw\n        self.path_manager: PathManager = PathManagerFactory().get(path_manager_key)\n        self.path_manager.set_strict_kwargs_checking(False)\n\n    # Remove annotation due to 'IOBase' is a general type and true type\n    # is determined at runtime based on mode. Some `DataPipe` requiring\n    # a subtype would cause mypy error.\n    # pyre-ignore[3]\n    def __iter__(self):\n        if self.mode in (\"b\", \"t\"):\n            self.mode = \"r\" + self.mode\n        for pathname in self.datapipe:\n            if not isinstance(pathname, str):\n                raise TypeError(\n                    \"Expected string type for pathname, but got {}\".format(\n                        type(pathname)\n                    )\n                )\n            yield (\n                pathname,\n                self.path_manager.open(pathname, self.mode, **self.open_kw),\n            )\n\n    def __len__(self) -> int:\n        if self.length == -1:\n            raise NotImplementedError\n        return self.length",
  "def _default_dp_selector(\n    datapipes: Sequence[IterDataPipe],\n) -> Sequence[IterDataPipe]:\n    worker_info = get_worker_info()\n    if worker_info is None:\n        return datapipes\n    else:\n        if worker_info.num_workers > len(datapipes):\n            raise ValueError(\n                f\"Number of workers {worker_info.num_workers} exceeds\"\n                f\"number of datapipes ({len(datapipes)})!\"\n            )\n        offsets = [0]\n        for num_workers in reversed(range(1, worker_info.num_workers + 1)):\n            remaining_dps = len(datapipes) - offsets[-1]\n            dps_to_assign = math.ceil(remaining_dps / num_workers)\n            offsets.append(offsets[-1] + dps_to_assign)\n        return datapipes[offsets[worker_info.id] : offsets[worker_info.id + 1]]",
  "class ParallelReadConcat(IterDataPipe):\n    r\"\"\":class:`ParallelReadConcat`.\n\n    Iterable DataPipe that concatenates multiple Iterable DataPipes.\n    When used with a DataLoader, assigns a subset of datapipes to each DataLoader worker\n    to allow for parallel reading.\n    Args:\n        datapipes: IterDataPipe instances to read from.\n        dp_selector: function that each DataLoader worker would use to determine the subset of datapipes\n        to read from.\n    Example::\n\n        datapipes = [\n            criteo_terabyte(\n                (f\"/home/local/datasets/criteo/shard_{idx}.tsv\",),\n            )\n            .batch(100)\n            .collate()\n            for idx in range(4)\n        ]\n        dataloader = DataLoader(\n            ParallelReadConcat(*datapipes), num_workers=4, batch_size=None\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        *datapipes: IterDataPipe,\n        dp_selector: Callable[\n            [Sequence[IterDataPipe]], Sequence[IterDataPipe]\n        ] = _default_dp_selector,\n    ) -> None:\n        super().__init__()\n        self.datapipes: Tuple[IterDataPipe, ...] = datapipes\n        self.dp_selector = dp_selector\n\n    # pyre-ignore[3]\n    def __iter__(self) -> Iterator[Any]:\n        selected_dps = self.dp_selector(self.datapipes)\n        for dp in selected_dps:\n            for data in dp:\n                yield data",
  "def to(self, device: torch.device, non_blocking: bool = False) -> \"Batch\":\n        return Batch(\n            dense_features=self.dense_features.to(\n                device=device, non_blocking=non_blocking\n            ),\n            sparse_features=self.sparse_features.to(\n                device=device, non_blocking=non_blocking\n            ),\n            labels=self.labels.to(device=device, non_blocking=non_blocking),\n        )",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n        self.dense_features.record_stream(stream)\n        self.sparse_features.record_stream(stream)\n        # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n        self.labels.record_stream(stream)",
  "def pin_memory(self) -> \"Batch\":\n        return Batch(\n            dense_features=self.dense_features.pin_memory(),\n            sparse_features=self.sparse_features.pin_memory(),\n            labels=self.labels.pin_memory(),\n        )",
  "def __init__(\n        self, datapipe: IterDataPipe, filter_fn: Callable[[int], bool]\n    ) -> None:\n        super().__init__()\n        self.datapipe = datapipe\n        self.filter_fn = filter_fn",
  "def __iter__(self) -> Iterator[Any]:\n        for idx, data in enumerate(self.datapipe):\n            if self.filter_fn(idx):\n                yield data",
  "def __init__(\n        self,\n        datapipe: IterDataPipe,\n        filter_fn: Callable[[random.Random], bool],\n        rand_gen: random.Random,\n    ) -> None:\n        super().__init__()\n        self.datapipe = datapipe\n        self.filter_fn = filter_fn\n        self.rand_gen = rand_gen\n        # pyre-ignore[4]\n        self.rand_gen_init_state: Tuple[Any, ...] = rand_gen.getstate()",
  "def __iter__(self) -> Iterator[Any]:\n        self.rand_gen.setstate(self.rand_gen_init_state)\n        for data in self.datapipe:\n            if self.filter_fn(self.rand_gen):\n                yield data",
  "def __init__(self, datapipe: IterDataPipe, limit: int) -> None:\n        super().__init__()\n        self.datapipe = datapipe\n        self.limit = limit",
  "def __iter__(self) -> Iterator[Any]:\n        for idx, data in enumerate(self.datapipe):\n            if idx >= self.limit:\n                break\n            yield data",
  "def __init__(\n        self,\n        datapipe: IterDataPipe[Tuple[str, \"IOBase\"]],\n        skip_first_line: bool = False,\n        # pyre-ignore[2]\n        **kw,\n    ) -> None:\n        super().__init__()\n        self.datapipe = datapipe\n        self.skip_first_line = skip_first_line\n        # pyre-ignore[4]\n        self.kw = kw",
  "def __iter__(self) -> Iterator[List[str]]:\n        for _, data in self.datapipe:\n            reader = csv.reader(data, **self.kw)\n            if self.skip_first_line:\n                next(reader, None)\n            for line in reader:\n                yield line",
  "def __init__(\n        self,\n        datapipe: Iterable[str],\n        mode: str = \"b\",\n        length: int = -1,\n        path_manager_key: str = PATH_MANAGER_KEY,\n        # pyre-ignore[2]\n        **open_kw,\n    ) -> None:\n        super().__init__()\n        self.datapipe: Iterable[str] = datapipe\n        self.mode: str = mode\n        if self.mode not in (\"b\", \"t\", \"rb\", \"rt\", \"r\"):\n            raise ValueError(\"Invalid mode {}\".format(mode))\n        # TODO: enforce typing for each instance based on mode, otherwise\n        #       `argument_validation` with this DataPipe may be potentially broken\n        self.length: int = length\n        # pyre-ignore[4]\n        self.open_kw = open_kw\n        self.path_manager: PathManager = PathManagerFactory().get(path_manager_key)\n        self.path_manager.set_strict_kwargs_checking(False)",
  "def __iter__(self):\n        if self.mode in (\"b\", \"t\"):\n            self.mode = \"r\" + self.mode\n        for pathname in self.datapipe:\n            if not isinstance(pathname, str):\n                raise TypeError(\n                    \"Expected string type for pathname, but got {}\".format(\n                        type(pathname)\n                    )\n                )\n            yield (\n                pathname,\n                self.path_manager.open(pathname, self.mode, **self.open_kw),\n            )",
  "def __len__(self) -> int:\n        if self.length == -1:\n            raise NotImplementedError\n        return self.length",
  "def __init__(\n        self,\n        *datapipes: IterDataPipe,\n        dp_selector: Callable[\n            [Sequence[IterDataPipe]], Sequence[IterDataPipe]\n        ] = _default_dp_selector,\n    ) -> None:\n        super().__init__()\n        self.datapipes: Tuple[IterDataPipe, ...] = datapipes\n        self.dp_selector = dp_selector",
  "def __iter__(self) -> Iterator[Any]:\n        selected_dps = self.dp_selector(self.datapipes)\n        for dp in selected_dps:\n            for data in dp:\n                yield data",
  "def _default_row_mapper(example: List[str]) -> Dict[str, Union[float, int, str]]:\n    return {\n        DEFAULT_COLUMN_NAMES[idx]: COLUMN_TYPE_CASTERS[idx](val)\n        for idx, val in enumerate(example)\n    }",
  "def _join_with_movies(datapipe: IterDataPipe, root: str) -> IterDataPipe:\n    movies_path = os.path.join(root, MOVIES_FILENAME)\n    movies_datapipe = LoadFiles((movies_path,), mode=\"r\")\n    movies_datapipe = ReadLinesFromCSV(\n        movies_datapipe,\n        skip_first_line=True,\n        delimiter=\",\",\n    )\n    movie_id_to_movie: Dict[str, List[str]] = {\n        row[0]: row[1:] for row in movies_datapipe\n    }\n\n    def join_rating_movie(val: List[str]) -> List[str]:\n        return val + movie_id_to_movie[val[1]]\n\n    return datapipe.map(join_rating_movie)",
  "def _movielens(\n    root: str,\n    *,\n    include_movies_data: bool = False,\n    # pyre-ignore[2]\n    row_mapper: Optional[Callable[[List[str]], Any]] = _default_row_mapper,\n    # pyre-ignore[2]\n    **open_kw,\n) -> IterDataPipe:\n    ratings_path = os.path.join(root, RATINGS_FILENAME)\n    datapipe = LoadFiles((ratings_path,), mode=\"r\", **open_kw)\n    datapipe = ReadLinesFromCSV(datapipe, skip_first_line=True, delimiter=\",\")\n\n    if include_movies_data:\n        datapipe = _join_with_movies(datapipe, root)\n    if row_mapper:\n        datapipe = datapipe.map(row_mapper)\n\n    return datapipe",
  "def movielens_20m(\n    root: str,\n    *,\n    include_movies_data: bool = False,\n    # pyre-ignore[2]\n    row_mapper: Optional[Callable[[List[str]], Any]] = _default_row_mapper,\n    # pyre-ignore[2]\n    **open_kw,\n) -> IterDataPipe:\n    \"\"\"`MovieLens 20M <https://grouplens.org/datasets/movielens/20m/>`_ Dataset\n    Args:\n        root (str): local path to root directory containing MovieLens 20M dataset files.\n        include_movies_data (bool): if True, adds movies data to each line.\n        row_mapper (Optional[Callable[[List[str]], Any]]): function to apply to each split line.\n        open_kw: options to pass to underlying invocation of iopath.common.file_io.PathManager.open.\n\n    Example::\n\n        datapipe = movielens_20m(\"/home/datasets/ml-20\")\n        datapipe = dp.iter.Batch(datapipe, 100)\n        datapipe = dp.iter.Collate(datapipe)\n        batch = next(iter(datapipe))\n    \"\"\"\n    return _movielens(\n        root,\n        include_movies_data=include_movies_data,\n        row_mapper=row_mapper,\n        **open_kw,\n    )",
  "def movielens_25m(\n    root: str,\n    *,\n    include_movies_data: bool = False,\n    # pyre-ignore[2]\n    row_mapper: Optional[Callable[[List[str]], Any]] = _default_row_mapper,\n    # pyre-ignore[2]\n    **open_kw,\n) -> IterDataPipe:\n    \"\"\"`MovieLens 25M <https://grouplens.org/datasets/movielens/25m/>`_ Dataset\n    Args:\n        root (str): local path to root directory containing MovieLens 25M dataset files.\n        include_movies_data (bool): if True, adds movies data to each line.\n        row_mapper (Optional[Callable[[List[str]], Any]]): function to apply to each split line.\n        open_kw: options to pass to underlying invocation of iopath.common.file_io.PathManager.open.\n\n    Example::\n\n        datapipe = movielens_25m(\"/home/datasets/ml-25\")\n        datapipe = dp.iter.Batch(datapipe, 100)\n        datapipe = dp.iter.Collate(datapipe)\n        batch = next(iter(datapipe))\n    \"\"\"\n    return _movielens(\n        root,\n        include_movies_data=include_movies_data,\n        row_mapper=row_mapper,\n        **open_kw,\n    )",
  "def join_rating_movie(val: List[str]) -> List[str]:\n        return val + movie_id_to_movie[val[1]]",
  "def _default_row_mapper(example: List[str]) -> Dict[str, Union[int, str]]:\n    column_names = reversed(DEFAULT_COLUMN_NAMES)\n    column_type_casters = reversed(COLUMN_TYPE_CASTERS)\n    return {\n        next(column_names): next(column_type_casters)(val) for val in reversed(example)\n    }",
  "class CriteoIterDataPipe(IterDataPipe):\n    \"\"\"\n    IterDataPipe that can be used to stream either the Criteo 1TB Click Logs Dataset\n    (https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/) or the\n    Kaggle/Criteo Display Advertising Dataset\n    (https://www.kaggle.com/c/criteo-display-ad-challenge/) from the source TSV\n    files.\n\n    Args:\n        paths (Iterable[str]): local paths to TSV files that constitute the Criteo\n            dataset.\n        row_mapper (Optional[Callable[[List[str]], Any]]): function to apply to each\n            split TSV line.\n        open_kw: options to pass to underlying invocation of\n            iopath.common.file_io.PathManager.open.\n\n    Example::\n\n        datapipe = CriteoIterDataPipe(\n            (\"/home/datasets/criteo/day_0.tsv\", \"/home/datasets/criteo/day_1.tsv\")\n        )\n        datapipe = dp.iter.Batcher(datapipe, 100)\n        datapipe = dp.iter.Collator(datapipe)\n        batch = next(iter(datapipe))\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: Iterable[str],\n        *,\n        # pyre-ignore[2]\n        row_mapper: Optional[Callable[[List[str]], Any]] = _default_row_mapper,\n        # pyre-ignore[2]\n        **open_kw,\n    ) -> None:\n        self.paths = paths\n        self.row_mapper = row_mapper\n        self.open_kw: Any = open_kw  # pyre-ignore[4]\n\n    # pyre-ignore[3]\n    def __iter__(self) -> Iterator[Any]:\n        worker_info = torch.utils.data.get_worker_info()\n        paths = self.paths\n        if worker_info is not None:\n            paths = (\n                path\n                for (idx, path) in enumerate(paths)\n                if idx % worker_info.num_workers == worker_info.id\n            )\n        datapipe = LoadFiles(paths, mode=\"r\", **self.open_kw)\n        datapipe = ReadLinesFromCSV(datapipe, delimiter=\"\\t\")\n        if self.row_mapper:\n            datapipe = dp.iter.Mapper(datapipe, self.row_mapper)\n        yield from datapipe",
  "def criteo_terabyte(\n    paths: Iterable[str],\n    *,\n    # pyre-ignore[2]\n    row_mapper: Optional[Callable[[List[str]], Any]] = _default_row_mapper,\n    # pyre-ignore[2]\n    **open_kw,\n) -> IterDataPipe:\n    \"\"\"`Criteo 1TB Click Logs <https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/>`_ Dataset\n\n    Args:\n        paths (Iterable[str]): local paths to TSV files that constitute the Criteo 1TB\n            dataset.\n        row_mapper (Optional[Callable[[List[str]], Any]]): function to apply to each\n            split TSV line.\n        open_kw: options to pass to underlying invocation of\n            iopath.common.file_io.PathManager.open.\n\n    Example::\n\n        datapipe = criteo_terabyte(\n            (\"/home/datasets/criteo/day_0.tsv\", \"/home/datasets/criteo/day_1.tsv\")\n        )\n        datapipe = dp.iter.Batcher(datapipe, 100)\n        datapipe = dp.iter.Collator(datapipe)\n        batch = next(iter(datapipe))\n    \"\"\"\n    return CriteoIterDataPipe(paths, row_mapper=row_mapper, **open_kw)",
  "def criteo_kaggle(\n    path: str,\n    *,\n    # pyre-ignore[2]\n    row_mapper: Optional[Callable[[List[str]], Any]] = _default_row_mapper,\n    # pyre-ignore[2]\n    **open_kw,\n) -> IterDataPipe:\n    \"\"\"`Kaggle/Criteo Display Advertising <https://www.kaggle.com/c/criteo-display-ad-challenge/>`_ Dataset\n\n    Args:\n        path (str): local path to train or test dataset file.\n        row_mapper (Optional[Callable[[List[str]], Any]]): function to apply to each split TSV line.\n        open_kw: options to pass to underlying invocation of iopath.common.file_io.PathManager.open.\n\n    Example::\n\n        train_datapipe = criteo_kaggle(\n            \"/home/datasets/criteo_kaggle/train.txt\",\n        )\n        example = next(iter(train_datapipe))\n        test_datapipe = criteo_kaggle(\n            \"/home/datasets/criteo_kaggle/test.txt\",\n        )\n        example = next(iter(test_datapipe))\n    \"\"\"\n    return CriteoIterDataPipe((path,), row_mapper=row_mapper, **open_kw)",
  "class BinaryCriteoUtils:\n    \"\"\"\n    Utility functions used to preprocess, save, load, partition, etc. the Criteo\n    dataset in a binary (numpy) format.\n    \"\"\"\n\n    @staticmethod\n    def tsv_to_npys(\n        in_file: str,\n        out_dense_file: str,\n        out_sparse_file: str,\n        out_labels_file: str,\n        dataset_name: str = \"criteo_1tb\",\n        path_manager_key: str = PATH_MANAGER_KEY,\n    ) -> None:\n        \"\"\"\n        Convert one Criteo tsv file to three npy files: one for dense (np.float32), one\n        for sparse (np.int32), and one for labels (np.int32).\n\n        The tsv file is expected to be part of the Criteo 1TB Click Logs Dataset (\"criteo_1tb\")\n        or the Criteo Kaggle Display Advertising Challenge dataset (\"criteo_kaggle\").\n\n        For the \"criteo_kaggle\" test set, we set the labels to -1 representing filler data,\n        because label data is not included in the \"criteo_kaggle\" test set.\n\n        Args:\n            in_file (str): Input tsv file path.\n            out_dense_file (str): Output dense npy file path.\n            out_sparse_file (str): Output sparse npy file path.\n            out_labels_file (str): Output labels npy file path.\n            dataset_name (str): The dataset name. \"criteo_1tb\" or \"criteo_kaggle\" is expected.\n            path_manager_key (str): Path manager key used to load from different\n                filesystems.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Add fake label for criteo_kaggle test set, which does not include label data\n        def row_mapper_with_fake_label_constant(\n            row: List[str],\n        ) -> Tuple[List[int], List[int], int]:\n            label = -1\n            dense = [int(row[i] or \"0\") for i in range(0, 0 + INT_FEATURE_COUNT)]\n            sparse = [\n                int(row[i] or \"0\", 16)\n                for i in range(\n                    0 + INT_FEATURE_COUNT, 0 + INT_FEATURE_COUNT + CAT_FEATURE_COUNT\n                )\n            ]\n            return dense, sparse, label\n\n        def row_mapper(row: List[str]) -> Tuple[List[int], List[int], int]:\n            # Missing values are mapped to zero for both dense and sparse features\n            label = int(row[0] or \"0\")\n            dense = [int(row[i] or \"0\") for i in range(1, 1 + INT_FEATURE_COUNT)]\n            sparse = [\n                int(row[i] or \"0\", 16)\n                for i in range(\n                    1 + INT_FEATURE_COUNT, 1 + INT_FEATURE_COUNT + CAT_FEATURE_COUNT\n                )\n            ]\n            return dense, sparse, label\n\n        dense, sparse, labels = [], [], []\n        for (row_dense, row_sparse, row_label) in CriteoIterDataPipe(\n            [in_file],\n            row_mapper=row_mapper\n            if not (dataset_name == \"criteo_kaggle\" and \"test\" in in_file)\n            else row_mapper_with_fake_label_constant,\n        ):\n            dense.append(row_dense)\n            sparse.append(row_sparse)\n            labels.append(row_label)\n\n        # PyTorch tensors can't handle uint32, but we can save space by not\n        # using int64. Numpy will automatically handle dense values >= 2 ** 31.\n        dense_np = np.array(dense, dtype=np.int32)\n        del dense\n        sparse_np = np.array(sparse, dtype=np.int32)\n        del sparse\n        labels_np = np.array(labels, dtype=np.int32)\n        del labels\n\n        # Log is expensive to compute at runtime.\n        dense_np += 3\n        dense_np = np.log(dense_np, dtype=np.float32)\n\n        # To be consistent with dense and sparse.\n        labels_np = labels_np.reshape((-1, 1))\n\n        path_manager = PathManagerFactory().get(path_manager_key)\n        for (fname, arr) in [\n            (out_dense_file, dense_np),\n            (out_sparse_file, sparse_np),\n            (out_labels_file, labels_np),\n        ]:\n            with path_manager.open(fname, \"wb\") as fout:\n                np.save(fout, arr)\n\n    @staticmethod\n    def get_shape_from_npy(\n        path: str, path_manager_key: str = PATH_MANAGER_KEY\n    ) -> Tuple[int, ...]:\n        \"\"\"\n        Returns the shape of an npy file using only its header.\n\n        Args:\n            path (str): Input npy file path.\n            path_manager_key (str): Path manager key used to load from different\n                filesystems.\n\n        Returns:\n            shape (Tuple[int, ...]): Shape tuple.\n        \"\"\"\n        path_manager = PathManagerFactory().get(path_manager_key)\n        with path_manager.open(path, \"rb\") as fin:\n            np.lib.format.read_magic(fin)\n            shape, _order, _dtype = np.lib.format.read_array_header_1_0(fin)\n            return shape\n\n    @staticmethod\n    def get_file_row_ranges_and_remainder(\n        lengths: List[int],\n        rank: int,\n        world_size: int,\n        start_row: int = 0,\n        last_row: Optional[int] = None,\n    ) -> Tuple[Dict[int, Tuple[int, int]], int]:\n        \"\"\"\n        Given a rank, world_size, and the lengths (number of rows) for a list of files,\n        return which files and which portions of those files (represented as row ranges\n        - all range indices are inclusive) should be handled by the rank. Each rank\n        will be assigned the same number of rows.\n\n        The ranges are determined in such a way that each rank deals with large\n        continuous ranges of files. This enables each rank to reduce the amount of data\n        it needs to read while avoiding seeks.\n\n        Args:\n            lengths (List[int]): A list of row counts for each file.\n            rank (int): rank.\n            world_size (int): world size.\n\n        Returns:\n            output (Tuple[Dict[int, Tuple[int, int]], int]): First item is a mapping of files\n            to the range in those files to be handled by the rank. The keys of this dict are indices.\n            The second item is the remainder of dataset length / world size.\n        \"\"\"\n\n        # All ..._g variables are globals indices (meaning they range from 0 to\n        # total_length - 1). All ..._l variables are local indices (meaning they range\n        # from 0 to lengths[i] - 1 for the ith file).\n        if last_row is None:\n            total_length = sum(lengths) - start_row\n        else:\n            total_length = last_row - start_row + 1\n\n        # Global indices that rank is responsible for. All ranges (left, right) are\n        # inclusive.\n        rows_per_rank = total_length // world_size\n        remainder = total_length % world_size\n        rows_per_rank = np.array([rows_per_rank for _ in range(world_size)])\n        rows_per_rank[:remainder] += 1\n        rank_rows_bins_csr = np.cumsum([0] + list(rows_per_rank))\n        rank_left_g = rank_rows_bins_csr[rank] + start_row\n        rank_right_g = rank_rows_bins_csr[rank + 1] - 1 + start_row\n\n        output = {}\n\n        # Find where range (rank_left_g, rank_right_g) intersects each file's range.\n        file_left_g, file_right_g = -1, -1\n        for idx, length in enumerate(lengths):\n            file_left_g = file_right_g + 1\n            file_right_g = file_left_g + length - 1\n\n            # If the ranges overlap.\n            if rank_left_g <= file_right_g and rank_right_g >= file_left_g:\n                overlap_left_g, overlap_right_g = max(rank_left_g, file_left_g), min(\n                    rank_right_g, file_right_g\n                )\n\n                # Convert overlap in global numbers to (local) numbers specific to the\n                # file.\n                overlap_left_l = overlap_left_g - file_left_g\n                overlap_right_l = overlap_right_g - file_left_g\n                output[idx] = (overlap_left_l, overlap_right_l)\n\n        return output, remainder\n\n    @staticmethod\n    def load_npy_range(\n        fname: str,\n        start_row: int,\n        num_rows: int,\n        path_manager_key: str = PATH_MANAGER_KEY,\n        mmap_mode: bool = False,\n    ) -> np.ndarray:\n        \"\"\"\n        Load part of an npy file.\n\n        NOTE: Assumes npy represents a numpy array of ndim 2.\n\n        Args:\n            fname (str): path string to npy file.\n            start_row (int): starting row from the npy file.\n            num_rows (int): number of rows to get from the npy file.\n            path_manager_key (str): Path manager key used to load from different\n                filesystems.\n\n        Returns:\n            output (np.ndarray): numpy array with the desired range of data from the\n                supplied npy file.\n        \"\"\"\n        path_manager = PathManagerFactory().get(path_manager_key)\n        with path_manager.open(fname, \"rb\") as fin:\n            np.lib.format.read_magic(fin)\n            shape, _order, dtype = np.lib.format.read_array_header_1_0(fin)\n            if len(shape) == 2:\n                total_rows, row_size = shape\n            else:\n                raise ValueError(\"Cannot load range for npy with ndim != 2.\")\n\n            if not (0 <= start_row < total_rows):\n                raise ValueError(\n                    f\"start_row ({start_row}) is out of bounds. It must be between 0 \"\n                    f\"and {total_rows - 1}, inclusive.\"\n                )\n            if not (start_row + num_rows <= total_rows):\n                raise ValueError(\n                    f\"num_rows ({num_rows}) exceeds number of available rows \"\n                    f\"({total_rows}) for the given start_row ({start_row}).\"\n                )\n            if mmap_mode:\n                data = np.load(fname, mmap_mode=\"r\")\n                data = data[start_row : start_row + num_rows]\n            else:\n                offset = start_row * row_size * dtype.itemsize\n                fin.seek(offset, os.SEEK_CUR)\n                num_entries = num_rows * row_size\n                data = np.fromfile(fin, dtype=dtype, count=num_entries)\n            return data.reshape((num_rows, row_size))\n\n    @staticmethod\n    def sparse_to_contiguous(\n        in_files: List[str],\n        output_dir: str,\n        frequency_threshold: int = FREQUENCY_THRESHOLD,\n        columns: int = CAT_FEATURE_COUNT,\n        path_manager_key: str = PATH_MANAGER_KEY,\n        output_file_suffix: str = \"_contig_freq.npy\",\n    ) -> None:\n        \"\"\"\n        Convert all sparse .npy files to have contiguous integers. Store in a separate\n        .npy file. All input files must be processed together because columns\n        can have matching IDs between files. Hence, they must be transformed\n        together. Also, the transformed IDs are not unique between columns. IDs\n        that appear less than frequency_threshold amount of times will be remapped\n        to have a value of 1.\n\n        Example transformation, frequency_threshold of 2:\n        day_0_sparse.npy\n        | col_0 | col_1 |\n        -----------------\n        | abc   | xyz   |\n        | iop   | xyz   |\n\n        day_1_sparse.npy\n        | col_0 | col_1 |\n        -----------------\n        | iop   | tuv   |\n        | lkj   | xyz   |\n\n        day_0_sparse_contig.npy\n        | col_0 | col_1 |\n        -----------------\n        | 1     | 2     |\n        | 2     | 2     |\n\n        day_1_sparse_contig.npy\n        | col_0 | col_1 |\n        -----------------\n        | 2     | 1     |\n        | 1     | 2     |\n\n        Args:\n            in_files List[str]: Input directory of npy files.\n            output_dir (str): Output directory of processed npy files.\n            frequency_threshold: IDs occurring less than this frequency will be remapped to a value of 1.\n            path_manager_key (str): Path manager key used to load from different filesystems.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Load each .npy file of sparse features. Transformations are made along the columns.\n        # Thereby, transpose the input to ease operations.\n        # E.g. file_to_features = {\"day_0_sparse\": [array([[3,6,7],[7,9,3]]}\n        file_to_features: Dict[str, np.ndarray] = {}\n        for f in in_files:\n            name = os.path.basename(f).split(\".\")[0]\n            file_to_features[name] = np.load(f).transpose()\n            print(f\"Successfully loaded file: {f}\")\n\n        # Iterate through each column in each file and map the sparse ids to contiguous ids.\n        for col in range(columns):\n            print(f\"Processing column: {col}\")\n\n            # Iterate through each row in each file for the current column and determine the\n            # frequency of each sparse id.\n            sparse_to_frequency: Dict[int, int] = {}\n            if frequency_threshold > 1:\n                for f in file_to_features:\n                    for _, sparse in enumerate(file_to_features[f][col]):\n                        if sparse in sparse_to_frequency:\n                            sparse_to_frequency[sparse] += 1\n                        else:\n                            sparse_to_frequency[sparse] = 1\n\n            # Iterate through each row in each file for the current column and remap each\n            # sparse id to a contiguous id. The contiguous ints start at a value of 2 so that\n            # infrequenct IDs (determined by the frequency_threshold) can be remapped to 1.\n            running_sum = 2\n            sparse_to_contiguous_int: Dict[int, int] = {}\n\n            for f in file_to_features:\n                print(f\"Processing file: {f}\")\n\n                for i, sparse in enumerate(file_to_features[f][col]):\n                    if sparse not in sparse_to_contiguous_int:\n                        # If the ID appears less than frequency_threshold amount of times\n                        # remap the value to 1.\n                        if (\n                            frequency_threshold > 1\n                            and sparse_to_frequency[sparse] < frequency_threshold\n                        ):\n                            sparse_to_contiguous_int[sparse] = 1\n                        else:\n                            sparse_to_contiguous_int[sparse] = running_sum\n                            running_sum += 1\n\n                    # Re-map sparse value to contiguous in place.\n                    file_to_features[f][col][i] = sparse_to_contiguous_int[sparse]\n\n        path_manager = PathManagerFactory().get(path_manager_key)\n        for f, features in file_to_features.items():\n            output_file = os.path.join(output_dir, f + output_file_suffix)\n            with path_manager.open(output_file, \"wb\") as fout:\n                print(f\"Writing file: {output_file}\")\n                # Transpose back the features when saving, as they were transposed when loading.\n                np.save(fout, features.transpose())\n\n    @staticmethod\n    def shuffle(\n        input_dir_labels_and_dense: str,\n        input_dir_sparse: str,\n        output_dir_shuffled: str,\n        rows_per_day: Dict[int, int],\n        output_dir_full_set: Optional[str] = None,\n        days: int = DAYS,\n        int_columns: int = INT_FEATURE_COUNT,\n        sparse_columns: int = CAT_FEATURE_COUNT,\n        path_manager_key: str = PATH_MANAGER_KEY,\n        random_seed: int = 0,\n    ) -> None:\n        \"\"\"\n        Shuffle the dataset. Expects the files to be in .npy format and the data\n        to be split by day and by dense, sparse and label data.\n        Dense data must be in: day_x_dense.npy\n        Sparse data must be in: day_x_sparse.npy\n        Labels data must be in: day_x_labels.npy\n\n        The dataset will be reconstructed, shuffled and then split back into\n        separate dense, sparse and labels files.\n\n        This will only shuffle the first DAYS-1 days as the training set. The final day will remain\n        untouched as the validation, and training set.\n\n        Args:\n            input_dir_labels_and_dense (str): Input directory of labels and dense npy files.\n            input_dir_sparse (str): Input directory of sparse npy files.\n            output_dir_shuffled (str): Output directory for shuffled labels, dense and sparse npy files.\n            rows_per_day Dict[int, int]: Number of rows in each file.\n            output_dir_full_set (str): Output directory of the full dataset, if desired.\n            days (int): Number of day files.\n            int_columns (int): Number of columns with dense features.\n            sparse_columns (int): Total number of categorical columns.\n            path_manager_key (str): Path manager key used to load from different filesystems.\n            random_seed (int): Random seed used for the random.shuffle operator.\n        \"\"\"\n\n        total_rows = sum(rows_per_day.values())\n        columns = int_columns + sparse_columns + 1  # add 1 for label column\n        full_dataset = np.zeros((total_rows, columns), dtype=np.float32)\n        curr_first_row = 0\n        curr_last_row = 0\n        for d in range(0, days - 1):\n            curr_last_row += rows_per_day[d]\n\n            # dense\n            path_to_file = os.path.join(\n                input_dir_labels_and_dense, f\"day_{d}_dense.npy\"\n            )\n            data = np.load(path_to_file)\n            print(\n                f\"Day {d} dense- {curr_first_row}-{curr_last_row} loaded files - {time.time()} - {path_to_file}\"\n            )\n\n            full_dataset[curr_first_row:curr_last_row, 0:int_columns] = data\n            del data\n\n            # sparse\n            path_to_file = os.path.join(input_dir_sparse, f\"day_{d}_sparse.npy\")\n            data = np.load(path_to_file)\n            print(\n                f\"Day {d} sparse- {curr_first_row}-{curr_last_row} loaded files - {time.time()} - {path_to_file}\"\n            )\n\n            full_dataset[curr_first_row:curr_last_row, int_columns : columns - 1] = data\n            del data\n\n            # labels\n            path_to_file = os.path.join(\n                input_dir_labels_and_dense, f\"day_{d}_labels.npy\"\n            )\n            data = np.load(path_to_file)\n            print(\n                f\"Day {d} labels- {curr_first_row}-{curr_last_row} loaded files - {time.time()} - {path_to_file}\"\n            )\n\n            full_dataset[curr_first_row:curr_last_row, columns - 1 :] = data\n            del data\n\n            curr_first_row = curr_last_row\n\n        path_manager = PathManagerFactory().get(path_manager_key)\n\n        # Save the full dataset\n        if output_dir_full_set is not None:\n            full_output_file = os.path.join(output_dir_full_set, \"full.npy\")\n            with path_manager.open(full_output_file, \"wb\") as fout:\n                print(f\"Writing full set file: {full_output_file}\")\n                np.save(fout, full_dataset)\n\n        print(f\"Shuffling dataset with random_seed={random_seed}\")\n        np.random.seed(random_seed)\n        np.random.shuffle(full_dataset)\n\n        # Slice and save each portion into dense, sparse and labels\n        curr_first_row = 0\n        curr_last_row = 0\n        for d in range(0, days - 1):\n            curr_last_row += rows_per_day[d]\n\n            # write dense columns\n            shuffled_dense_file = os.path.join(\n                output_dir_shuffled, f\"day_{d}_dense.npy\"\n            )\n            with path_manager.open(shuffled_dense_file, \"wb\") as fout:\n                print(\n                    f\"Writing rows {curr_first_row}-{curr_last_row-1} dense file: {shuffled_dense_file}\"\n                )\n                np.save(fout, full_dataset[curr_first_row:curr_last_row, 0:int_columns])\n\n            # write sparse columns\n            shuffled_sparse_file = os.path.join(\n                output_dir_shuffled, f\"day_{d}_sparse.npy\"\n            )\n            with path_manager.open(shuffled_sparse_file, \"wb\") as fout:\n                print(\n                    f\"Writing rows {curr_first_row}-{curr_last_row-1} sparse file: {shuffled_sparse_file}\"\n                )\n                np.save(\n                    fout,\n                    full_dataset[\n                        curr_first_row:curr_last_row, int_columns : columns - 1\n                    ].astype(np.int32),\n                )\n\n            # write labels columns\n            shuffled_labels_file = os.path.join(\n                output_dir_shuffled, f\"day_{d}_labels.npy\"\n            )\n            with path_manager.open(shuffled_labels_file, \"wb\") as fout:\n                print(\n                    f\"Writing rows {curr_first_row}-{curr_last_row-1} labels file: {shuffled_labels_file}\"\n                )\n                np.save(\n                    fout,\n                    full_dataset[curr_first_row:curr_last_row, columns - 1 :].astype(\n                        np.int32\n                    ),\n                )\n            curr_first_row = curr_last_row\n\n        # Directly copy over the last day's files since they will be used for validation and testing.\n        for (part, input_dir) in [\n            (\"sparse\", input_dir_sparse),\n            (\"dense\", input_dir_labels_and_dense),\n            (\"labels\", input_dir_labels_and_dense),\n        ]:\n            path_to_original = os.path.join(input_dir, f\"day_{days-1}_{part}.npy\")\n            val_train_path = os.path.join(\n                output_dir_shuffled, f\"day_{days-1}_{part}.npy\"\n            )\n            shutil.copyfile(path_to_original, val_train_path)\n            print(f\"Copying over {path_to_original} to {val_train_path}\")",
  "class InMemoryBinaryCriteoIterDataPipe(IterableDataset):\n    \"\"\"\n    Datapipe designed to operate over binary (npy) versions of Criteo datasets. Loads\n    the entire dataset into memory to prevent disk speed from affecting throughout. Each\n    rank reads only the data for the portion of the dataset it is responsible for.\n\n    The torchrec/datasets/scripts/npy_preproc_criteo.py script can be used to convert\n    the Criteo tsv files to the npy files expected by this dataset.\n\n    Args:\n        stage (str): \"train\", \"val\", or \"test\".\n        dense_paths (List[str]): List of path strings to dense npy files.\n        sparse_paths (List[str]): List of path strings to sparse npy files.\n        labels_paths (List[str]): List of path strings to labels npy files.\n        batch_size (int): batch size.\n        rank (int): rank.\n        world_size (int): world size.\n        shuffle_batches (bool): Whether to shuffle batches\n        hashes (Optional[int]): List of max categorical feature value for each feature.\n            Length of this list should be CAT_FEATURE_COUNT.\n        path_manager_key (str): Path manager key used to load from different\n            filesystems.\n\n    Example::\n\n        template = \"/home/datasets/criteo/1tb_binary/day_{}_{}.npy\"\n        datapipe = InMemoryBinaryCriteoIterDataPipe(\n            dense_paths=[template.format(0, \"dense\"), template.format(1, \"dense\")],\n            sparse_paths=[template.format(0, \"sparse\"), template.format(1, \"sparse\")],\n            labels_paths=[template.format(0, \"labels\"), template.format(1, \"labels\")],\n            batch_size=1024,\n            rank=torch.distributed.get_rank(),\n            world_size=torch.distributed.get_world_size(),\n        )\n        batch = next(iter(datapipe))\n    \"\"\"\n\n    def __init__(\n        self,\n        stage: str,\n        dense_paths: List[str],\n        sparse_paths: List[str],\n        labels_paths: List[str],\n        batch_size: int,\n        rank: int,\n        world_size: int,\n        drop_last: Optional[bool] = False,\n        shuffle_batches: bool = False,\n        shuffle_training_set: bool = False,\n        shuffle_training_set_random_seed: int = 0,\n        mmap_mode: bool = False,\n        hashes: Optional[List[int]] = None,\n        path_manager_key: str = PATH_MANAGER_KEY,\n    ) -> None:\n        self.stage = stage\n        self.dense_paths = dense_paths\n        self.sparse_paths = sparse_paths\n        self.labels_paths = labels_paths\n        self.batch_size = batch_size\n        self.rank = rank\n        self.world_size = world_size\n        self.drop_last = drop_last\n        self.shuffle_batches = shuffle_batches\n        self.shuffle_training_set = shuffle_training_set\n        np.random.seed(shuffle_training_set_random_seed)\n        self.mmap_mode = mmap_mode\n        self.hashes: np.ndarray = np.array(hashes).reshape((1, CAT_FEATURE_COUNT))\n        self.path_manager_key = path_manager_key\n        self.path_manager: PathManager = PathManagerFactory().get(path_manager_key)\n\n        if shuffle_training_set and stage == \"train\":\n            self._shuffle_and_load_data_for_rank()\n            self.world_size = 1\n            self.rank = 0\n        else:\n            m = \"r\" if mmap_mode else None\n            self.dense_arrs: List[np.ndarray] = [\n                np.load(f, mmap_mode=m) for f in self.dense_paths\n            ]\n            self.sparse_arrs: List[np.ndarray] = [\n                np.load(f, mmap_mode=m) for f in self.sparse_paths\n            ]\n            self.labels_arrs: List[np.ndarray] = [\n                np.load(f, mmap_mode=m) for f in self.labels_paths\n            ]\n        len_d0 = len(self.dense_arrs[0])\n        second_half_start_index = int(len_d0 // 2 + len_d0 % 2)\n        if stage == \"val\":\n            self.dense_arrs[0] = self.dense_arrs[0][:second_half_start_index, :]\n            self.sparse_arrs[0] = self.sparse_arrs[0][:second_half_start_index, :]\n            self.labels_arrs[0] = self.labels_arrs[0][:second_half_start_index, :]\n        elif stage == \"test\":\n            self.dense_arrs[0] = self.dense_arrs[0][second_half_start_index:, :]\n            self.sparse_arrs[0] = self.sparse_arrs[0][second_half_start_index:, :]\n            self.labels_arrs[0] = self.labels_arrs[0][second_half_start_index:, :]\n        # When mmap_mode is enabled, sparse features are hashed when\n        # samples are batched in def __iter__. Otherwise, the dataset has been\n        # preloaded with sparse features hashed in the preload stage, here:\n        if not self.mmap_mode and self.hashes is not None:\n            for sparse_arr in self.sparse_arrs:\n                sparse_arr %= self.hashes\n\n        self.num_rows_per_file: List[int] = list(map(len, self.dense_arrs))\n        total_rows = sum(self.num_rows_per_file)\n        self.num_full_batches: int = (\n            total_rows // batch_size // self.world_size * self.world_size\n        )\n        self.last_batch_sizes: np.ndarray = np.array(\n            [0 for _ in range(self.world_size)]\n        )\n        remainder = total_rows % (self.world_size * batch_size)\n        if not self.drop_last and 0 < remainder:\n            if remainder < self.world_size:\n                self.num_full_batches -= self.world_size\n                self.last_batch_sizes += batch_size\n            else:\n                self.last_batch_sizes += remainder // self.world_size\n            self.last_batch_sizes[: remainder % self.world_size] += 1\n\n        # These values are the same for the KeyedJaggedTensors in all batches, so they\n        # are computed once here. This avoids extra work from the KeyedJaggedTensor sync\n        # functions.\n        self._num_ids_in_batch: int = CAT_FEATURE_COUNT * (batch_size + 1)\n        self.keys: List[str] = DEFAULT_CAT_NAMES\n        self.lengths: torch.Tensor = torch.ones(\n            (self._num_ids_in_batch,), dtype=torch.int32\n        )\n        self.offsets: torch.Tensor = torch.arange(\n            0, self._num_ids_in_batch + 1, dtype=torch.int32\n        )\n        self._num_ids_in_batch -= CAT_FEATURE_COUNT\n        self.length_per_key: List[int] = CAT_FEATURE_COUNT * [batch_size]\n        self.offset_per_key: List[int] = [\n            batch_size * i for i in range(CAT_FEATURE_COUNT + 1)\n        ]\n        self.index_per_key: Dict[str, int] = {\n            key: i for (i, key) in enumerate(self.keys)\n        }\n\n    def _load_data_for_rank(self) -> None:\n        start_row, last_row = 0, None\n        if self.stage in [\"val\", \"test\"]:\n            # Last day's dataset is split into 2 sets: 1st half for \"val\"; 2nd for \"test\"\n            samples_in_file = BinaryCriteoUtils.get_shape_from_npy(\n                self.dense_paths[0], path_manager_key=self.path_manager_key\n            )[0]\n            start_row = 0\n            dataset_len = int(np.ceil(samples_in_file / 2.0))\n            if self.stage == \"test\":\n                start_row = dataset_len\n                dataset_len = samples_in_file - start_row\n            last_row = start_row + dataset_len - 1\n\n        row_ranges, remainder = BinaryCriteoUtils.get_file_row_ranges_and_remainder(\n            lengths=[\n                BinaryCriteoUtils.get_shape_from_npy(\n                    path, path_manager_key=self.path_manager_key\n                )[0]\n                for path in self.dense_paths\n            ],\n            rank=self.rank,\n            world_size=self.world_size,\n            start_row=start_row,\n            last_row=last_row,\n        )\n        self.remainder = remainder\n        self.dense_arrs, self.sparse_arrs, self.labels_arrs = [], [], []\n        for arrs, paths in zip(\n            [self.dense_arrs, self.sparse_arrs, self.labels_arrs],\n            [self.dense_paths, self.sparse_paths, self.labels_paths],\n        ):\n            for idx, (range_left, range_right) in row_ranges.items():\n                arrs.append(\n                    BinaryCriteoUtils.load_npy_range(\n                        paths[idx],\n                        range_left,\n                        range_right - range_left + 1,\n                        path_manager_key=self.path_manager_key,\n                        mmap_mode=self.mmap_mode,\n                    )\n                )\n\n    def _shuffle_and_load_data_for_rank(self) -> None:\n        world_size = self.world_size\n        rank = self.rank\n        dense_arrs = [np.load(f, mmap_mode=\"r\") for f in self.dense_paths]\n        sparse_arrs = [np.load(f, mmap_mode=\"r\") for f in self.sparse_paths]\n        labels_arrs = [np.load(f, mmap_mode=\"r\") for f in self.labels_paths]\n        num_rows_per_file = list(map(len, dense_arrs))\n        total_rows = sum(num_rows_per_file)\n        permutation_arr = np.random.permutation(total_rows)\n        self.remainder = total_rows % world_size\n        rows_per_rank = total_rows // world_size\n        rows_per_rank = np.array([rows_per_rank for _ in range(world_size)])\n        rows_per_rank[: self.remainder] += 1\n        rank_rows_bins = np.cumsum(rows_per_rank)\n        rank_rows_bins_csr = np.cumsum([0] + list(rows_per_rank))\n\n        rows = rows_per_rank[rank]\n        d_sample, s_sample, l_sample = (\n            dense_arrs[0][0],\n            sparse_arrs[0][0],\n            labels_arrs[0][0],\n        )\n        shuffled_dense_arr = np.empty((rows, len(d_sample)), d_sample.dtype)\n        shuffled_sparse_arr = np.empty((rows, len(s_sample)), s_sample.dtype)\n        shuffled_labels_arr = np.empty((rows, len(l_sample)), l_sample.dtype)\n\n        day_rows_bins_csr = np.cumsum(np.array([0] + num_rows_per_file))\n        for i in range(len(dense_arrs)):\n            start = day_rows_bins_csr[i]\n            end = day_rows_bins_csr[i + 1]\n            indices_to_take = np.where(\n                rank == np.digitize(permutation_arr[start:end], rank_rows_bins)\n            )[0]\n            output_indices = (\n                permutation_arr[start + indices_to_take] - rank_rows_bins_csr[rank]\n            )\n            shuffled_dense_arr[output_indices] = dense_arrs[i][indices_to_take]\n            shuffled_sparse_arr[output_indices] = sparse_arrs[i][indices_to_take]\n            shuffled_labels_arr[output_indices] = labels_arrs[i][indices_to_take]\n        self.dense_arrs = [shuffled_dense_arr]\n        self.sparse_arrs = [shuffled_sparse_arr]\n        self.labels_arrs = [shuffled_labels_arr]\n\n    def _np_arrays_to_batch(\n        self, dense: np.ndarray, sparse: np.ndarray, labels: np.ndarray\n    ) -> Batch:\n        if self.shuffle_batches:\n            # Shuffle all 3 in unison\n            shuffler = np.random.permutation(len(dense))\n            dense = dense[shuffler]\n            sparse = sparse[shuffler]\n            labels = labels[shuffler]\n\n        batch_size = len(dense)\n        num_ids_in_batch = CAT_FEATURE_COUNT * batch_size\n        if batch_size == self.batch_size:\n            length_per_key = self.length_per_key\n            offset_per_key = self.offset_per_key\n        else:\n            # handle last batch in dataset when it's an incomplete batch.\n            length_per_key = CAT_FEATURE_COUNT * [batch_size]\n            offset_per_key = [batch_size * i for i in range(CAT_FEATURE_COUNT + 1)]\n\n        return Batch(\n            dense_features=torch.from_numpy(dense),\n            sparse_features=KeyedJaggedTensor(\n                keys=self.keys,\n                # transpose + reshape(-1) incurs an additional copy.\n                values=torch.from_numpy(sparse.transpose(1, 0).reshape(-1)),\n                lengths=self.lengths[:num_ids_in_batch],\n                offsets=self.offsets[: num_ids_in_batch + 1],\n                stride=batch_size,\n                length_per_key=length_per_key,\n                offset_per_key=offset_per_key,\n                index_per_key=self.index_per_key,\n            ),\n            labels=torch.from_numpy(labels.reshape(-1)),\n        )\n\n    def __iter__(self) -> Iterator[Batch]:\n        # Invariant: buffer never contains more than batch_size rows.\n        buffer: Optional[List[np.ndarray]] = None\n\n        def append_to_buffer(\n            dense: np.ndarray, sparse: np.ndarray, labels: np.ndarray\n        ) -> None:\n            nonlocal buffer\n            if buffer is None:\n                buffer = [dense, sparse, labels]\n            else:\n                for idx, arr in enumerate([dense, sparse, labels]):\n                    buffer[idx] = np.concatenate((buffer[idx], arr))\n\n        # Maintain a buffer that can contain up to batch_size rows. Fill buffer as\n        # much as possible on each iteration. Only return a new batch when batch_size\n        # rows are filled.\n        file_idx = 0\n        row_idx = 0\n        batch_idx = 0\n        buffer_row_count = 0\n        cur_batch_size = (\n            self.batch_size if self.num_full_batches > 0 else self.last_batch_sizes[0]\n        )\n        while (\n            batch_idx\n            < self.num_full_batches + (self.last_batch_sizes[0] > 0) * self.world_size\n        ):\n            if buffer_row_count == cur_batch_size or file_idx == len(self.dense_arrs):\n                if batch_idx % self.world_size == self.rank:\n                    yield self._np_arrays_to_batch(*none_throws(buffer))\n                    buffer = None\n                buffer_row_count = 0\n                batch_idx += 1\n                if 0 <= batch_idx - self.num_full_batches < self.world_size and (\n                    self.last_batch_sizes[0] > 0\n                ):\n                    cur_batch_size = self.last_batch_sizes[\n                        batch_idx - self.num_full_batches\n                    ]\n            else:\n                rows_to_get = min(\n                    cur_batch_size - buffer_row_count,\n                    self.num_rows_per_file[file_idx] - row_idx,\n                )\n                buffer_row_count += rows_to_get\n                slice_ = slice(row_idx, row_idx + rows_to_get)\n\n                if batch_idx % self.world_size == self.rank:\n                    dense_inputs = self.dense_arrs[file_idx][slice_, :]\n                    sparse_inputs = self.sparse_arrs[file_idx][slice_, :]\n                    target_labels = self.labels_arrs[file_idx][slice_, :]\n\n                    if self.mmap_mode and self.hashes is not None:\n                        sparse_inputs = sparse_inputs % self.hashes\n\n                    append_to_buffer(\n                        dense_inputs,\n                        sparse_inputs,\n                        target_labels,\n                    )\n                row_idx += rows_to_get\n\n                if row_idx >= self.num_rows_per_file[file_idx]:\n                    file_idx += 1\n                    row_idx = 0\n\n    def __len__(self) -> int:\n        return self.num_full_batches // self.world_size + (self.last_batch_sizes[0] > 0)",
  "def __init__(\n        self,\n        paths: Iterable[str],\n        *,\n        # pyre-ignore[2]\n        row_mapper: Optional[Callable[[List[str]], Any]] = _default_row_mapper,\n        # pyre-ignore[2]\n        **open_kw,\n    ) -> None:\n        self.paths = paths\n        self.row_mapper = row_mapper\n        self.open_kw: Any = open_kw",
  "def __iter__(self) -> Iterator[Any]:\n        worker_info = torch.utils.data.get_worker_info()\n        paths = self.paths\n        if worker_info is not None:\n            paths = (\n                path\n                for (idx, path) in enumerate(paths)\n                if idx % worker_info.num_workers == worker_info.id\n            )\n        datapipe = LoadFiles(paths, mode=\"r\", **self.open_kw)\n        datapipe = ReadLinesFromCSV(datapipe, delimiter=\"\\t\")\n        if self.row_mapper:\n            datapipe = dp.iter.Mapper(datapipe, self.row_mapper)\n        yield from datapipe",
  "def tsv_to_npys(\n        in_file: str,\n        out_dense_file: str,\n        out_sparse_file: str,\n        out_labels_file: str,\n        dataset_name: str = \"criteo_1tb\",\n        path_manager_key: str = PATH_MANAGER_KEY,\n    ) -> None:\n        \"\"\"\n        Convert one Criteo tsv file to three npy files: one for dense (np.float32), one\n        for sparse (np.int32), and one for labels (np.int32).\n\n        The tsv file is expected to be part of the Criteo 1TB Click Logs Dataset (\"criteo_1tb\")\n        or the Criteo Kaggle Display Advertising Challenge dataset (\"criteo_kaggle\").\n\n        For the \"criteo_kaggle\" test set, we set the labels to -1 representing filler data,\n        because label data is not included in the \"criteo_kaggle\" test set.\n\n        Args:\n            in_file (str): Input tsv file path.\n            out_dense_file (str): Output dense npy file path.\n            out_sparse_file (str): Output sparse npy file path.\n            out_labels_file (str): Output labels npy file path.\n            dataset_name (str): The dataset name. \"criteo_1tb\" or \"criteo_kaggle\" is expected.\n            path_manager_key (str): Path manager key used to load from different\n                filesystems.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Add fake label for criteo_kaggle test set, which does not include label data\n        def row_mapper_with_fake_label_constant(\n            row: List[str],\n        ) -> Tuple[List[int], List[int], int]:\n            label = -1\n            dense = [int(row[i] or \"0\") for i in range(0, 0 + INT_FEATURE_COUNT)]\n            sparse = [\n                int(row[i] or \"0\", 16)\n                for i in range(\n                    0 + INT_FEATURE_COUNT, 0 + INT_FEATURE_COUNT + CAT_FEATURE_COUNT\n                )\n            ]\n            return dense, sparse, label\n\n        def row_mapper(row: List[str]) -> Tuple[List[int], List[int], int]:\n            # Missing values are mapped to zero for both dense and sparse features\n            label = int(row[0] or \"0\")\n            dense = [int(row[i] or \"0\") for i in range(1, 1 + INT_FEATURE_COUNT)]\n            sparse = [\n                int(row[i] or \"0\", 16)\n                for i in range(\n                    1 + INT_FEATURE_COUNT, 1 + INT_FEATURE_COUNT + CAT_FEATURE_COUNT\n                )\n            ]\n            return dense, sparse, label\n\n        dense, sparse, labels = [], [], []\n        for (row_dense, row_sparse, row_label) in CriteoIterDataPipe(\n            [in_file],\n            row_mapper=row_mapper\n            if not (dataset_name == \"criteo_kaggle\" and \"test\" in in_file)\n            else row_mapper_with_fake_label_constant,\n        ):\n            dense.append(row_dense)\n            sparse.append(row_sparse)\n            labels.append(row_label)\n\n        # PyTorch tensors can't handle uint32, but we can save space by not\n        # using int64. Numpy will automatically handle dense values >= 2 ** 31.\n        dense_np = np.array(dense, dtype=np.int32)\n        del dense\n        sparse_np = np.array(sparse, dtype=np.int32)\n        del sparse\n        labels_np = np.array(labels, dtype=np.int32)\n        del labels\n\n        # Log is expensive to compute at runtime.\n        dense_np += 3\n        dense_np = np.log(dense_np, dtype=np.float32)\n\n        # To be consistent with dense and sparse.\n        labels_np = labels_np.reshape((-1, 1))\n\n        path_manager = PathManagerFactory().get(path_manager_key)\n        for (fname, arr) in [\n            (out_dense_file, dense_np),\n            (out_sparse_file, sparse_np),\n            (out_labels_file, labels_np),\n        ]:\n            with path_manager.open(fname, \"wb\") as fout:\n                np.save(fout, arr)",
  "def get_shape_from_npy(\n        path: str, path_manager_key: str = PATH_MANAGER_KEY\n    ) -> Tuple[int, ...]:\n        \"\"\"\n        Returns the shape of an npy file using only its header.\n\n        Args:\n            path (str): Input npy file path.\n            path_manager_key (str): Path manager key used to load from different\n                filesystems.\n\n        Returns:\n            shape (Tuple[int, ...]): Shape tuple.\n        \"\"\"\n        path_manager = PathManagerFactory().get(path_manager_key)\n        with path_manager.open(path, \"rb\") as fin:\n            np.lib.format.read_magic(fin)\n            shape, _order, _dtype = np.lib.format.read_array_header_1_0(fin)\n            return shape",
  "def get_file_row_ranges_and_remainder(\n        lengths: List[int],\n        rank: int,\n        world_size: int,\n        start_row: int = 0,\n        last_row: Optional[int] = None,\n    ) -> Tuple[Dict[int, Tuple[int, int]], int]:\n        \"\"\"\n        Given a rank, world_size, and the lengths (number of rows) for a list of files,\n        return which files and which portions of those files (represented as row ranges\n        - all range indices are inclusive) should be handled by the rank. Each rank\n        will be assigned the same number of rows.\n\n        The ranges are determined in such a way that each rank deals with large\n        continuous ranges of files. This enables each rank to reduce the amount of data\n        it needs to read while avoiding seeks.\n\n        Args:\n            lengths (List[int]): A list of row counts for each file.\n            rank (int): rank.\n            world_size (int): world size.\n\n        Returns:\n            output (Tuple[Dict[int, Tuple[int, int]], int]): First item is a mapping of files\n            to the range in those files to be handled by the rank. The keys of this dict are indices.\n            The second item is the remainder of dataset length / world size.\n        \"\"\"\n\n        # All ..._g variables are globals indices (meaning they range from 0 to\n        # total_length - 1). All ..._l variables are local indices (meaning they range\n        # from 0 to lengths[i] - 1 for the ith file).\n        if last_row is None:\n            total_length = sum(lengths) - start_row\n        else:\n            total_length = last_row - start_row + 1\n\n        # Global indices that rank is responsible for. All ranges (left, right) are\n        # inclusive.\n        rows_per_rank = total_length // world_size\n        remainder = total_length % world_size\n        rows_per_rank = np.array([rows_per_rank for _ in range(world_size)])\n        rows_per_rank[:remainder] += 1\n        rank_rows_bins_csr = np.cumsum([0] + list(rows_per_rank))\n        rank_left_g = rank_rows_bins_csr[rank] + start_row\n        rank_right_g = rank_rows_bins_csr[rank + 1] - 1 + start_row\n\n        output = {}\n\n        # Find where range (rank_left_g, rank_right_g) intersects each file's range.\n        file_left_g, file_right_g = -1, -1\n        for idx, length in enumerate(lengths):\n            file_left_g = file_right_g + 1\n            file_right_g = file_left_g + length - 1\n\n            # If the ranges overlap.\n            if rank_left_g <= file_right_g and rank_right_g >= file_left_g:\n                overlap_left_g, overlap_right_g = max(rank_left_g, file_left_g), min(\n                    rank_right_g, file_right_g\n                )\n\n                # Convert overlap in global numbers to (local) numbers specific to the\n                # file.\n                overlap_left_l = overlap_left_g - file_left_g\n                overlap_right_l = overlap_right_g - file_left_g\n                output[idx] = (overlap_left_l, overlap_right_l)\n\n        return output, remainder",
  "def load_npy_range(\n        fname: str,\n        start_row: int,\n        num_rows: int,\n        path_manager_key: str = PATH_MANAGER_KEY,\n        mmap_mode: bool = False,\n    ) -> np.ndarray:\n        \"\"\"\n        Load part of an npy file.\n\n        NOTE: Assumes npy represents a numpy array of ndim 2.\n\n        Args:\n            fname (str): path string to npy file.\n            start_row (int): starting row from the npy file.\n            num_rows (int): number of rows to get from the npy file.\n            path_manager_key (str): Path manager key used to load from different\n                filesystems.\n\n        Returns:\n            output (np.ndarray): numpy array with the desired range of data from the\n                supplied npy file.\n        \"\"\"\n        path_manager = PathManagerFactory().get(path_manager_key)\n        with path_manager.open(fname, \"rb\") as fin:\n            np.lib.format.read_magic(fin)\n            shape, _order, dtype = np.lib.format.read_array_header_1_0(fin)\n            if len(shape) == 2:\n                total_rows, row_size = shape\n            else:\n                raise ValueError(\"Cannot load range for npy with ndim != 2.\")\n\n            if not (0 <= start_row < total_rows):\n                raise ValueError(\n                    f\"start_row ({start_row}) is out of bounds. It must be between 0 \"\n                    f\"and {total_rows - 1}, inclusive.\"\n                )\n            if not (start_row + num_rows <= total_rows):\n                raise ValueError(\n                    f\"num_rows ({num_rows}) exceeds number of available rows \"\n                    f\"({total_rows}) for the given start_row ({start_row}).\"\n                )\n            if mmap_mode:\n                data = np.load(fname, mmap_mode=\"r\")\n                data = data[start_row : start_row + num_rows]\n            else:\n                offset = start_row * row_size * dtype.itemsize\n                fin.seek(offset, os.SEEK_CUR)\n                num_entries = num_rows * row_size\n                data = np.fromfile(fin, dtype=dtype, count=num_entries)\n            return data.reshape((num_rows, row_size))",
  "def sparse_to_contiguous(\n        in_files: List[str],\n        output_dir: str,\n        frequency_threshold: int = FREQUENCY_THRESHOLD,\n        columns: int = CAT_FEATURE_COUNT,\n        path_manager_key: str = PATH_MANAGER_KEY,\n        output_file_suffix: str = \"_contig_freq.npy\",\n    ) -> None:\n        \"\"\"\n        Convert all sparse .npy files to have contiguous integers. Store in a separate\n        .npy file. All input files must be processed together because columns\n        can have matching IDs between files. Hence, they must be transformed\n        together. Also, the transformed IDs are not unique between columns. IDs\n        that appear less than frequency_threshold amount of times will be remapped\n        to have a value of 1.\n\n        Example transformation, frequency_threshold of 2:\n        day_0_sparse.npy\n        | col_0 | col_1 |\n        -----------------\n        | abc   | xyz   |\n        | iop   | xyz   |\n\n        day_1_sparse.npy\n        | col_0 | col_1 |\n        -----------------\n        | iop   | tuv   |\n        | lkj   | xyz   |\n\n        day_0_sparse_contig.npy\n        | col_0 | col_1 |\n        -----------------\n        | 1     | 2     |\n        | 2     | 2     |\n\n        day_1_sparse_contig.npy\n        | col_0 | col_1 |\n        -----------------\n        | 2     | 1     |\n        | 1     | 2     |\n\n        Args:\n            in_files List[str]: Input directory of npy files.\n            output_dir (str): Output directory of processed npy files.\n            frequency_threshold: IDs occurring less than this frequency will be remapped to a value of 1.\n            path_manager_key (str): Path manager key used to load from different filesystems.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Load each .npy file of sparse features. Transformations are made along the columns.\n        # Thereby, transpose the input to ease operations.\n        # E.g. file_to_features = {\"day_0_sparse\": [array([[3,6,7],[7,9,3]]}\n        file_to_features: Dict[str, np.ndarray] = {}\n        for f in in_files:\n            name = os.path.basename(f).split(\".\")[0]\n            file_to_features[name] = np.load(f).transpose()\n            print(f\"Successfully loaded file: {f}\")\n\n        # Iterate through each column in each file and map the sparse ids to contiguous ids.\n        for col in range(columns):\n            print(f\"Processing column: {col}\")\n\n            # Iterate through each row in each file for the current column and determine the\n            # frequency of each sparse id.\n            sparse_to_frequency: Dict[int, int] = {}\n            if frequency_threshold > 1:\n                for f in file_to_features:\n                    for _, sparse in enumerate(file_to_features[f][col]):\n                        if sparse in sparse_to_frequency:\n                            sparse_to_frequency[sparse] += 1\n                        else:\n                            sparse_to_frequency[sparse] = 1\n\n            # Iterate through each row in each file for the current column and remap each\n            # sparse id to a contiguous id. The contiguous ints start at a value of 2 so that\n            # infrequenct IDs (determined by the frequency_threshold) can be remapped to 1.\n            running_sum = 2\n            sparse_to_contiguous_int: Dict[int, int] = {}\n\n            for f in file_to_features:\n                print(f\"Processing file: {f}\")\n\n                for i, sparse in enumerate(file_to_features[f][col]):\n                    if sparse not in sparse_to_contiguous_int:\n                        # If the ID appears less than frequency_threshold amount of times\n                        # remap the value to 1.\n                        if (\n                            frequency_threshold > 1\n                            and sparse_to_frequency[sparse] < frequency_threshold\n                        ):\n                            sparse_to_contiguous_int[sparse] = 1\n                        else:\n                            sparse_to_contiguous_int[sparse] = running_sum\n                            running_sum += 1\n\n                    # Re-map sparse value to contiguous in place.\n                    file_to_features[f][col][i] = sparse_to_contiguous_int[sparse]\n\n        path_manager = PathManagerFactory().get(path_manager_key)\n        for f, features in file_to_features.items():\n            output_file = os.path.join(output_dir, f + output_file_suffix)\n            with path_manager.open(output_file, \"wb\") as fout:\n                print(f\"Writing file: {output_file}\")\n                # Transpose back the features when saving, as they were transposed when loading.\n                np.save(fout, features.transpose())",
  "def shuffle(\n        input_dir_labels_and_dense: str,\n        input_dir_sparse: str,\n        output_dir_shuffled: str,\n        rows_per_day: Dict[int, int],\n        output_dir_full_set: Optional[str] = None,\n        days: int = DAYS,\n        int_columns: int = INT_FEATURE_COUNT,\n        sparse_columns: int = CAT_FEATURE_COUNT,\n        path_manager_key: str = PATH_MANAGER_KEY,\n        random_seed: int = 0,\n    ) -> None:\n        \"\"\"\n        Shuffle the dataset. Expects the files to be in .npy format and the data\n        to be split by day and by dense, sparse and label data.\n        Dense data must be in: day_x_dense.npy\n        Sparse data must be in: day_x_sparse.npy\n        Labels data must be in: day_x_labels.npy\n\n        The dataset will be reconstructed, shuffled and then split back into\n        separate dense, sparse and labels files.\n\n        This will only shuffle the first DAYS-1 days as the training set. The final day will remain\n        untouched as the validation, and training set.\n\n        Args:\n            input_dir_labels_and_dense (str): Input directory of labels and dense npy files.\n            input_dir_sparse (str): Input directory of sparse npy files.\n            output_dir_shuffled (str): Output directory for shuffled labels, dense and sparse npy files.\n            rows_per_day Dict[int, int]: Number of rows in each file.\n            output_dir_full_set (str): Output directory of the full dataset, if desired.\n            days (int): Number of day files.\n            int_columns (int): Number of columns with dense features.\n            sparse_columns (int): Total number of categorical columns.\n            path_manager_key (str): Path manager key used to load from different filesystems.\n            random_seed (int): Random seed used for the random.shuffle operator.\n        \"\"\"\n\n        total_rows = sum(rows_per_day.values())\n        columns = int_columns + sparse_columns + 1  # add 1 for label column\n        full_dataset = np.zeros((total_rows, columns), dtype=np.float32)\n        curr_first_row = 0\n        curr_last_row = 0\n        for d in range(0, days - 1):\n            curr_last_row += rows_per_day[d]\n\n            # dense\n            path_to_file = os.path.join(\n                input_dir_labels_and_dense, f\"day_{d}_dense.npy\"\n            )\n            data = np.load(path_to_file)\n            print(\n                f\"Day {d} dense- {curr_first_row}-{curr_last_row} loaded files - {time.time()} - {path_to_file}\"\n            )\n\n            full_dataset[curr_first_row:curr_last_row, 0:int_columns] = data\n            del data\n\n            # sparse\n            path_to_file = os.path.join(input_dir_sparse, f\"day_{d}_sparse.npy\")\n            data = np.load(path_to_file)\n            print(\n                f\"Day {d} sparse- {curr_first_row}-{curr_last_row} loaded files - {time.time()} - {path_to_file}\"\n            )\n\n            full_dataset[curr_first_row:curr_last_row, int_columns : columns - 1] = data\n            del data\n\n            # labels\n            path_to_file = os.path.join(\n                input_dir_labels_and_dense, f\"day_{d}_labels.npy\"\n            )\n            data = np.load(path_to_file)\n            print(\n                f\"Day {d} labels- {curr_first_row}-{curr_last_row} loaded files - {time.time()} - {path_to_file}\"\n            )\n\n            full_dataset[curr_first_row:curr_last_row, columns - 1 :] = data\n            del data\n\n            curr_first_row = curr_last_row\n\n        path_manager = PathManagerFactory().get(path_manager_key)\n\n        # Save the full dataset\n        if output_dir_full_set is not None:\n            full_output_file = os.path.join(output_dir_full_set, \"full.npy\")\n            with path_manager.open(full_output_file, \"wb\") as fout:\n                print(f\"Writing full set file: {full_output_file}\")\n                np.save(fout, full_dataset)\n\n        print(f\"Shuffling dataset with random_seed={random_seed}\")\n        np.random.seed(random_seed)\n        np.random.shuffle(full_dataset)\n\n        # Slice and save each portion into dense, sparse and labels\n        curr_first_row = 0\n        curr_last_row = 0\n        for d in range(0, days - 1):\n            curr_last_row += rows_per_day[d]\n\n            # write dense columns\n            shuffled_dense_file = os.path.join(\n                output_dir_shuffled, f\"day_{d}_dense.npy\"\n            )\n            with path_manager.open(shuffled_dense_file, \"wb\") as fout:\n                print(\n                    f\"Writing rows {curr_first_row}-{curr_last_row-1} dense file: {shuffled_dense_file}\"\n                )\n                np.save(fout, full_dataset[curr_first_row:curr_last_row, 0:int_columns])\n\n            # write sparse columns\n            shuffled_sparse_file = os.path.join(\n                output_dir_shuffled, f\"day_{d}_sparse.npy\"\n            )\n            with path_manager.open(shuffled_sparse_file, \"wb\") as fout:\n                print(\n                    f\"Writing rows {curr_first_row}-{curr_last_row-1} sparse file: {shuffled_sparse_file}\"\n                )\n                np.save(\n                    fout,\n                    full_dataset[\n                        curr_first_row:curr_last_row, int_columns : columns - 1\n                    ].astype(np.int32),\n                )\n\n            # write labels columns\n            shuffled_labels_file = os.path.join(\n                output_dir_shuffled, f\"day_{d}_labels.npy\"\n            )\n            with path_manager.open(shuffled_labels_file, \"wb\") as fout:\n                print(\n                    f\"Writing rows {curr_first_row}-{curr_last_row-1} labels file: {shuffled_labels_file}\"\n                )\n                np.save(\n                    fout,\n                    full_dataset[curr_first_row:curr_last_row, columns - 1 :].astype(\n                        np.int32\n                    ),\n                )\n            curr_first_row = curr_last_row\n\n        # Directly copy over the last day's files since they will be used for validation and testing.\n        for (part, input_dir) in [\n            (\"sparse\", input_dir_sparse),\n            (\"dense\", input_dir_labels_and_dense),\n            (\"labels\", input_dir_labels_and_dense),\n        ]:\n            path_to_original = os.path.join(input_dir, f\"day_{days-1}_{part}.npy\")\n            val_train_path = os.path.join(\n                output_dir_shuffled, f\"day_{days-1}_{part}.npy\"\n            )\n            shutil.copyfile(path_to_original, val_train_path)\n            print(f\"Copying over {path_to_original} to {val_train_path}\")",
  "def __init__(\n        self,\n        stage: str,\n        dense_paths: List[str],\n        sparse_paths: List[str],\n        labels_paths: List[str],\n        batch_size: int,\n        rank: int,\n        world_size: int,\n        drop_last: Optional[bool] = False,\n        shuffle_batches: bool = False,\n        shuffle_training_set: bool = False,\n        shuffle_training_set_random_seed: int = 0,\n        mmap_mode: bool = False,\n        hashes: Optional[List[int]] = None,\n        path_manager_key: str = PATH_MANAGER_KEY,\n    ) -> None:\n        self.stage = stage\n        self.dense_paths = dense_paths\n        self.sparse_paths = sparse_paths\n        self.labels_paths = labels_paths\n        self.batch_size = batch_size\n        self.rank = rank\n        self.world_size = world_size\n        self.drop_last = drop_last\n        self.shuffle_batches = shuffle_batches\n        self.shuffle_training_set = shuffle_training_set\n        np.random.seed(shuffle_training_set_random_seed)\n        self.mmap_mode = mmap_mode\n        self.hashes: np.ndarray = np.array(hashes).reshape((1, CAT_FEATURE_COUNT))\n        self.path_manager_key = path_manager_key\n        self.path_manager: PathManager = PathManagerFactory().get(path_manager_key)\n\n        if shuffle_training_set and stage == \"train\":\n            self._shuffle_and_load_data_for_rank()\n            self.world_size = 1\n            self.rank = 0\n        else:\n            m = \"r\" if mmap_mode else None\n            self.dense_arrs: List[np.ndarray] = [\n                np.load(f, mmap_mode=m) for f in self.dense_paths\n            ]\n            self.sparse_arrs: List[np.ndarray] = [\n                np.load(f, mmap_mode=m) for f in self.sparse_paths\n            ]\n            self.labels_arrs: List[np.ndarray] = [\n                np.load(f, mmap_mode=m) for f in self.labels_paths\n            ]\n        len_d0 = len(self.dense_arrs[0])\n        second_half_start_index = int(len_d0 // 2 + len_d0 % 2)\n        if stage == \"val\":\n            self.dense_arrs[0] = self.dense_arrs[0][:second_half_start_index, :]\n            self.sparse_arrs[0] = self.sparse_arrs[0][:second_half_start_index, :]\n            self.labels_arrs[0] = self.labels_arrs[0][:second_half_start_index, :]\n        elif stage == \"test\":\n            self.dense_arrs[0] = self.dense_arrs[0][second_half_start_index:, :]\n            self.sparse_arrs[0] = self.sparse_arrs[0][second_half_start_index:, :]\n            self.labels_arrs[0] = self.labels_arrs[0][second_half_start_index:, :]\n        # When mmap_mode is enabled, sparse features are hashed when\n        # samples are batched in def __iter__. Otherwise, the dataset has been\n        # preloaded with sparse features hashed in the preload stage, here:\n        if not self.mmap_mode and self.hashes is not None:\n            for sparse_arr in self.sparse_arrs:\n                sparse_arr %= self.hashes\n\n        self.num_rows_per_file: List[int] = list(map(len, self.dense_arrs))\n        total_rows = sum(self.num_rows_per_file)\n        self.num_full_batches: int = (\n            total_rows // batch_size // self.world_size * self.world_size\n        )\n        self.last_batch_sizes: np.ndarray = np.array(\n            [0 for _ in range(self.world_size)]\n        )\n        remainder = total_rows % (self.world_size * batch_size)\n        if not self.drop_last and 0 < remainder:\n            if remainder < self.world_size:\n                self.num_full_batches -= self.world_size\n                self.last_batch_sizes += batch_size\n            else:\n                self.last_batch_sizes += remainder // self.world_size\n            self.last_batch_sizes[: remainder % self.world_size] += 1\n\n        # These values are the same for the KeyedJaggedTensors in all batches, so they\n        # are computed once here. This avoids extra work from the KeyedJaggedTensor sync\n        # functions.\n        self._num_ids_in_batch: int = CAT_FEATURE_COUNT * (batch_size + 1)\n        self.keys: List[str] = DEFAULT_CAT_NAMES\n        self.lengths: torch.Tensor = torch.ones(\n            (self._num_ids_in_batch,), dtype=torch.int32\n        )\n        self.offsets: torch.Tensor = torch.arange(\n            0, self._num_ids_in_batch + 1, dtype=torch.int32\n        )\n        self._num_ids_in_batch -= CAT_FEATURE_COUNT\n        self.length_per_key: List[int] = CAT_FEATURE_COUNT * [batch_size]\n        self.offset_per_key: List[int] = [\n            batch_size * i for i in range(CAT_FEATURE_COUNT + 1)\n        ]\n        self.index_per_key: Dict[str, int] = {\n            key: i for (i, key) in enumerate(self.keys)\n        }",
  "def _load_data_for_rank(self) -> None:\n        start_row, last_row = 0, None\n        if self.stage in [\"val\", \"test\"]:\n            # Last day's dataset is split into 2 sets: 1st half for \"val\"; 2nd for \"test\"\n            samples_in_file = BinaryCriteoUtils.get_shape_from_npy(\n                self.dense_paths[0], path_manager_key=self.path_manager_key\n            )[0]\n            start_row = 0\n            dataset_len = int(np.ceil(samples_in_file / 2.0))\n            if self.stage == \"test\":\n                start_row = dataset_len\n                dataset_len = samples_in_file - start_row\n            last_row = start_row + dataset_len - 1\n\n        row_ranges, remainder = BinaryCriteoUtils.get_file_row_ranges_and_remainder(\n            lengths=[\n                BinaryCriteoUtils.get_shape_from_npy(\n                    path, path_manager_key=self.path_manager_key\n                )[0]\n                for path in self.dense_paths\n            ],\n            rank=self.rank,\n            world_size=self.world_size,\n            start_row=start_row,\n            last_row=last_row,\n        )\n        self.remainder = remainder\n        self.dense_arrs, self.sparse_arrs, self.labels_arrs = [], [], []\n        for arrs, paths in zip(\n            [self.dense_arrs, self.sparse_arrs, self.labels_arrs],\n            [self.dense_paths, self.sparse_paths, self.labels_paths],\n        ):\n            for idx, (range_left, range_right) in row_ranges.items():\n                arrs.append(\n                    BinaryCriteoUtils.load_npy_range(\n                        paths[idx],\n                        range_left,\n                        range_right - range_left + 1,\n                        path_manager_key=self.path_manager_key,\n                        mmap_mode=self.mmap_mode,\n                    )\n                )",
  "def _shuffle_and_load_data_for_rank(self) -> None:\n        world_size = self.world_size\n        rank = self.rank\n        dense_arrs = [np.load(f, mmap_mode=\"r\") for f in self.dense_paths]\n        sparse_arrs = [np.load(f, mmap_mode=\"r\") for f in self.sparse_paths]\n        labels_arrs = [np.load(f, mmap_mode=\"r\") for f in self.labels_paths]\n        num_rows_per_file = list(map(len, dense_arrs))\n        total_rows = sum(num_rows_per_file)\n        permutation_arr = np.random.permutation(total_rows)\n        self.remainder = total_rows % world_size\n        rows_per_rank = total_rows // world_size\n        rows_per_rank = np.array([rows_per_rank for _ in range(world_size)])\n        rows_per_rank[: self.remainder] += 1\n        rank_rows_bins = np.cumsum(rows_per_rank)\n        rank_rows_bins_csr = np.cumsum([0] + list(rows_per_rank))\n\n        rows = rows_per_rank[rank]\n        d_sample, s_sample, l_sample = (\n            dense_arrs[0][0],\n            sparse_arrs[0][0],\n            labels_arrs[0][0],\n        )\n        shuffled_dense_arr = np.empty((rows, len(d_sample)), d_sample.dtype)\n        shuffled_sparse_arr = np.empty((rows, len(s_sample)), s_sample.dtype)\n        shuffled_labels_arr = np.empty((rows, len(l_sample)), l_sample.dtype)\n\n        day_rows_bins_csr = np.cumsum(np.array([0] + num_rows_per_file))\n        for i in range(len(dense_arrs)):\n            start = day_rows_bins_csr[i]\n            end = day_rows_bins_csr[i + 1]\n            indices_to_take = np.where(\n                rank == np.digitize(permutation_arr[start:end], rank_rows_bins)\n            )[0]\n            output_indices = (\n                permutation_arr[start + indices_to_take] - rank_rows_bins_csr[rank]\n            )\n            shuffled_dense_arr[output_indices] = dense_arrs[i][indices_to_take]\n            shuffled_sparse_arr[output_indices] = sparse_arrs[i][indices_to_take]\n            shuffled_labels_arr[output_indices] = labels_arrs[i][indices_to_take]\n        self.dense_arrs = [shuffled_dense_arr]\n        self.sparse_arrs = [shuffled_sparse_arr]\n        self.labels_arrs = [shuffled_labels_arr]",
  "def _np_arrays_to_batch(\n        self, dense: np.ndarray, sparse: np.ndarray, labels: np.ndarray\n    ) -> Batch:\n        if self.shuffle_batches:\n            # Shuffle all 3 in unison\n            shuffler = np.random.permutation(len(dense))\n            dense = dense[shuffler]\n            sparse = sparse[shuffler]\n            labels = labels[shuffler]\n\n        batch_size = len(dense)\n        num_ids_in_batch = CAT_FEATURE_COUNT * batch_size\n        if batch_size == self.batch_size:\n            length_per_key = self.length_per_key\n            offset_per_key = self.offset_per_key\n        else:\n            # handle last batch in dataset when it's an incomplete batch.\n            length_per_key = CAT_FEATURE_COUNT * [batch_size]\n            offset_per_key = [batch_size * i for i in range(CAT_FEATURE_COUNT + 1)]\n\n        return Batch(\n            dense_features=torch.from_numpy(dense),\n            sparse_features=KeyedJaggedTensor(\n                keys=self.keys,\n                # transpose + reshape(-1) incurs an additional copy.\n                values=torch.from_numpy(sparse.transpose(1, 0).reshape(-1)),\n                lengths=self.lengths[:num_ids_in_batch],\n                offsets=self.offsets[: num_ids_in_batch + 1],\n                stride=batch_size,\n                length_per_key=length_per_key,\n                offset_per_key=offset_per_key,\n                index_per_key=self.index_per_key,\n            ),\n            labels=torch.from_numpy(labels.reshape(-1)),\n        )",
  "def __iter__(self) -> Iterator[Batch]:\n        # Invariant: buffer never contains more than batch_size rows.\n        buffer: Optional[List[np.ndarray]] = None\n\n        def append_to_buffer(\n            dense: np.ndarray, sparse: np.ndarray, labels: np.ndarray\n        ) -> None:\n            nonlocal buffer\n            if buffer is None:\n                buffer = [dense, sparse, labels]\n            else:\n                for idx, arr in enumerate([dense, sparse, labels]):\n                    buffer[idx] = np.concatenate((buffer[idx], arr))\n\n        # Maintain a buffer that can contain up to batch_size rows. Fill buffer as\n        # much as possible on each iteration. Only return a new batch when batch_size\n        # rows are filled.\n        file_idx = 0\n        row_idx = 0\n        batch_idx = 0\n        buffer_row_count = 0\n        cur_batch_size = (\n            self.batch_size if self.num_full_batches > 0 else self.last_batch_sizes[0]\n        )\n        while (\n            batch_idx\n            < self.num_full_batches + (self.last_batch_sizes[0] > 0) * self.world_size\n        ):\n            if buffer_row_count == cur_batch_size or file_idx == len(self.dense_arrs):\n                if batch_idx % self.world_size == self.rank:\n                    yield self._np_arrays_to_batch(*none_throws(buffer))\n                    buffer = None\n                buffer_row_count = 0\n                batch_idx += 1\n                if 0 <= batch_idx - self.num_full_batches < self.world_size and (\n                    self.last_batch_sizes[0] > 0\n                ):\n                    cur_batch_size = self.last_batch_sizes[\n                        batch_idx - self.num_full_batches\n                    ]\n            else:\n                rows_to_get = min(\n                    cur_batch_size - buffer_row_count,\n                    self.num_rows_per_file[file_idx] - row_idx,\n                )\n                buffer_row_count += rows_to_get\n                slice_ = slice(row_idx, row_idx + rows_to_get)\n\n                if batch_idx % self.world_size == self.rank:\n                    dense_inputs = self.dense_arrs[file_idx][slice_, :]\n                    sparse_inputs = self.sparse_arrs[file_idx][slice_, :]\n                    target_labels = self.labels_arrs[file_idx][slice_, :]\n\n                    if self.mmap_mode and self.hashes is not None:\n                        sparse_inputs = sparse_inputs % self.hashes\n\n                    append_to_buffer(\n                        dense_inputs,\n                        sparse_inputs,\n                        target_labels,\n                    )\n                row_idx += rows_to_get\n\n                if row_idx >= self.num_rows_per_file[file_idx]:\n                    file_idx += 1\n                    row_idx = 0",
  "def __len__(self) -> int:\n        return self.num_full_batches // self.world_size + (self.last_batch_sizes[0] > 0)",
  "def row_mapper_with_fake_label_constant(\n            row: List[str],\n        ) -> Tuple[List[int], List[int], int]:\n            label = -1\n            dense = [int(row[i] or \"0\") for i in range(0, 0 + INT_FEATURE_COUNT)]\n            sparse = [\n                int(row[i] or \"0\", 16)\n                for i in range(\n                    0 + INT_FEATURE_COUNT, 0 + INT_FEATURE_COUNT + CAT_FEATURE_COUNT\n                )\n            ]\n            return dense, sparse, label",
  "def row_mapper(row: List[str]) -> Tuple[List[int], List[int], int]:\n            # Missing values are mapped to zero for both dense and sparse features\n            label = int(row[0] or \"0\")\n            dense = [int(row[i] or \"0\") for i in range(1, 1 + INT_FEATURE_COUNT)]\n            sparse = [\n                int(row[i] or \"0\", 16)\n                for i in range(\n                    1 + INT_FEATURE_COUNT, 1 + INT_FEATURE_COUNT + CAT_FEATURE_COUNT\n                )\n            ]\n            return dense, sparse, label",
  "def append_to_buffer(\n            dense: np.ndarray, sparse: np.ndarray, labels: np.ndarray\n        ) -> None:\n            nonlocal buffer\n            if buffer is None:\n                buffer = [dense, sparse, labels]\n            else:\n                for idx, arr in enumerate([dense, sparse, labels]):\n                    buffer[idx] = np.concatenate((buffer[idx], arr))",
  "class _RandomRecBatch:\n    generator: Optional[torch.Generator]\n\n    def __init__(\n        self,\n        keys: List[str],\n        batch_size: int,\n        hash_sizes: List[int],\n        ids_per_features: List[int],\n        num_dense: int,\n        manual_seed: Optional[int] = None,\n        num_generated_batches: int = 10,\n        num_batches: Optional[int] = None,\n        *,\n        min_ids_per_features: Optional[List[int]] = None,\n    ) -> None:\n\n        self.keys = keys\n        self.keys_length: int = len(keys)\n        self.batch_size = batch_size\n        self.hash_sizes = hash_sizes\n        self.ids_per_features = ids_per_features\n        self.min_ids_per_features: List[int] = (\n            min_ids_per_features\n            if min_ids_per_features\n            else [0] * len(ids_per_features)\n        )\n        self.num_dense = num_dense\n        self.num_batches = num_batches\n        self.num_generated_batches = num_generated_batches\n\n        if manual_seed is not None:\n            self.generator = torch.Generator()\n            self.generator.manual_seed(manual_seed)\n        else:\n            self.generator = None\n\n        self._generated_batches: List[Batch] = [\n            self._generate_batch() for _ in range(num_generated_batches)\n        ]\n        self.batch_index = 0\n\n    def __iter__(self) -> \"_RandomRecBatch\":\n        self.batch_index = 0\n        return self\n\n    def __next__(self) -> Batch:\n        if self.batch_index == self.num_batches:\n            raise StopIteration\n        if self.num_generated_batches >= 0:\n            batch = self._generated_batches[\n                self.batch_index % len(self._generated_batches)\n            ]\n        else:\n            batch = self._generate_batch()\n        self.batch_index += 1\n        return batch\n\n    def _generate_batch(self) -> Batch:\n\n        values = []\n        lengths = []\n        for key_idx, _ in enumerate(self.keys):\n            hash_size = self.hash_sizes[key_idx]\n            min_num_ids_in_batch = self.min_ids_per_features[key_idx]\n            max_num_ids_in_batch = self.ids_per_features[key_idx]\n            for _ in range(self.batch_size):\n                num_ids_in_batch = int(\n                    torch.randint(\n                        low=min_num_ids_in_batch,\n                        high=max_num_ids_in_batch + 1,\n                        size=(),\n                        generator=self.generator,\n                    ).item()\n                )\n                values.append(\n                    torch.randint(\n                        high=hash_size,\n                        size=(num_ids_in_batch,),\n                        generator=self.generator,\n                    )\n                )\n                lengths.extend([num_ids_in_batch])\n\n        sparse_features = KeyedJaggedTensor.from_lengths_sync(\n            keys=self.keys,\n            values=torch.cat(values),\n            lengths=torch.tensor(lengths, dtype=torch.int32),\n        )\n\n        dense_features = torch.randn(\n            self.batch_size,\n            self.num_dense,\n            generator=self.generator,\n        )\n        labels = torch.randint(\n            low=0,\n            high=2,\n            size=(self.batch_size,),\n            generator=self.generator,\n        )\n\n        batch = Batch(\n            dense_features=dense_features,\n            sparse_features=sparse_features,\n            labels=labels,\n        )\n        return batch",
  "class RandomRecDataset(IterableDataset[Batch]):\n    \"\"\"\n    Random iterable dataset used to generate batches for recommender systems\n    (RecSys). Currently produces unweighted sparse features only. TODO: Add\n    weighted sparse features.\n\n    Args:\n        keys (List[str]): List of feature names for sparse features.\n        batch_size (int): batch size.\n        hash_size (Optional[int]): Max sparse id value. All sparse IDs will be taken\n            modulo this value.\n        hash_sizes (Optional[List[int]]): Max sparse id value per feature in keys. Each\n            sparse ID will be taken modulo the corresponding value from this argument. Note, if this is used, hash_size will be ignored.\n        ids_per_feature (int): Number of IDs per sparse feature.\n        ids_per_features (int): Number of IDs per sparse feature in each key. Note, if this is used, ids_per_feature will be ignored.\n        num_dense (int): Number of dense features.\n        manual_seed (int): Seed for deterministic behavior.\n        num_batches: (Optional[int]): Num batches to generate before raising StopIteration\n        num_generated_batches int: Num batches to cache. If num_batches > num_generated batches, then we will cycle to the first generated batch.\n                                   If this value is negative, batches will be generated on the fly.\n        min_ids_per_feature (int): Minimum number of IDs per features.\n\n    Example::\n\n        dataset = RandomRecDataset(\n            keys=[\"feat1\", \"feat2\"],\n            batch_size=16,\n            hash_size=100_000,\n            ids_per_feature=1,\n            num_dense=13,\n        ),\n        example = next(iter(dataset))\n    \"\"\"\n\n    def __init__(\n        self,\n        keys: List[str],\n        batch_size: int,\n        hash_size: Optional[int] = 100,\n        hash_sizes: Optional[List[int]] = None,\n        ids_per_feature: Optional[int] = 2,\n        ids_per_features: Optional[List[int]] = None,\n        num_dense: int = 50,\n        manual_seed: Optional[int] = None,\n        num_batches: Optional[int] = None,\n        num_generated_batches: int = 10,\n        min_ids_per_feature: Optional[int] = None,\n        min_ids_per_features: Optional[List[int]] = None,\n    ) -> None:\n        super().__init__()\n\n        if hash_sizes is None:\n            hash_size = hash_size or 100\n            hash_sizes = [hash_size] * len(keys)\n\n        assert hash_sizes is not None\n        assert len(hash_sizes) == len(\n            keys\n        ), \"length of hash_sizes must be equal to the number of keys\"\n\n        if ids_per_features is None:\n            ids_per_feature = ids_per_feature or 2\n            ids_per_features = [ids_per_feature] * len(keys)\n\n        assert ids_per_features is not None\n\n        if min_ids_per_features is None:\n            min_ids_per_feature = (\n                min_ids_per_feature\n                if min_ids_per_feature is not None\n                else ids_per_feature\n            )\n            assert min_ids_per_feature is not None\n            min_ids_per_features = [min_ids_per_feature] * len(keys)\n\n        assert len(ids_per_features) == len(\n            keys\n        ), \"length of ids_per_features must be equal to the number of keys\"\n\n        self.batch_generator = _RandomRecBatch(\n            keys=keys,\n            batch_size=batch_size,\n            hash_sizes=hash_sizes,\n            ids_per_features=ids_per_features,\n            num_dense=num_dense,\n            manual_seed=manual_seed,\n            num_batches=None,\n            num_generated_batches=num_generated_batches,\n            min_ids_per_features=min_ids_per_features,\n        )\n        self.num_batches: int = cast(int, num_batches if not None else sys.maxsize)\n\n    def __iter__(self) -> Iterator[Batch]:\n        return itertools.islice(iter(self.batch_generator), self.num_batches)\n\n    def __len__(self) -> int:\n        return self.num_batches",
  "def __init__(\n        self,\n        keys: List[str],\n        batch_size: int,\n        hash_sizes: List[int],\n        ids_per_features: List[int],\n        num_dense: int,\n        manual_seed: Optional[int] = None,\n        num_generated_batches: int = 10,\n        num_batches: Optional[int] = None,\n        *,\n        min_ids_per_features: Optional[List[int]] = None,\n    ) -> None:\n\n        self.keys = keys\n        self.keys_length: int = len(keys)\n        self.batch_size = batch_size\n        self.hash_sizes = hash_sizes\n        self.ids_per_features = ids_per_features\n        self.min_ids_per_features: List[int] = (\n            min_ids_per_features\n            if min_ids_per_features\n            else [0] * len(ids_per_features)\n        )\n        self.num_dense = num_dense\n        self.num_batches = num_batches\n        self.num_generated_batches = num_generated_batches\n\n        if manual_seed is not None:\n            self.generator = torch.Generator()\n            self.generator.manual_seed(manual_seed)\n        else:\n            self.generator = None\n\n        self._generated_batches: List[Batch] = [\n            self._generate_batch() for _ in range(num_generated_batches)\n        ]\n        self.batch_index = 0",
  "def __iter__(self) -> \"_RandomRecBatch\":\n        self.batch_index = 0\n        return self",
  "def __next__(self) -> Batch:\n        if self.batch_index == self.num_batches:\n            raise StopIteration\n        if self.num_generated_batches >= 0:\n            batch = self._generated_batches[\n                self.batch_index % len(self._generated_batches)\n            ]\n        else:\n            batch = self._generate_batch()\n        self.batch_index += 1\n        return batch",
  "def _generate_batch(self) -> Batch:\n\n        values = []\n        lengths = []\n        for key_idx, _ in enumerate(self.keys):\n            hash_size = self.hash_sizes[key_idx]\n            min_num_ids_in_batch = self.min_ids_per_features[key_idx]\n            max_num_ids_in_batch = self.ids_per_features[key_idx]\n            for _ in range(self.batch_size):\n                num_ids_in_batch = int(\n                    torch.randint(\n                        low=min_num_ids_in_batch,\n                        high=max_num_ids_in_batch + 1,\n                        size=(),\n                        generator=self.generator,\n                    ).item()\n                )\n                values.append(\n                    torch.randint(\n                        high=hash_size,\n                        size=(num_ids_in_batch,),\n                        generator=self.generator,\n                    )\n                )\n                lengths.extend([num_ids_in_batch])\n\n        sparse_features = KeyedJaggedTensor.from_lengths_sync(\n            keys=self.keys,\n            values=torch.cat(values),\n            lengths=torch.tensor(lengths, dtype=torch.int32),\n        )\n\n        dense_features = torch.randn(\n            self.batch_size,\n            self.num_dense,\n            generator=self.generator,\n        )\n        labels = torch.randint(\n            low=0,\n            high=2,\n            size=(self.batch_size,),\n            generator=self.generator,\n        )\n\n        batch = Batch(\n            dense_features=dense_features,\n            sparse_features=sparse_features,\n            labels=labels,\n        )\n        return batch",
  "def __init__(\n        self,\n        keys: List[str],\n        batch_size: int,\n        hash_size: Optional[int] = 100,\n        hash_sizes: Optional[List[int]] = None,\n        ids_per_feature: Optional[int] = 2,\n        ids_per_features: Optional[List[int]] = None,\n        num_dense: int = 50,\n        manual_seed: Optional[int] = None,\n        num_batches: Optional[int] = None,\n        num_generated_batches: int = 10,\n        min_ids_per_feature: Optional[int] = None,\n        min_ids_per_features: Optional[List[int]] = None,\n    ) -> None:\n        super().__init__()\n\n        if hash_sizes is None:\n            hash_size = hash_size or 100\n            hash_sizes = [hash_size] * len(keys)\n\n        assert hash_sizes is not None\n        assert len(hash_sizes) == len(\n            keys\n        ), \"length of hash_sizes must be equal to the number of keys\"\n\n        if ids_per_features is None:\n            ids_per_feature = ids_per_feature or 2\n            ids_per_features = [ids_per_feature] * len(keys)\n\n        assert ids_per_features is not None\n\n        if min_ids_per_features is None:\n            min_ids_per_feature = (\n                min_ids_per_feature\n                if min_ids_per_feature is not None\n                else ids_per_feature\n            )\n            assert min_ids_per_feature is not None\n            min_ids_per_features = [min_ids_per_feature] * len(keys)\n\n        assert len(ids_per_features) == len(\n            keys\n        ), \"length of ids_per_features must be equal to the number of keys\"\n\n        self.batch_generator = _RandomRecBatch(\n            keys=keys,\n            batch_size=batch_size,\n            hash_sizes=hash_sizes,\n            ids_per_features=ids_per_features,\n            num_dense=num_dense,\n            manual_seed=manual_seed,\n            num_batches=None,\n            num_generated_batches=num_generated_batches,\n            min_ids_per_features=min_ids_per_features,\n        )\n        self.num_batches: int = cast(int, num_batches if not None else sys.maxsize)",
  "def __iter__(self) -> Iterator[Batch]:\n        return itertools.islice(iter(self.batch_generator), self.num_batches)",
  "def __len__(self) -> int:\n        return self.num_batches",
  "def parse_args(argv: List[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Shuffle preprocessed npy dataset.\")\n    parser.add_argument(\n        \"--input_dir_labels_and_dense\",\n        type=str,\n        required=True,\n        help=\"Input directory containing labels and dense features.\",\n    )\n    parser.add_argument(\n        \"--input_dir_sparse\",\n        type=str,\n        required=True,\n        help=\"Input directory with sparse features. Sometimes these\"\n        \" features can be stored in a separate directory from the\"\n        \" labels and dense features as extra pre-processing was\"\n        \" applied to them.\",\n    )\n    parser.add_argument(\n        \"--output_dir_full_set\",\n        type=str,\n        default=None,\n        help=\"If specified, store the full dataset (unshuffled).\",\n    )\n    parser.add_argument(\n        \"--output_dir_shuffled\",\n        type=str,\n        required=True,\n        help=\"Output directory to store split shuffled npy files.\",\n    )\n    parser.add_argument(\n        \"--random_seed\",\n        type=int,\n        default=0,\n        help=\"random seed for the dataset shuffle\",\n    )\n    return parser.parse_args(argv)",
  "def count_rows(rows_per_file, path, day):\n    day_file = os.path.join(path, f\"day_{day}_labels.npy\")\n    data = np.load(day_file)\n    num_rows = data.shape[0]\n\n    rows_per_file[day] = num_rows\n    print(f\"counted {num_rows} for {day_file}\")",
  "def main(argv: List[str]) -> None:\n    args = parse_args(argv)\n    input_dir_labels_and_dense = args.input_dir_labels_and_dense\n    input_dir_sparse = args.input_dir_sparse\n    output_dir_full_set = args.output_dir_full_set\n    output_dir_shuffled = args.output_dir_shuffled\n\n    # Count num rows in each day file.\n    rows_per_file = Manager().dict()\n\n    # Adjust the number of processes here if <24 processes available to run\n    # simultaneously.\n    processes = [\n        Process(\n            target=count_rows,\n            name=\"count_rows:day%i\" % i,\n            args=(rows_per_file, input_dir_labels_and_dense, i),\n        )\n        for i in range(0, DAYS - 1)\n    ]\n\n    for process in processes:\n        process.start()\n    for process in processes:\n        process.join()\n\n    BinaryCriteoUtils.shuffle(\n        input_dir_labels_and_dense,\n        input_dir_sparse,\n        output_dir_shuffled,\n        rows_per_file,\n        output_dir_full_set,\n        random_seed=args.random_seed,\n        days=DAYS,\n    )",
  "def parse_args(argv: List[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Criteo tsv -> npy preprocessing script.\"\n    )\n    parser.add_argument(\n        \"--input_dir\",\n        type=str,\n        required=True,\n        help=\"Input directory containing Criteo tsv files.\"\n        \"For criteo_1tb, files in the directory should be named day_{0-23}.\"\n        \"For criteo_kaggle, files in the directory should be train.txt & test.txt.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        required=True,\n        help=\"Output directory to store npy files.\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        choices=[\"criteo_1tb\", \"criteo_kaggle\"],\n        default=\"criteo_1tb\",\n        help=\"dataset for experiment, current support criteo_1tb, criteo_kaggle\",\n    )\n    return parser.parse_args(argv)",
  "def main(argv: List[str]) -> None:\n    \"\"\"\n    This function preprocesses the raw Criteo tsvs into the format (npy binary)\n    expected by InMemoryBinaryCriteoIterDataPipe.\n\n    Args:\n        argv (List[str]): Command line args.\n\n    Returns:\n        None.\n    \"\"\"\n\n    args = parse_args(argv)\n    input_dir = args.input_dir\n    output_dir = args.output_dir\n\n    if args.dataset_name == \"criteo_1tb\":\n        in_files_l = [f\"day_{i}\" for i in range(24)]\n        out_files_l = in_files_l\n    else:\n        # criteo_kaggle code path\n        in_files_l = [\"train.txt\", \"test.txt\"]\n        out_files_l = [\"train\", \"test\"]\n\n    for input, output in zip(in_files_l, out_files_l):\n        in_file_path = os.path.join(input_dir, input)\n        if not os.path.exists(in_file_path):\n            continue\n        dense_out_file_path = os.path.join(output_dir, output + \"_dense.npy\")\n        sparse_out_file_path = os.path.join(output_dir, output + \"_sparse.npy\")\n        labels_out_file_path = os.path.join(output_dir, output + \"_labels.npy\")\n        print(\n            f\"Processing {in_file_path}.\\nOutput will be saved to\\n{dense_out_file_path}\"\n            f\"\\n{sparse_out_file_path}\\n{labels_out_file_path}\"\n        )\n        BinaryCriteoUtils.tsv_to_npys(\n            in_file_path,\n            dense_out_file_path,\n            sparse_out_file_path,\n            labels_out_file_path,\n            args.dataset_name,\n        )\n        print(f\"Done processing {in_file_path}.\")",
  "def parse_args(argv: List[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Criteo sparse -> contiguous preprocessing script. \"\n    )\n    parser.add_argument(\n        \"--input_dir\",\n        type=str,\n        required=True,\n        help=\"Input directory containing the sparse features in numpy format (.npy). Files in the directory \"\n        \"should be named day_{0-23}_sparse.npy.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        required=True,\n        help=\"Output directory to store npy files.\",\n    )\n    parser.add_argument(\n        \"--frequency_threshold\",\n        type=int,\n        default=0,\n        help=\"IDs occuring less than this frequency will be remapped to an index of 1. If this value is not set (e.g. 0), no frequency thresholding will be applied.\",\n    )\n    return parser.parse_args(argv)",
  "def main(argv: List[str]) -> None:\n    \"\"\"\n    This function processes the sparse features (.npy) to be contiguous\n    and saves the result in a separate (.npy) file.\n\n    Args:\n        argv (List[str]): Command line args.\n\n    Returns:\n        None.\n    \"\"\"\n\n    args = parse_args(argv)\n    input_dir = args.input_dir\n    output_dir = args.output_dir\n\n    # Look for files that end in \"_sparse.npy\" since this processing is\n    # only applied to sparse data.\n\n    input_files = [os.path.join(input_dir, f\"day_{i}_sparse.npy\") for i in range(DAYS)]\n\n    if not input_files:\n        raise ValueError(\n            f\"There are no files that end with '_sparse.npy' in this directory: {input_dir}\"\n        )\n\n    print(f\"Processing files in: {input_files}. Outputs will be saved to {output_dir}.\")\n    BinaryCriteoUtils.sparse_to_contiguous(\n        input_files, output_dir, frequency_threshold=int(args.frequency_threshold)\n    )\n    print(\"Done processing.\")",
  "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Preprocess criteo dataset\")\n    parser.add_argument(\"--base_path\", \"-b\", dest=\"base_path\", help=\"Base path\")\n    parser.add_argument(\n        \"--shuffle_train\",\n        \"-s\",\n        dest=\"shuffle_train\",\n        default=False,\n        action=\"store_true\",\n        help=\"Base path\",\n    )\n    args = parser.parse_args()\n\n    return args",
  "def process_file(f, dst):\n    data = pd.read_parquet(f)\n    data = data[DEFAULT_COLUMN_NAMES]\n\n    data[DEFAULT_LABEL_NAME] = data[DEFAULT_LABEL_NAME].astype(np.int32)\n    data[DEFAULT_INT_NAMES] = data[DEFAULT_INT_NAMES].astype(np.float32)\n    data[DEFAULT_CAT_NAMES] = data[DEFAULT_CAT_NAMES].astype(np.int32)\n\n    data = data.to_records(index=False)\n    data = data.tobytes()\n\n    dst_file = dst + \"/\" + f.split(\"/\")[-1] + \".bin\"\n    with open(dst_file, \"wb\") as dst_fd:\n        dst_fd.write(data)",
  "def main():\n    start_time = time.time()\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--src_dir\", type=str)\n    parser.add_argument(\"--intermediate_dir\", type=str)\n    parser.add_argument(\"--dst_dir\", type=str)\n    parser.add_argument(\"--parallel_jobs\", default=20, type=int)\n    args = parser.parse_args()\n\n    print(\"Processing train files...\")\n    train_src_files = glob.glob(args.src_dir + \"/train/*.parquet\")\n    train_intermediate_dir = os.path.join(args.intermediate_dir, \"train\")\n    os.makedirs(train_intermediate_dir, exist_ok=True)\n\n    Parallel(n_jobs=args.parallel_jobs)(\n        delayed(process_file)(f, train_intermediate_dir)\n        for f in tqdm.tqdm(train_src_files)\n    )\n\n    print(\"Train files conversion done\")\n\n    print(\"Processing test files...\")\n    test_src_files = glob.glob(args.src_dir + \"/test/*.parquet\")\n    test_intermediate_dir = os.path.join(args.intermediate_dir, \"test\")\n    os.makedirs(test_intermediate_dir, exist_ok=True)\n\n    Parallel(n_jobs=args.parallel_jobs)(\n        delayed(process_file)(f, test_intermediate_dir)\n        for f in tqdm.tqdm(test_src_files)\n    )\n    print(\"Test files conversion done\")\n\n    print(\"Processing validation files...\")\n    valid_src_files = glob.glob(args.src_dir + \"/validation/*.parquet\")\n    valid_intermediate_dir = os.path.join(args.intermediate_dir, \"validation\")\n    os.makedirs(valid_intermediate_dir, exist_ok=True)\n\n    Parallel(n_jobs=args.parallel_jobs)(\n        delayed(process_file)(f, valid_intermediate_dir)\n        for f in tqdm.tqdm(valid_src_files)\n    )\n    print(\"Validation files conversion done\")\n\n    os.makedirs(args.dst_dir, exist_ok=True)\n\n    print(\"Concatenating train files\")\n    os.system(f\"cat {train_intermediate_dir}/*.bin > {args.dst_dir}/train_data.bin\")\n\n    print(\"Concatenating test files\")\n    os.system(f\"cat {test_intermediate_dir}/*.bin > {args.dst_dir}/test_data.bin\")\n\n    print(\"Concatenating validation files\")\n    os.system(\n        f\"cat {valid_intermediate_dir}/*.bin > {args.dst_dir}/validation_data.bin\"\n    )\n    print(f\"Processing took {time.time()-start_time:.2f} sec\")",
  "def split_binary_file(\n    binary_file_path: str,\n    output_dir: str,\n    categorical_feature_sizes: Sequence[int],\n    batch_size: int,\n    source_data_type: str = \"int32\",\n):\n    record_width = (\n        1 + len(DEFAULT_INT_NAMES) + len(categorical_feature_sizes)\n    )  # label + numerical + categorical\n    bytes_per_feature = np.__dict__[source_data_type]().nbytes\n    bytes_per_entry = record_width * bytes_per_feature\n\n    total_size = os.path.getsize(binary_file_path)\n    batches_num = int(math.ceil((total_size // bytes_per_entry) / batch_size))\n\n    file_streams = []\n    try:\n        input_data_f = open(binary_file_path, \"rb\")  # noqa: P201\n        file_streams.append(input_data_f)\n\n        numerical_f = open(  # noqa: P201\n            os.path.join(output_dir, \"numerical.bin\"), \"wb+\"\n        )\n        file_streams.append(numerical_f)\n\n        label_f = open(os.path.join(output_dir, \"label.bin\"), \"wb+\")  # noqa: P201\n        file_streams.append(label_f)\n\n        categorical_fs = []\n        for i in range(len(categorical_feature_sizes)):\n            fs = open(os.path.join(output_dir, f\"cat_{i}.bin\"), \"wb+\")  # noqa: P201\n            categorical_fs.append(fs)\n            file_streams.append(fs)\n\n        for _ in tqdm(range(batches_num)):\n            raw_data = np.frombuffer(\n                input_data_f.read(bytes_per_entry * batch_size), dtype=np.int32\n            )\n            batch_data = raw_data.reshape(-1, record_width)\n\n            numerical_features = batch_data[:, 1 : 1 + len(DEFAULT_INT_NAMES)].view(\n                dtype=np.float32\n            )\n            numerical_f.write(numerical_features.tobytes())\n\n            label = batch_data[:, 0]\n            label_f.write(label.astype(np.float32).tobytes())\n\n            cat_offset = len(DEFAULT_INT_NAMES) + 1\n            for cat_idx in range(CAT_FEATURE_COUNT):\n                cat_data = batch_data[\n                    :, (cat_idx + cat_offset) : (cat_idx + cat_offset + 1)\n                ].astype(np.int32)\n                categorical_fs[cat_idx].write(cat_data.tobytes())\n    finally:\n        for stream in file_streams:\n            stream.close()",
  "def split_dataset(dataset_dir: str, output_dir: str, batch_size: int):\n    train_file = os.path.join(dataset_dir, \"train_data.bin\")\n    test_file = os.path.join(dataset_dir, \"test_data.bin\")\n    val_file = os.path.join(dataset_dir, \"validation_data.bin\")\n\n    target_train = os.path.join(output_dir, \"train\")\n    target_test = os.path.join(output_dir, \"test\")\n    target_val = os.path.join(output_dir, \"validation\")\n\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(target_train, exist_ok=True)\n    os.makedirs(target_test, exist_ok=True)\n    os.makedirs(target_val, exist_ok=True)\n\n    split_binary_file(\n        test_file,\n        target_test,\n        NUM_EMBEDDINGS_PER_FEATURE,\n        batch_size,\n    )\n    split_binary_file(\n        train_file,\n        target_train,\n        NUM_EMBEDDINGS_PER_FEATURE,\n        batch_size,\n    )\n    split_binary_file(val_file, target_val, NUM_EMBEDDINGS_PER_FEATURE, batch_size)",
  "def convert_tsv_to_parquet(input_path: str, output_base_path: str):\n    config = {\n        \"engine\": \"csv\",\n        \"names\": DEFAULT_COLUMN_NAMES,\n        \"sep\": \"\\t\",\n        \"dtypes\": dtypes,\n        \"part_size\": \"128MB\",\n    }\n\n    output_path = os.path.join(output_base_path, \"criteo_parquet\")\n    if os.path.exists(output_path):\n        shutil.rmtree(output_path)\n    os.makedirs(output_path)\n\n    # split last day into two parts\n    number_of_lines = int(\n        subprocess.check_output(\n            (f'wc -l {os.path.join(input_path, \"day_23\")}').split()\n        ).split()[0]\n    )\n    valid_set_size = number_of_lines // 2\n    test_set_size = number_of_lines - valid_set_size\n\n    start_time = time.time()\n    with open(os.path.join(input_path, \"day_23.part0\"), \"w\") as f:\n        subprocess.run(\n            [\"head\", \"-n\", str(test_set_size), str(os.path.join(input_path, \"day_23\"))],\n            stdout=f,\n        )\n\n    with open(os.path.join(input_path, \"day_23.part1\"), \"w\") as f:\n        subprocess.run(\n            [\n                \"tail\",\n                \"-n\",\n                str(valid_set_size),\n                str(os.path.join(input_path, \"day_23\")),\n            ],\n            stdout=f,\n        )\n\n    print(f\"finished splitting the last day, took {time.time() - start_time}\")\n\n    input_paths = [\n        os.path.join(input_path, f\"day_{day}\") for day in range(DAYS - 1)\n    ] + [os.path.join(input_path, f\"day_23.part{i}\") for i in range(2)]\n\n    print(f\"handling the input paths: {input_paths}\")\n\n    tsv_dataset = nvt.Dataset(input_paths, **config)\n\n    print(\"finished loading the tsv dataset\")\n\n    tsv_dataset.to_parquet(\n        output_path,\n        preserve_files=True,\n    )",
  "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Convert criteo tsv to parquet\")\n    parser.add_argument(\n        \"--input_path\", \"-i\", dest=\"input_path\", help=\"Input path containing tsv files\"\n    )\n    parser.add_argument(\n        \"--output_base_path\", \"-o\", dest=\"output_base_path\", help=\"Output base path\"\n    )\n\n    args = parser.parse_args()\n\n    return args",
  "def setup_dask(dask_workdir):\n    if os.path.exists(dask_workdir):\n        shutil.rmtree(dask_workdir)\n    os.makedirs(dask_workdir)\n\n    device_limit_frac = 0.8  # Spill GPU-Worker memory to host at this limit.\n    device_pool_frac = 0.7\n\n    # Use total device size to calculate device limit and pool_size\n    device_size = device_mem_size(kind=\"total\")\n    device_limit = int(device_limit_frac * device_size)\n    device_pool_size = int(device_pool_frac * device_size)\n\n    cluster = LocalCUDACluster(\n        protocol=\"tcp\",\n        n_workers=len(numba.cuda.gpus),\n        CUDA_VISIBLE_DEVICES=range(len(numba.cuda.gpus)),\n        device_memory_limit=device_limit,\n        local_directory=dask_workdir,\n        rmm_pool_size=(device_pool_size // 256) * 256,\n    )\n\n    return Client(cluster)",
  "def in_backward_optimizer_filter(\n    named_parameters: Iterator[Tuple[str, nn.Parameter]], include: bool = False\n) -> Iterator[Tuple[str, nn.Parameter]]:\n    \"\"\"\n    Filters named_parameters for whether they are or or not params that use\n    the in_backward_optimizer.\n    Note: This only supports the in_backward_optimizer from PT-D's API.\n        The torchrec's equivalent API is deprecated and is not supported.\n    Args:\n    named_parameters(Iterator[Tuple[str, nn.Parameter]]): named_parameters\n    include(bool): If true, only yields params with in_backward_optimizer. If false, returns the outside set\n        Defaults to include params that are not in_backward (False)\n    \"\"\"\n    for fqn, param in named_parameters:\n        if hasattr(param, \"_in_backward_optimizers\") == include:\n            yield fqn, param",
  "class SGD(Optimizer):\n    r\"\"\"\n    Placeholder for SGD. This optimizer will not functionally run.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs\n\n    @torch.no_grad()\n    # pyre-ignore\n    def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "class LarsSGD(Optimizer):\n    r\"\"\"\n    Placeholder for LARS_SGD. This optimizer will not functionally run.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs\n\n    @torch.no_grad()\n    # pyre-ignore\n    def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "class LAMB(Optimizer):\n    r\"\"\"\n    Placeholder for LAMB. This optimizer will not functionally run.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs\n\n    @torch.no_grad()\n    # pyre-ignore\n    def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "class PartialRowWiseLAMB(Optimizer):\n    r\"\"\"\n    Placeholder for PartialRowWiseLAMB. This optimizer will not functionally run.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs\n\n    @torch.no_grad()\n    # pyre-ignore\n    def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "class Adam(Optimizer):\n    r\"\"\"\n    Placeholder for Adam. This optimizer will not functionally run.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs\n\n    @torch.no_grad()\n    # pyre-ignore\n    def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "class PartialRowWiseAdam(Optimizer):\n    r\"\"\"\n    Placeholder for PartialRowWiseAdam. This optimizer will not functionally run.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs\n\n    @torch.no_grad()\n    # pyre-ignore\n    def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "class Adagrad(Optimizer):\n    r\"\"\"\n    Placeholder for Adagrad. This optimizer will not functionally run.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs\n\n    @torch.no_grad()\n    # pyre-ignore\n    def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs",
  "def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs",
  "def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs",
  "def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs",
  "def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs",
  "def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs",
  "def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        # pyre-ignore\n        **kwargs,\n    ) -> None:\n        self._params = params\n        # pyre-ignore\n        self._kwargs = kwargs",
  "def step(self, closure=None) -> torch.Tensor:\n        raise NotImplementedError",
  "class FusedOptimizer(KeyedOptimizer, abc.ABC):\n    \"\"\"\n    Assumes that weight update is done during backward pass,\n    thus step() is a no-op.\n    \"\"\"\n\n    @abc.abstractmethod\n    # pyre-ignore [2]\n    def step(self, closure: Any = None) -> None:\n        ...\n\n    @abc.abstractmethod\n    def zero_grad(self, set_to_none: bool = False) -> None:\n        ...\n\n    def __repr__(self) -> str:\n        return optim.Optimizer.__repr__(self)",
  "class EmptyFusedOptimizer(FusedOptimizer):\n    \"\"\"\n    Fused Optimizer class with no-op step and no parameters to optimize over\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__({}, {}, {})\n\n    # pyre-ignore\n    def step(self, closure: Any = None) -> None:\n        pass\n\n    def zero_grad(self, set_to_none: bool = False) -> None:\n        pass",
  "class FusedOptimizerModule(abc.ABC):\n    \"\"\"\n    Module, which does weight update during backward pass.\n    \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def fused_optimizer(self) -> KeyedOptimizer:\n        ...",
  "def step(self, closure: Any = None) -> None:\n        ...",
  "def zero_grad(self, set_to_none: bool = False) -> None:\n        ...",
  "def __repr__(self) -> str:\n        return optim.Optimizer.__repr__(self)",
  "def __init__(self) -> None:\n        super().__init__({}, {}, {})",
  "def step(self, closure: Any = None) -> None:\n        pass",
  "def zero_grad(self, set_to_none: bool = False) -> None:\n        pass",
  "def fused_optimizer(self) -> KeyedOptimizer:\n        ...",
  "class WarmupPolicy(Enum):\n    NONE = \"none\"\n    LINEAR = \"linear\"\n    CONSTANT = \"constant\"\n    POLY = \"poly\"\n    STEP = \"step\"\n    INVSQRT = \"inv_sqrt\"",
  "class WarmupStage:\n    policy: WarmupPolicy = WarmupPolicy.LINEAR\n    max_iters: int = 1\n    value: float = 1.0\n    lr_scale: float = 1.0\n    # used as number denominator for iters in poly decay\n    # default to max_iters if not set to value > 0\n    # also used as stepsize in step decay\n    # default to 1 if not set to value > 0\n    decay_iters: int = -1",
  "def _lr_stages(stages: List[WarmupStage]) -> List[WarmupStage]:\n    last_stage = WarmupStage(policy=WarmupPolicy.NONE, max_iters=1 << 63, value=1.0)\n    if len(stages) == 0:\n        return [last_stage]\n\n    start_iter = 0\n    for stage in stages:\n        assert stage.max_iters > start_iter, (\n            f\"Max iter of the stage {stage} must be greater than the previous \"\n            f\"max iter {start_iter}\"\n        )\n        start_iter = stage.max_iters\n        if stage.decay_iters <= 0:\n            if stage.policy == WarmupPolicy.STEP:\n                stage.decay_iters = 1\n            else:\n                stage.decay_iters = stage.max_iters\n    return stages + [last_stage]",
  "def _get_multiplier(stage: WarmupStage, iter: int) -> float:\n    multiplier = 1.0\n    if stage.policy == WarmupPolicy.LINEAR:\n        multiplier = stage.value + (1.0 - stage.value) * iter / stage.max_iters\n    elif stage.policy == WarmupPolicy.CONSTANT:\n        multiplier = stage.value\n    elif stage.policy == WarmupPolicy.POLY:\n        multiplier = math.pow(1 - iter / stage.decay_iters, stage.value)\n    elif stage.policy == WarmupPolicy.STEP:\n        multiplier = math.pow(stage.value, iter // stage.decay_iters)\n    elif stage.policy == WarmupPolicy.INVSQRT:\n        multiplier = 1.0 / math.sqrt(iter)\n    return multiplier * stage.lr_scale",
  "class WarmupOptimizer(OptimizerWrapper):\n    \"\"\"\n    Adjusts learning rate according to the schedule.\n\n    Args:\n        optimizer (KeyedOptimizer): optimizer to wrap\n        stages (List[WarmupStage]): stages to go through\n        lr (float): initial learning rate\n        lr_param (str): learning rate parameter in parameter group.\n        param_name: Name of fake parameter to hold warmup state.\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer: KeyedOptimizer,\n        stages: List[WarmupStage],\n        lr: float = 0.1,\n        lr_param: str = \"lr\",\n        param_name: str = \"__warmup\",\n    ) -> None:\n        super().__init__(optimizer)\n        self._stages: List[WarmupStage] = _lr_stages(stages)\n        self._lr_param: str = lr_param\n        self._lr: float = lr\n        self._warmup_param: torch.nn.Parameter = torch.nn.Parameter()\n        # pyre-ignore [16]\n        self.params[param_name] = self._warmup_param\n        # for fused optimizer we will do first backward() pass before calling step()\n        self._set_lr(0, 0)\n\n    def _set_lr(self, iter_: int, stage_id: int) -> None:\n        lr = self._lr * _get_multiplier(self._stages[stage_id], iter_)\n        for param_group in self.param_groups:\n            # pyre-ignore [16]\n            param_group[self._lr_param] = lr\n\n    def _get_warmup_state(self) -> Tuple[int, int]:\n        if self._warmup_param in self.state:\n            iter_, stage_id = self.state[self._warmup_param][\"warmup\"].tolist()\n        else:\n            iter_ = 0\n            stage_id = 0\n        return iter_, stage_id\n\n    def post_load_state_dict(self) -> None:\n        iter_, stage_id = self._get_warmup_state()\n        logger.info(f\"Warmup Optimizer set to iteration {iter_}\")\n        self._set_lr(iter_, stage_id)\n\n    # pyre-ignore [2]\n    def step(self, closure: Any = None) -> None:\n        super().step(closure)\n        iter_, stage_id = self._get_warmup_state()\n\n        iter_ += 1\n        if iter_ > self._stages[stage_id].max_iters and stage_id + 1 < len(\n            self._stages\n        ):\n            stage_id += 1\n            logger.info(\n                \"Warmup Optimizer finishing \"\n                f\"{self._stages[stage_id - 1]} \"\n                \"switching to \"\n                f\"{self._stages[stage_id]}\"\n            )\n        self._set_lr(iter_, stage_id)\n\n        # pyre-ignore [16]\n        self.state[self._warmup_param] = {\n            \"warmup\": torch.tensor([iter_, stage_id], dtype=torch.long)\n        }",
  "def __init__(\n        self,\n        optimizer: KeyedOptimizer,\n        stages: List[WarmupStage],\n        lr: float = 0.1,\n        lr_param: str = \"lr\",\n        param_name: str = \"__warmup\",\n    ) -> None:\n        super().__init__(optimizer)\n        self._stages: List[WarmupStage] = _lr_stages(stages)\n        self._lr_param: str = lr_param\n        self._lr: float = lr\n        self._warmup_param: torch.nn.Parameter = torch.nn.Parameter()\n        # pyre-ignore [16]\n        self.params[param_name] = self._warmup_param\n        # for fused optimizer we will do first backward() pass before calling step()\n        self._set_lr(0, 0)",
  "def _set_lr(self, iter_: int, stage_id: int) -> None:\n        lr = self._lr * _get_multiplier(self._stages[stage_id], iter_)\n        for param_group in self.param_groups:\n            # pyre-ignore [16]\n            param_group[self._lr_param] = lr",
  "def _get_warmup_state(self) -> Tuple[int, int]:\n        if self._warmup_param in self.state:\n            iter_, stage_id = self.state[self._warmup_param][\"warmup\"].tolist()\n        else:\n            iter_ = 0\n            stage_id = 0\n        return iter_, stage_id",
  "def post_load_state_dict(self) -> None:\n        iter_, stage_id = self._get_warmup_state()\n        logger.info(f\"Warmup Optimizer set to iteration {iter_}\")\n        self._set_lr(iter_, stage_id)",
  "def step(self, closure: Any = None) -> None:\n        super().step(closure)\n        iter_, stage_id = self._get_warmup_state()\n\n        iter_ += 1\n        if iter_ > self._stages[stage_id].max_iters and stage_id + 1 < len(\n            self._stages\n        ):\n            stage_id += 1\n            logger.info(\n                \"Warmup Optimizer finishing \"\n                f\"{self._stages[stage_id - 1]} \"\n                \"switching to \"\n                f\"{self._stages[stage_id]}\"\n            )\n        self._set_lr(iter_, stage_id)\n\n        # pyre-ignore [16]\n        self.state[self._warmup_param] = {\n            \"warmup\": torch.tensor([iter_, stage_id], dtype=torch.long)\n        }",
  "class KeyedOptimizer(optim.Optimizer):\n    \"\"\"\n    Takes a dict of parameters and exposes state_dict by parameter key.\n\n    This implementation is much stricter than the one in torch.Optimizer:\n    it requires implementations to fully initialize their state during first optimization iteration,\n    and it prohibits loading an empty state into already initialized KeyedOptimizer and vise versa.\n\n    It also doesn't expose param_groups in state_dict() by default\n    Old behavior can be switch on by setting save_param_groups flag.\n    The reason is that during distributed training not all parameters are present on all ranks\n    and we identify param_group by its parameters.\n    In addition to that, param_groups are typically re-set during training initialization,\n    so it makes little sense to save them as a part of the state to begin with.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Mapping[str, Union[torch.Tensor, ShardedTensor]],\n        # pyre-ignore [2]\n        state: Mapping[Any, Any],\n        param_groups: Collection[Mapping[str, Any]],\n    ) -> None:\n        torch._C._log_api_usage_once(f\"torchrec.optim.{self.__class__.__name__}\")\n\n        # TODO: remove these and call super().__init__()\n        # super().__init__ calls add_param_group, which we've explicitly marked as not implemented.\n        # However, we need to ensure that all Optimizer member variables are created.\n        # pyre-ignore\n        self._optimizer_step_pre_hooks: Dict[int, Callable] = OrderedDict()\n        # pyre-ignore\n        self._optimizer_step_post_hooks: Dict[int, Callable] = OrderedDict()\n\n        # pyre-ignore\n        self.state: Mapping[Any, Any] = state\n        self.param_groups: Collection[Mapping[str, Any]] = param_groups\n        self.params = params\n        self.defaults: Dict[str, Any] = {\"_save_param_groups\": False}\n\n        params_set = set(params.values())\n        non_param_state_keys = [key for key in self.state if key not in params_set]\n        if len(non_param_state_keys) > 0:\n            raise ValueError(\n                \"All state keys must be params. The following keys are not: {}.\".format(\n                    non_param_state_keys\n                )\n            )\n\n    def state_dict(self) -> Dict[str, Any]:\n        \"\"\"\n        Returned state and param_groups will contain parameter keys\n        instead of parameter indices in torch.Optimizer.\n        This allows for advanced functionality like optimizer re-sharding to be implemented.\n\n        Can also handle classes and supported data structures that follow the PyTorch stateful\n        protocol.\n        \"\"\"\n\n        state = self.state\n        param_groups = self.param_groups\n        params = self.params\n        param_to_key = {param: key for key, param in params.items()}\n\n        ret_state = {}\n        for param, state_val in state.items():\n            if isinstance(state_val, dict):\n                ret_state[param_to_key[param]] = {}\n                for k, v in state_val.items():\n                    if hasattr(v, \"state_dict\") and callable(v.state_dict):\n                        ret_state[param_to_key[param]][k] = v.state_dict()\n                    else:\n                        ret_state[param_to_key[param]][k] = v\n            else:\n                ret_state[param_to_key[param]] = state_val\n\n        ret_groups = []\n        for group in param_groups:\n            param_keys = []\n            for param in group[\"params\"]:\n                param_keys.append(param_to_key[param])\n            ret_group = {\"params\": sorted(param_keys)}\n            for k, v in group.items():\n                if k != \"params\":\n                    ret_group[k] = deepcopy(v)\n            ret_groups.append(ret_group)\n\n        ret: Dict[str, object] = {\"state\": ret_state}\n        if self.defaults[\"_save_param_groups\"]:\n            ret[\"param_groups\"] = ret_groups\n        return ret\n\n    def post_load_state_dict(self) -> None:\n        pass\n\n    def load_state_dict(self, state_dict: Mapping[str, Any]) -> None:\n        \"\"\"\n        This implementation is much stricter than the one in torch.Optimizer:\n        it requires implementations to fully initialize their state during first optimization iteration,\n        and it prohibits loading an empty state into already initialized KeyedOptimizer and vise versa.\n\n        Because of introduced strictness it allows us to:\n            * do compatibility checks for state and param_groups, which improves usability\n            * avoid state duplication by directly copying into state tensors, e.g.\n              optimizer.step()  # make sure optimizer is initialized\n              sd = optimizer.state_dict()\n              load_checkpoint(sd)  # copy state directly into tensors, re-shard if needed\n              optimizer.load_state_dict(sd)  # replace param_groups\n        \"\"\"\n\n        new_state = state_dict[\"state\"]\n        state = self.state\n        params = self.params\n\n        # Load state\n        if len(state) != len(new_state):\n            raise ValueError(\n                f\"Different parameter count: {len(state)} vs {len(new_state)}\"\n            )\n        for param_key, param in params.items():\n            if param not in state:\n                continue\n            if param_key not in new_state:\n                raise ValueError(f\"Parameter {param_key} not found\")\n            if len(state[param]) != len(new_state[param_key]):\n                raise ValueError(\n                    f\"Different state size: {len(state[param])} vs {len(new_state[param_key])}\"\n                )\n            for state_key, state_val in state[param].items():\n                if state_key not in new_state[param_key]:\n                    raise ValueError(\n                        f\"State key {state_key} not found for param {param_key}\"\n                    )\n\n                new_state_val = new_state[param_key][state_key]\n                if isinstance(state_val, ShardedTensor):\n                    assert isinstance(new_state_val, ShardedTensor)\n                    num_shards = len(state_val.local_shards())\n                    num_new_shards = len(new_state_val.local_shards())\n                    if num_shards != num_new_shards:\n                        raise ValueError(\n                            f\"Different number of shards {num_shards} vs {num_new_shards} for {param_key}/{state_key}\"\n                        )\n                    for shard, new_shard in zip(\n                        state_val.local_shards(), new_state_val.local_shards()\n                    ):\n                        shard.tensor.detach().copy_(new_shard.tensor)\n                elif isinstance(state_val, torch.Tensor):\n                    assert isinstance(new_state_val, torch.Tensor)\n                    state_val.detach().copy_(new_state_val)\n                elif hasattr(state_val, \"load_state_dict\") and callable(\n                    state_val.load_state_dict\n                ):\n                    state_val.load_state_dict(new_state_val)\n                else:\n                    state[param][state_key] = deepcopy(new_state_val)\n\n        # Load param_groups.\n        if self.defaults[\"_save_param_groups\"]:\n            new_param_groups = state_dict[\"param_groups\"]\n            param_groups = self.param_groups\n\n            if len(param_groups) != len(new_param_groups):\n                raise ValueError(\n                    f\"Different param_groups count: {len(param_groups)} vs {len(new_param_groups)}\"\n                )\n            param_to_key = {param: key for key, param in params.items()}\n            group_map = {}\n            for group in param_groups:\n                param_keys = []\n                for param in group[\"params\"]:\n                    param_keys.append(param_to_key[param])\n                group_map[\"/\".join(sorted(param_keys))] = group\n            new_group_map = {}\n            for new_group in new_param_groups:\n                param_keys = []\n                for param_key in new_group[\"params\"]:\n                    param_keys.append(param_key)\n                new_group_map[\"/\".join(sorted(param_keys))] = new_group\n            for group_key, group in group_map.items():\n                if group_key not in new_group_map:\n                    raise ValueError(f\"Group {group_key} not found\")\n                new_group = new_group_map[group_key]\n                if len(group) != len(new_group):\n                    raise ValueError(\n                        f\"Different param_group size: {len(group)} vs {len(new_group)}\"\n                    )\n                for k in group:\n                    if k not in new_group:\n                        raise ValueError(\n                            f\"Group key {k} not found for group {group_key}\"\n                        )\n                    if k != \"params\":\n                        group[k] = deepcopy(new_group[k])\n\n        self.post_load_state_dict()\n\n    # pyre-ignore [2]\n    def add_param_group(self, param_group: Any) -> None:\n        raise NotImplementedError()\n\n    def init_state(\n        self,\n        sparse_grad_parameter_names: Optional[Set[str]] = None,\n    ) -> None:\n        \"\"\"\n        Runs a dummy optimizer step, which allows to initialize\n        optimizer state, which is typically lazy.\n        This allows us to do in-place loading of optimizer state from a checkpoint.\n        \"\"\"\n        for key, param in self.params.items():\n            if param.requires_grad:\n                t = torch.zeros_like(param)\n                if (\n                    sparse_grad_parameter_names is not None\n                    and key in sparse_grad_parameter_names\n                ):\n                    t = t.to_sparse()\n                param.grad = torch.autograd.Variable(t)\n        self.step(closure=None)\n\n    def save_param_groups(self, save: bool) -> None:\n        self.defaults[\"_save_param_groups\"] = save\n\n    def __getstate__(self) -> Dict[str, Any]:\n        return self.__dict__",
  "class CombinedOptimizer(KeyedOptimizer):\n    \"\"\"\n    Combines multiple KeyedOptimizers into one.\n\n    Meant to combine different optimizers for different submodules\n    \"\"\"\n\n    def __init__(\n        self, optims: List[Union[KeyedOptimizer, Tuple[str, KeyedOptimizer]]]\n    ) -> None:\n        self.defaults: Dict[str, Any] = {}\n        # Append empty optimizer key if not passed.\n        self._optims: List[Tuple[str, KeyedOptimizer]] = []\n        for key_value in optims:\n            if isinstance(key_value, KeyedOptimizer):\n                key_value = (\"\", key_value)\n            self._optims.append(key_value)\n\n        all_keys: Set[str] = set()\n        self.defaults[\"_save_param_groups\"] = (\n            False\n            if len(self._optims) == 0\n            else self._optims[0][1].defaults[\"_save_param_groups\"]\n        )\n        for opt_key, opt in self._optims:\n            assert (\n                self.defaults[\"_save_param_groups\"]\n                == opt.defaults[\"_save_param_groups\"]\n            )\n\n            for param_key in opt.params.keys():\n                new_param = CombinedOptimizer.prepend_opt_key(param_key, opt_key)\n                if new_param in all_keys:\n                    raise ValueError(f\"Duplicate param key {new_param}\")\n                all_keys.add(new_param)\n\n    def __repr__(self) -> str:\n        ret = []\n        for key, opt in self._optims:\n            ret.append(f\"{key}: {opt.__repr__()}\")\n        return \",\".join(ret)\n\n    def zero_grad(self, set_to_none: bool = False) -> None:\n        for _, opt in self._optims:\n            opt.zero_grad(set_to_none=set_to_none)\n\n    # pyre-ignore [2]\n    def step(self, closure: Any = None) -> None:\n        for _, opt in self._optims:\n            opt.step(closure=closure)\n\n    @property\n    def optimizers(self) -> List[Tuple[str, KeyedOptimizer]]:\n        return self._optims\n\n    @staticmethod\n    def prepend_opt_key(name: str, opt_key: str) -> str:\n        if not name:\n            return opt_key\n        return opt_key + (\".\" if opt_key else \"\") + name\n\n    @property\n    def param_groups(self) -> Collection[Mapping[str, Any]]:\n        return [\n            param_group for _, opt in self._optims for param_group in opt.param_groups\n        ]\n\n    @property\n    def params(self) -> Mapping[str, Union[torch.Tensor, ShardedTensor]]:\n        ret = {}\n        for opt_key, opt in self._optims:\n            for param_key, param in opt.params.items():\n                ret[CombinedOptimizer.prepend_opt_key(param_key, opt_key)] = param\n        return ret\n\n    @property\n    # pyre-ignore [3]\n    def state(self) -> Mapping[torch.Tensor, Any]:\n        ret = {}\n        for _, opt in self._optims:\n            for param, state in opt.state.items():\n                ret[param] = state\n        return ret\n\n    def post_load_state_dict(self) -> None:\n        for _, opt in self._optims:\n            opt.post_load_state_dict()\n\n    def save_param_groups(self, save: bool) -> None:\n        self.defaults[\"_save_param_groups\"] = save\n        for _, opt in self._optims:\n            opt.save_param_groups(save)",
  "class KeyedOptimizerWrapper(KeyedOptimizer):\n    \"\"\"\n    Takes a dict of parameters and exposes state_dict by parameter key.\n\n    Convenience wrapper to take in optim_factory callable to create KeyedOptimizer\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Mapping[str, Union[torch.Tensor, ShardedTensor]],\n        optim_factory: OptimizerFactory,\n    ) -> None:\n        self._optimizer: optim.Optimizer = optim_factory(list(params.values()))\n        super().__init__(params, self._optimizer.state, self._optimizer.param_groups)\n\n    def zero_grad(self, set_to_none: bool = False) -> None:\n        self._optimizer.zero_grad()\n\n    # pyre-ignore [2]\n    def step(self, closure: Any = None) -> None:\n        self._optimizer.step(closure=closure)",
  "class OptimizerWrapper(KeyedOptimizer):\n    \"\"\"\n    Wrapper which takes in a KeyedOptimizer and is a KeyedOptimizer\n\n    Subclass for Optimizers like GradientClippingOptimizer and WarmupOptimizer\n    \"\"\"\n\n    def __init__(self, optimizer: KeyedOptimizer) -> None:\n        self._optimizer = optimizer\n        self.params: Mapping[str, Union[torch.Tensor, ShardedTensor]] = optimizer.params\n        # pyre-ignore [4]\n        self.state: Mapping[Any, Any] = optimizer.state\n        self.param_groups: Collection[Mapping[str, Any]] = optimizer.param_groups\n        self.defaults: Dict[str, Any] = {\"_save_param_groups\": False}\n\n    def __repr__(self) -> str:\n        return self._optimizer.__repr__()\n\n    def zero_grad(self, set_to_none: bool = False) -> None:\n        self._optimizer.zero_grad(set_to_none=set_to_none)\n\n    # pyre-ignore [2]\n    def step(self, closure: Any = None) -> None:\n        self._optimizer.step(closure=closure)\n\n    # pyre-ignore [2]\n    def add_param_group(self, param_group: Any) -> None:\n        raise NotImplementedError()\n\n    def state_dict(self) -> Dict[str, Any]:\n        return self._optimizer.state_dict()\n\n    def post_load_state_dict(self) -> None:\n        self._optimizer.post_load_state_dict()\n\n    def load_state_dict(self, state_dict: Mapping[str, Any]) -> None:\n        self._optimizer.load_state_dict(state_dict)\n        # Reassign references because self._optimizer receives new state and param_group\n        # references after load_state_dict.\n        self.state = self._optimizer.state\n        self.param_groups = self._optimizer.param_groups\n\n        self.post_load_state_dict()\n\n    def save_param_groups(self, save: bool) -> None:\n        self._optimizer.save_param_groups(save)",
  "def __init__(\n        self,\n        params: Mapping[str, Union[torch.Tensor, ShardedTensor]],\n        # pyre-ignore [2]\n        state: Mapping[Any, Any],\n        param_groups: Collection[Mapping[str, Any]],\n    ) -> None:\n        torch._C._log_api_usage_once(f\"torchrec.optim.{self.__class__.__name__}\")\n\n        # TODO: remove these and call super().__init__()\n        # super().__init__ calls add_param_group, which we've explicitly marked as not implemented.\n        # However, we need to ensure that all Optimizer member variables are created.\n        # pyre-ignore\n        self._optimizer_step_pre_hooks: Dict[int, Callable] = OrderedDict()\n        # pyre-ignore\n        self._optimizer_step_post_hooks: Dict[int, Callable] = OrderedDict()\n\n        # pyre-ignore\n        self.state: Mapping[Any, Any] = state\n        self.param_groups: Collection[Mapping[str, Any]] = param_groups\n        self.params = params\n        self.defaults: Dict[str, Any] = {\"_save_param_groups\": False}\n\n        params_set = set(params.values())\n        non_param_state_keys = [key for key in self.state if key not in params_set]\n        if len(non_param_state_keys) > 0:\n            raise ValueError(\n                \"All state keys must be params. The following keys are not: {}.\".format(\n                    non_param_state_keys\n                )\n            )",
  "def state_dict(self) -> Dict[str, Any]:\n        \"\"\"\n        Returned state and param_groups will contain parameter keys\n        instead of parameter indices in torch.Optimizer.\n        This allows for advanced functionality like optimizer re-sharding to be implemented.\n\n        Can also handle classes and supported data structures that follow the PyTorch stateful\n        protocol.\n        \"\"\"\n\n        state = self.state\n        param_groups = self.param_groups\n        params = self.params\n        param_to_key = {param: key for key, param in params.items()}\n\n        ret_state = {}\n        for param, state_val in state.items():\n            if isinstance(state_val, dict):\n                ret_state[param_to_key[param]] = {}\n                for k, v in state_val.items():\n                    if hasattr(v, \"state_dict\") and callable(v.state_dict):\n                        ret_state[param_to_key[param]][k] = v.state_dict()\n                    else:\n                        ret_state[param_to_key[param]][k] = v\n            else:\n                ret_state[param_to_key[param]] = state_val\n\n        ret_groups = []\n        for group in param_groups:\n            param_keys = []\n            for param in group[\"params\"]:\n                param_keys.append(param_to_key[param])\n            ret_group = {\"params\": sorted(param_keys)}\n            for k, v in group.items():\n                if k != \"params\":\n                    ret_group[k] = deepcopy(v)\n            ret_groups.append(ret_group)\n\n        ret: Dict[str, object] = {\"state\": ret_state}\n        if self.defaults[\"_save_param_groups\"]:\n            ret[\"param_groups\"] = ret_groups\n        return ret",
  "def post_load_state_dict(self) -> None:\n        pass",
  "def load_state_dict(self, state_dict: Mapping[str, Any]) -> None:\n        \"\"\"\n        This implementation is much stricter than the one in torch.Optimizer:\n        it requires implementations to fully initialize their state during first optimization iteration,\n        and it prohibits loading an empty state into already initialized KeyedOptimizer and vise versa.\n\n        Because of introduced strictness it allows us to:\n            * do compatibility checks for state and param_groups, which improves usability\n            * avoid state duplication by directly copying into state tensors, e.g.\n              optimizer.step()  # make sure optimizer is initialized\n              sd = optimizer.state_dict()\n              load_checkpoint(sd)  # copy state directly into tensors, re-shard if needed\n              optimizer.load_state_dict(sd)  # replace param_groups\n        \"\"\"\n\n        new_state = state_dict[\"state\"]\n        state = self.state\n        params = self.params\n\n        # Load state\n        if len(state) != len(new_state):\n            raise ValueError(\n                f\"Different parameter count: {len(state)} vs {len(new_state)}\"\n            )\n        for param_key, param in params.items():\n            if param not in state:\n                continue\n            if param_key not in new_state:\n                raise ValueError(f\"Parameter {param_key} not found\")\n            if len(state[param]) != len(new_state[param_key]):\n                raise ValueError(\n                    f\"Different state size: {len(state[param])} vs {len(new_state[param_key])}\"\n                )\n            for state_key, state_val in state[param].items():\n                if state_key not in new_state[param_key]:\n                    raise ValueError(\n                        f\"State key {state_key} not found for param {param_key}\"\n                    )\n\n                new_state_val = new_state[param_key][state_key]\n                if isinstance(state_val, ShardedTensor):\n                    assert isinstance(new_state_val, ShardedTensor)\n                    num_shards = len(state_val.local_shards())\n                    num_new_shards = len(new_state_val.local_shards())\n                    if num_shards != num_new_shards:\n                        raise ValueError(\n                            f\"Different number of shards {num_shards} vs {num_new_shards} for {param_key}/{state_key}\"\n                        )\n                    for shard, new_shard in zip(\n                        state_val.local_shards(), new_state_val.local_shards()\n                    ):\n                        shard.tensor.detach().copy_(new_shard.tensor)\n                elif isinstance(state_val, torch.Tensor):\n                    assert isinstance(new_state_val, torch.Tensor)\n                    state_val.detach().copy_(new_state_val)\n                elif hasattr(state_val, \"load_state_dict\") and callable(\n                    state_val.load_state_dict\n                ):\n                    state_val.load_state_dict(new_state_val)\n                else:\n                    state[param][state_key] = deepcopy(new_state_val)\n\n        # Load param_groups.\n        if self.defaults[\"_save_param_groups\"]:\n            new_param_groups = state_dict[\"param_groups\"]\n            param_groups = self.param_groups\n\n            if len(param_groups) != len(new_param_groups):\n                raise ValueError(\n                    f\"Different param_groups count: {len(param_groups)} vs {len(new_param_groups)}\"\n                )\n            param_to_key = {param: key for key, param in params.items()}\n            group_map = {}\n            for group in param_groups:\n                param_keys = []\n                for param in group[\"params\"]:\n                    param_keys.append(param_to_key[param])\n                group_map[\"/\".join(sorted(param_keys))] = group\n            new_group_map = {}\n            for new_group in new_param_groups:\n                param_keys = []\n                for param_key in new_group[\"params\"]:\n                    param_keys.append(param_key)\n                new_group_map[\"/\".join(sorted(param_keys))] = new_group\n            for group_key, group in group_map.items():\n                if group_key not in new_group_map:\n                    raise ValueError(f\"Group {group_key} not found\")\n                new_group = new_group_map[group_key]\n                if len(group) != len(new_group):\n                    raise ValueError(\n                        f\"Different param_group size: {len(group)} vs {len(new_group)}\"\n                    )\n                for k in group:\n                    if k not in new_group:\n                        raise ValueError(\n                            f\"Group key {k} not found for group {group_key}\"\n                        )\n                    if k != \"params\":\n                        group[k] = deepcopy(new_group[k])\n\n        self.post_load_state_dict()",
  "def add_param_group(self, param_group: Any) -> None:\n        raise NotImplementedError()",
  "def init_state(\n        self,\n        sparse_grad_parameter_names: Optional[Set[str]] = None,\n    ) -> None:\n        \"\"\"\n        Runs a dummy optimizer step, which allows to initialize\n        optimizer state, which is typically lazy.\n        This allows us to do in-place loading of optimizer state from a checkpoint.\n        \"\"\"\n        for key, param in self.params.items():\n            if param.requires_grad:\n                t = torch.zeros_like(param)\n                if (\n                    sparse_grad_parameter_names is not None\n                    and key in sparse_grad_parameter_names\n                ):\n                    t = t.to_sparse()\n                param.grad = torch.autograd.Variable(t)\n        self.step(closure=None)",
  "def save_param_groups(self, save: bool) -> None:\n        self.defaults[\"_save_param_groups\"] = save",
  "def __getstate__(self) -> Dict[str, Any]:\n        return self.__dict__",
  "def __init__(\n        self, optims: List[Union[KeyedOptimizer, Tuple[str, KeyedOptimizer]]]\n    ) -> None:\n        self.defaults: Dict[str, Any] = {}\n        # Append empty optimizer key if not passed.\n        self._optims: List[Tuple[str, KeyedOptimizer]] = []\n        for key_value in optims:\n            if isinstance(key_value, KeyedOptimizer):\n                key_value = (\"\", key_value)\n            self._optims.append(key_value)\n\n        all_keys: Set[str] = set()\n        self.defaults[\"_save_param_groups\"] = (\n            False\n            if len(self._optims) == 0\n            else self._optims[0][1].defaults[\"_save_param_groups\"]\n        )\n        for opt_key, opt in self._optims:\n            assert (\n                self.defaults[\"_save_param_groups\"]\n                == opt.defaults[\"_save_param_groups\"]\n            )\n\n            for param_key in opt.params.keys():\n                new_param = CombinedOptimizer.prepend_opt_key(param_key, opt_key)\n                if new_param in all_keys:\n                    raise ValueError(f\"Duplicate param key {new_param}\")\n                all_keys.add(new_param)",
  "def __repr__(self) -> str:\n        ret = []\n        for key, opt in self._optims:\n            ret.append(f\"{key}: {opt.__repr__()}\")\n        return \",\".join(ret)",
  "def zero_grad(self, set_to_none: bool = False) -> None:\n        for _, opt in self._optims:\n            opt.zero_grad(set_to_none=set_to_none)",
  "def step(self, closure: Any = None) -> None:\n        for _, opt in self._optims:\n            opt.step(closure=closure)",
  "def optimizers(self) -> List[Tuple[str, KeyedOptimizer]]:\n        return self._optims",
  "def prepend_opt_key(name: str, opt_key: str) -> str:\n        if not name:\n            return opt_key\n        return opt_key + (\".\" if opt_key else \"\") + name",
  "def param_groups(self) -> Collection[Mapping[str, Any]]:\n        return [\n            param_group for _, opt in self._optims for param_group in opt.param_groups\n        ]",
  "def params(self) -> Mapping[str, Union[torch.Tensor, ShardedTensor]]:\n        ret = {}\n        for opt_key, opt in self._optims:\n            for param_key, param in opt.params.items():\n                ret[CombinedOptimizer.prepend_opt_key(param_key, opt_key)] = param\n        return ret",
  "def state(self) -> Mapping[torch.Tensor, Any]:\n        ret = {}\n        for _, opt in self._optims:\n            for param, state in opt.state.items():\n                ret[param] = state\n        return ret",
  "def post_load_state_dict(self) -> None:\n        for _, opt in self._optims:\n            opt.post_load_state_dict()",
  "def save_param_groups(self, save: bool) -> None:\n        self.defaults[\"_save_param_groups\"] = save\n        for _, opt in self._optims:\n            opt.save_param_groups(save)",
  "def __init__(\n        self,\n        params: Mapping[str, Union[torch.Tensor, ShardedTensor]],\n        optim_factory: OptimizerFactory,\n    ) -> None:\n        self._optimizer: optim.Optimizer = optim_factory(list(params.values()))\n        super().__init__(params, self._optimizer.state, self._optimizer.param_groups)",
  "def zero_grad(self, set_to_none: bool = False) -> None:\n        self._optimizer.zero_grad()",
  "def step(self, closure: Any = None) -> None:\n        self._optimizer.step(closure=closure)",
  "def __init__(self, optimizer: KeyedOptimizer) -> None:\n        self._optimizer = optimizer\n        self.params: Mapping[str, Union[torch.Tensor, ShardedTensor]] = optimizer.params\n        # pyre-ignore [4]\n        self.state: Mapping[Any, Any] = optimizer.state\n        self.param_groups: Collection[Mapping[str, Any]] = optimizer.param_groups\n        self.defaults: Dict[str, Any] = {\"_save_param_groups\": False}",
  "def __repr__(self) -> str:\n        return self._optimizer.__repr__()",
  "def zero_grad(self, set_to_none: bool = False) -> None:\n        self._optimizer.zero_grad(set_to_none=set_to_none)",
  "def step(self, closure: Any = None) -> None:\n        self._optimizer.step(closure=closure)",
  "def add_param_group(self, param_group: Any) -> None:\n        raise NotImplementedError()",
  "def state_dict(self) -> Dict[str, Any]:\n        return self._optimizer.state_dict()",
  "def post_load_state_dict(self) -> None:\n        self._optimizer.post_load_state_dict()",
  "def load_state_dict(self, state_dict: Mapping[str, Any]) -> None:\n        self._optimizer.load_state_dict(state_dict)\n        # Reassign references because self._optimizer receives new state and param_group\n        # references after load_state_dict.\n        self.state = self._optimizer.state\n        self.param_groups = self._optimizer.param_groups\n\n        self.post_load_state_dict()",
  "def save_param_groups(self, save: bool) -> None:\n        self._optimizer.save_param_groups(save)",
  "class RowWiseAdagrad(Optimizer):\n    r\"\"\"Implements Row wise Adagrad algorithm. This is an extension of the Adagrad algorithm\n    https://github.com/pytorch/pytorch/blob/master/torch/optim/adagrad.py, for use with\n    EmbeddingBag parameters, where we want the adaptive learning rate to be the same within an\n    embedding row. Since we only need to store state for an embedding row, rather than every single\n    parameter, we can have drastic memory savings (factor of embedding_dim).\n\n    Note that this implementation does not currently support sparse gradients.\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-2)\n        lr_decay (float, optional): learning rate decay (default: 0)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-10)\n        maximize (bool, optional): maximize the params based on the objective, instead of\n            minimizing (default: False)\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        lr: float = 1e-2,\n        lr_decay: float = 0.0,\n        weight_decay: float = 0.0,\n        initial_accumulator_value: float = 0.0,\n        eps: float = 1e-10,\n        *,\n        maximize: bool = False,\n        # pyre-ignore\n        **unused,\n    ) -> None:\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= lr_decay:\n            raise ValueError(\"Invalid lr_decay value: {}\".format(lr_decay))\n        if not 0.0 <= weight_decay:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if not 0.0 <= initial_accumulator_value:\n            raise ValueError(\n                \"Invalid initial_accumulator_value value: {}\".format(\n                    initial_accumulator_value\n                )\n            )\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n\n        defaults = dict(\n            lr=lr,\n            lr_decay=lr_decay,\n            eps=eps,\n            weight_decay=weight_decay,\n            initial_accumulator_value=initial_accumulator_value,\n            maximize=maximize,\n        )\n        super().__init__(params, defaults)\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                state = self.state[p]\n                state[\"step\"] = torch.tensor(0.0)\n                init_value = (\n                    complex(initial_accumulator_value, initial_accumulator_value)\n                    if torch.is_complex(p)\n                    else initial_accumulator_value\n                )\n                state[\"sum\"] = (\n                    # pyre-fixme[28]: Unexpected keyword argument `axis`.\n                    torch.full_like(p, init_value, memory_format=torch.preserve_format)\n                    .mean(axis=1)\n                    .view(-1, 1)\n                )\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\"maximize\", False)\n\n        state_values = list(self.state.values())\n        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(\n            state_values[0][\"step\"]\n        )\n        if not step_is_tensor:\n            for s in state_values:\n                s[\"step\"] = torch.tensor(float(s[\"step\"]))\n\n    def share_memory(self) -> None:\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                state = self.state[p]\n                state[\"sum\"].share_memory_()\n\n    @torch.no_grad()\n    # pyre-ignore\n    def step(self, closure=None) -> torch.Tensor:\n        \"\"\"Performs a single optimization step.\n        Args:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            params_with_grad = []\n            grads = []\n            state_sums = []\n            state_steps = []\n\n            for p in group[\"params\"]:\n                if p.grad is not None:\n                    params_with_grad.append(p)\n                    grads.append(p.grad)\n                    state = self.state[p]\n                    state_sums.append(state[\"sum\"])\n                    state_steps.append(state[\"step\"])\n\n            adagrad(\n                params_with_grad,\n                grads,\n                state_sums,\n                state_steps,\n                lr=group[\"lr\"],\n                weight_decay=group[\"weight_decay\"],\n                lr_decay=group[\"lr_decay\"],\n                eps=group[\"eps\"],\n                maximize=group[\"maximize\"],\n            )\n\n        return loss",
  "def adagrad(\n    params: List[Tensor],\n    grads: List[Tensor],\n    state_sums: List[Tensor],\n    state_steps: List[Tensor],\n    # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n    # setting these as kwargs for now as functional API is compiled by torch/distributed/optim\n    *,\n    lr: float,\n    weight_decay: float,\n    lr_decay: float,\n    eps: float,\n    maximize: bool,\n) -> None:\n    r\"\"\"Functional API that performs Adagrad algorithm computation.\n    See :class:`~torch.optim.Adagrad` for details.\n    \"\"\"\n\n    if not all([isinstance(t, torch.Tensor) for t in state_steps]):\n        raise RuntimeError(\n            \"API has changed, `state_steps` argument must contain a list of singleton tensors\"\n        )\n\n    _single_tensor_adagrad(\n        params,\n        grads,\n        state_sums,\n        state_steps,\n        lr=lr,\n        weight_decay=weight_decay,\n        lr_decay=lr_decay,\n        eps=eps,\n        maximize=maximize,\n    )",
  "def _single_tensor_adagrad(\n    params: List[Tensor],\n    grads: List[Tensor],\n    state_sums: List[Tensor],\n    state_steps: List[Tensor],\n    *,\n    lr: float,\n    weight_decay: float,\n    lr_decay: float,\n    eps: float,\n    maximize: bool,\n) -> None:\n\n    for (param, grad, state_sum, step_t) in zip(params, grads, state_sums, state_steps):\n        if grad.is_sparse:\n            raise RuntimeError(\"RowWise adagrad cannot be used with sparse gradients\")\n        # update step\n        step_t += 1\n        step = step_t.item()\n        grad = grad if not maximize else -grad\n\n        row_wise_grad = grad.mean(axis=1).view(-1, 1)\n        if weight_decay != 0:\n\n            grad = grad.add(param, alpha=weight_decay)\n            row_wise_grad = grad.add(param, alpha=weight_decay)\n\n        clr = lr / (1 + (step - 1) * lr_decay)\n\n        state_sum.addcmul_(row_wise_grad, row_wise_grad, value=1)\n        std = state_sum.sqrt().add_(eps)\n        param.addcdiv_(row_wise_grad, std, value=-clr)",
  "def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        lr: float = 1e-2,\n        lr_decay: float = 0.0,\n        weight_decay: float = 0.0,\n        initial_accumulator_value: float = 0.0,\n        eps: float = 1e-10,\n        *,\n        maximize: bool = False,\n        # pyre-ignore\n        **unused,\n    ) -> None:\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= lr_decay:\n            raise ValueError(\"Invalid lr_decay value: {}\".format(lr_decay))\n        if not 0.0 <= weight_decay:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if not 0.0 <= initial_accumulator_value:\n            raise ValueError(\n                \"Invalid initial_accumulator_value value: {}\".format(\n                    initial_accumulator_value\n                )\n            )\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n\n        defaults = dict(\n            lr=lr,\n            lr_decay=lr_decay,\n            eps=eps,\n            weight_decay=weight_decay,\n            initial_accumulator_value=initial_accumulator_value,\n            maximize=maximize,\n        )\n        super().__init__(params, defaults)\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                state = self.state[p]\n                state[\"step\"] = torch.tensor(0.0)\n                init_value = (\n                    complex(initial_accumulator_value, initial_accumulator_value)\n                    if torch.is_complex(p)\n                    else initial_accumulator_value\n                )\n                state[\"sum\"] = (\n                    # pyre-fixme[28]: Unexpected keyword argument `axis`.\n                    torch.full_like(p, init_value, memory_format=torch.preserve_format)\n                    .mean(axis=1)\n                    .view(-1, 1)\n                )",
  "def __setstate__(self, state: Dict[str, Any]) -> None:\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\"maximize\", False)\n\n        state_values = list(self.state.values())\n        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(\n            state_values[0][\"step\"]\n        )\n        if not step_is_tensor:\n            for s in state_values:\n                s[\"step\"] = torch.tensor(float(s[\"step\"]))",
  "def share_memory(self) -> None:\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                state = self.state[p]\n                state[\"sum\"].share_memory_()",
  "def step(self, closure=None) -> torch.Tensor:\n        \"\"\"Performs a single optimization step.\n        Args:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            params_with_grad = []\n            grads = []\n            state_sums = []\n            state_steps = []\n\n            for p in group[\"params\"]:\n                if p.grad is not None:\n                    params_with_grad.append(p)\n                    grads.append(p.grad)\n                    state = self.state[p]\n                    state_sums.append(state[\"sum\"])\n                    state_steps.append(state[\"step\"])\n\n            adagrad(\n                params_with_grad,\n                grads,\n                state_sums,\n                state_steps,\n                lr=group[\"lr\"],\n                weight_decay=group[\"weight_decay\"],\n                lr_decay=group[\"lr_decay\"],\n                eps=group[\"eps\"],\n                maximize=group[\"maximize\"],\n            )\n\n        return loss",
  "class GradientClipping(Enum):\n    NORM = \"norm\"\n    VALUE = \"value\"\n    NONE = \"none\"",
  "class GradientClippingOptimizer(OptimizerWrapper):\n    \"\"\"\n    Clips gradients before doing optimization step.\n\n    Args:\n        optimizer (KeyedOptimizer): optimizer to wrap\n        clipping (GradientClipping): how to clip gradients\n        max_gradient (float): max value for clipping\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer: KeyedOptimizer,\n        clipping: GradientClipping = GradientClipping.NONE,\n        max_gradient: float = 0.1,\n    ) -> None:\n        super().__init__(optimizer)\n        self._clipping = clipping\n        self._max_gradient = max_gradient\n\n        self._params: List[torch.Tensor] = []\n        for param_group in self.param_groups:\n            self._params += list(param_group[\"params\"])\n\n    # pyre-ignore [2]\n    def step(self, closure: Any = None) -> None:\n        if self._clipping == GradientClipping.NORM:\n            torch.nn.utils.clip_grad_norm_(self._params, self._max_gradient)\n        elif self._clipping == GradientClipping.VALUE:\n            torch.nn.utils.clip_grad_value_(self._params, self._max_gradient)\n\n        super().step(closure)",
  "def __init__(\n        self,\n        optimizer: KeyedOptimizer,\n        clipping: GradientClipping = GradientClipping.NONE,\n        max_gradient: float = 0.1,\n    ) -> None:\n        super().__init__(optimizer)\n        self._clipping = clipping\n        self._max_gradient = max_gradient\n\n        self._params: List[torch.Tensor] = []\n        for param_group in self.param_groups:\n            self._params += list(param_group[\"params\"])",
  "def step(self, closure: Any = None) -> None:\n        if self._clipping == GradientClipping.NORM:\n            torch.nn.utils.clip_grad_norm_(self._params, self._max_gradient)\n        elif self._clipping == GradientClipping.VALUE:\n            torch.nn.utils.clip_grad_value_(self._params, self._max_gradient)\n\n        super().step(closure)",
  "def apply_optimizer_in_backward(\n    optimizer_class: Type[torch.optim.Optimizer],\n    params: Iterable[torch.nn.Parameter],\n    optimizer_kwargs: Dict[str, Any],\n) -> None:\n    \"\"\"\n    NOTE: This API is deprecated. Please use Pytorch Distributed's _apply_optimizer_in_backward instead.\n\n    Upon backwards(), parameters will fire the corresponding optimizer\n    Each parameter will have the optimizer_class and optimizer_kwargs attached to\n    _optimizer and _optimizer_kwargs.\n\n    Note - gradients for these parameters will be set to None after backwards().\n    This means that any other (non applied) optimizer over this parameter will be\n    a no-op.\n\n    Args:\n        optimizer_class: Type[torch.optim.Optimizer]: Optimizer to apply to parameter\n        params: Iterator[nn.Parameter]: parameters to apply optimizer state to\n        optimizer_kwargs: Dict[str, Any]: kwargs to pass to optimizer constructor\n\n    Example::\n        params_generator = model.parameters()\n        param_1 = next(params_generator)\n        param_2 = list(params_generator)\n\n        apply_optimizer_in_backward(torch.optim.SGD, [param_1], {\"lr\": .02})\n        apply_optimizer_in_backward(torch.optim.Adam, param_2, {\"lr\": .04})\n\n        print(param_1._optimizer, param_1._optimizer_kwargs)\n        >> torch.optim.SGD, {\"lr\": .02}\n    \"\"\"\n\n    from torch.distributed.optim import _apply_optimizer_in_backward\n\n    warn(\n        \"This API is deprecated. Please use Pytorch Distributed's _apply_optimizer_in_backward API instead.\",\n        DeprecationWarning,\n    )\n    _apply_optimizer_in_backward(\n        optimizer_class=optimizer_class,\n        params=params,\n        optimizer_kwargs=optimizer_kwargs,\n    )",
  "def fake_range():\n    return torch._C._jit_tree_views.SourceRangeFactory(\"\", None, 0, 0).make_raw_range(\n        0, 1\n    )",
  "def dmp_fx_trace_forward(  # noqa: C901\n    # pyre-ignore\n    dmp,\n    tracer: torch.fx.Tracer,\n):\n    func = dmp._dmp_wrapped_module.forward\n    sign: inspect.Signature = inspect.signature(func)\n\n    module_to_type_str: Dict[str, Set[str]] = {}\n\n    def add_if_missing(module: str, type_str: str) -> None:\n        if module not in module_to_type_str:\n            _set = set()\n            _set.add(type_str)\n            module_to_type_str[module] = _set\n        else:\n            s = module_to_type_str[module]\n            if type_str not in s:\n                s.add(type_str)\n\n    def torch_no_import(t: torch.Type) -> bool:\n        return isinstance(\n            t, (torch.FloatType, torch.IntType, torch.ComplexType, torch.StringType)\n        )\n\n    def torch_typing(t: torch.Type) -> bool:\n        return isinstance(\n            t,\n            (\n                torch.TupleType,\n                torch.ListType,\n                torch.DictType,\n                torch.OptionalType,\n                torch.AnyType,\n            ),\n        )\n\n    exec_imports = []\n    args_call = \", \".join([f\"{p.name}\" for p in sign.parameters.values()])\n\n    types = []\n    try:\n        args_decls: List[str] = []\n        for p in sign.parameters.values():\n            pann = p.annotation\n\n            ptype = torch.jit.annotations.try_ann_to_type(pann, fake_range())\n            types.append(ptype)\n            args_decls.append(f\"{p.name}: {ptype}\")\n\n        while len(types) > 0:\n            t = types.pop()\n            if torch_no_import(t):\n                continue\n\n            t_base_name = f\"{t}\".split(\"[\")[0]\n            if torch_typing(t):\n                add_if_missing(\"typing\", t_base_name)\n            else:\n                if hasattr(t, \"__module__\") and not torch_no_import(t):\n                    m = t.__module__\n                    add_if_missing(f\"{m}\", f\"{t}\".split(\"[\")[0])\n\n            if hasattr(t, \"containedTypes\"):\n                contained_types = getattr(t, \"containedTypes\", None)()\n                for ctype in contained_types:\n                    types.append(ctype)\n\n            if hasattr(t, \"getElementType\"):\n                el_type = getattr(t, \"getElementType\", None)()\n\n        args_decl = \", \".join(args_decls)\n\n        for m, s in module_to_type_str.items():\n            ts = \", \".join(s)\n            exec_imports.append(f\"from {m} import {ts}\")\n    except Exception as e:\n        print(f\"Exception:{e}\")\n        # Catching here if source is not available to proceed hoping that jit will infer correct types without annotations.\n        # Often it fails here when can not access to dataclass generated __init__\n        args_decl = args_call\n\n    exec_def_fn_name = \"__fx_forward\"\n    exec_dmp_wrapper_local_name = \"_dmp_wrapped_module_local\"\n    _dmp_wrapped_module_local = dmp\n    locals_dict = locals()\n    exec_def = f\"def {exec_def_fn_name}({args_decl}):\\n    return {exec_dmp_wrapper_local_name}({args_call})\"\n\n    exec_imports_str = \"\\n\".join(exec_imports)\n    pycode = f\"{exec_imports_str}\\n{exec_def}\"\n\n    exec(pycode, locals_dict)  # noqa: P204  Allow use of exec\n\n    wrapper = locals_dict[exec_def_fn_name]\n    wrapper.__signature__ = sign\n\n    return wrapper",
  "def _fx_marker(s: str, any_proxy_unused: Any) -> None:\n    pass",
  "def fx_marker(s: str, any_proxy_unused: Any) -> None:\n    if is_fx_tracing():\n        _fx_marker(s, any_proxy_unused)",
  "def is_marker_node(node: torch.fx.Node, marker_name: str) -> bool:\n    # bool() syntax for pyre\n    return bool(\n        node.op == \"call_function\"\n        and node.target == _fx_marker\n        and isinstance(node.args[0], str)\n        and node.args[0] == marker_name\n    )",
  "def assert_fx_safe(condition: bool, message: str) -> None:\n    if not is_fx_tracing():\n        assert condition, message",
  "def add_if_missing(module: str, type_str: str) -> None:\n        if module not in module_to_type_str:\n            _set = set()\n            _set.add(type_str)\n            module_to_type_str[module] = _set\n        else:\n            s = module_to_type_str[module]\n            if type_str not in s:\n                s.add(type_str)",
  "def torch_no_import(t: torch.Type) -> bool:\n        return isinstance(\n            t, (torch.FloatType, torch.IntType, torch.ComplexType, torch.StringType)\n        )",
  "def torch_typing(t: torch.Type) -> bool:\n        return isinstance(\n            t,\n            (\n                torch.TupleType,\n                torch.ListType,\n                torch.DictType,\n                torch.OptionalType,\n                torch.AnyType,\n            ),\n        )",
  "def is_fx_tracing() -> bool:\n    return _is_fx_tracing_flag",
  "class Tracer(torch.fx.Tracer):\n    \"\"\"\n    Custom FX tracer for torchrec\n\n    See `Torch.FX documentation <https://pytorch.org/docs/stable/fx.html>`_\n\n    We create a custom FX tracer to trace torchrec based models. The custom tracer\n    handles python generic types (i.e. NoWait[T], Awaitable[T]) and lower it to\n    TorchScript if needed\n    \"\"\"\n\n    def __init__(self, leaf_modules: Optional[List[str]] = None) -> None:\n        super().__init__()\n        self._leaf_modules: List[str] = leaf_modules if leaf_modules is not None else []\n\n    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:\n        \"\"\"\n        Override FX definition to include quantized embedding bags\n        \"\"\"\n        if type(m).__name__ in self._leaf_modules:\n            return True\n        return super().is_leaf_module(m, module_qualified_name)\n\n    @compatibility(is_backward_compatible=True)\n    def trace(\n        self,\n        # pyre-ignore[2]: Missing parameter annotation [2]: Parameter `root` must have a type that does not contain `Any`\n        root: Union[torch.nn.Module, Callable[..., Any]],\n        concrete_args: Optional[Dict[str, Any]] = None,\n    ) -> Graph:\n        global _is_fx_tracing_flag\n        old_is_fx_tracing_flag = _is_fx_tracing_flag\n        _is_fx_tracing_flag = True\n\n        try:\n            # TODO(ivankobzarev): support DMP not only on the root level\n            from torchrec.distributed.model_parallel import DistributedModelParallel\n\n            if isinstance(root, DistributedModelParallel):\n                # In the case where the module is wrapped in DMP, you need to replace DMP's forward\n                # call with a new signature, one with explicit args, because fx can't handle variable args.\n                # Furthermore, we need to provide the `fn_root` argument because when tracing a function,\n                # fx uses an empty module as the root (unless one is explicitly provided), which leads to\n                # issues with path_of_module and named_buffers.\n\n                # TODO(shababayub): This can be removed if we either stop supporting dmp wrapping\n                # for fx trace or strip dmp name in named_modules path (much like named_buffers).\n                if isinstance(root, torch.nn.Module):\n                    for prefix, module in root.named_modules():\n                        # TODO(T140754678): Remove this workaround to _fx_path\n                        module._fx_path = prefix\n\n                dmp = root\n                graph = super().trace(\n                    root=dmp_fx_trace_forward(dmp, self),\n                    concrete_args=concrete_args,\n                )\n                self.root._dmp_wrapped_module = dmp._dmp_wrapped_module\n            else:\n                # Unwrapped dmp modules and composibility api will enter here.\n                graph = super().trace(\n                    root=root,\n                    concrete_args=concrete_args,\n                )\n        finally:\n            _is_fx_tracing_flag = old_is_fx_tracing_flag\n        return graph\n\n    # pyre-ignore[2]\n    def create_arg(self, a: Any) -> Argument:\n        \"\"\"\n        A method to specify the behavior of tracing when preparing values to\n        be used as arguments to nodes in the ``Graph``.\n\n        Adds support for the NoWait type in addition to the default tracer\n\n        Args:\n            a (Any): The value to be emitted as an ``Argument`` in the ``Graph``.\n\n        Returns:\n            Argument: The value ``a`` converted into the appropriate ``Argument``\n        \"\"\"\n        if isinstance(a, NoWait):\n            return self.create_node(\n                \"call_function\",\n                target=NoWait,\n                args=self.create_arg((a._obj,)),\n                kwargs={},\n                type_expr=NoWait,\n            )\n        # jit script has explicit convertions to torch.device from str\n        if isinstance(a, torch.device):\n            return super().create_arg(f\"{a.type}:{a.index}\")\n\n        # Not equivalent to when LazyAwaitable.wait() is called in eager. Here can be called earlier, as attr was not requested and this is not guranteed to be torch function\n        # TODO(ivankobzarev): support equivalent timing of LazyAwaitable\n        if isinstance(a, LazyAwaitable):\n            if a._result is None:\n                a._result = a.wait()\n            return super().create_arg(a._result)\n\n        return super().create_arg(a)\n\n    def path_of_module(self, mod: torch.nn.Module) -> str:\n        \"\"\"\n        Allows trace-ability of non registered modules. This is typically used for Table Batched Embeddings\n        made to look like nn.EmbeddingBags\n        \"\"\"\n\n        if hasattr(mod, \"_fx_path\"):\n            return mod._fx_path\n        else:\n            return super().path_of_module(mod)",
  "def symbolic_trace(\n    # pyre-ignore[24]\n    root: Union[torch.nn.Module, Callable],\n    concrete_args: Optional[Dict[str, Any]] = None,\n    leaf_modules: Optional[List[str]] = None,\n) -> torch.fx.GraphModule:\n    \"\"\"\n    Symbolic tracing API\n\n    Given an ``nn.Module`` or function instance ``root``, this function will return a ``GraphModule``\n    constructed by recording operations seen while tracing through ``root``.\n\n    ``concrete_args`` allows you to partially specialize your function, whether it's to remove control flow or data structures.\n\n    Args:\n        root (Union[torch.nn.Module, Callable]): Module or function to be traced and converted\n            into a Graph representation.\n        concrete_args (Optional[Dict[str, any]]): Inputs to be partially specialized\n\n    Returns:\n        GraphModule: a Module created from the recorded operations from ``root``.\n    \"\"\"\n    tracer = Tracer(leaf_modules)\n    graph = tracer.trace(root, concrete_args)\n    return torch.fx.GraphModule(root, graph)",
  "def __init__(self, leaf_modules: Optional[List[str]] = None) -> None:\n        super().__init__()\n        self._leaf_modules: List[str] = leaf_modules if leaf_modules is not None else []",
  "def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:\n        \"\"\"\n        Override FX definition to include quantized embedding bags\n        \"\"\"\n        if type(m).__name__ in self._leaf_modules:\n            return True\n        return super().is_leaf_module(m, module_qualified_name)",
  "def trace(\n        self,\n        # pyre-ignore[2]: Missing parameter annotation [2]: Parameter `root` must have a type that does not contain `Any`\n        root: Union[torch.nn.Module, Callable[..., Any]],\n        concrete_args: Optional[Dict[str, Any]] = None,\n    ) -> Graph:\n        global _is_fx_tracing_flag\n        old_is_fx_tracing_flag = _is_fx_tracing_flag\n        _is_fx_tracing_flag = True\n\n        try:\n            # TODO(ivankobzarev): support DMP not only on the root level\n            from torchrec.distributed.model_parallel import DistributedModelParallel\n\n            if isinstance(root, DistributedModelParallel):\n                # In the case where the module is wrapped in DMP, you need to replace DMP's forward\n                # call with a new signature, one with explicit args, because fx can't handle variable args.\n                # Furthermore, we need to provide the `fn_root` argument because when tracing a function,\n                # fx uses an empty module as the root (unless one is explicitly provided), which leads to\n                # issues with path_of_module and named_buffers.\n\n                # TODO(shababayub): This can be removed if we either stop supporting dmp wrapping\n                # for fx trace or strip dmp name in named_modules path (much like named_buffers).\n                if isinstance(root, torch.nn.Module):\n                    for prefix, module in root.named_modules():\n                        # TODO(T140754678): Remove this workaround to _fx_path\n                        module._fx_path = prefix\n\n                dmp = root\n                graph = super().trace(\n                    root=dmp_fx_trace_forward(dmp, self),\n                    concrete_args=concrete_args,\n                )\n                self.root._dmp_wrapped_module = dmp._dmp_wrapped_module\n            else:\n                # Unwrapped dmp modules and composibility api will enter here.\n                graph = super().trace(\n                    root=root,\n                    concrete_args=concrete_args,\n                )\n        finally:\n            _is_fx_tracing_flag = old_is_fx_tracing_flag\n        return graph",
  "def create_arg(self, a: Any) -> Argument:\n        \"\"\"\n        A method to specify the behavior of tracing when preparing values to\n        be used as arguments to nodes in the ``Graph``.\n\n        Adds support for the NoWait type in addition to the default tracer\n\n        Args:\n            a (Any): The value to be emitted as an ``Argument`` in the ``Graph``.\n\n        Returns:\n            Argument: The value ``a`` converted into the appropriate ``Argument``\n        \"\"\"\n        if isinstance(a, NoWait):\n            return self.create_node(\n                \"call_function\",\n                target=NoWait,\n                args=self.create_arg((a._obj,)),\n                kwargs={},\n                type_expr=NoWait,\n            )\n        # jit script has explicit convertions to torch.device from str\n        if isinstance(a, torch.device):\n            return super().create_arg(f\"{a.type}:{a.index}\")\n\n        # Not equivalent to when LazyAwaitable.wait() is called in eager. Here can be called earlier, as attr was not requested and this is not guranteed to be torch function\n        # TODO(ivankobzarev): support equivalent timing of LazyAwaitable\n        if isinstance(a, LazyAwaitable):\n            if a._result is None:\n                a._result = a.wait()\n            return super().create_arg(a._result)\n\n        return super().create_arg(a)",
  "def path_of_module(self, mod: torch.nn.Module) -> str:\n        \"\"\"\n        Allows trace-ability of non registered modules. This is typically used for Table Batched Embeddings\n        made to look like nn.EmbeddingBags\n        \"\"\"\n\n        if hasattr(mod, \"_fx_path\"):\n            return mod._fx_path\n        else:\n            return super().path_of_module(mod)",
  "def state_dict_gather(\n    src: Dict[str, Union[torch.Tensor, ShardedTensor]],\n    dst: Dict[str, torch.Tensor],\n) -> None:\n    \"\"\"\n    Gathers the values of the src state_dict of the keys present in the dst state_dict. Can handle ShardedTensors in the src state_dict.\n\n    Args:\n        src (Dict[str, Union[torch.Tensor, ShardedTensor]]): source's state_dict for this rank\n        dst (Dict[str, torch.Tensor]): destination's state_dict\n    \"\"\"\n    for key, dst_tensor in dst.items():\n        src_tensor = src[key]\n        if isinstance(src_tensor, ShardedTensor):\n            src_tensor.gather(out=dst_tensor if (dist.get_rank() == 0) else None)\n        elif isinstance(src_tensor, torch.Tensor):\n            dst_tensor.copy_(src_tensor)\n        else:\n            raise ValueError(f\"Unsupported tensor {key} type {type(src_tensor)}\")",
  "def state_dict_all_gather_keys(\n    state_dict: Dict[str, Union[torch.Tensor, ShardedTensor]],\n    pg: ProcessGroup,\n) -> List[str]:\n    \"\"\"\n    Gathers all the keys of the state_dict from all ranks. Can handle ShardedTensors in the state_dict.\n\n    Args:\n        state_dict (Dict[str, Union[torch.Tensor, ShardedTensor]]): keys of this state_dict will be gathered\n        pg (ProcessGroup): Process Group used for comms\n    \"\"\"\n    names = list(state_dict.keys())\n    all_names = [None] * dist.get_world_size(pg)\n    dist.all_gather_object(all_names, names, pg)\n    deduped_names = set()\n    for local_names in all_names:\n        # pyre-ignore[16]\n        for name in local_names:\n            deduped_names.add(name)\n    return sorted(deduped_names)",
  "def state_dict_to_device(\n    state_dict: Dict[str, Union[torch.Tensor, ShardedTensor]],\n    pg: ProcessGroup,\n    device: torch.device,\n) -> Dict[str, Union[torch.Tensor, ShardedTensor]]:\n    \"\"\"\n    Moves a state_dict to a device with a process group. Can handle ShardedTensors in the state_dict.\n\n    Args:\n        state_dict (Dict[str, Union[torch.Tensor, ShardedTensor]]): state_dict to move\n        pg (ProcessGroup): Process Group used for comms\n        device (torch.device): device to put state_dict on\n    \"\"\"\n    ret = {}\n    all_keys = state_dict_all_gather_keys(state_dict, pg)\n    for key in all_keys:\n        if key in state_dict:\n            tensor = state_dict[key]\n            if isinstance(tensor, ShardedTensor):\n                copied_shards = [\n                    Shard.from_tensor_and_offsets(\n                        tensor=shard.tensor.to(device),\n                        shard_offsets=shard.metadata.shard_offsets,\n                        rank=dist.get_rank(pg),\n                    )\n                    for shard in tensor.local_shards()\n                ]\n                ret[key] = ShardedTensor._init_from_local_shards(\n                    copied_shards,\n                    tensor.metadata().size,\n                    process_group=pg,\n                )\n            elif isinstance(tensor, torch.Tensor):\n                ret[key] = tensor.to(device)\n            else:\n                raise ValueError(f\"Unsupported tensor {key} type {type(tensor)}\")\n        else:\n            # No state_dict entries for table-wise sharding,\n            # but need to follow full-sync.\n            ret[key] = ShardedTensor._init_from_local_shards(\n                [],\n                [],\n                process_group=pg,\n            )\n    return ret",
  "def load_config_text(name: str) -> str:\n    return torch_package_importer.load_text(\"__configs\", name)",
  "def load_pickle_config(name: str, clazz: Type[T]) -> T:\n    loaded_obj = torch_package_importer.load_pickle(\"__configs\", name)\n    assert isinstance(\n        loaded_obj, clazz\n    ), f\"The loaded config {type(loaded_obj)} is not of type {clazz}\"\n    return loaded_obj",
  "class PredictFactoryPackager:\n    @classmethod\n    @abc.abstractclassmethod\n    def set_extern_modules(cls, pe: PackageExporter) -> None:\n        pass\n\n    @classmethod\n    @abc.abstractclassmethod\n    def set_mocked_modules(cls, pe: PackageExporter) -> None:\n        pass\n\n    @classmethod\n    def save_predict_factory(\n        cls,\n        predict_factory: Type[PredictFactory],\n        configs: Dict[str, Any],\n        output: Union[str, Path, BinaryIO],\n        extra_files: Dict[str, Union[str, bytes]],\n        loader_code: str = LOADER_CODE,\n        package_importer: Union[\n            torch.package.Importer, List[torch.package.Importer]\n        ] = torch.package.sys_importer,\n    ) -> None:\n        with PackageExporter(output, importer=package_importer) as pe:\n            # pyre-fixme[29]: `BoundMethod[abc.abstractclassmethod[None],\n            #  Type[PredictFactoryPackager]]` is not a function.\n            cls.set_extern_modules(pe)\n            # pyre-fixme[29]: `BoundMethod[abc.abstractclassmethod[None],\n            #  Type[PredictFactoryPackager]]` is not a function.\n            cls.set_mocked_modules(pe)\n            pe.extern([\"sys\"])\n            pe.intern(\"**\")\n            for k, v in extra_files.items():\n                if isinstance(v, str):\n                    pe.save_text(\"extra_files\", k, v)\n                elif isinstance(v, bytes):\n                    pe.save_binary(\"extra_files\", k, v)\n                else:\n                    raise ValueError(f\"Unsupported type {type(v)}\")\n            cls._save_predict_factory(\n                pe, predict_factory, configs, loader_code=loader_code\n            )\n\n    @classmethod\n    def _save_predict_factory(\n        cls,\n        pe: PackageExporter,\n        predict_factory: Type[PredictFactory],\n        configs: Dict[str, Any],\n        loader_code: str = LOADER_CODE,\n    ) -> None:\n        # If predict_factory is coming from a torch package,\n        # __module__ would have <torch_package_x> prefix.\n        # To save such predict factory, we need to remove\n        # the prefix.\n        package_name = predict_factory.__module__\n        if package_name.startswith(\"<torch_package_\"):\n            package_name = \".\".join(package_name.split(\".\")[1:])\n        # Save loader entry point.\n        code = loader_code.replace(\"%PACKAGE%\", package_name).replace(\n            \"%CLASS%\", predict_factory.__name__\n        )\n        pe.save_source_string(module_name=LOADER_MODULE, src=code)\n\n        # Save configs\n        for name, config in configs.items():\n            if isinstance(config, str):\n                pe.save_text(CONFIG_MODULE, name, config)\n            else:\n                pe.save_pickle(CONFIG_MODULE, name, config)\n\n        # Save predict factory.\n        pe.save_module(predict_factory.__module__)",
  "def set_extern_modules(cls, pe: PackageExporter) -> None:\n        pass",
  "def set_mocked_modules(cls, pe: PackageExporter) -> None:\n        pass",
  "def save_predict_factory(\n        cls,\n        predict_factory: Type[PredictFactory],\n        configs: Dict[str, Any],\n        output: Union[str, Path, BinaryIO],\n        extra_files: Dict[str, Union[str, bytes]],\n        loader_code: str = LOADER_CODE,\n        package_importer: Union[\n            torch.package.Importer, List[torch.package.Importer]\n        ] = torch.package.sys_importer,\n    ) -> None:\n        with PackageExporter(output, importer=package_importer) as pe:\n            # pyre-fixme[29]: `BoundMethod[abc.abstractclassmethod[None],\n            #  Type[PredictFactoryPackager]]` is not a function.\n            cls.set_extern_modules(pe)\n            # pyre-fixme[29]: `BoundMethod[abc.abstractclassmethod[None],\n            #  Type[PredictFactoryPackager]]` is not a function.\n            cls.set_mocked_modules(pe)\n            pe.extern([\"sys\"])\n            pe.intern(\"**\")\n            for k, v in extra_files.items():\n                if isinstance(v, str):\n                    pe.save_text(\"extra_files\", k, v)\n                elif isinstance(v, bytes):\n                    pe.save_binary(\"extra_files\", k, v)\n                else:\n                    raise ValueError(f\"Unsupported type {type(v)}\")\n            cls._save_predict_factory(\n                pe, predict_factory, configs, loader_code=loader_code\n            )",
  "def _save_predict_factory(\n        cls,\n        pe: PackageExporter,\n        predict_factory: Type[PredictFactory],\n        configs: Dict[str, Any],\n        loader_code: str = LOADER_CODE,\n    ) -> None:\n        # If predict_factory is coming from a torch package,\n        # __module__ would have <torch_package_x> prefix.\n        # To save such predict factory, we need to remove\n        # the prefix.\n        package_name = predict_factory.__module__\n        if package_name.startswith(\"<torch_package_\"):\n            package_name = \".\".join(package_name.split(\".\")[1:])\n        # Save loader entry point.\n        code = loader_code.replace(\"%PACKAGE%\", package_name).replace(\n            \"%CLASS%\", predict_factory.__name__\n        )\n        pe.save_source_string(module_name=LOADER_MODULE, src=code)\n\n        # Save configs\n        for name, config in configs.items():\n            if isinstance(config, str):\n                pe.save_text(CONFIG_MODULE, name, config)\n            else:\n                pe.save_pickle(CONFIG_MODULE, name, config)\n\n        # Save predict factory.\n        pe.save_module(predict_factory.__module__)",
  "def trim_torch_package_prefix_from_typename(typename: str) -> str:\n    if typename.startswith(\"<torch_package_\"):\n        # Trim off <torch_package_x> prefix.\n        typename = \".\".join(typename.split(\".\")[1:])\n    return typename",
  "def quantize_feature(\n    module: torch.nn.Module, inputs: Tuple[torch.Tensor, ...]\n) -> Tuple[torch.Tensor, ...]:\n    return tuple(\n        [\n            input.half()\n            if isinstance(input, torch.Tensor)\n            and input.dtype in [torch.float32, torch.float64]\n            else input\n            for input in inputs\n        ]\n    )",
  "def quantize_embeddings(\n    module: nn.Module,\n    dtype: torch.dtype,\n    inplace: bool,\n    additional_qconfig_spec_keys: Optional[List[Type[nn.Module]]] = None,\n    additional_mapping: Optional[Dict[Type[nn.Module], Type[nn.Module]]] = None,\n    output_dtype: torch.dtype = torch.float,\n    per_table_weight_dtype: Optional[Dict[str, torch.dtype]] = None,\n) -> nn.Module:\n    qconfig = TrecQuantConfig(\n        activation=quant.PlaceholderObserver.with_args(dtype=output_dtype),\n        weight=quant.PlaceholderObserver.with_args(dtype=dtype),\n        per_table_weight_dtype=per_table_weight_dtype,\n    )\n    qconfig_spec: Dict[Type[nn.Module], TrecQuantConfig] = {\n        trec.EmbeddingBagCollection: qconfig,\n    }\n    mapping: Dict[Type[nn.Module], Type[nn.Module]] = {\n        trec.EmbeddingBagCollection: trec_quant.EmbeddingBagCollection,\n    }\n    if additional_qconfig_spec_keys is not None:\n        for t in additional_qconfig_spec_keys:\n            qconfig_spec[t] = qconfig\n    if additional_mapping is not None:\n        mapping.update(additional_mapping)\n    return quant.quantize_dynamic(\n        module,\n        qconfig_spec=qconfig_spec,\n        mapping=mapping,\n        inplace=inplace,\n    )",
  "class QualNameMetadata:\n    need_preproc: bool",
  "class BatchingMetadata:\n    \"\"\"\n    Metadata class for batching, this should be kept in sync with the C++ definition.\n    \"\"\"\n\n    type: str\n    # cpu or cuda\n    device: str\n    # list of tensor suffixes to deserialize to pinned memory (e.g. \"lengths\")\n    # use \"\" (empty string) to pin without suffix\n    pinned: List[str]",
  "class PredictFactory(abc.ABC):\n    \"\"\"\n    Creates a model (with already learned weights) to be used inference time.\n    \"\"\"\n\n    @abc.abstractmethod\n    def create_predict_module(self) -> nn.Module:\n        \"\"\"\n        Returns already sharded model with allocated weights.\n        state_dict() must match TransformModule.transform_state_dict().\n        It assumes that torch.distributed.init_process_group was already called\n        and will shard model according to torch.distributed.get_world_size().\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def batching_metadata(self) -> Dict[str, BatchingMetadata]:\n        \"\"\"\n        Returns a dict from input name to BatchingMetadata. This infomation is used for batching for input requests.\n        \"\"\"\n        pass\n\n    def batching_metadata_json(self) -> str:\n        \"\"\"\n        Serialize the batching metadata to JSON, for ease of parsing with torch::deploy environments.\n        \"\"\"\n        return json.dumps(\n            {key: asdict(value) for key, value in self.batching_metadata().items()}\n        )\n\n    @abc.abstractmethod\n    def result_metadata(self) -> str:\n        \"\"\"\n        Returns a string which represents the result type. This information is used for result split.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def run_weights_independent_tranformations(\n        self, predict_module: torch.nn.Module\n    ) -> torch.nn.Module:\n        \"\"\"\n        Run transformations that don't rely on weights of the predict module. e.g. fx tracing, model\n        split etc.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def run_weights_dependent_transformations(\n        self, predict_module: torch.nn.Module\n    ) -> torch.nn.Module:\n        \"\"\"\n        Run transformations that depends on weights of the predict module. e.g. lowering to a backend.\n        \"\"\"\n        pass\n\n    def qualname_metadata(self) -> Dict[str, QualNameMetadata]:\n        \"\"\"\n        Returns a dict from qualname (method name) to QualNameMetadata. This is additional information for execution of specific methods of the model.\n        \"\"\"\n        return {}\n\n    def qualname_metadata_json(self) -> str:\n        \"\"\"\n        Serialize the qualname metadata to JSON, for ease of parsing with torch::deploy environments.\n        \"\"\"\n        return json.dumps(\n            {key: asdict(value) for key, value in self.qualname_metadata().items()}\n        )\n\n    def model_inputs_data(self) -> Dict[str, Any]:\n        \"\"\"\n        Returns a dict of various data for benchmarking input generation.\n        \"\"\"\n        return {}",
  "class PredictModule(nn.Module):\n    \"\"\"\n    Interface for modules to work in a torch.deploy based backend. Users should\n    override predict_forward to convert batch input format to module input format.\n\n    Call Args:\n        batch: a dict of input tensors\n\n    Returns:\n        output: a dict of output tensors\n\n    Args:\n        module: the actual predict module\n        device: the primary device for this module that will be used in forward calls.\n\n    Example::\n\n        module = PredictModule(torch.device(\"cuda\", torch.cuda.current_device()))\n    \"\"\"\n\n    def __init__(\n        self,\n        module: nn.Module,\n    ) -> None:\n        super().__init__()\n        self._module: nn.Module = module\n        # lazy init device from thread inited device guard\n        self._device: Optional[torch.device] = None\n        self._module.eval()\n\n    @property\n    def predict_module(\n        self,\n    ) -> nn.Module:\n        return self._module\n\n    @abc.abstractmethod\n    # pyre-fixme[3]\n    def predict_forward(self, batch: Dict[str, torch.Tensor]) -> Any:\n        pass\n\n    # pyre-fixme[3]\n    def forward(self, batch: Dict[str, torch.Tensor]) -> Any:\n        if self._device is None:\n            self._device = torch.device(\"cuda\", torch.cuda.current_device())\n        with torch.cuda.device(self._device), torch.inference_mode():\n            return self.predict_forward(batch)\n\n    # pyre-fixme[14]: `state_dict` overrides method defined in `Module` inconsistently.\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        # pyre-fixme[19]: Expected 0 positional arguments.\n        return self._module.state_dict(destination, prefix, keep_vars)",
  "def quantize_dense(\n    predict_module: PredictModule,\n    dtype: torch.dtype,\n    additional_embedding_module_type: List[Type[nn.Module]] = [],\n) -> nn.Module:\n    module = predict_module.predict_module\n    reassign = {}\n\n    for name, mod in module.named_children():\n        # both fused modules and observed custom modules are\n        # swapped as one unit\n        if not (\n            isinstance(mod, EmbeddingBagCollectionInterface)\n            or isinstance(mod, EmbeddingCollectionInterface)\n            or any([type(mod) is clazz for clazz in additional_embedding_module_type])\n        ):\n            if dtype == torch.half:\n                new_mod = mod.half()\n                new_mod.register_forward_pre_hook(quantize_feature)\n                reassign[name] = new_mod\n            else:\n                raise NotImplementedError(\n                    \"only fp16 is supported for non-embedding module lowering\"\n                )\n    for key, value in reassign.items():\n        module._modules[key] = value\n    return predict_module",
  "def create_predict_module(self) -> nn.Module:\n        \"\"\"\n        Returns already sharded model with allocated weights.\n        state_dict() must match TransformModule.transform_state_dict().\n        It assumes that torch.distributed.init_process_group was already called\n        and will shard model according to torch.distributed.get_world_size().\n        \"\"\"\n        pass",
  "def batching_metadata(self) -> Dict[str, BatchingMetadata]:\n        \"\"\"\n        Returns a dict from input name to BatchingMetadata. This infomation is used for batching for input requests.\n        \"\"\"\n        pass",
  "def batching_metadata_json(self) -> str:\n        \"\"\"\n        Serialize the batching metadata to JSON, for ease of parsing with torch::deploy environments.\n        \"\"\"\n        return json.dumps(\n            {key: asdict(value) for key, value in self.batching_metadata().items()}\n        )",
  "def result_metadata(self) -> str:\n        \"\"\"\n        Returns a string which represents the result type. This information is used for result split.\n        \"\"\"\n        pass",
  "def run_weights_independent_tranformations(\n        self, predict_module: torch.nn.Module\n    ) -> torch.nn.Module:\n        \"\"\"\n        Run transformations that don't rely on weights of the predict module. e.g. fx tracing, model\n        split etc.\n        \"\"\"\n        pass",
  "def run_weights_dependent_transformations(\n        self, predict_module: torch.nn.Module\n    ) -> torch.nn.Module:\n        \"\"\"\n        Run transformations that depends on weights of the predict module. e.g. lowering to a backend.\n        \"\"\"\n        pass",
  "def qualname_metadata(self) -> Dict[str, QualNameMetadata]:\n        \"\"\"\n        Returns a dict from qualname (method name) to QualNameMetadata. This is additional information for execution of specific methods of the model.\n        \"\"\"\n        return {}",
  "def qualname_metadata_json(self) -> str:\n        \"\"\"\n        Serialize the qualname metadata to JSON, for ease of parsing with torch::deploy environments.\n        \"\"\"\n        return json.dumps(\n            {key: asdict(value) for key, value in self.qualname_metadata().items()}\n        )",
  "def model_inputs_data(self) -> Dict[str, Any]:\n        \"\"\"\n        Returns a dict of various data for benchmarking input generation.\n        \"\"\"\n        return {}",
  "def __init__(\n        self,\n        module: nn.Module,\n    ) -> None:\n        super().__init__()\n        self._module: nn.Module = module\n        # lazy init device from thread inited device guard\n        self._device: Optional[torch.device] = None\n        self._module.eval()",
  "def predict_module(\n        self,\n    ) -> nn.Module:\n        return self._module",
  "def predict_forward(self, batch: Dict[str, torch.Tensor]) -> Any:\n        pass",
  "def forward(self, batch: Dict[str, torch.Tensor]) -> Any:\n        if self._device is None:\n            self._device = torch.device(\"cuda\", torch.cuda.current_device())\n        with torch.cuda.device(self._device), torch.inference_mode():\n            return self.predict_forward(batch)",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        # pyre-fixme[19]: Expected 0 positional arguments.\n        return self._module.state_dict(destination, prefix, keep_vars)",
  "def create_training_batch(args: argparse.Namespace) -> Batch:\n    return next(\n        iter(\n            DataLoader(\n                RandomRecDataset(\n                    keys=DEFAULT_CAT_NAMES,\n                    batch_size=args.batch_size,\n                    hash_size=args.num_embedding_features,\n                    ids_per_feature=1,\n                    num_dense=len(DEFAULT_INT_NAMES),\n                ),\n                batch_sampler=None,\n                pin_memory=False,\n                num_workers=0,\n            )\n        )\n    )",
  "def create_request(\n    batch: Batch, args: argparse.Namespace\n) -> predictor_pb2.PredictionRequest:\n    def to_bytes(tensor: torch.Tensor) -> bytes:\n        return tensor.cpu().numpy().tobytes()\n\n    float_features = predictor_pb2.FloatFeatures(\n        num_features=args.num_float_features,\n        values=to_bytes(batch.dense_features),\n    )\n\n    id_list_features = predictor_pb2.SparseFeatures(\n        num_features=args.num_id_list_features,\n        values=to_bytes(batch.sparse_features.values()),\n        lengths=to_bytes(batch.sparse_features.lengths()),\n    )\n\n    id_score_list_features = predictor_pb2.SparseFeatures(num_features=0)\n    embedding_features = predictor_pb2.FloatFeatures(num_features=0)\n    unary_features = predictor_pb2.SparseFeatures(num_features=0)\n\n    return predictor_pb2.PredictionRequest(\n        batch_size=args.batch_size,\n        float_features=float_features,\n        id_list_features=id_list_features,\n        id_score_list_features=id_score_list_features,\n        embedding_features=embedding_features,\n        unary_features=unary_features,\n    )",
  "def to_bytes(tensor: torch.Tensor) -> bytes:\n        return tensor.cpu().numpy().tobytes()",
  "def compute_cross_entropy(\n    labels: torch.Tensor,\n    predictions: torch.Tensor,\n    weights: torch.Tensor,\n    eta: float,\n) -> torch.Tensor:\n    predictions = predictions.double()\n    predictions.clamp_(min=eta, max=1 - eta)\n    cross_entropy = -weights * labels * torch.log2(predictions) - weights * (\n        1.0 - labels\n    ) * torch.log2(1.0 - predictions)\n    return cross_entropy",
  "def _compute_cross_entropy_norm(\n    mean_label: torch.Tensor,\n    pos_labels: torch.Tensor,\n    neg_labels: torch.Tensor,\n    eta: float,\n) -> torch.Tensor:\n    mean_label = mean_label.double()\n    mean_label.clamp_(min=eta, max=1 - eta)\n    return -pos_labels * torch.log2(mean_label) - neg_labels * torch.log2(\n        1.0 - mean_label\n    )",
  "def compute_ne(\n    ce_sum: torch.Tensor,\n    weighted_num_samples: torch.Tensor,\n    pos_labels: torch.Tensor,\n    neg_labels: torch.Tensor,\n    eta: float,\n) -> torch.Tensor:\n    mean_label = pos_labels / weighted_num_samples\n    ce_norm = _compute_cross_entropy_norm(mean_label, pos_labels, neg_labels, eta)\n    return ce_sum / ce_norm",
  "def compute_logloss(\n    ce_sum: torch.Tensor,\n    pos_labels: torch.Tensor,\n    neg_labels: torch.Tensor,\n    eta: float,\n) -> torch.Tensor:\n    labels_sum = pos_labels + neg_labels\n    labels_sum.clamp_(min=eta)\n    return ce_sum / labels_sum",
  "def get_ne_states(\n    labels: torch.Tensor, predictions: torch.Tensor, weights: torch.Tensor, eta: float\n) -> Dict[str, torch.Tensor]:\n    cross_entropy = compute_cross_entropy(\n        labels,\n        predictions,\n        weights,\n        eta,\n    )\n    return {\n        \"cross_entropy_sum\": torch.sum(cross_entropy, dim=-1),\n        \"weighted_num_samples\": torch.sum(weights, dim=-1),\n        \"pos_labels\": torch.sum(weights * labels, dim=-1),\n        \"neg_labels\": torch.sum(weights * (1.0 - labels), dim=-1),\n    }",
  "class NEMetricComputation(RecMetricComputation):\n    r\"\"\"\n    This class implements the RecMetricComputation for NE, i.e. Normalized Entropy.\n\n    The constructor arguments are defined in RecMetricComputation.\n    See the docstring of RecMetricComputation for more detail.\n\n    Args:\n        include_logloss (bool): return vanilla logloss as one of metrics results, on top of NE.\n    \"\"\"\n\n    def __init__(\n        self, *args: Any, include_logloss: bool = False, **kwargs: Any\n    ) -> None:\n        self._include_logloss: bool = include_logloss\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"cross_entropy_sum\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"weighted_num_samples\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"pos_labels\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"neg_labels\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self.eta = 1e-12\n\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for NEMetricComputation update\"\n            )\n        states = get_ne_states(labels, predictions, weights, self.eta)\n        num_samples = predictions.shape[-1]\n\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)\n\n    def _compute(self) -> List[MetricComputationReport]:\n        reports = [\n            MetricComputationReport(\n                name=MetricName.NE,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_ne(\n                    cast(torch.Tensor, self.cross_entropy_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                    cast(torch.Tensor, self.pos_labels),\n                    cast(torch.Tensor, self.neg_labels),\n                    self.eta,\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.NE,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_ne(\n                    self.get_window_state(\"cross_entropy_sum\"),\n                    self.get_window_state(\"weighted_num_samples\"),\n                    self.get_window_state(\"pos_labels\"),\n                    self.get_window_state(\"neg_labels\"),\n                    self.eta,\n                ),\n            ),\n        ]\n        if self._include_logloss:\n            reports += [\n                MetricComputationReport(\n                    name=MetricName.LOG_LOSS,\n                    metric_prefix=MetricPrefix.LIFETIME,\n                    value=compute_logloss(\n                        cast(torch.Tensor, self.cross_entropy_sum),\n                        cast(torch.Tensor, self.pos_labels),\n                        cast(torch.Tensor, self.neg_labels),\n                        self.eta,\n                    ),\n                ),\n                MetricComputationReport(\n                    name=MetricName.LOG_LOSS,\n                    metric_prefix=MetricPrefix.WINDOW,\n                    value=compute_logloss(\n                        self.get_window_state(\"cross_entropy_sum\"),\n                        self.get_window_state(\"pos_labels\"),\n                        self.get_window_state(\"neg_labels\"),\n                        self.eta,\n                    ),\n                ),\n            ]\n        return reports",
  "class NEMetric(RecMetric):\n    _namespace: MetricNamespace = MetricNamespace.NE\n    _computation_class: Type[RecMetricComputation] = NEMetricComputation",
  "def __init__(\n        self, *args: Any, include_logloss: bool = False, **kwargs: Any\n    ) -> None:\n        self._include_logloss: bool = include_logloss\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"cross_entropy_sum\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"weighted_num_samples\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"pos_labels\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"neg_labels\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self.eta = 1e-12",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for NEMetricComputation update\"\n            )\n        states = get_ne_states(labels, predictions, weights, self.eta)\n        num_samples = predictions.shape[-1]\n\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)",
  "def _compute(self) -> List[MetricComputationReport]:\n        reports = [\n            MetricComputationReport(\n                name=MetricName.NE,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_ne(\n                    cast(torch.Tensor, self.cross_entropy_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                    cast(torch.Tensor, self.pos_labels),\n                    cast(torch.Tensor, self.neg_labels),\n                    self.eta,\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.NE,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_ne(\n                    self.get_window_state(\"cross_entropy_sum\"),\n                    self.get_window_state(\"weighted_num_samples\"),\n                    self.get_window_state(\"pos_labels\"),\n                    self.get_window_state(\"neg_labels\"),\n                    self.eta,\n                ),\n            ),\n        ]\n        if self._include_logloss:\n            reports += [\n                MetricComputationReport(\n                    name=MetricName.LOG_LOSS,\n                    metric_prefix=MetricPrefix.LIFETIME,\n                    value=compute_logloss(\n                        cast(torch.Tensor, self.cross_entropy_sum),\n                        cast(torch.Tensor, self.pos_labels),\n                        cast(torch.Tensor, self.neg_labels),\n                        self.eta,\n                    ),\n                ),\n                MetricComputationReport(\n                    name=MetricName.LOG_LOSS,\n                    metric_prefix=MetricPrefix.WINDOW,\n                    value=compute_logloss(\n                        self.get_window_state(\"cross_entropy_sum\"),\n                        self.get_window_state(\"pos_labels\"),\n                        self.get_window_state(\"neg_labels\"),\n                        self.eta,\n                    ),\n                ),\n            ]\n        return reports",
  "class MetricComputationReport:\n    name: MetricNameBase\n    metric_prefix: MetricPrefix\n    value: torch.Tensor\n    description: Optional[str] = None",
  "class RecMetricException(Exception):\n    pass",
  "class WindowBuffer:\n    def __init__(self, max_size: int, max_buffer_count: int) -> None:\n        self._max_size: int = max_size\n        self._max_buffer_count: int = max_buffer_count\n\n        self._buffers: Deque[torch.Tensor] = deque(maxlen=max_buffer_count)\n        self._used_sizes: Deque[int] = deque(maxlen=max_buffer_count)\n        self._window_used_size = 0\n\n    def aggregate_state(\n        self, window_state: torch.Tensor, curr_state: torch.Tensor, size: int\n    ) -> None:\n        def remove(window_state: torch.Tensor) -> None:\n            window_state -= self._buffers.popleft()\n            self._window_used_size -= self._used_sizes.popleft()\n\n        if len(self._buffers) == self._buffers.maxlen:\n            remove(window_state)\n\n        self._buffers.append(curr_state)\n        self._used_sizes.append(size)\n        window_state += curr_state\n        self._window_used_size += size\n\n        while self._window_used_size > self._max_size:\n            remove(window_state)\n\n    @property\n    def buffers(self) -> Deque[torch.Tensor]:\n        return self._buffers",
  "class RecMetricComputation(Metric, abc.ABC):\n    r\"\"\"The internal computation class template.\n    A metric implementation should overwrite `update()` and `compute()`. These two\n    APIs focus on the actual mathematical meaning of the metric, without detailed\n    knowledge of model output and task information.\n\n    Args:\n        my_rank (int): the rank of this trainer.\n        batch_size (int): batch size used by this trainer.\n        n_tasks (int): the number tasks this communication object will have to compute.\n        window_size (int): the window size for the window metric.\n        compute_on_all_ranks (bool): whether to compute metrics on all ranks. This is\n            necessary if the non-leader rank wants to consume the metrics results.\n        should_validate_update (bool): whether to check the inputs of `update()` and\n            skip the update if the inputs are invalid. Invalid inputs include the case\n            where all examples have 0 weights for a batch.\n        process_group (Optional[ProcessGroup]): the process group used for the\n            communication. Will use the default process group if not specified.\n    \"\"\"\n    _batch_window_buffers: Optional[Dict[str, WindowBuffer]]\n\n    def __init__(\n        self,\n        my_rank: int,\n        batch_size: int,\n        n_tasks: int,\n        window_size: int,\n        compute_on_all_ranks: bool = False,\n        should_validate_update: bool = False,\n        process_group: Optional[dist.ProcessGroup] = None,\n        fused_update_limit: int = 0,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(process_group=process_group, *args, **kwargs)\n\n        self._my_rank = my_rank\n        self._n_tasks = n_tasks\n        self._batch_size = batch_size\n        self._window_size = window_size\n        self._compute_on_all_ranks = compute_on_all_ranks\n        self._should_validate_update = should_validate_update\n        if self._window_size > 0:\n            self._batch_window_buffers = {}\n        else:\n            self._batch_window_buffers = None\n        if self._should_validate_update:\n            self._add_state(\n                \"has_valid_update\",\n                torch.zeros(self._n_tasks, dtype=torch.uint8),\n                add_window_state=False,\n                dist_reduce_fx=lambda x: torch.any(x, dim=0).byte(),\n                persistent=True,\n            )\n\n    @staticmethod\n    def get_window_state_name(state_name: str) -> str:\n        return f\"window_{state_name}\"\n\n    def get_window_state(self, state_name: str) -> torch.Tensor:\n        return getattr(self, self.get_window_state_name(state_name))\n\n    def _add_state(\n        self, name: str, default: DefaultValueT, add_window_state: bool, **kwargs: Any\n    ) -> None:\n        \"\"\"\n        name (str): the name of this state. The state will be accessible\n            with `self.THE_NAME_YOU_DEFINE`.\n        default (DefaultValueT): the initial value of this state. The most common\n            initial value is `torch.zeros(self._n_tasks, dtype=torch.float)`, but\n            users need to check the math formula to decide what is the correct\n            initial value for the metric. Note the `self._n_tasks` in the above\n            code. As a metric may handle multiple tasks at the same time, the\n            highest dimension of a state should be `self._n_tasks`.\n        add_window_state (bool): when this is True, a `window_{name}` state will\n            be created to record the window state information for this state.\n        dist_reduce_fx (str): the reduction function when aggregating the local\n            state. For example, tower_qps uses \u201csum\u201d to aggregate the total\n            trained examples.\n        persistent (bool): set this to True if you want to save/checkpoint the\n            metric and this state is required to compute the checkpointed metric.\n        \"\"\"\n        # pyre-fixme[6]: Expected `Union[List[typing.Any], torch.Tensor]` for 2nd\n        #  param but got `DefaultValueT`.\n        super().add_state(name, default, **kwargs)\n        if add_window_state:\n            if self._batch_window_buffers is None:\n                raise RuntimeError(\n                    \"Users is adding a window state while window metric is disabled.\"\n                )\n            kwargs[\"persistent\"] = False\n            window_state_name = self.get_window_state_name(name)\n            # Avoid pyre error\n            assert isinstance(default, torch.Tensor)\n            super().add_state(window_state_name, default.detach().clone(), **kwargs)\n\n            self._batch_window_buffers[window_state_name] = WindowBuffer(\n                max_size=self._window_size,\n                max_buffer_count=MAX_BUFFER_COUNT,\n            )\n\n    def _aggregate_window_state(\n        self, state_name: str, state: torch.Tensor, num_samples: int\n    ) -> None:\n        if self._batch_window_buffers is None:\n            raise RuntimeError(\n                \"Users is adding a window state while window metric is disabled.\"\n            )\n        window_state_name = self.get_window_state_name(state_name)\n        assert self._batch_window_buffers is not None\n        self._batch_window_buffers[window_state_name].aggregate_state(\n            getattr(self, window_state_name), curr_state=state, size=num_samples\n        )\n\n    @abc.abstractmethod\n    # pyre-fixme[14]: `update` overrides method defined in `Metric` inconsistently.\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:  # pragma: no cover\n        pass\n\n    @abc.abstractmethod\n    def _compute(self) -> List[MetricComputationReport]:  # pragma: no cover\n        pass\n\n    def pre_compute(self) -> None:\n        r\"\"\"If a metric need to do some work before `compute()`, the metric\n        has to override this `pre_compute()`. One possible usage is to do\n        some pre-processing of the local state before `compute()` as TorchMetric\n        wraps `RecMetricComputation.compute()` and will do the global aggregation\n        before `RecMetricComputation.compute()` is called.\n        \"\"\"\n        return\n\n    def compute(self) -> List[MetricComputationReport]:\n        with record_function(f\"## {self.__class__.__name__}:compute ##\"):\n            if self._my_rank == 0 or self._compute_on_all_ranks:\n                return self._compute()\n            else:\n                return []\n\n    def local_compute(self) -> List[MetricComputationReport]:\n        return self._compute()\n\n    def reset(self) -> None:\n        super().reset()\n        if self._batch_window_buffers is not None:\n            self._batch_window_buffers = {\n                name: WindowBuffer(\n                    max_size=self._window_size,\n                    max_buffer_count=MAX_BUFFER_COUNT,\n                )\n                for name in self._batch_window_buffers\n            }",
  "class RecMetric(nn.Module, abc.ABC):\n    r\"\"\"The main class template to implement a recommendation metric.\n    This class contains the recommendation tasks information (RecTaskInfo) and\n    the actual computation object (RecMetricComputation). RecMetric processes\n    all the information related to RecTaskInfo and models, and passes the required\n    signals to the computation object, allowing the implementation of\n    RecMetricComputation to focus on the mathematical meaning.\n\n    A new metric that inherits RecMetric must override the following attributes\n    in its own `__init__()`: `_namespace` and `_metrics_computations`. No other\n    methods should be overridden.\n\n    Args:\n        world_size (int): the number of trainers.\n        my_rank (int): the rank of this trainer.\n        batch_size (int): batch size used by this trainer.\n        tasks (List[RecTaskInfo]): the information of the model tasks.\n        compute_mode (RecComputeMode): the computation mode. See RecComputeMode.\n        window_size (int): the window size for the window metric.\n        fused_update_limit (int): the maximum number of updates to be fused.\n        compute_on_all_ranks (bool): whether to compute metrics on all ranks. This\n            is necessary if the non-leader rank wants to consume global metrics result.\n        should_validate_update (bool): whether to check the inputs of `update()` and\n            skip the update if the inputs are invalid. Invalid inputs include the case\n            where all examples have 0 weights for a batch.\n        process_group (Optional[ProcessGroup]): the process group used for the\n            communication. Will use the default process group if not specified.\n\n    Example::\n\n        ne = NEMetric(\n            world_size=4,\n            my_rank=0,\n            batch_size=128,\n            tasks=DefaultTaskInfo,\n        )\n    \"\"\"\n    _computation_class: Type[RecMetricComputation]\n    _namespace: MetricNamespaceBase\n    _metrics_computations: nn.ModuleList\n\n    _tasks: List[RecTaskInfo]\n    _window_size: int\n    _tasks_iter: Callable[[str], ComputeIterType]\n    _update_buffers: Dict[str, List[RecModelOutput]]\n    _default_weights: Dict[Tuple[int, ...], torch.Tensor]\n\n    _required_inputs: Set[str]\n\n    PREDICTIONS: str = \"predictions\"\n    LABELS: str = \"labels\"\n    WEIGHTS: str = \"weights\"\n\n    def __init__(\n        self,\n        world_size: int,\n        my_rank: int,\n        batch_size: int,\n        tasks: List[RecTaskInfo],\n        compute_mode: RecComputeMode = RecComputeMode.UNFUSED_TASKS_COMPUTATION,\n        window_size: int = 100,\n        fused_update_limit: int = 0,\n        compute_on_all_ranks: bool = False,\n        should_validate_update: bool = False,\n        process_group: Optional[dist.ProcessGroup] = None,\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        torch._C._log_api_usage_once(\n            f\"torchrec.metrics.rec_metric.{self.__class__.__name__}\"\n        )\n        # TODO(stellaya): consider to inherit from TorchMetrics.Metric or\n        # TorchMetrics.MetricCollection.\n        if (\n            compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION\n            and fused_update_limit > 0\n        ):\n            raise ValueError(\n                \"The fused tasks computation and the fused update cannot be set at the same time\"\n            )\n        super().__init__()\n        self._world_size = world_size\n        self._my_rank = my_rank\n        self._window_size = math.ceil(window_size / world_size)\n        self._batch_size = batch_size\n        self._metrics_computations = nn.ModuleList()\n        self._tasks = tasks\n        self._compute_mode = compute_mode\n        self._fused_update_limit = fused_update_limit\n        self._should_validate_update = should_validate_update\n        self._default_weights = {}\n        self._required_inputs = set()\n        self._update_buffers = {\n            self.PREDICTIONS: [],\n            self.LABELS: [],\n            self.WEIGHTS: [],\n        }\n        if self._window_size < self._batch_size:\n            raise ValueError(\n                f\"Local window size must be larger than batch size. Got local window size {self._window_size} and batch size {self._batch_size}.\"\n            )\n\n        if compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION:\n            task_per_metric = len(self._tasks)\n            self._tasks_iter = self._fused_tasks_iter\n        else:\n            task_per_metric = 1\n            self._tasks_iter = self._unfused_tasks_iter\n\n        for task_config in (\n            [self._tasks]\n            if compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION\n            else self._tasks\n        ):\n            # pyre-ignore\n            kwargs[\"fused_update_limit\"] = fused_update_limit\n            # This Pyre error seems to be Pyre's bug as it can be inferred by mypy\n            # according to https://github.com/python/mypy/issues/3048.\n            # pyre-fixme[45]: Cannot instantiate abstract class `RecMetricCoputation`.\n            metric_computation = self._computation_class(\n                my_rank,\n                batch_size,\n                task_per_metric,\n                self._window_size,\n                compute_on_all_ranks,\n                self._should_validate_update,\n                process_group,\n                **{**kwargs, **self._get_task_kwargs(task_config)},\n            )\n            required_inputs = self._get_task_required_inputs(task_config)\n\n            self._metrics_computations.append(metric_computation)\n            self._required_inputs.update(required_inputs)\n\n    def _get_task_kwargs(\n        self, task_config: Union[RecTaskInfo, List[RecTaskInfo]]\n    ) -> Dict[str, Any]:\n        return {}\n\n    def _get_task_required_inputs(\n        self, task_config: Union[RecTaskInfo, List[RecTaskInfo]]\n    ) -> Set[str]:\n        return set()\n\n    # TODO(stellaya): Refactor the _[fused, unfused]_tasks_iter methods and replace the\n    # compute_scope str input with an enum\n    def _fused_tasks_iter(self, compute_scope: str) -> ComputeIterType:\n        assert len(self._metrics_computations) == 1\n        self._metrics_computations[0].pre_compute()\n        for metric_report in getattr(\n            self._metrics_computations[0], compute_scope + \"compute\"\n        )():\n            for task, metric_value, has_valid_update in zip(\n                self._tasks,\n                metric_report.value,\n                self._metrics_computations[0].has_valid_update\n                if self._should_validate_update\n                else itertools.repeat(\n                    1\n                ),  # has_valid_update > 0 means the update is valid\n            ):\n                # The attribute has_valid_update is a tensor whose length equals to the\n                # number of tasks. Each value in it is corresponding to whether a task\n                # has valid updates or not.\n                # If for a task there's no valid updates, the calculated metric_value\n                # will be meaningless, so we mask it with the default value, i.e. 0.\n                valid_metric_value = (\n                    metric_value\n                    if has_valid_update > 0\n                    else torch.zeros_like(metric_value)\n                )\n                yield task, metric_report.name, valid_metric_value, compute_scope + metric_report.metric_prefix.value, metric_report.description\n\n    def _unfused_tasks_iter(self, compute_scope: str) -> ComputeIterType:\n        for task, metric_computation in zip(self._tasks, self._metrics_computations):\n            metric_computation.pre_compute()\n            for metric_report in getattr(\n                metric_computation, compute_scope + \"compute\"\n            )():\n                # The attribute has_valid_update is a tensor with only 1 value\n                # corresponding to whether the task has valid updates or not.\n                # If there's no valid update, the calculated metric_report.value\n                # will be meaningless, so we mask it with the default value, i.e. 0.\n                valid_metric_value = (\n                    metric_report.value\n                    if not self._should_validate_update\n                    or metric_computation.has_valid_update[0] > 0\n                    else torch.zeros_like(metric_report.value)\n                )\n                yield task, metric_report.name, valid_metric_value, compute_scope + metric_report.metric_prefix.value, metric_report.description\n\n    def _fuse_update_buffers(self) -> Dict[str, RecModelOutput]:\n        def fuse(outputs: List[RecModelOutput]) -> RecModelOutput:\n            assert len(outputs) > 0\n            if isinstance(outputs[0], torch.Tensor):\n                return torch.cat(cast(List[torch.Tensor], outputs))\n            else:\n                task_outputs: Dict[str, List[torch.Tensor]] = defaultdict(list)\n                for output in outputs:\n                    assert isinstance(output, dict)\n                    for task_name, tensor in output.items():\n                        task_outputs[task_name].append(tensor)\n                return {\n                    name: torch.cat(tensors) for name, tensors in task_outputs.items()\n                }\n\n        ret: Dict[str, RecModelOutput] = {}\n        for key, output_list in self._update_buffers.items():\n            if len(output_list) > 0:\n                ret[key] = fuse(output_list)\n            else:\n                assert key == self.WEIGHTS\n            output_list.clear()\n        return ret\n\n    def _check_fused_update(self, force: bool) -> None:\n        if self._fused_update_limit <= 0:\n            return\n        if len(self._update_buffers[self.PREDICTIONS]) == 0:\n            return\n        if (\n            not force\n            and len(self._update_buffers[self.PREDICTIONS]) < self._fused_update_limit\n        ):\n            return\n        fused_arguments = self._fuse_update_buffers()\n        self._update(\n            predictions=fused_arguments[self.PREDICTIONS],\n            labels=fused_arguments[self.LABELS],\n            weights=fused_arguments.get(self.WEIGHTS, None),\n        )\n\n    def _create_default_weights(self, predictions: torch.Tensor) -> torch.Tensor:\n        # pyre-fixme[6]: For 1st param expected `Tuple[int, ...]` but got `Size`.\n        weights = self._default_weights.get(predictions.size(), None)\n        if weights is None:\n            weights = torch.ones_like(predictions)\n            # pyre-fixme[6]: For 1st param expected `Tuple[int, ...]` but got `Size`.\n            self._default_weights[predictions.size()] = weights\n        return weights\n\n    def _check_nonempty_weights(self, weights: torch.Tensor) -> torch.Tensor:\n        return torch.gt(torch.count_nonzero(weights, dim=-1), 0)\n\n    def _update(\n        self,\n        *,\n        predictions: RecModelOutput,\n        labels: RecModelOutput,\n        weights: Optional[RecModelOutput],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        with torch.no_grad():\n            if self._compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION:\n                assert isinstance(predictions, torch.Tensor) and isinstance(\n                    labels, torch.Tensor\n                )\n\n                predictions = (\n                    # Reshape the predictions to size([len(self._tasks), self._batch_size])\n                    predictions.view(-1, self._batch_size)\n                    if predictions.dim() == labels.dim()\n                    # predictions.dim() == labels.dim() + 1 for multiclass models\n                    else predictions.view(-1, self._batch_size, predictions.size()[-1])\n                )\n                labels = labels.view(-1, self._batch_size)\n                if weights is None:\n                    weights = self._create_default_weights(predictions)\n                else:\n                    assert isinstance(weights, torch.Tensor)\n                    weights = weights.view(-1, self._batch_size)\n                if self._should_validate_update:\n                    # has_valid_weights is a tensor of bool whose length equals to the number\n                    # of tasks. Each value in it is corresponding to whether the weights\n                    # are valid, i.e. are set to non-zero values for that task in this update.\n                    # If has_valid_weights are Falses for all the tasks, we just ignore this\n                    # update.\n                    has_valid_weights = self._check_nonempty_weights(weights)\n                    if torch.any(has_valid_weights):\n                        self._metrics_computations[0].update(\n                            predictions=predictions,\n                            labels=labels,\n                            weights=weights,\n                            **kwargs,\n                        )\n                        self._metrics_computations[0].has_valid_update.logical_or_(\n                            has_valid_weights\n                        )\n                else:\n                    self._metrics_computations[0].update(\n                        predictions=predictions,\n                        labels=labels,\n                        weights=weights,\n                        **kwargs,\n                    )\n            else:\n                for task, metric_ in zip(self._tasks, self._metrics_computations):\n                    if task.name not in predictions:\n                        continue\n                    # pyre-fixme[6]: For 1st argument expected `Union[None,\n                    #  List[typing.Any], int, slice, Tensor, typing.Tuple[typing.Any,\n                    #  ...]]` but got `str`.\n                    if torch.numel(predictions[task.name]) == 0:\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        assert torch.numel(labels[task.name]) == 0\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        assert weights is None or torch.numel(weights[task.name]) == 0\n                        continue\n                    task_predictions = (\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        predictions[task.name].view(1, -1)\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        if predictions[task.name].dim() == labels[task.name].dim()\n                        # predictions[task.name].dim() == labels[task.name].dim() + 1 for multiclass models\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        else predictions[task.name].view(\n                            1,\n                            -1,\n                            predictions[\n                                task.name  # pyre-fixme[6]: For 1st argument expected `Union[None,\n                                #  List[typing.Any], int, slice, Tensor,\n                                #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                            ].size()[-1],\n                        )\n                    )\n                    # pyre-fixme[6]: For 1st argument expected `Union[None,\n                    #  List[typing.Any], int, slice, Tensor, typing.Tuple[typing.Any,\n                    #  ...]]` but got `str`.\n                    task_labels = labels[task.name].view(1, -1)\n                    if weights is None:\n                        task_weights = self._create_default_weights(task_predictions)\n                    else:\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        task_weights = weights[task.name].view(1, -1)\n                    if self._should_validate_update:\n                        # has_valid_weights is a tensor with only 1 value corresponding to\n                        # whether the weights are valid, i.e. are set to non-zero values for\n                        # the task in this update.\n                        # If has_valid_update[0] is False, we just ignore this update.\n                        has_valid_weights = self._check_nonempty_weights(task_weights)\n                        if has_valid_weights[0]:\n                            metric_.has_valid_update.logical_or_(has_valid_weights)\n                        else:\n                            continue\n                    if \"required_inputs\" in kwargs:\n                        kwargs[\"required_inputs\"] = {\n                            k: v.view(task_labels.size())\n                            for k, v in kwargs[\"required_inputs\"].items()\n                        }\n                    metric_.update(\n                        predictions=task_predictions,\n                        labels=task_labels,\n                        weights=task_weights,\n                        **kwargs,\n                    )\n\n    def update(\n        self,\n        *,\n        predictions: RecModelOutput,\n        labels: RecModelOutput,\n        weights: Optional[RecModelOutput],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        with record_function(f\"## {self.__class__.__name__}:update ##\"):\n            if self._fused_update_limit > 0:\n                self._update_buffers[self.PREDICTIONS].append(predictions)\n                self._update_buffers[self.LABELS].append(labels)\n                if weights is not None:\n                    self._update_buffers[self.WEIGHTS].append(weights)\n                self._check_fused_update(force=False)\n            else:\n                self._update(\n                    predictions=predictions, labels=labels, weights=weights, **kwargs\n                )\n\n    # The implementation of compute is very similar to local_compute, but compute overwrites\n    # the abstract method compute in torchmetrics.Metric, which is wrapped by _wrap_compute\n    def compute(self) -> Dict[str, torch.Tensor]:\n        self._check_fused_update(force=True)\n        ret = {}\n        for task, metric_name, metric_value, prefix, description in self._tasks_iter(\n            \"\"\n        ):\n            metric_key = compose_metric_key(\n                self._namespace, task.name, metric_name, prefix, description\n            )\n            ret[metric_key] = metric_value\n        return ret\n\n    def local_compute(self) -> Dict[str, torch.Tensor]:\n        self._check_fused_update(force=True)\n        ret = {}\n        for task, metric_name, metric_value, prefix, description in self._tasks_iter(\n            \"local_\"\n        ):\n            metric_key = compose_metric_key(\n                self._namespace, task.name, metric_name, prefix, description\n            )\n            ret[metric_key] = metric_value\n        return ret\n\n    def sync(self) -> None:\n        for computation in self._metrics_computations:\n            computation.sync()\n\n    def unsync(self) -> None:\n        for computation in self._metrics_computations:\n            if computation._is_synced:\n                computation.unsync()\n\n    def reset(self) -> None:\n        for computation in self._metrics_computations:\n            computation.reset()\n\n    def get_memory_usage(self) -> Dict[torch.Tensor, int]:\n        r\"\"\"Estimates the memory of the rec metric instance's\n        underlying tensors; returns the map of tensor to size\n        \"\"\"\n        tensor_map = {}\n        attributes_q = deque(self.__dict__.values())\n        while attributes_q:\n            attribute = attributes_q.popleft()\n            if isinstance(attribute, torch.Tensor):\n                tensor_map[attribute] = (\n                    attribute.size().numel() * attribute.element_size()\n                )\n            elif isinstance(attribute, WindowBuffer):\n                attributes_q.extend(attribute.buffers)\n            elif isinstance(attribute, Mapping):\n                attributes_q.extend(attribute.values())\n            elif isinstance(attribute, Sequence) and not isinstance(attribute, str):\n                attributes_q.extend(attribute)\n            elif hasattr(attribute, \"__dict__\") and not isinstance(attribute, Enum):\n                attributes_q.extend(attribute.__dict__.values())\n        return tensor_map\n\n    # pyre-fixme[14]: `state_dict` overrides method defined in `Module` inconsistently.\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, torch.Tensor]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, torch.Tensor]:\n        # We need to flush the cached output to ensure checkpointing correctness.\n        self._check_fused_update(force=True)\n        destination = super().state_dict(\n            destination=destination, prefix=prefix, keep_vars=keep_vars\n        )\n        return self._metrics_computations.state_dict(\n            destination=destination,\n            prefix=f\"{prefix}_metrics_computations.\",\n            keep_vars=keep_vars,\n        )\n\n    def get_required_inputs(self) -> Set[str]:\n        return self._required_inputs",
  "class RecMetricList(nn.Module):\n    \"\"\"\n    A list module to encapulate multiple RecMetric instances and provide the\n    same interfaces as RecMetric.\n\n    Args:\n        rec_metrics (List[RecMetric]: the list of the input RecMetrics.\n\n    Call Args:\n        Not supported.\n\n    Returns:\n        Not supported.\n\n    Example::\n\n        ne = NEMetric(\n                 world_size=4,\n                 my_rank=0,\n                 batch_size=128,\n                 tasks=DefaultTaskInfo\n             )\n        metrics = RecMetricList([ne])\n    \"\"\"\n\n    rec_metrics: nn.ModuleList\n    required_inputs: Optional[List[str]]\n\n    def __init__(self, rec_metrics: List[RecMetric]) -> None:\n        # TODO(stellaya): consider to inherit from TorchMetrics.MetricCollection.\n        # The prequsite to use MetricCollection is that RecMetric inherits from\n        # TorchMetrics.Metric or TorchMetrics.MetricCollection\n\n        super().__init__()\n        self.rec_metrics = nn.ModuleList(rec_metrics)\n        self.required_inputs = (\n            list(\n                set().union(\n                    *[rec_metric.get_required_inputs() for rec_metric in rec_metrics]\n                )\n            )\n            or None\n        )\n\n    def __len__(self) -> int:\n        return len(self.rec_metrics)\n\n    def __getitem__(self, idx: int) -> nn.Module:\n        return self.rec_metrics[idx]\n\n    def get_required_inputs(self) -> Optional[List[str]]:\n        return self.required_inputs\n\n    def update(\n        self,\n        *,\n        predictions: RecModelOutput,\n        labels: RecModelOutput,\n        weights: RecModelOutput,\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        for metric in self.rec_metrics:\n            metric.update(\n                predictions=predictions, labels=labels, weights=weights, **kwargs\n            )\n\n    def compute(self) -> Dict[str, torch.Tensor]:\n        ret = {}\n        for metric in self.rec_metrics:\n            ret.update(metric.compute())\n        return ret\n\n    def local_compute(self) -> Dict[str, torch.Tensor]:\n        ret = {}\n        for metric in self.rec_metrics:\n            ret.update(metric.local_compute())\n        return ret\n\n    def sync(self) -> None:\n        for metric in self.rec_metrics:\n            metric.sync()\n\n    def unsync(self) -> None:\n        for metric in self.rec_metrics:\n            metric.unsync()\n\n    def reset(self) -> None:\n        for metric in self.rec_metrics:\n            metric.reset()",
  "def __init__(self, max_size: int, max_buffer_count: int) -> None:\n        self._max_size: int = max_size\n        self._max_buffer_count: int = max_buffer_count\n\n        self._buffers: Deque[torch.Tensor] = deque(maxlen=max_buffer_count)\n        self._used_sizes: Deque[int] = deque(maxlen=max_buffer_count)\n        self._window_used_size = 0",
  "def aggregate_state(\n        self, window_state: torch.Tensor, curr_state: torch.Tensor, size: int\n    ) -> None:\n        def remove(window_state: torch.Tensor) -> None:\n            window_state -= self._buffers.popleft()\n            self._window_used_size -= self._used_sizes.popleft()\n\n        if len(self._buffers) == self._buffers.maxlen:\n            remove(window_state)\n\n        self._buffers.append(curr_state)\n        self._used_sizes.append(size)\n        window_state += curr_state\n        self._window_used_size += size\n\n        while self._window_used_size > self._max_size:\n            remove(window_state)",
  "def buffers(self) -> Deque[torch.Tensor]:\n        return self._buffers",
  "def __init__(\n        self,\n        my_rank: int,\n        batch_size: int,\n        n_tasks: int,\n        window_size: int,\n        compute_on_all_ranks: bool = False,\n        should_validate_update: bool = False,\n        process_group: Optional[dist.ProcessGroup] = None,\n        fused_update_limit: int = 0,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(process_group=process_group, *args, **kwargs)\n\n        self._my_rank = my_rank\n        self._n_tasks = n_tasks\n        self._batch_size = batch_size\n        self._window_size = window_size\n        self._compute_on_all_ranks = compute_on_all_ranks\n        self._should_validate_update = should_validate_update\n        if self._window_size > 0:\n            self._batch_window_buffers = {}\n        else:\n            self._batch_window_buffers = None\n        if self._should_validate_update:\n            self._add_state(\n                \"has_valid_update\",\n                torch.zeros(self._n_tasks, dtype=torch.uint8),\n                add_window_state=False,\n                dist_reduce_fx=lambda x: torch.any(x, dim=0).byte(),\n                persistent=True,\n            )",
  "def get_window_state_name(state_name: str) -> str:\n        return f\"window_{state_name}\"",
  "def get_window_state(self, state_name: str) -> torch.Tensor:\n        return getattr(self, self.get_window_state_name(state_name))",
  "def _add_state(\n        self, name: str, default: DefaultValueT, add_window_state: bool, **kwargs: Any\n    ) -> None:\n        \"\"\"\n        name (str): the name of this state. The state will be accessible\n            with `self.THE_NAME_YOU_DEFINE`.\n        default (DefaultValueT): the initial value of this state. The most common\n            initial value is `torch.zeros(self._n_tasks, dtype=torch.float)`, but\n            users need to check the math formula to decide what is the correct\n            initial value for the metric. Note the `self._n_tasks` in the above\n            code. As a metric may handle multiple tasks at the same time, the\n            highest dimension of a state should be `self._n_tasks`.\n        add_window_state (bool): when this is True, a `window_{name}` state will\n            be created to record the window state information for this state.\n        dist_reduce_fx (str): the reduction function when aggregating the local\n            state. For example, tower_qps uses \u201csum\u201d to aggregate the total\n            trained examples.\n        persistent (bool): set this to True if you want to save/checkpoint the\n            metric and this state is required to compute the checkpointed metric.\n        \"\"\"\n        # pyre-fixme[6]: Expected `Union[List[typing.Any], torch.Tensor]` for 2nd\n        #  param but got `DefaultValueT`.\n        super().add_state(name, default, **kwargs)\n        if add_window_state:\n            if self._batch_window_buffers is None:\n                raise RuntimeError(\n                    \"Users is adding a window state while window metric is disabled.\"\n                )\n            kwargs[\"persistent\"] = False\n            window_state_name = self.get_window_state_name(name)\n            # Avoid pyre error\n            assert isinstance(default, torch.Tensor)\n            super().add_state(window_state_name, default.detach().clone(), **kwargs)\n\n            self._batch_window_buffers[window_state_name] = WindowBuffer(\n                max_size=self._window_size,\n                max_buffer_count=MAX_BUFFER_COUNT,\n            )",
  "def _aggregate_window_state(\n        self, state_name: str, state: torch.Tensor, num_samples: int\n    ) -> None:\n        if self._batch_window_buffers is None:\n            raise RuntimeError(\n                \"Users is adding a window state while window metric is disabled.\"\n            )\n        window_state_name = self.get_window_state_name(state_name)\n        assert self._batch_window_buffers is not None\n        self._batch_window_buffers[window_state_name].aggregate_state(\n            getattr(self, window_state_name), curr_state=state, size=num_samples\n        )",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:  # pragma: no cover\n        pass",
  "def _compute(self) -> List[MetricComputationReport]:  # pragma: no cover\n        pass",
  "def pre_compute(self) -> None:\n        r\"\"\"If a metric need to do some work before `compute()`, the metric\n        has to override this `pre_compute()`. One possible usage is to do\n        some pre-processing of the local state before `compute()` as TorchMetric\n        wraps `RecMetricComputation.compute()` and will do the global aggregation\n        before `RecMetricComputation.compute()` is called.\n        \"\"\"\n        return",
  "def compute(self) -> List[MetricComputationReport]:\n        with record_function(f\"## {self.__class__.__name__}:compute ##\"):\n            if self._my_rank == 0 or self._compute_on_all_ranks:\n                return self._compute()\n            else:\n                return []",
  "def local_compute(self) -> List[MetricComputationReport]:\n        return self._compute()",
  "def reset(self) -> None:\n        super().reset()\n        if self._batch_window_buffers is not None:\n            self._batch_window_buffers = {\n                name: WindowBuffer(\n                    max_size=self._window_size,\n                    max_buffer_count=MAX_BUFFER_COUNT,\n                )\n                for name in self._batch_window_buffers\n            }",
  "def __init__(\n        self,\n        world_size: int,\n        my_rank: int,\n        batch_size: int,\n        tasks: List[RecTaskInfo],\n        compute_mode: RecComputeMode = RecComputeMode.UNFUSED_TASKS_COMPUTATION,\n        window_size: int = 100,\n        fused_update_limit: int = 0,\n        compute_on_all_ranks: bool = False,\n        should_validate_update: bool = False,\n        process_group: Optional[dist.ProcessGroup] = None,\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        torch._C._log_api_usage_once(\n            f\"torchrec.metrics.rec_metric.{self.__class__.__name__}\"\n        )\n        # TODO(stellaya): consider to inherit from TorchMetrics.Metric or\n        # TorchMetrics.MetricCollection.\n        if (\n            compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION\n            and fused_update_limit > 0\n        ):\n            raise ValueError(\n                \"The fused tasks computation and the fused update cannot be set at the same time\"\n            )\n        super().__init__()\n        self._world_size = world_size\n        self._my_rank = my_rank\n        self._window_size = math.ceil(window_size / world_size)\n        self._batch_size = batch_size\n        self._metrics_computations = nn.ModuleList()\n        self._tasks = tasks\n        self._compute_mode = compute_mode\n        self._fused_update_limit = fused_update_limit\n        self._should_validate_update = should_validate_update\n        self._default_weights = {}\n        self._required_inputs = set()\n        self._update_buffers = {\n            self.PREDICTIONS: [],\n            self.LABELS: [],\n            self.WEIGHTS: [],\n        }\n        if self._window_size < self._batch_size:\n            raise ValueError(\n                f\"Local window size must be larger than batch size. Got local window size {self._window_size} and batch size {self._batch_size}.\"\n            )\n\n        if compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION:\n            task_per_metric = len(self._tasks)\n            self._tasks_iter = self._fused_tasks_iter\n        else:\n            task_per_metric = 1\n            self._tasks_iter = self._unfused_tasks_iter\n\n        for task_config in (\n            [self._tasks]\n            if compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION\n            else self._tasks\n        ):\n            # pyre-ignore\n            kwargs[\"fused_update_limit\"] = fused_update_limit\n            # This Pyre error seems to be Pyre's bug as it can be inferred by mypy\n            # according to https://github.com/python/mypy/issues/3048.\n            # pyre-fixme[45]: Cannot instantiate abstract class `RecMetricCoputation`.\n            metric_computation = self._computation_class(\n                my_rank,\n                batch_size,\n                task_per_metric,\n                self._window_size,\n                compute_on_all_ranks,\n                self._should_validate_update,\n                process_group,\n                **{**kwargs, **self._get_task_kwargs(task_config)},\n            )\n            required_inputs = self._get_task_required_inputs(task_config)\n\n            self._metrics_computations.append(metric_computation)\n            self._required_inputs.update(required_inputs)",
  "def _get_task_kwargs(\n        self, task_config: Union[RecTaskInfo, List[RecTaskInfo]]\n    ) -> Dict[str, Any]:\n        return {}",
  "def _get_task_required_inputs(\n        self, task_config: Union[RecTaskInfo, List[RecTaskInfo]]\n    ) -> Set[str]:\n        return set()",
  "def _fused_tasks_iter(self, compute_scope: str) -> ComputeIterType:\n        assert len(self._metrics_computations) == 1\n        self._metrics_computations[0].pre_compute()\n        for metric_report in getattr(\n            self._metrics_computations[0], compute_scope + \"compute\"\n        )():\n            for task, metric_value, has_valid_update in zip(\n                self._tasks,\n                metric_report.value,\n                self._metrics_computations[0].has_valid_update\n                if self._should_validate_update\n                else itertools.repeat(\n                    1\n                ),  # has_valid_update > 0 means the update is valid\n            ):\n                # The attribute has_valid_update is a tensor whose length equals to the\n                # number of tasks. Each value in it is corresponding to whether a task\n                # has valid updates or not.\n                # If for a task there's no valid updates, the calculated metric_value\n                # will be meaningless, so we mask it with the default value, i.e. 0.\n                valid_metric_value = (\n                    metric_value\n                    if has_valid_update > 0\n                    else torch.zeros_like(metric_value)\n                )\n                yield task, metric_report.name, valid_metric_value, compute_scope + metric_report.metric_prefix.value, metric_report.description",
  "def _unfused_tasks_iter(self, compute_scope: str) -> ComputeIterType:\n        for task, metric_computation in zip(self._tasks, self._metrics_computations):\n            metric_computation.pre_compute()\n            for metric_report in getattr(\n                metric_computation, compute_scope + \"compute\"\n            )():\n                # The attribute has_valid_update is a tensor with only 1 value\n                # corresponding to whether the task has valid updates or not.\n                # If there's no valid update, the calculated metric_report.value\n                # will be meaningless, so we mask it with the default value, i.e. 0.\n                valid_metric_value = (\n                    metric_report.value\n                    if not self._should_validate_update\n                    or metric_computation.has_valid_update[0] > 0\n                    else torch.zeros_like(metric_report.value)\n                )\n                yield task, metric_report.name, valid_metric_value, compute_scope + metric_report.metric_prefix.value, metric_report.description",
  "def _fuse_update_buffers(self) -> Dict[str, RecModelOutput]:\n        def fuse(outputs: List[RecModelOutput]) -> RecModelOutput:\n            assert len(outputs) > 0\n            if isinstance(outputs[0], torch.Tensor):\n                return torch.cat(cast(List[torch.Tensor], outputs))\n            else:\n                task_outputs: Dict[str, List[torch.Tensor]] = defaultdict(list)\n                for output in outputs:\n                    assert isinstance(output, dict)\n                    for task_name, tensor in output.items():\n                        task_outputs[task_name].append(tensor)\n                return {\n                    name: torch.cat(tensors) for name, tensors in task_outputs.items()\n                }\n\n        ret: Dict[str, RecModelOutput] = {}\n        for key, output_list in self._update_buffers.items():\n            if len(output_list) > 0:\n                ret[key] = fuse(output_list)\n            else:\n                assert key == self.WEIGHTS\n            output_list.clear()\n        return ret",
  "def _check_fused_update(self, force: bool) -> None:\n        if self._fused_update_limit <= 0:\n            return\n        if len(self._update_buffers[self.PREDICTIONS]) == 0:\n            return\n        if (\n            not force\n            and len(self._update_buffers[self.PREDICTIONS]) < self._fused_update_limit\n        ):\n            return\n        fused_arguments = self._fuse_update_buffers()\n        self._update(\n            predictions=fused_arguments[self.PREDICTIONS],\n            labels=fused_arguments[self.LABELS],\n            weights=fused_arguments.get(self.WEIGHTS, None),\n        )",
  "def _create_default_weights(self, predictions: torch.Tensor) -> torch.Tensor:\n        # pyre-fixme[6]: For 1st param expected `Tuple[int, ...]` but got `Size`.\n        weights = self._default_weights.get(predictions.size(), None)\n        if weights is None:\n            weights = torch.ones_like(predictions)\n            # pyre-fixme[6]: For 1st param expected `Tuple[int, ...]` but got `Size`.\n            self._default_weights[predictions.size()] = weights\n        return weights",
  "def _check_nonempty_weights(self, weights: torch.Tensor) -> torch.Tensor:\n        return torch.gt(torch.count_nonzero(weights, dim=-1), 0)",
  "def _update(\n        self,\n        *,\n        predictions: RecModelOutput,\n        labels: RecModelOutput,\n        weights: Optional[RecModelOutput],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        with torch.no_grad():\n            if self._compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION:\n                assert isinstance(predictions, torch.Tensor) and isinstance(\n                    labels, torch.Tensor\n                )\n\n                predictions = (\n                    # Reshape the predictions to size([len(self._tasks), self._batch_size])\n                    predictions.view(-1, self._batch_size)\n                    if predictions.dim() == labels.dim()\n                    # predictions.dim() == labels.dim() + 1 for multiclass models\n                    else predictions.view(-1, self._batch_size, predictions.size()[-1])\n                )\n                labels = labels.view(-1, self._batch_size)\n                if weights is None:\n                    weights = self._create_default_weights(predictions)\n                else:\n                    assert isinstance(weights, torch.Tensor)\n                    weights = weights.view(-1, self._batch_size)\n                if self._should_validate_update:\n                    # has_valid_weights is a tensor of bool whose length equals to the number\n                    # of tasks. Each value in it is corresponding to whether the weights\n                    # are valid, i.e. are set to non-zero values for that task in this update.\n                    # If has_valid_weights are Falses for all the tasks, we just ignore this\n                    # update.\n                    has_valid_weights = self._check_nonempty_weights(weights)\n                    if torch.any(has_valid_weights):\n                        self._metrics_computations[0].update(\n                            predictions=predictions,\n                            labels=labels,\n                            weights=weights,\n                            **kwargs,\n                        )\n                        self._metrics_computations[0].has_valid_update.logical_or_(\n                            has_valid_weights\n                        )\n                else:\n                    self._metrics_computations[0].update(\n                        predictions=predictions,\n                        labels=labels,\n                        weights=weights,\n                        **kwargs,\n                    )\n            else:\n                for task, metric_ in zip(self._tasks, self._metrics_computations):\n                    if task.name not in predictions:\n                        continue\n                    # pyre-fixme[6]: For 1st argument expected `Union[None,\n                    #  List[typing.Any], int, slice, Tensor, typing.Tuple[typing.Any,\n                    #  ...]]` but got `str`.\n                    if torch.numel(predictions[task.name]) == 0:\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        assert torch.numel(labels[task.name]) == 0\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        assert weights is None or torch.numel(weights[task.name]) == 0\n                        continue\n                    task_predictions = (\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        predictions[task.name].view(1, -1)\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        if predictions[task.name].dim() == labels[task.name].dim()\n                        # predictions[task.name].dim() == labels[task.name].dim() + 1 for multiclass models\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        else predictions[task.name].view(\n                            1,\n                            -1,\n                            predictions[\n                                task.name  # pyre-fixme[6]: For 1st argument expected `Union[None,\n                                #  List[typing.Any], int, slice, Tensor,\n                                #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                            ].size()[-1],\n                        )\n                    )\n                    # pyre-fixme[6]: For 1st argument expected `Union[None,\n                    #  List[typing.Any], int, slice, Tensor, typing.Tuple[typing.Any,\n                    #  ...]]` but got `str`.\n                    task_labels = labels[task.name].view(1, -1)\n                    if weights is None:\n                        task_weights = self._create_default_weights(task_predictions)\n                    else:\n                        # pyre-fixme[6]: For 1st argument expected `Union[None,\n                        #  List[typing.Any], int, slice, Tensor,\n                        #  typing.Tuple[typing.Any, ...]]` but got `str`.\n                        task_weights = weights[task.name].view(1, -1)\n                    if self._should_validate_update:\n                        # has_valid_weights is a tensor with only 1 value corresponding to\n                        # whether the weights are valid, i.e. are set to non-zero values for\n                        # the task in this update.\n                        # If has_valid_update[0] is False, we just ignore this update.\n                        has_valid_weights = self._check_nonempty_weights(task_weights)\n                        if has_valid_weights[0]:\n                            metric_.has_valid_update.logical_or_(has_valid_weights)\n                        else:\n                            continue\n                    if \"required_inputs\" in kwargs:\n                        kwargs[\"required_inputs\"] = {\n                            k: v.view(task_labels.size())\n                            for k, v in kwargs[\"required_inputs\"].items()\n                        }\n                    metric_.update(\n                        predictions=task_predictions,\n                        labels=task_labels,\n                        weights=task_weights,\n                        **kwargs,\n                    )",
  "def update(\n        self,\n        *,\n        predictions: RecModelOutput,\n        labels: RecModelOutput,\n        weights: Optional[RecModelOutput],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        with record_function(f\"## {self.__class__.__name__}:update ##\"):\n            if self._fused_update_limit > 0:\n                self._update_buffers[self.PREDICTIONS].append(predictions)\n                self._update_buffers[self.LABELS].append(labels)\n                if weights is not None:\n                    self._update_buffers[self.WEIGHTS].append(weights)\n                self._check_fused_update(force=False)\n            else:\n                self._update(\n                    predictions=predictions, labels=labels, weights=weights, **kwargs\n                )",
  "def compute(self) -> Dict[str, torch.Tensor]:\n        self._check_fused_update(force=True)\n        ret = {}\n        for task, metric_name, metric_value, prefix, description in self._tasks_iter(\n            \"\"\n        ):\n            metric_key = compose_metric_key(\n                self._namespace, task.name, metric_name, prefix, description\n            )\n            ret[metric_key] = metric_value\n        return ret",
  "def local_compute(self) -> Dict[str, torch.Tensor]:\n        self._check_fused_update(force=True)\n        ret = {}\n        for task, metric_name, metric_value, prefix, description in self._tasks_iter(\n            \"local_\"\n        ):\n            metric_key = compose_metric_key(\n                self._namespace, task.name, metric_name, prefix, description\n            )\n            ret[metric_key] = metric_value\n        return ret",
  "def sync(self) -> None:\n        for computation in self._metrics_computations:\n            computation.sync()",
  "def unsync(self) -> None:\n        for computation in self._metrics_computations:\n            if computation._is_synced:\n                computation.unsync()",
  "def reset(self) -> None:\n        for computation in self._metrics_computations:\n            computation.reset()",
  "def get_memory_usage(self) -> Dict[torch.Tensor, int]:\n        r\"\"\"Estimates the memory of the rec metric instance's\n        underlying tensors; returns the map of tensor to size\n        \"\"\"\n        tensor_map = {}\n        attributes_q = deque(self.__dict__.values())\n        while attributes_q:\n            attribute = attributes_q.popleft()\n            if isinstance(attribute, torch.Tensor):\n                tensor_map[attribute] = (\n                    attribute.size().numel() * attribute.element_size()\n                )\n            elif isinstance(attribute, WindowBuffer):\n                attributes_q.extend(attribute.buffers)\n            elif isinstance(attribute, Mapping):\n                attributes_q.extend(attribute.values())\n            elif isinstance(attribute, Sequence) and not isinstance(attribute, str):\n                attributes_q.extend(attribute)\n            elif hasattr(attribute, \"__dict__\") and not isinstance(attribute, Enum):\n                attributes_q.extend(attribute.__dict__.values())\n        return tensor_map",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, torch.Tensor]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, torch.Tensor]:\n        # We need to flush the cached output to ensure checkpointing correctness.\n        self._check_fused_update(force=True)\n        destination = super().state_dict(\n            destination=destination, prefix=prefix, keep_vars=keep_vars\n        )\n        return self._metrics_computations.state_dict(\n            destination=destination,\n            prefix=f\"{prefix}_metrics_computations.\",\n            keep_vars=keep_vars,\n        )",
  "def get_required_inputs(self) -> Set[str]:\n        return self._required_inputs",
  "def __init__(self, rec_metrics: List[RecMetric]) -> None:\n        # TODO(stellaya): consider to inherit from TorchMetrics.MetricCollection.\n        # The prequsite to use MetricCollection is that RecMetric inherits from\n        # TorchMetrics.Metric or TorchMetrics.MetricCollection\n\n        super().__init__()\n        self.rec_metrics = nn.ModuleList(rec_metrics)\n        self.required_inputs = (\n            list(\n                set().union(\n                    *[rec_metric.get_required_inputs() for rec_metric in rec_metrics]\n                )\n            )\n            or None\n        )",
  "def __len__(self) -> int:\n        return len(self.rec_metrics)",
  "def __getitem__(self, idx: int) -> nn.Module:\n        return self.rec_metrics[idx]",
  "def get_required_inputs(self) -> Optional[List[str]]:\n        return self.required_inputs",
  "def update(\n        self,\n        *,\n        predictions: RecModelOutput,\n        labels: RecModelOutput,\n        weights: RecModelOutput,\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        for metric in self.rec_metrics:\n            metric.update(\n                predictions=predictions, labels=labels, weights=weights, **kwargs\n            )",
  "def compute(self) -> Dict[str, torch.Tensor]:\n        ret = {}\n        for metric in self.rec_metrics:\n            ret.update(metric.compute())\n        return ret",
  "def local_compute(self) -> Dict[str, torch.Tensor]:\n        ret = {}\n        for metric in self.rec_metrics:\n            ret.update(metric.local_compute())\n        return ret",
  "def sync(self) -> None:\n        for metric in self.rec_metrics:\n            metric.sync()",
  "def unsync(self) -> None:\n        for metric in self.rec_metrics:\n            metric.unsync()",
  "def reset(self) -> None:\n        for metric in self.rec_metrics:\n            metric.reset()",
  "def remove(window_state: torch.Tensor) -> None:\n            window_state -= self._buffers.popleft()\n            self._window_used_size -= self._used_sizes.popleft()",
  "def fuse(outputs: List[RecModelOutput]) -> RecModelOutput:\n            assert len(outputs) > 0\n            if isinstance(outputs[0], torch.Tensor):\n                return torch.cat(cast(List[torch.Tensor], outputs))\n            else:\n                task_outputs: Dict[str, List[torch.Tensor]] = defaultdict(list)\n                for output in outputs:\n                    assert isinstance(output, dict)\n                    for task_name, tensor in output.items():\n                        task_outputs[task_name].append(tensor)\n                return {\n                    name: torch.cat(tensors) for name, tensors in task_outputs.items()\n                }",
  "def _compute_auc_helper(\n    predictions: torch.Tensor, labels: torch.Tensor, weights: torch.Tensor\n) -> torch.Tensor:\n    sorted_indices = torch.argsort(predictions, descending=True, dim=-1)\n    sorted_labels = torch.index_select(labels, dim=0, index=sorted_indices)\n    sorted_weights = torch.index_select(weights, dim=0, index=sorted_indices)\n    cum_fp = torch.cumsum(sorted_weights * (1.0 - sorted_labels), dim=0)\n    cum_tp = torch.cumsum(sorted_weights * sorted_labels, dim=0)\n    auc = torch.where(\n        cum_fp[-1] * cum_tp[-1] == 0,\n        0.5,  # 0.5 is the no-signal default value for auc.\n        torch.trapz(cum_tp, cum_fp) / cum_fp[-1] / cum_tp[-1],\n    )\n    return auc",
  "def compute_auc(\n    n_tasks: int, predictions: torch.Tensor, labels: torch.Tensor, weights: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    Computes AUC (Area Under the Curve) for binary classification.\n\n    Args:\n        n_tasks (int): number of tasks.\n        predictions (torch.Tensor): tensor of size (n_tasks, n_examples).\n        labels (torch.Tensor): tensor of size (n_tasks, n_examples).\n        weights (torch.Tensor): tensor of size (n_tasks, n_examples).\n    \"\"\"\n    aucs = []\n    for predictions_i, labels_i, weights_i in zip(predictions, labels, weights):\n        auc = _compute_auc_helper(predictions_i, labels_i, weights_i)\n        aucs.append(auc.view(1))\n    return torch.cat(aucs)",
  "def compute_auc_per_group(\n    n_tasks: int,\n    predictions: torch.Tensor,\n    labels: torch.Tensor,\n    weights: torch.Tensor,\n    grouping_keys: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    Computes AUC (Area Under the Curve) for binary classification for groups of predictions/labels.\n    Args:\n        n_tasks (int): number of tasks\n        predictions (torch.Tensor): tensor of size (n_tasks, n_examples)\n        labels (torch.Tensor): tensor of size (n_tasks, n_examples)\n        weights (torch.Tensor): tensor of size (n_tasks, n_examples)\n        grouping_keys (torch.Tensor): tensor of size (n_examples,)\n\n    Returns:\n        torch.Tensor: tensor of size (n_tasks,), average of AUCs per group.\n    \"\"\"\n    aucs = []\n    if grouping_keys.numel() != 0 and grouping_keys[0] == -1:\n        # we added padding  as the first elements during init to avoid floating point exception in sync()\n        # removing the paddings to avoid numerical errors.\n        grouping_keys = grouping_keys[1:]\n        predictions = predictions[:, 1:]\n        labels = labels[:, 1:]\n        weights = weights[:, 1:]\n\n    # get unique group indices\n    group_indices = torch.unique(grouping_keys)\n\n    for (predictions_i, labels_i, weights_i) in zip(predictions, labels, weights):\n        # Loop over each group\n        auc_groups_sum = torch.tensor([0], dtype=torch.float32)\n        for group_idx in group_indices:\n            # get predictions, labels, and weights for this group\n            group_mask = grouping_keys == group_idx\n            grouped_predictions = predictions_i[group_mask]\n            grouped_labels = labels_i[group_mask]\n            grouped_weights = weights_i[group_mask]\n\n            auc = _compute_auc_helper(\n                grouped_predictions, grouped_labels, grouped_weights\n            )\n            auc_groups_sum = auc_groups_sum.to(auc.device)\n            auc_groups_sum += auc.view(1)\n        avg_auc = (\n            auc_groups_sum / len(group_indices)\n            if len(group_indices) > 0\n            else torch.tensor([0.5], dtype=torch.float32)\n        )\n        aucs.append(avg_auc)\n    return torch.cat(aucs)",
  "def _state_reduction(state: List[torch.Tensor], dim: int = 1) -> List[torch.Tensor]:\n    return [torch.cat(state, dim=dim)]",
  "class AUCMetricComputation(RecMetricComputation):\n    r\"\"\"\n    This class implements the RecMetricComputation for AUC, i.e. Area Under the Curve.\n\n    The constructor arguments are defined in RecMetricComputation.\n    See the docstring of RecMetricComputation for more detail.\n    Args:\n        grouped_auc (bool): If True, computes AUC per group and returns average AUC across all groups.\n            The `grouping_keys` is provided during state updates along with predictions, labels, weights.\n            This feature is currently not enabled for `fused_update_limit`.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        grouped_auc: bool = False,\n        fused_update_limit: int = 0,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(*args, **kwargs)\n        if grouped_auc and fused_update_limit > 0:\n            raise RecMetricException(\n                \"Grouped AUC and Fused Update Limit cannot be enabled together yet.\"\n            )\n\n        self._grouped_auc: bool = grouped_auc\n        self._add_state(\n            PREDICTIONS,\n            [],\n            add_window_state=False,\n            dist_reduce_fx=_state_reduction,\n            persistent=False,\n        )\n        self._add_state(\n            LABELS,\n            [],\n            add_window_state=False,\n            dist_reduce_fx=_state_reduction,\n            persistent=False,\n        )\n        self._add_state(\n            WEIGHTS,\n            [],\n            add_window_state=False,\n            dist_reduce_fx=_state_reduction,\n            persistent=False,\n        )\n        if self._grouped_auc:\n            self._add_state(\n                GROUPING_KEYS,\n                [],\n                add_window_state=False,\n                dist_reduce_fx=_grouping_keys_state_reduction,\n                persistent=False,\n            )\n        self._init_states()\n\n    # The states values are set to empty lists in __init__() and reset(), and then we\n    # add a size (self._n_tasks, 1) tensor to each of the list as the initial values\n    # This is to bypass the limitation of state aggregation in TorchMetrics sync() when\n    # we try to checkpoint the states before update()\n    # The reason for using lists here is to avoid automatically stacking the tensors from\n    # all the trainers into one tensor in sync()\n    # The reason for using non-empty tensors as the first elements is to avoid the\n    # floating point exception thrown in sync() for aggregating empty tensors\n    def _init_states(self) -> None:\n        if len(getattr(self, PREDICTIONS)) > 0:\n            return\n\n        getattr(self, PREDICTIONS).append(\n            torch.zeros((self._n_tasks, 1), dtype=torch.float, device=self.device)\n        )\n        getattr(self, LABELS).append(\n            torch.zeros((self._n_tasks, 1), dtype=torch.float, device=self.device)\n        )\n        getattr(self, WEIGHTS).append(\n            torch.zeros((self._n_tasks, 1), dtype=torch.float, device=self.device)\n        )\n        if self._grouped_auc:\n            getattr(self, GROUPING_KEYS).append(torch.tensor([-1], device=self.device))\n\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        \"\"\"\n        Args:\n            predictions (torch.Tensor): tensor of size (n_task, n_examples)\n            labels (torch.Tensor): tensor of size (n_task, n_examples)\n            weights (torch.Tensor): tensor of size (n_task, n_examples)\n            grouping_key (torch.Tensor): Optional tensor of size (1, n_examples) that specifies the groups of\n                    predictions/labels per batch. If provided, the AUC metric also\n                    computes AUC per group and returns the average AUC across all groups.\n        \"\"\"\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for AUCMetricComputation update\"\n            )\n        predictions = predictions.float()\n        labels = labels.float()\n        weights = weights.float()\n        num_samples = getattr(self, PREDICTIONS)[0].size(-1)\n        batch_size = predictions.size(-1)\n        start_index = max(num_samples + batch_size - self._window_size, 0)\n        # Using `self.predictions =` will cause Pyre errors.\n        getattr(self, PREDICTIONS)[0] = torch.cat(\n            [\n                cast(torch.Tensor, getattr(self, PREDICTIONS)[0])[:, start_index:],\n                predictions,\n            ],\n            dim=-1,\n        )\n        getattr(self, LABELS)[0] = torch.cat(\n            [cast(torch.Tensor, getattr(self, LABELS)[0])[:, start_index:], labels],\n            dim=-1,\n        )\n        getattr(self, WEIGHTS)[0] = torch.cat(\n            [cast(torch.Tensor, getattr(self, WEIGHTS)[0])[:, start_index:], weights],\n            dim=-1,\n        )\n        if self._grouped_auc:\n            if REQUIRED_INPUTS not in kwargs or (\n                (grouping_keys := kwargs[REQUIRED_INPUTS].get(GROUPING_KEYS)) is None\n            ):\n                raise RecMetricException(\n                    f\"Input '{GROUPING_KEYS}' are required for AUCMetricComputation grouped update\"\n                )\n            getattr(self, GROUPING_KEYS)[0] = torch.cat(\n                [\n                    cast(torch.Tensor, getattr(self, GROUPING_KEYS)[0])[start_index:],\n                    grouping_keys.squeeze(),\n                ],\n                dim=0,\n            )\n\n    def _compute(self) -> List[MetricComputationReport]:\n        reports = [\n            MetricComputationReport(\n                name=MetricName.AUC,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_auc(\n                    self._n_tasks,\n                    cast(torch.Tensor, getattr(self, PREDICTIONS)[0]),\n                    cast(torch.Tensor, getattr(self, LABELS)[0]),\n                    cast(torch.Tensor, getattr(self, WEIGHTS)[0]),\n                ),\n            )\n        ]\n        if self._grouped_auc:\n            reports.append(\n                MetricComputationReport(\n                    name=MetricName.GROUPED_AUC,\n                    metric_prefix=MetricPrefix.WINDOW,\n                    value=compute_auc_per_group(\n                        self._n_tasks,\n                        cast(torch.Tensor, getattr(self, PREDICTIONS)[0]),\n                        cast(torch.Tensor, getattr(self, LABELS)[0]),\n                        cast(torch.Tensor, getattr(self, WEIGHTS)[0]),\n                        cast(torch.Tensor, getattr(self, GROUPING_KEYS)[0]),\n                    ),\n                )\n            )\n        return reports\n\n    def reset(self) -> None:\n        super().reset()\n        self._init_states()",
  "class AUCMetric(RecMetric):\n    _namespace: MetricNamespace = MetricNamespace.AUC\n    _computation_class: Type[RecMetricComputation] = AUCMetricComputation\n\n    def __init__(\n        self,\n        world_size: int,\n        my_rank: int,\n        batch_size: int,\n        tasks: List[RecTaskInfo],\n        compute_mode: RecComputeMode = RecComputeMode.UNFUSED_TASKS_COMPUTATION,\n        window_size: int = 100,\n        fused_update_limit: int = 0,\n        compute_on_all_ranks: bool = False,\n        should_validate_update: bool = False,\n        process_group: Optional[dist.ProcessGroup] = None,\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        super().__init__(\n            world_size=world_size,\n            my_rank=my_rank,\n            batch_size=batch_size,\n            tasks=tasks,\n            compute_mode=compute_mode,\n            window_size=window_size,\n            fused_update_limit=fused_update_limit,\n            compute_on_all_ranks=compute_on_all_ranks,\n            should_validate_update=should_validate_update,\n            process_group=process_group,\n            **kwargs,\n        )\n        if kwargs.get(\"grouped_auc\"):\n            self._required_inputs.add(GROUPING_KEYS)",
  "def __init__(\n        self,\n        *args: Any,\n        grouped_auc: bool = False,\n        fused_update_limit: int = 0,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(*args, **kwargs)\n        if grouped_auc and fused_update_limit > 0:\n            raise RecMetricException(\n                \"Grouped AUC and Fused Update Limit cannot be enabled together yet.\"\n            )\n\n        self._grouped_auc: bool = grouped_auc\n        self._add_state(\n            PREDICTIONS,\n            [],\n            add_window_state=False,\n            dist_reduce_fx=_state_reduction,\n            persistent=False,\n        )\n        self._add_state(\n            LABELS,\n            [],\n            add_window_state=False,\n            dist_reduce_fx=_state_reduction,\n            persistent=False,\n        )\n        self._add_state(\n            WEIGHTS,\n            [],\n            add_window_state=False,\n            dist_reduce_fx=_state_reduction,\n            persistent=False,\n        )\n        if self._grouped_auc:\n            self._add_state(\n                GROUPING_KEYS,\n                [],\n                add_window_state=False,\n                dist_reduce_fx=_grouping_keys_state_reduction,\n                persistent=False,\n            )\n        self._init_states()",
  "def _init_states(self) -> None:\n        if len(getattr(self, PREDICTIONS)) > 0:\n            return\n\n        getattr(self, PREDICTIONS).append(\n            torch.zeros((self._n_tasks, 1), dtype=torch.float, device=self.device)\n        )\n        getattr(self, LABELS).append(\n            torch.zeros((self._n_tasks, 1), dtype=torch.float, device=self.device)\n        )\n        getattr(self, WEIGHTS).append(\n            torch.zeros((self._n_tasks, 1), dtype=torch.float, device=self.device)\n        )\n        if self._grouped_auc:\n            getattr(self, GROUPING_KEYS).append(torch.tensor([-1], device=self.device))",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        \"\"\"\n        Args:\n            predictions (torch.Tensor): tensor of size (n_task, n_examples)\n            labels (torch.Tensor): tensor of size (n_task, n_examples)\n            weights (torch.Tensor): tensor of size (n_task, n_examples)\n            grouping_key (torch.Tensor): Optional tensor of size (1, n_examples) that specifies the groups of\n                    predictions/labels per batch. If provided, the AUC metric also\n                    computes AUC per group and returns the average AUC across all groups.\n        \"\"\"\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for AUCMetricComputation update\"\n            )\n        predictions = predictions.float()\n        labels = labels.float()\n        weights = weights.float()\n        num_samples = getattr(self, PREDICTIONS)[0].size(-1)\n        batch_size = predictions.size(-1)\n        start_index = max(num_samples + batch_size - self._window_size, 0)\n        # Using `self.predictions =` will cause Pyre errors.\n        getattr(self, PREDICTIONS)[0] = torch.cat(\n            [\n                cast(torch.Tensor, getattr(self, PREDICTIONS)[0])[:, start_index:],\n                predictions,\n            ],\n            dim=-1,\n        )\n        getattr(self, LABELS)[0] = torch.cat(\n            [cast(torch.Tensor, getattr(self, LABELS)[0])[:, start_index:], labels],\n            dim=-1,\n        )\n        getattr(self, WEIGHTS)[0] = torch.cat(\n            [cast(torch.Tensor, getattr(self, WEIGHTS)[0])[:, start_index:], weights],\n            dim=-1,\n        )\n        if self._grouped_auc:\n            if REQUIRED_INPUTS not in kwargs or (\n                (grouping_keys := kwargs[REQUIRED_INPUTS].get(GROUPING_KEYS)) is None\n            ):\n                raise RecMetricException(\n                    f\"Input '{GROUPING_KEYS}' are required for AUCMetricComputation grouped update\"\n                )\n            getattr(self, GROUPING_KEYS)[0] = torch.cat(\n                [\n                    cast(torch.Tensor, getattr(self, GROUPING_KEYS)[0])[start_index:],\n                    grouping_keys.squeeze(),\n                ],\n                dim=0,\n            )",
  "def _compute(self) -> List[MetricComputationReport]:\n        reports = [\n            MetricComputationReport(\n                name=MetricName.AUC,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_auc(\n                    self._n_tasks,\n                    cast(torch.Tensor, getattr(self, PREDICTIONS)[0]),\n                    cast(torch.Tensor, getattr(self, LABELS)[0]),\n                    cast(torch.Tensor, getattr(self, WEIGHTS)[0]),\n                ),\n            )\n        ]\n        if self._grouped_auc:\n            reports.append(\n                MetricComputationReport(\n                    name=MetricName.GROUPED_AUC,\n                    metric_prefix=MetricPrefix.WINDOW,\n                    value=compute_auc_per_group(\n                        self._n_tasks,\n                        cast(torch.Tensor, getattr(self, PREDICTIONS)[0]),\n                        cast(torch.Tensor, getattr(self, LABELS)[0]),\n                        cast(torch.Tensor, getattr(self, WEIGHTS)[0]),\n                        cast(torch.Tensor, getattr(self, GROUPING_KEYS)[0]),\n                    ),\n                )\n            )\n        return reports",
  "def reset(self) -> None:\n        super().reset()\n        self._init_states()",
  "def __init__(\n        self,\n        world_size: int,\n        my_rank: int,\n        batch_size: int,\n        tasks: List[RecTaskInfo],\n        compute_mode: RecComputeMode = RecComputeMode.UNFUSED_TASKS_COMPUTATION,\n        window_size: int = 100,\n        fused_update_limit: int = 0,\n        compute_on_all_ranks: bool = False,\n        should_validate_update: bool = False,\n        process_group: Optional[dist.ProcessGroup] = None,\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        super().__init__(\n            world_size=world_size,\n            my_rank=my_rank,\n            batch_size=batch_size,\n            tasks=tasks,\n            compute_mode=compute_mode,\n            window_size=window_size,\n            fused_update_limit=fused_update_limit,\n            compute_on_all_ranks=compute_on_all_ranks,\n            should_validate_update=should_validate_update,\n            process_group=process_group,\n            **kwargs,\n        )\n        if kwargs.get(\"grouped_auc\"):\n            self._required_inputs.add(GROUPING_KEYS)",
  "def is_empty_signals(\n    labels: torch.Tensor,\n    predictions: torch.Tensor,\n    weights: torch.Tensor,\n) -> bool:\n    return (\n        torch.numel(labels) <= 0\n        and torch.numel(predictions) <= 0\n        and torch.numel(weights) <= 0\n    )",
  "def parse_model_outputs(\n    label_name: str,\n    prediction_name: str,\n    weight_name: str,\n    model_out: Dict[str, torch.Tensor],\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:\n    labels = model_out[label_name].squeeze()\n    if not prediction_name:\n        assert not weight_name, \"weight name must be empty if prediction name is empty\"\n        return (labels, None, None)\n    assert isinstance(labels, torch.Tensor)\n    predictions = model_out[prediction_name].squeeze()\n    assert isinstance(predictions, torch.Tensor)\n    weights = model_out[weight_name].squeeze()\n    assert isinstance(weights, torch.Tensor)\n\n    if not is_empty_signals(labels, predictions, weights):\n        if labels.dim() == predictions.dim():\n            assert (torch.numel(labels) == torch.numel(predictions)) and (\n                torch.numel(labels) == torch.numel(weights)\n            ), (\n                \"Expect the same number of elements in labels, predictions, and weights. \"\n                f\"Instead got {torch.numel(labels)}, {torch.numel(predictions)}, \"\n                f\"{torch.numel(weights)}\"\n            )\n        else:  # For multiclass models, labels.size() = (batch_size), and predictions.size() = (batch_size, number_of_classes)\n            assert torch.numel(labels) == torch.numel(predictions) / predictions.size()[\n                -1\n            ] and torch.numel(labels) == torch.numel(weights)\n\n        # non-empty tensors need to have rank 1\n        if len(labels.size()) == 0:\n            labels = labels.unsqueeze(0)\n            predictions = predictions.unsqueeze(0)\n            weights = weights.unsqueeze(0)\n\n    return labels, predictions, weights",
  "def parse_required_inputs(\n    model_out: Dict[str, torch.Tensor], required_inputs_list: List[str]\n) -> Dict[str, torch.Tensor]:\n    required_inputs: Dict[str, torch.Tensor] = {}\n    for feature in required_inputs_list:\n        required_inputs[feature] = model_out[feature].squeeze()\n        assert isinstance(required_inputs[feature], torch.Tensor)\n    return required_inputs",
  "def parse_task_model_outputs(\n    tasks: List[RecTaskInfo],\n    model_out: Dict[str, torch.Tensor],\n    required_inputs_list: Optional[List[str]] = None,\n) -> Tuple[\n    Dict[str, torch.Tensor],\n    Dict[str, torch.Tensor],\n    Dict[str, torch.Tensor],\n    Dict[str, torch.Tensor],\n]:\n    all_labels: Dict[str, torch.Tensor] = {}\n    all_predictions: Dict[str, torch.Tensor] = {}\n    all_weights: Dict[str, torch.Tensor] = {}\n    all_required_inputs: Dict[str, torch.Tensor] = {}\n    for task in tasks:\n        labels, predictions, weights = parse_model_outputs(\n            task.label_name, task.prediction_name, task.weight_name, model_out\n        )\n        if predictions is not None and weights is not None:\n            if not is_empty_signals(labels, predictions, weights):\n                all_labels[task.name] = labels\n                all_predictions[task.name] = predictions\n                all_weights[task.name] = weights\n        else:\n            if torch.numel(labels) > 0:\n                all_labels[task.name] = labels\n\n    if required_inputs_list is not None:\n        all_required_inputs = parse_required_inputs(model_out, required_inputs_list)\n\n    return all_labels, all_predictions, all_weights, all_required_inputs",
  "def _validate_model_outputs(\n    labels: torch.Tensor,\n    predictions: torch.Tensor,\n    weights: torch.Tensor,\n    sessions: torch.Tensor,\n) -> None:\n    # check if tensors are of the same shape\n    assert labels.dim() == 2\n    assert labels.shape == predictions.shape\n    assert labels.shape == weights.shape\n    assert labels.shape == sessions.shape",
  "def ranking_within_session(\n    predictions: torch.Tensor,\n    session: torch.Tensor,\n) -> torch.Tensor:\n    # rank predictions that belong to the same session\n\n    #  Example:\n    #  predictions = [1.0, 0.0, 0.51, 0.8, 1.0, 0.0, 0.51, 0.8, 1.0, 0.0, 0.51, 0.8]\n    #  sessions =    [1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1]\n    #  return =      [0, 5, 3, 2, 1, 6, 4, 1, 0, 4, 3, 2]\n    n_tasks = predictions.size(0)\n    matching_session_id = session.view(-1, n_tasks) == session.view(n_tasks, -1)\n    predictions_relation = predictions.view(-1, n_tasks) >= predictions.view(\n        n_tasks, -1\n    )\n    relation_within_session = matching_session_id & predictions_relation\n    rank_within_session = torch.sum(matching_session_id, dim=-1) - torch.sum(\n        relation_within_session, dim=-1\n    )\n    return rank_within_session",
  "def _calc_num_true_pos(\n    labels: torch.Tensor, predictions: torch.Tensor, weights: torch.Tensor\n) -> torch.Tensor:\n    # predictions are expected to be 0 or 1 integers.\n    num_true_pos = torch.sum(weights * labels * (predictions == 1).double(), dim=-1)\n    return num_true_pos",
  "def _calc_num_false_neg(\n    labels: torch.Tensor, predictions: torch.Tensor, weights: torch.Tensor\n) -> torch.Tensor:\n    # predictions are expected to be 0 or 1 integers.\n    num_false_neg = torch.sum(weights * labels * (predictions == 0).double(), dim=-1)\n    return num_false_neg",
  "def _calc_recall(\n    num_true_pos: torch.Tensor, num_false_neg: torch.Tensor\n) -> torch.Tensor:\n    # if num_true_pos + num_false_neg == 0 then we set recall = NaN by default.\n    recall = torch.tensor([float(\"nan\")])\n    if (num_true_pos + num_false_neg).item() != 0:\n        recall = num_true_pos / (num_true_pos + num_false_neg)\n    else:\n        logger.warning(\n            \"Recall = NaN. Likely, it means that there were no positive examples passed to the metric yet.\"\n            \" Please, debug if you expect every batch to include positive examples.\"\n        )\n    return recall",
  "class RecallSessionMetricComputation(RecMetricComputation):\n    r\"\"\"\n    This class implements the RecMetricComputation for Recall on session level.\n\n    The constructor arguments are defined in RecMetricComputation.\n    See the docstring of RecMetricComputation for more detail.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        session_metric_def: SessionMetricDef,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            NUM_TRUE_POS,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            NUM_FALSE_NEGATIVE,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self.top_threshold: Optional[int] = session_metric_def.top_threshold\n        self.run_ranking_of_labels: bool = session_metric_def.run_ranking_of_labels\n        self.session_var_name: Optional[str] = session_metric_def.session_var_name\n\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        \"\"\"\n        Args:\n            predictions (torch.Tensor): tensor of size (n_task, n_examples)\n            labels (torch.Tensor): tensor of size (n_task, n_examples)\n            weights (torch.Tensor): tensor of size (n_task, n_examples)\n            session (torch.Tensor): Optional tensor of size (n_task, n_examples) that specifies the groups of\n                    predictions/labels per batch.\n        \"\"\"\n\n        if (\n            \"required_inputs\" not in kwargs\n            or self.session_var_name not in kwargs[\"required_inputs\"]\n        ):\n            raise RecMetricException(\n                \"Need the {} input to update the session metric\".format(\n                    self.session_var_name\n                )\n            )\n        # pyre-ignore\n        session = kwargs[\"required_inputs\"][self.session_var_name]\n        if predictions is None or weights is None or session is None:\n            raise RecMetricException(\n                \"Inputs 'predictions', 'weights' and 'session' should not be None for RecallSessionMetricComputation update\"\n            )\n        _validate_model_outputs(labels, predictions, weights, session)\n\n        predictions = predictions.double()\n        labels = labels.double()\n        weights = weights.double()\n\n        num_samples = predictions.shape[-1]\n        for state_name, state_value in self.get_recall_states(\n            labels=labels, predictions=predictions, weights=weights, session=session\n        ).items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)\n\n    def _compute(self) -> List[MetricComputationReport]:\n\n        return [\n            MetricComputationReport(\n                name=MetricName.RECALL_SESSION_LEVEL,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=_calc_recall(\n                    num_true_pos=cast(torch.Tensor, getattr(self, NUM_TRUE_POS)),\n                    num_false_neg=cast(torch.Tensor, getattr(self, NUM_FALSE_NEGATIVE)),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.RECALL_SESSION_LEVEL,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=_calc_recall(\n                    num_true_pos=self.get_window_state(NUM_TRUE_POS),\n                    num_false_neg=self.get_window_state(NUM_FALSE_NEGATIVE),\n                ),\n            ),\n        ]\n\n    def get_recall_states(\n        self,\n        labels: torch.Tensor,\n        predictions: torch.Tensor,\n        weights: torch.Tensor,\n        session: torch.Tensor,\n    ) -> Dict[str, torch.Tensor]:\n\n        predictions_ranked = ranking_within_session(predictions, session)\n        predictions_labels = (predictions_ranked < self.top_threshold).to(torch.int32)\n        if self.run_ranking_of_labels:\n            labels_ranked = ranking_within_session(labels, session)\n            labels = (labels_ranked < self.top_threshold).to(torch.int32)\n        num_true_pos = _calc_num_true_pos(labels, predictions_labels, weights)\n        num_false_neg = _calc_num_false_neg(labels, predictions_labels, weights)\n\n        return {NUM_TRUE_POS: num_true_pos, NUM_FALSE_NEGATIVE: num_false_neg}",
  "class RecallSessionMetric(RecMetric):\n    _namespace: MetricNamespace = MetricNamespace.RECALL_SESSION_LEVEL\n    _computation_class: Type[RecMetricComputation] = RecallSessionMetricComputation\n\n    def __init__(\n        self,\n        world_size: int,\n        my_rank: int,\n        batch_size: int,\n        tasks: List[RecTaskInfo],\n        compute_mode: RecComputeMode = RecComputeMode.UNFUSED_TASKS_COMPUTATION,\n        window_size: int = 100,\n        fused_update_limit: int = 0,\n        process_group: Optional[dist.ProcessGroup] = None,\n        **kwargs: Any,\n    ) -> None:\n        if compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION:\n            raise RecMetricException(\n                \"Fused computation is not supported for recall session-level metrics\"\n            )\n\n        if fused_update_limit > 0:\n            raise RecMetricException(\n                \"Fused update is not supported for recall session-level metrics\"\n            )\n        for task in tasks:\n            if task.session_metric_def is None:\n                raise RecMetricException(\n                    \"Please, specify the session metric definition\"\n                )\n            session_metric_def = task.session_metric_def\n            if session_metric_def.top_threshold is None:\n                raise RecMetricException(\"Please, specify the top threshold\")\n\n        super().__init__(\n            world_size=world_size,\n            my_rank=my_rank,\n            batch_size=batch_size,\n            tasks=tasks,\n            compute_mode=compute_mode,\n            window_size=window_size,\n            fused_update_limit=fused_update_limit,\n            process_group=process_group,\n            **kwargs,\n        )\n\n    def _get_task_kwargs(\n        self, task_config: Union[RecTaskInfo, List[RecTaskInfo]]\n    ) -> Dict[str, Any]:\n        if isinstance(task_config, list):\n            raise RecMetricException(\"Session metric can only take one task at a time\")\n\n        if task_config.session_metric_def is None:\n            raise RecMetricException(\"Please, specify the session metric definition\")\n\n        return {\"session_metric_def\": task_config.session_metric_def}\n\n    def _get_task_required_inputs(\n        self, task_config: Union[RecTaskInfo, List[RecTaskInfo]]\n    ) -> Set[str]:\n        if isinstance(task_config, list):\n            raise RecMetricException(\"Session metric can only take one task at a time\")\n\n        if task_config.session_metric_def is None:\n            raise RecMetricException(\"Please, specify the session metric definition\")\n\n        return (\n            {task_config.session_metric_def.session_var_name}\n            if task_config.session_metric_def.session_var_name\n            else set()\n        )",
  "def __init__(\n        self,\n        *args: Any,\n        session_metric_def: SessionMetricDef,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            NUM_TRUE_POS,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            NUM_FALSE_NEGATIVE,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self.top_threshold: Optional[int] = session_metric_def.top_threshold\n        self.run_ranking_of_labels: bool = session_metric_def.run_ranking_of_labels\n        self.session_var_name: Optional[str] = session_metric_def.session_var_name",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        \"\"\"\n        Args:\n            predictions (torch.Tensor): tensor of size (n_task, n_examples)\n            labels (torch.Tensor): tensor of size (n_task, n_examples)\n            weights (torch.Tensor): tensor of size (n_task, n_examples)\n            session (torch.Tensor): Optional tensor of size (n_task, n_examples) that specifies the groups of\n                    predictions/labels per batch.\n        \"\"\"\n\n        if (\n            \"required_inputs\" not in kwargs\n            or self.session_var_name not in kwargs[\"required_inputs\"]\n        ):\n            raise RecMetricException(\n                \"Need the {} input to update the session metric\".format(\n                    self.session_var_name\n                )\n            )\n        # pyre-ignore\n        session = kwargs[\"required_inputs\"][self.session_var_name]\n        if predictions is None or weights is None or session is None:\n            raise RecMetricException(\n                \"Inputs 'predictions', 'weights' and 'session' should not be None for RecallSessionMetricComputation update\"\n            )\n        _validate_model_outputs(labels, predictions, weights, session)\n\n        predictions = predictions.double()\n        labels = labels.double()\n        weights = weights.double()\n\n        num_samples = predictions.shape[-1]\n        for state_name, state_value in self.get_recall_states(\n            labels=labels, predictions=predictions, weights=weights, session=session\n        ).items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)",
  "def _compute(self) -> List[MetricComputationReport]:\n\n        return [\n            MetricComputationReport(\n                name=MetricName.RECALL_SESSION_LEVEL,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=_calc_recall(\n                    num_true_pos=cast(torch.Tensor, getattr(self, NUM_TRUE_POS)),\n                    num_false_neg=cast(torch.Tensor, getattr(self, NUM_FALSE_NEGATIVE)),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.RECALL_SESSION_LEVEL,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=_calc_recall(\n                    num_true_pos=self.get_window_state(NUM_TRUE_POS),\n                    num_false_neg=self.get_window_state(NUM_FALSE_NEGATIVE),\n                ),\n            ),\n        ]",
  "def get_recall_states(\n        self,\n        labels: torch.Tensor,\n        predictions: torch.Tensor,\n        weights: torch.Tensor,\n        session: torch.Tensor,\n    ) -> Dict[str, torch.Tensor]:\n\n        predictions_ranked = ranking_within_session(predictions, session)\n        predictions_labels = (predictions_ranked < self.top_threshold).to(torch.int32)\n        if self.run_ranking_of_labels:\n            labels_ranked = ranking_within_session(labels, session)\n            labels = (labels_ranked < self.top_threshold).to(torch.int32)\n        num_true_pos = _calc_num_true_pos(labels, predictions_labels, weights)\n        num_false_neg = _calc_num_false_neg(labels, predictions_labels, weights)\n\n        return {NUM_TRUE_POS: num_true_pos, NUM_FALSE_NEGATIVE: num_false_neg}",
  "def __init__(\n        self,\n        world_size: int,\n        my_rank: int,\n        batch_size: int,\n        tasks: List[RecTaskInfo],\n        compute_mode: RecComputeMode = RecComputeMode.UNFUSED_TASKS_COMPUTATION,\n        window_size: int = 100,\n        fused_update_limit: int = 0,\n        process_group: Optional[dist.ProcessGroup] = None,\n        **kwargs: Any,\n    ) -> None:\n        if compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION:\n            raise RecMetricException(\n                \"Fused computation is not supported for recall session-level metrics\"\n            )\n\n        if fused_update_limit > 0:\n            raise RecMetricException(\n                \"Fused update is not supported for recall session-level metrics\"\n            )\n        for task in tasks:\n            if task.session_metric_def is None:\n                raise RecMetricException(\n                    \"Please, specify the session metric definition\"\n                )\n            session_metric_def = task.session_metric_def\n            if session_metric_def.top_threshold is None:\n                raise RecMetricException(\"Please, specify the top threshold\")\n\n        super().__init__(\n            world_size=world_size,\n            my_rank=my_rank,\n            batch_size=batch_size,\n            tasks=tasks,\n            compute_mode=compute_mode,\n            window_size=window_size,\n            fused_update_limit=fused_update_limit,\n            process_group=process_group,\n            **kwargs,\n        )",
  "def _get_task_kwargs(\n        self, task_config: Union[RecTaskInfo, List[RecTaskInfo]]\n    ) -> Dict[str, Any]:\n        if isinstance(task_config, list):\n            raise RecMetricException(\"Session metric can only take one task at a time\")\n\n        if task_config.session_metric_def is None:\n            raise RecMetricException(\"Please, specify the session metric definition\")\n\n        return {\"session_metric_def\": task_config.session_metric_def}",
  "def _get_task_required_inputs(\n        self, task_config: Union[RecTaskInfo, List[RecTaskInfo]]\n    ) -> Set[str]:\n        if isinstance(task_config, list):\n            raise RecMetricException(\"Session metric can only take one task at a time\")\n\n        if task_config.session_metric_def is None:\n            raise RecMetricException(\"Please, specify the session metric definition\")\n\n        return (\n            {task_config.session_metric_def.session_var_name}\n            if task_config.session_metric_def.session_var_name\n            else set()\n        )",
  "class ThroughputMetric(nn.Module):\n    \"\"\"\n    The module to calculate throughput. Throughput is defined as the trained examples\n    across all ranks per second. For example, if the batch size on each rank is 512\n    and there are 32 ranks, throughput is 512 * 32 / time_to_train_one_step.\n\n    Args:\n        batch_size (int): batch size for the trainer\n        world_size (int): the number of trainers\n        window_seconds (int): Throughput use time-based window for window_throughput. This\n                              argument specify the window size in seconds.\n        warmup_steps (int): the number of warmup batches. No Throughput will be calculated\n                            before the warmup batches count reached.\n\n    Call Args:\n        Not supported.\n\n    Returns:\n        Not supported.\n\n    Example::\n\n        throughput = ThroughputMetric(\n                      batch_size=128,\n                      world_size=4,\n                      window_seconds=100,\n                      warmup_steps=100\n                  )\n    \"\"\"\n\n    _namespace: MetricNamespace = MetricNamespace.THROUGHPUT\n    _metric_name: MetricName = MetricName.THROUGHPUT\n    _batch_examples: int\n    _window_seconds: int\n    _warmup_steps: int\n    _window_time_lapse_buffer: Deque[float]\n    _window_time_lapse: float\n    _previous_ts: float\n    _lifetime_throughput_key: str\n    _window_throughput_key: str\n    _total_examples_key: str\n    _steps: int\n\n    def __init__(\n        self,\n        *,\n        batch_size: int,\n        world_size: int,\n        window_seconds: int,\n        warmup_steps: int = 100,\n    ) -> None:\n        super().__init__()\n        if window_seconds < 1:\n            raise ValueError(\n                \"window_seconds must be at least 1 to give window throughput \"\n                \"the minimum time window\"\n            )\n        if warmup_steps < 1:\n            raise ValueError(\n                \"warmup_steps must be at least 1 to give throughput a \"\n                \"reasonable begin time.\"\n            )\n\n        if window_seconds > MAX_WINDOW_TS:\n            logger.warn(\n                f\"window_seconds is greater than {MAX_WINDOW_TS}, capping to {MAX_WINDOW_TS} to make sure window_qps is not staled\"\n            )\n            window_seconds = MAX_WINDOW_TS\n\n        self._batch_examples = batch_size * world_size\n        self._window_seconds = window_seconds\n        self._warmup_steps = warmup_steps\n\n        self.register_buffer(\"total_examples\", torch.tensor(0, dtype=torch.long))\n        self.register_buffer(\"warmup_examples\", torch.tensor(0, dtype=torch.long))\n        self.register_buffer(\n            \"time_lapse_after_warmup\", torch.tensor(0, dtype=torch.double)\n        )\n\n        self._window_time_lapse_buffer = deque(maxlen=MAX_WINDOW_TS)\n        self._window_time_lapse = 0\n        self._previous_ts = 0\n\n        self._lifetime_throughput_key = compose_metric_key(\n            self._namespace,\n            str(self._namespace),\n            self._metric_name,\n            MetricPrefix.LIFETIME,\n        )\n        self._window_throughput_key = compose_metric_key(\n            self._namespace,\n            str(self._namespace),\n            self._metric_name,\n            MetricPrefix.WINDOW,\n        )\n        self._total_examples_key = compose_metric_key(\n            self._namespace,\n            str(self._namespace),\n            MetricName.TOTAL_EXAMPLES,\n        )\n\n        self._steps = 0\n\n    def _check_window(self) -> None:\n        while self._window_time_lapse > self._window_seconds:\n            self._window_time_lapse -= self._window_time_lapse_buffer.popleft()\n\n    def update(self) -> None:\n        ts = time.monotonic()\n        self._steps += 1\n        self.total_examples += self._batch_examples\n\n        if self._steps <= self._warmup_steps:\n            self.warmup_examples += self._batch_examples\n            if self._steps == self._warmup_steps:\n                self._previous_ts = ts\n        else:\n            time_lapse = ts - self._previous_ts\n            self.time_lapse_after_warmup += time_lapse\n            self._window_time_lapse += time_lapse\n            self._window_time_lapse_buffer.append(time_lapse)\n            self._check_window()\n            self._previous_ts = ts\n\n    def compute(self) -> Dict[str, torch.Tensor]:\n        ret = {self._total_examples_key: self.total_examples}\n        if self._steps > self._warmup_steps and (\n            not math.isclose(self.time_lapse_after_warmup.item(), 0)\n        ):\n            lifetime_throughput = (\n                self.total_examples - self.warmup_examples\n            ) / self.time_lapse_after_warmup\n            if not math.isclose(self._window_time_lapse, 0):\n                window_throughput = (\n                    len(self._window_time_lapse_buffer)\n                    * self._batch_examples\n                    / self._window_time_lapse\n                )\n            else:\n                window_throughput = 0.0\n            if not math.isclose(lifetime_throughput.item(), 0):\n                ret.update(\n                    {\n                        self._lifetime_throughput_key: torch.tensor(\n                            lifetime_throughput, dtype=torch.double\n                        ),\n                        self._window_throughput_key: torch.tensor(\n                            window_throughput, dtype=torch.double\n                        ),\n                    }\n                )\n        return ret",
  "def __init__(\n        self,\n        *,\n        batch_size: int,\n        world_size: int,\n        window_seconds: int,\n        warmup_steps: int = 100,\n    ) -> None:\n        super().__init__()\n        if window_seconds < 1:\n            raise ValueError(\n                \"window_seconds must be at least 1 to give window throughput \"\n                \"the minimum time window\"\n            )\n        if warmup_steps < 1:\n            raise ValueError(\n                \"warmup_steps must be at least 1 to give throughput a \"\n                \"reasonable begin time.\"\n            )\n\n        if window_seconds > MAX_WINDOW_TS:\n            logger.warn(\n                f\"window_seconds is greater than {MAX_WINDOW_TS}, capping to {MAX_WINDOW_TS} to make sure window_qps is not staled\"\n            )\n            window_seconds = MAX_WINDOW_TS\n\n        self._batch_examples = batch_size * world_size\n        self._window_seconds = window_seconds\n        self._warmup_steps = warmup_steps\n\n        self.register_buffer(\"total_examples\", torch.tensor(0, dtype=torch.long))\n        self.register_buffer(\"warmup_examples\", torch.tensor(0, dtype=torch.long))\n        self.register_buffer(\n            \"time_lapse_after_warmup\", torch.tensor(0, dtype=torch.double)\n        )\n\n        self._window_time_lapse_buffer = deque(maxlen=MAX_WINDOW_TS)\n        self._window_time_lapse = 0\n        self._previous_ts = 0\n\n        self._lifetime_throughput_key = compose_metric_key(\n            self._namespace,\n            str(self._namespace),\n            self._metric_name,\n            MetricPrefix.LIFETIME,\n        )\n        self._window_throughput_key = compose_metric_key(\n            self._namespace,\n            str(self._namespace),\n            self._metric_name,\n            MetricPrefix.WINDOW,\n        )\n        self._total_examples_key = compose_metric_key(\n            self._namespace,\n            str(self._namespace),\n            MetricName.TOTAL_EXAMPLES,\n        )\n\n        self._steps = 0",
  "def _check_window(self) -> None:\n        while self._window_time_lapse > self._window_seconds:\n            self._window_time_lapse -= self._window_time_lapse_buffer.popleft()",
  "def update(self) -> None:\n        ts = time.monotonic()\n        self._steps += 1\n        self.total_examples += self._batch_examples\n\n        if self._steps <= self._warmup_steps:\n            self.warmup_examples += self._batch_examples\n            if self._steps == self._warmup_steps:\n                self._previous_ts = ts\n        else:\n            time_lapse = ts - self._previous_ts\n            self.time_lapse_after_warmup += time_lapse\n            self._window_time_lapse += time_lapse\n            self._window_time_lapse_buffer.append(time_lapse)\n            self._check_window()\n            self._previous_ts = ts",
  "def compute(self) -> Dict[str, torch.Tensor]:\n        ret = {self._total_examples_key: self.total_examples}\n        if self._steps > self._warmup_steps and (\n            not math.isclose(self.time_lapse_after_warmup.item(), 0)\n        ):\n            lifetime_throughput = (\n                self.total_examples - self.warmup_examples\n            ) / self.time_lapse_after_warmup\n            if not math.isclose(self._window_time_lapse, 0):\n                window_throughput = (\n                    len(self._window_time_lapse_buffer)\n                    * self._batch_examples\n                    / self._window_time_lapse\n                )\n            else:\n                window_throughput = 0.0\n            if not math.isclose(lifetime_throughput.item(), 0):\n                ret.update(\n                    {\n                        self._lifetime_throughput_key: torch.tensor(\n                            lifetime_throughput, dtype=torch.double\n                        ),\n                        self._window_throughput_key: torch.tensor(\n                            window_throughput, dtype=torch.double\n                        ),\n                    }\n                )\n        return ret",
  "def compute_mae(\n    error_sum: torch.Tensor, weighted_num_samples: torch.Tensor\n) -> torch.Tensor:\n    return torch.where(\n        weighted_num_samples == 0.0, 0.0, error_sum / weighted_num_samples\n    ).double()",
  "def compute_error_sum(\n    labels: torch.Tensor, predictions: torch.Tensor, weights: torch.Tensor\n) -> torch.Tensor:\n    predictions = predictions.double()\n    return torch.sum(weights * torch.abs(labels - predictions), dim=-1)",
  "def get_mae_states(\n    labels: torch.Tensor, predictions: torch.Tensor, weights: torch.Tensor\n) -> Dict[str, torch.Tensor]:\n    return {\n        \"error_sum\": compute_error_sum(labels, predictions, weights),\n        \"weighted_num_samples\": torch.sum(weights, dim=-1),\n    }",
  "class MAEMetricComputation(RecMetricComputation):\n    r\"\"\"\n    This class implements the RecMetricComputation for MAE, i.e. Mean Absolute Error.\n\n    The constructor arguments are defined in RecMetricComputation.\n    See the docstring of RecMetricComputation for more detail.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"error_sum\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"weighted_num_samples\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n\n    # pyre-fixme[14]: `update` overrides method defined in `RecMetricComputation`\n    #  inconsistently.\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for MAEMetricComputation update\"\n            )\n        states = get_mae_states(labels, predictions, weights)\n        num_samples = predictions.shape[-1]\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)\n\n    def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.MAE,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_mae(\n                    cast(torch.Tensor, self.error_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.MAE,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_mae(\n                    self.get_window_state(ERROR_SUM),\n                    self.get_window_state(WEIGHTED_NUM_SAMPES),\n                ),\n            ),\n        ]",
  "class MAEMetric(RecMetric):\n    _namespace: MetricNamespace = MetricNamespace.MAE\n    _computation_class: Type[RecMetricComputation] = MAEMetricComputation",
  "def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"error_sum\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"weighted_num_samples\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for MAEMetricComputation update\"\n            )\n        states = get_mae_states(labels, predictions, weights)\n        num_samples = predictions.shape[-1]\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)",
  "def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.MAE,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_mae(\n                    cast(torch.Tensor, self.error_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.MAE,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_mae(\n                    self.get_window_state(ERROR_SUM),\n                    self.get_window_state(WEIGHTED_NUM_SAMPES),\n                ),\n            ),\n        ]",
  "class StrValueMixin:\n    def __str__(self) -> str:\n        # pyre-fixme[16]: `StrValueMixin` has no attribute `value`.\n        return self.value",
  "class MetricNameBase(StrValueMixin, Enum):\n    pass",
  "class MetricName(MetricNameBase):\n    DEFAULT = \"\"\n\n    NE = \"ne\"\n    LOG_LOSS = \"logloss\"\n    THROUGHPUT = \"throughput\"\n    TOTAL_EXAMPLES = \"total_examples\"\n    CTR = \"ctr\"\n    CALIBRATION = \"calibration\"\n    MSE = \"mse\"\n    MAE = \"mae\"\n    RMSE = \"rmse\"\n    AUC = \"auc\"\n    GROUPED_AUC = \"grouped_auc\"\n    RECALL_SESSION_LEVEL = \"recall_session_level\"\n    MULTICLASS_RECALL = \"multiclass_recall\"\n    WEIGHTED_AVG = \"weighted_avg\"\n    TOWER_QPS = \"qps\"\n    ACCURACY = \"accuracy\"\n    NDCG = \"ndcg\"",
  "class MetricNamespaceBase(StrValueMixin, Enum):\n    pass",
  "class MetricNamespace(MetricNamespaceBase):\n    DEFAULT = \"\"\n\n    NE = \"ne\"\n    THROUGHPUT = \"throughput\"\n    CTR = \"ctr\"\n    CALIBRATION = \"calibration\"\n    MSE = \"mse\"\n    AUC = \"auc\"\n    MAE = \"mae\"\n    ACCURACY = \"accuracy\"\n\n    OPTIMIZERS = \"optimizers\"\n    MODEL_CONFIGURATOR = \"model_configurator\"\n\n    MULTICLASS_RECALL = \"multiclass_recall\"\n\n    WEIGHTED_AVG = \"weighted_avg\"\n    RECALL_SESSION_LEVEL = \"recall_session_level\"\n\n    TOWER_QPS = \"qps\"\n    NDCG = \"ndcg\"",
  "class MetricPrefix(StrValueMixin, Enum):\n    DEFAULT = \"\"\n    LIFETIME = \"lifetime_\"\n    WINDOW = \"window_\"",
  "def task_wildcard_metrics_pattern(\n    namespace: MetricNamespaceBase,\n    metric_name: MetricNameBase,\n    metric_prefix: MetricPrefix = MetricPrefix.DEFAULT,\n) -> str:\n    r\"\"\"Get the re (regular expression) pattern to find a set of metrics\n    regardless task names. The motivation to have this API is from the past\n    bugs which tools hard-code the patterns but the naming change, causing\n    some testing issues.\n    \"\"\"\n    return rf\"{namespace}-.+\\|{metric_prefix}{metric_name}\"",
  "def compose_metric_namespace(\n    namespace: MetricNamespaceBase,\n    task_name: str,\n) -> str:\n    r\"\"\"Get the full namespace of a metric based on the input parameters\"\"\"\n    return f\"{namespace}-{task_name}\"",
  "def compose_customized_metric_key(\n    namespace: str,\n    metric_name: str,\n    description: Optional[str] = None,\n) -> str:\n    r\"\"\"Get the metric key. The input are unrestricted (string) namespace and\n    metric_name. This API should only be used by compose_metric_key() and\n    state metrics as the keys of state metrics are unknown.\n    \"\"\"\n    return f\"{namespace}|{metric_name}{description or ''}\"",
  "def compose_metric_key(\n    namespace: MetricNamespaceBase,\n    task_name: str,\n    metric_name: MetricNameBase,\n    metric_prefix: MetricPrefix = MetricPrefix.DEFAULT,\n    description: Optional[str] = None,\n) -> str:\n    r\"\"\"Get the metric key based on the input parameters\"\"\"\n    return compose_customized_metric_key(\n        compose_metric_namespace(namespace, task_name),\n        f\"{metric_prefix}{metric_name}\",\n        description,\n    )",
  "def __str__(self) -> str:\n        # pyre-fixme[16]: `StrValueMixin` has no attribute `value`.\n        return self.value",
  "def compute_calibration(\n    calibration_num: torch.Tensor, calibration_denom: torch.Tensor\n) -> torch.Tensor:\n    return torch.where(\n        calibration_denom <= 0.0, 0.0, calibration_num / calibration_denom\n    ).double()",
  "def get_calibration_states(\n    labels: torch.Tensor, predictions: torch.Tensor, weights: torch.Tensor\n) -> Dict[str, torch.Tensor]:\n    return {\n        CALIBRATION_NUM: torch.sum(predictions * weights, dim=-1),\n        CALIBRATION_DENOM: torch.sum(labels * weights, dim=-1),\n    }",
  "class CalibrationMetricComputation(RecMetricComputation):\n    r\"\"\"\n    This class implements the RecMetricComputation for Calibration, which is the\n    ratio between the prediction and the labels (conversions).\n\n    The constructor arguments are defined in RecMetricComputation.\n    See the docstring of RecMetricComputation for more detail.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            CALIBRATION_NUM,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            CALIBRATION_DENOM,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for CalibrationMetricComputation update\"\n            )\n        num_samples = predictions.shape[-1]\n        for state_name, state_value in get_calibration_states(\n            labels, predictions, weights\n        ).items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)\n\n    def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.CALIBRATION,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_calibration(\n                    cast(torch.Tensor, self.calibration_num),\n                    cast(torch.Tensor, self.calibration_denom),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.CALIBRATION,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_calibration(\n                    self.get_window_state(CALIBRATION_NUM),\n                    self.get_window_state(CALIBRATION_DENOM),\n                ),\n            ),\n        ]",
  "class CalibrationMetric(RecMetric):\n    _namespace: MetricNamespace = MetricNamespace.CALIBRATION\n    _computation_class: Type[RecMetricComputation] = CalibrationMetricComputation",
  "def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            CALIBRATION_NUM,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            CALIBRATION_DENOM,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for CalibrationMetricComputation update\"\n            )\n        num_samples = predictions.shape[-1]\n        for state_name, state_value in get_calibration_states(\n            labels, predictions, weights\n        ).items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)",
  "def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.CALIBRATION,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_calibration(\n                    cast(torch.Tensor, self.calibration_num),\n                    cast(torch.Tensor, self.calibration_denom),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.CALIBRATION,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_calibration(\n                    self.get_window_state(CALIBRATION_NUM),\n                    self.get_window_state(CALIBRATION_DENOM),\n                ),\n            ),\n        ]",
  "class StateMetric(abc.ABC):\n    \"\"\"\n    The interface of state metrics for a component (e.g., optimizer, qat).\n    \"\"\"\n\n    @abc.abstractmethod\n    def get_metrics(self) -> Dict[str, MetricValue]:\n        pass",
  "class RecMetricModule(nn.Module):\n    r\"\"\"\n    For the current recommendation models, we assume there will be three\n    types of metrics, 1.) RecMetric, 2.) Throughput, 3.) StateMetric.\n\n    RecMetric is a metric that is computed from the model outputs (labels,\n    predictions, weights).\n\n    Throughput is being a standalone type as its unique characteristic, time-based.\n\n    StateMetric is a metric that is computed based on a model componenet\n    (e.g., Optimizer) internal logic.\n\n    Args:\n        batch_size (int): batch size used by this trainer.\n        world_size (int): the number of trainers.\n        rec_tasks (Optional[List[RecTaskInfo]]): the information of the model tasks.\n        rec_metrics (Optional[RecMetricList]): the list of the RecMetrics.\n        throughput_metric (Optional[ThroughputMetric]): the ThroughputMetric.\n        state_metrics (Optional[Dict[str, StateMetric]]): the dict of StateMetrics.\n        compute_interval_steps (int): the intervals between two compute calls in the unit of batch number\n        memory_usage_limit_mb (float): the memory usage limit for OOM check\n\n    Call Args:\n        Not supported.\n\n    Returns:\n        Not supported.\n\n    Example:\n        >>> config = dataclasses.replace(\n        >>>     DefaultMetricsConfig, state_metrics=[StateMetricEnum.OPTIMIZERS]\n        >>> )\n        >>>\n        >>> metricModule = generate_metric_module(\n        >>>     metric_class=RecMetricModule,\n        >>>     metrics_config=config,\n        >>>     batch_size=128,\n        >>>     world_size=64,\n        >>>     my_rank=0,\n        >>>     state_metrics_mapping={StateMetricEnum.OPTIMIZERS: mock_optimizer},\n        >>>     device=torch.device(\"cpu\"),\n        >>>     pg=dist.new_group([0]),\n        >>> )\n    \"\"\"\n\n    batch_size: int\n    world_size: int\n    rec_tasks: List[RecTaskInfo]\n    rec_metrics: RecMetricList\n    throughput_metric: Optional[ThroughputMetric]\n    state_metrics: Dict[str, StateMetric]\n    memory_usage_limit_mb: float\n    memory_usage_mb_avg: float\n    oom_count: int\n    compute_count: int\n    last_compute_time: float\n\n    # TODO(chienchin): Reorganize the argument to directly accept a MetricsConfig.\n    def __init__(\n        self,\n        batch_size: int,\n        world_size: int,\n        rec_tasks: Optional[List[RecTaskInfo]] = None,\n        rec_metrics: Optional[RecMetricList] = None,\n        throughput_metric: Optional[ThroughputMetric] = None,\n        state_metrics: Optional[Dict[str, StateMetric]] = None,\n        compute_interval_steps: int = 100,\n        min_compute_interval: float = 0.0,\n        max_compute_interval: float = float(\"inf\"),\n        memory_usage_limit_mb: float = 512,\n    ) -> None:\n        super().__init__()\n        self.rec_tasks = rec_tasks if rec_tasks else []\n        self.rec_metrics = rec_metrics if rec_metrics else RecMetricList([])\n        self.throughput_metric = throughput_metric\n        self.state_metrics = state_metrics if state_metrics else {}\n        self.trained_batches: int = 0\n        self.batch_size = batch_size\n        self.world_size = world_size\n        self.memory_usage_limit_mb = memory_usage_limit_mb\n        self.memory_usage_mb_avg = 0.0\n        self.oom_count = 0\n        self.compute_count = 0\n\n        self.compute_interval_steps = compute_interval_steps\n        self.min_compute_interval = min_compute_interval\n        self.max_compute_interval = max_compute_interval\n        if self.min_compute_interval == 0.0 and self.max_compute_interval == float(\n            \"inf\"\n        ):\n            self.min_compute_interval = -1.0\n            self.max_compute_interval = -1.0\n        else:\n            if self.max_compute_interval <= 0.0:\n                raise ValueError(\"Max compute interval should not be smaller than 0.0.\")\n            if self.min_compute_interval < 0.0:\n                raise ValueError(\"Min compute interval should not be smaller than 0.0.\")\n        self.register_buffer(\n            \"_compute_interval_steps\",\n            torch.zeros(1, dtype=torch.int32),\n            persistent=False,\n        )\n        self.last_compute_time = -1.0\n\n    def get_memory_usage(self) -> int:\n        r\"\"\"Total memory of unique RecMetric tensors in bytes\"\"\"\n        total = {}\n        for metric in self.rec_metrics.rec_metrics:\n            total.update(metric.get_memory_usage())\n        return sum(total.values())\n\n    def check_memory_usage(self, compute_count: int) -> None:\n        memory_usage_mb = self.get_memory_usage() / (10**6)\n        if memory_usage_mb > self.memory_usage_limit_mb:\n            self.oom_count += 1\n            logger.warning(\n                f\"MetricModule is using {memory_usage_mb}MB. \"\n                f\"This is larger than the limit{self.memory_usage_limit_mb}MB. \"\n                f\"This is the f{self.oom_count}th OOM.\"\n            )\n\n        if (\n            compute_count > MEMORY_AVG_WARNING_WARMUP\n            and memory_usage_mb\n            > self.memory_usage_mb_avg * ((100 + MEMORY_AVG_WARNING_PERCENTAGE) / 100)\n        ):\n            logger.warning(\n                f\"MetricsModule is using more than {MEMORY_AVG_WARNING_PERCENTAGE}% of \"\n                f\"the average memory usage. Current usage: {memory_usage_mb}MB.\"\n            )\n\n        self.memory_usage_mb_avg = (\n            self.memory_usage_mb_avg * (compute_count - 1) + memory_usage_mb\n        ) / compute_count\n\n    def _update_rec_metrics(\n        self, model_out: Dict[str, torch.Tensor], **kwargs: Any\n    ) -> None:\n        r\"\"\"the internal update function to parse the model output.\n        Override this function if the implementation cannot support\n        the model output format.\n        \"\"\"\n        if self.rec_metrics and self.rec_tasks:\n            labels, predictions, weights, required_inputs = parse_task_model_outputs(\n                self.rec_tasks, model_out, self.get_required_inputs()\n            )\n            if required_inputs:\n                kwargs[\"required_inputs\"] = required_inputs\n\n            self.rec_metrics.update(\n                predictions=predictions,\n                labels=labels,\n                weights=weights,\n                **kwargs,\n            )\n\n    def update(self, model_out: Dict[str, torch.Tensor], **kwargs: Any) -> None:\n        r\"\"\"update() is called per batch, usually right after forward() to\n        update the local states of metrics based on the model_output.\n\n        Throughput.update() is also called due to the implementation sliding window\n        throughput.\n        \"\"\"\n        with record_function(\"## RecMetricModule:update ##\"):\n            self._update_rec_metrics(model_out, **kwargs)\n            if self.throughput_metric:\n                self.throughput_metric.update()\n            self.trained_batches += 1\n\n    def _adjust_compute_interval(self) -> None:\n        \"\"\"\n        Adjust the compute interval (in batches) based on the first two time\n        elapsed between the first two compute().\n        \"\"\"\n        if self.last_compute_time > 0 and self.min_compute_interval >= 0:\n            now = time.time()\n            interval = now - self.last_compute_time\n            if not (self.max_compute_interval >= interval >= self.min_compute_interval):\n                per_step_time = interval / self.compute_interval_steps\n\n                assert (\n                    self.max_compute_interval != float(\"inf\")\n                    or self.min_compute_interval != 0.0\n                ), (\n                    \"The compute time interval is \"\n                    f\"[{self.max_compute_interval}, {self.min_compute_interval}]. \"\n                    \"Something is not correct of this range. __init__() should have \"\n                    \"captured this earlier.\"\n                )\n                if self.max_compute_interval == float(\"inf\"):\n                    # The `per_step_time` is not perfectly measured -- each\n                    # step training time can vary. Since max_compute_interval\n                    # is set to infinite, adding 1.0 to the `min_compute_interval`\n                    # increase the chance that the final compute interval is\n                    # indeed larger than `min_compute_interval`.\n                    self._compute_interval_steps[0] = int(\n                        (self.min_compute_interval + 1.0) / per_step_time\n                    )\n                elif self.min_compute_interval == 0.0:\n                    # Similar to the above if, subtracting 1.0 from\n                    # `max_compute_interval` to compute `_compute_interval_steps`\n                    # can increase the chance that the final compute interval\n                    # is indeed smaller than `max_compute_interval`\n                    offset = 0.0 if self.max_compute_interval <= 1.0 else 1.0\n                    self._compute_interval_steps[0] = int(\n                        (self.max_compute_interval - offset) / per_step_time\n                    )\n                else:\n                    self._compute_interval_steps[0] = int(\n                        (self.max_compute_interval + self.min_compute_interval)\n                        / 2\n                        / per_step_time\n                    )\n                dist.all_reduce(self._compute_interval_steps, op=dist.ReduceOp.MAX)\n            self.compute_interval_steps = int(self._compute_interval_steps.item())\n            self.min_compute_interval = -1.0\n            self.max_compute_interval = -1.0\n        self.last_compute_time = time.time()\n\n    def should_compute(self) -> bool:\n        return self.trained_batches % self.compute_interval_steps == 0\n\n    def compute(self) -> Dict[str, MetricValue]:\n        r\"\"\"compute() is called when the global metrics are required, usually\n        right before logging the metrics results to the data sink.\n        \"\"\"\n        self.compute_count += 1\n        self.check_memory_usage(self.compute_count)\n        with record_function(\"## RecMetricModule:compute ##\"):\n            ret: Dict[str, MetricValue] = {}\n            if self.rec_metrics:\n                self._adjust_compute_interval()\n                ret.update(self.rec_metrics.compute())\n            if self.throughput_metric:\n                ret.update(self.throughput_metric.compute())\n            if self.state_metrics:\n                for namespace, component in self.state_metrics.items():\n                    ret.update(\n                        {\n                            f\"{compose_customized_metric_key(namespace, metric_name)}\": metric_value\n                            for metric_name, metric_value in component.get_metrics().items()\n                        }\n                    )\n        return ret\n\n    def local_compute(self) -> Dict[str, MetricValue]:\n        r\"\"\"local_compute() is called when per-trainer metrics are required. It's\n        can be used for debugging. Currently only rec_metrics is supported.\n        \"\"\"\n        ret: Dict[str, MetricValue] = {}\n        if self.rec_metrics:\n            ret.update(self.rec_metrics.local_compute())\n        return ret\n\n    def sync(self) -> None:\n        self.rec_metrics.sync()\n\n    def unsync(self) -> None:\n        self.rec_metrics.unsync()\n\n    def reset(self) -> None:\n        self.rec_metrics.reset()\n\n    def get_required_inputs(self) -> Optional[List[str]]:\n        return self.rec_metrics.get_required_inputs()",
  "def _generate_rec_metrics(\n    metrics_config: MetricsConfig,\n    world_size: int,\n    my_rank: int,\n    batch_size: int,\n    process_group: Optional[dist.ProcessGroup] = None,\n) -> RecMetricList:\n    rec_metrics = []\n    for metric_enum, metric_def in metrics_config.rec_metrics.items():\n\n        kwargs: Dict[str, Any] = {}\n        if metric_def and metric_def.arguments is not None:\n            kwargs = metric_def.arguments\n\n        rec_tasks: List[RecTaskInfo] = []\n        if metric_def.rec_tasks and metric_def.rec_task_indices:\n            raise ValueError(\n                \"Only one of RecMetricDef.rec_tasks and RecMetricDef.rec_task_indices \"\n                \"should be specified.\"\n            )\n        if metric_def.rec_tasks:\n            rec_tasks = metric_def.rec_tasks\n        elif metric_def.rec_task_indices:\n            rec_tasks = [\n                metrics_config.rec_tasks[idx] for idx in metric_def.rec_task_indices\n            ]\n        else:\n            raise ValueError(\n                \"One of RecMetricDef.rec_tasks and RecMetricDef.rec_task_indices \"\n                \"should be a non-empty list\"\n            )\n\n        rec_metrics.append(\n            REC_METRICS_MAPPING[metric_enum](\n                world_size=world_size,\n                my_rank=my_rank,\n                batch_size=batch_size,\n                tasks=rec_tasks,\n                compute_mode=metrics_config.rec_compute_mode,\n                window_size=metric_def.window_size,\n                fused_update_limit=metrics_config.fused_update_limit,\n                compute_on_all_ranks=metrics_config.compute_on_all_ranks,\n                should_validate_update=metrics_config.should_validate_update,\n                process_group=process_group,\n                **kwargs,\n            )\n        )\n    return RecMetricList(rec_metrics)",
  "def _generate_state_metrics(\n    metrics_config: MetricsConfig,\n    state_metrics_mapping: Dict[StateMetricEnum, StateMetric],\n) -> Dict[str, StateMetric]:\n    state_metrics: Dict[str, StateMetric] = {}\n    for metric_enum in metrics_config.state_metrics:\n        metric_namespace: Optional[\n            MetricNamespace\n        ] = STATE_METRICS_NAMESPACE_MAPPING.get(metric_enum, None)\n        if metric_namespace is None:\n            raise ValueError(f\"Unknown StateMetrics {metric_enum}\")\n        full_namespace = compose_metric_namespace(\n            metric_namespace, str(metric_namespace)\n        )\n        state_metrics[full_namespace] = state_metrics_mapping[metric_enum]\n    return state_metrics",
  "def generate_metric_module(\n    metric_class: Type[RecMetricModule],\n    metrics_config: MetricsConfig,\n    batch_size: int,\n    world_size: int,\n    my_rank: int,\n    state_metrics_mapping: Dict[StateMetricEnum, StateMetric],\n    device: torch.device,\n    process_group: Optional[dist.ProcessGroup] = None,\n) -> RecMetricModule:\n    rec_metrics = _generate_rec_metrics(\n        metrics_config, world_size, my_rank, batch_size, process_group\n    )\n    if metrics_config.throughput_metric:\n        throughput_metric = ThroughputMetric(\n            batch_size=batch_size,\n            world_size=world_size,\n            window_seconds=metrics_config.throughput_metric.window_size,\n        )\n    else:\n        throughput_metric = None\n    state_metrics = _generate_state_metrics(metrics_config, state_metrics_mapping)\n    metrics = metric_class(\n        batch_size=batch_size,\n        world_size=world_size,\n        rec_tasks=metrics_config.rec_tasks,\n        rec_metrics=rec_metrics,\n        throughput_metric=throughput_metric,\n        state_metrics=state_metrics,\n        compute_interval_steps=metrics_config.compute_interval_steps,\n        min_compute_interval=metrics_config.min_compute_interval,\n        max_compute_interval=metrics_config.max_compute_interval,\n    )\n    metrics.to(device)\n    return metrics",
  "def get_metrics(self) -> Dict[str, MetricValue]:\n        pass",
  "def __init__(\n        self,\n        batch_size: int,\n        world_size: int,\n        rec_tasks: Optional[List[RecTaskInfo]] = None,\n        rec_metrics: Optional[RecMetricList] = None,\n        throughput_metric: Optional[ThroughputMetric] = None,\n        state_metrics: Optional[Dict[str, StateMetric]] = None,\n        compute_interval_steps: int = 100,\n        min_compute_interval: float = 0.0,\n        max_compute_interval: float = float(\"inf\"),\n        memory_usage_limit_mb: float = 512,\n    ) -> None:\n        super().__init__()\n        self.rec_tasks = rec_tasks if rec_tasks else []\n        self.rec_metrics = rec_metrics if rec_metrics else RecMetricList([])\n        self.throughput_metric = throughput_metric\n        self.state_metrics = state_metrics if state_metrics else {}\n        self.trained_batches: int = 0\n        self.batch_size = batch_size\n        self.world_size = world_size\n        self.memory_usage_limit_mb = memory_usage_limit_mb\n        self.memory_usage_mb_avg = 0.0\n        self.oom_count = 0\n        self.compute_count = 0\n\n        self.compute_interval_steps = compute_interval_steps\n        self.min_compute_interval = min_compute_interval\n        self.max_compute_interval = max_compute_interval\n        if self.min_compute_interval == 0.0 and self.max_compute_interval == float(\n            \"inf\"\n        ):\n            self.min_compute_interval = -1.0\n            self.max_compute_interval = -1.0\n        else:\n            if self.max_compute_interval <= 0.0:\n                raise ValueError(\"Max compute interval should not be smaller than 0.0.\")\n            if self.min_compute_interval < 0.0:\n                raise ValueError(\"Min compute interval should not be smaller than 0.0.\")\n        self.register_buffer(\n            \"_compute_interval_steps\",\n            torch.zeros(1, dtype=torch.int32),\n            persistent=False,\n        )\n        self.last_compute_time = -1.0",
  "def get_memory_usage(self) -> int:\n        r\"\"\"Total memory of unique RecMetric tensors in bytes\"\"\"\n        total = {}\n        for metric in self.rec_metrics.rec_metrics:\n            total.update(metric.get_memory_usage())\n        return sum(total.values())",
  "def check_memory_usage(self, compute_count: int) -> None:\n        memory_usage_mb = self.get_memory_usage() / (10**6)\n        if memory_usage_mb > self.memory_usage_limit_mb:\n            self.oom_count += 1\n            logger.warning(\n                f\"MetricModule is using {memory_usage_mb}MB. \"\n                f\"This is larger than the limit{self.memory_usage_limit_mb}MB. \"\n                f\"This is the f{self.oom_count}th OOM.\"\n            )\n\n        if (\n            compute_count > MEMORY_AVG_WARNING_WARMUP\n            and memory_usage_mb\n            > self.memory_usage_mb_avg * ((100 + MEMORY_AVG_WARNING_PERCENTAGE) / 100)\n        ):\n            logger.warning(\n                f\"MetricsModule is using more than {MEMORY_AVG_WARNING_PERCENTAGE}% of \"\n                f\"the average memory usage. Current usage: {memory_usage_mb}MB.\"\n            )\n\n        self.memory_usage_mb_avg = (\n            self.memory_usage_mb_avg * (compute_count - 1) + memory_usage_mb\n        ) / compute_count",
  "def _update_rec_metrics(\n        self, model_out: Dict[str, torch.Tensor], **kwargs: Any\n    ) -> None:\n        r\"\"\"the internal update function to parse the model output.\n        Override this function if the implementation cannot support\n        the model output format.\n        \"\"\"\n        if self.rec_metrics and self.rec_tasks:\n            labels, predictions, weights, required_inputs = parse_task_model_outputs(\n                self.rec_tasks, model_out, self.get_required_inputs()\n            )\n            if required_inputs:\n                kwargs[\"required_inputs\"] = required_inputs\n\n            self.rec_metrics.update(\n                predictions=predictions,\n                labels=labels,\n                weights=weights,\n                **kwargs,\n            )",
  "def update(self, model_out: Dict[str, torch.Tensor], **kwargs: Any) -> None:\n        r\"\"\"update() is called per batch, usually right after forward() to\n        update the local states of metrics based on the model_output.\n\n        Throughput.update() is also called due to the implementation sliding window\n        throughput.\n        \"\"\"\n        with record_function(\"## RecMetricModule:update ##\"):\n            self._update_rec_metrics(model_out, **kwargs)\n            if self.throughput_metric:\n                self.throughput_metric.update()\n            self.trained_batches += 1",
  "def _adjust_compute_interval(self) -> None:\n        \"\"\"\n        Adjust the compute interval (in batches) based on the first two time\n        elapsed between the first two compute().\n        \"\"\"\n        if self.last_compute_time > 0 and self.min_compute_interval >= 0:\n            now = time.time()\n            interval = now - self.last_compute_time\n            if not (self.max_compute_interval >= interval >= self.min_compute_interval):\n                per_step_time = interval / self.compute_interval_steps\n\n                assert (\n                    self.max_compute_interval != float(\"inf\")\n                    or self.min_compute_interval != 0.0\n                ), (\n                    \"The compute time interval is \"\n                    f\"[{self.max_compute_interval}, {self.min_compute_interval}]. \"\n                    \"Something is not correct of this range. __init__() should have \"\n                    \"captured this earlier.\"\n                )\n                if self.max_compute_interval == float(\"inf\"):\n                    # The `per_step_time` is not perfectly measured -- each\n                    # step training time can vary. Since max_compute_interval\n                    # is set to infinite, adding 1.0 to the `min_compute_interval`\n                    # increase the chance that the final compute interval is\n                    # indeed larger than `min_compute_interval`.\n                    self._compute_interval_steps[0] = int(\n                        (self.min_compute_interval + 1.0) / per_step_time\n                    )\n                elif self.min_compute_interval == 0.0:\n                    # Similar to the above if, subtracting 1.0 from\n                    # `max_compute_interval` to compute `_compute_interval_steps`\n                    # can increase the chance that the final compute interval\n                    # is indeed smaller than `max_compute_interval`\n                    offset = 0.0 if self.max_compute_interval <= 1.0 else 1.0\n                    self._compute_interval_steps[0] = int(\n                        (self.max_compute_interval - offset) / per_step_time\n                    )\n                else:\n                    self._compute_interval_steps[0] = int(\n                        (self.max_compute_interval + self.min_compute_interval)\n                        / 2\n                        / per_step_time\n                    )\n                dist.all_reduce(self._compute_interval_steps, op=dist.ReduceOp.MAX)\n            self.compute_interval_steps = int(self._compute_interval_steps.item())\n            self.min_compute_interval = -1.0\n            self.max_compute_interval = -1.0\n        self.last_compute_time = time.time()",
  "def should_compute(self) -> bool:\n        return self.trained_batches % self.compute_interval_steps == 0",
  "def compute(self) -> Dict[str, MetricValue]:\n        r\"\"\"compute() is called when the global metrics are required, usually\n        right before logging the metrics results to the data sink.\n        \"\"\"\n        self.compute_count += 1\n        self.check_memory_usage(self.compute_count)\n        with record_function(\"## RecMetricModule:compute ##\"):\n            ret: Dict[str, MetricValue] = {}\n            if self.rec_metrics:\n                self._adjust_compute_interval()\n                ret.update(self.rec_metrics.compute())\n            if self.throughput_metric:\n                ret.update(self.throughput_metric.compute())\n            if self.state_metrics:\n                for namespace, component in self.state_metrics.items():\n                    ret.update(\n                        {\n                            f\"{compose_customized_metric_key(namespace, metric_name)}\": metric_value\n                            for metric_name, metric_value in component.get_metrics().items()\n                        }\n                    )\n        return ret",
  "def local_compute(self) -> Dict[str, MetricValue]:\n        r\"\"\"local_compute() is called when per-trainer metrics are required. It's\n        can be used for debugging. Currently only rec_metrics is supported.\n        \"\"\"\n        ret: Dict[str, MetricValue] = {}\n        if self.rec_metrics:\n            ret.update(self.rec_metrics.local_compute())\n        return ret",
  "def sync(self) -> None:\n        self.rec_metrics.sync()",
  "def unsync(self) -> None:\n        self.rec_metrics.unsync()",
  "def reset(self) -> None:\n        self.rec_metrics.reset()",
  "def get_required_inputs(self) -> Optional[List[str]]:\n        return self.rec_metrics.get_required_inputs()",
  "def compute_accuracy(\n    accuracy_sum: torch.Tensor, weighted_num_samples: torch.Tensor\n) -> torch.Tensor:\n    return torch.where(\n        weighted_num_samples == 0.0, 0.0, accuracy_sum / weighted_num_samples\n    ).double()",
  "def compute_accuracy_sum(\n    labels: torch.Tensor,\n    predictions: torch.Tensor,\n    weights: torch.Tensor,\n    threshold: float = 0.5,\n) -> torch.Tensor:\n    predictions = predictions.double()\n    return torch.sum(weights * ((predictions >= threshold) == labels), dim=-1)",
  "def get_accuracy_states(\n    labels: torch.Tensor,\n    predictions: torch.Tensor,\n    weights: Optional[torch.Tensor],\n    threshold: float = 0.5,\n) -> Dict[str, torch.Tensor]:\n    if weights is None:\n        weights = torch.ones_like(predictions)\n    return {\n        \"accuracy_sum\": compute_accuracy_sum(labels, predictions, weights, threshold),\n        \"weighted_num_samples\": torch.sum(weights, dim=-1),\n    }",
  "class AccuracyMetricComputation(RecMetricComputation):\n    r\"\"\"\n    This class implements the RecMetricComputation for Accuracy.\n\n    The constructor arguments are defined in RecMetricComputation.\n    See the docstring of RecMetricComputation for more detail.\n\n    Args:\n        threshold (float): If provided, computes accuracy metrics cutting off at\n            the specified threshold.\n    \"\"\"\n\n    def __init__(self, *args: Any, threshold: float = 0.5, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"accuracy_sum\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"weighted_num_samples\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._threshold: float = threshold\n\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' should not be None for AccuracyMetricComputation update\"\n            )\n        states = get_accuracy_states(labels, predictions, weights, self._threshold)\n        num_samples = predictions.shape[-1]\n\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)\n\n    def _compute(self) -> List[MetricComputationReport]:\n        reports = [\n            MetricComputationReport(\n                name=MetricName.ACCURACY,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_accuracy(\n                    cast(torch.Tensor, self.accuracy_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.ACCURACY,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_accuracy(\n                    self.get_window_state(\"accuracy_sum\"),\n                    self.get_window_state(\"weighted_num_samples\"),\n                ),\n            ),\n        ]\n        return reports",
  "class AccuracyMetric(RecMetric):\n    _namespace: MetricNamespace = MetricNamespace.ACCURACY\n    _computation_class: Type[RecMetricComputation] = AccuracyMetricComputation",
  "def __init__(self, *args: Any, threshold: float = 0.5, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"accuracy_sum\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"weighted_num_samples\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._threshold: float = threshold",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' should not be None for AccuracyMetricComputation update\"\n            )\n        states = get_accuracy_states(labels, predictions, weights, self._threshold)\n        num_samples = predictions.shape[-1]\n\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)",
  "def _compute(self) -> List[MetricComputationReport]:\n        reports = [\n            MetricComputationReport(\n                name=MetricName.ACCURACY,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_accuracy(\n                    cast(torch.Tensor, self.accuracy_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.ACCURACY,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_accuracy(\n                    self.get_window_state(\"accuracy_sum\"),\n                    self.get_window_state(\"weighted_num_samples\"),\n                ),\n            ),\n        ]\n        return reports",
  "def _compute_tower_qps(\n    num_examples: torch.Tensor, time_lapse: torch.Tensor\n) -> torch.Tensor:\n    return torch.where(time_lapse <= 0.0, 0.0, num_examples / time_lapse).double()",
  "def _max_reduction(state: torch.Tensor) -> torch.Tensor:\n    return torch.max(state, dim=0).values",
  "class TowerQPSMetricComputation(RecMetricComputation):\n    r\"\"\"\n    This class implements the RecMetricComputation for tower QPS.\n\n    The constructor arguments are defined in RecMetricComputation.\n    See the docstring of RecMetricComputation for more detail.\n    \"\"\"\n\n    _previous_ts: float\n    _steps: int\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        self._warmup_steps: int = kwargs.pop(\"warmup_steps\")\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            NUM_EXAMPLES,\n            torch.zeros(self._n_tasks, dtype=torch.long),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            WARMUP_EXAMPLES,\n            torch.zeros(self._n_tasks, dtype=torch.long),\n            add_window_state=False,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            TIME_LAPSE,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=_max_reduction,\n            persistent=True,\n        )\n        self._previous_ts = 0\n        self._steps = 0\n\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        self._steps += 1\n        num_examples_scalar = labels.shape[-1]\n        num_examples = torch.tensor(num_examples_scalar, dtype=torch.long)\n        self_num_examples = getattr(self, NUM_EXAMPLES)\n        self_num_examples += num_examples\n        ts = time.monotonic()\n        if self._steps <= self._warmup_steps:\n            self_warmup_examples = getattr(self, WARMUP_EXAMPLES)\n            self_warmup_examples += num_examples\n            if self._steps == self._warmup_steps:\n                self._previous_ts = ts\n        else:\n            self._aggregate_window_state(\n                NUM_EXAMPLES, num_examples, num_examples_scalar\n            )\n            time_lapse = torch.tensor(ts - self._previous_ts, dtype=torch.double)\n            self_time_lapse = getattr(self, TIME_LAPSE)\n            self_time_lapse += time_lapse\n            self._aggregate_window_state(TIME_LAPSE, time_lapse, num_examples_scalar)\n            self._previous_ts = ts\n\n    def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.TOWER_QPS,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=_compute_tower_qps(\n                    cast(torch.Tensor, self.num_examples)\n                    - cast(torch.Tensor, self.warmup_examples),\n                    cast(torch.Tensor, self.time_lapse),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.TOWER_QPS,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=_compute_tower_qps(\n                    self.get_window_state(NUM_EXAMPLES),\n                    self.get_window_state(TIME_LAPSE),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.TOTAL_EXAMPLES,\n                metric_prefix=MetricPrefix.DEFAULT,\n                value=cast(torch.Tensor, self.num_examples).detach(),\n            ),\n        ]",
  "class TowerQPSMetric(RecMetric):\n    r\"\"\"\n    TowerQPSMetric defines the tower QPS metric.\n    Tower QPS's formula is training example count / time\n    where training example count  = sum(examples for trainer 1, ... examples for trainer n)\n    and time = max(time for trainer 1, ... time for trainer n)\n    It's mostly used for cases where there's no fixed batch size\n    For example for Pyper MTML models, given the same input, different tasks may have\n    different numbers of examples to process\n\n    Args:\n        world_size (int): the number of trainers.\n        my_rank (int): the rank of this trainer.\n        batch_size (int): batch size used by this trainer.\n        tasks (List[RecTaskInfo]): the information of the model tasks.\n        compute_mode (RecComputeMode): the computation mode. See RecComputeMode.\n        window_size (int): the window size for the window metric.\n        fused_update_limit (int): the maximum number of updates to be fused.\n        process_group (Optional[ProcessGroup]): the process group used for the\n            communication. Will use the default process group if not specified.\n\n    Call Args:\n        Not supported.\n\n    Returns:\n        Not supported.\n\n    Example::\n\n        For world_size = 4, suppose we have 1 step after warmup\n        predictions = [\n            [0.8033, 0.0662, 0.7559],\n            [0.1821, 0.9652, 0.4602],\n            [0.8545, 0.4758, 0.2220],\n            [0.1021, 0.2469, 0.7259],\n        ],\n        previous_ts = [278.94, 312.16, 286.96, 291.43]\n        ts = [281.35, 316.45, 289.47, 295.55]\n\n        num_examples = [3, 3, 3, 3]\n        time_lapse = [2.41, 4.29, 2.51, 4.12]\n\n        tower_qps = torch.sum(num_examples) / torch.max(time_lapse) = 2.80\n    \"\"\"\n\n    _namespace: MetricNamespace = MetricNamespace.TOWER_QPS\n    _computation_class: Type[RecMetricComputation] = TowerQPSMetricComputation\n\n    def __init__(\n        self,\n        world_size: int,\n        my_rank: int,\n        batch_size: int,\n        tasks: List[RecTaskInfo],\n        compute_mode: RecComputeMode = RecComputeMode.UNFUSED_TASKS_COMPUTATION,\n        window_size: int = 100,\n        fused_update_limit: int = 0,\n        process_group: Optional[dist.ProcessGroup] = None,\n        warmup_steps: int = WARMUP_STEPS,\n        **kwargs: Any,\n    ) -> None:\n        if fused_update_limit > 0:\n            raise RecMetricException(\"Fused update is not supported for tower QPS\")\n\n        kwargs[\"warmup_steps\"] = warmup_steps\n\n        super().__init__(\n            world_size=world_size,\n            my_rank=my_rank,\n            batch_size=batch_size,\n            tasks=tasks,\n            compute_mode=compute_mode,\n            window_size=window_size,\n            fused_update_limit=fused_update_limit,\n            process_group=process_group,\n            **kwargs,\n        )\n\n    def update(\n        self,\n        *,\n        predictions: Optional[RecModelOutput],\n        labels: RecModelOutput,\n        weights: Optional[RecModelOutput],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        with torch.no_grad():\n            if self._compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION:\n                if not isinstance(labels, torch.Tensor):\n                    raise RecMetricException(\n                        \"Fused computation only support where 'labels' is a tensor\"\n                    )\n                labels = labels.view(-1, self._batch_size)\n                if self._should_validate_update:\n                    # Set the default value to be all True. When weights is None, it's considered\n                    # to be a valid input, and we'll use the default value\n                    has_valid_weights = torch.ones(\n                        len(self._tasks),\n                        dtype=torch.bool,\n                        device=self._metrics_computations[0].has_valid_update.device,\n                    )\n                    if weights is not None:\n                        if not isinstance(weights, torch.Tensor):\n                            raise RecMetricException(\n                                \"Fused computation only support where 'weights' is a tensor\"\n                            )\n                        has_valid_weights = torch.gt(\n                            torch.count_nonzero(\n                                weights.view(-1, self._batch_size), dim=-1\n                            ),\n                            0,\n                        )\n\n                    if torch.any(has_valid_weights):\n                        self._metrics_computations[0].update(\n                            predictions=None, labels=labels, weights=None\n                        )\n                        self._metrics_computations[0].has_valid_update.logical_or_(\n                            has_valid_weights\n                        )\n                else:\n                    self._metrics_computations[0].update(\n                        predictions=None, labels=labels, weights=None\n                    )\n            else:\n                for task, metric_ in zip(self._tasks, self._metrics_computations):\n                    if task.name not in labels:\n                        continue\n                    # pyre-fixme[6]: For 1st argument expected `Union[None,\n                    #  List[typing.Any], int, slice, Tensor, typing.Tuple[typing.Any,\n                    #  ...]]` but got `str`.\n                    task_labels = labels[task.name].view(1, -1)\n                    if self._should_validate_update:\n                        has_valid_weights = torch.ones(\n                            1, dtype=torch.bool, device=metric_.has_valid_update.device\n                        )\n                        if weights is not None and task.name in weights:\n                            has_valid_weights = torch.gt(\n                                torch.count_nonzero(\n                                    # pyre-fixme[6]: For 1st argument expected\n                                    #  `Union[None, List[typing.Any], int, slice,\n                                    #  Tensor, typing.Tuple[typing.Any, ...]]` but got\n                                    #  `str`.\n                                    weights[task.name].view(1, -1),\n                                    dim=-1,\n                                ),\n                                0,\n                            )\n                        if has_valid_weights[0]:\n                            metric_.has_valid_update.logical_or_(has_valid_weights)\n                        else:\n                            continue\n                    metric_.update(\n                        predictions=None,\n                        labels=task_labels,\n                        weights=None,\n                    )",
  "def __init__(self, *args: Any, **kwargs: Any) -> None:\n        self._warmup_steps: int = kwargs.pop(\"warmup_steps\")\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            NUM_EXAMPLES,\n            torch.zeros(self._n_tasks, dtype=torch.long),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            WARMUP_EXAMPLES,\n            torch.zeros(self._n_tasks, dtype=torch.long),\n            add_window_state=False,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            TIME_LAPSE,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=_max_reduction,\n            persistent=True,\n        )\n        self._previous_ts = 0\n        self._steps = 0",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        self._steps += 1\n        num_examples_scalar = labels.shape[-1]\n        num_examples = torch.tensor(num_examples_scalar, dtype=torch.long)\n        self_num_examples = getattr(self, NUM_EXAMPLES)\n        self_num_examples += num_examples\n        ts = time.monotonic()\n        if self._steps <= self._warmup_steps:\n            self_warmup_examples = getattr(self, WARMUP_EXAMPLES)\n            self_warmup_examples += num_examples\n            if self._steps == self._warmup_steps:\n                self._previous_ts = ts\n        else:\n            self._aggregate_window_state(\n                NUM_EXAMPLES, num_examples, num_examples_scalar\n            )\n            time_lapse = torch.tensor(ts - self._previous_ts, dtype=torch.double)\n            self_time_lapse = getattr(self, TIME_LAPSE)\n            self_time_lapse += time_lapse\n            self._aggregate_window_state(TIME_LAPSE, time_lapse, num_examples_scalar)\n            self._previous_ts = ts",
  "def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.TOWER_QPS,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=_compute_tower_qps(\n                    cast(torch.Tensor, self.num_examples)\n                    - cast(torch.Tensor, self.warmup_examples),\n                    cast(torch.Tensor, self.time_lapse),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.TOWER_QPS,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=_compute_tower_qps(\n                    self.get_window_state(NUM_EXAMPLES),\n                    self.get_window_state(TIME_LAPSE),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.TOTAL_EXAMPLES,\n                metric_prefix=MetricPrefix.DEFAULT,\n                value=cast(torch.Tensor, self.num_examples).detach(),\n            ),\n        ]",
  "def __init__(\n        self,\n        world_size: int,\n        my_rank: int,\n        batch_size: int,\n        tasks: List[RecTaskInfo],\n        compute_mode: RecComputeMode = RecComputeMode.UNFUSED_TASKS_COMPUTATION,\n        window_size: int = 100,\n        fused_update_limit: int = 0,\n        process_group: Optional[dist.ProcessGroup] = None,\n        warmup_steps: int = WARMUP_STEPS,\n        **kwargs: Any,\n    ) -> None:\n        if fused_update_limit > 0:\n            raise RecMetricException(\"Fused update is not supported for tower QPS\")\n\n        kwargs[\"warmup_steps\"] = warmup_steps\n\n        super().__init__(\n            world_size=world_size,\n            my_rank=my_rank,\n            batch_size=batch_size,\n            tasks=tasks,\n            compute_mode=compute_mode,\n            window_size=window_size,\n            fused_update_limit=fused_update_limit,\n            process_group=process_group,\n            **kwargs,\n        )",
  "def update(\n        self,\n        *,\n        predictions: Optional[RecModelOutput],\n        labels: RecModelOutput,\n        weights: Optional[RecModelOutput],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        with torch.no_grad():\n            if self._compute_mode == RecComputeMode.FUSED_TASKS_COMPUTATION:\n                if not isinstance(labels, torch.Tensor):\n                    raise RecMetricException(\n                        \"Fused computation only support where 'labels' is a tensor\"\n                    )\n                labels = labels.view(-1, self._batch_size)\n                if self._should_validate_update:\n                    # Set the default value to be all True. When weights is None, it's considered\n                    # to be a valid input, and we'll use the default value\n                    has_valid_weights = torch.ones(\n                        len(self._tasks),\n                        dtype=torch.bool,\n                        device=self._metrics_computations[0].has_valid_update.device,\n                    )\n                    if weights is not None:\n                        if not isinstance(weights, torch.Tensor):\n                            raise RecMetricException(\n                                \"Fused computation only support where 'weights' is a tensor\"\n                            )\n                        has_valid_weights = torch.gt(\n                            torch.count_nonzero(\n                                weights.view(-1, self._batch_size), dim=-1\n                            ),\n                            0,\n                        )\n\n                    if torch.any(has_valid_weights):\n                        self._metrics_computations[0].update(\n                            predictions=None, labels=labels, weights=None\n                        )\n                        self._metrics_computations[0].has_valid_update.logical_or_(\n                            has_valid_weights\n                        )\n                else:\n                    self._metrics_computations[0].update(\n                        predictions=None, labels=labels, weights=None\n                    )\n            else:\n                for task, metric_ in zip(self._tasks, self._metrics_computations):\n                    if task.name not in labels:\n                        continue\n                    # pyre-fixme[6]: For 1st argument expected `Union[None,\n                    #  List[typing.Any], int, slice, Tensor, typing.Tuple[typing.Any,\n                    #  ...]]` but got `str`.\n                    task_labels = labels[task.name].view(1, -1)\n                    if self._should_validate_update:\n                        has_valid_weights = torch.ones(\n                            1, dtype=torch.bool, device=metric_.has_valid_update.device\n                        )\n                        if weights is not None and task.name in weights:\n                            has_valid_weights = torch.gt(\n                                torch.count_nonzero(\n                                    # pyre-fixme[6]: For 1st argument expected\n                                    #  `Union[None, List[typing.Any], int, slice,\n                                    #  Tensor, typing.Tuple[typing.Any, ...]]` but got\n                                    #  `str`.\n                                    weights[task.name].view(1, -1),\n                                    dim=-1,\n                                ),\n                                0,\n                            )\n                        if has_valid_weights[0]:\n                            metric_.has_valid_update.logical_or_(has_valid_weights)\n                        else:\n                            continue\n                    metric_.update(\n                        predictions=None,\n                        labels=task_labels,\n                        weights=None,\n                    )",
  "def compute_true_positives_at_k(\n    predictions: torch.Tensor,\n    labels: torch.Tensor,\n    weights: torch.Tensor,\n    n_classes: int,\n) -> torch.Tensor:\n    \"\"\"\n    Compute and return a list of weighted true positives (true predictions) at k. When k = 0,\n    tp is counted when the 1st predicted class matches the label. When k = 1, tp is counted\n    when either the 1st or 2nd predicted class matches the label.\n\n    Args:\n        predictions (Tensor): Tensor of label predictions with shape of (n_sample, n_class) or (n_task, n_sample, n_class).\n        labels (Tensor): Tensor of ground truth labels with shape of (n_sample, ) or (n_task, n_sample).\n        weights (Tensor): Tensor of weight on each sample, with shape of (n_sample, ) or (n_task, n_sample).\n        n_classes (int): Number of classes.\n\n    Output:\n        true_positives_list (Tensor): Tensor of true positives with shape of (n_class, ) or (n_task, n_class).\n\n    Examples:\n\n        >>> predictions = torch.tensor([[0.9, 0.1, 0, 0, 0], [0.1, 0.2, 0.25, 0.15, 0.3], [0, 1.0, 0, 0, 0], [0, 0, 0.2, 0.7, 0.1]])\n        >>> labels = torch.tensor([0, 3, 1, 2])\n        >>> weights = torch.tensor([1, 0.25, 0.5, 0.25])\n        >>> n_classes = 5\n        >>> true_positives_list = compute_multiclass_k_sum(predictions, labels, n_classes)\n        >>> true_positives_list\n        tensor([1.5000, 1.7500, 1.7500, 2.0000, 2.0000])\n\n    \"\"\"\n    ranks = torch.argsort(predictions, dim=-1, descending=True)\n    true_positives = (\n        torch.zeros(1, device=predictions.device)\n        if predictions.ndim == 2\n        else torch.zeros(predictions.shape[0], 1, device=predictions.device)\n    )\n    true_positives_list = torch.tensor([], device=predictions.device)\n\n    for k in range(n_classes):\n        mask = torch.unsqueeze(labels, dim=-1) == ranks[..., k : k + 1]\n        mask = mask * torch.unsqueeze(weights, dim=-1)\n        true_positives += mask.sum(dim=-2)\n        true_positives_list = torch.cat((true_positives_list, true_positives), dim=-1)\n\n    return true_positives_list",
  "def compute_multiclass_recall_at_k(\n    tp_at_k: torch.Tensor,\n    total_weights: torch.Tensor,\n) -> torch.Tensor:\n    return tp_at_k / torch.unsqueeze(total_weights, dim=-1)",
  "def get_multiclass_recall_states(\n    predictions: torch.Tensor,\n    labels: torch.Tensor,\n    weights: torch.Tensor,\n    n_classes: int,\n) -> Dict[str, torch.Tensor]:\n    true_positives_at_k_sum = compute_true_positives_at_k(\n        predictions, labels, weights, n_classes\n    )\n    return {\n        \"tp_at_k\": true_positives_at_k_sum,\n        \"total_weights\": torch.sum(weights, dim=-1),\n    }",
  "class MulticlassRecallMetricComputation(RecMetricComputation):\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        self._n_classes: int = kwargs.pop(\"number_of_classes\")\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"tp_at_k\",\n            torch.zeros(self._n_tasks, self._n_classes, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"total_weights\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for MulticlassRecallMetricComputation update\"\n            )\n        states = get_multiclass_recall_states(\n            predictions, labels, weights, self._n_classes\n        )\n        num_samples = predictions.shape[-2]\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)\n\n    def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.MULTICLASS_RECALL,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_multiclass_recall_at_k(\n                    cast(torch.Tensor, self.tp_at_k),\n                    cast(torch.Tensor, self.total_weights),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.MULTICLASS_RECALL,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_multiclass_recall_at_k(\n                    self.get_window_state(\"tp_at_k\"),\n                    self.get_window_state(\"total_weights\"),\n                ),\n            ),\n        ]",
  "class MulticlassRecallMetric(RecMetric):\n    _namespace: MetricNamespace = MetricNamespace.MULTICLASS_RECALL\n    _computation_class: Type[RecMetricComputation] = MulticlassRecallMetricComputation",
  "def __init__(self, *args: Any, **kwargs: Any) -> None:\n        self._n_classes: int = kwargs.pop(\"number_of_classes\")\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"tp_at_k\",\n            torch.zeros(self._n_tasks, self._n_classes, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"total_weights\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for MulticlassRecallMetricComputation update\"\n            )\n        states = get_multiclass_recall_states(\n            predictions, labels, weights, self._n_classes\n        )\n        num_samples = predictions.shape[-2]\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)",
  "def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.MULTICLASS_RECALL,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_multiclass_recall_at_k(\n                    cast(torch.Tensor, self.tp_at_k),\n                    cast(torch.Tensor, self.total_weights),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.MULTICLASS_RECALL,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_multiclass_recall_at_k(\n                    self.get_window_state(\"tp_at_k\"),\n                    self.get_window_state(\"total_weights\"),\n                ),\n            ),\n        ]",
  "def session_ids_to_lengths(\n    session_ids: torch.Tensor, device: torch.device\n) -> torch.Tensor:\n    \"\"\"\n    Convert session_ids to lengths tensor. It is used in all session-wise loss\n    computations.\n\n    Args:\n        session_ids: a list of string session_ids,\n            e.g., [\"session_1\", \"session_2\", \"session_2\"]\n        device: the device to put session_ids into\n\n    Returns:\n        session_lengths(torch.Tensor): a tensor of session lengths, e.g., tensor([1, 2])\n    \"\"\"\n\n    session_lengths: List[int] = []\n    if len(session_ids) == 0:\n        return torch.zeros(1)\n    length = 1\n    for i in range(len(session_ids) - 1):\n        if session_ids[i] == session_ids[i + 1]:\n            length += 1\n        else:\n            session_lengths.append(length)\n            length = 1\n    session_lengths.append(length)\n\n    return torch.tensor(session_lengths, dtype=torch.int, device=device)",
  "def compute_lambda_ndcg(\n    prediction: torch.Tensor,\n    label: torch.Tensor,\n    weight: torch.Tensor,\n    session_lengths: torch.Tensor,\n    use_exp_gain: bool,\n) -> torch.Tensor:\n    \"\"\"\n    Compute the sum lambda NDCG loss from a group of sessions.\n\n    Args:\n        prediction(torch.Tensor): a tensor of predicted scores\n        label(torch.Tensor): a tensor of labels\n        weight(torch.Tensor): a tensor of weights\n        session_lengths(torch.Tensor): a tensor of session lengths converted from\n            session_ids\n        use_exp_gain(bool): whether to use exponential gain or not\n\n    Returns:\n        sum_loss(torch.Tensor): a tensor of the sum of the ndcg loss\n    \"\"\"\n\n    loss = torch.zeros_like(session_lengths, dtype=torch.double)\n\n    cur_index = int(0)\n    for i, session_length in enumerate(session_lengths):\n        data_indexes = torch.arange(\n            cur_index,\n            cur_index + int(session_length),\n            dtype=torch.long,\n            device=prediction.device,\n        )\n        session_loss = compute_lambda_ndcg_by_session(\n            prediction=torch.take(prediction, data_indexes),\n            label=torch.take(label, data_indexes),\n            use_exp_gain=use_exp_gain,\n        )\n        loss[i] = session_loss * torch.max(torch.take(weight, data_indexes))\n        cur_index += session_length\n\n    return torch.sum(loss)",
  "def compute_lambda_ndcg_by_session(\n    prediction: torch.Tensor,\n    label: torch.Tensor,\n    use_exp_gain: bool,\n) -> torch.Tensor:\n    \"\"\"\n    Compute the lambda NDCG loss for one session.\n\n    Args:\n        prediction(torch.Tensor): a tensor of predicted scores\n        label(torch.Tensor): a tensor of labels\n        use_exp_gain(bool): whether to use exponential gain or not\n\n    Returns:\n        loss(torch.Tensor): a tensor of approximate ndcg loss\n    \"\"\"\n\n    gain = torch.exp2(label).sub(1.0) if use_exp_gain else label\n    discounts = get_position_discounts(prediction)\n\n    idcg = gain @ get_position_discounts(label)\n    idcg = torch.max(idcg, torch.tensor(1e-6))\n    dcg = gain @ discounts\n\n    return 1 - dcg / idcg",
  "def get_position_discounts(t: torch.Tensor) -> torch.Tensor:\n    orders = torch.argsort(torch.argsort(t, descending=True))\n    return torch.reciprocal(torch.log2(orders.add(2.0))).type(torch.double)",
  "def _validate_model_outputs(\n    predictions: torch.Tensor,\n    labels: torch.Tensor,\n    weights: torch.Tensor,\n    session_ids: torch.Tensor,\n) -> None:\n    assert predictions.shape == labels.shape == weights.shape == session_ids.shape\n    assert (\n        predictions.dim() == 2 and predictions.shape[0] > 0 and predictions.shape[1] > 0\n    )",
  "def get_ndcg_states(\n    labels: torch.Tensor,\n    predictions: torch.Tensor,\n    weights: torch.Tensor,\n    session_ids: torch.Tensor,\n    exponential_gain: bool,\n) -> Dict[str, torch.Tensor]:\n    n_tasks = labels.shape[0]\n    sum_ndcg = torch.zeros(n_tasks, dtype=torch.double)\n\n    for i in range(n_tasks):\n        session_lengths = session_ids_to_lengths(\n            session_ids=session_ids[i], device=predictions.device\n        )\n\n        sum_ndcg[i] = compute_lambda_ndcg(\n            prediction=predictions[i],\n            label=labels[i],\n            weight=weights[i],\n            session_lengths=session_lengths,\n            use_exp_gain=exponential_gain,\n        )\n\n    return {SUM_NDCG: sum_ndcg, NUM_SESSIONS: session_lengths.shape[-1]}",
  "def compute_ndcg(sum_ndcg: torch.Tensor, num_sessions: torch.Tensor) -> torch.Tensor:\n    return sum_ndcg / num_sessions",
  "class NDCGComputation(RecMetricComputation):\n    r\"\"\"\n    This class implements the RecMetricComputation for NDCG, i.e. Normalized Discounted Cumulative Gain.\n\n    The constructor arguments are defined in RecMetricComputation.\n    See the docstring of RecMetricComputation for more detail.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        exponential_gain: bool = False,\n        session_key: str = SESSION_KEY,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(*args, **kwargs)\n        self._exponential_gain: bool = exponential_gain\n        self._session_key: str = session_key\n\n        self._add_state(\n            SUM_NDCG,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            NUM_SESSIONS,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        \"\"\"\n        Args:\n            predictions (torch.Tensor): tensor of size (n_task, n_examples)\n            labels (torch.Tensor): tensor of size (n_task, n_examples)\n            weights (torch.Tensor): tensor of size (n_task, n_examples)\n        \"\"\"\n\n        if (\n            REQUIRED_INPUTS not in kwargs\n            or self._session_key not in kwargs[REQUIRED_INPUTS]\n        ):\n            raise RecMetricException(\n                f\"{self._session_key} input is required to calculate NDCG\"\n            )\n\n        session_ids = kwargs[REQUIRED_INPUTS][self._session_key]\n        if predictions is None or weights is None or session_ids is None:\n            raise RecMetricException(\n                \"Inputs 'predictions', 'weights' and 'session_ids' should not be None for NDCGMetricComputation update\"\n            )\n\n        _validate_model_outputs(predictions, labels, weights, session_ids)\n\n        predictions = predictions.double()\n        labels = labels.double()\n        weights = weights.double()\n\n        states = get_ndcg_states(\n            labels=labels,\n            predictions=predictions,\n            weights=weights,\n            session_ids=session_ids,\n            exponential_gain=self._exponential_gain,\n        )\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, predictions.shape[-1])\n\n    def _compute(self) -> List[MetricComputationReport]:\n\n        return [\n            MetricComputationReport(\n                name=MetricName.NDCG,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_ndcg(\n                    sum_ndcg=cast(torch.Tensor, getattr(self, SUM_NDCG)),\n                    num_sessions=cast(torch.Tensor, getattr(self, NUM_SESSIONS)),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.NDCG,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_ndcg(\n                    sum_ndcg=self.get_window_state(SUM_NDCG),\n                    num_sessions=self.get_window_state(NUM_SESSIONS),\n                ),\n            ),\n        ]",
  "class NDCGMetric(RecMetric):\n    _namespace: MetricNamespace = MetricNamespace.NDCG\n    _computation_class: Type[RecMetricComputation] = NDCGComputation",
  "def __init__(\n        self,\n        *args: Any,\n        exponential_gain: bool = False,\n        session_key: str = SESSION_KEY,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(*args, **kwargs)\n        self._exponential_gain: bool = exponential_gain\n        self._session_key: str = session_key\n\n        self._add_state(\n            SUM_NDCG,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            NUM_SESSIONS,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        \"\"\"\n        Args:\n            predictions (torch.Tensor): tensor of size (n_task, n_examples)\n            labels (torch.Tensor): tensor of size (n_task, n_examples)\n            weights (torch.Tensor): tensor of size (n_task, n_examples)\n        \"\"\"\n\n        if (\n            REQUIRED_INPUTS not in kwargs\n            or self._session_key not in kwargs[REQUIRED_INPUTS]\n        ):\n            raise RecMetricException(\n                f\"{self._session_key} input is required to calculate NDCG\"\n            )\n\n        session_ids = kwargs[REQUIRED_INPUTS][self._session_key]\n        if predictions is None or weights is None or session_ids is None:\n            raise RecMetricException(\n                \"Inputs 'predictions', 'weights' and 'session_ids' should not be None for NDCGMetricComputation update\"\n            )\n\n        _validate_model_outputs(predictions, labels, weights, session_ids)\n\n        predictions = predictions.double()\n        labels = labels.double()\n        weights = weights.double()\n\n        states = get_ndcg_states(\n            labels=labels,\n            predictions=predictions,\n            weights=weights,\n            session_ids=session_ids,\n            exponential_gain=self._exponential_gain,\n        )\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, predictions.shape[-1])",
  "def _compute(self) -> List[MetricComputationReport]:\n\n        return [\n            MetricComputationReport(\n                name=MetricName.NDCG,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_ndcg(\n                    sum_ndcg=cast(torch.Tensor, getattr(self, SUM_NDCG)),\n                    num_sessions=cast(torch.Tensor, getattr(self, NUM_SESSIONS)),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.NDCG,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_ndcg(\n                    sum_ndcg=self.get_window_state(SUM_NDCG),\n                    num_sessions=self.get_window_state(NUM_SESSIONS),\n                ),\n            ),\n        ]",
  "def get_mean(value_sum: torch.Tensor, num_samples: torch.Tensor) -> torch.Tensor:\n    return value_sum / num_samples",
  "class WeightedAvgMetricComputation(RecMetricComputation):\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"weighted_sum\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"weighted_num_samples\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        num_samples = labels.shape[0]\n        predictions = cast(torch.Tensor, predictions)\n        weights = cast(torch.Tensor, weights)\n        states = {\n            \"weighted_sum\": (predictions * weights).sum(dim=-1),\n            \"weighted_num_samples\": weights.sum(dim=-1),\n        }\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)\n\n    def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.WEIGHTED_AVG,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=get_mean(\n                    cast(torch.Tensor, self.weighted_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.WEIGHTED_AVG,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=get_mean(\n                    self.get_window_state(\"weighted_sum\"),\n                    self.get_window_state(\"weighted_num_samples\"),\n                ),\n            ),\n        ]",
  "class WeightedAvgMetric(RecMetric):\n    _namespace: MetricNamespace = MetricNamespace.WEIGHTED_AVG\n    _computation_class: Type[RecMetricComputation] = WeightedAvgMetricComputation",
  "def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"weighted_sum\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"weighted_num_samples\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        num_samples = labels.shape[0]\n        predictions = cast(torch.Tensor, predictions)\n        weights = cast(torch.Tensor, weights)\n        states = {\n            \"weighted_sum\": (predictions * weights).sum(dim=-1),\n            \"weighted_num_samples\": weights.sum(dim=-1),\n        }\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)",
  "def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.WEIGHTED_AVG,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=get_mean(\n                    cast(torch.Tensor, self.weighted_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.WEIGHTED_AVG,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=get_mean(\n                    self.get_window_state(\"weighted_sum\"),\n                    self.get_window_state(\"weighted_num_samples\"),\n                ),\n            ),\n        ]",
  "class RecMetricEnumBase(StrValueMixin, Enum):\n    pass",
  "class RecMetricEnum(RecMetricEnumBase):\n    NE = \"ne\"\n    LOG_LOSS = \"log_loss\"\n    CTR = \"ctr\"\n    AUC = \"auc\"\n    CALIBRATION = \"calibration\"\n    MSE = \"mse\"\n    MAE = \"mae\"\n    MULTICLASS_RECALL = \"multiclass_recall\"\n    RECALL_SESSION_LEVEL = \"recall_session_level\"\n    WEIGHTED_AVG = \"weighted_avg\"\n    TOWER_QPS = \"tower_qps\"\n    ACCURACY = \"accuracy\"\n    NDCG = \"ndcg\"",
  "class SessionMetricDef:\n    # hyperparameters required for session level metrics\n    # session_var_name: name of session tensor in the model_out\n    # top_threshold: predictiones ranked in top \"top_threshold\" are considered as positive\n    # run_ranking_of_labels: if True, labels are also ranked as predictions\n    session_var_name: str\n    top_threshold: Optional[int] = None\n    run_ranking_of_labels: bool = False",
  "class RecTaskInfo:\n    name: str = \"DefaultTask\"\n    label_name: str = \"label\"\n    prediction_name: str = \"prediction\"\n    weight_name: str = \"weight\"\n    session_metric_def: Optional[\n        SessionMetricDef\n    ] = None",
  "class RecComputeMode(Enum):\n    \"\"\"This Enum lists the supported computation modes for RecMetrics.\n\n    FUSED_TASKS_COMPUTATION indicates that RecMetrics will fuse the computation\n    for multiple tasks of the same metric. This can be used by modules where the\n    outputs of all the tasks are vectorized.\n    \"\"\"\n\n    FUSED_TASKS_COMPUTATION = 1\n    UNFUSED_TASKS_COMPUTATION = 2",
  "class RecMetricDef:\n    \"\"\"The dataclass that defines a RecMetric.\n\n    Args:\n        rec_tasks (List[RecTaskInfo]): this and next fields specify the RecTask\n            information. ``rec_tasks`` specifies the RecTask information while\n            ``rec_task_indices`` represents the indices that point to the\n            RecTask information stored in the parent ``MetricsConfig``. Only one\n            of the two fields should be specified.\n        rec_task_indices (List[int]): see the doscstring of ``rec_tasks``.\n        window_size (int): the window size for this metric. Note that this is global window size.\n            The local window size is window_size / world_size, and must be larger than batch size.\n        arguments (Optional[Dict[str, Any]]): any propritary arguments to be used\n            by this Metric.\n    \"\"\"\n\n    rec_tasks: List[RecTaskInfo] = field(default_factory=list)\n    rec_task_indices: List[int] = field(default_factory=list)\n    window_size: int = _DEFAULT_WINDOW_SIZE\n    arguments: Optional[Dict[str, Any]] = None",
  "class StateMetricEnum(StrValueMixin, Enum):\n    OPTIMIZERS = \"optimizers\"\n    MODEL_CONFIGURATOR = \"model_configurator\"",
  "class ThroughputDef:\n    window_size: int = _DEFAULT_THROUGHPUT_WINDOW_SECONDS",
  "class MetricsConfig:\n    \"\"\"The dataclass that lists all the configurations to be used by the\n    MetricModule.\n\n    Args:\n        rec_tasks (List[RecTaskInfo]): the list of RecTasks that will be shared\n            by all the metrics.\n        rec_metrics (Dict[RecMetricEnum, RecMetricDef]): the confiurations of\n            the RecMetric objects.\n        throughput_metric: (Optional[ThroughputDef]): the configurations of the ThroughputMetric\n            object.\n        rec_compute_mode (RecComputeMode): the computation mode for the\n            RecMetric objects. This will be applied to all the RecMetric\n            objects defined by ``rec_metrics``.\n        fused_update_limit (int): the maximum updates that can be fused. The\n            default is 0 which means no fusion. Setting this field to 1 is\n            logically identical to 0.  If this field ii larger than 1,\n            RecMetrics will perform the actual update every ``update()`` calls.\n        state_metrics (List[StateMetricEnum]): indicates what state_metrics\n            will be enabled.\n        compute_interval_steps(int): computing metrics every step can be\n            expsensive. This field is used to specify the computation interval\n            in batch count. `should_compute()` return True if the current\n            trained batch count match the setting.\n        min_compute_interval(float): minimum compute interval in seconds.\n            If this value is set (should be larger than 0), MetricModule will\n            adjust `compute_interval_steps` after the second compute() is called.\n        max_compute_interval(float): maximum compute interval in seconds.\n            If this value is set (should be larger than 0), MetricModule will\n            adjust `compute_interval_steps` after the second compute() is called.\n        compute_on_all_ranks (bool): whether to compute rec metrics on all ranks.\n            If False, only compute on rank 0.\n        should_validate_update (bool): whether to check the inputs of update() and skip\n            update if the inputs are invalid. Invalid inputs include the case where all\n            examples have 0 weights for a batch.\n    \"\"\"\n\n    rec_tasks: List[RecTaskInfo] = field(default_factory=list)\n    rec_metrics: Dict[RecMetricEnum, RecMetricDef] = field(default_factory=dict)\n    throughput_metric: Optional[ThroughputDef] = None\n    rec_compute_mode: RecComputeMode = RecComputeMode.UNFUSED_TASKS_COMPUTATION\n    fused_update_limit: int = 0\n    state_metrics: List[StateMetricEnum] = field(default_factory=list)\n    compute_interval_steps: int = 100\n    min_compute_interval: float = 0.0\n    max_compute_interval: float = float(\"inf\")\n    compute_on_all_ranks: bool = False\n    should_validate_update: bool = False",
  "def compute_ctr(ctr_num: torch.Tensor, ctr_denom: torch.Tensor) -> torch.Tensor:\n    return torch.where(ctr_denom == 0.0, 0.0, ctr_num / ctr_denom).double()",
  "def get_ctr_states(\n    labels: torch.Tensor, predictions: torch.Tensor, weights: torch.Tensor\n) -> Dict[str, torch.Tensor]:\n    return {\n        CTR_NUM: torch.sum(labels * weights, dim=-1),\n        CTR_DENOM: torch.sum(weights, dim=-1),\n    }",
  "class CTRMetricComputation(RecMetricComputation):\n    r\"\"\"\n    This class implements the RecMetricComputation for CTR, i.e. Click Through Rate,\n    which is the ratio between the predicted positive examples and the total examples.\n\n    The constructor arguments are defined in RecMetricComputation.\n    See the docstring of RecMetricComputation for more detail.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            CTR_NUM,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            CTR_DENOM,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for CTRMetricComputation update\"\n            )\n        num_samples = predictions.shape[-1]\n        for state_name, state_value in get_ctr_states(\n            labels, predictions, weights\n        ).items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)\n\n    def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.CTR,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_ctr(\n                    cast(torch.Tensor, self.ctr_num),\n                    cast(torch.Tensor, self.ctr_denom),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.CTR,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_ctr(\n                    self.get_window_state(CTR_NUM),\n                    self.get_window_state(CTR_DENOM),\n                ),\n            ),\n        ]",
  "class CTRMetric(RecMetric):\n    _namespace: MetricNamespace = MetricNamespace.CTR\n    _computation_class: Type[RecMetricComputation] = CTRMetricComputation",
  "def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            CTR_NUM,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            CTR_DENOM,\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for CTRMetricComputation update\"\n            )\n        num_samples = predictions.shape[-1]\n        for state_name, state_value in get_ctr_states(\n            labels, predictions, weights\n        ).items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)",
  "def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.CTR,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_ctr(\n                    cast(torch.Tensor, self.ctr_num),\n                    cast(torch.Tensor, self.ctr_denom),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.CTR,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_ctr(\n                    self.get_window_state(CTR_NUM),\n                    self.get_window_state(CTR_DENOM),\n                ),\n            ),\n        ]",
  "def compute_mse(\n    error_sum: torch.Tensor, weighted_num_samples: torch.Tensor\n) -> torch.Tensor:\n    return torch.where(\n        weighted_num_samples == 0.0, 0.0, error_sum / weighted_num_samples\n    ).double()",
  "def compute_rmse(\n    error_sum: torch.Tensor, weighted_num_samples: torch.Tensor\n) -> torch.Tensor:\n    return torch.where(\n        weighted_num_samples == 0.0, 0.0, torch.sqrt(error_sum / weighted_num_samples)\n    ).double()",
  "def compute_error_sum(\n    labels: torch.Tensor, predictions: torch.Tensor, weights: torch.Tensor\n) -> torch.Tensor:\n    predictions = predictions.double()\n    return torch.sum(weights * torch.square(labels - predictions), dim=-1)",
  "def get_mse_states(\n    labels: torch.Tensor, predictions: torch.Tensor, weights: torch.Tensor\n) -> Dict[str, torch.Tensor]:\n    return {\n        \"error_sum\": compute_error_sum(labels, predictions, weights),\n        \"weighted_num_samples\": torch.sum(weights, dim=-1),\n    }",
  "class MSEMetricComputation(RecMetricComputation):\n    r\"\"\"\n    This class implements the RecMetricComputation for MSE, i.e. Mean Squared Error.\n\n    The constructor arguments are defined in RecMetricComputation.\n    See the docstring of RecMetricComputation for more detail.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"error_sum\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"weighted_num_samples\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n\n    def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for MSEMetricComputation update\"\n            )\n        states = get_mse_states(labels, predictions, weights)\n        num_samples = predictions.shape[-1]\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)\n\n    def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.MSE,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_mse(\n                    cast(torch.Tensor, self.error_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.RMSE,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_rmse(\n                    cast(torch.Tensor, self.error_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.MSE,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_mse(\n                    self.get_window_state(ERROR_SUM),\n                    self.get_window_state(WEIGHTED_NUM_SAMPES),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.RMSE,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_rmse(\n                    self.get_window_state(ERROR_SUM),\n                    self.get_window_state(WEIGHTED_NUM_SAMPES),\n                ),\n            ),\n        ]",
  "class MSEMetric(RecMetric):\n    _namespace: MetricNamespace = MetricNamespace.MSE\n    _computation_class: Type[RecMetricComputation] = MSEMetricComputation",
  "def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._add_state(\n            \"error_sum\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )\n        self._add_state(\n            \"weighted_num_samples\",\n            torch.zeros(self._n_tasks, dtype=torch.double),\n            add_window_state=True,\n            dist_reduce_fx=\"sum\",\n            persistent=True,\n        )",
  "def update(\n        self,\n        *,\n        predictions: Optional[torch.Tensor],\n        labels: torch.Tensor,\n        weights: Optional[torch.Tensor],\n        **kwargs: Dict[str, Any],\n    ) -> None:\n        if predictions is None or weights is None:\n            raise RecMetricException(\n                \"Inputs 'predictions' and 'weights' should not be None for MSEMetricComputation update\"\n            )\n        states = get_mse_states(labels, predictions, weights)\n        num_samples = predictions.shape[-1]\n        for state_name, state_value in states.items():\n            state = getattr(self, state_name)\n            state += state_value\n            self._aggregate_window_state(state_name, state_value, num_samples)",
  "def _compute(self) -> List[MetricComputationReport]:\n        return [\n            MetricComputationReport(\n                name=MetricName.MSE,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_mse(\n                    cast(torch.Tensor, self.error_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.RMSE,\n                metric_prefix=MetricPrefix.LIFETIME,\n                value=compute_rmse(\n                    cast(torch.Tensor, self.error_sum),\n                    cast(torch.Tensor, self.weighted_num_samples),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.MSE,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_mse(\n                    self.get_window_state(ERROR_SUM),\n                    self.get_window_state(WEIGHTED_NUM_SAMPES),\n                ),\n            ),\n            MetricComputationReport(\n                name=MetricName.RMSE,\n                metric_prefix=MetricPrefix.WINDOW,\n                value=compute_rmse(\n                    self.get_window_state(ERROR_SUM),\n                    self.get_window_state(WEIGHTED_NUM_SAMPES),\n                ),\n            ),\n        ]",
  "class CrossNet(torch.nn.Module):\n    r\"\"\"\n    `Cross Network <https://arxiv.org/abs/1708.05123>`_:\n\n    Cross Net is a stack of \"crossing\" operations on a tensor of shape :math:`(*, N)`\n    to the same shape, effectively creating :math:`N` learnable polynomical functions\n    over the input tensor.\n\n    In this module, the crossing operations are defined based on a full rank matrix\n    (NxN), such that the crossing effect can cover all bits on each layer. On each layer\n    l, the tensor is transformed into:\n\n    .. math ::    x_{l+1} = x_0 * (W_l \\cdot x_l + b_l) + x_l\n\n    where :math:`W_l` is a square matrix :math:`(NxN)`, :math:`*` means element-wise\n    multiplication, :math:`\\cdot` means matrix multiplication.\n\n    Args:\n        in_features (int): the dimension of the input.\n        num_layers (int): the number of layers in the module.\n\n    Example::\n\n        batch_size = 3\n        num_layers = 2\n        in_features = 10\n        input = torch.randn(batch_size, in_features)\n        dcn = CrossNet(num_layers=num_layers)\n        output = dcn(input)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        num_layers: int,\n    ) -> None:\n        super().__init__()\n        self._num_layers = num_layers\n        self.kernels: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(torch.empty(in_features, in_features))\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.bias: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(torch.nn.init.zeros_(torch.empty(in_features, 1)))\n                for i in range(self._num_layers)\n            ]\n        )\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor with shape [batch_size, in_features].\n\n        Returns:\n            torch.Tensor: tensor with shape [batch_size, in_features].\n        \"\"\"\n        x_0 = input.unsqueeze(2)  # (B, N, 1)\n        x_l = x_0\n\n        for layer in range(self._num_layers):\n            xl_w = torch.matmul(self.kernels[layer], x_l)  # (B, N, 1)\n            x_l = x_0 * (xl_w + self.bias[layer]) + x_l  # (B, N, 1)\n\n        return torch.squeeze(x_l, dim=2)",
  "class LowRankCrossNet(torch.nn.Module):\n    r\"\"\"\n    Low Rank Cross Net is a highly efficient cross net. Instead of using full rank cross\n    matrices (NxN) at each layer, it will use two kernels :math:`W (N x r)` and\n    :math:`V (r x N)`, where `r << N`, to simplify the matrix multiplication.\n\n    On each layer l, the tensor is transformed into:\n\n    .. math::    x_{l+1} = x_0 * (W_l \\cdot (V_l \\cdot x_l) + b_l) + x_l\n\n    where :math:`W_l` is either a vector, :math:`*` means element-wise multiplication,\n    and :math:`\\cdot` means matrix multiplication.\n\n    NOTE:\n        Rank `r` should be chosen smartly. Usually, we  expect `r < N/2` to have\n        computational savings; we should expect :math:`r ~= N/4` to preserve the\n        accuracy of the full rank cross net.\n\n    Args:\n        in_features (int): the dimension of the input.\n        num_layers (int): the number of layers in the module.\n        low_rank (int): the rank setup of the cross matrix (default = 1).\n            Value must be always >= 1.\n\n    Example::\n\n        batch_size = 3\n        num_layers = 2\n        in_features = 10\n        input = torch.randn(batch_size, in_features)\n        dcn = LowRankCrossNet(num_layers=num_layers, low_rank=3)\n        output = dcn(input)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        num_layers: int,\n        low_rank: int = 1,\n    ) -> None:\n        super().__init__()\n        assert low_rank >= 1, \"Low rank must be larger or equal to 1\"\n\n        self._num_layers = num_layers\n        self._low_rank = low_rank\n        self.W_kernels: torch.nn.ParameterList = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(\n                        torch.empty(in_features, self._low_rank)\n                    )\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.V_kernels: torch.nn.ParameterList = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(\n                        torch.empty(self._low_rank, in_features)\n                    )\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.bias: torch.nn.ParameterList = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(torch.nn.init.zeros_(torch.empty(in_features)))\n                for i in range(self._num_layers)\n            ]\n        )\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor with shape [batch_size, in_features].\n\n        Returns:\n            torch.Tensor: tensor with shape [batch_size, in_features].\n        \"\"\"\n\n        x_0 = input\n        x_l = x_0\n\n        for layer in range(self._num_layers):\n            x_l_v = torch.nn.functional.linear(x_l, self.V_kernels[layer])\n            x_l_w = torch.nn.functional.linear(x_l_v, self.W_kernels[layer])\n            x_l = x_0 * (x_l_w + self.bias[layer]) + x_l  # (B, N)\n\n        return x_l",
  "class VectorCrossNet(torch.nn.Module):\n    r\"\"\"\n    Vector Cross Network can be refered as\n    `DCN-V1 <https://arxiv.org/pdf/1708.05123.pdf>`_.\n\n    It is also a specialized low rank cross net, where rank=1. In this version, on each\n    layer, instead of keeping two kernels W and V, we only keep one vector kernel W\n    (Nx1). We use the dot operation to compute the \"crossing\" effect of the features,\n    thus saving two matrix multiplications to further reduce computational cost and cut\n    the number of learnable parameters.\n\n    On each layer l, the tensor is transformed into\n\n    .. math::    x_{l+1} = x_0 * (W_l . x_l + b_l) + x_l\n\n    where :math:`W_l` is either a vector, :math:`*` means element-wise multiplication;\n    :math:`.` means dot operations.\n\n    Args:\n        in_features (int): the dimension of the input.\n        num_layers (int): the number of layers in the module.\n\n    Example::\n\n        batch_size = 3\n        num_layers = 2\n        in_features = 10\n        input = torch.randn(batch_size, in_features)\n        dcn = VectorCrossNet(num_layers=num_layers)\n        output = dcn(input)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        num_layers: int,\n    ) -> None:\n        super().__init__()\n        self._num_layers = num_layers\n        self.kernels: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(torch.empty(in_features, 1))\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.bias: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(torch.nn.init.zeros_(torch.empty(in_features, 1)))\n                for i in range(self._num_layers)\n            ]\n        )\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor with shape [batch_size, in_features].\n\n        Returns:\n            torch.Tensor: tensor with shape [batch_size, in_features].\n        \"\"\"\n\n        x_0 = input.unsqueeze(2)  # (B, N, 1)\n        x_l = x_0\n\n        for layer in range(self._num_layers):\n            xl_w = torch.tensordot(\n                x_l,\n                self.kernels[layer],\n                dims=([1], [0]),\n            )  # (B, 1, 1)\n            x_l = torch.matmul(x_0, xl_w) + self.bias[layer] + x_l  # (B, N, 1)\n\n        return torch.squeeze(x_l, dim=2)",
  "class LowRankMixtureCrossNet(torch.nn.Module):\n    r\"\"\"\n    Low Rank Mixture Cross Net is a DCN V2 implementation from the `paper\n    <https://arxiv.org/pdf/2008.13535.pdf>`_:\n\n    `LowRankMixtureCrossNet` defines the learnable crossing parameter per layer as a\n    low-rank matrix :math:`(N*r)` together with mixture of experts. Compared to\n    `LowRankCrossNet`, instead of relying on one single expert to learn feature crosses,\n    this module leverages such :math:`K` experts; each learning feature interactions in\n    different subspaces, and adaptively combining the learned crosses using a gating\n    mechanism that depends on input :math:`x`..\n\n    On each layer l, the tensor is transformed into:\n\n    .. math::    x_{l+1} = MoE({expert_i : i \\in K_{experts}}) + x_l\n\n    and each :math:`expert_i` is defined as:\n\n    .. math::   expert_i = x_0 * (U_{li} \\cdot g(C_{li} \\cdot g(V_{li} \\cdot x_l)) + b_l)\n\n    where :math:`U_{li} (N, r)`, :math:`C_{li} (r, r)` and :math:`V_{li} (r, N)` are\n    low-rank matrices, :math:`*` means element-wise multiplication, :math:`x` means\n    matrix multiplication, and :math:`g()` is the non-linear activation function.\n\n    When num_expert is 1, the gate evaluation and MOE will be skipped to save\n    computation.\n\n    Args:\n        in_features (int): the dimension of the input.\n        num_layers (int): the number of layers in the module.\n        low_rank (int): the rank setup of the cross matrix (default = 1).\n            Value must be always >= 1\n        activation (Union[torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]):\n            the non-linear activation function, used in defining experts.\n            Default is relu.\n\n    Example::\n\n        batch_size = 3\n        num_layers = 2\n        in_features = 10\n        input = torch.randn(batch_size, in_features)\n        dcn = LowRankCrossNet(num_layers=num_layers, num_experts=5, low_rank=3)\n        output = dcn(input)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        num_layers: int,\n        num_experts: int = 1,\n        low_rank: int = 1,\n        activation: Union[\n            torch.nn.Module,\n            Callable[[torch.Tensor], torch.Tensor],\n        ] = torch.relu,\n    ) -> None:\n        super().__init__()\n        assert num_experts >= 1, \"num_experts must be larger or equal to 1\"\n        assert low_rank >= 1, \"Low rank must be larger or equal to 1\"\n\n        self._num_layers = num_layers\n        self._num_experts = num_experts\n        self._low_rank = low_rank\n        self._in_features = in_features\n        self.U_kernels: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(\n                        torch.empty(\n                            self._num_experts, self._in_features, self._low_rank\n                        )\n                    )\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.V_kernels: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(\n                        torch.empty(\n                            self._num_experts, self._low_rank, self._in_features\n                        )\n                    )\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.bias: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.zeros_(torch.empty(self._in_features, 1))\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.gates: Optional[torch.nn.Module] = (\n            torch.nn.ModuleList(\n                [\n                    torch.nn.Linear(self._in_features, 1, bias=False)\n                    for i in range(self._num_experts)\n                ]\n            )\n            if self._num_experts > 1\n            else None\n        )\n\n        self._activation = activation\n        self.C_kernels: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(\n                        torch.empty(self._num_experts, self._low_rank, self._low_rank)\n                    )\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor with shape [batch_size, in_features].\n\n        Returns:\n            torch.Tensor: tensor with shape [batch_size, in_features].\n        \"\"\"\n\n        x_0 = input.unsqueeze(2)  # (B, N, 1)\n        x_l = x_0\n\n        for layer in range(self._num_layers):\n            # set up gating:\n            if self._num_experts > 1:\n                gating = []\n                for i in range(self._num_experts):\n                    # pyre-ignore[16]: `Optional` has no attribute `__getitem__`.\n                    gating.append(self.gates[i](x_l.squeeze(2)))\n                gating = torch.stack(gating, 1)  # (B, K, 1)\n\n            # set up experts\n            experts = []\n            for i in range(self._num_experts):\n                expert = torch.matmul(\n                    self.V_kernels[layer][i],\n                    x_l,\n                )  # (B, r, 1)\n                expert = torch.matmul(\n                    self.C_kernels[layer][i],\n                    self._activation(expert),\n                )  # (B, r, 1)\n                expert = torch.matmul(\n                    self.U_kernels[layer][i],\n                    self._activation(expert),\n                )  # (B, N, 1)\n                expert = x_0 * (expert + self.bias[layer])  # (B, N, 1)\n                experts.append(expert.squeeze(2))  # (B, N)\n            experts = torch.stack(experts, 2)  # (B, N, K)\n\n            if self._num_experts > 1:\n                # MOE update\n                moe = torch.matmul(\n                    experts,\n                    # pyre-ignore[61]: `gating` may not be initialized here.\n                    torch.nn.functional.softmax(gating, 1),\n                )  # (B, N, 1)\n                x_l = moe + x_l  # (B, N, 1)\n            else:\n                x_l = experts + x_l  # (B, N, 1)\n\n        return torch.squeeze(x_l, dim=2)",
  "def __init__(\n        self,\n        in_features: int,\n        num_layers: int,\n    ) -> None:\n        super().__init__()\n        self._num_layers = num_layers\n        self.kernels: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(torch.empty(in_features, in_features))\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.bias: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(torch.nn.init.zeros_(torch.empty(in_features, 1)))\n                for i in range(self._num_layers)\n            ]\n        )",
  "def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor with shape [batch_size, in_features].\n\n        Returns:\n            torch.Tensor: tensor with shape [batch_size, in_features].\n        \"\"\"\n        x_0 = input.unsqueeze(2)  # (B, N, 1)\n        x_l = x_0\n\n        for layer in range(self._num_layers):\n            xl_w = torch.matmul(self.kernels[layer], x_l)  # (B, N, 1)\n            x_l = x_0 * (xl_w + self.bias[layer]) + x_l  # (B, N, 1)\n\n        return torch.squeeze(x_l, dim=2)",
  "def __init__(\n        self,\n        in_features: int,\n        num_layers: int,\n        low_rank: int = 1,\n    ) -> None:\n        super().__init__()\n        assert low_rank >= 1, \"Low rank must be larger or equal to 1\"\n\n        self._num_layers = num_layers\n        self._low_rank = low_rank\n        self.W_kernels: torch.nn.ParameterList = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(\n                        torch.empty(in_features, self._low_rank)\n                    )\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.V_kernels: torch.nn.ParameterList = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(\n                        torch.empty(self._low_rank, in_features)\n                    )\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.bias: torch.nn.ParameterList = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(torch.nn.init.zeros_(torch.empty(in_features)))\n                for i in range(self._num_layers)\n            ]\n        )",
  "def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor with shape [batch_size, in_features].\n\n        Returns:\n            torch.Tensor: tensor with shape [batch_size, in_features].\n        \"\"\"\n\n        x_0 = input\n        x_l = x_0\n\n        for layer in range(self._num_layers):\n            x_l_v = torch.nn.functional.linear(x_l, self.V_kernels[layer])\n            x_l_w = torch.nn.functional.linear(x_l_v, self.W_kernels[layer])\n            x_l = x_0 * (x_l_w + self.bias[layer]) + x_l  # (B, N)\n\n        return x_l",
  "def __init__(\n        self,\n        in_features: int,\n        num_layers: int,\n    ) -> None:\n        super().__init__()\n        self._num_layers = num_layers\n        self.kernels: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(torch.empty(in_features, 1))\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.bias: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(torch.nn.init.zeros_(torch.empty(in_features, 1)))\n                for i in range(self._num_layers)\n            ]\n        )",
  "def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor with shape [batch_size, in_features].\n\n        Returns:\n            torch.Tensor: tensor with shape [batch_size, in_features].\n        \"\"\"\n\n        x_0 = input.unsqueeze(2)  # (B, N, 1)\n        x_l = x_0\n\n        for layer in range(self._num_layers):\n            xl_w = torch.tensordot(\n                x_l,\n                self.kernels[layer],\n                dims=([1], [0]),\n            )  # (B, 1, 1)\n            x_l = torch.matmul(x_0, xl_w) + self.bias[layer] + x_l  # (B, N, 1)\n\n        return torch.squeeze(x_l, dim=2)",
  "def __init__(\n        self,\n        in_features: int,\n        num_layers: int,\n        num_experts: int = 1,\n        low_rank: int = 1,\n        activation: Union[\n            torch.nn.Module,\n            Callable[[torch.Tensor], torch.Tensor],\n        ] = torch.relu,\n    ) -> None:\n        super().__init__()\n        assert num_experts >= 1, \"num_experts must be larger or equal to 1\"\n        assert low_rank >= 1, \"Low rank must be larger or equal to 1\"\n\n        self._num_layers = num_layers\n        self._num_experts = num_experts\n        self._low_rank = low_rank\n        self._in_features = in_features\n        self.U_kernels: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(\n                        torch.empty(\n                            self._num_experts, self._in_features, self._low_rank\n                        )\n                    )\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.V_kernels: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(\n                        torch.empty(\n                            self._num_experts, self._low_rank, self._in_features\n                        )\n                    )\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.bias: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.zeros_(torch.empty(self._in_features, 1))\n                )\n                for i in range(self._num_layers)\n            ]\n        )\n        self.gates: Optional[torch.nn.Module] = (\n            torch.nn.ModuleList(\n                [\n                    torch.nn.Linear(self._in_features, 1, bias=False)\n                    for i in range(self._num_experts)\n                ]\n            )\n            if self._num_experts > 1\n            else None\n        )\n\n        self._activation = activation\n        self.C_kernels: torch.nn.Module = torch.nn.ParameterList(\n            [\n                torch.nn.Parameter(\n                    torch.nn.init.xavier_normal_(\n                        torch.empty(self._num_experts, self._low_rank, self._low_rank)\n                    )\n                )\n                for i in range(self._num_layers)\n            ]\n        )",
  "def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor with shape [batch_size, in_features].\n\n        Returns:\n            torch.Tensor: tensor with shape [batch_size, in_features].\n        \"\"\"\n\n        x_0 = input.unsqueeze(2)  # (B, N, 1)\n        x_l = x_0\n\n        for layer in range(self._num_layers):\n            # set up gating:\n            if self._num_experts > 1:\n                gating = []\n                for i in range(self._num_experts):\n                    # pyre-ignore[16]: `Optional` has no attribute `__getitem__`.\n                    gating.append(self.gates[i](x_l.squeeze(2)))\n                gating = torch.stack(gating, 1)  # (B, K, 1)\n\n            # set up experts\n            experts = []\n            for i in range(self._num_experts):\n                expert = torch.matmul(\n                    self.V_kernels[layer][i],\n                    x_l,\n                )  # (B, r, 1)\n                expert = torch.matmul(\n                    self.C_kernels[layer][i],\n                    self._activation(expert),\n                )  # (B, r, 1)\n                expert = torch.matmul(\n                    self.U_kernels[layer][i],\n                    self._activation(expert),\n                )  # (B, N, 1)\n                expert = x_0 * (expert + self.bias[layer])  # (B, N, 1)\n                experts.append(expert.squeeze(2))  # (B, N)\n            experts = torch.stack(experts, 2)  # (B, N, K)\n\n            if self._num_experts > 1:\n                # MOE update\n                moe = torch.matmul(\n                    experts,\n                    # pyre-ignore[61]: `gating` may not be initialized here.\n                    torch.nn.functional.softmax(gating, 1),\n                )  # (B, N, 1)\n                x_l = moe + x_l  # (B, N, 1)\n            else:\n                x_l = experts + x_l  # (B, N, 1)\n\n        return torch.squeeze(x_l, dim=2)",
  "def _get_flatten_input(inputs: List[torch.Tensor]) -> torch.Tensor:\n    return torch.cat(\n        [input.flatten(1) for input in inputs],\n        dim=1,\n    )",
  "class DeepFM(nn.Module):\n    r\"\"\"\n    This is the `DeepFM module <https://arxiv.org/pdf/1703.04247.pdf>`_\n\n    This module does not cover the end-end functionality of the published paper.\n    Instead, it covers only the deep component of the publication. It is used to learn\n    high-order feature interactions. If low-order feature interactions should\n    be learnt, please use `FactorizationMachine` module instead, which will share\n    the same embedding input of this module.\n\n    To support modeling flexibility, we customize the key components as:\n\n    * Different from the public paper, we change the input from raw sparse features to\n      embeddings of the features. It allows flexibility in embedding dimensions and the\n      number of embeddings, as long as all embedding tensors have the same batch size.\n\n    * On top of the public paper, we allow users to customize the hidden layer to be any\n      module, not limited to just MLP.\n\n    The general architecture of the module is like::\n\n                                1 x 10                  output\n                                 /|\\\n                                  |                     pass into `dense_module`\n                                  |\n                                1 x 90\n                                 /|\\\n                                  |                     concat\n                                  |\n                        1 x 20, 1 x 30, 1 x 40          list of embeddings\n\n    Args:\n        dense_module (nn.Module):\n            any customized module that can be used (such as MLP) in DeepFM. The\n            `in_features` of this module must be equal to the element counts. For\n            example, if the input embedding is `[randn(3, 2, 3), randn(3, 4, 5)]`, the\n            `in_features` should be: 2*3+4*5.\n\n    Example::\n\n        import torch\n        from torchrec.fb.modules.deepfm import DeepFM\n        from torchrec.fb.modules.mlp import LazyMLP\n        batch_size = 3\n        output_dim = 30\n        # the input embedding are a torch.Tensor of [batch_size, num_embeddings, embedding_dim]\n        input_embeddings = [\n            torch.randn(batch_size, 2, 64),\n            torch.randn(batch_size, 2, 32),\n        ]\n        dense_module = nn.Linear(192, output_dim)\n        deepfm = DeepFM(dense_module=dense_module)\n        deep_fm_output = deepfm(embeddings=input_embeddings)\n    \"\"\"\n\n    def __init__(\n        self,\n        dense_module: nn.Module,\n    ) -> None:\n        super().__init__()\n        self.dense_module = dense_module\n\n    def forward(\n        self,\n        embeddings: List[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            embeddings (List[torch.Tensor]):\n                The list of all embeddings (e.g. dense, common_sparse,\n                specialized_sparse,\n                embedding_features, raw_embedding_features) in the shape of::\n\n                    (batch_size, num_embeddings, embedding_dim)\n\n                For the ease of operation, embeddings that have the same embedding\n                dimension have the option to be stacked into a single tensor. For\n                example, when we have 1 trained embedding with dimension=32, 5 native\n                embeddings with dimension=64, and 3 dense features with dimension=16, we\n                can prepare the embeddings list to be the list of::\n\n                    tensor(B, 1, 32) (trained_embedding with num_embeddings=1, embedding_dim=32)\n                    tensor(B, 5, 64) (native_embedding with num_embeddings=5, embedding_dim=64)\n                    tensor(B, 3, 16) (dense_features with num_embeddings=3, embedding_dim=32)\n\n                .. note::\n                    `batch_size` of all input tensors need to be identical.\n\n        Returns:\n            torch.Tensor: output of `dense_module` with flattened and concatenated `embeddings` as input.\n        \"\"\"\n\n        # flatten each embedding to be [B, N, D] -> [B, N*D], then cat them all on dim=1\n        deepfm_input = _get_flatten_input(embeddings)\n        deepfm_output = self.dense_module(deepfm_input)\n        return deepfm_output",
  "class FactorizationMachine(nn.Module):\n    r\"\"\"\n    This is the Factorization Machine module, mentioned in the `DeepFM paper\n    <https://arxiv.org/pdf/1703.04247.pdf>`_:\n\n    This module does not cover the end-end functionality of the published paper.\n    Instead, it covers only the FM part of the publication, and is used to learn\n    2nd-order feature interactions.\n\n    To support modeling flexibility, we customize the key components as different from\n    the public paper:\n        We change the input from raw sparse features to embeddings of the features.\n        This allows flexibility in embedding dimensions and the number of embeddings,\n        as long as all embedding tensors have the same batch size.\n\n    The general architecture of the module is like::\n\n                                1 x 10                  output\n                                 /|\\\n                                  |                     pass into `dense_module`\n                                  |\n                                1 x 90\n                                 /|\\\n                                  |                     concat\n                                  |\n                        1 x 20, 1 x 30, 1 x 40          list of embeddings\n\n    Example::\n\n        batch_size = 3\n        # the input embedding are in torch.Tensor of [batch_size, num_embeddings, embedding_dim]\n        input_embeddings = [\n            torch.randn(batch_size, 2, 64),\n            torch.randn(batch_size, 2, 32),\n        ]\n        fm = FactorizationMachine()\n        output = fm(embeddings=input_embeddings)\n    \"\"\"\n\n    def __init__(\n        self,\n    ) -> None:\n        super().__init__()\n\n    def forward(\n        self,\n        embeddings: List[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            embeddings (List[torch.Tensor]):\n                The list of all embeddings (e.g. dense, common_sparse,\n                specialized_sparse, embedding_features, raw_embedding_features) in the\n                shape of::\n\n                    (batch_size, num_embeddings, embedding_dim)\n\n                For the ease of operation, embeddings that have the same embedding\n                dimension have the option to be stacked into a single tensor. For\n                example, when we have 1 trained embedding with dimension=32, 5 native\n                embeddings with dimension=64, and 3 dense features with dimension=16, we\n                can prepare the embeddings list to be the list of::\n\n                    tensor(B, 1, 32) (trained_embedding with num_embeddings=1, embedding_dim=32)\n                    tensor(B, 5, 64) (native_embedding with num_embeddings=5, embedding_dim=64)\n                    tensor(B, 3, 16) (dense_features with num_embeddings=3, embedding_dim=32)\n\n                NOTE:\n                    `batch_size` of all input tensors need to be identical.\n\n        Returns:\n            torch.Tensor: output of fm with flattened and concatenated `embeddings` as input. Expected to be [B, 1].\n        \"\"\"\n\n        # flatten each embedding to be [B, N, D] -> [B, N*D], then cat them all on dim=1\n        fm_input = _get_flatten_input(embeddings)\n        sum_of_input = torch.sum(fm_input, dim=1, keepdim=True)\n        sum_of_square = torch.sum(fm_input * fm_input, dim=1, keepdim=True)\n        square_of_sum = sum_of_input * sum_of_input\n        cross_term = square_of_sum - sum_of_square\n        cross_term = torch.sum(cross_term, dim=1, keepdim=True) * 0.5  # [B, 1]\n        return cross_term",
  "def __init__(\n        self,\n        dense_module: nn.Module,\n    ) -> None:\n        super().__init__()\n        self.dense_module = dense_module",
  "def forward(\n        self,\n        embeddings: List[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            embeddings (List[torch.Tensor]):\n                The list of all embeddings (e.g. dense, common_sparse,\n                specialized_sparse,\n                embedding_features, raw_embedding_features) in the shape of::\n\n                    (batch_size, num_embeddings, embedding_dim)\n\n                For the ease of operation, embeddings that have the same embedding\n                dimension have the option to be stacked into a single tensor. For\n                example, when we have 1 trained embedding with dimension=32, 5 native\n                embeddings with dimension=64, and 3 dense features with dimension=16, we\n                can prepare the embeddings list to be the list of::\n\n                    tensor(B, 1, 32) (trained_embedding with num_embeddings=1, embedding_dim=32)\n                    tensor(B, 5, 64) (native_embedding with num_embeddings=5, embedding_dim=64)\n                    tensor(B, 3, 16) (dense_features with num_embeddings=3, embedding_dim=32)\n\n                .. note::\n                    `batch_size` of all input tensors need to be identical.\n\n        Returns:\n            torch.Tensor: output of `dense_module` with flattened and concatenated `embeddings` as input.\n        \"\"\"\n\n        # flatten each embedding to be [B, N, D] -> [B, N*D], then cat them all on dim=1\n        deepfm_input = _get_flatten_input(embeddings)\n        deepfm_output = self.dense_module(deepfm_input)\n        return deepfm_output",
  "def __init__(\n        self,\n    ) -> None:\n        super().__init__()",
  "def forward(\n        self,\n        embeddings: List[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            embeddings (List[torch.Tensor]):\n                The list of all embeddings (e.g. dense, common_sparse,\n                specialized_sparse, embedding_features, raw_embedding_features) in the\n                shape of::\n\n                    (batch_size, num_embeddings, embedding_dim)\n\n                For the ease of operation, embeddings that have the same embedding\n                dimension have the option to be stacked into a single tensor. For\n                example, when we have 1 trained embedding with dimension=32, 5 native\n                embeddings with dimension=64, and 3 dense features with dimension=16, we\n                can prepare the embeddings list to be the list of::\n\n                    tensor(B, 1, 32) (trained_embedding with num_embeddings=1, embedding_dim=32)\n                    tensor(B, 5, 64) (native_embedding with num_embeddings=5, embedding_dim=64)\n                    tensor(B, 3, 16) (dense_features with num_embeddings=3, embedding_dim=32)\n\n                NOTE:\n                    `batch_size` of all input tensors need to be identical.\n\n        Returns:\n            torch.Tensor: output of fm with flattened and concatenated `embeddings` as input. Expected to be [B, 1].\n        \"\"\"\n\n        # flatten each embedding to be [B, N, D] -> [B, N*D], then cat them all on dim=1\n        fm_input = _get_flatten_input(embeddings)\n        sum_of_input = torch.sum(fm_input, dim=1, keepdim=True)\n        sum_of_square = torch.sum(fm_input * fm_input, dim=1, keepdim=True)\n        square_of_sum = sum_of_input * sum_of_input\n        cross_term = square_of_sum - sum_of_square\n        cross_term = torch.sum(cross_term, dim=1, keepdim=True) * 0.5  # [B, 1]\n        return cross_term",
  "def extract_module_or_tensor_callable(\n    module_or_callable: Union[\n        Callable[[], torch.nn.Module],\n        torch.nn.Module,\n        Callable[[torch.Tensor], torch.Tensor],\n    ]\n) -> Union[torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]:\n    try:\n        # pyre-ignore[20]: PositionalOnly call expects argument in position 0\n        module = module_or_callable()\n        if isinstance(module, torch.nn.Module):\n            return module\n        else:\n            raise ValueError(\n                \"Expected callable that takes no input to return \"\n                \"a torch.nn.Module, but got: {}\".format(type(module))\n            )\n    except TypeError as e:\n        if \"required positional argument\" in str(e):\n            # pyre-ignore[7]: Expected `Union[typing.Callable[[torch.Tensor], torch.Tensor], torch.nn.Module]`\n            return module_or_callable\n        raise",
  "def get_module_output_dimension(\n    module: Union[Callable[[torch.Tensor], torch.Tensor], torch.nn.Module],\n    in_features: int,\n) -> int:\n    input = torch.zeros(1, in_features)\n    output = module(input)\n    return output.size(-1)",
  "def check_module_output_dimension(\n    module: Union[Iterable[torch.nn.Module], torch.nn.Module],\n    in_features: int,\n    out_features: int,\n) -> bool:\n    \"\"\"\n    Verify that the out_features of a given module or a list of modules matches the\n    specified number. If a list of modules or a ModuleList is given, recursively check\n    all the submodules.\n    \"\"\"\n    if isinstance(module, list) or isinstance(module, torch.nn.ModuleList):\n        return all(\n            check_module_output_dimension(submodule, in_features, out_features)\n            for submodule in module\n        )\n    else:\n        # pyre-fixme[6]: Expected `Union[typing.Callable[[torch.Tensor],\n        #  torch.Tensor], torch.nn.Module]` for 1st param but got\n        #  `Union[Iterable[torch.nn.Module], torch.nn.Module]`.\n        return get_module_output_dimension(module, in_features) == out_features",
  "def init_mlp_weights_xavier_uniform(m: torch.nn.Module) -> None:\n    if isinstance(m, torch.nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.0)",
  "def construct_modulelist_from_single_module(\n    module: torch.nn.Module, sizes: Tuple[int, ...]\n) -> torch.nn.Module:\n    \"\"\"\n    Given a single module, construct a (nested) ModuleList of size of sizes by making\n    copies of the provided module and reinitializing the Linear layers.\n    \"\"\"\n    if len(sizes) == 1:\n        return torch.nn.ModuleList(\n            [\n                copy.deepcopy(module).apply(init_mlp_weights_xavier_uniform)\n                for _ in range(sizes[0])\n            ]\n        )\n    else:\n        # recursively create nested ModuleList\n        return torch.nn.ModuleList(\n            [\n                construct_modulelist_from_single_module(module, sizes[1:])\n                for _ in range(sizes[0])\n            ]\n        )",
  "def convert_list_of_modules_to_modulelist(\n    modules: Iterable[torch.nn.Module], sizes: Tuple[int, ...]\n) -> torch.nn.Module:\n    assert (\n        # pyre-fixme[6]: Expected `Sized` for 1st param but got\n        #  `Iterable[torch.nn.Module]`.\n        len(modules)\n        == sizes[0]\n    ), f\"the counts of modules ({len(modules)}) do not match with the required counts {sizes}\"\n    if len(sizes) == 1:\n        return torch.nn.ModuleList(modules)\n    else:\n        # recursively create nested list\n        return torch.nn.ModuleList(\n            convert_list_of_modules_to_modulelist(m, sizes[1:]) for m in modules\n        )",
  "class PoolingType(Enum):\n    SUM = \"SUM\"\n    MEAN = \"MEAN\"\n    NONE = \"NONE\"",
  "def to_fbgemm_bounds_check_mode(\n    bounds_check_mode: BoundsCheckMode,\n) -> FbgemmBoundsCheckMode:\n    if bounds_check_mode == BoundsCheckMode.FATAL:\n        return FbgemmBoundsCheckMode.FATAL\n    elif bounds_check_mode == BoundsCheckMode.WARNING:\n        return FbgemmBoundsCheckMode.WARNING\n    elif bounds_check_mode == BoundsCheckMode.IGNORE:\n        return FbgemmBoundsCheckMode.IGNORE\n    elif bounds_check_mode == BoundsCheckMode.NONE:\n        return FbgemmBoundsCheckMode.NONE\n    else:\n        raise Exception(f\"Invalid bounds check mode {bounds_check_mode}\")",
  "def to_fbgemm_cache_algorithm(cache_algorithm: CacheAlgorithm) -> FbgemmCacheAlgorithm:\n    if cache_algorithm == CacheAlgorithm.LRU:\n        return FbgemmCacheAlgorithm.LRU\n    elif cache_algorithm == CacheAlgorithm.LFU:\n        return FbgemmCacheAlgorithm.LFU\n    else:\n        raise Exception(f\"Invalid cache algorithm {cache_algorithm}\")",
  "def dtype_to_data_type(dtype: torch.dtype) -> DataType:\n    if dtype == torch.float:\n        return DataType.FP32\n    elif dtype == torch.float16 or dtype == torch.half:\n        return DataType.FP16\n    elif dtype == torch.bfloat16:\n        return DataType.BF16\n    elif dtype in {torch.int, torch.int32}:\n        return DataType.INT32\n    elif dtype in {torch.long, torch.int64}:\n        return DataType.INT64\n    elif dtype in {torch.quint8, torch.qint8, torch.int8}:\n        return DataType.INT8\n    elif dtype == torch.uint8:\n        return DataType.UINT8\n    elif dtype == torch.quint4x2:\n        return DataType.INT4\n    elif dtype == torch.quint2x4:\n        return DataType.INT2\n    else:\n        raise Exception(f\"Invalid data type {dtype}\")",
  "def pooling_type_to_pooling_mode(pooling_type: PoolingType) -> PoolingMode:\n    if pooling_type.value == PoolingType.SUM.value:\n        return PoolingMode.SUM\n    elif pooling_type.value == PoolingType.MEAN.value:\n        return PoolingMode.MEAN\n    elif pooling_type.value == PoolingType.NONE.value:\n        return PoolingMode.NONE\n    else:\n        raise Exception(f\"Invalid pooling type {pooling_type}\")",
  "def pooling_type_to_str(pooling_type: PoolingType) -> str:\n    if pooling_type.value == PoolingType.SUM.value:\n        return \"sum\"\n    elif pooling_type.value == PoolingType.MEAN.value:\n        return \"mean\"\n    else:\n        raise ValueError(f\"Unsupported pooling type {pooling_type}\")",
  "def data_type_to_sparse_type(data_type: DataType) -> SparseType:\n    if data_type == DataType.FP32:\n        return SparseType.FP32\n    elif data_type == DataType.FP16:\n        return SparseType.FP16\n    elif data_type == DataType.BF16:\n        return SparseType.BF16\n    elif data_type == DataType.INT8 or data_type == DataType.UINT8:\n        return SparseType.INT8\n    elif data_type == DataType.INT4:\n        return SparseType.INT4\n    elif data_type == DataType.INT2:\n        return SparseType.INT2\n    else:\n        raise ValueError(f\"Invalid DataType {data_type}\")",
  "def data_type_to_dtype(data_type: DataType) -> torch.dtype:\n    if data_type.value == DataType.FP32.value:\n        return torch.float32\n    elif data_type.value == DataType.FP16.value:\n        return torch.float16\n    elif data_type.value == DataType.BF16.value:\n        return torch.bfloat16\n    elif data_type.value == DataType.INT64.value:\n        return torch.int64\n    elif data_type.value == DataType.INT32.value:\n        return torch.int32\n    elif data_type.value == DataType.INT8.value:\n        return torch.int8\n    elif data_type.value == DataType.UINT8.value:\n        return torch.uint8\n    elif data_type.value == DataType.INT4.value:\n        return torch.quint4x2\n    elif data_type.value == DataType.INT2.value:\n        return torch.quint2x4\n    else:\n        raise ValueError(f\"DataType {data_type} cannot be converted to dtype\")",
  "class BaseEmbeddingConfig:\n    num_embeddings: int\n    embedding_dim: int\n    name: str = \"\"\n    data_type: DataType = DataType.FP32\n    feature_names: List[str] = field(default_factory=list)\n    weight_init_max: Optional[float] = None\n    weight_init_min: Optional[float] = None\n\n    init_fn: Optional[Callable[[torch.Tensor], Optional[torch.Tensor]]] = None\n    # when the position_weighted feature is in this table config,\n    # enable this flag to support rw_sharding\n    need_pos: bool = False\n\n    def get_weight_init_max(self) -> float:\n        if self.weight_init_max is None:\n            return sqrt(1 / self.num_embeddings)\n        else:\n            return self.weight_init_max\n\n    def get_weight_init_min(self) -> float:\n        if self.weight_init_min is None:\n            return -sqrt(1 / self.num_embeddings)\n        else:\n            return self.weight_init_min\n\n    def num_features(self) -> int:\n        return len(self.feature_names)\n\n    def __post_init__(self) -> None:\n        if self.init_fn is None:\n            self.init_fn = partial(\n                torch.nn.init.uniform_,\n                a=self.get_weight_init_min(),\n                b=self.get_weight_init_max(),\n            )",
  "class EmbeddingTableConfig(BaseEmbeddingConfig):\n    pooling: PoolingType = PoolingType.SUM\n    is_weighted: bool = False\n    has_feature_processor: bool = False\n    embedding_names: List[str] = field(default_factory=list)",
  "class EmbeddingBagConfig(BaseEmbeddingConfig):\n    pooling: PoolingType = PoolingType.SUM",
  "class EmbeddingConfig(BaseEmbeddingConfig):\n    pass",
  "class TrecQuantConfig(NamedTuple):\n    activation: torch.quantization.PlaceholderObserver\n    weight: torch.quantization.PlaceholderObserver\n    per_table_weight_dtype: Optional[Dict[str, torch.dtype]] = None",
  "def get_weight_init_max(self) -> float:\n        if self.weight_init_max is None:\n            return sqrt(1 / self.num_embeddings)\n        else:\n            return self.weight_init_max",
  "def get_weight_init_min(self) -> float:\n        if self.weight_init_min is None:\n            return -sqrt(1 / self.num_embeddings)\n        else:\n            return self.weight_init_min",
  "def num_features(self) -> int:\n        return len(self.feature_names)",
  "def __post_init__(self) -> None:\n        if self.init_fn is None:\n            self.init_fn = partial(\n                torch.nn.init.uniform_,\n                a=self.get_weight_init_min(),\n                b=self.get_weight_init_max(),\n            )",
  "def tower_input_params(module: nn.Module) -> Tuple[bool, bool]:\n    \"\"\"\n    Utilty to compute the mapping of tower KJT args to pass to the embedding modules.\n\n    Args:\n        module (nn.Module):\n    Returns:\n        Tuple[bool, bool]: tuple of 2 booleans representing if KJT and weighted KJT are required, respectively.\n    \"\"\"\n    if isinstance(module, EmbeddingCollection):\n        return True, False\n    elif isinstance(module, EmbeddingBagCollection):\n        return not module.is_weighted(), module.is_weighted()\n    # default to assuming both kjt and weight_kjt required\n    return True, True",
  "class EmbeddingTower(nn.Module):\n    \"\"\"\n    Logical \"Tower\" of embeddings directly passed to provided interaction.\n    All TorchRec shardable embedding modules are supported.\n\n    Args:\n        embedding_module (nn.Module):\n        interaction_module (nn.Module):\n        device (Optional[torch.device]):\n\n    Example::\n\n        ebc, interaction = EmbeddingBagCollection(), MyInteractionModule()\n        tower = EmbeddingTower(ebc, interaction, device)\n        kjt = KeyedJaggedTensor()\n        output = tower(kjt)\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_module: nn.Module,\n        interaction_module: nn.Module,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self.embedding = embedding_module\n        self.interaction = interaction_module\n\n    def forward(\n        self,\n        # pyre-ignore [2]\n        *args,\n        # pyre-ignore [2]\n        **kwargs,\n    ) -> torch.Tensor:\n        \"\"\"\n        Executes the embedding module and interaction module.\n\n        Args:\n            *args (Any): user provided positional arguments.\n            **kwargs (Any): user provided keyword arguments.\n\n        Returns:\n            torch.Tensor: 2-D tensor of shape of `N X B`, where `B` is local batch size.\n        \"\"\"\n        embeddings = self.embedding(*args, **kwargs)\n        return self.interaction(embeddings)",
  "class EmbeddingTowerCollection(nn.Module):\n    \"\"\"\n    Collection of EmbeddingTowers.\n\n    Args:\n        towers (List[EmbeddingTower]): list of embedding towers that make up the\n            collection.\n        device (Optional[torch.device]): default compute device.\n\n    Example::\n\n        ebc, ebc_interaction = EmbeddingBagCollection(), MyEBCInteractionModule()\n        eb, eb_interaction = EmbeddingCollection(), MyECInteractionModule()\n        tower_0 = EmbeddingTower(ebc, ebc_interaction, device)\n        tower_1 = EmbeddingTower(eb, eb_interaction, device)\n        tower_collection = EmbeddingTowerCollection([tower_0, tower_1])\n        kjt = KeyedJaggedTensor()\n        output = tower_collection(kjt)\n    \"\"\"\n\n    def __init__(\n        self,\n        towers: List[EmbeddingTower],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self.towers = nn.ModuleList(towers)\n        self._input_params: List[Tuple[bool, bool]] = []\n        for tower in towers:\n            self._input_params.append(tower_input_params(tower.embedding))\n\n    def forward(\n        self,\n        features: Optional[KeyedJaggedTensor] = None,\n        weighted_features: Optional[KeyedJaggedTensor] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Executes the collection of towers.\n\n        Features and/or weighted features must be provided as required by the\n        underlying embedding modules.\n\n        Args:\n            features (Optional[KeyedJaggedTensor]):\n            weighted_features (Optional[KeyedJaggedTensor]):\n\n        Returns:\n            torch.Tensor: 2-D tensor of shape `M X B`, where `M = sum(N_i)` for tower output i, and `B` is local batch size.\n        \"\"\"\n\n        tower_outputs = []\n        for tower, input_params in zip(self.towers, self._input_params):\n            has_kjt_param, has_wkjt_param = input_params\n            if has_kjt_param and has_wkjt_param:\n                assert features is not None\n                assert weighted_features is not None\n                tower_outputs.append(tower(features, weighted_features))\n            elif has_wkjt_param:\n                assert weighted_features is not None\n                tower_outputs.append(tower(weighted_features))\n            else:\n                assert features is not None\n                tower_outputs.append(tower(features))\n\n        return torch.cat(tower_outputs, dim=1)",
  "def __init__(\n        self,\n        embedding_module: nn.Module,\n        interaction_module: nn.Module,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self.embedding = embedding_module\n        self.interaction = interaction_module",
  "def forward(\n        self,\n        # pyre-ignore [2]\n        *args,\n        # pyre-ignore [2]\n        **kwargs,\n    ) -> torch.Tensor:\n        \"\"\"\n        Executes the embedding module and interaction module.\n\n        Args:\n            *args (Any): user provided positional arguments.\n            **kwargs (Any): user provided keyword arguments.\n\n        Returns:\n            torch.Tensor: 2-D tensor of shape of `N X B`, where `B` is local batch size.\n        \"\"\"\n        embeddings = self.embedding(*args, **kwargs)\n        return self.interaction(embeddings)",
  "def __init__(\n        self,\n        towers: List[EmbeddingTower],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self.towers = nn.ModuleList(towers)\n        self._input_params: List[Tuple[bool, bool]] = []\n        for tower in towers:\n            self._input_params.append(tower_input_params(tower.embedding))",
  "def forward(\n        self,\n        features: Optional[KeyedJaggedTensor] = None,\n        weighted_features: Optional[KeyedJaggedTensor] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Executes the collection of towers.\n\n        Features and/or weighted features must be provided as required by the\n        underlying embedding modules.\n\n        Args:\n            features (Optional[KeyedJaggedTensor]):\n            weighted_features (Optional[KeyedJaggedTensor]):\n\n        Returns:\n            torch.Tensor: 2-D tensor of shape `M X B`, where `M = sum(N_i)` for tower output i, and `B` is local batch size.\n        \"\"\"\n\n        tower_outputs = []\n        for tower, input_params in zip(self.towers, self._input_params):\n            has_kjt_param, has_wkjt_param = input_params\n            if has_kjt_param and has_wkjt_param:\n                assert features is not None\n                assert weighted_features is not None\n                tower_outputs.append(tower(features, weighted_features))\n            elif has_wkjt_param:\n                assert weighted_features is not None\n                tower_outputs.append(tower(weighted_features))\n            else:\n                assert features is not None\n                tower_outputs.append(tower(features))\n\n        return torch.cat(tower_outputs, dim=1)",
  "class FeatureProcessor(nn.Module):\n    \"\"\"\n    Abstract base class for feature processor.\n\n    Args:\n        features (JaggedTensor]): feature representation\n\n    Returns:\n        JaggedTensor: modified JT\n\n\n    Example::\n        jt = JaggedTensor(...)\n        fp = FeatureProcessor(...)\n        fp_jt = FeatureProcessor(fp)\n    \"\"\"\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        features: JaggedTensor,\n    ) -> JaggedTensor:\n        \"\"\"\n        Args:\n        features (JaggedTensor]): feature representation\n\n        Returns:\n            JaggedTensor: modified JT\n        \"\"\"\n        pass",
  "class PositionWeightedModule(FeatureProcessor):\n    \"\"\"\n    Adds position weights to id list features.\n\n    Args:\n        `max_length`, a.k.a truncation size, specifies the maximum number of ids\n        each sample has. For each feature, its position weight parameter size is\n        `max_length`.\n    \"\"\"\n\n    def __init__(self, max_feature_length: int) -> None:\n        super().__init__()\n        self.position_weight = nn.Parameter(\n            torch.empty([max_feature_length]).fill_(1.0)\n        )\n\n    def forward(\n        self,\n        features: JaggedTensor,\n    ) -> JaggedTensor:\n\n        \"\"\"\n        Args:\n            features (JaggedTensor]): feature representation\n\n        Returns:\n            JaggedTensor: same as input features with `weights` field being populated.\n        \"\"\"\n\n        seq = torch.ops.fbgemm.offsets_range(\n            features.offsets().long(), torch.numel(features.values())\n        )\n        weighted_features = JaggedTensor(\n            values=features.values(),\n            lengths=features.lengths(),\n            offsets=features.offsets(),\n            weights=torch.gather(self.position_weight, dim=0, index=seq),\n        )\n        return weighted_features",
  "class FeatureProcessorsCollection(nn.Module):\n    \"\"\"\n    Abstract base class for feature processor.\n\n    Args:\n        features (KeyedJaggedTensor]): feature representation\n\n    Returns:\n        KeyedJaggedTensor: modified KJT\n\n\n    Example::\n        kjt = JaggedTensor(...)\n        grouped_fp = FeatureProcessorsCollection(...)\n        fp_kjt = grouped_fp(kjt)\n    \"\"\"\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedJaggedTensor:\n        \"\"\"\n        Args:\n        features (JaggedTensor]): feature representation\n\n        Returns:\n            JaggedTensor: modified JT\n        \"\"\"\n        pass",
  "def get_weights_list(\n    cat_seq: torch.Tensor,\n    features: KeyedJaggedTensor,\n    position_weights: Dict[str, nn.Parameter],\n) -> Optional[torch.Tensor]:\n    weights_list = []\n    seqs = torch.split(cat_seq, features.length_per_key())\n    for key, seq in zip(features.keys(), seqs):\n        if key in position_weights.keys():\n            weights_list.append(torch.gather(position_weights[key], dim=0, index=seq))\n        else:\n            weights_list.append(\n                torch.ones(seq.shape[0], device=features.values().device)\n            )\n    return torch.cat(weights_list) if weights_list else features.weights_or_none()",
  "class PositionWeightedModuleCollection(FeatureProcessorsCollection):\n    def __init__(\n        self, max_feature_lengths: Dict[str, int], device: Optional[torch.device] = None\n    ) -> None:\n        super().__init__()\n        self.max_feature_lengths = max_feature_lengths\n        for length in self.max_feature_lengths.values():\n            if length <= 0:\n                raise\n\n        self.position_weights: nn.ParameterDict = nn.ParameterDict()\n        # needed since nn.ParameterDict isn't torchscriptable (get_items)\n        self.position_weights_dict: Dict[str, nn.Parameter] = {}\n\n        for key, length in max_feature_lengths.items():\n            self.position_weights[key] = nn.Parameter(\n                torch.empty([length], device=device).fill_(1.0)\n            )\n            self.position_weights_dict[key] = self.position_weights[key]\n\n    def forward(self, features: KeyedJaggedTensor) -> KeyedJaggedTensor:\n        cat_seq = torch.ops.fbgemm.offsets_range(\n            features.offsets().long(), torch.numel(features.values())\n        )\n\n        return KeyedJaggedTensor(\n            keys=features.keys(),\n            values=features.values(),\n            weights=get_weights_list(cat_seq, features, self.position_weights_dict),\n            lengths=features.lengths(),\n            offsets=features.offsets(),\n            stride=features.stride(),\n            length_per_key=features.length_per_key(),\n        )",
  "def forward(\n        self,\n        features: JaggedTensor,\n    ) -> JaggedTensor:\n        \"\"\"\n        Args:\n        features (JaggedTensor]): feature representation\n\n        Returns:\n            JaggedTensor: modified JT\n        \"\"\"\n        pass",
  "def __init__(self, max_feature_length: int) -> None:\n        super().__init__()\n        self.position_weight = nn.Parameter(\n            torch.empty([max_feature_length]).fill_(1.0)\n        )",
  "def forward(\n        self,\n        features: JaggedTensor,\n    ) -> JaggedTensor:\n\n        \"\"\"\n        Args:\n            features (JaggedTensor]): feature representation\n\n        Returns:\n            JaggedTensor: same as input features with `weights` field being populated.\n        \"\"\"\n\n        seq = torch.ops.fbgemm.offsets_range(\n            features.offsets().long(), torch.numel(features.values())\n        )\n        weighted_features = JaggedTensor(\n            values=features.values(),\n            lengths=features.lengths(),\n            offsets=features.offsets(),\n            weights=torch.gather(self.position_weight, dim=0, index=seq),\n        )\n        return weighted_features",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedJaggedTensor:\n        \"\"\"\n        Args:\n        features (JaggedTensor]): feature representation\n\n        Returns:\n            JaggedTensor: modified JT\n        \"\"\"\n        pass",
  "def __init__(\n        self, max_feature_lengths: Dict[str, int], device: Optional[torch.device] = None\n    ) -> None:\n        super().__init__()\n        self.max_feature_lengths = max_feature_lengths\n        for length in self.max_feature_lengths.values():\n            if length <= 0:\n                raise\n\n        self.position_weights: nn.ParameterDict = nn.ParameterDict()\n        # needed since nn.ParameterDict isn't torchscriptable (get_items)\n        self.position_weights_dict: Dict[str, nn.Parameter] = {}\n\n        for key, length in max_feature_lengths.items():\n            self.position_weights[key] = nn.Parameter(\n                torch.empty([length], device=device).fill_(1.0)\n            )\n            self.position_weights_dict[key] = self.position_weights[key]",
  "def forward(self, features: KeyedJaggedTensor) -> KeyedJaggedTensor:\n        cat_seq = torch.ops.fbgemm.offsets_range(\n            features.offsets().long(), torch.numel(features.values())\n        )\n\n        return KeyedJaggedTensor(\n            keys=features.keys(),\n            values=features.values(),\n            weights=get_weights_list(cat_seq, features, self.position_weights_dict),\n            lengths=features.lengths(),\n            offsets=features.offsets(),\n            stride=features.stride(),\n            length_per_key=features.length_per_key(),\n        )",
  "class EmbeddingFusedOptimizer(FusedOptimizer):\n    \"\"\"\n    EmbeddingFusedOptimizer exposes the internal SplitTableBatchedEmbeddingBagsCodeGen optimizer state.\n    It can be used like a normal optimizer to perform functionalities such as loading/saving optimizers\n    and updating learning rates.\n\n    Args:\n        tables (List[BaseEmbeddingConfig]): list of embedding tables.\n        emb_module (SplitTableBatchedEmbeddingBagsCodeGen): Fbgemm module whose optimizer state we want to expose\n    Example:\n        See usage in _BatchedFusedEmbeddingLookups\n    \"\"\"\n\n    def __init__(  # noqa C901\n        self,\n        tables: List[BaseEmbeddingConfig],\n        emb_module: SplitTableBatchedEmbeddingBagsCodegen,\n    ) -> None:\n        self._emb_module: SplitTableBatchedEmbeddingBagsCodegen = emb_module\n\n        # pyre-ignore [33]\n        state: Dict[Any, Any] = {}\n        param_group: Dict[str, Any] = {\n            \"params\": [],\n            \"lr\": emb_module.optimizer_args.learning_rate,\n        }\n\n        params: Dict[str, torch.Tensor] = {}\n\n        # Fused optimizers use buffers (they don't use autograd) and we want to make sure\n        # that state_dict look identical to no-fused version.\n        split_embedding_weights = emb_module.split_embedding_weights()\n        split_optimizer_states = emb_module.split_optimizer_states()\n\n        for embedding_weight, optimizer_states, table in zip(\n            split_embedding_weights, split_optimizer_states, tables\n        ):\n            weight = embedding_weight\n\n            state[weight] = {}\n            param_group[\"params\"].append(weight)\n            param_key = table.name + \".weight\"\n            params[param_key] = weight\n\n            if len(optimizer_states) >= 1:\n                state[weight][f\"{param_key}.momentum_1\"] = optimizer_states[0]\n            if len(optimizer_states) >= 2:\n                state[weight][f\"{param_key}.momentum_2\"] = optimizer_states[1]\n\n        super().__init__(params, state, [param_group])\n\n    def zero_grad(self, set_to_none: bool = False) -> None:\n        # pyre-ignore [16]\n        self._emb_module.set_learning_rate(self.param_groups[0][\"lr\"])\n\n    # pyre-ignore [2]\n    def step(self, closure: Any = None) -> None:\n        # pyre-ignore [16]\n        self._emb_module.set_learning_rate(self.param_groups[0][\"lr\"])",
  "class _BatchedFusedEmbeddingLookups(nn.Module, FusedOptimizerModule):\n    \"\"\"\n    _BatchedFusedEmbeddingLookups is a thin wrapper we have around SplitTableBatchedEmbeddingBagsCodegen.\n    This is not meant to be directly used. Instead use FusedEmbeddingBagCollection/FusedEmbeddingCollection (which in turn utilizes this).\n\n    Example\n    -------\n    >>> See usage in FusedEmbeddingBagCollection and FusedEmbeddingCollection\n\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_tables: List[BaseEmbeddingConfig],\n        data_type: DataType,\n        pooling: PoolingType,\n        optimizer_type: EmbOptimType,\n        optimizer_kwargs: Dict[str, Any],\n        device: torch.device,\n        embedding_location: EmbeddingLocation,\n    ) -> None:\n        super().__init__()\n\n        self._rows: List[int] = []\n        self._weight_init_mins: List[float] = []\n        self._weight_init_maxs: List[float] = []\n        self._num_embeddings: List[int] = []\n        self._cols: List[int] = []\n        self._feature_table_map: List[int] = []\n        self._emb_names: List[str] = []\n        self._embedding_tables = embedding_tables\n\n        for idx, table in enumerate(embedding_tables):\n            self._rows.append(table.num_embeddings)\n            self._weight_init_mins.append(table.get_weight_init_min())\n            self._weight_init_maxs.append(table.get_weight_init_max())\n            self._num_embeddings.append(table.num_embeddings)\n            self._cols.append(table.embedding_dim)\n            self._feature_table_map.extend([idx] * table.num_features())\n\n        compute_device = ComputeDevice.CPU\n        if device.type == \"cuda\":\n            compute_device = ComputeDevice.CUDA\n\n        self._emb_module: SplitTableBatchedEmbeddingBagsCodegen = (\n            SplitTableBatchedEmbeddingBagsCodegen(\n                list(\n                    zip(\n                        self._num_embeddings,\n                        self._cols,\n                        [embedding_location] * len(embedding_tables),\n                        [compute_device] * len(embedding_tables),\n                    )\n                ),\n                feature_table_map=self._feature_table_map,\n                pooling_mode=pooling_type_to_pooling_mode(pooling),\n                device=device,\n                optimizer=optimizer_type,\n                **optimizer_kwargs,\n            )\n        )\n        self._optim: EmbeddingFusedOptimizer = EmbeddingFusedOptimizer(\n            embedding_tables,\n            self._emb_module,\n        )\n\n        self._init_parameters()\n\n    def forward(\n        self,\n        values: torch.Tensor,\n        offsets: torch.Tensor,\n        weights: Optional[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            values (torch.Tensor): Tensor containing bags of indicies into the embedding matrix.\n            offsets (torch.Tensor): Starting index position of each sequence\n            weights (Optional[torch.Tensor]): weights to use to calculate weighted pooling\n        Returns:\n            Tensor output of `(B , table_1_embedding_dim + table_2_embedding_dim + ...)`\n        \"\"\"\n\n        return self._emb_module(\n            indices=values,\n            offsets=offsets,\n            per_sample_weights=weights,\n        )\n\n    def _init_parameters(self) -> None:\n        assert len(self._num_embeddings) == len(\n            self._emb_module.split_embedding_weights()\n        )\n        for (rows, emb_dim, weight_init_min, weight_init_max, param) in zip(\n            self._rows,\n            self._cols,\n            self._weight_init_mins,\n            self._weight_init_maxs,\n            self._emb_module.split_embedding_weights(),\n        ):\n            assert param.shape == (rows, emb_dim)\n            param.data.uniform_(\n                weight_init_min,\n                weight_init_max,\n            )\n\n    def split_embedding_weights(self) -> List[torch.Tensor]:\n        return self._emb_module.split_embedding_weights()\n\n    def fused_optimizer(self) -> FusedOptimizer:\n        return self._optim\n\n    def parameters(\n        self, prefix: str = \"\", recurse: bool = True\n    ) -> Iterator[nn.Parameter]:\n        yield cast(nn.Parameter, self._emb_module.weights)\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        assert (\n            remove_duplicate\n        ), \"remove_duplicate=False not supported in _BatchedFusedEmbeddingLookups.named_parameters\"\n        for table, weight in zip(\n            self._embedding_tables, self.split_embedding_weights()\n        ):\n            name = table.name\n            key = f\"{prefix}.{name}\" if (prefix and name) else (prefix + name)\n            yield key, cast(nn.Parameter, weight)\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert (\n            remove_duplicate\n        ), \"remove_duplicate=False not supported in _BatchedFusedEmbeddingLookups.named_buffers\"\n        for table, param in zip(self._embedding_tables, self.split_embedding_weights()):\n            name = f\"{table.name}.weight\"\n            key = f\"{prefix}.{name}\" if (prefix and name) else (prefix + name)\n            yield key, param\n\n    def buffers(self, prefix: str = \"\", recurse: bool = True) -> Iterator[torch.Tensor]:\n        yield from self.split_embedding_weights()\n\n    def flush(self) -> None:\n        self._emb_module.flush()",
  "def convert_optimizer_type_and_kwargs(\n    optimizer_type: Type[torch.optim.Optimizer],\n    optimizer_kwargs: Dict[str, Any],\n) -> Optional[Tuple[EmbOptimType, Dict[str, Any]]]:\n    optimizer_kwargs = copy.deepcopy(optimizer_kwargs)\n    if \"lr\" in optimizer_kwargs:\n        optimizer_kwargs[\"learning_rate\"] = optimizer_kwargs[\"lr\"]\n        optimizer_kwargs.pop(\"lr\")\n\n    if optimizer_type == torch.optim.SGD:\n        return (\n            EmbOptimType.EXACT_SGD,\n            optimizer_kwargs,\n        )\n    elif optimizer_type == torch.optim.Adagrad:\n        return (EmbOptimType.EXACT_ADAGRAD, optimizer_kwargs)\n    elif optimizer_type == trec_optim.RowWiseAdagrad:\n        return (EmbOptimType.EXACT_ROWWISE_ADAGRAD, optimizer_kwargs)\n    elif optimizer_type == torch.optim.Adam:\n        return (EmbOptimType.ADAM, optimizer_kwargs)\n\n    return None",
  "class FusedEmbeddingBagCollection(\n    EmbeddingBagCollectionInterface, FusedOptimizerModule\n):\n    \"\"\"\n    FusedEmbeddingBagCollection represents a collection of pooled embeddings (`EmbeddingBags`).\n    It utilizes a technique called Optimizer fusion (register the optimizer with model). The semantics\n    of this is that during the backwards pass, the registered optimizer will be called.\n\n    It processes sparse data in the form of `KeyedJaggedTensor` with values of the form\n    [F X B X L] where:\n\n    * F: features (keys)\n    * B: batch size\n    * L: length of sparse features (jagged)\n\n    and outputs a `KeyedTensor` with values of the form [B x F x D] where:\n\n    * F: features (keys)\n    * D: each feature's (key's) embedding dimension\n    * B: batch size\n\n    Args:\n        tables (List[EmbeddingBagConfig]): list of embedding tables.\n        is_weighted (bool): whether input `KeyedJaggedTensor` is weighted.\n        optimizer (Type[torch.optim.Optimizer]): fusion optimizer type\n        optimizer_kwargs: Dict[str, Any]: fusion optimizer kwargs\n        device (Optional[torch.device]): compute device.\n\n    Example::\n\n        table_0 = EmbeddingBagConfig(\n            name=\"t1\", embedding_dim=4, num_embeddings=10, feature_names=[\"f1\"]\n        )\n        table_1 = EmbeddingBagConfig(\n            name=\"t2\", embedding_dim=8, num_embeddings=10, feature_names=[\"f2\"]\n        )\n\n        ebc = FusedEmbeddingBagCollection(tables=[table_0, table_1], optimizer_type=torch.optim.SGD, optimizer_kwargs={\"lr\": .01})\n\n        #        0       1        2  <-- batch\n        # \"f1\"   [0,1] None    [2]\n        # \"f2\"   [3]    [4]    [5,6,7]\n        #  ^\n        # feature\n\n        features = KeyedJaggedTensor(\n            keys=[\"f1\", \"f2\"],\n            values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n            offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n        )\n\n        pooled_embeddings = ebc(features)\n        print(pooled_embeddings.values())\n        tensor([[ 0.2093,  0.1395,  0.1571,  0.3583,  0.0421,  0.0037, -0.0692,  0.0663,\n          0.2166, -0.3150, -0.2771, -0.0301],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0165, -0.1225,  0.2483,  0.0624,\n         -0.1168, -0.0509, -0.1309,  0.3059],\n        [ 0.0811, -0.1779, -0.1443,  0.1097, -0.4410, -0.4036,  0.4458, -0.2735,\n         -0.3080, -0.2102, -0.0564,  0.5583]], grad_fn=<CatBackward0>)\n        print(pooled_embeddings.keys())\n        ['f1', 'f2']\n        print(pooled_embeddings.offset_per_key())\n        [0, 4, 12]\n    \"\"\"\n\n    def __init__(\n        self,\n        tables: List[EmbeddingBagConfig],\n        optimizer_type: Type[torch.optim.Optimizer],\n        optimizer_kwargs: Dict[str, Any],\n        is_weighted: bool = False,\n        device: Optional[torch.device] = None,\n        location: Optional[EmbeddingLocation] = None,\n    ) -> None:\n        super().__init__()\n\n        self._optimizer_type = optimizer_type\n        self._optimizer_kwargs = optimizer_kwargs\n        self._device: torch.device = (\n            device if device is not None else torch.device(\"cpu\")\n        )\n\n        emb_optim_and_kwargs = convert_optimizer_type_and_kwargs(\n            optimizer_type, optimizer_kwargs\n        )\n        if emb_optim_and_kwargs is None:\n            raise ValueError(\n                f\"Cannot fuse optimizer_type={optimizer_type} with kwargs {optimizer_kwargs}\"\n            )\n        (emb_optim_type, emb_opt_kwargs) = emb_optim_and_kwargs\n\n        if location in [\n            EmbeddingLocation.DEVICE,\n            EmbeddingLocation.MANAGED,\n            EmbeddingLocation.MANAGED_CACHING,\n        ]:\n            assert device is not None and device.type in [\n                \"cuda\",\n                \"meta\",\n            ], f\"Using location={location} requires device=cuda or meta\"\n\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        if location is None:\n            if device.type in [\"cpu\", \"meta\"]:\n                location = EmbeddingLocation.HOST\n            elif device.type == \"cuda\":\n                location = EmbeddingLocation.DEVICE\n            else:\n                raise ValueError(\"EmbeddingLocation could not be set\")\n\n        self._is_weighted = is_weighted\n        self._embedding_bag_configs = tables\n\n        # Registering in a List instead of ModuleList because we want don't want them to be auto-registered.\n        # Their states will be modified via self.embedding_bags\n        self._emb_modules: List[nn.Module] = []\n\n        self._key_to_tables: Dict[\n            Tuple[PoolingType, DataType], List[EmbeddingBagConfig]\n        ] = defaultdict(list)\n\n        self._length_per_key: List[int] = []\n\n        for table in tables:\n            self._length_per_key.extend(\n                [table.embedding_dim] * len(table.feature_names)\n            )\n\n            key = (table.pooling, table.data_type)\n            self._key_to_tables[key].append(table)\n\n        optims = []\n        for key, tables in self._key_to_tables.items():\n            (pooling, data_type) = key\n            emb_module = _BatchedFusedEmbeddingLookups(\n                cast(List[BaseEmbeddingConfig], tables),\n                data_type=data_type,\n                pooling=pooling,\n                optimizer_type=emb_optim_type,\n                optimizer_kwargs=emb_opt_kwargs,\n                device=device,\n                embedding_location=location,\n            )\n            self._emb_modules.append(emb_module)\n            params: Dict[str, torch.Tensor] = {}\n            for param_key, weight in emb_module.fused_optimizer().params.items():\n                params[f\"embedding_bags.{param_key}\"] = weight\n            optims.append((\"\", emb_module.fused_optimizer()))\n\n        self._optim: CombinedOptimizer = CombinedOptimizer(optims)\n        self._embedding_names: List[str] = list(\n            itertools.chain(*get_embedding_names_by_table(self._embedding_bag_configs))\n        )\n\n        # We map over the parameters from FBGEMM backed kernels to the canonical nn.EmbeddingBag\n        # representation. This provides consistency between this class and the EmbeddingBagCollection's\n        # nn.Module API calls (state_dict, named_modules, etc)\n        self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n        for (_key, tables), emb_module in zip(\n            self._key_to_tables.items(), self._emb_modules\n        ):\n            for embedding_config, weight in zip(\n                tables,\n                emb_module.split_embedding_weights(),\n                #  torch._tensor.Tensor]` is not a function.\n            ):\n                self.embedding_bags[embedding_config.name] = torch.nn.Module()\n                self.embedding_bags[embedding_config.name].register_parameter(\n                    \"weight\", torch.nn.Parameter(weight)\n                )\n\n    def forward(self, features: KeyedJaggedTensor) -> KeyedTensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            KeyedTensor\n        \"\"\"\n        assert features is not None\n        feature_dict = features.to_dict()\n        embeddings = []\n\n        for emb_op, (_key, tables) in zip(\n            self._emb_modules, self._key_to_tables.items()\n        ):\n            indicies = []\n            lengths = []\n            offsets = []\n            weights = []\n\n            for table in tables:\n                for feature in table.feature_names:\n                    f = feature_dict[feature]\n                    indicies.append(f.values())\n                    lengths.append(f.lengths())\n                    if self._is_weighted:\n                        weights.append(f.weights())\n\n            indicies = torch.cat(indicies)\n            lengths = torch.cat(lengths)\n\n            offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)\n            if self._is_weighted:\n                weights = torch.cat(weights)\n\n            embeddings.append(\n                emb_op(\n                    indicies.int(),\n                    offsets.int(),\n                    weights if self._is_weighted else None,\n                )\n            )\n\n        embeddings = torch.cat(embeddings, dim=1)\n\n        return KeyedTensor(\n            keys=self._embedding_names,\n            values=embeddings,\n            length_per_key=self._length_per_key,\n        )\n\n    def _get_name(self) -> str:\n        return \"FusedEmbeddingBagCollection\"\n\n    @property\n    def device(self) -> torch.device:\n        return self._device\n\n    def embedding_bag_configs(self) -> List[EmbeddingBagConfig]:\n        return self._embedding_bag_configs\n\n    def is_weighted(self) -> bool:\n        return self._is_weighted\n\n    def optimizer_type(self) -> Type[torch.optim.Optimizer]:\n        return self._optimizer_type\n\n    def optimizer_kwargs(self) -> Dict[str, Any]:\n        return self._optimizer_kwargs\n\n    def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim",
  "class FusedEmbeddingCollection(EmbeddingCollectionInterface, FusedOptimizerModule):\n    \"\"\"\n    EmbeddingCollection represents a unsharded collection of non-pooled embeddings. The semantics\n    of this module is that during the backwards pass, the registered optimizer will be called.\n\n    It processes sparse data in the form of `KeyedJaggedTensor` of the form [F X B X L]\n    where:\n\n    * F: features (keys)\n    * B: batch size\n    * L: length of sparse features (variable)\n\n    and outputs `Dict[feature (key), JaggedTensor]`.\n    Each `JaggedTensor` contains values of the form (B * L) X D\n    where:\n\n    * B: batch size\n    * L: length of sparse features (jagged)\n    * D: each feature's (key's) embedding dimension and lengths are of the form L\n\n    Args:\n        tables (List[EmbeddingConfig]): list of embedding tables.\n        device (Optional[torch.device]): default compute device.\n        need_indices (bool): if we need to pass indices to the final lookup dict.\n\n    Example::\n\n        e1_config = EmbeddingConfig(\n            name=\"t1\", embedding_dim=3, num_embeddings=10, feature_names=[\"f1\"]\n        )\n        e2_config = EmbeddingConfig(\n            name=\"t2\", embedding_dim=3, num_embeddings=10, feature_names=[\"f2\"]\n        )\n\n        ec = EmbeddingCollection(tables=[e1_config, e2_config])\n\n        #     0       1        2  <-- batch\n        # 0   [0,1] None    [2]\n        # 1   [3]    [4]    [5,6,7]\n        # ^\n        # feature\n\n        features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f1\", \"f2\"],\n            values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n            offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n        )\n        feature_embeddings = ec(features)\n        print(feature_embeddings['f2'].values())\n        tensor([[-0.2050,  0.5478,  0.6054],\n        [ 0.7352,  0.3210, -3.0399],\n        [ 0.1279, -0.1756, -0.4130],\n        [ 0.7519, -0.4341, -0.0499],\n        [ 0.9329, -1.0697, -0.8095]], grad_fn=<EmbeddingBackward>)\n    \"\"\"\n\n    # noqa lint\n    def __init__(\n        self,\n        tables: List[EmbeddingConfig],\n        optimizer_type: Type[torch.optim.Optimizer],\n        optimizer_kwargs: Dict[str, Any],\n        device: Optional[torch.device] = None,\n        need_indices: bool = False,\n        location: Optional[EmbeddingLocation] = None,\n    ) -> None:\n        super().__init__()\n\n        self._optimizer_type = optimizer_type\n        self._optimizer_kwargs = optimizer_kwargs\n\n        emb_optim_and_kwargs = convert_optimizer_type_and_kwargs(\n            optimizer_type, optimizer_kwargs\n        )\n        if emb_optim_and_kwargs is None:\n            raise ValueError(\n                f\"Cannot fuse optimizer_type={optimizer_type} with kwargs {optimizer_kwargs}\"\n            )\n        (emb_optim_type, emb_opt_kwargs) = emb_optim_and_kwargs\n\n        if location in [\n            EmbeddingLocation.DEVICE,\n            EmbeddingLocation.MANAGED,\n            EmbeddingLocation.MANAGED_CACHING,\n        ]:\n            assert device is not None and device.type in [\n                \"cuda\",\n                \"meta\",\n            ], f\"Using location={location} requires device=cuda or meta\"\n\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        assert device.type in [\n            \"cuda\",\n            \"meta\",\n        ], \"FusedEmbeddingCollection is only supported for device in [CUDA, meta] currently. There are plans to support device=CPU.\"\n\n        if location is None:\n            if device.type in [\"cpu\", \"meta\"]:\n                location = EmbeddingLocation.HOST\n            elif device.type == \"cuda\":\n                location = EmbeddingLocation.DEVICE\n            else:\n                raise ValueError(\"EmbeddingLocation could not be set\")\n\n        self._embedding_configs = tables\n        self._need_indices: bool = need_indices\n        self._embedding_dim: int = -1\n\n        # Registering in a List instead of ModuleList because we want don't want them to be auto-registered.\n        # Their states will be modified via self.embedding_bags\n        self._emb_modules: List[nn.Module] = []\n\n        self._key_to_tables: Dict[DataType, List[EmbeddingConfig]] = defaultdict(list)\n\n        seen_features = set()\n        self._shared_features: Set[str] = set()\n        for table in tables:\n            key = table.data_type\n            self._key_to_tables[key].append(table)\n\n            if self._embedding_dim == -1:\n                self._embedding_dim = table.embedding_dim\n            elif self._embedding_dim != table.embedding_dim:\n                raise ValueError(\n                    \"All tables in a EmbeddingCollection are required to have same embedding dimension.\"\n                )\n            for feature in table.feature_names:\n                if feature in seen_features:\n                    self._shared_features.add(feature)\n                else:\n                    seen_features.add(feature)\n\n        optims = []\n        for key, tables in self._key_to_tables.items():\n            data_type = key\n            emb_module = _BatchedFusedEmbeddingLookups(\n                cast(List[BaseEmbeddingConfig], tables),\n                data_type=data_type,\n                pooling=PoolingType.NONE,\n                optimizer_type=emb_optim_type,\n                optimizer_kwargs=emb_opt_kwargs,\n                device=device,\n                embedding_location=location,\n            )\n            self._emb_modules.append(emb_module)\n            params: Dict[str, torch.Tensor] = {}\n            for param_key, weight in emb_module.fused_optimizer().params.items():\n                params[f\"embeddings.{param_key}\"] = weight\n            optims.append((\"\", emb_module.fused_optimizer()))\n\n        self._optim: CombinedOptimizer = CombinedOptimizer(optims)\n        self._embedding_names: List[str] = list(\n            itertools.chain(*get_embedding_names_by_table(self._embedding_configs))\n        )\n\n        self._embedding_names_by_table: List[List[str]] = get_embedding_names_by_table(\n            self._embedding_configs,\n        )\n\n        # We map over the parameters from FBGEMM backed kernels to the canonical nn.EmbeddingBag\n        # representation. This provides consistency between this class and the EmbeddingBagCollection's\n        # nn.Module API calls (state_dict, named_modules, etc)\n        self.embeddings: nn.ModuleDict = nn.ModuleDict()\n        for (_key, tables), emb_module in zip(\n            self._key_to_tables.items(), self._emb_modules\n        ):\n            for embedding_config, weight in zip(\n                tables,\n                emb_module.split_embedding_weights(),\n                #  torch._tensor.Tensor]` is not a function.\n            ):\n                self.embeddings[embedding_config.name] = torch.nn.Module()\n                self.embeddings[embedding_config.name].register_parameter(\n                    \"weight\", torch.nn.Parameter(weight)\n                )\n\n    def forward(self, features: KeyedJaggedTensor) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            Dict[str, JaggedTensor]\n        \"\"\"\n        assert features is not None\n        feature_dict = features.to_dict()\n\n        feature_embeddings: Dict[str, JaggedTensor] = {}\n\n        for emb_op, (_key, tables) in zip(\n            self._emb_modules, self._key_to_tables.items()\n        ):\n            indicies = []\n            lengths = []\n            offsets = []\n\n            feature_names = []\n            feature_lengths = []\n            feature_values = []\n            splits = []\n\n            for table in tables:\n                for feature in table.feature_names:\n                    f = feature_dict[feature]\n                    indicies.append(f.values())\n                    lengths.append(f.lengths())\n\n                    if feature in self._shared_features:\n                        feature = f\"{feature}@{table.name}\"\n\n                    feature_names.append(feature)\n                    feature_values.append(f.values())\n                    feature_lengths.append(f.lengths())\n                    splits.append(torch.sum(feature_lengths[-1]))\n\n            indicies = torch.cat(indicies)\n            lengths = torch.cat(lengths)\n            offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)\n\n            lookups = emb_op(indicies.int(), offsets.int(), weights=None)\n            lookups = torch.split(lookups, split_size_or_sections=splits)\n\n            for feature, lookup, feature_length, values in zip(\n                feature_names, lookups, feature_lengths, feature_values\n            ):\n                feature_embeddings[feature] = JaggedTensor(\n                    values=lookup,\n                    lengths=feature_length,\n                    # hack to return kJT positional indicies in return type.\n                    weights=values if self.need_indices() else None,\n                )\n\n        return feature_embeddings\n\n    def _get_name(self) -> str:\n        return \"FusedEmbeddingCollection\"\n\n    def embedding_configs(self) -> List[EmbeddingConfig]:\n        return self._embedding_configs\n\n    def embedding_names_by_table(self) -> List[List[str]]:\n        return self._embedding_names_by_table\n\n    def embedding_dim(self) -> int:\n        return self._embedding_dim\n\n    def optimizer_type(self) -> Type[torch.optim.Optimizer]:\n        return self._optimizer_type\n\n    def optimizer_kwargs(self) -> Dict[str, Any]:\n        return self._optimizer_kwargs\n\n    def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim\n\n    def need_indices(self) -> bool:\n        return self._need_indices",
  "def fuse_embedding_optimizer(\n    model: nn.Module,\n    optimizer_type: Type[torch.optim.Optimizer],\n    optimizer_kwargs: Dict[str, Any],\n    device: torch.device,\n    location: Optional[EmbeddingLocation] = None,\n) -> nn.Module:\n    \"\"\"\n    Recursively replaces EmbeddingBagCollection and EmbeddingCollection with\n    FusedEmbeddingBagCollection and FusedEmbeddingCollection in a model subtree.\n    The fused modules will be initialized using the passed in optimizer parameters, and model location.\n\n    Args:\n        model: (nn.Module):\n        optimizer_type: (Type[torch.optim.Optimizer]):\n        optimizer_kwargs: (Dict[Str, Any]):\n        device (Optional[torch.device]):\n        location: (Optional[EmbeddingLocation]): GPU location placement\n    Returns\n        nn.Module: input nn.Module with Fused Embedding Modules\n\n    Example::\n        ebc = EmbeddingBagCollection()\n        my_model = ExampleModel(ebc)\n        my_model = fused_embedding_optimizer(my_model, optimizer_type=torch.optim.SGD, optimizer_kwargs={\"lr\": .01})\n        kjt = KeyedJaggedTensor()\n        output = my_model(kjt)\n    \"\"\"\n    # Replace all EBCs and ECs in a with a corresponding FusedEmbeddingModule.\n\n    # check if top-level module is EBC/EC\n    if isinstance(model, EmbeddingBagCollection):\n        return FusedEmbeddingBagCollection(\n            model.embedding_bag_configs(),\n            optimizer_type=optimizer_type,\n            optimizer_kwargs=optimizer_kwargs,\n            device=device,\n            location=location,\n        )\n    if isinstance(model, EmbeddingCollection):\n        return FusedEmbeddingCollection(\n            model.embedding_configs(),\n            optimizer_type=optimizer_type,\n            optimizer_kwargs=optimizer_kwargs,\n            device=device,\n            location=location,\n        )\n\n    def replace(_model: nn.Module) -> None:\n        for child_name, child in _model.named_children():\n            if isinstance(child, EmbeddingBagCollection):\n                setattr(\n                    _model,\n                    child_name,\n                    FusedEmbeddingBagCollection(\n                        tables=child.embedding_bag_configs(),\n                        optimizer_type=optimizer_type,\n                        optimizer_kwargs=optimizer_kwargs,\n                        device=device,\n                        location=location,\n                    ),\n                )\n            elif isinstance(child, EmbeddingCollection):\n                setattr(\n                    _model,\n                    child_name,\n                    FusedEmbeddingCollection(\n                        tables=child.embedding_configs(),\n                        optimizer_type=optimizer_type,\n                        optimizer_kwargs=optimizer_kwargs,\n                        device=device,\n                        location=location,\n                    ),\n                )\n            else:\n                replace(child)\n\n    replace(model)\n    return model",
  "def __init__(  # noqa C901\n        self,\n        tables: List[BaseEmbeddingConfig],\n        emb_module: SplitTableBatchedEmbeddingBagsCodegen,\n    ) -> None:\n        self._emb_module: SplitTableBatchedEmbeddingBagsCodegen = emb_module\n\n        # pyre-ignore [33]\n        state: Dict[Any, Any] = {}\n        param_group: Dict[str, Any] = {\n            \"params\": [],\n            \"lr\": emb_module.optimizer_args.learning_rate,\n        }\n\n        params: Dict[str, torch.Tensor] = {}\n\n        # Fused optimizers use buffers (they don't use autograd) and we want to make sure\n        # that state_dict look identical to no-fused version.\n        split_embedding_weights = emb_module.split_embedding_weights()\n        split_optimizer_states = emb_module.split_optimizer_states()\n\n        for embedding_weight, optimizer_states, table in zip(\n            split_embedding_weights, split_optimizer_states, tables\n        ):\n            weight = embedding_weight\n\n            state[weight] = {}\n            param_group[\"params\"].append(weight)\n            param_key = table.name + \".weight\"\n            params[param_key] = weight\n\n            if len(optimizer_states) >= 1:\n                state[weight][f\"{param_key}.momentum_1\"] = optimizer_states[0]\n            if len(optimizer_states) >= 2:\n                state[weight][f\"{param_key}.momentum_2\"] = optimizer_states[1]\n\n        super().__init__(params, state, [param_group])",
  "def zero_grad(self, set_to_none: bool = False) -> None:\n        # pyre-ignore [16]\n        self._emb_module.set_learning_rate(self.param_groups[0][\"lr\"])",
  "def step(self, closure: Any = None) -> None:\n        # pyre-ignore [16]\n        self._emb_module.set_learning_rate(self.param_groups[0][\"lr\"])",
  "def __init__(\n        self,\n        embedding_tables: List[BaseEmbeddingConfig],\n        data_type: DataType,\n        pooling: PoolingType,\n        optimizer_type: EmbOptimType,\n        optimizer_kwargs: Dict[str, Any],\n        device: torch.device,\n        embedding_location: EmbeddingLocation,\n    ) -> None:\n        super().__init__()\n\n        self._rows: List[int] = []\n        self._weight_init_mins: List[float] = []\n        self._weight_init_maxs: List[float] = []\n        self._num_embeddings: List[int] = []\n        self._cols: List[int] = []\n        self._feature_table_map: List[int] = []\n        self._emb_names: List[str] = []\n        self._embedding_tables = embedding_tables\n\n        for idx, table in enumerate(embedding_tables):\n            self._rows.append(table.num_embeddings)\n            self._weight_init_mins.append(table.get_weight_init_min())\n            self._weight_init_maxs.append(table.get_weight_init_max())\n            self._num_embeddings.append(table.num_embeddings)\n            self._cols.append(table.embedding_dim)\n            self._feature_table_map.extend([idx] * table.num_features())\n\n        compute_device = ComputeDevice.CPU\n        if device.type == \"cuda\":\n            compute_device = ComputeDevice.CUDA\n\n        self._emb_module: SplitTableBatchedEmbeddingBagsCodegen = (\n            SplitTableBatchedEmbeddingBagsCodegen(\n                list(\n                    zip(\n                        self._num_embeddings,\n                        self._cols,\n                        [embedding_location] * len(embedding_tables),\n                        [compute_device] * len(embedding_tables),\n                    )\n                ),\n                feature_table_map=self._feature_table_map,\n                pooling_mode=pooling_type_to_pooling_mode(pooling),\n                device=device,\n                optimizer=optimizer_type,\n                **optimizer_kwargs,\n            )\n        )\n        self._optim: EmbeddingFusedOptimizer = EmbeddingFusedOptimizer(\n            embedding_tables,\n            self._emb_module,\n        )\n\n        self._init_parameters()",
  "def forward(\n        self,\n        values: torch.Tensor,\n        offsets: torch.Tensor,\n        weights: Optional[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            values (torch.Tensor): Tensor containing bags of indicies into the embedding matrix.\n            offsets (torch.Tensor): Starting index position of each sequence\n            weights (Optional[torch.Tensor]): weights to use to calculate weighted pooling\n        Returns:\n            Tensor output of `(B , table_1_embedding_dim + table_2_embedding_dim + ...)`\n        \"\"\"\n\n        return self._emb_module(\n            indices=values,\n            offsets=offsets,\n            per_sample_weights=weights,\n        )",
  "def _init_parameters(self) -> None:\n        assert len(self._num_embeddings) == len(\n            self._emb_module.split_embedding_weights()\n        )\n        for (rows, emb_dim, weight_init_min, weight_init_max, param) in zip(\n            self._rows,\n            self._cols,\n            self._weight_init_mins,\n            self._weight_init_maxs,\n            self._emb_module.split_embedding_weights(),\n        ):\n            assert param.shape == (rows, emb_dim)\n            param.data.uniform_(\n                weight_init_min,\n                weight_init_max,\n            )",
  "def split_embedding_weights(self) -> List[torch.Tensor]:\n        return self._emb_module.split_embedding_weights()",
  "def fused_optimizer(self) -> FusedOptimizer:\n        return self._optim",
  "def parameters(\n        self, prefix: str = \"\", recurse: bool = True\n    ) -> Iterator[nn.Parameter]:\n        yield cast(nn.Parameter, self._emb_module.weights)",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        assert (\n            remove_duplicate\n        ), \"remove_duplicate=False not supported in _BatchedFusedEmbeddingLookups.named_parameters\"\n        for table, weight in zip(\n            self._embedding_tables, self.split_embedding_weights()\n        ):\n            name = table.name\n            key = f\"{prefix}.{name}\" if (prefix and name) else (prefix + name)\n            yield key, cast(nn.Parameter, weight)",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert (\n            remove_duplicate\n        ), \"remove_duplicate=False not supported in _BatchedFusedEmbeddingLookups.named_buffers\"\n        for table, param in zip(self._embedding_tables, self.split_embedding_weights()):\n            name = f\"{table.name}.weight\"\n            key = f\"{prefix}.{name}\" if (prefix and name) else (prefix + name)\n            yield key, param",
  "def buffers(self, prefix: str = \"\", recurse: bool = True) -> Iterator[torch.Tensor]:\n        yield from self.split_embedding_weights()",
  "def flush(self) -> None:\n        self._emb_module.flush()",
  "def __init__(\n        self,\n        tables: List[EmbeddingBagConfig],\n        optimizer_type: Type[torch.optim.Optimizer],\n        optimizer_kwargs: Dict[str, Any],\n        is_weighted: bool = False,\n        device: Optional[torch.device] = None,\n        location: Optional[EmbeddingLocation] = None,\n    ) -> None:\n        super().__init__()\n\n        self._optimizer_type = optimizer_type\n        self._optimizer_kwargs = optimizer_kwargs\n        self._device: torch.device = (\n            device if device is not None else torch.device(\"cpu\")\n        )\n\n        emb_optim_and_kwargs = convert_optimizer_type_and_kwargs(\n            optimizer_type, optimizer_kwargs\n        )\n        if emb_optim_and_kwargs is None:\n            raise ValueError(\n                f\"Cannot fuse optimizer_type={optimizer_type} with kwargs {optimizer_kwargs}\"\n            )\n        (emb_optim_type, emb_opt_kwargs) = emb_optim_and_kwargs\n\n        if location in [\n            EmbeddingLocation.DEVICE,\n            EmbeddingLocation.MANAGED,\n            EmbeddingLocation.MANAGED_CACHING,\n        ]:\n            assert device is not None and device.type in [\n                \"cuda\",\n                \"meta\",\n            ], f\"Using location={location} requires device=cuda or meta\"\n\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        if location is None:\n            if device.type in [\"cpu\", \"meta\"]:\n                location = EmbeddingLocation.HOST\n            elif device.type == \"cuda\":\n                location = EmbeddingLocation.DEVICE\n            else:\n                raise ValueError(\"EmbeddingLocation could not be set\")\n\n        self._is_weighted = is_weighted\n        self._embedding_bag_configs = tables\n\n        # Registering in a List instead of ModuleList because we want don't want them to be auto-registered.\n        # Their states will be modified via self.embedding_bags\n        self._emb_modules: List[nn.Module] = []\n\n        self._key_to_tables: Dict[\n            Tuple[PoolingType, DataType], List[EmbeddingBagConfig]\n        ] = defaultdict(list)\n\n        self._length_per_key: List[int] = []\n\n        for table in tables:\n            self._length_per_key.extend(\n                [table.embedding_dim] * len(table.feature_names)\n            )\n\n            key = (table.pooling, table.data_type)\n            self._key_to_tables[key].append(table)\n\n        optims = []\n        for key, tables in self._key_to_tables.items():\n            (pooling, data_type) = key\n            emb_module = _BatchedFusedEmbeddingLookups(\n                cast(List[BaseEmbeddingConfig], tables),\n                data_type=data_type,\n                pooling=pooling,\n                optimizer_type=emb_optim_type,\n                optimizer_kwargs=emb_opt_kwargs,\n                device=device,\n                embedding_location=location,\n            )\n            self._emb_modules.append(emb_module)\n            params: Dict[str, torch.Tensor] = {}\n            for param_key, weight in emb_module.fused_optimizer().params.items():\n                params[f\"embedding_bags.{param_key}\"] = weight\n            optims.append((\"\", emb_module.fused_optimizer()))\n\n        self._optim: CombinedOptimizer = CombinedOptimizer(optims)\n        self._embedding_names: List[str] = list(\n            itertools.chain(*get_embedding_names_by_table(self._embedding_bag_configs))\n        )\n\n        # We map over the parameters from FBGEMM backed kernels to the canonical nn.EmbeddingBag\n        # representation. This provides consistency between this class and the EmbeddingBagCollection's\n        # nn.Module API calls (state_dict, named_modules, etc)\n        self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n        for (_key, tables), emb_module in zip(\n            self._key_to_tables.items(), self._emb_modules\n        ):\n            for embedding_config, weight in zip(\n                tables,\n                emb_module.split_embedding_weights(),\n                #  torch._tensor.Tensor]` is not a function.\n            ):\n                self.embedding_bags[embedding_config.name] = torch.nn.Module()\n                self.embedding_bags[embedding_config.name].register_parameter(\n                    \"weight\", torch.nn.Parameter(weight)\n                )",
  "def forward(self, features: KeyedJaggedTensor) -> KeyedTensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            KeyedTensor\n        \"\"\"\n        assert features is not None\n        feature_dict = features.to_dict()\n        embeddings = []\n\n        for emb_op, (_key, tables) in zip(\n            self._emb_modules, self._key_to_tables.items()\n        ):\n            indicies = []\n            lengths = []\n            offsets = []\n            weights = []\n\n            for table in tables:\n                for feature in table.feature_names:\n                    f = feature_dict[feature]\n                    indicies.append(f.values())\n                    lengths.append(f.lengths())\n                    if self._is_weighted:\n                        weights.append(f.weights())\n\n            indicies = torch.cat(indicies)\n            lengths = torch.cat(lengths)\n\n            offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)\n            if self._is_weighted:\n                weights = torch.cat(weights)\n\n            embeddings.append(\n                emb_op(\n                    indicies.int(),\n                    offsets.int(),\n                    weights if self._is_weighted else None,\n                )\n            )\n\n        embeddings = torch.cat(embeddings, dim=1)\n\n        return KeyedTensor(\n            keys=self._embedding_names,\n            values=embeddings,\n            length_per_key=self._length_per_key,\n        )",
  "def _get_name(self) -> str:\n        return \"FusedEmbeddingBagCollection\"",
  "def device(self) -> torch.device:\n        return self._device",
  "def embedding_bag_configs(self) -> List[EmbeddingBagConfig]:\n        return self._embedding_bag_configs",
  "def is_weighted(self) -> bool:\n        return self._is_weighted",
  "def optimizer_type(self) -> Type[torch.optim.Optimizer]:\n        return self._optimizer_type",
  "def optimizer_kwargs(self) -> Dict[str, Any]:\n        return self._optimizer_kwargs",
  "def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim",
  "def __init__(\n        self,\n        tables: List[EmbeddingConfig],\n        optimizer_type: Type[torch.optim.Optimizer],\n        optimizer_kwargs: Dict[str, Any],\n        device: Optional[torch.device] = None,\n        need_indices: bool = False,\n        location: Optional[EmbeddingLocation] = None,\n    ) -> None:\n        super().__init__()\n\n        self._optimizer_type = optimizer_type\n        self._optimizer_kwargs = optimizer_kwargs\n\n        emb_optim_and_kwargs = convert_optimizer_type_and_kwargs(\n            optimizer_type, optimizer_kwargs\n        )\n        if emb_optim_and_kwargs is None:\n            raise ValueError(\n                f\"Cannot fuse optimizer_type={optimizer_type} with kwargs {optimizer_kwargs}\"\n            )\n        (emb_optim_type, emb_opt_kwargs) = emb_optim_and_kwargs\n\n        if location in [\n            EmbeddingLocation.DEVICE,\n            EmbeddingLocation.MANAGED,\n            EmbeddingLocation.MANAGED_CACHING,\n        ]:\n            assert device is not None and device.type in [\n                \"cuda\",\n                \"meta\",\n            ], f\"Using location={location} requires device=cuda or meta\"\n\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        assert device.type in [\n            \"cuda\",\n            \"meta\",\n        ], \"FusedEmbeddingCollection is only supported for device in [CUDA, meta] currently. There are plans to support device=CPU.\"\n\n        if location is None:\n            if device.type in [\"cpu\", \"meta\"]:\n                location = EmbeddingLocation.HOST\n            elif device.type == \"cuda\":\n                location = EmbeddingLocation.DEVICE\n            else:\n                raise ValueError(\"EmbeddingLocation could not be set\")\n\n        self._embedding_configs = tables\n        self._need_indices: bool = need_indices\n        self._embedding_dim: int = -1\n\n        # Registering in a List instead of ModuleList because we want don't want them to be auto-registered.\n        # Their states will be modified via self.embedding_bags\n        self._emb_modules: List[nn.Module] = []\n\n        self._key_to_tables: Dict[DataType, List[EmbeddingConfig]] = defaultdict(list)\n\n        seen_features = set()\n        self._shared_features: Set[str] = set()\n        for table in tables:\n            key = table.data_type\n            self._key_to_tables[key].append(table)\n\n            if self._embedding_dim == -1:\n                self._embedding_dim = table.embedding_dim\n            elif self._embedding_dim != table.embedding_dim:\n                raise ValueError(\n                    \"All tables in a EmbeddingCollection are required to have same embedding dimension.\"\n                )\n            for feature in table.feature_names:\n                if feature in seen_features:\n                    self._shared_features.add(feature)\n                else:\n                    seen_features.add(feature)\n\n        optims = []\n        for key, tables in self._key_to_tables.items():\n            data_type = key\n            emb_module = _BatchedFusedEmbeddingLookups(\n                cast(List[BaseEmbeddingConfig], tables),\n                data_type=data_type,\n                pooling=PoolingType.NONE,\n                optimizer_type=emb_optim_type,\n                optimizer_kwargs=emb_opt_kwargs,\n                device=device,\n                embedding_location=location,\n            )\n            self._emb_modules.append(emb_module)\n            params: Dict[str, torch.Tensor] = {}\n            for param_key, weight in emb_module.fused_optimizer().params.items():\n                params[f\"embeddings.{param_key}\"] = weight\n            optims.append((\"\", emb_module.fused_optimizer()))\n\n        self._optim: CombinedOptimizer = CombinedOptimizer(optims)\n        self._embedding_names: List[str] = list(\n            itertools.chain(*get_embedding_names_by_table(self._embedding_configs))\n        )\n\n        self._embedding_names_by_table: List[List[str]] = get_embedding_names_by_table(\n            self._embedding_configs,\n        )\n\n        # We map over the parameters from FBGEMM backed kernels to the canonical nn.EmbeddingBag\n        # representation. This provides consistency between this class and the EmbeddingBagCollection's\n        # nn.Module API calls (state_dict, named_modules, etc)\n        self.embeddings: nn.ModuleDict = nn.ModuleDict()\n        for (_key, tables), emb_module in zip(\n            self._key_to_tables.items(), self._emb_modules\n        ):\n            for embedding_config, weight in zip(\n                tables,\n                emb_module.split_embedding_weights(),\n                #  torch._tensor.Tensor]` is not a function.\n            ):\n                self.embeddings[embedding_config.name] = torch.nn.Module()\n                self.embeddings[embedding_config.name].register_parameter(\n                    \"weight\", torch.nn.Parameter(weight)\n                )",
  "def forward(self, features: KeyedJaggedTensor) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            Dict[str, JaggedTensor]\n        \"\"\"\n        assert features is not None\n        feature_dict = features.to_dict()\n\n        feature_embeddings: Dict[str, JaggedTensor] = {}\n\n        for emb_op, (_key, tables) in zip(\n            self._emb_modules, self._key_to_tables.items()\n        ):\n            indicies = []\n            lengths = []\n            offsets = []\n\n            feature_names = []\n            feature_lengths = []\n            feature_values = []\n            splits = []\n\n            for table in tables:\n                for feature in table.feature_names:\n                    f = feature_dict[feature]\n                    indicies.append(f.values())\n                    lengths.append(f.lengths())\n\n                    if feature in self._shared_features:\n                        feature = f\"{feature}@{table.name}\"\n\n                    feature_names.append(feature)\n                    feature_values.append(f.values())\n                    feature_lengths.append(f.lengths())\n                    splits.append(torch.sum(feature_lengths[-1]))\n\n            indicies = torch.cat(indicies)\n            lengths = torch.cat(lengths)\n            offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)\n\n            lookups = emb_op(indicies.int(), offsets.int(), weights=None)\n            lookups = torch.split(lookups, split_size_or_sections=splits)\n\n            for feature, lookup, feature_length, values in zip(\n                feature_names, lookups, feature_lengths, feature_values\n            ):\n                feature_embeddings[feature] = JaggedTensor(\n                    values=lookup,\n                    lengths=feature_length,\n                    # hack to return kJT positional indicies in return type.\n                    weights=values if self.need_indices() else None,\n                )\n\n        return feature_embeddings",
  "def _get_name(self) -> str:\n        return \"FusedEmbeddingCollection\"",
  "def embedding_configs(self) -> List[EmbeddingConfig]:\n        return self._embedding_configs",
  "def embedding_names_by_table(self) -> List[List[str]]:\n        return self._embedding_names_by_table",
  "def embedding_dim(self) -> int:\n        return self._embedding_dim",
  "def optimizer_type(self) -> Type[torch.optim.Optimizer]:\n        return self._optimizer_type",
  "def optimizer_kwargs(self) -> Dict[str, Any]:\n        return self._optimizer_kwargs",
  "def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim",
  "def need_indices(self) -> bool:\n        return self._need_indices",
  "def replace(_model: nn.Module) -> None:\n        for child_name, child in _model.named_children():\n            if isinstance(child, EmbeddingBagCollection):\n                setattr(\n                    _model,\n                    child_name,\n                    FusedEmbeddingBagCollection(\n                        tables=child.embedding_bag_configs(),\n                        optimizer_type=optimizer_type,\n                        optimizer_kwargs=optimizer_kwargs,\n                        device=device,\n                        location=location,\n                    ),\n                )\n            elif isinstance(child, EmbeddingCollection):\n                setattr(\n                    _model,\n                    child_name,\n                    FusedEmbeddingCollection(\n                        tables=child.embedding_configs(),\n                        optimizer_type=optimizer_type,\n                        optimizer_kwargs=optimizer_kwargs,\n                        device=device,\n                        location=location,\n                    ),\n                )\n            else:\n                replace(child)",
  "class SwishLayerNorm(nn.Module):\n    \"\"\"\n    Applies the Swish function with layer normalization: `Y = X * Sigmoid(LayerNorm(X))`.\n\n    Args:\n        input_dims (Union[int, List[int], torch.Size]): dimensions to normalize over.\n            If an input tensor has shape [batch_size, d1, d2, d3], setting\n            input_dim=[d2, d3] will do the layer normalization on last two dimensions.\n        device (Optional[torch.device]): default compute device.\n\n    Example::\n\n        sln = SwishLayerNorm(100)\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dims: Union[int, List[int], torch.Size],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self.norm: torch.nn.modules.Sequential = nn.Sequential(\n            nn.LayerNorm(input_dims, device=device),\n            nn.Sigmoid(),\n        )\n\n    def forward(\n        self,\n        input: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): an input tensor.\n\n        Returns:\n            torch.Tensor: an output tensor.\n        \"\"\"\n        return input * self.norm(input)",
  "def __init__(\n        self,\n        input_dims: Union[int, List[int], torch.Size],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self.norm: torch.nn.modules.Sequential = nn.Sequential(\n            nn.LayerNorm(input_dims, device=device),\n            nn.Sigmoid(),\n        )",
  "def forward(\n        self,\n        input: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): an input tensor.\n\n        Returns:\n            torch.Tensor: an output tensor.\n        \"\"\"\n        return input * self.norm(input)",
  "def evict(\n    evictions: Dict[str, Optional[torch.Tensor]], ebc: EmbeddingBagCollection\n) -> None:\n    # TODO: write function\n    return",
  "class ManagedCollisionEmbeddingBagCollection(nn.Module):\n    \"\"\"\n    ManagedCollisionEmbeddingBagCollection represents a EmbeddingBagCollection module and a set of managed collision modules.\n    The inputs into the MC-EBC will first be modified by the managed collision module before being passed into the embedding bag collection.\n\n    For details of input and output types, see EmbeddingBagCollection\n\n    Args:\n        embedding_bag_collection: EmbeddingBagCollection to lookup embeddings\n        managed_collision_modules: Dict of managed collision modules\n        return_remapped_features (bool): whether to return remapped input features\n            in addition to embeddings\n\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        managed_collision_collection: ManagedCollisionCollection,\n        return_remapped_features: bool = False,\n    ) -> None:\n        super().__init__()\n        self._embedding_bag_collection = embedding_bag_collection\n        self._managed_collision_collection = managed_collision_collection\n        self._return_remapped_features = return_remapped_features\n\n        assert (\n            self._embedding_bag_collection.embedding_bag_configs()\n            == self._managed_collision_collection.embedding_configs()\n        ), \"Embedding Collection and Managed Collision Collection must contain the Embedding Configs\"\n\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n        force_insert: bool = False,\n    ) -> Tuple[KeyedTensor, Optional[KeyedJaggedTensor]]:\n\n        features = self._managed_collision_collection(features, force_insert)\n\n        pooled_embeddings = self._embedding_bag_collection(features)\n\n        evict(\n            self._managed_collision_collection.evict(), self._embedding_bag_collection\n        )\n\n        if not self._return_remapped_features:\n            return pooled_embeddings, None\n        return pooled_embeddings, features",
  "def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        managed_collision_collection: ManagedCollisionCollection,\n        return_remapped_features: bool = False,\n    ) -> None:\n        super().__init__()\n        self._embedding_bag_collection = embedding_bag_collection\n        self._managed_collision_collection = managed_collision_collection\n        self._return_remapped_features = return_remapped_features\n\n        assert (\n            self._embedding_bag_collection.embedding_bag_configs()\n            == self._managed_collision_collection.embedding_configs()\n        ), \"Embedding Collection and Managed Collision Collection must contain the Embedding Configs\"",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n        force_insert: bool = False,\n    ) -> Tuple[KeyedTensor, Optional[KeyedJaggedTensor]]:\n\n        features = self._managed_collision_collection(features, force_insert)\n\n        pooled_embeddings = self._embedding_bag_collection(features)\n\n        evict(\n            self._managed_collision_collection.evict(), self._embedding_bag_collection\n        )\n\n        if not self._return_remapped_features:\n            return pooled_embeddings, None\n        return pooled_embeddings, features",
  "def apply_mc_method_to_jt_dict(\n    method: str,\n    features_dict: Dict[str, JaggedTensor],\n    table_to_features: Dict[str, List[str]],\n    managed_collisions: nn.ModuleDict,\n    force_insert: bool = False,\n) -> Dict[str, JaggedTensor]:\n    \"\"\"\n    Applies an MC method to a dictionary of JaggedTensors, returning the updated dictionary with same ordering\n    \"\"\"\n    mc_output: Dict[str, JaggedTensor] = features_dict.copy()\n    for table, features in table_to_features.items():\n        mc_input: Dict[str, JaggedTensor] = {}\n        for feature in features:\n            mc_input[feature] = features_dict[feature]\n        mc_module = managed_collisions[table]\n        attr = getattr(mc_module, method)\n        if force_insert:\n            mc_output.update(attr(mc_input, force_insert))\n        else:\n            mc_output.update(attr(mc_input))\n    return mc_output",
  "class ManagedCollisionModule(nn.Module):\n    \"\"\"\n    Abstract base class for ManagedCollisionModule.\n    Maps input ids to range [0, max_output_id).\n\n    Args:\n        max_output_id (int): Max output value of remapped ids.\n        input_hash_size (int): Max value of input range i.e. [0, input_hash_size)\n        remapping_range_start_index (int): Relative start index of remapping range\n        device (torch.device): default compute device.\n\n    Example::\n        jt = JaggedTensor(...)\n        mcm = ManagedCollisionModule(...)\n        mcm_jt = mcm(fp)\n    \"\"\"\n\n    def __init__(\n        self,\n        device: torch.device,\n    ) -> None:\n        # slots is the number of rows to map from global id to\n        # for example, if we want to manage 1000 ids to 10 slots\n        super().__init__()\n        self._device = device\n\n    @abc.abstractmethod\n    def preprocess(\n        self,\n        features: Dict[str, JaggedTensor],\n    ) -> Dict[str, JaggedTensor]:\n        pass\n\n    @property\n    def device(self) -> torch.device:\n        return self._device\n\n    @abc.abstractmethod\n    def evict(self) -> Optional[torch.Tensor]:\n        \"\"\"\n        Returns None if no eviction should be done this iteration. Otherwise, return ids of slots to reset.\n        On eviction, this module should reset its state for those slots, with the assumptionn that the downstream module\n        will handle this properly.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        features: Dict[str, JaggedTensor],\n    ) -> Dict[str, JaggedTensor]:\n        pass\n\n    @abc.abstractmethod\n    def output_size(self) -> int:\n        \"\"\"\n        Returns numerical range of output, for validation vs. downstream embedding lookups\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def input_size(self) -> int:\n        \"\"\"\n        Returns numerical range of input, for sharding info\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def rebuild_with_output_id_range(\n        self,\n        output_id_range: Tuple[int, int],\n        device: Optional[torch.device] = None,\n    ) -> \"ManagedCollisionModule\":\n        \"\"\"\n        Used for creating local MC modules for RW sharding, hack for now\n        \"\"\"\n        pass",
  "class ManagedCollisionCollection(nn.Module):\n    \"\"\"\n    ManagedCollisionCollection represents a collection of managed collision modules.\n    The inputs passed to the MCC will be remapped by the managed collision modules\n        and returned.\n    Args:\n        managed_collision_modules (Dict[str, ManagedCollisionModule]): Dict of managed collision modules\n        embedding_confgs (List[BaseEmbeddingConfig]): List of embedding configs, for each table with a managed collsion module\n    \"\"\"\n\n    def __init__(\n        self,\n        managed_collision_modules: Dict[str, ManagedCollisionModule],\n        embedding_configs: List[BaseEmbeddingConfig],\n    ) -> None:\n        super().__init__()\n        self._managed_collision_modules = nn.ModuleDict(managed_collision_modules)\n        self._embedding_configs = embedding_configs\n        self._feature_to_table: Dict[str, str] = {\n            feature: config.name\n            for config in embedding_configs\n            for feature in config.feature_names\n        }\n        self._table_to_features: Dict[str, List[str]] = defaultdict(list)\n        for feature, table in self._feature_to_table.items():\n            self._table_to_features[table].append(feature)\n\n        table_to_config = {config.name: config for config in embedding_configs}\n\n        for name, config in table_to_config.items():\n            if name not in managed_collision_modules:\n                raise ValueError(\n                    f\"Table {name} is not present in managed_collision_modules\"\n                )\n            assert (\n                managed_collision_modules[name].output_size() == config.num_embeddings\n            ), (\n                f\"max_output_id in managed collision module for {name} \"\n                f\"must match {config.num_embeddings}\"\n            )\n\n    def embedding_configs(self) -> List[BaseEmbeddingConfig]:\n        return self._embedding_configs\n\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n        force_insert: bool = False,\n    ) -> KeyedJaggedTensor:\n        features_dict = apply_mc_method_to_jt_dict(\n            \"preprocess\",\n            features_dict=features.to_dict(),\n            table_to_features=self._table_to_features,\n            managed_collisions=self._managed_collision_modules,\n        )\n        features_dict = apply_mc_method_to_jt_dict(\n            \"profile\",\n            features_dict=features_dict,\n            table_to_features=self._table_to_features,\n            managed_collisions=self._managed_collision_modules,\n            force_insert=force_insert,\n        )\n        features_dict = apply_mc_method_to_jt_dict(\n            \"remap\",\n            features_dict=features_dict,\n            table_to_features=self._table_to_features,\n            managed_collisions=self._managed_collision_modules,\n        )\n        return KeyedJaggedTensor.from_jt_dict(features_dict)\n\n    def evict(self) -> Dict[str, Optional[torch.Tensor]]:\n        evictions: Dict[str, Optional[torch.Tensor]] = {}\n        for (\n            table,\n            managed_collision_module,\n        ) in self._managed_collision_modules.items():\n            evictions[table] = managed_collision_module.evict()\n        return evictions",
  "class MCHEvictionPolicyMetadataInfo(NamedTuple):\n    metadata_name: str\n    is_mch_metadata: bool\n    is_history_metadata: bool",
  "class MCHEvictionPolicy(abc.ABC):\n    @property\n    @abc.abstractmethod\n    def metadata_info(self) -> List[MCHEvictionPolicyMetadataInfo]:\n        pass\n\n    @abc.abstractmethod\n    def record_history_metadata(\n        self,\n        current_iter: int,\n        incoming_ids: torch.Tensor,\n        history_metadata: Dict[str, torch.Tensor],\n    ) -> None:\n        \"\"\"\n        Args:\n        current_iter (int): current iteration\n        incoming_ids (torch.Tensor): incoming ids\n        history_metadata (Dict[str, torch.Tensor]): history metadata dict\n\n        Compute and record metadata based on incoming ids\n            for the implemented eviction policy.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def coalesce_history_metadata(\n        self,\n        current_iter: int,\n        history_metadata: Dict[str, torch.Tensor],\n        unique_ids_counts: torch.Tensor,\n        unique_inverse_mapping: torch.Tensor,\n        additional_ids: Optional[torch.Tensor] = None,\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Args:\n        history_metadata (Dict[str, torch.Tensor]): history metadata dict\n        additional_ids (torch.Tensor): additional ids to be used as part of history\n        unique_inverse_mapping (torch.Tensor): torch.unique inverse mapping generated from\n            torch.cat[history_accumulator, additional_ids]. used to map history metadata tensor\n            indices to their coalesced tensor indices.\n\n        Coalesce metadata history buffers and return dict of processed metadata tensors.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def update_metadata_and_generate_eviction_scores(\n        self,\n        current_iter: int,\n        mch_size: int,\n        coalesced_history_argsort_mapping: torch.Tensor,\n        coalesced_history_sorted_unique_ids_counts: torch.Tensor,\n        coalesced_history_mch_matching_elements_mask: torch.Tensor,\n        coalesced_history_mch_matching_indices: torch.Tensor,\n        mch_metadata: Dict[str, torch.Tensor],\n        coalesced_history_metadata: Dict[str, torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n\n\n        Returns Tuple of (evicted_indices, selected_new_indices) where:\n            evicted_indices are indices in the mch map to be evicted, and\n            selected_new_indices are the indices of the ids in the coalesced\n            history that are to be added to the mch.\n        \"\"\"\n        pass\n\n    def _compute_selected_eviction_and_replacement_indices(\n        self,\n        pivot: int,\n        eviction_scores: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # NOTE these are like indices\n        argsorted_eviction_scores = torch.argsort(\n            eviction_scores, descending=True, stable=True\n        )\n\n        # indices with values >= zch_size in the top zch_size scores correspond\n        #   to new incoming ids to be added to zch\n        selected_new_ids_mask = argsorted_eviction_scores[:pivot] >= pivot\n        # indices with values < zch_size outside the top zch_size scores correspond\n        #   to existing zch ids to be evicted\n        evicted_ids_mask = argsorted_eviction_scores[pivot:] < pivot\n        evicted_indices = argsorted_eviction_scores[pivot:][evicted_ids_mask]\n        selected_new_indices = (\n            argsorted_eviction_scores[:pivot][selected_new_ids_mask] - pivot\n        )\n\n        return evicted_indices, selected_new_indices",
  "class LFU_EvictionPolicy(MCHEvictionPolicy):\n    def __init__(self) -> None:\n        self._metadata_info = [\n            MCHEvictionPolicyMetadataInfo(\n                metadata_name=\"counts\",\n                is_mch_metadata=True,\n                is_history_metadata=False,\n            ),\n        ]\n\n    @property\n    def metadata_info(self) -> List[MCHEvictionPolicyMetadataInfo]:\n        return self._metadata_info\n\n    def record_history_metadata(\n        self,\n        current_iter: int,\n        incoming_ids: torch.Tensor,\n        history_metadata: Dict[str, torch.Tensor],\n    ) -> None:\n        # no-op; no history buffers\n        pass\n\n    def coalesce_history_metadata(\n        self,\n        current_iter: int,\n        history_metadata: Dict[str, torch.Tensor],\n        unique_ids_counts: torch.Tensor,\n        unique_inverse_mapping: torch.Tensor,\n        additional_ids: Optional[torch.Tensor] = None,\n    ) -> Dict[str, torch.Tensor]:\n        # no-op; no history buffers\n        return {}\n\n    def update_metadata_and_generate_eviction_scores(\n        self,\n        current_iter: int,\n        mch_size: int,\n        coalesced_history_argsort_mapping: torch.Tensor,\n        coalesced_history_sorted_unique_ids_counts: torch.Tensor,\n        coalesced_history_mch_matching_elements_mask: torch.Tensor,\n        coalesced_history_mch_matching_indices: torch.Tensor,\n        mch_metadata: Dict[str, torch.Tensor],\n        coalesced_history_metadata: Dict[str, torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        mch_counts = mch_metadata[\"counts\"]\n        # update metadata for matching ids\n        mch_counts[\n            coalesced_history_mch_matching_indices\n        ] += coalesced_history_sorted_unique_ids_counts[\n            coalesced_history_mch_matching_elements_mask\n        ]\n\n        # incoming non-matching ids\n        new_sorted_uniq_ids_counts = coalesced_history_sorted_unique_ids_counts[\n            ~coalesced_history_mch_matching_elements_mask\n        ]\n\n        # TODO: find cleaner way to avoid last element of zch\n\n        mch_counts[mch_size - 1] = torch.iinfo(torch.int64).max\n\n        merged_counts = torch.cat(\n            [\n                mch_counts,\n                new_sorted_uniq_ids_counts,\n            ]\n        )\n        # calculate evicted and replacement indices\n        (\n            evicted_indices,\n            selected_new_indices,\n        ) = self._compute_selected_eviction_and_replacement_indices(\n            mch_size,\n            merged_counts,\n        )\n\n        # update metadata for evicted ids\n        mch_counts[evicted_indices] = new_sorted_uniq_ids_counts[selected_new_indices]\n\n        return evicted_indices, selected_new_indices",
  "class DistanceLFU_EvictionPolicy(MCHEvictionPolicy):\n    def __init__(self, decay_exponent: int = 2) -> None:\n        self._metadata_info = [\n            MCHEvictionPolicyMetadataInfo(\n                metadata_name=\"counts\",\n                is_mch_metadata=True,\n                is_history_metadata=False,\n            ),\n            MCHEvictionPolicyMetadataInfo(\n                metadata_name=\"last_access_iter\",\n                is_mch_metadata=True,\n                is_history_metadata=True,\n            ),\n        ]\n        self._decay_exponent = decay_exponent\n\n    @property\n    def metadata_info(self) -> List[MCHEvictionPolicyMetadataInfo]:\n        return self._metadata_info\n\n    def record_history_metadata(\n        self,\n        current_iter: int,\n        incoming_ids: torch.Tensor,\n        history_metadata: Dict[str, torch.Tensor],\n    ) -> None:\n        history_last_access_iter = history_metadata[\"last_access_iter\"]\n        history_last_access_iter[:] = current_iter\n\n    def coalesce_history_metadata(\n        self,\n        current_iter: int,\n        history_metadata: Dict[str, torch.Tensor],\n        unique_ids_counts: torch.Tensor,\n        unique_inverse_mapping: torch.Tensor,\n        additional_ids: Optional[torch.Tensor] = None,\n    ) -> Dict[str, torch.Tensor]:\n        coalesced_history_metadata: Dict[str, torch.Tensor] = {}\n        history_last_access_iter = history_metadata[\"last_access_iter\"]\n        if additional_ids is not None:\n            history_last_access_iter = torch.cat(\n                [\n                    history_last_access_iter,\n                    torch.full_like(additional_ids, current_iter),\n                ]\n            )\n        coalesced_history_metadata[\"last_access_iter\"] = torch.zeros_like(\n            unique_ids_counts\n        ).scatter_reduce_(\n            0,\n            unique_inverse_mapping,\n            history_last_access_iter,\n            reduce=\"amax\",\n            include_self=False,\n        )\n        return coalesced_history_metadata\n\n    def update_metadata_and_generate_eviction_scores(\n        self,\n        current_iter: int,\n        mch_size: int,\n        coalesced_history_argsort_mapping: torch.Tensor,\n        coalesced_history_sorted_unique_ids_counts: torch.Tensor,\n        coalesced_history_mch_matching_elements_mask: torch.Tensor,\n        coalesced_history_mch_matching_indices: torch.Tensor,\n        mch_metadata: Dict[str, torch.Tensor],\n        coalesced_history_metadata: Dict[str, torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        mch_counts = mch_metadata[\"counts\"]\n        mch_last_access_iter = mch_metadata[\"last_access_iter\"]\n\n        # sort coalesced history metadata\n        coalesced_history_metadata[\"last_access_iter\"].copy_(\n            coalesced_history_metadata[\"last_access_iter\"][\n                coalesced_history_argsort_mapping\n            ]\n        )\n        coalesced_history_sorted_uniq_ids_last_access_iter = coalesced_history_metadata[\n            \"last_access_iter\"\n        ]\n\n        # update metadata for matching ids\n        mch_counts[\n            coalesced_history_mch_matching_indices\n        ] += coalesced_history_sorted_unique_ids_counts[\n            coalesced_history_mch_matching_elements_mask\n        ]\n        mch_last_access_iter[\n            coalesced_history_mch_matching_indices\n        ] = coalesced_history_sorted_uniq_ids_last_access_iter[\n            coalesced_history_mch_matching_elements_mask\n        ]\n\n        # incoming non-matching ids\n        new_sorted_uniq_ids_counts = coalesced_history_sorted_unique_ids_counts[\n            ~coalesced_history_mch_matching_elements_mask\n        ]\n        new_sorted_uniq_ids_last_access = (\n            coalesced_history_sorted_uniq_ids_last_access_iter[\n                ~coalesced_history_mch_matching_elements_mask\n            ]\n        )\n\n        # TODO: find cleaner way to avoid last element of zch\n        mch_counts[mch_size - 1] = torch.iinfo(torch.int64).max\n        mch_last_access_iter[mch_size - 1] = current_iter\n\n        merged_counts = torch.cat(\n            [\n                mch_counts,\n                new_sorted_uniq_ids_counts,\n            ]\n        )\n        merged_access_iter = torch.cat(\n            [\n                mch_last_access_iter,\n                new_sorted_uniq_ids_last_access,\n            ]\n        )\n        merged_weighted_distance = torch.pow(\n            current_iter - merged_access_iter + 1,\n            self._decay_exponent,\n        )\n        # merged eviction scores are the eviction scores calculated for the\n        #   tensor torch.cat[_mch_sorted_raw_ids, frequency_sorted_uniq_ids[~matching_eles]]\n        # lower scores are evicted first.\n        merged_eviction_scores = torch.div(merged_counts, merged_weighted_distance)\n\n        # calculate evicted and replacement indices\n        (\n            evicted_indices,\n            selected_new_indices,\n        ) = self._compute_selected_eviction_and_replacement_indices(\n            mch_size,\n            merged_eviction_scores,\n        )\n\n        # update metadata for evicted ids\n        mch_counts[evicted_indices] = new_sorted_uniq_ids_counts[selected_new_indices]\n\n        mch_last_access_iter[evicted_indices] = new_sorted_uniq_ids_last_access[\n            selected_new_indices\n        ]\n\n        return evicted_indices, selected_new_indices",
  "class MCHManagedCollisionModule(ManagedCollisionModule):\n    \"\"\"\n    ZCH / MCH managed collision module\n\n    Args:\n        zch_size (int): range of output ids, within [output_size_offset, output_size_offset + zch_size - 1)\n        device (torch.device): device on which this module will be executed\n        eviction_policy (eviction policy): eviction policy to be used\n        eviction_interval (int): interval of eviction policy is triggered\n        input_hash_size (int): input feature id range, will be passed to input_hash_func as second arg\n        input_hash_func (Optional[Callable]): function used to generate hashes for input features.  This function is typically used to drive uniform distribution over range same or greater than input data\n        input_history_buffer_size (Optional[int]): size of history buffer where we store input ids for eviction policy calculations\n        mch_size (Optional[int]): size of residual output (ie. legacy MCH), experimental feature.  Ids are internally shifted by output_size_offset + zch_output_range\n        mch_hash_func (Optional[Callable]): function used to generate hashes for residual feature. will hash down to mch_size.\n        output_global_offset (int): offset of the output id for output range, typically only used in sharding applications.\n    \"\"\"\n\n    def __init__(\n        self,\n        zch_size: int,\n        device: torch.device,\n        eviction_policy: MCHEvictionPolicy,\n        eviction_interval: int,\n        input_hash_size: int = 2**63,\n        input_hash_func: Optional[Callable[[torch.Tensor, int], torch.Tensor]] = None,\n        input_history_buffer_size: Optional[int] = None,\n        mch_size: Optional[int] = None,  # experimental\n        mch_hash_func: Optional[Callable[[torch.Tensor, int], torch.Tensor]] = None,\n        output_global_offset: int = 0,  # typically not provided by user\n    ) -> None:\n        super().__init__(device)\n\n        if input_history_buffer_size is None:\n            input_history_buffer_size = zch_size * 10\n        self._input_history_buffer_size: int = input_history_buffer_size\n        self._input_hash_size = input_hash_size\n        self._zch_size: int = zch_size\n        assert self._zch_size > 0, \"zch_size must be > 0\"\n        self._mch_size: int = 0\n        if mch_size is not None:\n            self._mch_size = mch_size\n            assert (\n                mch_hash_func is not None\n            ), \"mch_hash_func must be provided if mch_size is provided\"\n        self._output_global_offset: int = output_global_offset\n        self._mch_hash_func = mch_hash_func\n        self._input_hash_func = input_hash_func\n\n        self._eviction_interval = eviction_interval\n        self._eviction_policy = eviction_policy\n\n        self._current_iter: int = 0\n        self._init_buffers()\n\n        ## ------ history info ------\n        self._mch_metadata: Dict[str, torch.Tensor] = {}\n        self._history_metadata: Dict[str, torch.Tensor] = {}\n        self._init_metadata_buffers()\n        self._current_history_buffer_offset: int = 0\n\n        self._evicted: bool = False\n\n    def _init_buffers(self) -> None:\n        self.register_buffer(\n            \"_mch_sorted_raw_ids\",\n            torch.full(\n                (self._zch_size,),\n                torch.iinfo(torch.int64).max,\n                dtype=torch.int64,\n                device=self.device,\n            ),\n        )\n        self.register_buffer(\n            \"_mch_remapped_ids_mapping\",\n            torch.arange(self._zch_size, dtype=torch.int64, device=self.device),\n        )\n\n        # TODO: explicitly create / destory this buffer when going between training / eval\n        self._history_accumulator: torch.Tensor = torch.empty(\n            self._input_history_buffer_size if self.training else 0,\n            dtype=torch.int64,\n            device=self.device,\n        )\n\n        self._evicted_emb_indices: torch.Tensor = torch.empty((1,), device=self.device)\n\n    def _init_metadata_buffers(self) -> None:\n        eviction_metadata_info = self._eviction_policy.metadata_info\n        for metadata in eviction_metadata_info:\n            metadata_name, is_mch_metadata, is_history_metadata = metadata\n            # mch_metadata\n            if is_mch_metadata:\n                buffer_name = \"_mch_\" + metadata_name\n                self.register_buffer(\n                    buffer_name,\n                    torch.zeros(\n                        (self._zch_size,),\n                        dtype=torch.int64,\n                        device=self.device,\n                    ),\n                )\n                self._mch_metadata[metadata_name] = getattr(self, buffer_name)\n            # history_metadata\n            if is_history_metadata:\n                buffer_name = \"_history_\" + metadata_name\n                self.register_buffer(\n                    buffer_name,\n                    torch.zeros(\n                        self._input_history_buffer_size,\n                        dtype=torch.int64,\n                        device=self.device,\n                    ),\n                    # not checkpointed\n                    persistent=False,\n                )\n                self._history_metadata[metadata_name] = getattr(self, buffer_name)\n\n    @torch.no_grad()\n    def preprocess(self, features: Dict[str, JaggedTensor]) -> Dict[str, JaggedTensor]:\n        if self._input_hash_func is None:\n            return features\n        preprocessed_features: Dict[str, JaggedTensor] = {}\n        for name, feature in features.items():\n            preprocessed_features[name] = JaggedTensor(\n                # pyre-ignore [29]\n                values=self._input_hash_func(feature.values(), self._input_hash_size),\n                lengths=feature.lengths(),\n                offsets=feature.offsets(),\n                weights=feature.weights_or_none(),\n            )\n        return preprocessed_features\n\n    @torch.no_grad()\n    def _match_indices(\n        self, sorted_sequence: torch.Tensor, search_values: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        searched_indices = torch.searchsorted(sorted_sequence[:-1], search_values)\n        retrieved_ids = sorted_sequence[searched_indices]\n        matching_eles = retrieved_ids == search_values\n        matched_indices = searched_indices[matching_eles]\n        return (matching_eles, matched_indices)\n\n    @torch.no_grad()\n    def _sort_mch_buffers(self) -> None:\n        mch_sorted_raw_ids = self._mch_sorted_raw_ids\n        argsorted_sorted_raw_ids = torch.argsort(mch_sorted_raw_ids, stable=True)\n        mch_sorted_raw_ids.copy_(mch_sorted_raw_ids[argsorted_sorted_raw_ids])\n        self._mch_remapped_ids_mapping.copy_(\n            self._mch_remapped_ids_mapping[argsorted_sorted_raw_ids]\n        )\n        for mch_metadata_buffer in self._mch_metadata.values():\n            mch_metadata_buffer.copy_(mch_metadata_buffer[argsorted_sorted_raw_ids])\n\n    @torch.no_grad()\n    def _update_and_evict(\n        self,\n        uniq_ids: torch.Tensor,\n        uniq_ids_counts: torch.Tensor,\n        uniq_ids_metadata: Dict[str, torch.Tensor],\n    ) -> None:\n        argsorted_uniq_ids_counts = torch.argsort(\n            uniq_ids_counts, descending=True, stable=True\n        )\n        frequency_sorted_uniq_ids = uniq_ids[argsorted_uniq_ids_counts]\n        frequency_sorted_uniq_ids_counts = uniq_ids_counts[argsorted_uniq_ids_counts]\n\n        matching_eles, matched_indices = self._match_indices(\n            self._mch_sorted_raw_ids, frequency_sorted_uniq_ids\n        )\n\n        new_frequency_sorted_uniq_ids = frequency_sorted_uniq_ids[~matching_eles]\n\n        # evicted_indices are indices in the mch map to be evicted, and\n        #   selected_new_indices are the indices of the ids in the coalesced\n        #   history that are to be added to the mch.\n        (\n            evicted_indices,\n            selected_new_indices,\n        ) = self._eviction_policy.update_metadata_and_generate_eviction_scores(\n            self._current_iter,\n            self._zch_size,\n            argsorted_uniq_ids_counts,\n            frequency_sorted_uniq_ids_counts,\n            matching_eles,\n            matched_indices,\n            self._mch_metadata,\n            uniq_ids_metadata,\n        )\n        self._mch_sorted_raw_ids[evicted_indices] = new_frequency_sorted_uniq_ids[\n            selected_new_indices\n        ]\n\n        # NOTE evicted ids for emb reset\n        # if evicted flag is already set, then existing evicted ids havent been\n        # consumed by evict(). append new evicted ids to the list\n        if self._evicted:\n            self._evicted_emb_indices = torch.unique(\n                torch.cat(\n                    [\n                        self._evicted_emb_indices,\n                        self._mch_remapped_ids_mapping[evicted_indices],\n                    ]\n                )\n            )\n        else:\n            self._evicted_emb_indices = self._mch_remapped_ids_mapping[evicted_indices]\n        self._evicted = True\n\n        # re-sort for next search\n        self._sort_mch_buffers()\n\n    @torch.no_grad()\n    def _coalesce_history(self) -> None:\n        current_history_accumulator = self._history_accumulator[\n            : self._current_history_buffer_offset\n        ]\n        uniq_ids, uniq_inverse_mapping, uniq_ids_counts = torch.unique(\n            current_history_accumulator,\n            return_inverse=True,\n            return_counts=True,\n        )\n        coalesced_eviction_history_metadata = (\n            self._eviction_policy.coalesce_history_metadata(\n                self._current_iter,\n                {\n                    metadata_name: metadata_buffer[\n                        : self._current_history_buffer_offset\n                    ]\n                    for metadata_name, metadata_buffer in self._history_metadata.items()\n                },\n                uniq_ids_counts,\n                uniq_inverse_mapping,\n            )\n        )\n        self._update_and_evict(\n            uniq_ids, uniq_ids_counts, coalesced_eviction_history_metadata\n        )\n        # reset buffer offset\n        self._current_history_buffer_offset = 0\n\n    @torch.no_grad()\n    def profile(\n        self,\n        features: Dict[str, JaggedTensor],\n        force_insert: bool = False,\n    ) -> Dict[str, JaggedTensor]:\n        self._current_iter += 1\n        if not self.training:\n            return features\n\n        for _, feature in features.items():\n            values = feature.values()\n            # TODO: Find a better way to force_insert, doesnt really work...\n            if force_insert:\n                values = values.repeat(5)\n\n            free_elements = (\n                self._input_history_buffer_size - self._current_history_buffer_offset\n            )\n            values = values[:free_elements]\n            self._history_accumulator[\n                self._current_history_buffer_offset : self._current_history_buffer_offset\n                + values.shape[0]\n            ] = values\n            self._eviction_policy.record_history_metadata(\n                self._current_iter,\n                values,\n                {\n                    metadata_name: metadata_buffer[\n                        self._current_history_buffer_offset : self._current_history_buffer_offset\n                        + values.shape[0]\n                    ]\n                    for metadata_name, metadata_buffer in self._history_metadata.items()\n                },\n            )\n            self._current_history_buffer_offset += values.shape[0]\n\n        # coalesce history / evict\n        if self._current_iter % self._eviction_interval == 0 or force_insert:\n            self._coalesce_history()\n\n        return features\n\n    @torch.no_grad()\n    def remap(self, features: Dict[str, JaggedTensor]) -> Dict[str, JaggedTensor]:\n\n        remapped_features: Dict[str, JaggedTensor] = {}\n        for name, feature in features.items():\n            values = feature.values()\n            remapped_ids = torch.empty_like(values)\n\n            # compute overlap between incoming IDs and remapping table\n            searched_indices = torch.searchsorted(self._mch_sorted_raw_ids[:-1], values)\n            retrieved_indices = self._mch_sorted_raw_ids[searched_indices]\n            # identify matching inputs IDs\n            matching_indices = retrieved_indices == values\n            # update output with remapped matching IDs\n            remapped_ids[matching_indices] = self._mch_remapped_ids_mapping[\n                searched_indices[matching_indices]\n            ]\n            # select non-matching values\n\n            if self._mch_size:\n                non_matching_values = values[~matching_indices]\n                # pyre-ignore [29]\n                hashed_non_matching = self._mch_hash_func(\n                    non_matching_values, self._mch_size\n                ).add(self._zch_size)\n                # offset hash ids to their starting range\n                remapped_ids[~matching_indices] = hashed_non_matching\n            else:\n                remapped_ids[~matching_indices] = self._zch_size - 1\n\n            remapped_features[name] = JaggedTensor(\n                values=remapped_ids,\n                lengths=feature.lengths(),\n                offsets=feature.offsets(),\n                weights=feature.weights_or_none(),\n            )\n        return remapped_features\n\n    @torch.no_grad()\n    def forward(\n        self,\n        features: Dict[str, JaggedTensor],\n        force_insert: bool = False,\n    ) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Args:\n        feature (JaggedTensor]): feature representation\n        force_insert (bool): force insert this step\n        Returns:\n            Dict[str, JaggedTensor]: modified JT\n        \"\"\"\n\n        features = self.profile(\n            features,\n            force_insert=force_insert,\n        )\n        return self.remap(features)\n\n    def output_size(self) -> int:\n        return self._zch_size + self._mch_size\n\n    def input_size(self) -> int:\n        return self._input_hash_size\n\n    @torch.no_grad()\n    def evict(self) -> Optional[torch.Tensor]:\n        if self._evicted:\n            self._evicted = False\n            return self._evicted_emb_indices\n        else:\n            return None\n\n    def rebuild_with_output_id_range(\n        self,\n        output_id_range: Tuple[int, int],\n        device: Optional[torch.device] = None,\n    ) -> \"MCHManagedCollisionModule\":\n\n        new_output_size = output_id_range[1] - output_id_range[0]\n\n        new_zch_size = int(self._zch_size * (new_output_size / self.output_size()))\n        new_mch_size = new_output_size - new_zch_size\n        new_input_history_buffer_size = int(\n            self._input_history_buffer_size * (new_output_size / self.output_size())\n        )\n\n        return type(self)(\n            zch_size=new_zch_size,\n            device=device or self.device,\n            input_history_buffer_size=new_input_history_buffer_size,\n            eviction_policy=self._eviction_policy,\n            eviction_interval=self._eviction_interval,\n            input_hash_size=self._input_hash_size,\n            input_hash_func=self._input_hash_func,\n            mch_size=new_mch_size if new_mch_size > 0 else None,\n            mch_hash_func=self._mch_hash_func,\n            output_global_offset=output_id_range[0],\n        )",
  "def __init__(\n        self,\n        device: torch.device,\n    ) -> None:\n        # slots is the number of rows to map from global id to\n        # for example, if we want to manage 1000 ids to 10 slots\n        super().__init__()\n        self._device = device",
  "def preprocess(\n        self,\n        features: Dict[str, JaggedTensor],\n    ) -> Dict[str, JaggedTensor]:\n        pass",
  "def device(self) -> torch.device:\n        return self._device",
  "def evict(self) -> Optional[torch.Tensor]:\n        \"\"\"\n        Returns None if no eviction should be done this iteration. Otherwise, return ids of slots to reset.\n        On eviction, this module should reset its state for those slots, with the assumptionn that the downstream module\n        will handle this properly.\n        \"\"\"\n        pass",
  "def forward(\n        self,\n        features: Dict[str, JaggedTensor],\n    ) -> Dict[str, JaggedTensor]:\n        pass",
  "def output_size(self) -> int:\n        \"\"\"\n        Returns numerical range of output, for validation vs. downstream embedding lookups\n        \"\"\"\n        pass",
  "def input_size(self) -> int:\n        \"\"\"\n        Returns numerical range of input, for sharding info\n        \"\"\"\n        pass",
  "def rebuild_with_output_id_range(\n        self,\n        output_id_range: Tuple[int, int],\n        device: Optional[torch.device] = None,\n    ) -> \"ManagedCollisionModule\":\n        \"\"\"\n        Used for creating local MC modules for RW sharding, hack for now\n        \"\"\"\n        pass",
  "def __init__(\n        self,\n        managed_collision_modules: Dict[str, ManagedCollisionModule],\n        embedding_configs: List[BaseEmbeddingConfig],\n    ) -> None:\n        super().__init__()\n        self._managed_collision_modules = nn.ModuleDict(managed_collision_modules)\n        self._embedding_configs = embedding_configs\n        self._feature_to_table: Dict[str, str] = {\n            feature: config.name\n            for config in embedding_configs\n            for feature in config.feature_names\n        }\n        self._table_to_features: Dict[str, List[str]] = defaultdict(list)\n        for feature, table in self._feature_to_table.items():\n            self._table_to_features[table].append(feature)\n\n        table_to_config = {config.name: config for config in embedding_configs}\n\n        for name, config in table_to_config.items():\n            if name not in managed_collision_modules:\n                raise ValueError(\n                    f\"Table {name} is not present in managed_collision_modules\"\n                )\n            assert (\n                managed_collision_modules[name].output_size() == config.num_embeddings\n            ), (\n                f\"max_output_id in managed collision module for {name} \"\n                f\"must match {config.num_embeddings}\"\n            )",
  "def embedding_configs(self) -> List[BaseEmbeddingConfig]:\n        return self._embedding_configs",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n        force_insert: bool = False,\n    ) -> KeyedJaggedTensor:\n        features_dict = apply_mc_method_to_jt_dict(\n            \"preprocess\",\n            features_dict=features.to_dict(),\n            table_to_features=self._table_to_features,\n            managed_collisions=self._managed_collision_modules,\n        )\n        features_dict = apply_mc_method_to_jt_dict(\n            \"profile\",\n            features_dict=features_dict,\n            table_to_features=self._table_to_features,\n            managed_collisions=self._managed_collision_modules,\n            force_insert=force_insert,\n        )\n        features_dict = apply_mc_method_to_jt_dict(\n            \"remap\",\n            features_dict=features_dict,\n            table_to_features=self._table_to_features,\n            managed_collisions=self._managed_collision_modules,\n        )\n        return KeyedJaggedTensor.from_jt_dict(features_dict)",
  "def evict(self) -> Dict[str, Optional[torch.Tensor]]:\n        evictions: Dict[str, Optional[torch.Tensor]] = {}\n        for (\n            table,\n            managed_collision_module,\n        ) in self._managed_collision_modules.items():\n            evictions[table] = managed_collision_module.evict()\n        return evictions",
  "def metadata_info(self) -> List[MCHEvictionPolicyMetadataInfo]:\n        pass",
  "def record_history_metadata(\n        self,\n        current_iter: int,\n        incoming_ids: torch.Tensor,\n        history_metadata: Dict[str, torch.Tensor],\n    ) -> None:\n        \"\"\"\n        Args:\n        current_iter (int): current iteration\n        incoming_ids (torch.Tensor): incoming ids\n        history_metadata (Dict[str, torch.Tensor]): history metadata dict\n\n        Compute and record metadata based on incoming ids\n            for the implemented eviction policy.\n        \"\"\"\n        pass",
  "def coalesce_history_metadata(\n        self,\n        current_iter: int,\n        history_metadata: Dict[str, torch.Tensor],\n        unique_ids_counts: torch.Tensor,\n        unique_inverse_mapping: torch.Tensor,\n        additional_ids: Optional[torch.Tensor] = None,\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Args:\n        history_metadata (Dict[str, torch.Tensor]): history metadata dict\n        additional_ids (torch.Tensor): additional ids to be used as part of history\n        unique_inverse_mapping (torch.Tensor): torch.unique inverse mapping generated from\n            torch.cat[history_accumulator, additional_ids]. used to map history metadata tensor\n            indices to their coalesced tensor indices.\n\n        Coalesce metadata history buffers and return dict of processed metadata tensors.\n        \"\"\"\n        pass",
  "def update_metadata_and_generate_eviction_scores(\n        self,\n        current_iter: int,\n        mch_size: int,\n        coalesced_history_argsort_mapping: torch.Tensor,\n        coalesced_history_sorted_unique_ids_counts: torch.Tensor,\n        coalesced_history_mch_matching_elements_mask: torch.Tensor,\n        coalesced_history_mch_matching_indices: torch.Tensor,\n        mch_metadata: Dict[str, torch.Tensor],\n        coalesced_history_metadata: Dict[str, torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n\n\n        Returns Tuple of (evicted_indices, selected_new_indices) where:\n            evicted_indices are indices in the mch map to be evicted, and\n            selected_new_indices are the indices of the ids in the coalesced\n            history that are to be added to the mch.\n        \"\"\"\n        pass",
  "def _compute_selected_eviction_and_replacement_indices(\n        self,\n        pivot: int,\n        eviction_scores: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # NOTE these are like indices\n        argsorted_eviction_scores = torch.argsort(\n            eviction_scores, descending=True, stable=True\n        )\n\n        # indices with values >= zch_size in the top zch_size scores correspond\n        #   to new incoming ids to be added to zch\n        selected_new_ids_mask = argsorted_eviction_scores[:pivot] >= pivot\n        # indices with values < zch_size outside the top zch_size scores correspond\n        #   to existing zch ids to be evicted\n        evicted_ids_mask = argsorted_eviction_scores[pivot:] < pivot\n        evicted_indices = argsorted_eviction_scores[pivot:][evicted_ids_mask]\n        selected_new_indices = (\n            argsorted_eviction_scores[:pivot][selected_new_ids_mask] - pivot\n        )\n\n        return evicted_indices, selected_new_indices",
  "def __init__(self) -> None:\n        self._metadata_info = [\n            MCHEvictionPolicyMetadataInfo(\n                metadata_name=\"counts\",\n                is_mch_metadata=True,\n                is_history_metadata=False,\n            ),\n        ]",
  "def metadata_info(self) -> List[MCHEvictionPolicyMetadataInfo]:\n        return self._metadata_info",
  "def record_history_metadata(\n        self,\n        current_iter: int,\n        incoming_ids: torch.Tensor,\n        history_metadata: Dict[str, torch.Tensor],\n    ) -> None:\n        # no-op; no history buffers\n        pass",
  "def coalesce_history_metadata(\n        self,\n        current_iter: int,\n        history_metadata: Dict[str, torch.Tensor],\n        unique_ids_counts: torch.Tensor,\n        unique_inverse_mapping: torch.Tensor,\n        additional_ids: Optional[torch.Tensor] = None,\n    ) -> Dict[str, torch.Tensor]:\n        # no-op; no history buffers\n        return {}",
  "def update_metadata_and_generate_eviction_scores(\n        self,\n        current_iter: int,\n        mch_size: int,\n        coalesced_history_argsort_mapping: torch.Tensor,\n        coalesced_history_sorted_unique_ids_counts: torch.Tensor,\n        coalesced_history_mch_matching_elements_mask: torch.Tensor,\n        coalesced_history_mch_matching_indices: torch.Tensor,\n        mch_metadata: Dict[str, torch.Tensor],\n        coalesced_history_metadata: Dict[str, torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        mch_counts = mch_metadata[\"counts\"]\n        # update metadata for matching ids\n        mch_counts[\n            coalesced_history_mch_matching_indices\n        ] += coalesced_history_sorted_unique_ids_counts[\n            coalesced_history_mch_matching_elements_mask\n        ]\n\n        # incoming non-matching ids\n        new_sorted_uniq_ids_counts = coalesced_history_sorted_unique_ids_counts[\n            ~coalesced_history_mch_matching_elements_mask\n        ]\n\n        # TODO: find cleaner way to avoid last element of zch\n\n        mch_counts[mch_size - 1] = torch.iinfo(torch.int64).max\n\n        merged_counts = torch.cat(\n            [\n                mch_counts,\n                new_sorted_uniq_ids_counts,\n            ]\n        )\n        # calculate evicted and replacement indices\n        (\n            evicted_indices,\n            selected_new_indices,\n        ) = self._compute_selected_eviction_and_replacement_indices(\n            mch_size,\n            merged_counts,\n        )\n\n        # update metadata for evicted ids\n        mch_counts[evicted_indices] = new_sorted_uniq_ids_counts[selected_new_indices]\n\n        return evicted_indices, selected_new_indices",
  "def __init__(self, decay_exponent: int = 2) -> None:\n        self._metadata_info = [\n            MCHEvictionPolicyMetadataInfo(\n                metadata_name=\"counts\",\n                is_mch_metadata=True,\n                is_history_metadata=False,\n            ),\n            MCHEvictionPolicyMetadataInfo(\n                metadata_name=\"last_access_iter\",\n                is_mch_metadata=True,\n                is_history_metadata=True,\n            ),\n        ]\n        self._decay_exponent = decay_exponent",
  "def metadata_info(self) -> List[MCHEvictionPolicyMetadataInfo]:\n        return self._metadata_info",
  "def record_history_metadata(\n        self,\n        current_iter: int,\n        incoming_ids: torch.Tensor,\n        history_metadata: Dict[str, torch.Tensor],\n    ) -> None:\n        history_last_access_iter = history_metadata[\"last_access_iter\"]\n        history_last_access_iter[:] = current_iter",
  "def coalesce_history_metadata(\n        self,\n        current_iter: int,\n        history_metadata: Dict[str, torch.Tensor],\n        unique_ids_counts: torch.Tensor,\n        unique_inverse_mapping: torch.Tensor,\n        additional_ids: Optional[torch.Tensor] = None,\n    ) -> Dict[str, torch.Tensor]:\n        coalesced_history_metadata: Dict[str, torch.Tensor] = {}\n        history_last_access_iter = history_metadata[\"last_access_iter\"]\n        if additional_ids is not None:\n            history_last_access_iter = torch.cat(\n                [\n                    history_last_access_iter,\n                    torch.full_like(additional_ids, current_iter),\n                ]\n            )\n        coalesced_history_metadata[\"last_access_iter\"] = torch.zeros_like(\n            unique_ids_counts\n        ).scatter_reduce_(\n            0,\n            unique_inverse_mapping,\n            history_last_access_iter,\n            reduce=\"amax\",\n            include_self=False,\n        )\n        return coalesced_history_metadata",
  "def update_metadata_and_generate_eviction_scores(\n        self,\n        current_iter: int,\n        mch_size: int,\n        coalesced_history_argsort_mapping: torch.Tensor,\n        coalesced_history_sorted_unique_ids_counts: torch.Tensor,\n        coalesced_history_mch_matching_elements_mask: torch.Tensor,\n        coalesced_history_mch_matching_indices: torch.Tensor,\n        mch_metadata: Dict[str, torch.Tensor],\n        coalesced_history_metadata: Dict[str, torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        mch_counts = mch_metadata[\"counts\"]\n        mch_last_access_iter = mch_metadata[\"last_access_iter\"]\n\n        # sort coalesced history metadata\n        coalesced_history_metadata[\"last_access_iter\"].copy_(\n            coalesced_history_metadata[\"last_access_iter\"][\n                coalesced_history_argsort_mapping\n            ]\n        )\n        coalesced_history_sorted_uniq_ids_last_access_iter = coalesced_history_metadata[\n            \"last_access_iter\"\n        ]\n\n        # update metadata for matching ids\n        mch_counts[\n            coalesced_history_mch_matching_indices\n        ] += coalesced_history_sorted_unique_ids_counts[\n            coalesced_history_mch_matching_elements_mask\n        ]\n        mch_last_access_iter[\n            coalesced_history_mch_matching_indices\n        ] = coalesced_history_sorted_uniq_ids_last_access_iter[\n            coalesced_history_mch_matching_elements_mask\n        ]\n\n        # incoming non-matching ids\n        new_sorted_uniq_ids_counts = coalesced_history_sorted_unique_ids_counts[\n            ~coalesced_history_mch_matching_elements_mask\n        ]\n        new_sorted_uniq_ids_last_access = (\n            coalesced_history_sorted_uniq_ids_last_access_iter[\n                ~coalesced_history_mch_matching_elements_mask\n            ]\n        )\n\n        # TODO: find cleaner way to avoid last element of zch\n        mch_counts[mch_size - 1] = torch.iinfo(torch.int64).max\n        mch_last_access_iter[mch_size - 1] = current_iter\n\n        merged_counts = torch.cat(\n            [\n                mch_counts,\n                new_sorted_uniq_ids_counts,\n            ]\n        )\n        merged_access_iter = torch.cat(\n            [\n                mch_last_access_iter,\n                new_sorted_uniq_ids_last_access,\n            ]\n        )\n        merged_weighted_distance = torch.pow(\n            current_iter - merged_access_iter + 1,\n            self._decay_exponent,\n        )\n        # merged eviction scores are the eviction scores calculated for the\n        #   tensor torch.cat[_mch_sorted_raw_ids, frequency_sorted_uniq_ids[~matching_eles]]\n        # lower scores are evicted first.\n        merged_eviction_scores = torch.div(merged_counts, merged_weighted_distance)\n\n        # calculate evicted and replacement indices\n        (\n            evicted_indices,\n            selected_new_indices,\n        ) = self._compute_selected_eviction_and_replacement_indices(\n            mch_size,\n            merged_eviction_scores,\n        )\n\n        # update metadata for evicted ids\n        mch_counts[evicted_indices] = new_sorted_uniq_ids_counts[selected_new_indices]\n\n        mch_last_access_iter[evicted_indices] = new_sorted_uniq_ids_last_access[\n            selected_new_indices\n        ]\n\n        return evicted_indices, selected_new_indices",
  "def __init__(\n        self,\n        zch_size: int,\n        device: torch.device,\n        eviction_policy: MCHEvictionPolicy,\n        eviction_interval: int,\n        input_hash_size: int = 2**63,\n        input_hash_func: Optional[Callable[[torch.Tensor, int], torch.Tensor]] = None,\n        input_history_buffer_size: Optional[int] = None,\n        mch_size: Optional[int] = None,  # experimental\n        mch_hash_func: Optional[Callable[[torch.Tensor, int], torch.Tensor]] = None,\n        output_global_offset: int = 0,  # typically not provided by user\n    ) -> None:\n        super().__init__(device)\n\n        if input_history_buffer_size is None:\n            input_history_buffer_size = zch_size * 10\n        self._input_history_buffer_size: int = input_history_buffer_size\n        self._input_hash_size = input_hash_size\n        self._zch_size: int = zch_size\n        assert self._zch_size > 0, \"zch_size must be > 0\"\n        self._mch_size: int = 0\n        if mch_size is not None:\n            self._mch_size = mch_size\n            assert (\n                mch_hash_func is not None\n            ), \"mch_hash_func must be provided if mch_size is provided\"\n        self._output_global_offset: int = output_global_offset\n        self._mch_hash_func = mch_hash_func\n        self._input_hash_func = input_hash_func\n\n        self._eviction_interval = eviction_interval\n        self._eviction_policy = eviction_policy\n\n        self._current_iter: int = 0\n        self._init_buffers()\n\n        ## ------ history info ------\n        self._mch_metadata: Dict[str, torch.Tensor] = {}\n        self._history_metadata: Dict[str, torch.Tensor] = {}\n        self._init_metadata_buffers()\n        self._current_history_buffer_offset: int = 0\n\n        self._evicted: bool = False",
  "def _init_buffers(self) -> None:\n        self.register_buffer(\n            \"_mch_sorted_raw_ids\",\n            torch.full(\n                (self._zch_size,),\n                torch.iinfo(torch.int64).max,\n                dtype=torch.int64,\n                device=self.device,\n            ),\n        )\n        self.register_buffer(\n            \"_mch_remapped_ids_mapping\",\n            torch.arange(self._zch_size, dtype=torch.int64, device=self.device),\n        )\n\n        # TODO: explicitly create / destory this buffer when going between training / eval\n        self._history_accumulator: torch.Tensor = torch.empty(\n            self._input_history_buffer_size if self.training else 0,\n            dtype=torch.int64,\n            device=self.device,\n        )\n\n        self._evicted_emb_indices: torch.Tensor = torch.empty((1,), device=self.device)",
  "def _init_metadata_buffers(self) -> None:\n        eviction_metadata_info = self._eviction_policy.metadata_info\n        for metadata in eviction_metadata_info:\n            metadata_name, is_mch_metadata, is_history_metadata = metadata\n            # mch_metadata\n            if is_mch_metadata:\n                buffer_name = \"_mch_\" + metadata_name\n                self.register_buffer(\n                    buffer_name,\n                    torch.zeros(\n                        (self._zch_size,),\n                        dtype=torch.int64,\n                        device=self.device,\n                    ),\n                )\n                self._mch_metadata[metadata_name] = getattr(self, buffer_name)\n            # history_metadata\n            if is_history_metadata:\n                buffer_name = \"_history_\" + metadata_name\n                self.register_buffer(\n                    buffer_name,\n                    torch.zeros(\n                        self._input_history_buffer_size,\n                        dtype=torch.int64,\n                        device=self.device,\n                    ),\n                    # not checkpointed\n                    persistent=False,\n                )\n                self._history_metadata[metadata_name] = getattr(self, buffer_name)",
  "def preprocess(self, features: Dict[str, JaggedTensor]) -> Dict[str, JaggedTensor]:\n        if self._input_hash_func is None:\n            return features\n        preprocessed_features: Dict[str, JaggedTensor] = {}\n        for name, feature in features.items():\n            preprocessed_features[name] = JaggedTensor(\n                # pyre-ignore [29]\n                values=self._input_hash_func(feature.values(), self._input_hash_size),\n                lengths=feature.lengths(),\n                offsets=feature.offsets(),\n                weights=feature.weights_or_none(),\n            )\n        return preprocessed_features",
  "def _match_indices(\n        self, sorted_sequence: torch.Tensor, search_values: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        searched_indices = torch.searchsorted(sorted_sequence[:-1], search_values)\n        retrieved_ids = sorted_sequence[searched_indices]\n        matching_eles = retrieved_ids == search_values\n        matched_indices = searched_indices[matching_eles]\n        return (matching_eles, matched_indices)",
  "def _sort_mch_buffers(self) -> None:\n        mch_sorted_raw_ids = self._mch_sorted_raw_ids\n        argsorted_sorted_raw_ids = torch.argsort(mch_sorted_raw_ids, stable=True)\n        mch_sorted_raw_ids.copy_(mch_sorted_raw_ids[argsorted_sorted_raw_ids])\n        self._mch_remapped_ids_mapping.copy_(\n            self._mch_remapped_ids_mapping[argsorted_sorted_raw_ids]\n        )\n        for mch_metadata_buffer in self._mch_metadata.values():\n            mch_metadata_buffer.copy_(mch_metadata_buffer[argsorted_sorted_raw_ids])",
  "def _update_and_evict(\n        self,\n        uniq_ids: torch.Tensor,\n        uniq_ids_counts: torch.Tensor,\n        uniq_ids_metadata: Dict[str, torch.Tensor],\n    ) -> None:\n        argsorted_uniq_ids_counts = torch.argsort(\n            uniq_ids_counts, descending=True, stable=True\n        )\n        frequency_sorted_uniq_ids = uniq_ids[argsorted_uniq_ids_counts]\n        frequency_sorted_uniq_ids_counts = uniq_ids_counts[argsorted_uniq_ids_counts]\n\n        matching_eles, matched_indices = self._match_indices(\n            self._mch_sorted_raw_ids, frequency_sorted_uniq_ids\n        )\n\n        new_frequency_sorted_uniq_ids = frequency_sorted_uniq_ids[~matching_eles]\n\n        # evicted_indices are indices in the mch map to be evicted, and\n        #   selected_new_indices are the indices of the ids in the coalesced\n        #   history that are to be added to the mch.\n        (\n            evicted_indices,\n            selected_new_indices,\n        ) = self._eviction_policy.update_metadata_and_generate_eviction_scores(\n            self._current_iter,\n            self._zch_size,\n            argsorted_uniq_ids_counts,\n            frequency_sorted_uniq_ids_counts,\n            matching_eles,\n            matched_indices,\n            self._mch_metadata,\n            uniq_ids_metadata,\n        )\n        self._mch_sorted_raw_ids[evicted_indices] = new_frequency_sorted_uniq_ids[\n            selected_new_indices\n        ]\n\n        # NOTE evicted ids for emb reset\n        # if evicted flag is already set, then existing evicted ids havent been\n        # consumed by evict(). append new evicted ids to the list\n        if self._evicted:\n            self._evicted_emb_indices = torch.unique(\n                torch.cat(\n                    [\n                        self._evicted_emb_indices,\n                        self._mch_remapped_ids_mapping[evicted_indices],\n                    ]\n                )\n            )\n        else:\n            self._evicted_emb_indices = self._mch_remapped_ids_mapping[evicted_indices]\n        self._evicted = True\n\n        # re-sort for next search\n        self._sort_mch_buffers()",
  "def _coalesce_history(self) -> None:\n        current_history_accumulator = self._history_accumulator[\n            : self._current_history_buffer_offset\n        ]\n        uniq_ids, uniq_inverse_mapping, uniq_ids_counts = torch.unique(\n            current_history_accumulator,\n            return_inverse=True,\n            return_counts=True,\n        )\n        coalesced_eviction_history_metadata = (\n            self._eviction_policy.coalesce_history_metadata(\n                self._current_iter,\n                {\n                    metadata_name: metadata_buffer[\n                        : self._current_history_buffer_offset\n                    ]\n                    for metadata_name, metadata_buffer in self._history_metadata.items()\n                },\n                uniq_ids_counts,\n                uniq_inverse_mapping,\n            )\n        )\n        self._update_and_evict(\n            uniq_ids, uniq_ids_counts, coalesced_eviction_history_metadata\n        )\n        # reset buffer offset\n        self._current_history_buffer_offset = 0",
  "def profile(\n        self,\n        features: Dict[str, JaggedTensor],\n        force_insert: bool = False,\n    ) -> Dict[str, JaggedTensor]:\n        self._current_iter += 1\n        if not self.training:\n            return features\n\n        for _, feature in features.items():\n            values = feature.values()\n            # TODO: Find a better way to force_insert, doesnt really work...\n            if force_insert:\n                values = values.repeat(5)\n\n            free_elements = (\n                self._input_history_buffer_size - self._current_history_buffer_offset\n            )\n            values = values[:free_elements]\n            self._history_accumulator[\n                self._current_history_buffer_offset : self._current_history_buffer_offset\n                + values.shape[0]\n            ] = values\n            self._eviction_policy.record_history_metadata(\n                self._current_iter,\n                values,\n                {\n                    metadata_name: metadata_buffer[\n                        self._current_history_buffer_offset : self._current_history_buffer_offset\n                        + values.shape[0]\n                    ]\n                    for metadata_name, metadata_buffer in self._history_metadata.items()\n                },\n            )\n            self._current_history_buffer_offset += values.shape[0]\n\n        # coalesce history / evict\n        if self._current_iter % self._eviction_interval == 0 or force_insert:\n            self._coalesce_history()\n\n        return features",
  "def remap(self, features: Dict[str, JaggedTensor]) -> Dict[str, JaggedTensor]:\n\n        remapped_features: Dict[str, JaggedTensor] = {}\n        for name, feature in features.items():\n            values = feature.values()\n            remapped_ids = torch.empty_like(values)\n\n            # compute overlap between incoming IDs and remapping table\n            searched_indices = torch.searchsorted(self._mch_sorted_raw_ids[:-1], values)\n            retrieved_indices = self._mch_sorted_raw_ids[searched_indices]\n            # identify matching inputs IDs\n            matching_indices = retrieved_indices == values\n            # update output with remapped matching IDs\n            remapped_ids[matching_indices] = self._mch_remapped_ids_mapping[\n                searched_indices[matching_indices]\n            ]\n            # select non-matching values\n\n            if self._mch_size:\n                non_matching_values = values[~matching_indices]\n                # pyre-ignore [29]\n                hashed_non_matching = self._mch_hash_func(\n                    non_matching_values, self._mch_size\n                ).add(self._zch_size)\n                # offset hash ids to their starting range\n                remapped_ids[~matching_indices] = hashed_non_matching\n            else:\n                remapped_ids[~matching_indices] = self._zch_size - 1\n\n            remapped_features[name] = JaggedTensor(\n                values=remapped_ids,\n                lengths=feature.lengths(),\n                offsets=feature.offsets(),\n                weights=feature.weights_or_none(),\n            )\n        return remapped_features",
  "def forward(\n        self,\n        features: Dict[str, JaggedTensor],\n        force_insert: bool = False,\n    ) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Args:\n        feature (JaggedTensor]): feature representation\n        force_insert (bool): force insert this step\n        Returns:\n            Dict[str, JaggedTensor]: modified JT\n        \"\"\"\n\n        features = self.profile(\n            features,\n            force_insert=force_insert,\n        )\n        return self.remap(features)",
  "def output_size(self) -> int:\n        return self._zch_size + self._mch_size",
  "def input_size(self) -> int:\n        return self._input_hash_size",
  "def evict(self) -> Optional[torch.Tensor]:\n        if self._evicted:\n            self._evicted = False\n            return self._evicted_emb_indices\n        else:\n            return None",
  "def rebuild_with_output_id_range(\n        self,\n        output_id_range: Tuple[int, int],\n        device: Optional[torch.device] = None,\n    ) -> \"MCHManagedCollisionModule\":\n\n        new_output_size = output_id_range[1] - output_id_range[0]\n\n        new_zch_size = int(self._zch_size * (new_output_size / self.output_size()))\n        new_mch_size = new_output_size - new_zch_size\n        new_input_history_buffer_size = int(\n            self._input_history_buffer_size * (new_output_size / self.output_size())\n        )\n\n        return type(self)(\n            zch_size=new_zch_size,\n            device=device or self.device,\n            input_history_buffer_size=new_input_history_buffer_size,\n            eviction_policy=self._eviction_policy,\n            eviction_interval=self._eviction_interval,\n            input_hash_size=self._input_hash_size,\n            input_hash_func=self._input_hash_func,\n            mch_size=new_mch_size if new_mch_size > 0 else None,\n            mch_hash_func=self._mch_hash_func,\n            output_global_offset=output_id_range[0],\n        )",
  "def apply_feature_processors_to_kjt(\n    features: KeyedJaggedTensor,\n    feature_processors: Dict[str, nn.Module],\n) -> KeyedJaggedTensor:\n\n    processed_weights = []\n    features_dict = features.to_dict()\n\n    for key in features.keys():\n        jt = features_dict[key]\n        if key in feature_processors:\n            fp_jt = feature_processors[key](jt)\n            processed_weights.append(fp_jt.weights())\n        else:\n            processed_weights.append(\n                torch.ones(jt.values().shape[0], device=jt.values().device),\n            )\n\n    return KeyedJaggedTensor(\n        keys=features.keys(),\n        values=features.values(),\n        weights=torch.cat(processed_weights)\n        if processed_weights\n        else features.weights_or_none(),\n        lengths=features.lengths(),\n        offsets=features._offsets,\n        stride=features._stride,\n        length_per_key=features._length_per_key,\n        offset_per_key=features._offset_per_key,\n        index_per_key=features._index_per_key,\n    )",
  "class FeatureProcessedEmbeddingBagCollection(nn.Module):\n    \"\"\"\n    FeatureProcessedEmbeddingBagCollection represents a EmbeddingBagCollection module and a set of feature processor modules.\n    The inputs into the FP-EBC will first be modified by the feature processor before being passed into the embedding bag collection.\n\n    For details of input and output types, see EmbeddingBagCollection\n\n\n    Args:\n        embedding_bag_collection (EmbeddingBagCollection): ebc module\n        feature_processors (Dict[str, FeatureProcessor]): feature processors\n    Example::\n        fp_ebc = FeatureProcessedEmbeddingBagCollection(\n            EmbeddingBagCollection(...),\n            {\n                \"feature_1\": FeatureProcessorModule(...),\n                \"feature_2\": FeatureProcessorModule2(...),\n            }\n        )\n\n        features = KeyedJaggedTensor(\n            keys=[\"f1\", \"f2\"],\n            values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n            offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n        )\n\n        >>> fp_ebc(features).to_dict()\n        {\n            \"feature_1\": torch.Tensor(...)\n            \"feature_2\": torch.Tensor(...)\n        }\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        feature_processors: Union[\n            Dict[str, FeatureProcessor], FeatureProcessorsCollection\n        ],\n    ) -> None:\n        super().__init__()\n        self._embedding_bag_collection = embedding_bag_collection\n        self._feature_processors: Union[nn.ModuleDict, FeatureProcessorsCollection]\n\n        if isinstance(feature_processors, FeatureProcessorsCollection):\n            self._feature_processors = feature_processors\n        else:\n            self._feature_processors = nn.ModuleDict(feature_processors)\n\n            assert set(\n                sum(\n                    [\n                        config.feature_names\n                        for config in self._embedding_bag_collection.embedding_bag_configs()\n                    ],\n                    [],\n                )\n            ) == set(\n                feature_processors.keys()\n            ), \"Passed in feature processors do not match feature names of embedding bag\"\n\n        assert (\n            embedding_bag_collection.is_weighted()\n        ), \"EmbeddingBagCollection must accept weighted inputs for feature processor\"\n\n        feature_names_set: Set[str] = set()\n        for table_config in self._embedding_bag_collection.embedding_bag_configs():\n            feature_names_set.update(table_config.feature_names)\n        self._feature_names: List[str] = list(feature_names_set)\n\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedTensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            KeyedTensor\n        \"\"\"\n\n        if isinstance(self._feature_processors, FeatureProcessorsCollection):\n            fp_features = self._feature_processors(features)\n        else:\n            # TODO: This path isn't currently scriptable. May be hard to support Dict[nn.Module]. Workaround is to always use FP-Collections\n            fp_features = apply_feature_processors_to_kjt(\n                features,\n                self._feature_processors,\n            )\n\n        return self._embedding_bag_collection(fp_features)",
  "def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        feature_processors: Union[\n            Dict[str, FeatureProcessor], FeatureProcessorsCollection\n        ],\n    ) -> None:\n        super().__init__()\n        self._embedding_bag_collection = embedding_bag_collection\n        self._feature_processors: Union[nn.ModuleDict, FeatureProcessorsCollection]\n\n        if isinstance(feature_processors, FeatureProcessorsCollection):\n            self._feature_processors = feature_processors\n        else:\n            self._feature_processors = nn.ModuleDict(feature_processors)\n\n            assert set(\n                sum(\n                    [\n                        config.feature_names\n                        for config in self._embedding_bag_collection.embedding_bag_configs()\n                    ],\n                    [],\n                )\n            ) == set(\n                feature_processors.keys()\n            ), \"Passed in feature processors do not match feature names of embedding bag\"\n\n        assert (\n            embedding_bag_collection.is_weighted()\n        ), \"EmbeddingBagCollection must accept weighted inputs for feature processor\"\n\n        feature_names_set: Set[str] = set()\n        for table_config in self._embedding_bag_collection.embedding_bag_configs():\n            feature_names_set.update(table_config.feature_names)\n        self._feature_names: List[str] = list(feature_names_set)",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedTensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            KeyedTensor\n        \"\"\"\n\n        if isinstance(self._feature_processors, FeatureProcessorsCollection):\n            fp_features = self._feature_processors(features)\n        else:\n            # TODO: This path isn't currently scriptable. May be hard to support Dict[nn.Module]. Workaround is to always use FP-Collections\n            fp_features = apply_feature_processors_to_kjt(\n                features,\n                self._feature_processors,\n            )\n\n        return self._embedding_bag_collection(fp_features)",
  "class Perceptron(torch.nn.Module):\n    \"\"\"\n    Applies a linear transformation and activation.\n\n    Args:\n        in_size (int): number of elements in each input sample.\n        out_size (int): number of elements in each output sample.\n        bias (bool): if set to ``False``, the layer will not learn an additive bias.\n            Default: ``True``.\n        activation (Union[torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]):\n            the activation function to apply to the output of linear transformation.\n            Default: torch.relu.\n        device (Optional[torch.device]): default compute device.\n\n    Example::\n\n        batch_size = 3\n        in_size = 40\n        input = torch.randn(batch_size, in_size)\n\n        out_size = 16\n        perceptron = Perceptron(in_size, out_size, bias=True)\n\n        output = perceptron(input)\n        assert list(output) == [batch_size, out_size]\n    \"\"\"\n\n    def __init__(\n        self,\n        in_size: int,\n        out_size: int,\n        bias: bool = True,\n        activation: Union[\n            torch.nn.Module,\n            Callable[[torch.Tensor], torch.Tensor],\n        ] = torch.relu,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.modules.{self.__class__.__name__}\")\n        self._out_size = out_size\n        self._in_size = in_size\n        self._linear: nn.Linear = nn.Linear(\n            self._in_size, self._out_size, bias=bias, device=device\n        )\n        self._activation_fn: Callable[[torch.Tensor], torch.Tensor] = activation\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor of shape (B, I) where I is number of elements\n                in each input sample.\n\n        Returns:\n            torch.Tensor: tensor of shape (B, O) where O is number of elements per\n                channel in each output sample (i.e. `out_size`).\n        \"\"\"\n        return self._activation_fn(self._linear(input))",
  "class MLP(torch.nn.Module):\n    \"\"\"\n    Applies a stack of Perceptron modules sequentially (i.e. Multi-Layer Perceptron).\n\n    Args:\n        in_size (int): `in_size` of the input.\n        layer_sizes (List[int]): `out_size` of each Perceptron module.\n        bias (bool): if set to False, the layer will not learn an additive bias.\n            Default: True.\n        activation (str, Union[Callable[[], torch.nn.Module], torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]):\n            the activation function to apply to the output of linear transformation of\n            each Perceptron module.\n            If `activation` is a `str`, we currently only support the follow strings, as\n            \"relu\", \"sigmoid\", and \"swish_layernorm\".\n            If `activation` is a `Callable[[], torch.nn.Module]`, `activation()` will be\n            called once per Perceptron module to generate the activation module for that\n            Perceptron module, and the parameters won't be shared between those activation\n            modules.\n            One use case is when all the activation modules share the same constructor\n            arguments, but don't share the actual module parameters.\n            Default: torch.relu.\n        device (Optional[torch.device]): default compute device.\n\n    Example::\n\n        batch_size = 3\n        in_size = 40\n        input = torch.randn(batch_size, in_size)\n\n        layer_sizes = [16, 8, 4]\n        mlp_module = MLP(in_size, layer_sizes, bias=True)\n        output = mlp_module(input)\n        assert list(output.shape) == [batch_size, layer_sizes[-1]]\n    \"\"\"\n\n    def __init__(\n        self,\n        in_size: int,\n        layer_sizes: List[int],\n        bias: bool = True,\n        activation: Union[\n            str,\n            Callable[[], torch.nn.Module],\n            torch.nn.Module,\n            Callable[[torch.Tensor], torch.Tensor],\n        ] = torch.relu,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n\n        if activation == \"relu\":\n            activation = torch.relu\n        elif activation == \"sigmoid\":\n            activation = torch.sigmoid\n\n        if not isinstance(activation, str):\n            self._mlp: torch.nn.Module = torch.nn.Sequential(\n                *[\n                    Perceptron(\n                        layer_sizes[i - 1] if i > 0 else in_size,\n                        layer_sizes[i],\n                        bias=bias,\n                        activation=extract_module_or_tensor_callable(activation),\n                        device=device,\n                    )\n                    for i in range(len(layer_sizes))\n                ]\n            )\n        else:\n            if activation == \"swish_layernorm\":\n                self._mlp: torch.nn.Module = torch.nn.Sequential(\n                    *[\n                        Perceptron(\n                            layer_sizes[i - 1] if i > 0 else in_size,\n                            layer_sizes[i],\n                            bias=bias,\n                            activation=SwishLayerNorm(layer_sizes[i], device=device),\n                            device=device,\n                        )\n                        for i in range(len(layer_sizes))\n                    ]\n                )\n            else:\n                assert (\n                    ValueError\n                ), \"This MLP only support str version activation function of relu, sigmoid, and swish_layernorm\"\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor of shape (B, I) where I is number of elements\n                in each input sample.\n\n        Returns:\n            torch.Tensor: tensor of shape (B, O) where O is `out_size` of the last Perceptron module.\n        \"\"\"\n        return self._mlp(input)",
  "def __init__(\n        self,\n        in_size: int,\n        out_size: int,\n        bias: bool = True,\n        activation: Union[\n            torch.nn.Module,\n            Callable[[torch.Tensor], torch.Tensor],\n        ] = torch.relu,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.modules.{self.__class__.__name__}\")\n        self._out_size = out_size\n        self._in_size = in_size\n        self._linear: nn.Linear = nn.Linear(\n            self._in_size, self._out_size, bias=bias, device=device\n        )\n        self._activation_fn: Callable[[torch.Tensor], torch.Tensor] = activation",
  "def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor of shape (B, I) where I is number of elements\n                in each input sample.\n\n        Returns:\n            torch.Tensor: tensor of shape (B, O) where O is number of elements per\n                channel in each output sample (i.e. `out_size`).\n        \"\"\"\n        return self._activation_fn(self._linear(input))",
  "def __init__(\n        self,\n        in_size: int,\n        layer_sizes: List[int],\n        bias: bool = True,\n        activation: Union[\n            str,\n            Callable[[], torch.nn.Module],\n            torch.nn.Module,\n            Callable[[torch.Tensor], torch.Tensor],\n        ] = torch.relu,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n\n        if activation == \"relu\":\n            activation = torch.relu\n        elif activation == \"sigmoid\":\n            activation = torch.sigmoid\n\n        if not isinstance(activation, str):\n            self._mlp: torch.nn.Module = torch.nn.Sequential(\n                *[\n                    Perceptron(\n                        layer_sizes[i - 1] if i > 0 else in_size,\n                        layer_sizes[i],\n                        bias=bias,\n                        activation=extract_module_or_tensor_callable(activation),\n                        device=device,\n                    )\n                    for i in range(len(layer_sizes))\n                ]\n            )\n        else:\n            if activation == \"swish_layernorm\":\n                self._mlp: torch.nn.Module = torch.nn.Sequential(\n                    *[\n                        Perceptron(\n                            layer_sizes[i - 1] if i > 0 else in_size,\n                            layer_sizes[i],\n                            bias=bias,\n                            activation=SwishLayerNorm(layer_sizes[i], device=device),\n                            device=device,\n                        )\n                        for i in range(len(layer_sizes))\n                    ]\n                )\n            else:\n                assert (\n                    ValueError\n                ), \"This MLP only support str version activation function of relu, sigmoid, and swish_layernorm\"",
  "def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input (torch.Tensor): tensor of shape (B, I) where I is number of elements\n                in each input sample.\n\n        Returns:\n            torch.Tensor: tensor of shape (B, O) where O is `out_size` of the last Perceptron module.\n        \"\"\"\n        return self._mlp(input)",
  "class EmbeddingBagCollectionInterface(abc.ABC, nn.Module):\n    \"\"\"\n    Interface for `EmbeddingBagCollection`.\n    \"\"\"\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedTensor:\n        pass\n\n    @abc.abstractmethod\n    def embedding_bag_configs(\n        self,\n    ) -> List[EmbeddingBagConfig]:\n        pass\n\n    @abc.abstractmethod\n    def is_weighted(self) -> bool:\n        pass",
  "def get_embedding_names_by_table(\n    tables: Union[List[EmbeddingBagConfig], List[EmbeddingConfig]],\n) -> List[List[str]]:\n    shared_feature: Dict[str, bool] = {}\n    for embedding_config in tables:\n        for feature_name in embedding_config.feature_names:\n            if feature_name not in shared_feature:\n                shared_feature[feature_name] = False\n            else:\n                shared_feature[feature_name] = True\n    embedding_names_by_table: List[List[str]] = []\n    for embedding_config in tables:\n        embedding_names: List[str] = []\n        for feature_name in embedding_config.feature_names:\n            if shared_feature[feature_name]:\n                embedding_names.append(feature_name + \"@\" + embedding_config.name)\n            else:\n                embedding_names.append(feature_name)\n        embedding_names_by_table.append(embedding_names)\n    return embedding_names_by_table",
  "class EmbeddingBagCollection(EmbeddingBagCollectionInterface):\n    \"\"\"\n    EmbeddingBagCollection represents a collection of pooled embeddings (`EmbeddingBags`).\n\n    It processes sparse data in the form of `KeyedJaggedTensor` with values of the form\n    [F X B X L] where:\n\n    * F: features (keys)\n    * B: batch size\n    * L: length of sparse features (jagged)\n\n    and outputs a `KeyedTensor` with values of the form [B * (F * D)] where:\n\n    * F: features (keys)\n    * D: each feature's (key's) embedding dimension\n    * B: batch size\n\n    Args:\n        tables (List[EmbeddingBagConfig]): list of embedding tables.\n        is_weighted (bool): whether input `KeyedJaggedTensor` is weighted.\n        device (Optional[torch.device]): default compute device.\n\n    Example::\n\n        table_0 = EmbeddingBagConfig(\n            name=\"t1\", embedding_dim=3, num_embeddings=10, feature_names=[\"f1\"]\n        )\n        table_1 = EmbeddingBagConfig(\n            name=\"t2\", embedding_dim=4, num_embeddings=10, feature_names=[\"f2\"]\n        )\n\n        ebc = EmbeddingBagCollection(tables=[table_0, table_1])\n\n        #        0       1        2  <-- batch\n        # \"f1\"   [0,1] None    [2]\n        # \"f2\"   [3]    [4]    [5,6,7]\n        #  ^\n        # feature\n\n        features = KeyedJaggedTensor(\n            keys=[\"f1\", \"f2\"],\n            values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n            offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n        )\n\n        pooled_embeddings = ebc(features)\n        print(pooled_embeddings.values())\n        tensor([[-0.8899, -0.1342, -1.9060, -0.0905, -0.2814, -0.9369, -0.7783],\n            [ 0.0000,  0.0000,  0.0000,  0.1598,  0.0695,  1.3265, -0.1011],\n            [-0.4256, -1.1846, -2.1648, -1.0893,  0.3590, -1.9784, -0.7681]],\n            grad_fn=<CatBackward0>)\n        print(pooled_embeddings.keys())\n        ['f1', 'f2']\n        print(pooled_embeddings.offset_per_key())\n        tensor([0, 3, 7])\n    \"\"\"\n\n    def __init__(\n        self,\n        tables: List[EmbeddingBagConfig],\n        is_weighted: bool = False,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.modules.{self.__class__.__name__}\")\n        self._is_weighted = is_weighted\n        self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n        self._embedding_bag_configs = tables\n        self._lengths_per_embedding: List[int] = []\n        self._device: torch.device = (\n            device if device is not None else torch.device(\"cpu\")\n        )\n\n        table_names = set()\n        for embedding_config in tables:\n            if embedding_config.name in table_names:\n                raise ValueError(f\"Duplicate table name {embedding_config.name}\")\n            table_names.add(embedding_config.name)\n            dtype = (\n                torch.float32\n                if embedding_config.data_type == DataType.FP32\n                else torch.float16\n            )\n            self.embedding_bags[embedding_config.name] = nn.EmbeddingBag(\n                num_embeddings=embedding_config.num_embeddings,\n                embedding_dim=embedding_config.embedding_dim,\n                mode=pooling_type_to_str(embedding_config.pooling),\n                device=self._device,\n                include_last_offset=True,\n                dtype=dtype,\n            )\n\n            if not embedding_config.feature_names:\n                embedding_config.feature_names = [embedding_config.name]\n            self._lengths_per_embedding.extend(\n                len(embedding_config.feature_names) * [embedding_config.embedding_dim]\n            )\n\n        self._embedding_names: List[str] = [\n            embedding\n            for embeddings in get_embedding_names_by_table(tables)\n            for embedding in embeddings\n        ]\n        self._feature_names: List[List[str]] = [table.feature_names for table in tables]\n        self.reset_parameters()\n\n    def forward(self, features: KeyedJaggedTensor) -> KeyedTensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            KeyedTensor\n        \"\"\"\n\n        pooled_embeddings: List[torch.Tensor] = []\n\n        feature_dict = features.to_dict()\n        for i, embedding_bag in enumerate(self.embedding_bags.values()):\n            for feature_name in self._feature_names[i]:\n                f = feature_dict[feature_name]\n                res = embedding_bag(\n                    input=f.values(),\n                    offsets=f.offsets(),\n                    per_sample_weights=f.weights() if self._is_weighted else None,\n                ).float()\n                pooled_embeddings.append(res)\n        data = torch.cat(pooled_embeddings, dim=1)\n        return KeyedTensor(\n            keys=self._embedding_names,\n            values=data,\n            length_per_key=self._lengths_per_embedding,\n        )\n\n    def is_weighted(self) -> bool:\n        return self._is_weighted\n\n    def embedding_bag_configs(self) -> List[EmbeddingBagConfig]:\n        return self._embedding_bag_configs\n\n    @property\n    def device(self) -> torch.device:\n        return self._device\n\n    def reset_parameters(self) -> None:\n        if (isinstance(self.device, torch.device) and self.device.type == \"meta\") or (\n            isinstance(self.device, str) and self.device == \"meta\"\n        ):\n            return\n        # Initialize embedding bags weights with init_fn\n        for table_config in self._embedding_bag_configs:\n            assert table_config.init_fn is not None\n            param = self.embedding_bags[f\"{table_config.name}\"].weight\n            # pyre-ignore\n            table_config.init_fn(param)",
  "class EmbeddingCollectionInterface(abc.ABC, nn.Module):\n    \"\"\"\n    Interface for `EmbeddingCollection`.\n    \"\"\"\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> Dict[str, JaggedTensor]:\n        pass\n\n    @abc.abstractmethod\n    def embedding_configs(\n        self,\n    ) -> List[EmbeddingConfig]:\n        pass\n\n    @abc.abstractmethod\n    def need_indices(self) -> bool:\n        pass\n\n    @abc.abstractmethod\n    def embedding_dim(self) -> int:\n        pass\n\n    @abc.abstractmethod\n    def embedding_names_by_table(self) -> List[List[str]]:\n        pass",
  "class EmbeddingCollection(EmbeddingCollectionInterface):\n    \"\"\"\n    EmbeddingCollection represents a collection of non-pooled embeddings.\n\n    It processes sparse data in the form of `KeyedJaggedTensor` of the form [F X B X L]\n    where:\n\n    * F: features (keys)\n    * B: batch size\n    * L: length of sparse features (variable)\n\n    and outputs `Dict[feature (key), JaggedTensor]`.\n    Each `JaggedTensor` contains values of the form (B * L) X D\n    where:\n\n    * B: batch size\n    * L: length of sparse features (jagged)\n    * D: each feature's (key's) embedding dimension and lengths are of the form L\n\n    Args:\n        tables (List[EmbeddingConfig]): list of embedding tables.\n        device (Optional[torch.device]): default compute device.\n        need_indices (bool): if we need to pass indices to the final lookup dict.\n\n    Example::\n\n        e1_config = EmbeddingConfig(\n            name=\"t1\", embedding_dim=3, num_embeddings=10, feature_names=[\"f1\"]\n        )\n        e2_config = EmbeddingConfig(\n            name=\"t2\", embedding_dim=3, num_embeddings=10, feature_names=[\"f2\"]\n        )\n\n        ec = EmbeddingCollection(tables=[e1_config, e2_config])\n\n        #     0       1        2  <-- batch\n        # 0   [0,1] None    [2]\n        # 1   [3]    [4]    [5,6,7]\n        # ^\n        # feature\n\n        features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f1\", \"f2\"],\n            values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n            offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n        )\n        feature_embeddings = ec(features)\n        print(feature_embeddings['f2'].values())\n        tensor([[-0.2050,  0.5478,  0.6054],\n        [ 0.7352,  0.3210, -3.0399],\n        [ 0.1279, -0.1756, -0.4130],\n        [ 0.7519, -0.4341, -0.0499],\n        [ 0.9329, -1.0697, -0.8095]], grad_fn=<EmbeddingBackward>)\n    \"\"\"\n\n    def __init__(  # noqa C901\n        self,\n        tables: List[EmbeddingConfig],\n        device: Optional[torch.device] = None,\n        need_indices: bool = False,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.modules.{self.__class__.__name__}\")\n        self.embeddings: nn.ModuleDict = nn.ModuleDict()\n        self._embedding_configs = tables\n        self._embedding_dim: int = -1\n        self._need_indices: bool = need_indices\n        self._device: torch.device = (\n            device if device is not None else torch.device(\"cpu\")\n        )\n\n        table_names = set()\n        for config in tables:\n            if config.name in table_names:\n                raise ValueError(f\"Duplicate table name {config.name}\")\n            table_names.add(config.name)\n            self._embedding_dim = (\n                config.embedding_dim if self._embedding_dim < 0 else self._embedding_dim\n            )\n            if self._embedding_dim != config.embedding_dim:\n                raise ValueError(\n                    \"All tables in a EmbeddingCollection are required to have same embedding dimension.\"\n                )\n            dtype = (\n                torch.float32 if config.data_type == DataType.FP32 else torch.float16\n            )\n            self.embeddings[config.name] = nn.Embedding(\n                num_embeddings=config.num_embeddings,\n                embedding_dim=config.embedding_dim,\n                device=device,\n                dtype=dtype,\n            )\n            if config.init_fn is not None:\n                config.init_fn(self.embeddings[config.name].weight)\n\n            if not config.feature_names:\n                config.feature_names = [config.name]\n\n        self._embedding_names_by_table: List[List[str]] = get_embedding_names_by_table(\n            tables\n        )\n        self._feature_names: List[List[str]] = [table.feature_names for table in tables]\n\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            Dict[str, JaggedTensor]\n        \"\"\"\n\n        feature_embeddings: Dict[str, JaggedTensor] = {}\n        jt_dict: Dict[str, JaggedTensor] = features.to_dict()\n        for i, emb_module in enumerate(self.embeddings.values()):\n            feature_names = self._feature_names[i]\n            embedding_names = self._embedding_names_by_table[i]\n            for j, embedding_name in enumerate(embedding_names):\n                feature_name = feature_names[j]\n                f = jt_dict[feature_name]\n                lookup = emb_module(\n                    input=f.values(),\n                ).float()\n                feature_embeddings[embedding_name] = JaggedTensor(\n                    values=lookup,\n                    lengths=f.lengths(),\n                    weights=f.values() if self._need_indices else None,\n                )\n        return feature_embeddings\n\n    def need_indices(self) -> bool:\n        return self._need_indices\n\n    def embedding_dim(self) -> int:\n        return self._embedding_dim\n\n    def embedding_configs(self) -> List[EmbeddingConfig]:\n        return self._embedding_configs\n\n    def embedding_names_by_table(self) -> List[List[str]]:\n        return self._embedding_names_by_table\n\n    @property\n    def device(self) -> torch.device:\n        return self._device\n\n    def reset_parameters(self) -> None:\n        if (isinstance(self.device, torch.device) and self.device.type == \"meta\") or (\n            isinstance(self.device, str) and self.device == \"meta\"\n        ):\n            return\n        # Initialize embedding bags weights with init_fn\n        for table_config in self._embedding_configs:\n            assert table_config.init_fn is not None\n            param = self.embeddings[f\"{table_config.name}\"].weight\n            # pyre-ignore\n            table_config.init_fn(param)",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedTensor:\n        pass",
  "def embedding_bag_configs(\n        self,\n    ) -> List[EmbeddingBagConfig]:\n        pass",
  "def is_weighted(self) -> bool:\n        pass",
  "def __init__(\n        self,\n        tables: List[EmbeddingBagConfig],\n        is_weighted: bool = False,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.modules.{self.__class__.__name__}\")\n        self._is_weighted = is_weighted\n        self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n        self._embedding_bag_configs = tables\n        self._lengths_per_embedding: List[int] = []\n        self._device: torch.device = (\n            device if device is not None else torch.device(\"cpu\")\n        )\n\n        table_names = set()\n        for embedding_config in tables:\n            if embedding_config.name in table_names:\n                raise ValueError(f\"Duplicate table name {embedding_config.name}\")\n            table_names.add(embedding_config.name)\n            dtype = (\n                torch.float32\n                if embedding_config.data_type == DataType.FP32\n                else torch.float16\n            )\n            self.embedding_bags[embedding_config.name] = nn.EmbeddingBag(\n                num_embeddings=embedding_config.num_embeddings,\n                embedding_dim=embedding_config.embedding_dim,\n                mode=pooling_type_to_str(embedding_config.pooling),\n                device=self._device,\n                include_last_offset=True,\n                dtype=dtype,\n            )\n\n            if not embedding_config.feature_names:\n                embedding_config.feature_names = [embedding_config.name]\n            self._lengths_per_embedding.extend(\n                len(embedding_config.feature_names) * [embedding_config.embedding_dim]\n            )\n\n        self._embedding_names: List[str] = [\n            embedding\n            for embeddings in get_embedding_names_by_table(tables)\n            for embedding in embeddings\n        ]\n        self._feature_names: List[List[str]] = [table.feature_names for table in tables]\n        self.reset_parameters()",
  "def forward(self, features: KeyedJaggedTensor) -> KeyedTensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            KeyedTensor\n        \"\"\"\n\n        pooled_embeddings: List[torch.Tensor] = []\n\n        feature_dict = features.to_dict()\n        for i, embedding_bag in enumerate(self.embedding_bags.values()):\n            for feature_name in self._feature_names[i]:\n                f = feature_dict[feature_name]\n                res = embedding_bag(\n                    input=f.values(),\n                    offsets=f.offsets(),\n                    per_sample_weights=f.weights() if self._is_weighted else None,\n                ).float()\n                pooled_embeddings.append(res)\n        data = torch.cat(pooled_embeddings, dim=1)\n        return KeyedTensor(\n            keys=self._embedding_names,\n            values=data,\n            length_per_key=self._lengths_per_embedding,\n        )",
  "def is_weighted(self) -> bool:\n        return self._is_weighted",
  "def embedding_bag_configs(self) -> List[EmbeddingBagConfig]:\n        return self._embedding_bag_configs",
  "def device(self) -> torch.device:\n        return self._device",
  "def reset_parameters(self) -> None:\n        if (isinstance(self.device, torch.device) and self.device.type == \"meta\") or (\n            isinstance(self.device, str) and self.device == \"meta\"\n        ):\n            return\n        # Initialize embedding bags weights with init_fn\n        for table_config in self._embedding_bag_configs:\n            assert table_config.init_fn is not None\n            param = self.embedding_bags[f\"{table_config.name}\"].weight\n            # pyre-ignore\n            table_config.init_fn(param)",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> Dict[str, JaggedTensor]:\n        pass",
  "def embedding_configs(\n        self,\n    ) -> List[EmbeddingConfig]:\n        pass",
  "def need_indices(self) -> bool:\n        pass",
  "def embedding_dim(self) -> int:\n        pass",
  "def embedding_names_by_table(self) -> List[List[str]]:\n        pass",
  "def __init__(  # noqa C901\n        self,\n        tables: List[EmbeddingConfig],\n        device: Optional[torch.device] = None,\n        need_indices: bool = False,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.modules.{self.__class__.__name__}\")\n        self.embeddings: nn.ModuleDict = nn.ModuleDict()\n        self._embedding_configs = tables\n        self._embedding_dim: int = -1\n        self._need_indices: bool = need_indices\n        self._device: torch.device = (\n            device if device is not None else torch.device(\"cpu\")\n        )\n\n        table_names = set()\n        for config in tables:\n            if config.name in table_names:\n                raise ValueError(f\"Duplicate table name {config.name}\")\n            table_names.add(config.name)\n            self._embedding_dim = (\n                config.embedding_dim if self._embedding_dim < 0 else self._embedding_dim\n            )\n            if self._embedding_dim != config.embedding_dim:\n                raise ValueError(\n                    \"All tables in a EmbeddingCollection are required to have same embedding dimension.\"\n                )\n            dtype = (\n                torch.float32 if config.data_type == DataType.FP32 else torch.float16\n            )\n            self.embeddings[config.name] = nn.Embedding(\n                num_embeddings=config.num_embeddings,\n                embedding_dim=config.embedding_dim,\n                device=device,\n                dtype=dtype,\n            )\n            if config.init_fn is not None:\n                config.init_fn(self.embeddings[config.name].weight)\n\n            if not config.feature_names:\n                config.feature_names = [config.name]\n\n        self._embedding_names_by_table: List[List[str]] = get_embedding_names_by_table(\n            tables\n        )\n        self._feature_names: List[List[str]] = [table.feature_names for table in tables]",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): KJT of form [F X B X L].\n\n        Returns:\n            Dict[str, JaggedTensor]\n        \"\"\"\n\n        feature_embeddings: Dict[str, JaggedTensor] = {}\n        jt_dict: Dict[str, JaggedTensor] = features.to_dict()\n        for i, emb_module in enumerate(self.embeddings.values()):\n            feature_names = self._feature_names[i]\n            embedding_names = self._embedding_names_by_table[i]\n            for j, embedding_name in enumerate(embedding_names):\n                feature_name = feature_names[j]\n                f = jt_dict[feature_name]\n                lookup = emb_module(\n                    input=f.values(),\n                ).float()\n                feature_embeddings[embedding_name] = JaggedTensor(\n                    values=lookup,\n                    lengths=f.lengths(),\n                    weights=f.values() if self._need_indices else None,\n                )\n        return feature_embeddings",
  "def need_indices(self) -> bool:\n        return self._need_indices",
  "def embedding_dim(self) -> int:\n        return self._embedding_dim",
  "def embedding_configs(self) -> List[EmbeddingConfig]:\n        return self._embedding_configs",
  "def embedding_names_by_table(self) -> List[List[str]]:\n        return self._embedding_names_by_table",
  "def device(self) -> torch.device:\n        return self._device",
  "def reset_parameters(self) -> None:\n        if (isinstance(self.device, torch.device) and self.device.type == \"meta\") or (\n            isinstance(self.device, str) and self.device == \"meta\"\n        ):\n            return\n        # Initialize embedding bags weights with init_fn\n        for table_config in self._embedding_configs:\n            assert table_config.init_fn is not None\n            param = self.embeddings[f\"{table_config.name}\"].weight\n            # pyre-ignore\n            table_config.init_fn(param)",
  "class BaseFeatureProcessor(nn.Module):\n    \"\"\"\n    Abstract base class for feature processor.\n    \"\"\"\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        features: Dict[str, JaggedTensor],\n    ) -> Dict[str, JaggedTensor]:\n        pass",
  "def position_weighted_module_update_features(\n    features: Dict[str, JaggedTensor],\n    weighted_features: Dict[str, JaggedTensor],\n) -> Dict[str, JaggedTensor]:\n    features.update(weighted_features)\n    return features",
  "class PositionWeightedModule(BaseFeatureProcessor):\n    \"\"\"\n    Adds position weights to id list features.\n\n    Args:\n        max_feature_lengths (Dict[str, int]): feature name to `max_length` mapping.\n            `max_length`, a.k.a truncation size, specifies the maximum number of ids\n            each sample has. For each feature, its position weight parameter size is\n            `max_length`.\n    \"\"\"\n\n    def __init__(\n        self,\n        max_feature_lengths: Dict[str, int],\n    ) -> None:\n        super().__init__()\n        self.max_feature_lengths = max_feature_lengths\n        self.position_weights: nn.ParameterDict = nn.ParameterDict()\n        for key, length in max_feature_lengths.items():\n            self.position_weights[key] = nn.Parameter(torch.empty([length]).fill_(1.0))\n\n    def forward(\n        self,\n        features: Dict[str, JaggedTensor],\n    ) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Args:\n            features (Dict[str, JaggedTensor]): dictionary of keys to `JaggedTensor`,\n                representing the features.\n\n        Returns:\n            Dict[str, JaggedTensor]: same as input features with `weights` field being populated.\n        \"\"\"\n\n        weighted_features: Dict[str, JaggedTensor] = {}\n        for key, position_weight in self.position_weights.items():\n            seq = torch.ops.fbgemm.offsets_range(\n                features[key].offsets().long(), torch.numel(features[key].values())\n            )\n            weighted_features[key] = JaggedTensor(\n                values=features[key].values(),\n                lengths=features[key].lengths(),\n                offsets=features[key].offsets(),\n                weights=torch.gather(position_weight, dim=0, index=seq),\n            )\n        return position_weighted_module_update_features(features, weighted_features)",
  "class BaseGroupedFeatureProcessor(nn.Module):\n    \"\"\"\n    Abstract base class for grouped feature processor\n    \"\"\"\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedJaggedTensor:\n        pass",
  "class PositionWeightedProcessor(BaseGroupedFeatureProcessor):\n    \"\"\"\n    PositionWeightedProcessor represents a processor to apply position weight to a KeyedJaggedTensor.\n\n    It can handle both unsharded and sharded input and output corresponding output\n\n    Args:\n        max_feature_lengths (Dict[str, int]): Dict of feature_lengths, the key is the feature_name and value is length.\n        device (Optional[torch.device]): default compute device.\n\n    Example::\n\n        keys=[\"Feature0\", \"Feature1\", \"Feature2\"]\n        values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 3, 4, 5, 6, 7])\n        lengths=torch.tensor([2, 0, 1, 1, 1, 3, 2, 3, 0])\n        features = KeyedJaggedTensor.from_lengths_sync(keys=keys, values=values, lengths=lengths)\n        pw = FeatureProcessorCollection(\n            feature_processor_modules={key: PositionWeightedFeatureProcessor(max_feature_length=100) for key in keys}\n        )\n        result = pw(features)\n        # result is\n        # KeyedJaggedTensor({\n        #     \"Feature0\": {\n        #         \"values\": [[0, 1], [], [2]],\n        #         \"weights\": [[1.0, 1.0], [], [1.0]]\n        #     },\n        #     \"Feature1\": {\n        #         \"values\": [[3], [4], [5, 6, 7]],\n        #         \"weights\": [[1.0], [1.0], [1.0, 1.0, 1.0]]\n        #     },\n        #     \"Feature2\": {\n        #         \"values\": [[3, 4], [5, 6, 7], []],\n        #         \"weights\": [[1.0, 1.0], [1.0, 1.0, 1.0], []]\n        #     }\n        # })\n    \"\"\"\n\n    def __init__(\n        self,\n        max_feature_lengths: Dict[str, int],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self.max_feature_lengths = max_feature_lengths\n        for length in self.max_feature_lengths.values():\n            if length <= 0:\n                raise\n        self.position_weights: nn.ParameterDict = nn.ParameterDict()\n        for key, length in max_feature_lengths.items():\n            self.position_weights[key] = nn.Parameter(\n                torch.empty([length], device=device).fill_(1.0)\n            )\n\n    def forward(self, features: KeyedJaggedTensor) -> KeyedJaggedTensor:\n        \"\"\"\n        In unsharded or non-pipelined model, the input features both contain fp_feature\n        and non_fp_features, and the output will filter out non_fp features\n        In sharded pipelining model, the input features can only contain either none\n        or all feature_processed features, since the input feature comes from the\n        input_dist() of ebc which will filter out the keys not in the ebc. And the\n        input size is same as output size\n\n        Args:\n            features (KeyedJaggedTensor): input features\n\n        Returns:\n            KeyedJaggedTensor\n        \"\"\"\n        if is_fx_tracing():\n            features_dict = features.to_dict()\n            weighted_features_names: List[str] = []\n            weighted_features_values: List[torch.Tensor] = []\n            weighted_features_lengths: List[torch.Tensor] = []\n            weighted_features_weights: List[torch.Tensor] = []\n            for key, position_weight in self.position_weights.items():\n                seq = torch.ops.fbgemm.offsets_range(\n                    features_dict[key].offsets().long(),\n                    torch.numel(features_dict[key].values()),\n                )\n                weighted_features_names.append(key)\n                weighted_features_values.append(features_dict[key].values())\n                weighted_features_lengths.append(features_dict[key].lengths())\n                weighted_features_weights.append(\n                    torch.gather(position_weight, dim=0, index=seq)\n                )\n            return KeyedJaggedTensor.from_lengths_sync(\n                keys=weighted_features_names,\n                values=torch.cat(weighted_features_values),\n                lengths=torch.cat(weighted_features_lengths),\n                weights=torch.cat(weighted_features_weights),\n            )\n        else:\n            feature_names = features.keys()\n            lengths = features.lengths()\n            offsets = features.offsets()\n            values = features.values()\n            length_per_key = features.length_per_key()\n            weights = features.weights_or_none()\n            batch_size = features.stride()\n\n            has_fp_id_list_feature = False\n            has_normal_id_list_feature = False\n\n            if weights is None:\n                cat_seq = torch.ops.fbgemm.offsets_range(\n                    offsets.long(), torch.numel(values)\n                )\n            else:\n                # for row-wise sharding\n                cat_seq = weights.long()\n            seqs = torch.split(cat_seq, features.length_per_key())\n\n            for feature_name in feature_names:\n                if feature_name in self.max_feature_lengths:\n                    has_fp_id_list_feature = True\n                else:\n                    has_normal_id_list_feature = True\n\n            # in sharded pipelining model, the input features can only contain either none\n            # or all feature_processed features, since the input feature comes from the\n            # input_dist() of ebc which will filter out the keys not in the ebc\n            # for the input features both contain fp_feature and normal_features, it could be\n            # unsharded or non-pipelined sharded models\n            if has_fp_id_list_feature:\n                # for sharded pipeling\n                if not has_normal_id_list_feature:\n                    processed_features_weights: List[torch.Tensor] = []\n                    for feature_index, feature_name in enumerate(feature_names):\n                        processed_weight = torch.gather(\n                            self.position_weights[feature_name],\n                            dim=0,\n                            index=seqs[feature_index],\n                        )\n                        processed_features_weights.append(processed_weight)\n                    fp_features = KeyedJaggedTensor(\n                        keys=feature_names,\n                        values=values,\n                        weights=torch.cat(processed_features_weights),\n                        lengths=lengths,\n                        offsets=offsets,\n                        stride=batch_size,\n                        length_per_key=length_per_key,\n                        offset_per_key=features.offset_per_key(),\n                        index_per_key=features._key_indices(),\n                    )\n                # for unsharded or sharded non-pipeling\n                else:\n                    feature_values = values.split(length_per_key)\n                    feature_lengths = lengths.split(batch_size)\n                    processed_features_names: List[str] = []\n                    processed_features_lengths: List[torch.Tensor] = []\n                    processed_features_values: List[torch.Tensor] = []\n                    processed_features_weights: List[torch.Tensor] = []\n                    for feature_index, feature_name in enumerate(feature_names):\n                        if feature_name in self.max_feature_lengths:\n                            feature_value = feature_values[feature_index]\n                            feature_length = feature_lengths[feature_index]\n                            processed_weight = torch.gather(\n                                self.position_weights[feature_name],\n                                dim=0,\n                                index=seqs[feature_index],\n                            )\n                            processed_features_names.append(feature_name)\n                            processed_features_lengths.append(feature_length)\n                            processed_features_values.append(feature_value)\n                            processed_features_weights.append(processed_weight)\n                    fp_features = KeyedJaggedTensor.from_lengths_sync(\n                        keys=processed_features_names,\n                        values=torch.cat(processed_features_values),\n                        lengths=torch.cat(processed_features_lengths),\n                        weights=torch.cat(processed_features_weights),\n                    )\n                return fp_features\n            # normal id_list feature\n            else:\n                return features\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        yield from ()\n\n    # pyre-fixme[14]: `state_dict` overrides method defined in `Module` inconsistently.\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n        for name, param in self.position_weights.items():\n            destination[prefix + f\"position_weights.{name}\"] = (\n                param if keep_vars else param.detach()\n            )\n        return destination",
  "def forward(\n        self,\n        features: Dict[str, JaggedTensor],\n    ) -> Dict[str, JaggedTensor]:\n        pass",
  "def __init__(\n        self,\n        max_feature_lengths: Dict[str, int],\n    ) -> None:\n        super().__init__()\n        self.max_feature_lengths = max_feature_lengths\n        self.position_weights: nn.ParameterDict = nn.ParameterDict()\n        for key, length in max_feature_lengths.items():\n            self.position_weights[key] = nn.Parameter(torch.empty([length]).fill_(1.0))",
  "def forward(\n        self,\n        features: Dict[str, JaggedTensor],\n    ) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Args:\n            features (Dict[str, JaggedTensor]): dictionary of keys to `JaggedTensor`,\n                representing the features.\n\n        Returns:\n            Dict[str, JaggedTensor]: same as input features with `weights` field being populated.\n        \"\"\"\n\n        weighted_features: Dict[str, JaggedTensor] = {}\n        for key, position_weight in self.position_weights.items():\n            seq = torch.ops.fbgemm.offsets_range(\n                features[key].offsets().long(), torch.numel(features[key].values())\n            )\n            weighted_features[key] = JaggedTensor(\n                values=features[key].values(),\n                lengths=features[key].lengths(),\n                offsets=features[key].offsets(),\n                weights=torch.gather(position_weight, dim=0, index=seq),\n            )\n        return position_weighted_module_update_features(features, weighted_features)",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedJaggedTensor:\n        pass",
  "def __init__(\n        self,\n        max_feature_lengths: Dict[str, int],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self.max_feature_lengths = max_feature_lengths\n        for length in self.max_feature_lengths.values():\n            if length <= 0:\n                raise\n        self.position_weights: nn.ParameterDict = nn.ParameterDict()\n        for key, length in max_feature_lengths.items():\n            self.position_weights[key] = nn.Parameter(\n                torch.empty([length], device=device).fill_(1.0)\n            )",
  "def forward(self, features: KeyedJaggedTensor) -> KeyedJaggedTensor:\n        \"\"\"\n        In unsharded or non-pipelined model, the input features both contain fp_feature\n        and non_fp_features, and the output will filter out non_fp features\n        In sharded pipelining model, the input features can only contain either none\n        or all feature_processed features, since the input feature comes from the\n        input_dist() of ebc which will filter out the keys not in the ebc. And the\n        input size is same as output size\n\n        Args:\n            features (KeyedJaggedTensor): input features\n\n        Returns:\n            KeyedJaggedTensor\n        \"\"\"\n        if is_fx_tracing():\n            features_dict = features.to_dict()\n            weighted_features_names: List[str] = []\n            weighted_features_values: List[torch.Tensor] = []\n            weighted_features_lengths: List[torch.Tensor] = []\n            weighted_features_weights: List[torch.Tensor] = []\n            for key, position_weight in self.position_weights.items():\n                seq = torch.ops.fbgemm.offsets_range(\n                    features_dict[key].offsets().long(),\n                    torch.numel(features_dict[key].values()),\n                )\n                weighted_features_names.append(key)\n                weighted_features_values.append(features_dict[key].values())\n                weighted_features_lengths.append(features_dict[key].lengths())\n                weighted_features_weights.append(\n                    torch.gather(position_weight, dim=0, index=seq)\n                )\n            return KeyedJaggedTensor.from_lengths_sync(\n                keys=weighted_features_names,\n                values=torch.cat(weighted_features_values),\n                lengths=torch.cat(weighted_features_lengths),\n                weights=torch.cat(weighted_features_weights),\n            )\n        else:\n            feature_names = features.keys()\n            lengths = features.lengths()\n            offsets = features.offsets()\n            values = features.values()\n            length_per_key = features.length_per_key()\n            weights = features.weights_or_none()\n            batch_size = features.stride()\n\n            has_fp_id_list_feature = False\n            has_normal_id_list_feature = False\n\n            if weights is None:\n                cat_seq = torch.ops.fbgemm.offsets_range(\n                    offsets.long(), torch.numel(values)\n                )\n            else:\n                # for row-wise sharding\n                cat_seq = weights.long()\n            seqs = torch.split(cat_seq, features.length_per_key())\n\n            for feature_name in feature_names:\n                if feature_name in self.max_feature_lengths:\n                    has_fp_id_list_feature = True\n                else:\n                    has_normal_id_list_feature = True\n\n            # in sharded pipelining model, the input features can only contain either none\n            # or all feature_processed features, since the input feature comes from the\n            # input_dist() of ebc which will filter out the keys not in the ebc\n            # for the input features both contain fp_feature and normal_features, it could be\n            # unsharded or non-pipelined sharded models\n            if has_fp_id_list_feature:\n                # for sharded pipeling\n                if not has_normal_id_list_feature:\n                    processed_features_weights: List[torch.Tensor] = []\n                    for feature_index, feature_name in enumerate(feature_names):\n                        processed_weight = torch.gather(\n                            self.position_weights[feature_name],\n                            dim=0,\n                            index=seqs[feature_index],\n                        )\n                        processed_features_weights.append(processed_weight)\n                    fp_features = KeyedJaggedTensor(\n                        keys=feature_names,\n                        values=values,\n                        weights=torch.cat(processed_features_weights),\n                        lengths=lengths,\n                        offsets=offsets,\n                        stride=batch_size,\n                        length_per_key=length_per_key,\n                        offset_per_key=features.offset_per_key(),\n                        index_per_key=features._key_indices(),\n                    )\n                # for unsharded or sharded non-pipeling\n                else:\n                    feature_values = values.split(length_per_key)\n                    feature_lengths = lengths.split(batch_size)\n                    processed_features_names: List[str] = []\n                    processed_features_lengths: List[torch.Tensor] = []\n                    processed_features_values: List[torch.Tensor] = []\n                    processed_features_weights: List[torch.Tensor] = []\n                    for feature_index, feature_name in enumerate(feature_names):\n                        if feature_name in self.max_feature_lengths:\n                            feature_value = feature_values[feature_index]\n                            feature_length = feature_lengths[feature_index]\n                            processed_weight = torch.gather(\n                                self.position_weights[feature_name],\n                                dim=0,\n                                index=seqs[feature_index],\n                            )\n                            processed_features_names.append(feature_name)\n                            processed_features_lengths.append(feature_length)\n                            processed_features_values.append(feature_value)\n                            processed_features_weights.append(processed_weight)\n                    fp_features = KeyedJaggedTensor.from_lengths_sync(\n                        keys=processed_features_names,\n                        values=torch.cat(processed_features_values),\n                        lengths=torch.cat(processed_features_lengths),\n                        weights=torch.cat(processed_features_weights),\n                    )\n                return fp_features\n            # normal id_list feature\n            else:\n                return features",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        yield from ()",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n        for name, param in self.position_weights.items():\n            destination[prefix + f\"position_weights.{name}\"] = (\n                param if keep_vars else param.detach()\n            )\n        return destination",
  "def _apply_functions_after_first_forward(\n    module: torch.nn.Module,\n    # pyre-ignore[2]\n    input: Any,\n    # pyre-ignore[2]\n    output: Any,\n) -> None:\n    _functions_to_lazy_apply = getattr(module, \"_functions_to_lazy_apply\", None)\n    if _functions_to_lazy_apply is not None:\n        for fn in _functions_to_lazy_apply:\n            module.apply(fn)\n        delattr(module, \"_functions_to_lazy_apply\")\n    module._lazy_apply_hook.remove()\n    delattr(module, \"_lazy_apply_hook\")",
  "def lazy_apply(\n    module: torch.nn.Module, fn: Callable[[torch.nn.Module], None]\n) -> torch.nn.Module:\n    \"\"\"Attaches a function to a module, which will be applied recursively to every\n    submodule (as returned by `.children()`) of the module as well as the module itself\n    right after the first forward pass (i.e. after all submodules and parameters have\n    been initialized).\n\n    Typical use includes initializing the numerical value of the parameters of a lazy\n    module (i.e. modules inherited from `LazyModuleMixin`).\n\n    NOTE:\n        `lazy_apply()` can be used on both lazy and non-lazy modules.\n\n    Args:\n        module (torch.nn.Module): module to recursively apply `fn` on.\n        fn (Callable[[torch.nn.Module], None]): function to be attached to `module` and\n            later be applied to each submodule of `module` and the `module` itself.\n\n    Returns:\n        torch.nn.Module: `module` with `fn` attached.\n\n    Example::\n\n        @torch.no_grad()\n        def init_weights(m):\n            print(m)\n            if type(m) == torch.nn.LazyLinear:\n                m.weight.fill_(1.0)\n                print(m.weight)\n\n        linear = torch.nn.LazyLinear(2)\n        lazy_apply(linear, init_weights)  # doesn't run `init_weights` immediately\n        input = torch.randn(2, 10)\n        linear(input)  # runs `init_weights` only once, right after first forward pass\n\n        seq = torch.nn.Sequential(torch.nn.LazyLinear(2), torch.nn.LazyLinear(2))\n        lazy_apply(seq, init_weights)  # doesn't run `init_weights` immediately\n        input = torch.randn(2, 10)\n        seq(input)  # runs `init_weights` only once, right after first forward pass\n    \"\"\"\n\n    if not hasattr(module, \"_functions_to_lazy_apply\"):\n        module._functions_to_lazy_apply = []\n    if not hasattr(module, \"_lazy_apply_hook\"):\n        module._lazy_apply_hook = module.register_forward_hook(\n            _apply_functions_after_first_forward\n        )\n    module._functions_to_lazy_apply.append(fn)\n    return module",
  "class _LazyExtensionProtocol(_LazyProtocol):\n    # pyre-ignore[2,3]\n    def _call_impl(self, *input, **kwargs):\n        ...",
  "class LazyModuleExtensionMixin(LazyModuleMixin):\n    \"\"\"\n    This is a temporary extension of `LazyModuleMixin` to support passing keyword\n    arguments to lazy module's `forward` method.\n\n    The long-term plan is to upstream this feature to `LazyModuleMixin`. Please see\n    https://github.com/pytorch/pytorch/issues/59923 for details.\n\n    Please see `TestLazyModuleExtensionMixin`, which contains unit tests that ensure:\n      * `LazyModuleExtensionMixin._infer_parameters` has source code parity with\n        torch.nn.modules.lazy.LazyModuleMixin._infer_parameters, except that the former\n        can accept keyword arguments.\n      * `LazyModuleExtensionMixin._call_impl` has source code parity with\n        `torch.nn.Module._call_impl`, except that the former can pass keyword arguments\n        to forward pre hooks.\"\n    \"\"\"\n\n    def apply(self, fn: Callable[[torch.nn.Module], None]) -> torch.nn.Module:\n        \"\"\"Applies `fn` recursively to every submodule (as returned by `.children()`)\n        as well as self. Typical use includes initializing the parameters of a model.\n\n        NOTE:\n            Calling `apply()` on an uninitialized lazy-module will result in an error.\n            User is required to initialize a lazy-module (by doing a dummy forward pass)\n            before calling `apply()` on the lazy-module.\n\n        Args:\n            fn (torch.nn.Module -> None): function to be applied to each submodule.\n\n        Returns:\n            torch.nn.Module: self\n\n        Example::\n\n            @torch.no_grad()\n            def init_weights(m):\n                print(m)\n                if type(m) == torch.nn.LazyLinear:\n                    m.weight.fill_(1.0)\n                    print(m.weight)\n\n            linear = torch.nn.LazyLinear(2)\n            linear.apply(init_weights)  # this fails, because `linear` (a lazy-module) hasn't been initialized yet\n\n            input = torch.randn(2, 10)\n            linear(input)  # run a dummy forward pass to initialize the lazy-module\n\n            linear.apply(init_weights)  # this works now\n        \"\"\"\n\n        if hasattr(self, \"_initialize_hook\"):\n            raise RuntimeError(\n                \"Module {} has not been initialized. \".format(self)\n                + \"Please run a dummy forward pass on the model to initialize all modules, \"\n                + \"or use torchrec.modules.lazy_extension.lazy_apply to attach a function \"\n                + \"to this module which would be applied after this module is initialized.\"\n            )\n        # If the module is already initialized, call `super().apply(fn)` to\n        # run the usual apply logic.\n        # pyre-ignore[16]\n        return super().apply(fn)\n\n    # fmt: off\n    # pyre-ignore[2, 47]\n    # pyre-fixme[14]: `_infer_parameters` overrides method defined in\n    #  `LazyModuleMixin` inconsistently.\n    def _infer_parameters(self: _LazyExtensionProtocol, module, input, kwargs) -> None:\n        r\"\"\"Infers the size and initializes the parameters according to the\n        provided input batch.\n        Given a module that contains parameters that were declared inferrable\n        using :class:`torch.nn.parameter.ParameterMode.Infer`, runs a forward pass\n        in the complete module using the provided input to initialize all the parameters\n        as needed.\n        The module is set into evaluation mode before running the forward pass in order\n        to avoid saving statistics or calculating gradients\n        \"\"\"\n        module.initialize_parameters(*input, **kwargs)\n        if module.has_uninitialized_params():\n            raise RuntimeError(f'module {self._get_name()} has not been fully initialized')\n        module._initialize_hook.remove()\n        module._load_hook.remove()\n        delattr(module, '_initialize_hook')\n        delattr(module, '_load_hook')\n        if module.cls_to_become is not None:\n            module.__class__ = module.cls_to_become\n    # fmt: on\n\n    # fmt: off\n    # pyre-ignore[2,3]\n    def _call_impl(self, *input, **kwargs):  # noqa: C901\n        # pyre-ignore[16]\n        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n        # If we don't have any hooks, we want to skip the rest of the logic in\n        # this function, and just call forward.\n        # pyre-ignore[16]\n        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n                or _global_forward_hooks or _global_forward_pre_hooks):\n            return forward_call(*input, **kwargs)\n        # Do not call functions when jit is used\n        full_backward_hooks, non_full_backward_hooks = [], []\n        if self._backward_hooks or _global_backward_hooks:\n            # pyre-ignore[16]\n            full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks()\n        if _global_forward_pre_hooks or self._forward_pre_hooks:\n            # pyre-ignore[60]: Concatenation not yet support for multiple variadic\n            #  tuples: `*torch.nn.modules.module._global_forward_pre_hooks.values(),\n            #  *self._forward_pre_hooks.values()`.\n            for hook in (*_global_forward_pre_hooks.values(), *self._forward_pre_hooks.values()):\n                if len(inspect.signature(hook).parameters) == 3:\n                    result = hook(self, input, kwargs)\n                else:\n                    result = hook(self, input)\n                if result is not None:\n                    if not isinstance(result, tuple):\n                        result = (result,)\n                    input = result\n\n        bw_hook = None\n        if full_backward_hooks:\n            # pyre-fixme[20]: Argument `user_pre_hooks` expected.\n            bw_hook = hooks.BackwardHook(self, full_backward_hooks)\n            input = bw_hook.setup_input_hook(input)\n\n        result = forward_call(*input, **kwargs)\n        if _global_forward_hooks or self._forward_hooks:\n            # pyre-ignore[60]: Concatenation not yet support for multiple variadic\n            #  tuples: `*torch.nn.modules.module._global_forward_hooks.values(),\n            #  *self._forward_hooks.values()`.\n            for hook in (*_global_forward_hooks.values(), *self._forward_hooks.values()):\n                hook_result = hook(self, input, result)\n                if hook_result is not None:\n                    result = hook_result\n\n        if bw_hook:\n            result = bw_hook.setup_output_hook(result)\n\n        # Handle the non-full backward hooks\n        if non_full_backward_hooks:\n            var = result\n            while not isinstance(var, torch.Tensor):\n                if isinstance(var, dict):\n                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n                else:\n                    var = var[0]\n            grad_fn = var.grad_fn\n            if grad_fn is not None:\n                for hook in non_full_backward_hooks:\n                    wrapper = functools.partial(hook, self)\n                    functools.update_wrapper(wrapper, hook)\n                    grad_fn.register_hook(wrapper)\n                # pyre-ignore[16]\n                self._maybe_warn_non_full_backward_hook(input, result, grad_fn)\n\n        return result\n    # fmt: on\n\n    # pyre-ignore[4]\n    # pyre-fixme[15]: `__call__` overrides attribute defined in `type` inconsistently.\n    __call__: Callable[..., Any] = _call_impl",
  "def _call_impl(self, *input, **kwargs):\n        ...",
  "def apply(self, fn: Callable[[torch.nn.Module], None]) -> torch.nn.Module:\n        \"\"\"Applies `fn` recursively to every submodule (as returned by `.children()`)\n        as well as self. Typical use includes initializing the parameters of a model.\n\n        NOTE:\n            Calling `apply()` on an uninitialized lazy-module will result in an error.\n            User is required to initialize a lazy-module (by doing a dummy forward pass)\n            before calling `apply()` on the lazy-module.\n\n        Args:\n            fn (torch.nn.Module -> None): function to be applied to each submodule.\n\n        Returns:\n            torch.nn.Module: self\n\n        Example::\n\n            @torch.no_grad()\n            def init_weights(m):\n                print(m)\n                if type(m) == torch.nn.LazyLinear:\n                    m.weight.fill_(1.0)\n                    print(m.weight)\n\n            linear = torch.nn.LazyLinear(2)\n            linear.apply(init_weights)  # this fails, because `linear` (a lazy-module) hasn't been initialized yet\n\n            input = torch.randn(2, 10)\n            linear(input)  # run a dummy forward pass to initialize the lazy-module\n\n            linear.apply(init_weights)  # this works now\n        \"\"\"\n\n        if hasattr(self, \"_initialize_hook\"):\n            raise RuntimeError(\n                \"Module {} has not been initialized. \".format(self)\n                + \"Please run a dummy forward pass on the model to initialize all modules, \"\n                + \"or use torchrec.modules.lazy_extension.lazy_apply to attach a function \"\n                + \"to this module which would be applied after this module is initialized.\"\n            )\n        # If the module is already initialized, call `super().apply(fn)` to\n        # run the usual apply logic.\n        # pyre-ignore[16]\n        return super().apply(fn)",
  "def _infer_parameters(self: _LazyExtensionProtocol, module, input, kwargs) -> None:\n        r\"\"\"Infers the size and initializes the parameters according to the\n        provided input batch.\n        Given a module that contains parameters that were declared inferrable\n        using :class:`torch.nn.parameter.ParameterMode.Infer`, runs a forward pass\n        in the complete module using the provided input to initialize all the parameters\n        as needed.\n        The module is set into evaluation mode before running the forward pass in order\n        to avoid saving statistics or calculating gradients\n        \"\"\"\n        module.initialize_parameters(*input, **kwargs)\n        if module.has_uninitialized_params():\n            raise RuntimeError(f'module {self._get_name()} has not been fully initialized')\n        module._initialize_hook.remove()\n        module._load_hook.remove()\n        delattr(module, '_initialize_hook')\n        delattr(module, '_load_hook')\n        if module.cls_to_become is not None:\n            module.__class__ = module.cls_to_become",
  "def _call_impl(self, *input, **kwargs):  # noqa: C901\n        # pyre-ignore[16]\n        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n        # If we don't have any hooks, we want to skip the rest of the logic in\n        # this function, and just call forward.\n        # pyre-ignore[16]\n        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n                or _global_forward_hooks or _global_forward_pre_hooks):\n            return forward_call(*input, **kwargs)\n        # Do not call functions when jit is used\n        full_backward_hooks, non_full_backward_hooks = [], []\n        if self._backward_hooks or _global_backward_hooks:\n            # pyre-ignore[16]\n            full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks()\n        if _global_forward_pre_hooks or self._forward_pre_hooks:\n            # pyre-ignore[60]: Concatenation not yet support for multiple variadic\n            #  tuples: `*torch.nn.modules.module._global_forward_pre_hooks.values(),\n            #  *self._forward_pre_hooks.values()`.\n            for hook in (*_global_forward_pre_hooks.values(), *self._forward_pre_hooks.values()):\n                if len(inspect.signature(hook).parameters) == 3:\n                    result = hook(self, input, kwargs)\n                else:\n                    result = hook(self, input)\n                if result is not None:\n                    if not isinstance(result, tuple):\n                        result = (result,)\n                    input = result\n\n        bw_hook = None\n        if full_backward_hooks:\n            # pyre-fixme[20]: Argument `user_pre_hooks` expected.\n            bw_hook = hooks.BackwardHook(self, full_backward_hooks)\n            input = bw_hook.setup_input_hook(input)\n\n        result = forward_call(*input, **kwargs)\n        if _global_forward_hooks or self._forward_hooks:\n            # pyre-ignore[60]: Concatenation not yet support for multiple variadic\n            #  tuples: `*torch.nn.modules.module._global_forward_hooks.values(),\n            #  *self._forward_hooks.values()`.\n            for hook in (*_global_forward_hooks.values(), *self._forward_hooks.values()):\n                hook_result = hook(self, input, result)\n                if hook_result is not None:\n                    result = hook_result\n\n        if bw_hook:\n            result = bw_hook.setup_output_hook(result)\n\n        # Handle the non-full backward hooks\n        if non_full_backward_hooks:\n            var = result\n            while not isinstance(var, torch.Tensor):\n                if isinstance(var, dict):\n                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n                else:\n                    var = var[0]\n            grad_fn = var.grad_fn\n            if grad_fn is not None:\n                for hook in non_full_backward_hooks:\n                    wrapper = functools.partial(hook, self)\n                    functools.update_wrapper(wrapper, hook)\n                    grad_fn.register_hook(wrapper)\n                # pyre-ignore[16]\n                self._maybe_warn_non_full_backward_hook(input, result, grad_fn)\n\n        return result",
  "class SparseArch(nn.Module):\n    \"\"\"\n    Processes the sparse features of the DeepFMNN model. Does embedding lookups for all\n    EmbeddingBag and embedding features of each collection.\n\n    Args:\n        embedding_bag_collection (EmbeddingBagCollection): represents a collection of\n            pooled embeddings.\n\n    Example::\n\n        eb1_config = EmbeddingBagConfig(\n            name=\"t1\", embedding_dim=3, num_embeddings=10, feature_names=[\"f1\"]\n        )\n        eb2_config = EmbeddingBagConfig(\n            name=\"t2\", embedding_dim=4, num_embeddings=10, feature_names=[\"f2\"]\n        )\n\n        ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])\n\n        #     0       1        2  <-- batch\n        # 0   [0,1] None    [2]\n        # 1   [3]    [4]    [5,6,7]\n        # ^\n        # feature\n        features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f1\", \"f2\"],\n            values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n            offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n        )\n\n        sparse_arch(features)\n    \"\"\"\n\n    def __init__(self, embedding_bag_collection: EmbeddingBagCollection) -> None:\n        super().__init__()\n        self.embedding_bag_collection: EmbeddingBagCollection = embedding_bag_collection\n\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedTensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor):\n\n        Returns:\n            KeyedJaggedTensor: an output KJT of size F * D X B.\n        \"\"\"\n        return self.embedding_bag_collection(features)",
  "class DenseArch(nn.Module):\n    \"\"\"\n    Processes the dense features of DeepFMNN model. Output layer is sized to\n    the embedding_dimension of the EmbeddingBagCollection embeddings.\n\n    Args:\n        in_features (int): dimensionality of the dense input features.\n        hidden_layer_size (int): sizes of the hidden layers in the DenseArch.\n        embedding_dim (int): the same size of the embedding_dimension of sparseArch.\n        device (torch.device): default compute device.\n\n    Example::\n\n        B = 20\n        D = 3\n        in_features = 10\n        dense_arch = DenseArch(\n            in_features=in_features, hidden_layer_size=10, embedding_dim=D\n        )\n\n        dense_arch_input = torch.rand((B, in_features))\n        dense_embedded = dense_arch(dense_arch_input)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        hidden_layer_size: int,\n        embedding_dim: int,\n    ) -> None:\n        super().__init__()\n        self.model: nn.Module = nn.Sequential(\n            nn.Linear(in_features, hidden_layer_size),\n            nn.ReLU(),\n            nn.Linear(hidden_layer_size, embedding_dim),\n            nn.ReLU(),\n        )\n\n    def forward(self, features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (torch.Tensor): size B X `num_features`.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X D.\n        \"\"\"\n        return self.model(features)",
  "class FMInteractionArch(nn.Module):\n    \"\"\"\n    Processes the output of both `SparseArch` (sparse_features) and `DenseArch`\n    (dense_features) and apply the general DeepFM interaction according to the\n    external source of DeepFM paper: https://arxiv.org/pdf/1703.04247.pdf\n\n    The output dimension is expected to be a cat of `dense_features`, D.\n\n    Args:\n        fm_in_features (int): the input dimension of `dense_module` in DeepFM. For\n            example, if the input embeddings is [randn(3, 2, 3), randn(3, 4, 5)], then\n            the `fm_in_features` should be: 2 * 3 + 4 * 5.\n        sparse_feature_names (List[str]): length of F.\n        deep_fm_dimension (int): output of the deep interaction (DI) in the DeepFM arch.\n\n    Example::\n\n        D = 3\n        B = 10\n        keys = [\"f1\", \"f2\"]\n        F = len(keys)\n        fm_inter_arch = FMInteractionArch(sparse_feature_names=keys)\n        dense_features = torch.rand((B, D))\n        sparse_features = KeyedTensor(\n            keys=keys,\n            length_per_key=[D, D],\n            values=torch.rand((B, D * F)),\n        )\n        cat_fm_output = fm_inter_arch(dense_features, sparse_features)\n    \"\"\"\n\n    def __init__(\n        self,\n        fm_in_features: int,\n        sparse_feature_names: List[str],\n        deep_fm_dimension: int,\n    ) -> None:\n        super().__init__()\n        self.sparse_feature_names: List[str] = sparse_feature_names\n        self.deep_fm = DeepFM(\n            dense_module=nn.Sequential(\n                nn.Linear(fm_in_features, deep_fm_dimension),\n                nn.ReLU(),\n            )\n        )\n        self.fm = FactorizationMachine()\n\n    def forward(\n        self, dense_features: torch.Tensor, sparse_features: KeyedTensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): tensor of size B X D.\n            sparse_features (KeyedJaggedTensor): KJT of size F * D X B.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X (D + DI + 1).\n        \"\"\"\n        if len(self.sparse_feature_names) == 0:\n            return dense_features\n\n        tensor_list: List[torch.Tensor] = [dense_features]\n        # dense/sparse interaction\n        # size B X F\n        for feature_name in self.sparse_feature_names:\n            tensor_list.append(sparse_features[feature_name])\n\n        deep_interaction = self.deep_fm(tensor_list)\n        fm_interaction = self.fm(tensor_list)\n\n        return torch.cat([dense_features, deep_interaction, fm_interaction], dim=1)",
  "class OverArch(nn.Module):\n    \"\"\"\n    Final Arch - simple MLP. The output is just one target.\n\n    Args:\n        in_features (int): the output dimension of the interaction arch.\n\n    Example::\n\n        B = 20\n        over_arch = OverArch()\n        logits = over_arch(torch.rand((B, 10)))\n    \"\"\"\n\n    def __init__(self, in_features: int) -> None:\n        super().__init__()\n        self.model: nn.Module = nn.Sequential(\n            nn.Linear(in_features, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (torch.Tensor):\n\n        Returns:\n            torch.Tensor: an output tensor of size B X 1.\n        \"\"\"\n        return self.model(features)",
  "class SimpleDeepFMNN(nn.Module):\n    \"\"\"\n    Basic recsys module with DeepFM arch. Processes sparse features by\n    learning pooled embeddings for each feature. Learns the relationship between\n    dense features and sparse features by projecting dense features into the same\n    embedding space. Learns the interaction among those dense and sparse features\n    by deep_fm proposed in this paper: https://arxiv.org/pdf/1703.04247.pdf\n\n    The module assumes all sparse features have the same embedding dimension\n    (i.e, each `EmbeddingBagConfig` uses the same embedding_dim)\n\n    The following notation is used throughout the documentation for the models:\n\n    * F: number of sparse features\n    * D: embedding_dimension of sparse features\n    * B: batch size\n    * num_features: number of dense features\n\n    Args:\n        num_dense_features (int): the number of input dense features.\n        embedding_bag_collection (EmbeddingBagCollection): collection of embedding bags\n            used to define `SparseArch`.\n        hidden_layer_size (int): the hidden layer size used in dense module.\n        deep_fm_dimension (int): the output layer size used in `deep_fm`'s deep\n            interaction module.\n\n    Example::\n\n        B = 2\n        D = 8\n\n        eb1_config = EmbeddingBagConfig(\n            name=\"t1\", embedding_dim=D, num_embeddings=100, feature_names=[\"f1\", \"f3\"]\n        )\n        eb2_config = EmbeddingBagConfig(\n            name=\"t2\",\n            embedding_dim=D,\n            num_embeddings=100,\n            feature_names=[\"f2\"],\n        )\n\n        ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])\n        sparse_nn = SimpleDeepFMNN(\n            embedding_bag_collection=ebc, hidden_layer_size=20, over_embedding_dim=5\n        )\n\n        features = torch.rand((B, 100))\n\n        #     0       1\n        # 0   [1,2] [4,5]\n        # 1   [4,3] [2,9]\n        # ^\n        # feature\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f1\", \"f3\"],\n            values=torch.tensor([1, 2, 4, 5, 4, 3, 2, 9]),\n            offsets=torch.tensor([0, 2, 4, 6, 8]),\n        )\n\n        logits = sparse_nn(\n            dense_features=features,\n            sparse_features=sparse_features,\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        num_dense_features: int,\n        embedding_bag_collection: EmbeddingBagCollection,\n        hidden_layer_size: int,\n        deep_fm_dimension: int,\n    ) -> None:\n        super().__init__()\n        assert (\n            len(embedding_bag_collection.embedding_bag_configs()) > 0\n        ), \"At least one embedding bag is required\"\n        for i in range(1, len(embedding_bag_collection.embedding_bag_configs())):\n            conf_prev = embedding_bag_collection.embedding_bag_configs()[i - 1]\n            conf = embedding_bag_collection.embedding_bag_configs()[i]\n            assert (\n                conf_prev.embedding_dim == conf.embedding_dim\n            ), \"All EmbeddingBagConfigs must have the same dimension\"\n        embedding_dim: int = embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n\n        feature_names = []\n\n        fm_in_features = embedding_dim\n        for conf in embedding_bag_collection.embedding_bag_configs():\n            for feat in conf.feature_names:\n                feature_names.append(feat)\n                fm_in_features += conf.embedding_dim\n\n        self.sparse_arch = SparseArch(embedding_bag_collection)\n        self.dense_arch = DenseArch(\n            in_features=num_dense_features,\n            hidden_layer_size=hidden_layer_size,\n            embedding_dim=embedding_dim,\n        )\n        self.inter_arch = FMInteractionArch(\n            fm_in_features=fm_in_features,\n            sparse_feature_names=feature_names,\n            deep_fm_dimension=deep_fm_dimension,\n        )\n        over_in_features = embedding_dim + deep_fm_dimension + 1\n        self.over_arch = OverArch(over_in_features)\n\n    def forward(\n        self,\n        dense_features: torch.Tensor,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): the dense features.\n            sparse_features (KeyedJaggedTensor): the sparse features.\n\n        Returns:\n            torch.Tensor: logits with size B X 1.\n        \"\"\"\n        embedded_dense = self.dense_arch(dense_features)\n        embedded_sparse = self.sparse_arch(sparse_features)\n        concatenated_dense = self.inter_arch(\n            dense_features=embedded_dense, sparse_features=embedded_sparse\n        )\n        logits = self.over_arch(concatenated_dense)\n        return logits",
  "def __init__(self, embedding_bag_collection: EmbeddingBagCollection) -> None:\n        super().__init__()\n        self.embedding_bag_collection: EmbeddingBagCollection = embedding_bag_collection",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedTensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor):\n\n        Returns:\n            KeyedJaggedTensor: an output KJT of size F * D X B.\n        \"\"\"\n        return self.embedding_bag_collection(features)",
  "def __init__(\n        self,\n        in_features: int,\n        hidden_layer_size: int,\n        embedding_dim: int,\n    ) -> None:\n        super().__init__()\n        self.model: nn.Module = nn.Sequential(\n            nn.Linear(in_features, hidden_layer_size),\n            nn.ReLU(),\n            nn.Linear(hidden_layer_size, embedding_dim),\n            nn.ReLU(),\n        )",
  "def forward(self, features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (torch.Tensor): size B X `num_features`.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X D.\n        \"\"\"\n        return self.model(features)",
  "def __init__(\n        self,\n        fm_in_features: int,\n        sparse_feature_names: List[str],\n        deep_fm_dimension: int,\n    ) -> None:\n        super().__init__()\n        self.sparse_feature_names: List[str] = sparse_feature_names\n        self.deep_fm = DeepFM(\n            dense_module=nn.Sequential(\n                nn.Linear(fm_in_features, deep_fm_dimension),\n                nn.ReLU(),\n            )\n        )\n        self.fm = FactorizationMachine()",
  "def forward(\n        self, dense_features: torch.Tensor, sparse_features: KeyedTensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): tensor of size B X D.\n            sparse_features (KeyedJaggedTensor): KJT of size F * D X B.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X (D + DI + 1).\n        \"\"\"\n        if len(self.sparse_feature_names) == 0:\n            return dense_features\n\n        tensor_list: List[torch.Tensor] = [dense_features]\n        # dense/sparse interaction\n        # size B X F\n        for feature_name in self.sparse_feature_names:\n            tensor_list.append(sparse_features[feature_name])\n\n        deep_interaction = self.deep_fm(tensor_list)\n        fm_interaction = self.fm(tensor_list)\n\n        return torch.cat([dense_features, deep_interaction, fm_interaction], dim=1)",
  "def __init__(self, in_features: int) -> None:\n        super().__init__()\n        self.model: nn.Module = nn.Sequential(\n            nn.Linear(in_features, 1),\n            nn.Sigmoid(),\n        )",
  "def forward(self, features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (torch.Tensor):\n\n        Returns:\n            torch.Tensor: an output tensor of size B X 1.\n        \"\"\"\n        return self.model(features)",
  "def __init__(\n        self,\n        num_dense_features: int,\n        embedding_bag_collection: EmbeddingBagCollection,\n        hidden_layer_size: int,\n        deep_fm_dimension: int,\n    ) -> None:\n        super().__init__()\n        assert (\n            len(embedding_bag_collection.embedding_bag_configs()) > 0\n        ), \"At least one embedding bag is required\"\n        for i in range(1, len(embedding_bag_collection.embedding_bag_configs())):\n            conf_prev = embedding_bag_collection.embedding_bag_configs()[i - 1]\n            conf = embedding_bag_collection.embedding_bag_configs()[i]\n            assert (\n                conf_prev.embedding_dim == conf.embedding_dim\n            ), \"All EmbeddingBagConfigs must have the same dimension\"\n        embedding_dim: int = embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n\n        feature_names = []\n\n        fm_in_features = embedding_dim\n        for conf in embedding_bag_collection.embedding_bag_configs():\n            for feat in conf.feature_names:\n                feature_names.append(feat)\n                fm_in_features += conf.embedding_dim\n\n        self.sparse_arch = SparseArch(embedding_bag_collection)\n        self.dense_arch = DenseArch(\n            in_features=num_dense_features,\n            hidden_layer_size=hidden_layer_size,\n            embedding_dim=embedding_dim,\n        )\n        self.inter_arch = FMInteractionArch(\n            fm_in_features=fm_in_features,\n            sparse_feature_names=feature_names,\n            deep_fm_dimension=deep_fm_dimension,\n        )\n        over_in_features = embedding_dim + deep_fm_dimension + 1\n        self.over_arch = OverArch(over_in_features)",
  "def forward(\n        self,\n        dense_features: torch.Tensor,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): the dense features.\n            sparse_features (KeyedJaggedTensor): the sparse features.\n\n        Returns:\n            torch.Tensor: logits with size B X 1.\n        \"\"\"\n        embedded_dense = self.dense_arch(dense_features)\n        embedded_sparse = self.sparse_arch(sparse_features)\n        concatenated_dense = self.inter_arch(\n            dense_features=embedded_dense, sparse_features=embedded_sparse\n        )\n        logits = self.over_arch(concatenated_dense)\n        return logits",
  "def choose(n: int, k: int) -> int:\n    \"\"\"\n    Simple implementation of math.comb for Python 3.7 compatibility.\n    \"\"\"\n    if 0 <= k <= n:\n        ntok = 1\n        ktok = 1\n        for t in range(1, min(k, n - k) + 1):\n            ntok *= n\n            ktok *= t\n            n -= 1\n        return ntok // ktok\n    else:\n        return 0",
  "class SparseArch(nn.Module):\n    \"\"\"\n    Processes the sparse features of DLRM. Does embedding lookups for all EmbeddingBag\n    and embedding features of each collection.\n\n    Args:\n        embedding_bag_collection (EmbeddingBagCollection): represents a collection of\n            pooled embeddings.\n\n    Example::\n\n        eb1_config = EmbeddingBagConfig(\n           name=\"t1\", embedding_dim=3, num_embeddings=10, feature_names=[\"f1\"]\n        )\n        eb2_config = EmbeddingBagConfig(\n           name=\"t2\", embedding_dim=4, num_embeddings=10, feature_names=[\"f2\"]\n        )\n\n        ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])\n        sparse_arch = SparseArch(embedding_bag_collection)\n\n        #     0       1        2  <-- batch\n        # 0   [0,1] None    [2]\n        # 1   [3]    [4]    [5,6,7]\n        # ^\n        # feature\n        features = KeyedJaggedTensor.from_offsets_sync(\n           keys=[\"f1\", \"f2\"],\n           values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n           offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n        )\n\n        sparse_embeddings = sparse_arch(features)\n    \"\"\"\n\n    def __init__(self, embedding_bag_collection: EmbeddingBagCollection) -> None:\n        super().__init__()\n        self.embedding_bag_collection: EmbeddingBagCollection = embedding_bag_collection\n        assert (\n            self.embedding_bag_collection.embedding_bag_configs\n        ), \"Embedding bag collection cannot be empty!\"\n        self.D: int = self.embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n        self._sparse_feature_names: List[str] = [\n            name\n            for conf in embedding_bag_collection.embedding_bag_configs()\n            for name in conf.feature_names\n        ]\n\n        self.F: int = len(self._sparse_feature_names)\n\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): an input tensor of sparse features.\n\n        Returns:\n            torch.Tensor: tensor of shape B X F X D.\n        \"\"\"\n\n        sparse_features: KeyedTensor = self.embedding_bag_collection(features)\n\n        sparse: Dict[str, torch.Tensor] = sparse_features.to_dict()\n        sparse_values: List[torch.Tensor] = []\n        for name in self.sparse_feature_names:\n            sparse_values.append(sparse[name])\n\n        return torch.cat(sparse_values, dim=1).reshape(-1, self.F, self.D)\n\n    @property\n    def sparse_feature_names(self) -> List[str]:\n        return self._sparse_feature_names",
  "class DenseArch(nn.Module):\n    \"\"\"\n    Processes the dense features of DLRM model.\n\n    Args:\n        in_features (int): dimensionality of the dense input features.\n        layer_sizes (List[int]): list of layer sizes.\n        device (Optional[torch.device]): default compute device.\n\n    Example::\n\n        B = 20\n        D = 3\n        dense_arch = DenseArch(10, layer_sizes=[15, D])\n        dense_embedded = dense_arch(torch.rand((B, 10)))\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        layer_sizes: List[int],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self.model: nn.Module = MLP(\n            in_features, layer_sizes, bias=True, activation=\"relu\", device=device\n        )\n\n    def forward(self, features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (torch.Tensor): an input tensor of dense features.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X D.\n        \"\"\"\n        return self.model(features)",
  "class InteractionArch(nn.Module):\n    \"\"\"\n    Processes the output of both `SparseArch` (sparse_features) and `DenseArch`\n    (dense_features). Returns the pairwise dot product of each sparse feature pair,\n    the dot product of each sparse features with the output of the dense layer,\n    and the dense layer itself (all concatenated).\n\n    .. note::\n        The dimensionality of the `dense_features` (D) is expected to match the\n        dimensionality of the `sparse_features` so that the dot products between them\n        can be computed.\n\n\n    Args:\n        num_sparse_features (int): F.\n\n    Example::\n\n        D = 3\n        B = 10\n        keys = [\"f1\", \"f2\"]\n        F = len(keys)\n        inter_arch = InteractionArch(num_sparse_features=len(keys))\n\n        dense_features = torch.rand((B, D))\n        sparse_features = torch.rand((B, F, D))\n\n        #  B X (D + F + F choose 2)\n        concat_dense = inter_arch(dense_features, sparse_features)\n    \"\"\"\n\n    def __init__(self, num_sparse_features: int) -> None:\n        super().__init__()\n        self.F: int = num_sparse_features\n        self.triu_indices: torch.Tensor = torch.triu_indices(\n            self.F + 1, self.F + 1, offset=1\n        )\n\n    def forward(\n        self, dense_features: torch.Tensor, sparse_features: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): an input tensor of size B X D.\n            sparse_features (torch.Tensor): an input tensor of size B X F X D.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X (D + F + F choose 2).\n        \"\"\"\n        if self.F <= 0:\n            return dense_features\n        (B, D) = dense_features.shape\n\n        combined_values = torch.cat(\n            (dense_features.unsqueeze(1), sparse_features), dim=1\n        )\n\n        # dense/sparse + sparse/sparse interaction\n        # size B X (F + F choose 2)\n        interactions = torch.bmm(\n            combined_values, torch.transpose(combined_values, 1, 2)\n        )\n        interactions_flat = interactions[:, self.triu_indices[0], self.triu_indices[1]]\n\n        return torch.cat((dense_features, interactions_flat), dim=1)",
  "class InteractionDCNArch(nn.Module):\n    \"\"\"\n    Processes the output of both `SparseArch` (sparse_features) and `DenseArch`\n    (dense_features). Returns the output of a Deep Cross Net v2\n    https://arxiv.org/pdf/2008.13535.pdf with a low rank approximation for the\n    weight matrix. The input and output sizes are the same for this\n    interaction layer (F*D + D).\n\n    .. note::\n        The dimensionality of the `dense_features` (D) is expected to match the\n        dimensionality of the `sparse_features` so that the dot products between them\n        can be computed.\n\n\n    Args:\n        num_sparse_features (int): F.\n\n    Example::\n\n        D = 3\n        B = 10\n        keys = [\"f1\", \"f2\"]\n        F = len(keys)\n        DCN = LowRankCrossNet(\n            in_features = F*D+D,\n            dcn_num_layers = 2,\n            dnc_low_rank_dim = 4,\n        )\n        inter_arch = InteractionDCNArch(\n            num_sparse_features=len(keys),\n            crossnet=DCN,\n        )\n\n        dense_features = torch.rand((B, D))\n        sparse_features = torch.rand((B, F, D))\n\n        #  B X (F*D + D)\n        concat_dense = inter_arch(dense_features, sparse_features)\n    \"\"\"\n\n    def __init__(self, num_sparse_features: int, crossnet: nn.Module) -> None:\n        super().__init__()\n        self.F: int = num_sparse_features\n        self.crossnet = crossnet\n\n    def forward(\n        self, dense_features: torch.Tensor, sparse_features: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): an input tensor of size B X D.\n            sparse_features (torch.Tensor): an input tensor of size B X F X D.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X (F*D + D).\n        \"\"\"\n        if self.F <= 0:\n            return dense_features\n        (B, D) = dense_features.shape\n\n        combined_values = torch.cat(\n            (dense_features.unsqueeze(1), sparse_features), dim=1\n        )\n\n        # size B X (F*D + D)\n        return self.crossnet(combined_values.reshape([B, -1]))",
  "class InteractionProjectionArch(nn.Module):\n    \"\"\"\n    Processes the output of both `SparseArch` (sparse_features) and `DenseArch`\n    (dense_features). Return Y*Z and the dense layer itself (all concatenated)\n    where Y is the output of interaction branch 1 and Z is the output of interaction\n    branch 2. Y and Z are of size Bx(F1xD) and Bx(DxF2) respectively for some F1 and F2.\n\n    .. note::\n\n        The dimensionality of the `dense_features` (D) is expected to match the\n        dimensionality of the `sparse_features` so that the dot products between them\n        can be computed.\n        The output dimension of the 2 interaction branches should be a multiple\n        of D.\n\n\n    Args:\n        num_sparse_features (int): F.\n        interaction_branch1 (nn.Module): MLP module for the first branch of\n            interaction layer\n        interaction_branch2 (nn.Module): MLP module for the second branch of\n            interaction layer\n\n    Example::\n\n        D = 3\n        B = 10\n        keys = [\"f1\", \"f2\"]\n        F = len(keys)\n        # Assume last layer of\n        I1 = DenseArch(\n            in_features= 3 * D + D,\n            layer_sizes=[4*D, 4*D], # F1 = 4\n            device=dense_device,\n        )\n        I2 = DenseArch(\n            in_features= 3 * D + D,\n            layer_sizes=[4*D, 4*D], # F2 = 4\n            device=dense_device,\n        )\n        inter_arch = InteractionProjectionArch(\n                        num_sparse_features=len(keys),\n                        interaction_branch1 = I1,\n                        interaction_branch2 = I2,\n                    )\n\n        dense_features = torch.rand((B, D))\n        sparse_features = torch.rand((B, F, D))\n\n        #  B X (D + F1 * F2)\n        concat_dense = inter_arch(dense_features, sparse_features)\n    \"\"\"\n\n    def __init__(\n        self,\n        num_sparse_features: int,\n        interaction_branch1: nn.Module,\n        interaction_branch2: nn.Module,\n    ) -> None:\n        super().__init__()\n        self.F: int = num_sparse_features\n        self.interaction_branch1 = interaction_branch1\n        self.interaction_branch2 = interaction_branch2\n\n    def forward(\n        self, dense_features: torch.Tensor, sparse_features: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): an input tensor of size B X D.\n            sparse_features (torch.Tensor): an input tensor of size B X F X D.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X (D + F1 * F2)) where\n            F1*D and F2*D are the output dimensions of the 2 interaction MLPs.\n        \"\"\"\n        if self.F <= 0:\n            return dense_features\n        (B, D) = dense_features.shape\n\n        combined_values = torch.cat(\n            (dense_features.unsqueeze(1), sparse_features), dim=1\n        )\n\n        interaction_branch1_out = self.interaction_branch1(\n            torch.reshape(combined_values, (B, -1))\n        )\n\n        interaction_branch2_out = self.interaction_branch2(\n            torch.reshape(combined_values, (B, -1))\n        )\n\n        interactions = torch.bmm(\n            interaction_branch1_out.reshape([B, -1, D]),\n            interaction_branch2_out.reshape([B, D, -1]),\n        )\n        interactions_flat = torch.reshape(interactions, (B, -1))\n\n        return torch.cat((dense_features, interactions_flat), dim=1)",
  "class OverArch(nn.Module):\n    \"\"\"\n    Final Arch of DLRM - simple MLP over OverArch.\n\n    Args:\n        in_features (int): size of the input.\n        layer_sizes (List[int]): sizes of the layers of the `OverArch`.\n        device (Optional[torch.device]): default compute device.\n\n    Example::\n\n        B = 20\n        D = 3\n        over_arch = OverArch(10, [5, 1])\n        logits = over_arch(torch.rand((B, 10)))\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        layer_sizes: List[int],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        if len(layer_sizes) <= 1:\n            raise ValueError(\"OverArch must have multiple layers.\")\n        self.model: nn.Module = nn.Sequential(\n            MLP(\n                in_features,\n                layer_sizes[:-1],\n                bias=True,\n                activation=\"relu\",\n                device=device,\n            ),\n            nn.Linear(layer_sizes[-2], layer_sizes[-1], bias=True, device=device),\n        )\n\n    def forward(self, features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (torch.Tensor):\n\n        Returns:\n            torch.Tensor: size B X layer_sizes[-1]\n        \"\"\"\n        return self.model(features)",
  "class DLRM(nn.Module):\n    \"\"\"\n    Recsys model from \"Deep Learning Recommendation Model for Personalization and\n    Recommendation Systems\" (https://arxiv.org/abs/1906.00091). Processes sparse\n    features by learning pooled embeddings for each feature. Learns the relationship\n    between dense features and sparse features by projecting dense features into the\n    same embedding space. Also, learns the pairwise relationships between sparse\n    features.\n\n    The module assumes all sparse features have the same embedding dimension\n    (i.e. each EmbeddingBagConfig uses the same embedding_dim).\n\n    The following notation is used throughout the documentation for the models:\n\n    * F: number of sparse features\n    * D: embedding_dimension of sparse features\n    * B: batch size\n    * num_features: number of dense features\n\n    Args:\n        embedding_bag_collection (EmbeddingBagCollection): collection of embedding bags\n            used to define `SparseArch`.\n        dense_in_features (int): the dimensionality of the dense input features.\n        dense_arch_layer_sizes (List[int]): the layer sizes for the `DenseArch`.\n        over_arch_layer_sizes (List[int]): the layer sizes for the `OverArch`.\n            The output dimension of the `InteractionArch` should not be manually\n            specified here.\n        dense_device (Optional[torch.device]): default compute device.\n\n    Example::\n\n        B = 2\n        D = 8\n\n        eb1_config = EmbeddingBagConfig(\n            name=\"t1\", embedding_dim=D, num_embeddings=100, feature_names=[\"f1\"]\n        )\n        eb2_config = EmbeddingBagConfig(\n            name=\"t2\",\n            embedding_dim=D,\n            num_embeddings=100,\n            feature_names=[\"f2\"],\n        )\n\n        ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])\n        model = DLRM(\n            embedding_bag_collection=ebc,\n            dense_in_features=100,\n            dense_arch_layer_sizes=[20, D],\n            over_arch_layer_sizes=[5, 1],\n        )\n\n        features = torch.rand((B, 100))\n\n        #     0       1\n        # 0   [1,2] [4,5]\n        # 1   [4,3] [2,9]\n        # ^\n        # feature\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f1\", \"f2\"],\n            values=torch.tensor([1, 2, 4, 5, 4, 3, 2, 9]),\n            offsets=torch.tensor([0, 2, 4, 6, 8]),\n        )\n\n        logits = model(\n            dense_features=features,\n            sparse_features=sparse_features,\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        dense_in_features: int,\n        dense_arch_layer_sizes: List[int],\n        over_arch_layer_sizes: List[int],\n        dense_device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        assert (\n            len(embedding_bag_collection.embedding_bag_configs()) > 0\n        ), \"At least one embedding bag is required\"\n        for i in range(1, len(embedding_bag_collection.embedding_bag_configs())):\n            conf_prev = embedding_bag_collection.embedding_bag_configs()[i - 1]\n            conf = embedding_bag_collection.embedding_bag_configs()[i]\n            assert (\n                conf_prev.embedding_dim == conf.embedding_dim\n            ), \"All EmbeddingBagConfigs must have the same dimension\"\n        embedding_dim: int = embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n        if dense_arch_layer_sizes[-1] != embedding_dim:\n            raise ValueError(\n                f\"embedding_bag_collection dimension ({embedding_dim}) and final dense \"\n                \"arch layer size ({dense_arch_layer_sizes[-1]}) must match.\"\n            )\n\n        self.sparse_arch: SparseArch = SparseArch(embedding_bag_collection)\n        num_sparse_features: int = len(self.sparse_arch.sparse_feature_names)\n\n        self.dense_arch = DenseArch(\n            in_features=dense_in_features,\n            layer_sizes=dense_arch_layer_sizes,\n            device=dense_device,\n        )\n\n        self.inter_arch = InteractionArch(\n            num_sparse_features=num_sparse_features,\n        )\n\n        over_in_features: int = (\n            embedding_dim + choose(num_sparse_features, 2) + num_sparse_features\n        )\n\n        self.over_arch = OverArch(\n            in_features=over_in_features,\n            layer_sizes=over_arch_layer_sizes,\n            device=dense_device,\n        )\n\n    def forward(\n        self,\n        dense_features: torch.Tensor,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): the dense features.\n            sparse_features (KeyedJaggedTensor): the sparse features.\n\n        Returns:\n            torch.Tensor: logits.\n        \"\"\"\n        embedded_dense = self.dense_arch(dense_features)\n        embedded_sparse = self.sparse_arch(sparse_features)\n        concatenated_dense = self.inter_arch(\n            dense_features=embedded_dense, sparse_features=embedded_sparse\n        )\n        logits = self.over_arch(concatenated_dense)\n        return logits",
  "class DLRM_Projection(DLRM):\n    \"\"\"\n    Recsys model modified from the original model from \"Deep Learning Recommendation\n    Model for Personalization and Recommendation Systems\"\n    (https://arxiv.org/abs/1906.00091). Similar to DLRM module but has\n    additional MLPs in the interaction layer (along 2 branches).\n\n    The module assumes all sparse features have the same embedding dimension\n    (i.e. each EmbeddingBagConfig uses the same embedding_dim).\n\n    The following notation is used throughout the documentation for the models:\n\n    * F: number of sparse features\n    * D: embedding_dimension of sparse features\n    * B: batch size\n    * num_features: number of dense features\n\n    Args:\n        embedding_bag_collection (EmbeddingBagCollection): collection of embedding bags\n            used to define `SparseArch`.\n        dense_in_features (int): the dimensionality of the dense input features.\n        dense_arch_layer_sizes (List[int]): the layer sizes for the `DenseArch`.\n        over_arch_layer_sizes (List[int]): the layer sizes for the `OverArch`.\n            The output dimension of the `InteractionArch` should not be manually\n            specified here.\n        interaction_branch1_layer_sizes (List[int]): the layer sizes for first branch of\n            interaction layer. The output dimension must be a multiple of D.\n        interaction_branch2_layer_sizes (List[int]):the layer sizes for second branch of\n            interaction layer. The output dimension must be a multiple of D.\n        dense_device (Optional[torch.device]): default compute device.\n\n    Example::\n\n        B = 2\n        D = 8\n\n        eb1_config = EmbeddingBagConfig(\n           name=\"t1\", embedding_dim=D, num_embeddings=100, feature_names=[\"f1\", \"f3\"]\n        )\n        eb2_config = EmbeddingBagConfig(\n           name=\"t2\",\n           embedding_dim=D,\n           num_embeddings=100,\n           feature_names=[\"f2\"],\n        )\n\n        ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])\n        model = DLRM_Projection(\n           embedding_bag_collection=ebc,\n           dense_in_features=100,\n           dense_arch_layer_sizes=[20, D],\n           interaction_branch1_layer_sizes=[3*D+D, 4*D],\n           interaction_branch2_layer_sizes=[3*D+D, 4*D],\n           over_arch_layer_sizes=[5, 1],\n        )\n\n        features = torch.rand((B, 100))\n\n        #     0       1\n        # 0   [1,2] [4,5]\n        # 1   [4,3] [2,9]\n        # ^\n        # feature\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n           keys=[\"f1\", \"f3\"],\n           values=torch.tensor([1, 2, 4, 5, 4, 3, 2, 9]),\n           offsets=torch.tensor([0, 2, 4, 6, 8]),\n        )\n\n        logits = model(\n           dense_features=features,\n           sparse_features=sparse_features,\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        dense_in_features: int,\n        dense_arch_layer_sizes: List[int],\n        over_arch_layer_sizes: List[int],\n        interaction_branch1_layer_sizes: List[int],\n        interaction_branch2_layer_sizes: List[int],\n        dense_device: Optional[torch.device] = None,\n    ) -> None:\n        # initialize DLRM\n        # sparse arch and dense arch are initialized via DLRM\n        super().__init__(\n            embedding_bag_collection,\n            dense_in_features,\n            dense_arch_layer_sizes,\n            over_arch_layer_sizes,\n            dense_device,\n        )\n\n        embedding_dim: int = embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n        num_sparse_features: int = len(self.sparse_arch.sparse_feature_names)\n\n        # Fix interaction and over arch for DLRM_Projeciton\n        if interaction_branch1_layer_sizes[-1] % embedding_dim != 0:\n            raise ValueError(\n                \"Final interaction branch1 layer size \"\n                \"({}) is not a multiple of embedding size ({})\".format(\n                    interaction_branch1_layer_sizes[-1], embedding_dim\n                )\n            )\n        projected_dim_1: int = interaction_branch1_layer_sizes[-1] // embedding_dim\n        interaction_branch1 = DenseArch(\n            in_features=num_sparse_features * embedding_dim\n            + dense_arch_layer_sizes[-1],\n            layer_sizes=interaction_branch1_layer_sizes,\n            device=dense_device,\n        )\n\n        if interaction_branch2_layer_sizes[-1] % embedding_dim != 0:\n            raise ValueError(\n                \"Final interaction branch2 layer size \"\n                \"({}) is not a multiple of embedding size ({})\".format(\n                    interaction_branch2_layer_sizes[-1], embedding_dim\n                )\n            )\n        projected_dim_2: int = interaction_branch2_layer_sizes[-1] // embedding_dim\n        interaction_branch2 = DenseArch(\n            in_features=num_sparse_features * embedding_dim\n            + dense_arch_layer_sizes[-1],\n            layer_sizes=interaction_branch2_layer_sizes,\n            device=dense_device,\n        )\n\n        self.inter_arch = InteractionProjectionArch(\n            num_sparse_features=num_sparse_features,\n            interaction_branch1=interaction_branch1,\n            interaction_branch2=interaction_branch2,\n        )\n\n        over_in_features: int = embedding_dim + projected_dim_1 * projected_dim_2\n\n        self.over_arch = OverArch(\n            in_features=over_in_features,\n            layer_sizes=over_arch_layer_sizes,\n            device=dense_device,\n        )",
  "class DLRM_DCN(DLRM):\n    \"\"\"\n    Recsys model with DCN modified from the original model from \"Deep Learning Recommendation\n    Model for Personalization and Recommendation Systems\"\n    (https://arxiv.org/abs/1906.00091). Similar to DLRM module but has\n    DeepCrossNet https://arxiv.org/pdf/2008.13535.pdf as the interaction layer.\n\n    The module assumes all sparse features have the same embedding dimension\n    (i.e. each EmbeddingBagConfig uses the same embedding_dim).\n\n    The following notation is used throughout the documentation for the models:\n\n    * F: number of sparse features\n    * D: embedding_dimension of sparse features\n    * B: batch size\n    * num_features: number of dense features\n\n    Args:\n        embedding_bag_collection (EmbeddingBagCollection): collection of embedding bags\n            used to define `SparseArch`.\n        dense_in_features (int): the dimensionality of the dense input features.\n        dense_arch_layer_sizes (List[int]): the layer sizes for the `DenseArch`.\n        over_arch_layer_sizes (List[int]): the layer sizes for the `OverArch`.\n            The output dimension of the `InteractionArch` should not be manually\n            specified here.\n        dcn_num_layers (int): the number of DCN layers in the interaction.\n        dcn_low_rank_dim (int): the dimensionality of low rank approximation\n            used in the dcn layers.\n        dense_device (Optional[torch.device]): default compute device.\n\n    Example::\n\n        B = 2\n        D = 8\n\n        eb1_config = EmbeddingBagConfig(\n           name=\"t1\", embedding_dim=D, num_embeddings=100, feature_names=[\"f1\", \"f3\"]\n        )\n        eb2_config = EmbeddingBagConfig(\n           name=\"t2\",\n           embedding_dim=D,\n           num_embeddings=100,\n           feature_names=[\"f2\"],\n        )\n\n        ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])\n        model = DLRM_DCN(\n           embedding_bag_collection=ebc,\n           dense_in_features=100,\n           dense_arch_layer_sizes=[20, D],\n           dcn_num_layers=2,\n           dcn_low_rank_dim=8,\n           over_arch_layer_sizes=[5, 1],\n        )\n\n        features = torch.rand((B, 100))\n\n        #     0       1\n        # 0   [1,2] [4,5]\n        # 1   [4,3] [2,9]\n        # ^\n        # feature\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n           keys=[\"f1\", \"f3\"],\n           values=torch.tensor([1, 2, 4, 5, 4, 3, 2, 9]),\n           offsets=torch.tensor([0, 2, 4, 6, 8]),\n        )\n\n        logits = model(\n           dense_features=features,\n           sparse_features=sparse_features,\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        dense_in_features: int,\n        dense_arch_layer_sizes: List[int],\n        over_arch_layer_sizes: List[int],\n        dcn_num_layers: int,\n        dcn_low_rank_dim: int,\n        dense_device: Optional[torch.device] = None,\n    ) -> None:\n        # initialize DLRM\n        # sparse arch and dense arch are initialized via DLRM\n        super().__init__(\n            embedding_bag_collection,\n            dense_in_features,\n            dense_arch_layer_sizes,\n            over_arch_layer_sizes,\n            dense_device,\n        )\n\n        embedding_dim: int = embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n        num_sparse_features: int = len(self.sparse_arch.sparse_feature_names)\n\n        # Fix interaction and over arch for DLRM_DCN\n\n        crossnet = LowRankCrossNet(\n            in_features=(num_sparse_features + 1) * embedding_dim,\n            num_layers=dcn_num_layers,\n            low_rank=dcn_low_rank_dim,\n        )\n\n        self.inter_arch = InteractionDCNArch(\n            num_sparse_features=num_sparse_features,\n            crossnet=crossnet,\n        )\n\n        over_in_features: int = (num_sparse_features + 1) * embedding_dim\n\n        self.over_arch = OverArch(\n            in_features=over_in_features,\n            layer_sizes=over_arch_layer_sizes,\n            device=dense_device,\n        )",
  "class DLRMTrain(nn.Module):\n    \"\"\"\n    nn.Module to wrap DLRM model to use with train_pipeline.\n\n    DLRM Recsys model from \"Deep Learning Recommendation Model for Personalization and\n    Recommendation Systems\" (https://arxiv.org/abs/1906.00091). Processes sparse\n    features by learning pooled embeddings for each feature. Learns the relationship\n    between dense features and sparse features by projecting dense features into the\n    same embedding space. Also, learns the pairwise relationships between sparse\n    features.\n\n    The module assumes all sparse features have the same embedding dimension\n    (i.e, each EmbeddingBagConfig uses the same embedding_dim)\n\n    Args:\n        dlrm_module: DLRM module (DLRM or DLRM_Projection or DLRM_DCN) to be used in\n        training\n\n    Example::\n\n        ebc = EmbeddingBagCollection(config=ebc_config)\n        dlrm_module = DLRM(\n           embedding_bag_collection=ebc,\n           dense_in_features=100,\n           dense_arch_layer_sizes=[20],\n           over_arch_layer_sizes=[5, 1],\n        )\n        dlrm_model = DLRMTrain(dlrm_module)\n    \"\"\"\n\n    def __init__(\n        self,\n        dlrm_module: DLRM,\n    ) -> None:\n        super().__init__()\n        self.model = dlrm_module\n        self.loss_fn: nn.Module = nn.BCEWithLogitsLoss()\n\n    def forward(\n        self, batch: Batch\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:\n        \"\"\"\n        Args:\n            batch: batch used with criteo and random data from torchrec.datasets\n        Returns:\n            Tuple[loss, Tuple[loss, logits, labels]]\n        \"\"\"\n        logits = self.model(batch.dense_features, batch.sparse_features)\n        logits = logits.squeeze(-1)\n        loss = self.loss_fn(logits, batch.labels.float())\n\n        return loss, (loss.detach(), logits.detach(), batch.labels.detach())",
  "def __init__(self, embedding_bag_collection: EmbeddingBagCollection) -> None:\n        super().__init__()\n        self.embedding_bag_collection: EmbeddingBagCollection = embedding_bag_collection\n        assert (\n            self.embedding_bag_collection.embedding_bag_configs\n        ), \"Embedding bag collection cannot be empty!\"\n        self.D: int = self.embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n        self._sparse_feature_names: List[str] = [\n            name\n            for conf in embedding_bag_collection.embedding_bag_configs()\n            for name in conf.feature_names\n        ]\n\n        self.F: int = len(self._sparse_feature_names)",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor): an input tensor of sparse features.\n\n        Returns:\n            torch.Tensor: tensor of shape B X F X D.\n        \"\"\"\n\n        sparse_features: KeyedTensor = self.embedding_bag_collection(features)\n\n        sparse: Dict[str, torch.Tensor] = sparse_features.to_dict()\n        sparse_values: List[torch.Tensor] = []\n        for name in self.sparse_feature_names:\n            sparse_values.append(sparse[name])\n\n        return torch.cat(sparse_values, dim=1).reshape(-1, self.F, self.D)",
  "def sparse_feature_names(self) -> List[str]:\n        return self._sparse_feature_names",
  "def __init__(\n        self,\n        in_features: int,\n        layer_sizes: List[int],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self.model: nn.Module = MLP(\n            in_features, layer_sizes, bias=True, activation=\"relu\", device=device\n        )",
  "def forward(self, features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (torch.Tensor): an input tensor of dense features.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X D.\n        \"\"\"\n        return self.model(features)",
  "def __init__(self, num_sparse_features: int) -> None:\n        super().__init__()\n        self.F: int = num_sparse_features\n        self.triu_indices: torch.Tensor = torch.triu_indices(\n            self.F + 1, self.F + 1, offset=1\n        )",
  "def forward(\n        self, dense_features: torch.Tensor, sparse_features: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): an input tensor of size B X D.\n            sparse_features (torch.Tensor): an input tensor of size B X F X D.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X (D + F + F choose 2).\n        \"\"\"\n        if self.F <= 0:\n            return dense_features\n        (B, D) = dense_features.shape\n\n        combined_values = torch.cat(\n            (dense_features.unsqueeze(1), sparse_features), dim=1\n        )\n\n        # dense/sparse + sparse/sparse interaction\n        # size B X (F + F choose 2)\n        interactions = torch.bmm(\n            combined_values, torch.transpose(combined_values, 1, 2)\n        )\n        interactions_flat = interactions[:, self.triu_indices[0], self.triu_indices[1]]\n\n        return torch.cat((dense_features, interactions_flat), dim=1)",
  "def __init__(self, num_sparse_features: int, crossnet: nn.Module) -> None:\n        super().__init__()\n        self.F: int = num_sparse_features\n        self.crossnet = crossnet",
  "def forward(\n        self, dense_features: torch.Tensor, sparse_features: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): an input tensor of size B X D.\n            sparse_features (torch.Tensor): an input tensor of size B X F X D.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X (F*D + D).\n        \"\"\"\n        if self.F <= 0:\n            return dense_features\n        (B, D) = dense_features.shape\n\n        combined_values = torch.cat(\n            (dense_features.unsqueeze(1), sparse_features), dim=1\n        )\n\n        # size B X (F*D + D)\n        return self.crossnet(combined_values.reshape([B, -1]))",
  "def __init__(\n        self,\n        num_sparse_features: int,\n        interaction_branch1: nn.Module,\n        interaction_branch2: nn.Module,\n    ) -> None:\n        super().__init__()\n        self.F: int = num_sparse_features\n        self.interaction_branch1 = interaction_branch1\n        self.interaction_branch2 = interaction_branch2",
  "def forward(\n        self, dense_features: torch.Tensor, sparse_features: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): an input tensor of size B X D.\n            sparse_features (torch.Tensor): an input tensor of size B X F X D.\n\n        Returns:\n            torch.Tensor: an output tensor of size B X (D + F1 * F2)) where\n            F1*D and F2*D are the output dimensions of the 2 interaction MLPs.\n        \"\"\"\n        if self.F <= 0:\n            return dense_features\n        (B, D) = dense_features.shape\n\n        combined_values = torch.cat(\n            (dense_features.unsqueeze(1), sparse_features), dim=1\n        )\n\n        interaction_branch1_out = self.interaction_branch1(\n            torch.reshape(combined_values, (B, -1))\n        )\n\n        interaction_branch2_out = self.interaction_branch2(\n            torch.reshape(combined_values, (B, -1))\n        )\n\n        interactions = torch.bmm(\n            interaction_branch1_out.reshape([B, -1, D]),\n            interaction_branch2_out.reshape([B, D, -1]),\n        )\n        interactions_flat = torch.reshape(interactions, (B, -1))\n\n        return torch.cat((dense_features, interactions_flat), dim=1)",
  "def __init__(\n        self,\n        in_features: int,\n        layer_sizes: List[int],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        if len(layer_sizes) <= 1:\n            raise ValueError(\"OverArch must have multiple layers.\")\n        self.model: nn.Module = nn.Sequential(\n            MLP(\n                in_features,\n                layer_sizes[:-1],\n                bias=True,\n                activation=\"relu\",\n                device=device,\n            ),\n            nn.Linear(layer_sizes[-2], layer_sizes[-1], bias=True, device=device),\n        )",
  "def forward(self, features: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (torch.Tensor):\n\n        Returns:\n            torch.Tensor: size B X layer_sizes[-1]\n        \"\"\"\n        return self.model(features)",
  "def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        dense_in_features: int,\n        dense_arch_layer_sizes: List[int],\n        over_arch_layer_sizes: List[int],\n        dense_device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        assert (\n            len(embedding_bag_collection.embedding_bag_configs()) > 0\n        ), \"At least one embedding bag is required\"\n        for i in range(1, len(embedding_bag_collection.embedding_bag_configs())):\n            conf_prev = embedding_bag_collection.embedding_bag_configs()[i - 1]\n            conf = embedding_bag_collection.embedding_bag_configs()[i]\n            assert (\n                conf_prev.embedding_dim == conf.embedding_dim\n            ), \"All EmbeddingBagConfigs must have the same dimension\"\n        embedding_dim: int = embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n        if dense_arch_layer_sizes[-1] != embedding_dim:\n            raise ValueError(\n                f\"embedding_bag_collection dimension ({embedding_dim}) and final dense \"\n                \"arch layer size ({dense_arch_layer_sizes[-1]}) must match.\"\n            )\n\n        self.sparse_arch: SparseArch = SparseArch(embedding_bag_collection)\n        num_sparse_features: int = len(self.sparse_arch.sparse_feature_names)\n\n        self.dense_arch = DenseArch(\n            in_features=dense_in_features,\n            layer_sizes=dense_arch_layer_sizes,\n            device=dense_device,\n        )\n\n        self.inter_arch = InteractionArch(\n            num_sparse_features=num_sparse_features,\n        )\n\n        over_in_features: int = (\n            embedding_dim + choose(num_sparse_features, 2) + num_sparse_features\n        )\n\n        self.over_arch = OverArch(\n            in_features=over_in_features,\n            layer_sizes=over_arch_layer_sizes,\n            device=dense_device,\n        )",
  "def forward(\n        self,\n        dense_features: torch.Tensor,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): the dense features.\n            sparse_features (KeyedJaggedTensor): the sparse features.\n\n        Returns:\n            torch.Tensor: logits.\n        \"\"\"\n        embedded_dense = self.dense_arch(dense_features)\n        embedded_sparse = self.sparse_arch(sparse_features)\n        concatenated_dense = self.inter_arch(\n            dense_features=embedded_dense, sparse_features=embedded_sparse\n        )\n        logits = self.over_arch(concatenated_dense)\n        return logits",
  "def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        dense_in_features: int,\n        dense_arch_layer_sizes: List[int],\n        over_arch_layer_sizes: List[int],\n        interaction_branch1_layer_sizes: List[int],\n        interaction_branch2_layer_sizes: List[int],\n        dense_device: Optional[torch.device] = None,\n    ) -> None:\n        # initialize DLRM\n        # sparse arch and dense arch are initialized via DLRM\n        super().__init__(\n            embedding_bag_collection,\n            dense_in_features,\n            dense_arch_layer_sizes,\n            over_arch_layer_sizes,\n            dense_device,\n        )\n\n        embedding_dim: int = embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n        num_sparse_features: int = len(self.sparse_arch.sparse_feature_names)\n\n        # Fix interaction and over arch for DLRM_Projeciton\n        if interaction_branch1_layer_sizes[-1] % embedding_dim != 0:\n            raise ValueError(\n                \"Final interaction branch1 layer size \"\n                \"({}) is not a multiple of embedding size ({})\".format(\n                    interaction_branch1_layer_sizes[-1], embedding_dim\n                )\n            )\n        projected_dim_1: int = interaction_branch1_layer_sizes[-1] // embedding_dim\n        interaction_branch1 = DenseArch(\n            in_features=num_sparse_features * embedding_dim\n            + dense_arch_layer_sizes[-1],\n            layer_sizes=interaction_branch1_layer_sizes,\n            device=dense_device,\n        )\n\n        if interaction_branch2_layer_sizes[-1] % embedding_dim != 0:\n            raise ValueError(\n                \"Final interaction branch2 layer size \"\n                \"({}) is not a multiple of embedding size ({})\".format(\n                    interaction_branch2_layer_sizes[-1], embedding_dim\n                )\n            )\n        projected_dim_2: int = interaction_branch2_layer_sizes[-1] // embedding_dim\n        interaction_branch2 = DenseArch(\n            in_features=num_sparse_features * embedding_dim\n            + dense_arch_layer_sizes[-1],\n            layer_sizes=interaction_branch2_layer_sizes,\n            device=dense_device,\n        )\n\n        self.inter_arch = InteractionProjectionArch(\n            num_sparse_features=num_sparse_features,\n            interaction_branch1=interaction_branch1,\n            interaction_branch2=interaction_branch2,\n        )\n\n        over_in_features: int = embedding_dim + projected_dim_1 * projected_dim_2\n\n        self.over_arch = OverArch(\n            in_features=over_in_features,\n            layer_sizes=over_arch_layer_sizes,\n            device=dense_device,\n        )",
  "def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        dense_in_features: int,\n        dense_arch_layer_sizes: List[int],\n        over_arch_layer_sizes: List[int],\n        dcn_num_layers: int,\n        dcn_low_rank_dim: int,\n        dense_device: Optional[torch.device] = None,\n    ) -> None:\n        # initialize DLRM\n        # sparse arch and dense arch are initialized via DLRM\n        super().__init__(\n            embedding_bag_collection,\n            dense_in_features,\n            dense_arch_layer_sizes,\n            over_arch_layer_sizes,\n            dense_device,\n        )\n\n        embedding_dim: int = embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n        num_sparse_features: int = len(self.sparse_arch.sparse_feature_names)\n\n        # Fix interaction and over arch for DLRM_DCN\n\n        crossnet = LowRankCrossNet(\n            in_features=(num_sparse_features + 1) * embedding_dim,\n            num_layers=dcn_num_layers,\n            low_rank=dcn_low_rank_dim,\n        )\n\n        self.inter_arch = InteractionDCNArch(\n            num_sparse_features=num_sparse_features,\n            crossnet=crossnet,\n        )\n\n        over_in_features: int = (num_sparse_features + 1) * embedding_dim\n\n        self.over_arch = OverArch(\n            in_features=over_in_features,\n            layer_sizes=over_arch_layer_sizes,\n            device=dense_device,\n        )",
  "def __init__(\n        self,\n        dlrm_module: DLRM,\n    ) -> None:\n        super().__init__()\n        self.model = dlrm_module\n        self.loss_fn: nn.Module = nn.BCEWithLogitsLoss()",
  "def forward(\n        self, batch: Batch\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:\n        \"\"\"\n        Args:\n            batch: batch used with criteo and random data from torchrec.datasets\n        Returns:\n            Tuple[loss, Tuple[loss, logits, labels]]\n        \"\"\"\n        logits = self.model(batch.dense_features, batch.sparse_features)\n        logits = logits.squeeze(-1)\n        loss = self.loss_fn(logits, batch.labels.float())\n\n        return loss, (loss.detach(), logits.detach(), batch.labels.detach())",
  "class InteractionTransformerArch(nn.Module):\n    \"\"\"\n    Processes the output of both `SparseArch` (sparse_features) and `DenseArch`\n    (dense_features). Returns the output of the nn.transformerencoder,\n    that takes the combined values of both sparse features and the output of the dense layer,\n    and the dense layer itself (i.e. concat(dense layer output, transformer encoder output).\n    Note: This model is for benchmarking purposes only, i.e. to measure the performance of transformer + embeddings using the dlrm models.\n    It is not intended to increase model convergence metrics.\n    Implemented TE as described here:\n    https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html?highlight=transformer+encoder#torch.nn.TransformerEncoder\n    BERT Transformer Paper: https://arxiv.org/abs/1810.04805\n    Attention is All you Need: https://arxiv.org/abs/1706.03762\n\n\n    .. note::\n        The dimensionality of the `dense_features` (D) is expected to match the\n        dimensionality of the `sparse_features` so that the dot products between them\n        can be computed.\n    Args:\n        num_sparse_features (int): F.\n        embedding_dim: int,\n        nhead: int, #number of attention heads\n        ntransformer_layers: int, #number of transformer layers.\n    Example::\n        D = 8   #must divisible by number of transformer heads\n        B = 10\n        keys = [\"f1\", \"f2\"]\n        F = len(keys)\n        inter_arch = InteractionTransormerArch(num_sparse_features=len(keys))\n        dense_features = torch.rand((B, D))\n        sparse_features = torch.rand((B, F, D))\n        #  B X (D * (F + 1))\n        concat_dense = inter_arch(dense_features, sparse_features)\n    \"\"\"\n\n    def __init__(\n        self,\n        num_sparse_features: int,\n        embedding_dim: int,\n        nhead: int = 8,\n        ntransformer_layers: int = 4,\n    ) -> None:\n        super().__init__()\n        self.F: int = num_sparse_features\n        self.nhead = nhead\n        self.ntransformer_layers = ntransformer_layers\n        transformer_encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embedding_dim,\n            nhead=self.nhead,\n        )\n        self.interarch_TE = nn.TransformerEncoder(\n            transformer_encoder_layer, num_layers=self.ntransformer_layers\n        )\n\n    def forward(\n        self, dense_features: torch.Tensor, sparse_features: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): an input tensor of size B X D.\n            sparse_features (torch.Tensor): an input tensor of size B X F X D.\n        Returns:\n            torch.Tensor: an output tensor of size B X (D + F + F choose 2).\n        \"\"\"\n        if self.F <= 0:\n            return dense_features\n        (B, D) = dense_features.shape\n        combined_values = torch.cat(\n            (dense_features.unsqueeze(1), sparse_features), dim=1\n        )\n        # Transformer for Interactions\n        transformer_interactions = self.interarch_TE(combined_values)\n        interactions_flat = torch.reshape(transformer_interactions, (B, -1))\n        return interactions_flat",
  "class DLRM_Transformer(DLRM):\n    \"\"\"\n    Recsys model from \"Deep Learning Recommendation Model for Personalization and\n    Recommendation Systems\" (https://arxiv.org/abs/1906.00091). Processes sparse\n    features by learning pooled embeddings for each feature. On the interaction layer,\n    the relationship between dense features and sparse features is learned through a transformer encoder layer\n    https://arxiv.org/abs/1706.03762.\n    The module assumes all sparse features have the same embedding dimension\n    (i.e. each EmbeddingBagConfig uses the same embedding_dim).\n    The following notation is used throughout the documentation for the models:\n    * F: number of sparse features\n    * D: embedding_dimension of sparse features\n    * B: batch size\n    * num_features: number of dense features\n    Args:\n        embedding_bag_collection (EmbeddingBagCollection): collection of embedding bags\n            used to define `SparseArch`.\n        dense_in_features (int): the dimensionality of the dense input features.\n        dense_arch_layer_sizes (List[int]): the layer sizes for the `DenseArch`.\n        over_arch_layer_sizes (List[int]): the layer sizes for the `OverArch`.\n            The output dimension of the `InteractionArch` should not be manually\n            specified here.\n        nhead: int: Number of multi-attention heads\n        ntransformer_layers: int: Number of transformer encoder layers\n        dense_device (Optional[torch.device]): default compute device.\n    Example::\n        B = 2\n        D = 8\n        eb1_config = EmbeddingBagConfig(\n           name=\"t1\", embedding_dim=D, num_embeddings=100, feature_names=[\"f1\", \"f3\"]\n        )\n        eb2_config = EmbeddingBagConfig(\n           name=\"t2\",\n           embedding_dim=D,\n           num_embeddings=100,\n           feature_names=[\"f2\"],\n        )\n        ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])\n        model = DLRM_Transformer(\n           embedding_bag_collection=ebc,\n           dense_in_features=100,\n           dense_arch_layer_sizes=[20],\n           over_arch_layer_sizes=[5, 1],\n        )\n        features = torch.rand((B, 100))\n        #     0       1\n        # 0   [1,2] [4,5]\n        # 1   [4,3] [2,9]\n        # ^\n        # feature\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n           keys=[\"f1\", \"f3\"],\n           values=torch.tensor([1, 2, 4, 5, 4, 3, 2, 9]),\n           offsets=torch.tensor([0, 2, 4, 6, 8]),\n        )\n        logits = model(\n           dense_features=features,\n           sparse_features=sparse_features,\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        dense_in_features: int,\n        dense_arch_layer_sizes: List[int],\n        over_arch_layer_sizes: List[int],\n        nhead: int = 8,\n        ntransformer_layers: int = 4,\n        dense_device: Optional[torch.device] = None,\n    ) -> None:\n        # initialize DLRM\n        # sparse arch and dense arch are initialized via DLRM\n        super().__init__(\n            embedding_bag_collection,\n            dense_in_features,\n            dense_arch_layer_sizes,\n            over_arch_layer_sizes,\n            dense_device,\n        )\n        embedding_dim: int = embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n        num_sparse_features: int = len(self.sparse_arch.sparse_feature_names)\n        self.inter_arch = InteractionTransformerArch(\n            num_sparse_features=num_sparse_features,\n            embedding_dim=embedding_dim,\n            nhead=nhead,\n            ntransformer_layers=ntransformer_layers,\n        )\n        over_in_features: int = (num_sparse_features + 1) * embedding_dim\n        self.over_arch = OverArch(\n            in_features=over_in_features,\n            layer_sizes=over_arch_layer_sizes,\n            device=dense_device,\n        )",
  "def __init__(\n        self,\n        num_sparse_features: int,\n        embedding_dim: int,\n        nhead: int = 8,\n        ntransformer_layers: int = 4,\n    ) -> None:\n        super().__init__()\n        self.F: int = num_sparse_features\n        self.nhead = nhead\n        self.ntransformer_layers = ntransformer_layers\n        transformer_encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embedding_dim,\n            nhead=self.nhead,\n        )\n        self.interarch_TE = nn.TransformerEncoder(\n            transformer_encoder_layer, num_layers=self.ntransformer_layers\n        )",
  "def forward(\n        self, dense_features: torch.Tensor, sparse_features: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            dense_features (torch.Tensor): an input tensor of size B X D.\n            sparse_features (torch.Tensor): an input tensor of size B X F X D.\n        Returns:\n            torch.Tensor: an output tensor of size B X (D + F + F choose 2).\n        \"\"\"\n        if self.F <= 0:\n            return dense_features\n        (B, D) = dense_features.shape\n        combined_values = torch.cat(\n            (dense_features.unsqueeze(1), sparse_features), dim=1\n        )\n        # Transformer for Interactions\n        transformer_interactions = self.interarch_TE(combined_values)\n        interactions_flat = torch.reshape(transformer_interactions, (B, -1))\n        return interactions_flat",
  "def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        dense_in_features: int,\n        dense_arch_layer_sizes: List[int],\n        over_arch_layer_sizes: List[int],\n        nhead: int = 8,\n        ntransformer_layers: int = 4,\n        dense_device: Optional[torch.device] = None,\n    ) -> None:\n        # initialize DLRM\n        # sparse arch and dense arch are initialized via DLRM\n        super().__init__(\n            embedding_bag_collection,\n            dense_in_features,\n            dense_arch_layer_sizes,\n            over_arch_layer_sizes,\n            dense_device,\n        )\n        embedding_dim: int = embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n        num_sparse_features: int = len(self.sparse_arch.sparse_feature_names)\n        self.inter_arch = InteractionTransformerArch(\n            num_sparse_features=num_sparse_features,\n            embedding_dim=embedding_dim,\n            nhead=nhead,\n            ntransformer_layers=ntransformer_layers,\n        )\n        over_in_features: int = (num_sparse_features + 1) * embedding_dim\n        self.over_arch = OverArch(\n            in_features=over_in_features,\n            layer_sizes=over_arch_layer_sizes,\n            device=dense_device,\n        )",
  "class InteractionArchTransformerTest(unittest.TestCase):\n    def test_basic(self) -> None:\n        D = 8\n        B = 10\n        # multi-head attentions\n        nhead = 8\n        ntransformer_layers = 4\n        keys = [\"f1\", \"f2\"]\n        F = len(keys)\n        inter_arch = InteractionTransformerArch(\n            num_sparse_features=F,\n            embedding_dim=D,\n            nhead=nhead,\n            ntransformer_layers=ntransformer_layers,\n        )\n        dense_features = torch.rand((B, D))\n        sparse_features = torch.rand((B, F, D))\n        concat_dense = inter_arch(dense_features, sparse_features)\n        #  B X (D + F + F choose 2)\n        self.assertEqual(concat_dense.size(), (B, D * (F + 1)))\n\n    def test_larger(self) -> None:\n        D = 16\n        B = 20\n        # multi-head attentions\n        nhead = 8\n        ntransformer_layers = 4\n        keys = [\"f1\", \"f2\", \"f3\", \"f4\"]\n        F = len(keys)\n        inter_arch = InteractionTransformerArch(\n            num_sparse_features=F,\n            embedding_dim=D,\n            nhead=nhead,\n            ntransformer_layers=ntransformer_layers,\n        )\n        dense_features = torch.rand((B, D))\n        sparse_features = torch.rand((B, F, D))\n        concat_dense = inter_arch(dense_features, sparse_features)\n        self.assertEqual(concat_dense.size(), (B, D * (F + 1)))\n\n    def test_correctness(self) -> None:\n        D = 4\n        B = 3\n        # multi-head attentions\n        nhead = 4\n        ntransformer_layers = 4\n        keys = [\n            \"f1\",\n            \"f2\",\n            \"f3\",\n            \"f4\",\n        ]\n        F = len(keys)\n        # place the manual_seed before the InteractionTransformerArch object to generate the same initialization random values in the Transformer\n        torch.manual_seed(0)\n        inter_arch = InteractionTransformerArch(\n            num_sparse_features=F,\n            embedding_dim=D,\n            nhead=nhead,\n            ntransformer_layers=ntransformer_layers,\n        )\n        dense_features = torch.rand((B, D))\n        sparse_features = torch.rand((B, F, D))\n        concat_dense = inter_arch(dense_features, sparse_features)\n        self.assertEqual(concat_dense.size(), (B, D * (F + 1)))\n        expected = torch.tensor(\n            [\n                [\n                    -0.4411,\n                    0.2487,\n                    -1.2685,\n                    1.4610,\n                    1.3110,\n                    0.5152,\n                    -0.4960,\n                    -1.3303,\n                    -0.3962,\n                    -0.0623,\n                    -1.1371,\n                    1.5956,\n                    0.2431,\n                    -1.6820,\n                    0.5242,\n                    0.9148,\n                    1.3033,\n                    0.6409,\n                    -0.9577,\n                    -0.9866,\n                ],\n                [\n                    -1.0850,\n                    -0.0366,\n                    -0.4862,\n                    1.6078,\n                    1.1254,\n                    -0.9989,\n                    -0.9927,\n                    0.8661,\n                    -0.1704,\n                    1.0223,\n                    -1.5580,\n                    0.7060,\n                    -0.3081,\n                    -1.3686,\n                    0.2788,\n                    1.3979,\n                    0.0328,\n                    1.5470,\n                    -0.3670,\n                    -1.2128,\n                ],\n                [\n                    -1.5917,\n                    -0.0995,\n                    0.7302,\n                    0.9609,\n                    0.6606,\n                    1.0238,\n                    -0.1017,\n                    -1.5827,\n                    -0.6761,\n                    -1.0771,\n                    0.2262,\n                    1.5269,\n                    -0.5671,\n                    -1.2114,\n                    1.4503,\n                    0.3281,\n                    -0.6540,\n                    -1.2925,\n                    0.9134,\n                    1.0331,\n                ],\n            ]\n        )\n        self.assertTrue(\n            torch.allclose(\n                concat_dense,\n                expected,\n                rtol=1e-4,\n                atol=1e-4,\n            )\n        )\n\n    def test_numerical_stability(self) -> None:\n        D = 4\n        B = 3\n        # multi-head attentions\n        nhead = 4\n        ntransformer_layers = 4\n        keys = [\"f1\", \"f2\"]\n        F = len(keys)\n        torch.manual_seed(0)\n        inter_arch = InteractionTransformerArch(\n            num_sparse_features=F,\n            embedding_dim=D,\n            nhead=nhead,\n            ntransformer_layers=ntransformer_layers,\n        )\n        dense_features = 10 * torch.rand(B, D)\n        sparse_features = 10 * torch.rand(B, F, D)\n        concat_dense = inter_arch(dense_features, sparse_features)\n        expected = torch.LongTensor(\n            [\n                [0, 1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 1],\n                [0, 0, 0, 1, -1, 0, 1, 0, 0, 0, 1, 0],\n                [-1, 0, 0, 0, 0, 0, -1, 1, 0, 1, -1, 0],\n            ]\n        )\n        self.assertTrue(torch.equal(concat_dense.long(), expected))",
  "class DLRMTransformerTest(unittest.TestCase):\n    def test_basic(self) -> None:\n        torch.manual_seed(0)\n        B = 2\n        D = 8\n        dense_in_features = 100\n        eb1_config = EmbeddingBagConfig(\n            name=\"t1\", embedding_dim=D, num_embeddings=100, feature_names=[\"f1\", \"f3\"]\n        )\n        eb2_config = EmbeddingBagConfig(\n            name=\"t2\",\n            embedding_dim=D,\n            num_embeddings=100,\n            feature_names=[\"f2\"],\n        )\n        ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])\n        sparse_nn = DLRM_Transformer(\n            embedding_bag_collection=ebc,\n            dense_in_features=dense_in_features,\n            dense_arch_layer_sizes=[20, D],\n            over_arch_layer_sizes=[5, 1],\n        )\n        features = torch.rand((B, dense_in_features))\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f1\", \"f3\", \"f2\"],\n            values=torch.tensor([1, 2, 4, 5, 4, 3, 2, 9, 1, 2, 3]),\n            offsets=torch.tensor([0, 2, 4, 6, 8, 10, 11]),\n        )\n        logits = sparse_nn(\n            dense_features=features,\n            sparse_features=sparse_features,\n        )\n        self.assertEqual(logits.size(), (B, 1))\n        expected_logits = torch.tensor([[-0.2593], [-0.1043]])\n        self.assertTrue(\n            torch.allclose(\n                logits,\n                expected_logits,\n                rtol=1e-4,\n                atol=1e-4,\n            )\n        )\n\n    def test_one_sparse(self) -> None:\n        B = 2\n        D = 8\n        dense_in_features = 100\n        eb1_config = EmbeddingBagConfig(\n            name=\"t2\",\n            embedding_dim=D,\n            num_embeddings=100,\n            feature_names=[\"f2\"],\n        )\n        ebc = EmbeddingBagCollection(tables=[eb1_config])\n        sparse_nn = DLRM_Transformer(\n            embedding_bag_collection=ebc,\n            dense_in_features=dense_in_features,\n            dense_arch_layer_sizes=[20, D],\n            over_arch_layer_sizes=[5, 1],\n        )\n        features = torch.rand((B, dense_in_features))\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f2\"],\n            values=torch.tensor(range(3)),\n            offsets=torch.tensor([0, 2, 3]),\n        )\n        logits = sparse_nn(\n            dense_features=features,\n            sparse_features=sparse_features,\n        )\n        self.assertEqual(logits.size(), (B, 1))\n\n    def test_no_sparse(self) -> None:\n        ebc = EmbeddingBagCollection(tables=[])\n        D_unused = 1\n        with self.assertRaises(AssertionError):\n            DLRM_Transformer(\n                embedding_bag_collection=ebc,\n                dense_in_features=100,\n                dense_arch_layer_sizes=[20, D_unused],\n                over_arch_layer_sizes=[5, 1],\n            )",
  "class DLRMTransformerTrainTest(unittest.TestCase):\n    def test_basic(self) -> None:\n        B = 2\n        D = 8\n        dense_in_features = 100\n        eb1_config = EmbeddingBagConfig(\n            name=\"t2\",\n            embedding_dim=D,\n            num_embeddings=100,\n            feature_names=[\"f2\"],\n        )\n        ebc = EmbeddingBagCollection(tables=[eb1_config])\n        dlrm_module = DLRM_Transformer(\n            embedding_bag_collection=ebc,\n            dense_in_features=dense_in_features,\n            dense_arch_layer_sizes=[20, D],\n            over_arch_layer_sizes=[5, 1],\n        )\n        dlrm = DLRMTrain(dlrm_module)\n        features = torch.rand((B, dense_in_features))\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f2\"],\n            values=torch.tensor(range(3)),\n            offsets=torch.tensor([0, 2, 3]),\n        )\n        batch = Batch(\n            dense_features=features,\n            sparse_features=sparse_features,\n            labels=torch.randint(2, (B,)),\n        )\n        _, (_, logits, _) = dlrm(batch)\n        self.assertEqual(logits.size(), (B,))",
  "def test_basic(self) -> None:\n        D = 8\n        B = 10\n        # multi-head attentions\n        nhead = 8\n        ntransformer_layers = 4\n        keys = [\"f1\", \"f2\"]\n        F = len(keys)\n        inter_arch = InteractionTransformerArch(\n            num_sparse_features=F,\n            embedding_dim=D,\n            nhead=nhead,\n            ntransformer_layers=ntransformer_layers,\n        )\n        dense_features = torch.rand((B, D))\n        sparse_features = torch.rand((B, F, D))\n        concat_dense = inter_arch(dense_features, sparse_features)\n        #  B X (D + F + F choose 2)\n        self.assertEqual(concat_dense.size(), (B, D * (F + 1)))",
  "def test_larger(self) -> None:\n        D = 16\n        B = 20\n        # multi-head attentions\n        nhead = 8\n        ntransformer_layers = 4\n        keys = [\"f1\", \"f2\", \"f3\", \"f4\"]\n        F = len(keys)\n        inter_arch = InteractionTransformerArch(\n            num_sparse_features=F,\n            embedding_dim=D,\n            nhead=nhead,\n            ntransformer_layers=ntransformer_layers,\n        )\n        dense_features = torch.rand((B, D))\n        sparse_features = torch.rand((B, F, D))\n        concat_dense = inter_arch(dense_features, sparse_features)\n        self.assertEqual(concat_dense.size(), (B, D * (F + 1)))",
  "def test_correctness(self) -> None:\n        D = 4\n        B = 3\n        # multi-head attentions\n        nhead = 4\n        ntransformer_layers = 4\n        keys = [\n            \"f1\",\n            \"f2\",\n            \"f3\",\n            \"f4\",\n        ]\n        F = len(keys)\n        # place the manual_seed before the InteractionTransformerArch object to generate the same initialization random values in the Transformer\n        torch.manual_seed(0)\n        inter_arch = InteractionTransformerArch(\n            num_sparse_features=F,\n            embedding_dim=D,\n            nhead=nhead,\n            ntransformer_layers=ntransformer_layers,\n        )\n        dense_features = torch.rand((B, D))\n        sparse_features = torch.rand((B, F, D))\n        concat_dense = inter_arch(dense_features, sparse_features)\n        self.assertEqual(concat_dense.size(), (B, D * (F + 1)))\n        expected = torch.tensor(\n            [\n                [\n                    -0.4411,\n                    0.2487,\n                    -1.2685,\n                    1.4610,\n                    1.3110,\n                    0.5152,\n                    -0.4960,\n                    -1.3303,\n                    -0.3962,\n                    -0.0623,\n                    -1.1371,\n                    1.5956,\n                    0.2431,\n                    -1.6820,\n                    0.5242,\n                    0.9148,\n                    1.3033,\n                    0.6409,\n                    -0.9577,\n                    -0.9866,\n                ],\n                [\n                    -1.0850,\n                    -0.0366,\n                    -0.4862,\n                    1.6078,\n                    1.1254,\n                    -0.9989,\n                    -0.9927,\n                    0.8661,\n                    -0.1704,\n                    1.0223,\n                    -1.5580,\n                    0.7060,\n                    -0.3081,\n                    -1.3686,\n                    0.2788,\n                    1.3979,\n                    0.0328,\n                    1.5470,\n                    -0.3670,\n                    -1.2128,\n                ],\n                [\n                    -1.5917,\n                    -0.0995,\n                    0.7302,\n                    0.9609,\n                    0.6606,\n                    1.0238,\n                    -0.1017,\n                    -1.5827,\n                    -0.6761,\n                    -1.0771,\n                    0.2262,\n                    1.5269,\n                    -0.5671,\n                    -1.2114,\n                    1.4503,\n                    0.3281,\n                    -0.6540,\n                    -1.2925,\n                    0.9134,\n                    1.0331,\n                ],\n            ]\n        )\n        self.assertTrue(\n            torch.allclose(\n                concat_dense,\n                expected,\n                rtol=1e-4,\n                atol=1e-4,\n            )\n        )",
  "def test_numerical_stability(self) -> None:\n        D = 4\n        B = 3\n        # multi-head attentions\n        nhead = 4\n        ntransformer_layers = 4\n        keys = [\"f1\", \"f2\"]\n        F = len(keys)\n        torch.manual_seed(0)\n        inter_arch = InteractionTransformerArch(\n            num_sparse_features=F,\n            embedding_dim=D,\n            nhead=nhead,\n            ntransformer_layers=ntransformer_layers,\n        )\n        dense_features = 10 * torch.rand(B, D)\n        sparse_features = 10 * torch.rand(B, F, D)\n        concat_dense = inter_arch(dense_features, sparse_features)\n        expected = torch.LongTensor(\n            [\n                [0, 1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 1],\n                [0, 0, 0, 1, -1, 0, 1, 0, 0, 0, 1, 0],\n                [-1, 0, 0, 0, 0, 0, -1, 1, 0, 1, -1, 0],\n            ]\n        )\n        self.assertTrue(torch.equal(concat_dense.long(), expected))",
  "def test_basic(self) -> None:\n        torch.manual_seed(0)\n        B = 2\n        D = 8\n        dense_in_features = 100\n        eb1_config = EmbeddingBagConfig(\n            name=\"t1\", embedding_dim=D, num_embeddings=100, feature_names=[\"f1\", \"f3\"]\n        )\n        eb2_config = EmbeddingBagConfig(\n            name=\"t2\",\n            embedding_dim=D,\n            num_embeddings=100,\n            feature_names=[\"f2\"],\n        )\n        ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])\n        sparse_nn = DLRM_Transformer(\n            embedding_bag_collection=ebc,\n            dense_in_features=dense_in_features,\n            dense_arch_layer_sizes=[20, D],\n            over_arch_layer_sizes=[5, 1],\n        )\n        features = torch.rand((B, dense_in_features))\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f1\", \"f3\", \"f2\"],\n            values=torch.tensor([1, 2, 4, 5, 4, 3, 2, 9, 1, 2, 3]),\n            offsets=torch.tensor([0, 2, 4, 6, 8, 10, 11]),\n        )\n        logits = sparse_nn(\n            dense_features=features,\n            sparse_features=sparse_features,\n        )\n        self.assertEqual(logits.size(), (B, 1))\n        expected_logits = torch.tensor([[-0.2593], [-0.1043]])\n        self.assertTrue(\n            torch.allclose(\n                logits,\n                expected_logits,\n                rtol=1e-4,\n                atol=1e-4,\n            )\n        )",
  "def test_one_sparse(self) -> None:\n        B = 2\n        D = 8\n        dense_in_features = 100\n        eb1_config = EmbeddingBagConfig(\n            name=\"t2\",\n            embedding_dim=D,\n            num_embeddings=100,\n            feature_names=[\"f2\"],\n        )\n        ebc = EmbeddingBagCollection(tables=[eb1_config])\n        sparse_nn = DLRM_Transformer(\n            embedding_bag_collection=ebc,\n            dense_in_features=dense_in_features,\n            dense_arch_layer_sizes=[20, D],\n            over_arch_layer_sizes=[5, 1],\n        )\n        features = torch.rand((B, dense_in_features))\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f2\"],\n            values=torch.tensor(range(3)),\n            offsets=torch.tensor([0, 2, 3]),\n        )\n        logits = sparse_nn(\n            dense_features=features,\n            sparse_features=sparse_features,\n        )\n        self.assertEqual(logits.size(), (B, 1))",
  "def test_no_sparse(self) -> None:\n        ebc = EmbeddingBagCollection(tables=[])\n        D_unused = 1\n        with self.assertRaises(AssertionError):\n            DLRM_Transformer(\n                embedding_bag_collection=ebc,\n                dense_in_features=100,\n                dense_arch_layer_sizes=[20, D_unused],\n                over_arch_layer_sizes=[5, 1],\n            )",
  "def test_basic(self) -> None:\n        B = 2\n        D = 8\n        dense_in_features = 100\n        eb1_config = EmbeddingBagConfig(\n            name=\"t2\",\n            embedding_dim=D,\n            num_embeddings=100,\n            feature_names=[\"f2\"],\n        )\n        ebc = EmbeddingBagCollection(tables=[eb1_config])\n        dlrm_module = DLRM_Transformer(\n            embedding_bag_collection=ebc,\n            dense_in_features=dense_in_features,\n            dense_arch_layer_sizes=[20, D],\n            over_arch_layer_sizes=[5, 1],\n        )\n        dlrm = DLRMTrain(dlrm_module)\n        features = torch.rand((B, dense_in_features))\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n            keys=[\"f2\"],\n            values=torch.tensor(range(3)),\n            offsets=torch.tensor([0, 2, 3]),\n        )\n        batch = Batch(\n            dense_features=features,\n            sparse_features=sparse_features,\n            labels=torch.randint(2, (B,)),\n        )\n        _, (_, logits, _) = dlrm(batch)\n        self.assertEqual(logits.size(), (B,))",
  "def _pin_and_move(tensor: torch.Tensor, device: torch.device) -> torch.Tensor:\n    return (\n        tensor\n        if device.type == \"cpu\"\n        else tensor.pin_memory().to(device=device, non_blocking=True)\n    )",
  "def _cumsum(o: List[int]) -> List[int]:\n    ret = [0] * (len(o) + 1)\n    for i in range(len(o)):\n        ret[i + 1] = ret[i] + o[i]\n    return ret",
  "def _to_offsets(lengths: torch.Tensor) -> torch.Tensor:\n    return torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)",
  "def _to_lengths(offsets: torch.Tensor) -> torch.Tensor:\n    return offsets[1:] - offsets[:-1]",
  "def _batched_lengths_to_offsets(lengths: torch.Tensor) -> torch.Tensor:\n    (f, b) = lengths.shape\n    offsets_0 = lengths.new_zeros((f, 1))\n    offsets_1 = torch.cumsum(lengths, dim=-1).to(lengths.dtype)\n    offsets = torch.cat([offsets_0, offsets_1], dim=-1)\n    return offsets",
  "def _maybe_compute_lengths(\n    lengths: Optional[torch.Tensor], offsets: Optional[torch.Tensor]\n) -> torch.Tensor:\n    if lengths is None:\n        assert offsets is not None\n        lengths = _to_lengths(offsets)\n    return lengths",
  "def _maybe_compute_offsets(\n    lengths: Optional[torch.Tensor], offsets: Optional[torch.Tensor]\n) -> torch.Tensor:\n    if offsets is None:\n        assert lengths is not None\n        offsets = _to_offsets(lengths)\n    return offsets",
  "def _get_weights_or_throw(weights: Optional[torch.Tensor]) -> torch.Tensor:\n    assert weights is not None, \"This (Keyed)JaggedTensor doesn't have weights.\"\n    return weights",
  "def _assert_offsets_or_lengths_is_provided(\n    offsets: Optional[torch.Tensor], lengths: Optional[torch.Tensor]\n) -> None:\n    assert offsets is not None or lengths is not None, \"Must provide lengths or offsets\"",
  "def _regroup_keyed_tensors(\n    keyed_tensors: List[\"KeyedTensor\"], groups: List[List[str]]\n) -> List[torch.Tensor]:\n    # Shortcut for no re-grouping\n    if len(keyed_tensors) == len(groups):\n        match = True\n        for kt, group in zip(keyed_tensors, groups):\n            if kt.keys() != group:\n                match = False\n                break\n        if match:\n            return [kt.values() for kt in keyed_tensors]\n\n    embedding_dicts = [keyed_tensor.to_dict() for keyed_tensor in keyed_tensors]\n    lengths = [keyed_tensor.length_per_key() for keyed_tensor in keyed_tensors]\n    indices = [keyed_tensor._key_indices() for keyed_tensor in keyed_tensors]\n    key_dim = keyed_tensors[0].key_dim()\n\n    key_to_idx: dict[str, int] = {}\n    for (i, keyed_tensor) in enumerate(keyed_tensors):\n        for key in keyed_tensor.keys():\n            key_to_idx[key] = i\n\n    # Rearrange values based on groups with a single torch.cat operation.\n    split_lengths: List[int] = []\n    cat_input: List[torch.Tensor] = []\n    for group in groups:\n        group_length = 0\n        for name in group:\n            cat_input.append(embedding_dicts[key_to_idx[name]][name])\n            group_length += lengths[key_to_idx[name]][indices[key_to_idx[name]][name]]\n        split_lengths.append(group_length)\n    rearranged_values = torch.cat(cat_input, key_dim)\n\n    return list(rearranged_values.split(split_lengths, dim=key_dim))",
  "def _values_string(values: torch.Tensor, start: int, end: int) -> str:\n    size = values.size()\n    if len(size) == 1:\n        return \"[\" + \", \".join([str(value.item()) for value in values[start:end]]) + \"]\"\n    elif len(size) == 2:\n        values_list: List[str] = []\n        for value in values[start:end]:\n            values_list.append(\"[\" + \", \".join([str(s.item()) for s in value]) + \"]\")\n        return \"[\" + \", \".join(values_list) + \"]\"\n    else:\n        raise ValueError(\n            \"the values dimension is larger than 2, we don't support printing\"\n        )",
  "def _jagged_values_string(\n    values: torch.Tensor,\n    offsets: torch.Tensor,\n    offset_start: int,\n    offset_end: int,\n) -> str:\n    return (\n        \"[\"\n        + \", \".join(\n            [\n                # pyre-fixme[6]: For 2nd param expected `int` but got `Tensor`.\n                # pyre-fixme[6]: For 3rd param expected `int` but got `Tensor`.\n                _values_string(values, offsets[index], offsets[index + 1])\n                for index in range(offset_start, offset_end)\n            ]\n        )\n        + \"]\"\n    )",
  "def _optional_mask(\n    tensor: Optional[torch.Tensor], mask: torch.Tensor\n) -> Optional[torch.Tensor]:\n\n    return tensor[mask] if tensor is not None else None",
  "def _arange(*args, **kwargs) -> torch.Tensor:\n    return torch.arange(*args, **kwargs)",
  "class JaggedTensorMeta(abc.ABCMeta, torch.fx._symbolic_trace.ProxyableClassMeta):\n    pass",
  "class JaggedTensor(Pipelineable, metaclass=JaggedTensorMeta):\n    \"\"\"\n    Represents an (optionally weighted) jagged tensor.\n\n    A `JaggedTensor` is a tensor with a *jagged dimension* which is dimension whose\n    slices may be of different lengths. See `KeyedJaggedTensor` for full example.\n\n    Implementation is torch.jit.script-able.\n\n    NOTE:\n        We will NOT do input validation as it's expensive, you should always pass in the\n        valid lengths, offsets, etc.\n\n    Args:\n        values (torch.Tensor): values tensor in dense representation.\n        weights (Optional[torch.Tensor]): if values have weights. Tensor with same shape\n            as values.\n        lengths (Optional[torch.Tensor]): jagged slices, represented as lengths.\n        offsets (Optional[torch.Tensor]): jagged slices, represented as cumulative\n            offsets.\n    \"\"\"\n\n    _fields = [\"_values\", \"_weights\", \"_lengths\", \"_offsets\"]\n\n    def __init__(\n        self,\n        values: torch.Tensor,\n        weights: Optional[torch.Tensor] = None,\n        lengths: Optional[torch.Tensor] = None,\n        offsets: Optional[torch.Tensor] = None,\n    ) -> None:\n\n        self._values: torch.Tensor = values\n        self._weights: Optional[torch.Tensor] = weights\n        _assert_offsets_or_lengths_is_provided(offsets, lengths)\n        if offsets is not None:\n            _assert_tensor_has_no_elements_or_has_integers(offsets, \"offsets\")\n        if lengths is not None:\n            _assert_tensor_has_no_elements_or_has_integers(lengths, \"lengths\")\n        self._lengths: Optional[torch.Tensor] = lengths\n        self._offsets: Optional[torch.Tensor] = offsets\n\n    @staticmethod\n    def empty(\n        is_weighted: bool = False,\n        device: Optional[torch.device] = None,\n        values_dtype: Optional[torch.dtype] = None,\n        weights_dtype: Optional[torch.dtype] = None,\n        lengths_dtype: torch.dtype = torch.int32,\n    ) -> \"JaggedTensor\":\n        weights = (\n            torch.tensor([], dtype=weights_dtype, device=device)\n            if is_weighted\n            else None\n        )\n        return JaggedTensor(\n            values=torch.tensor([], dtype=values_dtype, device=device),\n            offsets=torch.tensor([], dtype=lengths_dtype, device=device),\n            lengths=torch.tensor([], dtype=lengths_dtype, device=device),\n            weights=weights,\n        )\n\n    @staticmethod\n    def from_dense_lengths(\n        values: torch.Tensor,\n        lengths: torch.Tensor,\n        weights: Optional[torch.Tensor] = None,\n    ) -> \"JaggedTensor\":\n        \"\"\"\n        Constructs `JaggedTensor` from dense values/weights of shape (B, N,).\n\n        Note that `lengths` is still of shape (B,).\n        \"\"\"\n\n        mask2d = (\n            _arange(end=values.size(1), device=values.device).expand(values.size(0), -1)\n        ) < lengths.unsqueeze(-1)\n        return JaggedTensor(\n            values=values[mask2d],\n            weights=_optional_mask(weights, mask2d),\n            lengths=lengths,\n        )\n\n    @staticmethod\n    def from_dense(\n        values: List[torch.Tensor],\n        weights: Optional[List[torch.Tensor]] = None,\n    ) -> \"JaggedTensor\":\n        \"\"\"\n        Constructs `JaggedTensor` from dense values/weights of shape (B, N,).\n\n        Note that `lengths` and `offsets` are still of shape (B,).\n\n        Args:\n            values (List[torch.Tensor]): a list of tensors for dense representation\n            weights (Optional[List[torch.Tensor]]): if values have weights, tensor with\n                the same shape as values.\n\n        Returns:\n            JaggedTensor: JaggedTensor created from 2D dense tensor.\n\n        Example::\n\n            values = [\n                torch.Tensor([1.0]),\n                torch.Tensor(),\n                torch.Tensor([7.0, 8.0]),\n                torch.Tensor([10.0, 11.0, 12.0]),\n            ]\n            weights = [\n                torch.Tensor([1.0]),\n                torch.Tensor(),\n                torch.Tensor([7.0, 8.0]),\n                torch.Tensor([10.0, 11.0, 12.0]),\n            ]\n            j1 = JaggedTensor.from_dense(\n                values=values,\n                weights=weights,\n            )\n\n            # j1 = [[1.0], [], [7.0], [8.0], [10.0, 11.0, 12.0]]\n        \"\"\"\n        lengths = torch.IntTensor([value.size(0) for value in values])\n        values_tensor = torch.cat(values, dim=0)\n        weights_tensor = torch.cat(weights, dim=0) if weights is not None else None\n\n        return JaggedTensor(\n            values=values_tensor,\n            weights=weights_tensor,\n            lengths=lengths,\n        )\n\n    def to_dense(self) -> List[torch.Tensor]:\n        \"\"\"\n        Constructs a dense-representation of the JT's values.\n\n        Returns:\n            List[torch.Tensor]: list of tensors.\n\n        Example::\n\n            values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n            offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8])\n            jt = JaggedTensor(values=values, offsets=offsets)\n\n            values_list = jt.to_dense()\n\n            # values_list = [\n            #     torch.tensor([1.0, 2.0]),\n            #     torch.tensor([]),\n            #     torch.tensor([3.0]),\n            #     torch.tensor([4.0]),\n            #     torch.tensor([5.0]),\n            #     torch.tensor([6.0, 7.0, 8.0]),\n            # ]\n        \"\"\"\n        tensor_list = []\n        for index in range(self.offsets().size(0) - 1):\n            offset = self.offsets()[index].item()\n            next_offset = self.offsets()[index + 1].item()\n            tensor_list.append(self.values()[offset:next_offset])\n        return tensor_list\n\n    def to_dense_weights(self) -> Optional[List[torch.Tensor]]:\n        \"\"\"\n        Constructs a dense-representation of the JT's weights.\n\n        Returns:\n            Optional[List[torch.Tensor]]: list of tensors, `None` if no weights.\n\n        Example::\n\n            values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n            weights = torch.Tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n            offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8])\n            jt = JaggedTensor(values=values, weights=weights, offsets=offsets)\n\n            weights_list = jt.to_dense_weights()\n\n            # weights_list = [\n            #     torch.tensor([0.1, 0.2]),\n            #     torch.tensor([]),\n            #     torch.tensor([0.3]),\n            #     torch.tensor([0.4]),\n            #     torch.tensor([0.5]),\n            #     torch.tensor([0.6, 0.7, 0.8]),\n            # ]\n        \"\"\"\n        if self.weights_or_none() is None:\n            return None\n        tensor_list = []\n        for index in range(self.offsets().size(0) - 1):\n            offset = self.offsets()[index].item()\n            next_offset = self.offsets()[index + 1].item()\n            tensor_list.append(self.weights()[offset:next_offset])\n        return tensor_list\n\n    def to_padded_dense(\n        self,\n        desired_length: Optional[int] = None,\n        padding_value: float = 0.0,\n    ) -> torch.Tensor:\n        \"\"\"\n        Constructs a 2D dense tensor from the JT's values of shape (B, N,).\n\n        Note that `B` is the length of self.lengths() and `N` is the longest feature\n        length or `desired_length`.\n\n        If `desired_length` > `length` we will pad with `padding_value`, otherwise we\n        will select the last value at `desired_length`.\n\n        Args:\n            desired_length (int): the length of the tensor.\n            padding_value (float): padding value if we need to pad.\n\n        Returns:\n            torch.Tensor: 2d dense tensor.\n\n        Example::\n\n            values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n            offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8])\n            jt = JaggedTensor(values=values, offsets=offsets)\n\n            dt = jt.to_padded_dense(\n                desired_length=2,\n                padding_value=10.0,\n            )\n\n            # dt = [\n            #     [1.0, 2.0],\n            #     [10.0, 10.0],\n            #     [3.0, 10.0],\n            #     [4.0, 10.0],\n            #     [5.0, 10.0],\n            #     [6.0, 7.0],\n            # ]\n        \"\"\"\n        if desired_length is None:\n            N = int(torch.max(self.lengths()).item())\n        else:\n            N = desired_length\n        return torch.ops.fbgemm.jagged_to_padded_dense(\n            self.values(), [self.offsets()], [N], padding_value\n        )\n\n    def to_padded_dense_weights(\n        self,\n        desired_length: Optional[int] = None,\n        padding_value: float = 0.0,\n    ) -> Optional[torch.Tensor]:\n        \"\"\"\n        Constructs a 2D dense tensor from the JT's weights of shape (B, N,).\n\n        Note that `B` is the length of self.lengths() and `N` is the longest feature\n        length or `desired_length`.\n\n        If `desired_length` > `length` we will pad with `padding_value`, otherwise we\n        will select the last value at `desired_length`.\n\n        Args:\n            desired_length (int): the length of the tensor.\n            padding_value (float): padding value if we need to pad.\n\n        Returns:\n            Optional[torch.Tensor]: 2d dense tensor, `None` if no weights.\n\n        Example::\n\n            values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n            weights = torch.Tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n            offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8])\n            jt = JaggedTensor(values=values, weights=weights, offsets=offsets)\n\n            d_wt = jt.to_padded_dense_weights(\n                desired_length=2,\n                padding_value=1.0,\n            )\n\n            # d_wt = [\n            #     [0.1, 0.2],\n            #     [1.0, 1.0],\n            #     [0.3, 1.0],\n            #     [0.4, 1.0],\n            #     [0.5, 1.0],\n            #     [0.6, 0.7],\n            # ]\n        \"\"\"\n        if self.weights_or_none() is None:\n            return None\n        if desired_length is None:\n            N = int(torch.max(self.lengths()).item())\n        else:\n            N = desired_length\n        return torch.ops.fbgemm.jagged_to_padded_dense(\n            self.weights(), [self.offsets()], [N], padding_value\n        )\n\n    def lengths(self) -> torch.Tensor:\n        _lengths = _maybe_compute_lengths(self._lengths, self._offsets)\n        self._lengths = _lengths\n        return _lengths\n\n    def lengths_or_none(self) -> Optional[torch.Tensor]:\n        return self._lengths\n\n    def offsets(self) -> torch.Tensor:\n        _offsets = _maybe_compute_offsets(self._lengths, self._offsets)\n        self._offsets = _offsets\n        return _offsets\n\n    def offsets_or_none(self) -> Optional[torch.Tensor]:\n        return self._offsets\n\n    def values(self) -> torch.Tensor:\n        return self._values\n\n    def weights(self) -> torch.Tensor:\n        return _get_weights_or_throw(self._weights)\n\n    def weights_or_none(self) -> Optional[torch.Tensor]:\n        return self._weights\n\n    def to(self, device: torch.device, non_blocking: bool = False) -> \"JaggedTensor\":\n        weights = self._weights\n        lengths = self._lengths\n        offsets = self._offsets\n        return JaggedTensor(\n            values=self._values.to(device, non_blocking=non_blocking),\n            weights=weights.to(device, non_blocking=non_blocking)\n            if weights is not None\n            else None,\n            lengths=lengths.to(device, non_blocking=non_blocking)\n            if lengths is not None\n            else None,\n            offsets=offsets.to(device, non_blocking=non_blocking)\n            if offsets is not None\n            else None,\n        )\n\n    @torch.jit.unused\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n        self._values.record_stream(stream)\n        weights = self._weights\n        lengths = self._lengths\n        offsets = self._offsets\n        if weights is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            weights.record_stream(stream)\n        if lengths is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            lengths.record_stream(stream)\n        if offsets is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            offsets.record_stream(stream)\n\n    def __str__(self) -> str:\n        offsets = self.offsets()\n\n        if self._weights is None:\n            return (\n                \"JaggedTensor({\\n    \"\n                + _jagged_values_string(self._values, offsets, 0, len(offsets) - 1)\n                + \"\\n})\\n\"\n            )\n\n        return (\n            \"JaggedTensor({\\n\"\n            + '    \"values\": '\n            + _jagged_values_string(self._values, offsets, 0, len(offsets) - 1)\n            + ',\\n    \"weights\": '\n            + _jagged_values_string(\n                _get_weights_or_throw(self._weights), offsets, 0, len(offsets) - 1\n            )\n            + \"\\n})\\n\"\n        )",
  "def _jt_flatten(\n    t: JaggedTensor,\n) -> Tuple[List[Optional[torch.Tensor]], None]:\n    return [getattr(t, a) for a in JaggedTensor._fields], None",
  "def _jt_unflatten(values: List[Optional[torch.Tensor]], context: None) -> JaggedTensor:\n    return JaggedTensor(*values)",
  "def _jt_flatten_spec(t: JaggedTensor, spec: TreeSpec) -> List[Optional[torch.Tensor]]:\n    return [getattr(t, a) for a in JaggedTensor._fields]",
  "def _assert_tensor_has_no_elements_or_has_integers(\n    tensor: torch.Tensor, tensor_name: str\n) -> None:\n    assert tensor.numel() == 0 or tensor.dtype in [\n        torch.long,\n        torch.int,\n        torch.short,\n        torch.int8,\n        torch.uint8,\n    ], \"{} must be of integer type, but got {}\".format(tensor_name, tensor.dtype)",
  "def _maybe_compute_index_per_key(\n    keys: List[str],\n    index_per_key: Optional[Dict[str, int]],\n) -> Dict[str, int]:\n    if index_per_key is None:\n        index_per_key = {key: i for i, key in enumerate(keys)}\n    return index_per_key",
  "def _maybe_compute_stride_kjt(\n    keys: List[str],\n    stride: Optional[int],\n    lengths: Optional[torch.Tensor],\n    offsets: Optional[torch.Tensor],\n) -> int:\n    if stride is None:\n        if len(keys) == 0:\n            stride = 0\n        elif offsets is not None and offsets.numel() > 0:\n            stride = (offsets.numel() - 1) // len(keys)\n        elif lengths is not None:\n            stride = lengths.numel() // len(keys)\n        else:\n            stride = 0\n    return stride",
  "def _maybe_compute_stride_kjt_scripted(\n    keys: List[str],\n    stride: Optional[int],\n    lengths: Optional[torch.Tensor],\n    offsets: Optional[torch.Tensor],\n) -> torch.Tensor:\n    return torch.tensor([_maybe_compute_stride_kjt(keys, stride, lengths, offsets)])",
  "def _length_per_key_from_stride_per_key(\n    lengths: torch.Tensor, stride_per_key: List[int]\n) -> List[int]:\n    return [\n        int(torch.sum(chunk).item()) for chunk in torch.split(lengths, stride_per_key)\n    ]",
  "def _maybe_compute_length_per_key(\n    keys: List[str],\n    stride: int,\n    stride_per_key: List[int],\n    variable_stride_per_key: bool,\n    length_per_key: Optional[List[int]],\n    lengths: Optional[torch.Tensor],\n    offsets: Optional[torch.Tensor],\n) -> List[int]:\n    if length_per_key is None:\n        if len(keys) and offsets is not None and len(offsets) > 0:\n            _length: List[int] = (\n                _length_per_key_from_stride_per_key(torch.diff(offsets), stride_per_key)\n                if variable_stride_per_key\n                else torch.sum(torch.diff(offsets).view(-1, stride), dim=1).tolist()\n            )\n        elif len(keys) and lengths is not None:\n            _length: List[int] = (\n                _length_per_key_from_stride_per_key(lengths, stride_per_key)\n                if variable_stride_per_key\n                else (\n                    torch.sum(lengths.view(-1, stride), dim=1).tolist()\n                    if lengths.numel() != 0\n                    else [0] * len(keys)\n                )\n            )\n        else:\n            _length: List[int] = []\n        length_per_key = _length\n    return length_per_key",
  "def _maybe_compute_offset_per_key(\n    keys: List[str],\n    stride: int,\n    stride_per_key: List[int],\n    variable_stride_per_key: bool,\n    length_per_key: Optional[List[int]],\n    offset_per_key: Optional[List[int]],\n    lengths: Optional[torch.Tensor],\n    offsets: Optional[torch.Tensor],\n) -> Tuple[List[int], List[int]]:\n    if length_per_key is None:\n        _length_per_key: List[int] = _maybe_compute_length_per_key(\n            keys=keys,\n            stride=stride,\n            stride_per_key=stride_per_key,\n            variable_stride_per_key=variable_stride_per_key,\n            length_per_key=length_per_key,\n            lengths=lengths,\n            offsets=offsets,\n        )\n        return _length_per_key, _cumsum(_length_per_key)\n    elif offset_per_key is None:\n        return length_per_key, _cumsum(length_per_key)\n    else:\n        return length_per_key, offset_per_key",
  "def _jagged_tensor_string(\n    key: str,\n    values: torch.Tensor,\n    weights: Optional[torch.Tensor],\n    offsets: torch.Tensor,\n    offset_start: int,\n    offset_end: int,\n) -> str:\n    if weights is None:\n        return '\"{}\": '.format(key) + _jagged_values_string(\n            values, offsets, offset_start, offset_end\n        )\n\n    return (\n        '\"{}\"'.format(key)\n        + ': {\\n        \"values\": '\n        + _jagged_values_string(values, offsets, offset_start, offset_end)\n        + ',\\n        \"weights\": '\n        + _jagged_values_string(\n            _get_weights_or_throw(weights), offsets, offset_start, offset_end\n        )\n        + \"\\n    }\"\n    )",
  "class ComputeKJTToJTDict(torch.nn.Module):\n    \"\"\"Converts a KeyedJaggedTensor to a dict of JaggedTensors.\n\n    Args:\n\n    Example::\n        #              0       1        2  <-- dim_1\n        # \"Feature0\"   [V0,V1] None    [V2]\n        # \"Feature1\"   [V3]    [V4]    [V5,V6,V7]\n        #   ^\n        #  dim_0\n\n        would return\n\n        {\n            \"Feature0\": JaggedTensor([[V0,V1],None,V2]),\n            \"Feature1\": JaggedTensor([V3,V4,[V5,V6,V7]]),\n        }\n    \"\"\"\n\n    def forward(\n        self, keyed_jagged_tensor: \"KeyedJaggedTensor\"\n    ) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Converts a KeyedJaggedTensor into a dict of JaggedTensors.\n\n        Args:\n            keyed_jagged_tensor (KeyedJaggedTensor): tensor to convert\n        Returns:\n            Dict[str, JaggedTensor]\n        \"\"\"\n        return _maybe_compute_kjt_to_jt_dict(\n            stride=keyed_jagged_tensor.stride(),\n            stride_per_key=keyed_jagged_tensor.stride_per_key(),\n            keys=keyed_jagged_tensor.keys(),\n            length_per_key=keyed_jagged_tensor.length_per_key(),\n            values=keyed_jagged_tensor.values(),\n            lengths=keyed_jagged_tensor.lengths(),\n            variable_stride_per_key=keyed_jagged_tensor.variable_stride_per_key(),\n            weights=keyed_jagged_tensor.weights_or_none(),\n            jt_dict=keyed_jagged_tensor._jt_dict,\n        )",
  "def _maybe_compute_kjt_to_jt_dict(\n    stride: int,\n    stride_per_key: List[int],\n    keys: List[str],\n    length_per_key: List[int],\n    values: torch.Tensor,\n    lengths: torch.Tensor,\n    variable_stride_per_key: bool,\n    weights: Optional[torch.Tensor],\n    jt_dict: Optional[Dict[str, JaggedTensor]],\n) -> Dict[str, JaggedTensor]:\n    if not length_per_key:\n        return {}\n\n    if jt_dict is None:\n        _jt_dict: Dict[str, JaggedTensor] = {}\n        values_list = torch.split(values, length_per_key)\n        if variable_stride_per_key:\n            split_lengths = torch.split(lengths, stride_per_key)\n            split_offsets = [\n                torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)\n                for lengths in split_lengths\n            ]\n        else:\n            split_lengths = torch.unbind(\n                lengths.view(-1, stride) if lengths.numel() != 0 else lengths, dim=0\n            )\n            split_offsets = torch.unbind(\n                _batched_lengths_to_offsets(lengths.view(-1, stride))\n                if lengths.numel() != 0\n                else lengths,\n                dim=0,\n            )\n\n        if weights is not None:\n            weights_list = torch.split(weights, length_per_key)\n            for idx, key in enumerate(keys):\n                length = split_lengths[idx]\n                offset = split_offsets[idx]\n                _jt_dict[key] = JaggedTensor(\n                    lengths=length,\n                    offsets=offset,\n                    values=values_list[idx],\n                    weights=weights_list[idx],\n                )\n        else:\n            for idx, key in enumerate(keys):\n                length = split_lengths[idx]\n                offset = split_offsets[idx]\n                _jt_dict[key] = JaggedTensor(\n                    lengths=length,\n                    offsets=offset,\n                    values=values_list[idx],\n                )\n        jt_dict = _jt_dict\n    return jt_dict",
  "def _merge_weights_or_none(\n    a_weights: Optional[torch.Tensor],\n    b_weights: Optional[torch.Tensor],\n) -> Optional[torch.Tensor]:\n    assert not (\n        (a_weights is None) ^ (b_weights is None)\n    ), \"Can only merge weighted or unweighted KJTs.\"\n    if a_weights is None:\n        return None\n    # pyre-ignore[6]\n    return torch.cat([a_weights, b_weights], dim=0)",
  "def _sum_by_splits(input_list: List[int], splits: List[int]) -> List[int]:\n    return [\n        sum(input_list[sum(splits[:i]) : sum(splits[:i]) + n])\n        for i, n in enumerate(splits)\n    ]",
  "def jt_is_equal(jt_1: \"JaggedTensor\", jt_2: \"JaggedTensor\") -> bool:\n    \"\"\"This function checks if two JaggedTensors are equal by comparing their internal representations.\n    The comparison is done by comparing the values of the internal representations themselves.\n    For optional fields, None values are treated as equal.\n\n    Args:\n        jt_1 (JaggedTensor): the first JaggedTensor\n        jt_2 (JaggedTensor): the second JaggedTensor\n\n    Returns:\n        bool: True if both JaggedTensors have the same values\n    \"\"\"\n\n    if not isinstance(jt_1, JaggedTensor) or not isinstance(jt_2, JaggedTensor):\n        return False\n\n    if not _check_attributes(jt_1.values(), jt_2.values(), torch.allclose):\n        return False\n\n    _force_length_offset_computation(jt_1)\n    _force_length_offset_computation(jt_2)\n\n    attributes_to_check = [\n        (jt_1.weights_or_none(), jt_2.weights_or_none()),\n        (jt_1.lengths_or_none(), jt_2.lengths_or_none()),\n        (jt_1.offsets_or_none(), jt_2.offsets_or_none()),\n    ]\n\n    for attr_1, attr_2 in attributes_to_check:\n        if not _check_attributes(\n            attr_1,\n            attr_2,\n            torch.allclose if isinstance(attr_1, torch.Tensor) else operator.eq,\n        ):\n            return False\n\n    return True",
  "def kjt_is_equal(kjt_1: \"KeyedJaggedTensor\", kjt_2: \"KeyedJaggedTensor\") -> bool:\n    \"\"\"This function checks if two KeyedJaggedTensors are equal by comparing their internal representations.\n    The comparison is done by comparing the values of the internal representations themselves.\n    For optional fields, None values are treated as equal.\n    We compare the keys by ensuring that they have the same length and that the corresponding keys are the same order and same values.\n\n    Args:\n        kjt_1 (KeyedJaggedTensor): the first KeyedJaggedTensor\n        kjt_2 (KeyedJaggedTensor): the second KeyedJaggedTensor\n\n    Returns:\n        bool: True if both KeyedJaggedTensors have the same values\n    \"\"\"\n    if not isinstance(kjt_1, KeyedJaggedTensor) or not isinstance(\n        kjt_2, KeyedJaggedTensor\n    ):\n        return False\n\n    # check for missing/extra keys\n    if len(kjt_1.keys()) != len(kjt_2.keys()):\n        return False\n\n    # check if all keys are equal and in same order\n    for a, b in zip(kjt_1.keys(), kjt_2.keys()):\n        if a != b:\n            return False\n\n    if not _check_attributes(kjt_1.values(), kjt_2.values(), torch.allclose):\n        return False\n\n    _force_length_offset_computation(kjt_1)\n    _force_length_offset_computation(kjt_2)\n    # sync length and offset per key as well\n    kjt_1.sync()\n    kjt_2.sync()\n\n    attributes_to_check = [\n        (kjt_1.lengths_or_none(), kjt_2.lengths_or_none()),\n        (kjt_1.weights_or_none(), kjt_2.weights_or_none()),\n        (kjt_1.offsets_or_none(), kjt_2.offsets_or_none()),\n        (kjt_1.length_per_key_or_none(), kjt_2.length_per_key_or_none()),\n        (kjt_1.offset_per_key_or_none(), kjt_2.offset_per_key_or_none()),\n        (kjt_1.stride(), kjt_2.stride()),\n    ]\n\n    for attr_1, attr_2 in attributes_to_check:\n        if not _check_attributes(\n            attr_1,\n            attr_2,\n            torch.allclose if isinstance(attr_1, torch.Tensor) else operator.eq,\n        ):\n            return False\n\n    return True",
  "def _force_length_offset_computation(\n    kjt: Union[\"KeyedJaggedTensor\", \"JaggedTensor\"]\n) -> None:\n    \"\"\"Helper function to force length/offset computation for KJT or JT\n    Mainly used for testing equality, as equal KJT's/JT's can be formed from just using lengths or offsets.\n    One can be derived from the other so to ensure properly equality checking we force the computation of\n    the other attribute if it can be done.\n    \"\"\"\n    offsets = kjt.offsets_or_none()\n    lengths = kjt.lengths_or_none()\n    if offsets is not None and lengths is None:\n        kjt.lengths()\n    elif lengths is not None and offsets is None:\n        kjt.offsets()",
  "def _check_attributes(\n    attr_1: Union[torch.Tensor, List[int], List[str], int, None],\n    attr_2: Union[torch.Tensor, List[int], List[str], int, None],\n    comparison_func: Callable[[Any, Any], bool],  # pyre-ignore[2]\n) -> bool:\n    \"\"\"Helper function to check if two attributes are equal.\n\n    Args:\n        attr_1: The first attribute.\n        attr_2: The second attribute.\n        comparison_func (function): Function to compare the attributes.\n\n    Returns:\n        bool: False if the attributes are not equal or one is None while the other isn't, otherwise True.\n    \"\"\"\n    if attr_1 is not None and attr_2 is not None:\n        # allclose throws error for different tensor sizes, we check manually for this\n        if (\n            comparison_func == torch.allclose\n            and attr_1.size() != attr_2.size()  # pyre-ignore[16]\n        ):\n            return False\n        if not comparison_func(attr_1, attr_2):\n            return False\n    elif attr_1 is not None or attr_2 is not None:\n        return False\n\n    return True",
  "class KeyedJaggedTensor(Pipelineable, metaclass=JaggedTensorMeta):\n    \"\"\"Represents an (optionally weighted) keyed jagged tensor.\n\n    A `KeyedJaggedTensor` is a tensor with a *jagged dimension* which is dimension whose\n    slices may be of different lengths. Keyed on first dimension and jagged on the last\n    dimension.\n\n    Implementation is torch.jit.script-able.\n\n    Args:\n        keys (List[str]): keys to the jagged Tensor.\n        values (torch.Tensor): values tensor in dense representation.\n        weights (Optional[torch.Tensor]): if the values have weights. Tensor with the\n            same shape as values.\n        lengths (Optional[torch.Tensor]): jagged slices, represented as lengths.\n        offsets (Optional[torch.Tensor]): jagged slices, represented as cumulative\n            offsets.\n        stride (Optional[int]): number of examples per batch.\n        stride_per_key_per_rank (Optional[List[List[int]]]): batch size\n            (number of examples) per key per rank, with the outer list representing the\n            keys and the inner list representing the values.\n            Each value in the inner list represents the number of examples in the batch\n            from the rank of its index in a distributed context.\n        length_per_key (Optional[List[int]]): start length for each key.\n        offset_per_key (Optional[List[int]]): start offset for each key and final\n            offset.\n        index_per_key (Optional[Dict[str, int]]): index for each key.\n        jt_dict (Optional[Dict[str, JaggedTensor]]):\n\n    Example::\n\n        #              0       1        2  <-- dim_1\n        # \"Feature0\"   [V0,V1] None    [V2]\n        # \"Feature1\"   [V3]    [V4]    [V5,V6,V7]\n        #   ^\n        #  dim_0\n\n        dim_0: keyed dimension (ie. `Feature0`, `Feature1`)\n        dim_1: optional second dimension (ie. batch size)\n        dim_2: The jagged dimension which has slice lengths between 0-3 in the above example\n\n        # We represent this data with following inputs:\n\n        values: torch.Tensor = [V0, V1, V2, V3, V4, V5, V6, V7]  # V == any tensor datatype\n        weights: torch.Tensor = [W0, W1, W2, W3, W4, W5, W6, W7]  # W == any tensor datatype\n        lengths: torch.Tensor = [2, 0, 1, 1, 1, 3]  # representing the jagged slice\n        offsets: torch.Tensor = [0, 2, 2, 3, 4, 5, 8]  # offsets from 0 for each jagged slice\n        keys: List[str] = [\"Feature0\", \"Feature1\"]  # correspond to each value of dim_0\n        index_per_key: Dict[str, int] = {\"Feature0\": 0, \"Feature1\": 1}  # index for each key\n        offset_per_key: List[int] = [0, 3, 8]  # start offset for each key and final offset\n    \"\"\"\n\n    # This is the subset of fields on KJT which are required (all other fields\n    # can be derived from these fields, and are only cached)\n    _fields = [\n        \"_values\",\n        \"_weights\",\n        \"_lengths\",\n        \"_offsets\",\n    ]\n\n    def __init__(\n        self,\n        keys: List[str],\n        values: torch.Tensor,\n        weights: Optional[torch.Tensor] = None,\n        lengths: Optional[torch.Tensor] = None,\n        offsets: Optional[torch.Tensor] = None,\n        stride: Optional[int] = None,\n        stride_per_key_per_rank: Optional[List[List[int]]] = None,\n        # Below exposed to ensure torch.script-able\n        length_per_key: Optional[List[int]] = None,\n        offset_per_key: Optional[List[int]] = None,\n        index_per_key: Optional[Dict[str, int]] = None,\n        jt_dict: Optional[Dict[str, JaggedTensor]] = None,\n    ) -> None:\n        self._keys: List[str] = keys\n        self._values: torch.Tensor = values\n        self._weights: Optional[torch.Tensor] = weights\n        if offsets is not None:\n            _assert_tensor_has_no_elements_or_has_integers(offsets, \"offsets\")\n        if lengths is not None:\n            _assert_tensor_has_no_elements_or_has_integers(lengths, \"lengths\")\n        self._lengths: Optional[torch.Tensor] = lengths\n        self._offsets: Optional[torch.Tensor] = offsets\n\n        self._stride_per_key_per_rank: List[List[int]] = []\n        self._variable_stride_per_key: bool = False\n        self._stride: int = -1\n\n        if stride_per_key_per_rank is not None:\n            if stride is not None:\n                raise ValueError(\n                    \"Cannot initialize KJT with both `stride` and `stride_per_key_per_rank`\"\n                )\n            self._stride_per_key_per_rank = stride_per_key_per_rank\n            self._variable_stride_per_key = True\n            if not stride_per_key_per_rank:\n                self._stride = 0\n            elif all(s == self.stride_per_key()[0] for s in self.stride_per_key()):\n                self._stride = self.stride_per_key()[0]\n        else:\n            if torch.jit.is_tracing():\n                stride = _maybe_compute_stride_kjt_scripted(\n                    keys, stride, lengths, offsets\n                )[0]\n            else:\n                stride = _maybe_compute_stride_kjt(keys, stride, lengths, offsets)\n            self._stride = stride\n            self._stride_per_key_per_rank = [[stride]] * len(self._keys)\n\n        # lazy fields\n        self._length_per_key: Optional[List[int]] = length_per_key\n        self._offset_per_key: Optional[List[int]] = offset_per_key\n        self._index_per_key: Optional[Dict[str, int]] = index_per_key\n        self._jt_dict: Optional[Dict[str, JaggedTensor]] = jt_dict\n        self._lengths_offset_per_key: List[int] = []\n\n    @staticmethod\n    def from_offsets_sync(\n        keys: List[str],\n        values: torch.Tensor,\n        offsets: torch.Tensor,\n        weights: Optional[torch.Tensor] = None,\n        stride: Optional[int] = None,\n        stride_per_key_per_rank: Optional[List[List[int]]] = None,\n    ) -> \"KeyedJaggedTensor\":\n        kjt = KeyedJaggedTensor(\n            keys=keys,\n            values=values,\n            weights=weights,\n            offsets=offsets,\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank,\n        )\n        return kjt.sync()\n\n    @staticmethod\n    def from_lengths_sync(\n        keys: List[str],\n        values: torch.Tensor,\n        lengths: torch.Tensor,\n        weights: Optional[torch.Tensor] = None,\n        stride: Optional[int] = None,\n        stride_per_key_per_rank: Optional[List[List[int]]] = None,\n    ) -> \"KeyedJaggedTensor\":\n        kjt = KeyedJaggedTensor(\n            keys=keys,\n            values=values,\n            weights=weights,\n            lengths=lengths,\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank,\n        )\n        return kjt.sync()\n\n    @staticmethod\n    def concat(\n        kjt_list: List[\"KeyedJaggedTensor\"],\n    ) -> \"KeyedJaggedTensor\":\n        if len(kjt_list) == 0:\n            raise ValueError(\"Can't concat empty KJT list\")\n\n        is_weighted: bool = kjt_list[0].weights_or_none() is not None\n        has_length_per_key: bool = True\n\n        length_per_key: List[int] = []\n        keys: List[str] = []\n        value_list: List[torch.Tensor] = []\n        weight_list: List[torch.Tensor] = []\n        length_list: List[torch.Tensor] = []\n        stride_per_key_per_rank: List[List[int]] = []\n        stride: Optional[int] = None\n        variable_stride_per_key_list = [\n            kjt.variable_stride_per_key() for kjt in kjt_list\n        ]\n        assert all(variable_stride_per_key_list) or not any(\n            variable_stride_per_key_list\n        ), \"variable stride per key must be consistent for all KJTs\"\n        variable_stride_per_key = all(variable_stride_per_key_list)\n\n        for kjt in kjt_list:\n            curr_is_weighted: bool = kjt.weights_or_none() is not None\n            if is_weighted != curr_is_weighted:\n                raise ValueError(\"Can't merge weighted KJT with unweighted KJT\")\n            _length_per_key: Optional[List[int]] = None\n            if kjt._length_per_key is None:\n                has_length_per_key = False\n            else:\n                _length_per_key = kjt._length_per_key\n            if has_length_per_key and _length_per_key is not None:\n                length_per_key += _length_per_key\n            keys += kjt.keys()\n            value_list.append(kjt.values())\n            if is_weighted:\n                weight_list.append(kjt.weights())\n            length_list.append(kjt.lengths())\n            if variable_stride_per_key:\n                stride_per_key_per_rank += kjt.stride_per_key_per_rank()\n            elif stride is None:\n                stride = kjt.stride()\n            else:\n                assert stride == kjt.stride(), \"strides must be consistent for all KJTs\"\n\n        return KeyedJaggedTensor(\n            keys=keys,\n            values=torch.cat(value_list, dim=0),\n            weights=torch.cat(weight_list, dim=0) if is_weighted else None,\n            lengths=torch.cat(length_list, dim=0),\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank\n            if variable_stride_per_key\n            else None,\n            length_per_key=length_per_key if has_length_per_key else None,\n        )\n\n    @staticmethod\n    def empty(\n        is_weighted: bool = False,\n        device: Optional[torch.device] = None,\n        values_dtype: Optional[torch.dtype] = None,\n        weights_dtype: Optional[torch.dtype] = None,\n        lengths_dtype: torch.dtype = torch.int32,\n    ) -> \"KeyedJaggedTensor\":\n        weights = None\n        if is_weighted is True:\n            weights = torch.tensor([], dtype=weights_dtype, device=device)\n\n        return KeyedJaggedTensor(\n            keys=[],\n            values=torch.tensor([], dtype=values_dtype, device=device),\n            weights=weights,\n            lengths=torch.tensor([], dtype=lengths_dtype, device=device),\n            stride=0,\n        )\n\n    @staticmethod\n    def empty_like(kjt: \"KeyedJaggedTensor\") -> \"KeyedJaggedTensor\":\n        stride, stride_per_key_per_rank = (\n            (None, kjt.stride_per_key_per_rank())\n            if kjt.variable_stride_per_key()\n            else (kjt.stride(), None)\n        )\n        return KeyedJaggedTensor(\n            keys=[],\n            values=torch.tensor([], device=kjt.device(), dtype=kjt.values().dtype),\n            weights=None\n            if kjt.weights_or_none() is None\n            else torch.tensor([], device=kjt.device(), dtype=kjt.weights().dtype),\n            lengths=torch.tensor([], device=kjt.device(), dtype=kjt.lengths().dtype),\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank,\n        )\n\n    @staticmethod\n    def from_jt_dict(jt_dict: Dict[str, JaggedTensor]) -> \"KeyedJaggedTensor\":\n        \"\"\"\n        Constructs a KeyedJaggedTensor from a Dict[str, JaggedTensor],\n        but this function will ONLY work if the JaggedTensors all\n        have the same \"implicit\" batch_size dimension.\n\n        Basically, we can visualize JaggedTensors as 2-D tensors\n        of the format of [batch_size x variable_feature_dim].\n        In case, we have some batch without a feature value,\n        the input JaggedTensor could just not include any values.\n\n        But KeyedJaggedTensor (by default) typically pad \"None\"\n        so that all the JaggedTensors stored in the KeyedJaggedTensor\n        have the same batch_size dimension. That is, in the case,\n        the JaggedTensor input didn't automatically pad\n        for the empty batches, this function would error / not work.\n\n        Consider the visualization of the following KeyedJaggedTensor:\n        #              0       1        2  <-- dim_1\n        # \"Feature0\"   [V0,V1] None    [V2]\n        # \"Feature1\"   [V3]    [V4]    [V5,V6,V7]\n        #   ^\n        #  dim_0\n\n        Notice that the inputs for this KeyedJaggedTensor would have looked like:\n            values: torch.Tensor = [V0, V1, V2, V3, V4, V5, V6, V7]  # V == any tensor datatype\n            weights: torch.Tensor = [W0, W1, W2, W3, W4, W5, W6, W7]  # W == any tensor datatype\n            lengths: torch.Tensor = [2, 0, 1, 1, 1, 3]  # representing the jagged slice\n            offsets: torch.Tensor = [0, 2, 2, 3, 4, 5, 8]  # offsets from 0 for each jagged slice\n            keys: List[str] = [\"Feature0\", \"Feature1\"]  # correspond to each value of dim_0\n            index_per_key: Dict[str, int] = {\"Feature0\": 0, \"Feature1\": 1}  # index for each key\n            offset_per_key: List[int] = [0, 3, 8]  # start offset for each key and final offset\n\n        Now if the input jt_dict = {\n            # \"Feature0\"   [V0,V1] [V2]\n            # \"Feature1\"   [V3]    [V4]    [V5,V6,V7]\n        } and the \"None\" is left out from each JaggedTensor,\n        then this function would fail as we would not correctly\n        be able to pad \"None\" as it does not technically know\n        the correct batch / place to pad within the JaggedTensor.\n\n        Essentially, the lengths Tensor inferred by this function\n        would be [2, 1, 1, 1, 3] indicating variable batch_size\n        dim_1 violates the existing assumption / precondition\n        that KeyedJaggedTensor's should have fixed batch_size dimension.\n\n        \"\"\"\n        kjt_keys = list(jt_dict.keys())\n        kjt_vals_list: List[torch.Tensor] = []\n        kjt_lens_list: List[torch.Tensor] = []\n        kjt_weights_list: List[torch.Tensor] = []\n        stride_per_key: List[int] = []\n        for jt in jt_dict.values():\n            stride_per_key.append(len(jt.lengths()))\n            kjt_vals_list.append(jt.values())\n            kjt_lens_list.append(jt.lengths())\n            weight = jt.weights_or_none()\n            if weight is not None:\n                kjt_weights_list.append(weight)\n        kjt_vals = torch.concat(kjt_vals_list)\n        kjt_lens = torch.concat(kjt_lens_list)\n        kjt_weights = (\n            torch.concat(kjt_weights_list) if len(kjt_weights_list) > 0 else None\n        )\n        kjt_stride, kjt_stride_per_key_per_rank = (\n            (stride_per_key[0], None)\n            if all(s == stride_per_key[0] for s in stride_per_key)\n            else (None, [[stride] for stride in stride_per_key])\n        )\n        kjt = KeyedJaggedTensor(\n            keys=kjt_keys,\n            values=kjt_vals,\n            weights=kjt_weights,\n            lengths=kjt_lens,\n            stride=kjt_stride,\n            stride_per_key_per_rank=kjt_stride_per_key_per_rank,\n        ).sync()\n        return kjt\n\n    def sync(self) -> \"KeyedJaggedTensor\":\n        self.length_per_key()\n        self.offset_per_key()\n        return self\n\n    def unsync(self) -> \"KeyedJaggedTensor\":\n        self._length_per_key = None\n        self._offset_per_key = None\n        return self\n\n    def device(self) -> torch.device:\n        return self._values.device\n\n    def lengths(self) -> torch.Tensor:\n        _lengths = _maybe_compute_lengths(self._lengths, self._offsets)\n        self._lengths = _lengths\n        return _lengths\n\n    def lengths_or_none(self) -> Optional[torch.Tensor]:\n        return self._lengths\n\n    def offsets(self) -> torch.Tensor:\n        _offsets = _maybe_compute_offsets(self._lengths, self._offsets)\n        self._offsets = _offsets\n        return _offsets\n\n    def offsets_or_none(self) -> Optional[torch.Tensor]:\n        return self._offsets\n\n    def keys(self) -> List[str]:\n        return self._keys\n\n    def values(self) -> torch.Tensor:\n        return self._values\n\n    def weights(self) -> torch.Tensor:\n        return _get_weights_or_throw(self._weights)\n\n    def weights_or_none(self) -> Optional[torch.Tensor]:\n        return self._weights\n\n    def stride(self) -> int:\n        return self._stride\n\n    def stride_per_key(self) -> List[int]:\n        return [sum(stride) for stride in self._stride_per_key_per_rank]\n\n    def stride_per_key_per_rank(self) -> List[List[int]]:\n        return self._stride_per_key_per_rank\n\n    def variable_stride_per_key(self) -> bool:\n        return self._variable_stride_per_key\n\n    def _key_indices(self) -> Dict[str, int]:\n        _index_per_key: Dict[str, int] = _maybe_compute_index_per_key(\n            self._keys,\n            self._index_per_key,\n        )\n        self._index_per_key = _index_per_key\n        return _index_per_key\n\n    def length_per_key(self) -> List[int]:\n        _length_per_key = _maybe_compute_length_per_key(\n            keys=self._keys,\n            stride=self.stride(),\n            stride_per_key=self.stride_per_key(),\n            variable_stride_per_key=self.variable_stride_per_key(),\n            length_per_key=self._length_per_key,\n            lengths=self._lengths,\n            offsets=self._offsets,\n        )\n        self._length_per_key = _length_per_key\n        return _length_per_key\n\n    def length_per_key_or_none(self) -> Optional[List[int]]:\n        return self._length_per_key\n\n    def offset_per_key(self) -> List[int]:\n        _length_per_key, _offset_per_key = _maybe_compute_offset_per_key(\n            keys=self._keys,\n            stride=self.stride(),\n            stride_per_key=self.stride_per_key(),\n            variable_stride_per_key=self.variable_stride_per_key(),\n            length_per_key=self._length_per_key,\n            offset_per_key=self._offset_per_key,\n            lengths=self._lengths,\n            offsets=self._offsets,\n        )\n        self._length_per_key = _length_per_key\n        self._offset_per_key = _offset_per_key\n        return _offset_per_key\n\n    def offset_per_key_or_none(self) -> Optional[List[int]]:\n        return self._offset_per_key\n\n    def lengths_offset_per_key(self) -> List[int]:\n        if not self._lengths_offset_per_key:\n            self._lengths_offset_per_key = _cumsum(self.stride_per_key())\n        return self._lengths_offset_per_key\n\n    def split(self, segments: List[int]) -> List[\"KeyedJaggedTensor\"]:\n        split_list: List[KeyedJaggedTensor] = []\n        start = 0\n        start_offset = 0\n        _length_per_key = self.length_per_key()\n        _offset_per_key = self.offset_per_key()\n        for segment in segments:\n            end = start + segment\n            end_offset = _offset_per_key[end]\n            keys: List[str] = self._keys[start:end]\n            stride, stride_per_key_per_rank = (\n                (None, self.stride_per_key_per_rank()[start:end])\n                if self.variable_stride_per_key()\n                else (self._stride, None)\n            )\n            if segment == len(self._keys):\n                # no torch slicing required\n                split_list.append(\n                    KeyedJaggedTensor(\n                        keys=self._keys,\n                        values=self._values,\n                        weights=self.weights_or_none(),\n                        lengths=self._lengths,\n                        offsets=self._offsets,\n                        stride=stride,\n                        stride_per_key_per_rank=stride_per_key_per_rank,\n                        length_per_key=self._length_per_key,\n                        offset_per_key=self._offset_per_key,\n                        index_per_key=self._index_per_key,\n                        jt_dict=self._jt_dict,\n                    )\n                )\n            elif segment == 0:\n                empty_int_list: List[int] = torch.jit.annotate(List[int], [])\n                split_list.append(\n                    KeyedJaggedTensor(\n                        keys=keys,\n                        values=torch.tensor(\n                            empty_int_list,\n                            device=self.device(),\n                            dtype=self._values.dtype,\n                        ),\n                        weights=None\n                        if self.weights_or_none() is None\n                        else torch.tensor(\n                            empty_int_list,\n                            device=self.device(),\n                            dtype=self.weights().dtype,\n                        ),\n                        lengths=torch.tensor(\n                            empty_int_list, device=self.device(), dtype=torch.int\n                        ),\n                        offsets=torch.tensor(\n                            empty_int_list, device=self.device(), dtype=torch.int\n                        ),\n                        stride=stride,\n                        stride_per_key_per_rank=stride_per_key_per_rank,\n                        length_per_key=None,\n                        offset_per_key=None,\n                        index_per_key=None,\n                        jt_dict=None,\n                    )\n                )\n            else:\n                split_length_per_key = _length_per_key[start:end]\n                split_list.append(\n                    KeyedJaggedTensor(\n                        keys=keys,\n                        values=self._values[start_offset:end_offset],\n                        weights=None\n                        if self.weights_or_none() is None\n                        else self.weights()[start_offset:end_offset],\n                        lengths=self.lengths()[\n                            self.lengths_offset_per_key()[\n                                start\n                            ] : self.lengths_offset_per_key()[end]\n                        ],\n                        offsets=None,\n                        stride=stride,\n                        stride_per_key_per_rank=stride_per_key_per_rank,\n                        length_per_key=split_length_per_key,\n                        offset_per_key=None,\n                        index_per_key=None,\n                        jt_dict=None,\n                    )\n                )\n            start = end\n            start_offset = end_offset\n        return split_list\n\n    def permute(\n        self, indices: List[int], indices_tensor: Optional[torch.Tensor] = None\n    ) -> \"KeyedJaggedTensor\":\n\n        if indices_tensor is None:\n            indices_tensor = torch.tensor(\n                indices, dtype=torch.int, device=self.device()\n            )\n\n        length_per_key = self.length_per_key()\n        permuted_keys: List[str] = []\n        permuted_stride_per_key_per_rank: List[List[int]] = []\n        permuted_length_per_key: List[int] = []\n        permuted_lengths_sum = 0\n        for index in indices:\n            key = self.keys()[index]\n            permuted_keys.append(key)\n            permuted_stride_per_key_per_rank.append(\n                self.stride_per_key_per_rank()[index]\n            )\n            permuted_length_per_key.append(length_per_key[index])\n            permuted_lengths_sum += length_per_key[index]\n        if self.variable_stride_per_key():\n            length_per_key_tensor = _pin_and_move(\n                torch.tensor(self.length_per_key()), self.device()\n            )\n            stride_per_key_tensor = _pin_and_move(\n                torch.tensor(self.stride_per_key()), self.device()\n            )\n            (_, permuted_lengths, _,) = torch.ops.fbgemm.permute_1D_sparse_data(\n                indices_tensor,\n                stride_per_key_tensor,\n                self.lengths(),\n                None,\n                None,\n            )\n            (\n                _,\n                permuted_values,\n                permuted_weights,\n            ) = torch.ops.fbgemm.permute_1D_sparse_data(\n                indices_tensor,\n                length_per_key_tensor,\n                self.values(),\n                self.weights_or_none(),\n                None,\n            )\n        else:\n            (\n                permuted_lengths,\n                permuted_values,\n                permuted_weights,\n            ) = torch.ops.fbgemm.permute_2D_sparse_data(\n                indices_tensor,\n                self.lengths().view(len(self._keys), -1),\n                self.values(),\n                self.weights_or_none(),\n                permuted_lengths_sum,\n            )\n        stride, optional_permuted_stride_per_key_per_rank = (\n            (None, permuted_stride_per_key_per_rank)\n            if self.variable_stride_per_key()\n            else (self._stride, None)\n        )\n        kjt = KeyedJaggedTensor(\n            keys=permuted_keys,\n            values=permuted_values,\n            weights=permuted_weights,\n            lengths=permuted_lengths.view(-1),\n            offsets=None,\n            stride=stride,\n            stride_per_key_per_rank=optional_permuted_stride_per_key_per_rank,\n            length_per_key=permuted_length_per_key if len(permuted_keys) > 0 else None,\n            offset_per_key=None,\n            index_per_key=None,\n            jt_dict=None,\n        )\n        return kjt\n\n    def __getitem__(self, key: str) -> JaggedTensor:\n        offset_per_key = self.offset_per_key()\n        index = self._key_indices()[key]\n        start_offset = offset_per_key[index]\n        end_offset = (\n            offset_per_key[index + 1]\n            if index + 1 < len(offset_per_key)\n            else start_offset\n        )\n        return JaggedTensor(\n            values=self._values[start_offset:end_offset],\n            weights=None\n            if self.weights_or_none() is None\n            else self.weights()[start_offset:end_offset],\n            lengths=self.lengths()[\n                self.lengths_offset_per_key()[index] : self.lengths_offset_per_key()[\n                    index + 1\n                ]\n            ],\n            offsets=None,\n        )\n\n    def to_dict(self) -> Dict[str, JaggedTensor]:\n        _jt_dict = _maybe_compute_kjt_to_jt_dict(\n            stride=self.stride(),\n            stride_per_key=self.stride_per_key(),\n            keys=self.keys(),\n            length_per_key=self.length_per_key(),\n            lengths=self.lengths(),\n            values=self.values(),\n            variable_stride_per_key=self.variable_stride_per_key(),\n            weights=self.weights_or_none(),\n            jt_dict=self._jt_dict,\n        )\n        self._jt_dict = _jt_dict\n        return _jt_dict\n\n    @torch.jit.unused\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n        self._values.record_stream(stream)\n        weights = self._weights\n        lengths = self._lengths\n        offsets = self._offsets\n        if weights is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            weights.record_stream(stream)\n        if lengths is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            lengths.record_stream(stream)\n        if offsets is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            offsets.record_stream(stream)\n\n    def to(\n        self, device: torch.device, non_blocking: bool = False\n    ) -> \"KeyedJaggedTensor\":\n        weights = self._weights\n        lengths = self._lengths\n        offsets = self._offsets\n        stride, stride_per_key_per_rank = (\n            (None, self._stride_per_key_per_rank)\n            if self.variable_stride_per_key()\n            else (self._stride, None)\n        )\n        length_per_key = self._length_per_key\n        offset_per_key = self._offset_per_key\n        index_per_key = self._index_per_key\n        jt_dict = self._jt_dict\n\n        return KeyedJaggedTensor(\n            keys=self._keys,\n            values=self._values.to(device, non_blocking=non_blocking),\n            weights=weights.to(device, non_blocking=non_blocking)\n            if weights is not None\n            else None,\n            lengths=lengths.to(device, non_blocking=non_blocking)\n            if lengths is not None\n            else None,\n            offsets=offsets.to(device, non_blocking=non_blocking)\n            if offsets is not None\n            else None,\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank,\n            length_per_key=length_per_key,\n            offset_per_key=offset_per_key,\n            index_per_key=index_per_key,\n            jt_dict=jt_dict,\n        )\n\n    def __str__(self) -> str:\n        if len(self._keys) == 0 or self._offsets is None and self._lengths is None:\n            return \"KeyedJaggedTensor()\\n\"\n        offsets = self.offsets()\n\n        return (\n            \"KeyedJaggedTensor({\\n\"\n            + \",\\n\".join(\n                [\n                    \"    \"\n                    + _jagged_tensor_string(\n                        self._keys[index],\n                        self._values,\n                        self._weights,\n                        offsets,\n                        sum(self.stride_per_key()[:index]),\n                        sum(self.stride_per_key()[: index + 1]),\n                    )\n                    for index in range(len(self._keys))\n                ]\n            )\n            + \"\\n})\\n\"\n        )\n\n    def pin_memory(self) -> \"KeyedJaggedTensor\":\n        weights = self._weights\n        lengths = self._lengths\n        offsets = self._offsets\n        stride, stride_per_key_per_rank = (\n            (None, self._stride_per_key_per_rank)\n            if self.variable_stride_per_key()\n            else (self._stride, None)\n        )\n\n        return KeyedJaggedTensor(\n            keys=self._keys,\n            values=self._values.pin_memory(),\n            weights=weights.pin_memory() if weights is not None else None,\n            lengths=lengths.pin_memory() if lengths is not None else None,\n            offsets=offsets.pin_memory() if offsets is not None else None,\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank,\n            length_per_key=self._length_per_key,\n            offset_per_key=self._offset_per_key,\n            index_per_key=self._index_per_key,\n            jt_dict=None,\n        )\n\n    def dist_labels(self) -> List[str]:\n        labels = [\"lengths\", \"values\"]\n        if self.variable_stride_per_key():\n            labels.append(\"strides\")\n        if self.weights_or_none() is not None:\n            labels.append(\"weights\")\n        return labels\n\n    def dist_splits(self, key_splits: List[int]) -> List[List[int]]:\n        batch_size_per_split = _sum_by_splits(self.stride_per_key(), key_splits)\n        length_per_split = _sum_by_splits(self.length_per_key(), key_splits)\n        splits = [batch_size_per_split, length_per_split]\n        if self.variable_stride_per_key():\n            splits.append(key_splits)\n        if self.weights_or_none() is not None:\n            splits.append(length_per_split)\n        return splits\n\n    def dist_tensors(self) -> List[torch.Tensor]:\n        tensors = [self.lengths(), self.values()]\n        if self.variable_stride_per_key():\n            strides = _pin_and_move(torch.tensor(self.stride_per_key()), self.device())\n            tensors.append(strides)\n        if self.weights_or_none() is not None:\n            tensors.append(self.weights())\n        return tensors\n\n    @staticmethod\n    def dist_init(\n        keys: List[str],\n        tensors: List[torch.Tensor],\n        variable_stride_per_key: bool,\n        num_workers: int,\n        recat: Optional[torch.Tensor],\n        stride_per_rank: Optional[List[int]],\n    ) -> \"KeyedJaggedTensor\":\n        assert len(tensors) in [2, 3, 4]\n        lengths = tensors[0]\n        values = tensors[1]\n        stride_per_rank_per_key = tensors[2] if variable_stride_per_key else None\n        weights = (\n            tensors[-1]\n            if (variable_stride_per_key and len(tensors) == 4)\n            or (not variable_stride_per_key and len(tensors) == 3)\n            else None\n        )\n\n        if variable_stride_per_key:\n            assert stride_per_rank_per_key is not None\n            stride_per_key_per_rank: List[List[int]] = stride_per_rank_per_key.view(\n                num_workers, len(keys)\n            ).T.tolist()\n            strides_cumsum: List[int] = torch.ops.fbgemm.asynchronous_complete_cumsum(\n                stride_per_rank_per_key\n            ).tolist()\n            cumsum_lengths = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)\n            length_per_key = (\n                cumsum_lengths[strides_cumsum[1:]] - cumsum_lengths[strides_cumsum[:-1]]\n            )\n            with record_function(\"## all2all_data:recat_values ##\"):\n                if recat is not None and recat.numel() > 0:\n                    (_, lengths, _,) = torch.ops.fbgemm.permute_1D_sparse_data(\n                        recat,\n                        stride_per_rank_per_key,\n                        lengths,\n                        None,\n                        None,\n                    )\n                    (_, values, weights,) = torch.ops.fbgemm.permute_1D_sparse_data(\n                        recat,\n                        length_per_key,\n                        values,\n                        weights,\n                        None,\n                    )\n            if not stride_per_key_per_rank:\n                stride_per_key_per_rank = [[0]] * len(keys)\n            kjt = KeyedJaggedTensor(\n                keys=keys,\n                values=values,\n                weights=weights,\n                lengths=lengths,\n                stride_per_key_per_rank=stride_per_key_per_rank,\n            )\n            return kjt.sync()\n        else:\n            assert stride_per_rank is not None\n            with record_function(\"## all2all_data:recat_values ##\"):\n                if recat is not None and recat.numel() > 0:\n                    stride = stride_per_rank[0]\n                    if all(s == stride for s in stride_per_rank):\n                        (\n                            lengths,\n                            values,\n                            weights,\n                        ) = torch.ops.fbgemm.permute_2D_sparse_data(\n                            recat,\n                            lengths.view(-1, stride),\n                            values,\n                            weights,\n                            values.numel(),\n                        )\n                        lengths = lengths.view(-1)\n                    else:  # variable batch size per rank\n                        (\n                            lengths,\n                            values,\n                            weights,\n                        ) = torch.ops.fbgemm.permute_1D_sparse_data(\n                            recat,\n                            lengths.view(-1),\n                            values,\n                            weights,\n                            values.numel(),\n                        )\n            kjt = KeyedJaggedTensor(\n                keys=keys,\n                values=values,\n                weights=weights,\n                lengths=lengths,\n                stride=sum(stride_per_rank),\n            )\n            return kjt.sync()",
  "def _kjt_flatten(\n    t: KeyedJaggedTensor,\n) -> Tuple[List[Optional[torch.Tensor]], List[str]]:\n    return [getattr(t, a) for a in KeyedJaggedTensor._fields], t._keys",
  "def _kjt_unflatten(\n    values: List[Optional[torch.Tensor]], context: List[str]  # context is the _keys\n) -> KeyedJaggedTensor:\n    return KeyedJaggedTensor(context, *values)",
  "def _kjt_flatten_spec(\n    t: KeyedJaggedTensor, spec: TreeSpec\n) -> List[Optional[torch.Tensor]]:\n    return [getattr(t, a) for a in KeyedJaggedTensor._fields]",
  "def _maybe_compute_offset_per_key_kt(\n    length_per_key: List[int],\n    offset_per_key: Optional[List[int]],\n) -> List[int]:\n    if offset_per_key is None:\n        offset_per_key = _cumsum(length_per_key)\n    return offset_per_key",
  "def _keyed_values_string(values: torch.Tensor) -> str:\n    return (\n        \"[\"\n        + \", \".join([_values_string(value, 0, len(value)) for value in values])\n        + \"]\"\n    )",
  "class KeyedTensor(Pipelineable, metaclass=JaggedTensorMeta):\n    \"\"\"\n    KeyedTensor holds a concatenated list of dense tensors, each of which can be\n    accessed by a key.\n\n    The keyed dimension can be of variable length (length_per_key).\n    Common use cases uses include storage of pooled embeddings of different dimensions.\n\n    Implementation is torch.jit.script-able.\n\n    Args:\n        keys (List[str]): list of keys.\n        length_per_key (List[int]): length of each key along key dimension.\n        values (torch.Tensor): dense tensor, concatenated typically along key dimension.\n        key_dim (int): key dimension, zero indexed - defaults to 1\n            (typically B is 0-dimension).\n\n    Example::\n\n        # kt is KeyedTensor holding\n\n        #                         0           1           2\n        #     \"Embedding A\"    [1,1]       [1,1]        [1,1]\n        #     \"Embedding B\"    [2,1,2]     [2,1,2]      [2,1,2]\n        #     \"Embedding C\"    [3,1,2,3]   [3,1,2,3]    [3,1,2,3]\n\n        tensor_list = [\n            torch.tensor([[1,1]] * 3),\n            torch.tensor([[2,1,2]] * 3),\n            torch.tensor([[3,1,2,3]] * 3),\n        ]\n\n        keys = [\"Embedding A\", \"Embedding B\", \"Embedding C\"]\n\n        kt = KeyedTensor.from_tensor_list(keys, tensor_list)\n\n        kt.values()\n            # tensor(\n            #     [\n            #         [1, 1, 2, 1, 2, 3, 1, 2, 3],\n            #         [1, 1, 2, 1, 2, 3, 1, 2, 3],\n            #         [1, 1, 2, 1, 2, 3, 1, 2, 3],\n            #     ]\n            # )\n\n        kt[\"Embedding B\"]\n            # tensor([[2, 1, 2], [2, 1, 2], [2, 1, 2]])\n    \"\"\"\n\n    def __init__(\n        self,\n        keys: List[str],\n        length_per_key: List[int],\n        values: torch.Tensor,\n        key_dim: int = 1,\n        # Below exposed to ensure torch.script-able\n        offset_per_key: Optional[List[int]] = None,\n        index_per_key: Optional[Dict[str, int]] = None,\n    ) -> None:\n        self._keys = keys\n        self._length_per_key = length_per_key\n        self._values = values\n        self._key_dim = key_dim\n\n        self._offset_per_key: Optional[List[int]] = offset_per_key\n        self._index_per_key: Optional[Dict[str, int]] = index_per_key\n\n    @staticmethod\n    def from_tensor_list(\n        keys: List[str], tensors: List[torch.Tensor], key_dim: int = 1, cat_dim: int = 1\n    ) -> \"KeyedTensor\":\n        length_per_key = [tensor.shape[key_dim] for tensor in tensors]\n        return KeyedTensor(\n            keys=keys,\n            length_per_key=length_per_key,\n            values=torch.cat(tensors, dim=cat_dim),\n            key_dim=key_dim,\n        )\n\n    def keys(self) -> List[str]:\n        return self._keys\n\n    def values(self) -> torch.Tensor:\n        return self._values\n\n    def key_dim(self) -> int:\n        return self._key_dim\n\n    def offset_per_key(self) -> List[int]:\n        _offset_per_key = _maybe_compute_offset_per_key_kt(\n            self._length_per_key,\n            self._offset_per_key,\n        )\n        self._offset_per_key = _offset_per_key\n        return _offset_per_key\n\n    def length_per_key(self) -> List[int]:\n        return self._length_per_key\n\n    def _key_indices(self) -> Dict[str, int]:\n        _index_per_key = _maybe_compute_index_per_key(\n            self._keys,\n            self._index_per_key,\n        )\n        self._index_per_key = _index_per_key\n        return _index_per_key\n\n    def __getitem__(self, key: str) -> torch.Tensor:\n        index = self._key_indices()[key]\n        start = self.offset_per_key()[index]\n        length = self._length_per_key[index]\n        return self._values.narrow(dim=self._key_dim, start=start, length=length)\n\n    def to_dict(self) -> Dict[str, torch.Tensor]:\n        indices = self._key_indices()\n        lengths = self._length_per_key\n        split_values = self._values.split(lengths, dim=self._key_dim)\n        return {key: split_values[index] for (key, index) in indices.items()}\n\n    @staticmethod\n    def regroup(\n        keyed_tensors: List[\"KeyedTensor\"], groups: List[List[str]]\n    ) -> List[torch.Tensor]:\n        return _regroup_keyed_tensors(keyed_tensors, groups)\n\n    @staticmethod\n    def regroup_as_dict(\n        keyed_tensors: List[\"KeyedTensor\"], groups: List[List[str]], keys: List[str]\n    ) -> Dict[str, torch.Tensor]:\n        assert len(groups) == len(keys), \"Groups and keys should have same length\"\n        embeddings_list = _regroup_keyed_tensors(keyed_tensors, groups)\n        embeddings_dict: Dict[str, torch.Tensor] = {}\n        for i, key in enumerate(keys):\n            embeddings_dict[key] = embeddings_list[i]\n        return embeddings_dict\n\n    @torch.jit.unused\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n        self._values.record_stream(stream)\n\n    def to(self, device: torch.device, non_blocking: bool = False) -> \"KeyedTensor\":\n        return KeyedTensor(\n            keys=self._keys,\n            length_per_key=self._length_per_key,\n            values=self._values.to(device, non_blocking=non_blocking),\n            key_dim=self._key_dim,\n            offset_per_key=self._offset_per_key,\n            index_per_key=self._index_per_key,\n        )\n\n    def __str__(self) -> str:\n        if len(self._keys) == 0:\n            return \"KeyedTensor()\\n\"\n\n        return (\n            \"KeyedTensor({\\n\"\n            + \",\\n\".join(\n                [\n                    '    \"{}\": '.format(key) + _keyed_values_string(self[key])\n                    for key in self._keys\n                ]\n            )\n            + \"\\n})\\n\"\n        )",
  "def __init__(\n        self,\n        values: torch.Tensor,\n        weights: Optional[torch.Tensor] = None,\n        lengths: Optional[torch.Tensor] = None,\n        offsets: Optional[torch.Tensor] = None,\n    ) -> None:\n\n        self._values: torch.Tensor = values\n        self._weights: Optional[torch.Tensor] = weights\n        _assert_offsets_or_lengths_is_provided(offsets, lengths)\n        if offsets is not None:\n            _assert_tensor_has_no_elements_or_has_integers(offsets, \"offsets\")\n        if lengths is not None:\n            _assert_tensor_has_no_elements_or_has_integers(lengths, \"lengths\")\n        self._lengths: Optional[torch.Tensor] = lengths\n        self._offsets: Optional[torch.Tensor] = offsets",
  "def empty(\n        is_weighted: bool = False,\n        device: Optional[torch.device] = None,\n        values_dtype: Optional[torch.dtype] = None,\n        weights_dtype: Optional[torch.dtype] = None,\n        lengths_dtype: torch.dtype = torch.int32,\n    ) -> \"JaggedTensor\":\n        weights = (\n            torch.tensor([], dtype=weights_dtype, device=device)\n            if is_weighted\n            else None\n        )\n        return JaggedTensor(\n            values=torch.tensor([], dtype=values_dtype, device=device),\n            offsets=torch.tensor([], dtype=lengths_dtype, device=device),\n            lengths=torch.tensor([], dtype=lengths_dtype, device=device),\n            weights=weights,\n        )",
  "def from_dense_lengths(\n        values: torch.Tensor,\n        lengths: torch.Tensor,\n        weights: Optional[torch.Tensor] = None,\n    ) -> \"JaggedTensor\":\n        \"\"\"\n        Constructs `JaggedTensor` from dense values/weights of shape (B, N,).\n\n        Note that `lengths` is still of shape (B,).\n        \"\"\"\n\n        mask2d = (\n            _arange(end=values.size(1), device=values.device).expand(values.size(0), -1)\n        ) < lengths.unsqueeze(-1)\n        return JaggedTensor(\n            values=values[mask2d],\n            weights=_optional_mask(weights, mask2d),\n            lengths=lengths,\n        )",
  "def from_dense(\n        values: List[torch.Tensor],\n        weights: Optional[List[torch.Tensor]] = None,\n    ) -> \"JaggedTensor\":\n        \"\"\"\n        Constructs `JaggedTensor` from dense values/weights of shape (B, N,).\n\n        Note that `lengths` and `offsets` are still of shape (B,).\n\n        Args:\n            values (List[torch.Tensor]): a list of tensors for dense representation\n            weights (Optional[List[torch.Tensor]]): if values have weights, tensor with\n                the same shape as values.\n\n        Returns:\n            JaggedTensor: JaggedTensor created from 2D dense tensor.\n\n        Example::\n\n            values = [\n                torch.Tensor([1.0]),\n                torch.Tensor(),\n                torch.Tensor([7.0, 8.0]),\n                torch.Tensor([10.0, 11.0, 12.0]),\n            ]\n            weights = [\n                torch.Tensor([1.0]),\n                torch.Tensor(),\n                torch.Tensor([7.0, 8.0]),\n                torch.Tensor([10.0, 11.0, 12.0]),\n            ]\n            j1 = JaggedTensor.from_dense(\n                values=values,\n                weights=weights,\n            )\n\n            # j1 = [[1.0], [], [7.0], [8.0], [10.0, 11.0, 12.0]]\n        \"\"\"\n        lengths = torch.IntTensor([value.size(0) for value in values])\n        values_tensor = torch.cat(values, dim=0)\n        weights_tensor = torch.cat(weights, dim=0) if weights is not None else None\n\n        return JaggedTensor(\n            values=values_tensor,\n            weights=weights_tensor,\n            lengths=lengths,\n        )",
  "def to_dense(self) -> List[torch.Tensor]:\n        \"\"\"\n        Constructs a dense-representation of the JT's values.\n\n        Returns:\n            List[torch.Tensor]: list of tensors.\n\n        Example::\n\n            values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n            offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8])\n            jt = JaggedTensor(values=values, offsets=offsets)\n\n            values_list = jt.to_dense()\n\n            # values_list = [\n            #     torch.tensor([1.0, 2.0]),\n            #     torch.tensor([]),\n            #     torch.tensor([3.0]),\n            #     torch.tensor([4.0]),\n            #     torch.tensor([5.0]),\n            #     torch.tensor([6.0, 7.0, 8.0]),\n            # ]\n        \"\"\"\n        tensor_list = []\n        for index in range(self.offsets().size(0) - 1):\n            offset = self.offsets()[index].item()\n            next_offset = self.offsets()[index + 1].item()\n            tensor_list.append(self.values()[offset:next_offset])\n        return tensor_list",
  "def to_dense_weights(self) -> Optional[List[torch.Tensor]]:\n        \"\"\"\n        Constructs a dense-representation of the JT's weights.\n\n        Returns:\n            Optional[List[torch.Tensor]]: list of tensors, `None` if no weights.\n\n        Example::\n\n            values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n            weights = torch.Tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n            offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8])\n            jt = JaggedTensor(values=values, weights=weights, offsets=offsets)\n\n            weights_list = jt.to_dense_weights()\n\n            # weights_list = [\n            #     torch.tensor([0.1, 0.2]),\n            #     torch.tensor([]),\n            #     torch.tensor([0.3]),\n            #     torch.tensor([0.4]),\n            #     torch.tensor([0.5]),\n            #     torch.tensor([0.6, 0.7, 0.8]),\n            # ]\n        \"\"\"\n        if self.weights_or_none() is None:\n            return None\n        tensor_list = []\n        for index in range(self.offsets().size(0) - 1):\n            offset = self.offsets()[index].item()\n            next_offset = self.offsets()[index + 1].item()\n            tensor_list.append(self.weights()[offset:next_offset])\n        return tensor_list",
  "def to_padded_dense(\n        self,\n        desired_length: Optional[int] = None,\n        padding_value: float = 0.0,\n    ) -> torch.Tensor:\n        \"\"\"\n        Constructs a 2D dense tensor from the JT's values of shape (B, N,).\n\n        Note that `B` is the length of self.lengths() and `N` is the longest feature\n        length or `desired_length`.\n\n        If `desired_length` > `length` we will pad with `padding_value`, otherwise we\n        will select the last value at `desired_length`.\n\n        Args:\n            desired_length (int): the length of the tensor.\n            padding_value (float): padding value if we need to pad.\n\n        Returns:\n            torch.Tensor: 2d dense tensor.\n\n        Example::\n\n            values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n            offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8])\n            jt = JaggedTensor(values=values, offsets=offsets)\n\n            dt = jt.to_padded_dense(\n                desired_length=2,\n                padding_value=10.0,\n            )\n\n            # dt = [\n            #     [1.0, 2.0],\n            #     [10.0, 10.0],\n            #     [3.0, 10.0],\n            #     [4.0, 10.0],\n            #     [5.0, 10.0],\n            #     [6.0, 7.0],\n            # ]\n        \"\"\"\n        if desired_length is None:\n            N = int(torch.max(self.lengths()).item())\n        else:\n            N = desired_length\n        return torch.ops.fbgemm.jagged_to_padded_dense(\n            self.values(), [self.offsets()], [N], padding_value\n        )",
  "def to_padded_dense_weights(\n        self,\n        desired_length: Optional[int] = None,\n        padding_value: float = 0.0,\n    ) -> Optional[torch.Tensor]:\n        \"\"\"\n        Constructs a 2D dense tensor from the JT's weights of shape (B, N,).\n\n        Note that `B` is the length of self.lengths() and `N` is the longest feature\n        length or `desired_length`.\n\n        If `desired_length` > `length` we will pad with `padding_value`, otherwise we\n        will select the last value at `desired_length`.\n\n        Args:\n            desired_length (int): the length of the tensor.\n            padding_value (float): padding value if we need to pad.\n\n        Returns:\n            Optional[torch.Tensor]: 2d dense tensor, `None` if no weights.\n\n        Example::\n\n            values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n            weights = torch.Tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n            offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8])\n            jt = JaggedTensor(values=values, weights=weights, offsets=offsets)\n\n            d_wt = jt.to_padded_dense_weights(\n                desired_length=2,\n                padding_value=1.0,\n            )\n\n            # d_wt = [\n            #     [0.1, 0.2],\n            #     [1.0, 1.0],\n            #     [0.3, 1.0],\n            #     [0.4, 1.0],\n            #     [0.5, 1.0],\n            #     [0.6, 0.7],\n            # ]\n        \"\"\"\n        if self.weights_or_none() is None:\n            return None\n        if desired_length is None:\n            N = int(torch.max(self.lengths()).item())\n        else:\n            N = desired_length\n        return torch.ops.fbgemm.jagged_to_padded_dense(\n            self.weights(), [self.offsets()], [N], padding_value\n        )",
  "def lengths(self) -> torch.Tensor:\n        _lengths = _maybe_compute_lengths(self._lengths, self._offsets)\n        self._lengths = _lengths\n        return _lengths",
  "def lengths_or_none(self) -> Optional[torch.Tensor]:\n        return self._lengths",
  "def offsets(self) -> torch.Tensor:\n        _offsets = _maybe_compute_offsets(self._lengths, self._offsets)\n        self._offsets = _offsets\n        return _offsets",
  "def offsets_or_none(self) -> Optional[torch.Tensor]:\n        return self._offsets",
  "def values(self) -> torch.Tensor:\n        return self._values",
  "def weights(self) -> torch.Tensor:\n        return _get_weights_or_throw(self._weights)",
  "def weights_or_none(self) -> Optional[torch.Tensor]:\n        return self._weights",
  "def to(self, device: torch.device, non_blocking: bool = False) -> \"JaggedTensor\":\n        weights = self._weights\n        lengths = self._lengths\n        offsets = self._offsets\n        return JaggedTensor(\n            values=self._values.to(device, non_blocking=non_blocking),\n            weights=weights.to(device, non_blocking=non_blocking)\n            if weights is not None\n            else None,\n            lengths=lengths.to(device, non_blocking=non_blocking)\n            if lengths is not None\n            else None,\n            offsets=offsets.to(device, non_blocking=non_blocking)\n            if offsets is not None\n            else None,\n        )",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n        self._values.record_stream(stream)\n        weights = self._weights\n        lengths = self._lengths\n        offsets = self._offsets\n        if weights is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            weights.record_stream(stream)\n        if lengths is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            lengths.record_stream(stream)\n        if offsets is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            offsets.record_stream(stream)",
  "def __str__(self) -> str:\n        offsets = self.offsets()\n\n        if self._weights is None:\n            return (\n                \"JaggedTensor({\\n    \"\n                + _jagged_values_string(self._values, offsets, 0, len(offsets) - 1)\n                + \"\\n})\\n\"\n            )\n\n        return (\n            \"JaggedTensor({\\n\"\n            + '    \"values\": '\n            + _jagged_values_string(self._values, offsets, 0, len(offsets) - 1)\n            + ',\\n    \"weights\": '\n            + _jagged_values_string(\n                _get_weights_or_throw(self._weights), offsets, 0, len(offsets) - 1\n            )\n            + \"\\n})\\n\"\n        )",
  "def forward(\n        self, keyed_jagged_tensor: \"KeyedJaggedTensor\"\n    ) -> Dict[str, JaggedTensor]:\n        \"\"\"\n        Converts a KeyedJaggedTensor into a dict of JaggedTensors.\n\n        Args:\n            keyed_jagged_tensor (KeyedJaggedTensor): tensor to convert\n        Returns:\n            Dict[str, JaggedTensor]\n        \"\"\"\n        return _maybe_compute_kjt_to_jt_dict(\n            stride=keyed_jagged_tensor.stride(),\n            stride_per_key=keyed_jagged_tensor.stride_per_key(),\n            keys=keyed_jagged_tensor.keys(),\n            length_per_key=keyed_jagged_tensor.length_per_key(),\n            values=keyed_jagged_tensor.values(),\n            lengths=keyed_jagged_tensor.lengths(),\n            variable_stride_per_key=keyed_jagged_tensor.variable_stride_per_key(),\n            weights=keyed_jagged_tensor.weights_or_none(),\n            jt_dict=keyed_jagged_tensor._jt_dict,\n        )",
  "def __init__(\n        self,\n        keys: List[str],\n        values: torch.Tensor,\n        weights: Optional[torch.Tensor] = None,\n        lengths: Optional[torch.Tensor] = None,\n        offsets: Optional[torch.Tensor] = None,\n        stride: Optional[int] = None,\n        stride_per_key_per_rank: Optional[List[List[int]]] = None,\n        # Below exposed to ensure torch.script-able\n        length_per_key: Optional[List[int]] = None,\n        offset_per_key: Optional[List[int]] = None,\n        index_per_key: Optional[Dict[str, int]] = None,\n        jt_dict: Optional[Dict[str, JaggedTensor]] = None,\n    ) -> None:\n        self._keys: List[str] = keys\n        self._values: torch.Tensor = values\n        self._weights: Optional[torch.Tensor] = weights\n        if offsets is not None:\n            _assert_tensor_has_no_elements_or_has_integers(offsets, \"offsets\")\n        if lengths is not None:\n            _assert_tensor_has_no_elements_or_has_integers(lengths, \"lengths\")\n        self._lengths: Optional[torch.Tensor] = lengths\n        self._offsets: Optional[torch.Tensor] = offsets\n\n        self._stride_per_key_per_rank: List[List[int]] = []\n        self._variable_stride_per_key: bool = False\n        self._stride: int = -1\n\n        if stride_per_key_per_rank is not None:\n            if stride is not None:\n                raise ValueError(\n                    \"Cannot initialize KJT with both `stride` and `stride_per_key_per_rank`\"\n                )\n            self._stride_per_key_per_rank = stride_per_key_per_rank\n            self._variable_stride_per_key = True\n            if not stride_per_key_per_rank:\n                self._stride = 0\n            elif all(s == self.stride_per_key()[0] for s in self.stride_per_key()):\n                self._stride = self.stride_per_key()[0]\n        else:\n            if torch.jit.is_tracing():\n                stride = _maybe_compute_stride_kjt_scripted(\n                    keys, stride, lengths, offsets\n                )[0]\n            else:\n                stride = _maybe_compute_stride_kjt(keys, stride, lengths, offsets)\n            self._stride = stride\n            self._stride_per_key_per_rank = [[stride]] * len(self._keys)\n\n        # lazy fields\n        self._length_per_key: Optional[List[int]] = length_per_key\n        self._offset_per_key: Optional[List[int]] = offset_per_key\n        self._index_per_key: Optional[Dict[str, int]] = index_per_key\n        self._jt_dict: Optional[Dict[str, JaggedTensor]] = jt_dict\n        self._lengths_offset_per_key: List[int] = []",
  "def from_offsets_sync(\n        keys: List[str],\n        values: torch.Tensor,\n        offsets: torch.Tensor,\n        weights: Optional[torch.Tensor] = None,\n        stride: Optional[int] = None,\n        stride_per_key_per_rank: Optional[List[List[int]]] = None,\n    ) -> \"KeyedJaggedTensor\":\n        kjt = KeyedJaggedTensor(\n            keys=keys,\n            values=values,\n            weights=weights,\n            offsets=offsets,\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank,\n        )\n        return kjt.sync()",
  "def from_lengths_sync(\n        keys: List[str],\n        values: torch.Tensor,\n        lengths: torch.Tensor,\n        weights: Optional[torch.Tensor] = None,\n        stride: Optional[int] = None,\n        stride_per_key_per_rank: Optional[List[List[int]]] = None,\n    ) -> \"KeyedJaggedTensor\":\n        kjt = KeyedJaggedTensor(\n            keys=keys,\n            values=values,\n            weights=weights,\n            lengths=lengths,\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank,\n        )\n        return kjt.sync()",
  "def concat(\n        kjt_list: List[\"KeyedJaggedTensor\"],\n    ) -> \"KeyedJaggedTensor\":\n        if len(kjt_list) == 0:\n            raise ValueError(\"Can't concat empty KJT list\")\n\n        is_weighted: bool = kjt_list[0].weights_or_none() is not None\n        has_length_per_key: bool = True\n\n        length_per_key: List[int] = []\n        keys: List[str] = []\n        value_list: List[torch.Tensor] = []\n        weight_list: List[torch.Tensor] = []\n        length_list: List[torch.Tensor] = []\n        stride_per_key_per_rank: List[List[int]] = []\n        stride: Optional[int] = None\n        variable_stride_per_key_list = [\n            kjt.variable_stride_per_key() for kjt in kjt_list\n        ]\n        assert all(variable_stride_per_key_list) or not any(\n            variable_stride_per_key_list\n        ), \"variable stride per key must be consistent for all KJTs\"\n        variable_stride_per_key = all(variable_stride_per_key_list)\n\n        for kjt in kjt_list:\n            curr_is_weighted: bool = kjt.weights_or_none() is not None\n            if is_weighted != curr_is_weighted:\n                raise ValueError(\"Can't merge weighted KJT with unweighted KJT\")\n            _length_per_key: Optional[List[int]] = None\n            if kjt._length_per_key is None:\n                has_length_per_key = False\n            else:\n                _length_per_key = kjt._length_per_key\n            if has_length_per_key and _length_per_key is not None:\n                length_per_key += _length_per_key\n            keys += kjt.keys()\n            value_list.append(kjt.values())\n            if is_weighted:\n                weight_list.append(kjt.weights())\n            length_list.append(kjt.lengths())\n            if variable_stride_per_key:\n                stride_per_key_per_rank += kjt.stride_per_key_per_rank()\n            elif stride is None:\n                stride = kjt.stride()\n            else:\n                assert stride == kjt.stride(), \"strides must be consistent for all KJTs\"\n\n        return KeyedJaggedTensor(\n            keys=keys,\n            values=torch.cat(value_list, dim=0),\n            weights=torch.cat(weight_list, dim=0) if is_weighted else None,\n            lengths=torch.cat(length_list, dim=0),\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank\n            if variable_stride_per_key\n            else None,\n            length_per_key=length_per_key if has_length_per_key else None,\n        )",
  "def empty(\n        is_weighted: bool = False,\n        device: Optional[torch.device] = None,\n        values_dtype: Optional[torch.dtype] = None,\n        weights_dtype: Optional[torch.dtype] = None,\n        lengths_dtype: torch.dtype = torch.int32,\n    ) -> \"KeyedJaggedTensor\":\n        weights = None\n        if is_weighted is True:\n            weights = torch.tensor([], dtype=weights_dtype, device=device)\n\n        return KeyedJaggedTensor(\n            keys=[],\n            values=torch.tensor([], dtype=values_dtype, device=device),\n            weights=weights,\n            lengths=torch.tensor([], dtype=lengths_dtype, device=device),\n            stride=0,\n        )",
  "def empty_like(kjt: \"KeyedJaggedTensor\") -> \"KeyedJaggedTensor\":\n        stride, stride_per_key_per_rank = (\n            (None, kjt.stride_per_key_per_rank())\n            if kjt.variable_stride_per_key()\n            else (kjt.stride(), None)\n        )\n        return KeyedJaggedTensor(\n            keys=[],\n            values=torch.tensor([], device=kjt.device(), dtype=kjt.values().dtype),\n            weights=None\n            if kjt.weights_or_none() is None\n            else torch.tensor([], device=kjt.device(), dtype=kjt.weights().dtype),\n            lengths=torch.tensor([], device=kjt.device(), dtype=kjt.lengths().dtype),\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank,\n        )",
  "def from_jt_dict(jt_dict: Dict[str, JaggedTensor]) -> \"KeyedJaggedTensor\":\n        \"\"\"\n        Constructs a KeyedJaggedTensor from a Dict[str, JaggedTensor],\n        but this function will ONLY work if the JaggedTensors all\n        have the same \"implicit\" batch_size dimension.\n\n        Basically, we can visualize JaggedTensors as 2-D tensors\n        of the format of [batch_size x variable_feature_dim].\n        In case, we have some batch without a feature value,\n        the input JaggedTensor could just not include any values.\n\n        But KeyedJaggedTensor (by default) typically pad \"None\"\n        so that all the JaggedTensors stored in the KeyedJaggedTensor\n        have the same batch_size dimension. That is, in the case,\n        the JaggedTensor input didn't automatically pad\n        for the empty batches, this function would error / not work.\n\n        Consider the visualization of the following KeyedJaggedTensor:\n        #              0       1        2  <-- dim_1\n        # \"Feature0\"   [V0,V1] None    [V2]\n        # \"Feature1\"   [V3]    [V4]    [V5,V6,V7]\n        #   ^\n        #  dim_0\n\n        Notice that the inputs for this KeyedJaggedTensor would have looked like:\n            values: torch.Tensor = [V0, V1, V2, V3, V4, V5, V6, V7]  # V == any tensor datatype\n            weights: torch.Tensor = [W0, W1, W2, W3, W4, W5, W6, W7]  # W == any tensor datatype\n            lengths: torch.Tensor = [2, 0, 1, 1, 1, 3]  # representing the jagged slice\n            offsets: torch.Tensor = [0, 2, 2, 3, 4, 5, 8]  # offsets from 0 for each jagged slice\n            keys: List[str] = [\"Feature0\", \"Feature1\"]  # correspond to each value of dim_0\n            index_per_key: Dict[str, int] = {\"Feature0\": 0, \"Feature1\": 1}  # index for each key\n            offset_per_key: List[int] = [0, 3, 8]  # start offset for each key and final offset\n\n        Now if the input jt_dict = {\n            # \"Feature0\"   [V0,V1] [V2]\n            # \"Feature1\"   [V3]    [V4]    [V5,V6,V7]\n        } and the \"None\" is left out from each JaggedTensor,\n        then this function would fail as we would not correctly\n        be able to pad \"None\" as it does not technically know\n        the correct batch / place to pad within the JaggedTensor.\n\n        Essentially, the lengths Tensor inferred by this function\n        would be [2, 1, 1, 1, 3] indicating variable batch_size\n        dim_1 violates the existing assumption / precondition\n        that KeyedJaggedTensor's should have fixed batch_size dimension.\n\n        \"\"\"\n        kjt_keys = list(jt_dict.keys())\n        kjt_vals_list: List[torch.Tensor] = []\n        kjt_lens_list: List[torch.Tensor] = []\n        kjt_weights_list: List[torch.Tensor] = []\n        stride_per_key: List[int] = []\n        for jt in jt_dict.values():\n            stride_per_key.append(len(jt.lengths()))\n            kjt_vals_list.append(jt.values())\n            kjt_lens_list.append(jt.lengths())\n            weight = jt.weights_or_none()\n            if weight is not None:\n                kjt_weights_list.append(weight)\n        kjt_vals = torch.concat(kjt_vals_list)\n        kjt_lens = torch.concat(kjt_lens_list)\n        kjt_weights = (\n            torch.concat(kjt_weights_list) if len(kjt_weights_list) > 0 else None\n        )\n        kjt_stride, kjt_stride_per_key_per_rank = (\n            (stride_per_key[0], None)\n            if all(s == stride_per_key[0] for s in stride_per_key)\n            else (None, [[stride] for stride in stride_per_key])\n        )\n        kjt = KeyedJaggedTensor(\n            keys=kjt_keys,\n            values=kjt_vals,\n            weights=kjt_weights,\n            lengths=kjt_lens,\n            stride=kjt_stride,\n            stride_per_key_per_rank=kjt_stride_per_key_per_rank,\n        ).sync()\n        return kjt",
  "def sync(self) -> \"KeyedJaggedTensor\":\n        self.length_per_key()\n        self.offset_per_key()\n        return self",
  "def unsync(self) -> \"KeyedJaggedTensor\":\n        self._length_per_key = None\n        self._offset_per_key = None\n        return self",
  "def device(self) -> torch.device:\n        return self._values.device",
  "def lengths(self) -> torch.Tensor:\n        _lengths = _maybe_compute_lengths(self._lengths, self._offsets)\n        self._lengths = _lengths\n        return _lengths",
  "def lengths_or_none(self) -> Optional[torch.Tensor]:\n        return self._lengths",
  "def offsets(self) -> torch.Tensor:\n        _offsets = _maybe_compute_offsets(self._lengths, self._offsets)\n        self._offsets = _offsets\n        return _offsets",
  "def offsets_or_none(self) -> Optional[torch.Tensor]:\n        return self._offsets",
  "def keys(self) -> List[str]:\n        return self._keys",
  "def values(self) -> torch.Tensor:\n        return self._values",
  "def weights(self) -> torch.Tensor:\n        return _get_weights_or_throw(self._weights)",
  "def weights_or_none(self) -> Optional[torch.Tensor]:\n        return self._weights",
  "def stride(self) -> int:\n        return self._stride",
  "def stride_per_key(self) -> List[int]:\n        return [sum(stride) for stride in self._stride_per_key_per_rank]",
  "def stride_per_key_per_rank(self) -> List[List[int]]:\n        return self._stride_per_key_per_rank",
  "def variable_stride_per_key(self) -> bool:\n        return self._variable_stride_per_key",
  "def _key_indices(self) -> Dict[str, int]:\n        _index_per_key: Dict[str, int] = _maybe_compute_index_per_key(\n            self._keys,\n            self._index_per_key,\n        )\n        self._index_per_key = _index_per_key\n        return _index_per_key",
  "def length_per_key(self) -> List[int]:\n        _length_per_key = _maybe_compute_length_per_key(\n            keys=self._keys,\n            stride=self.stride(),\n            stride_per_key=self.stride_per_key(),\n            variable_stride_per_key=self.variable_stride_per_key(),\n            length_per_key=self._length_per_key,\n            lengths=self._lengths,\n            offsets=self._offsets,\n        )\n        self._length_per_key = _length_per_key\n        return _length_per_key",
  "def length_per_key_or_none(self) -> Optional[List[int]]:\n        return self._length_per_key",
  "def offset_per_key(self) -> List[int]:\n        _length_per_key, _offset_per_key = _maybe_compute_offset_per_key(\n            keys=self._keys,\n            stride=self.stride(),\n            stride_per_key=self.stride_per_key(),\n            variable_stride_per_key=self.variable_stride_per_key(),\n            length_per_key=self._length_per_key,\n            offset_per_key=self._offset_per_key,\n            lengths=self._lengths,\n            offsets=self._offsets,\n        )\n        self._length_per_key = _length_per_key\n        self._offset_per_key = _offset_per_key\n        return _offset_per_key",
  "def offset_per_key_or_none(self) -> Optional[List[int]]:\n        return self._offset_per_key",
  "def lengths_offset_per_key(self) -> List[int]:\n        if not self._lengths_offset_per_key:\n            self._lengths_offset_per_key = _cumsum(self.stride_per_key())\n        return self._lengths_offset_per_key",
  "def split(self, segments: List[int]) -> List[\"KeyedJaggedTensor\"]:\n        split_list: List[KeyedJaggedTensor] = []\n        start = 0\n        start_offset = 0\n        _length_per_key = self.length_per_key()\n        _offset_per_key = self.offset_per_key()\n        for segment in segments:\n            end = start + segment\n            end_offset = _offset_per_key[end]\n            keys: List[str] = self._keys[start:end]\n            stride, stride_per_key_per_rank = (\n                (None, self.stride_per_key_per_rank()[start:end])\n                if self.variable_stride_per_key()\n                else (self._stride, None)\n            )\n            if segment == len(self._keys):\n                # no torch slicing required\n                split_list.append(\n                    KeyedJaggedTensor(\n                        keys=self._keys,\n                        values=self._values,\n                        weights=self.weights_or_none(),\n                        lengths=self._lengths,\n                        offsets=self._offsets,\n                        stride=stride,\n                        stride_per_key_per_rank=stride_per_key_per_rank,\n                        length_per_key=self._length_per_key,\n                        offset_per_key=self._offset_per_key,\n                        index_per_key=self._index_per_key,\n                        jt_dict=self._jt_dict,\n                    )\n                )\n            elif segment == 0:\n                empty_int_list: List[int] = torch.jit.annotate(List[int], [])\n                split_list.append(\n                    KeyedJaggedTensor(\n                        keys=keys,\n                        values=torch.tensor(\n                            empty_int_list,\n                            device=self.device(),\n                            dtype=self._values.dtype,\n                        ),\n                        weights=None\n                        if self.weights_or_none() is None\n                        else torch.tensor(\n                            empty_int_list,\n                            device=self.device(),\n                            dtype=self.weights().dtype,\n                        ),\n                        lengths=torch.tensor(\n                            empty_int_list, device=self.device(), dtype=torch.int\n                        ),\n                        offsets=torch.tensor(\n                            empty_int_list, device=self.device(), dtype=torch.int\n                        ),\n                        stride=stride,\n                        stride_per_key_per_rank=stride_per_key_per_rank,\n                        length_per_key=None,\n                        offset_per_key=None,\n                        index_per_key=None,\n                        jt_dict=None,\n                    )\n                )\n            else:\n                split_length_per_key = _length_per_key[start:end]\n                split_list.append(\n                    KeyedJaggedTensor(\n                        keys=keys,\n                        values=self._values[start_offset:end_offset],\n                        weights=None\n                        if self.weights_or_none() is None\n                        else self.weights()[start_offset:end_offset],\n                        lengths=self.lengths()[\n                            self.lengths_offset_per_key()[\n                                start\n                            ] : self.lengths_offset_per_key()[end]\n                        ],\n                        offsets=None,\n                        stride=stride,\n                        stride_per_key_per_rank=stride_per_key_per_rank,\n                        length_per_key=split_length_per_key,\n                        offset_per_key=None,\n                        index_per_key=None,\n                        jt_dict=None,\n                    )\n                )\n            start = end\n            start_offset = end_offset\n        return split_list",
  "def permute(\n        self, indices: List[int], indices_tensor: Optional[torch.Tensor] = None\n    ) -> \"KeyedJaggedTensor\":\n\n        if indices_tensor is None:\n            indices_tensor = torch.tensor(\n                indices, dtype=torch.int, device=self.device()\n            )\n\n        length_per_key = self.length_per_key()\n        permuted_keys: List[str] = []\n        permuted_stride_per_key_per_rank: List[List[int]] = []\n        permuted_length_per_key: List[int] = []\n        permuted_lengths_sum = 0\n        for index in indices:\n            key = self.keys()[index]\n            permuted_keys.append(key)\n            permuted_stride_per_key_per_rank.append(\n                self.stride_per_key_per_rank()[index]\n            )\n            permuted_length_per_key.append(length_per_key[index])\n            permuted_lengths_sum += length_per_key[index]\n        if self.variable_stride_per_key():\n            length_per_key_tensor = _pin_and_move(\n                torch.tensor(self.length_per_key()), self.device()\n            )\n            stride_per_key_tensor = _pin_and_move(\n                torch.tensor(self.stride_per_key()), self.device()\n            )\n            (_, permuted_lengths, _,) = torch.ops.fbgemm.permute_1D_sparse_data(\n                indices_tensor,\n                stride_per_key_tensor,\n                self.lengths(),\n                None,\n                None,\n            )\n            (\n                _,\n                permuted_values,\n                permuted_weights,\n            ) = torch.ops.fbgemm.permute_1D_sparse_data(\n                indices_tensor,\n                length_per_key_tensor,\n                self.values(),\n                self.weights_or_none(),\n                None,\n            )\n        else:\n            (\n                permuted_lengths,\n                permuted_values,\n                permuted_weights,\n            ) = torch.ops.fbgemm.permute_2D_sparse_data(\n                indices_tensor,\n                self.lengths().view(len(self._keys), -1),\n                self.values(),\n                self.weights_or_none(),\n                permuted_lengths_sum,\n            )\n        stride, optional_permuted_stride_per_key_per_rank = (\n            (None, permuted_stride_per_key_per_rank)\n            if self.variable_stride_per_key()\n            else (self._stride, None)\n        )\n        kjt = KeyedJaggedTensor(\n            keys=permuted_keys,\n            values=permuted_values,\n            weights=permuted_weights,\n            lengths=permuted_lengths.view(-1),\n            offsets=None,\n            stride=stride,\n            stride_per_key_per_rank=optional_permuted_stride_per_key_per_rank,\n            length_per_key=permuted_length_per_key if len(permuted_keys) > 0 else None,\n            offset_per_key=None,\n            index_per_key=None,\n            jt_dict=None,\n        )\n        return kjt",
  "def __getitem__(self, key: str) -> JaggedTensor:\n        offset_per_key = self.offset_per_key()\n        index = self._key_indices()[key]\n        start_offset = offset_per_key[index]\n        end_offset = (\n            offset_per_key[index + 1]\n            if index + 1 < len(offset_per_key)\n            else start_offset\n        )\n        return JaggedTensor(\n            values=self._values[start_offset:end_offset],\n            weights=None\n            if self.weights_or_none() is None\n            else self.weights()[start_offset:end_offset],\n            lengths=self.lengths()[\n                self.lengths_offset_per_key()[index] : self.lengths_offset_per_key()[\n                    index + 1\n                ]\n            ],\n            offsets=None,\n        )",
  "def to_dict(self) -> Dict[str, JaggedTensor]:\n        _jt_dict = _maybe_compute_kjt_to_jt_dict(\n            stride=self.stride(),\n            stride_per_key=self.stride_per_key(),\n            keys=self.keys(),\n            length_per_key=self.length_per_key(),\n            lengths=self.lengths(),\n            values=self.values(),\n            variable_stride_per_key=self.variable_stride_per_key(),\n            weights=self.weights_or_none(),\n            jt_dict=self._jt_dict,\n        )\n        self._jt_dict = _jt_dict\n        return _jt_dict",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n        self._values.record_stream(stream)\n        weights = self._weights\n        lengths = self._lengths\n        offsets = self._offsets\n        if weights is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            weights.record_stream(stream)\n        if lengths is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            lengths.record_stream(stream)\n        if offsets is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            offsets.record_stream(stream)",
  "def to(\n        self, device: torch.device, non_blocking: bool = False\n    ) -> \"KeyedJaggedTensor\":\n        weights = self._weights\n        lengths = self._lengths\n        offsets = self._offsets\n        stride, stride_per_key_per_rank = (\n            (None, self._stride_per_key_per_rank)\n            if self.variable_stride_per_key()\n            else (self._stride, None)\n        )\n        length_per_key = self._length_per_key\n        offset_per_key = self._offset_per_key\n        index_per_key = self._index_per_key\n        jt_dict = self._jt_dict\n\n        return KeyedJaggedTensor(\n            keys=self._keys,\n            values=self._values.to(device, non_blocking=non_blocking),\n            weights=weights.to(device, non_blocking=non_blocking)\n            if weights is not None\n            else None,\n            lengths=lengths.to(device, non_blocking=non_blocking)\n            if lengths is not None\n            else None,\n            offsets=offsets.to(device, non_blocking=non_blocking)\n            if offsets is not None\n            else None,\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank,\n            length_per_key=length_per_key,\n            offset_per_key=offset_per_key,\n            index_per_key=index_per_key,\n            jt_dict=jt_dict,\n        )",
  "def __str__(self) -> str:\n        if len(self._keys) == 0 or self._offsets is None and self._lengths is None:\n            return \"KeyedJaggedTensor()\\n\"\n        offsets = self.offsets()\n\n        return (\n            \"KeyedJaggedTensor({\\n\"\n            + \",\\n\".join(\n                [\n                    \"    \"\n                    + _jagged_tensor_string(\n                        self._keys[index],\n                        self._values,\n                        self._weights,\n                        offsets,\n                        sum(self.stride_per_key()[:index]),\n                        sum(self.stride_per_key()[: index + 1]),\n                    )\n                    for index in range(len(self._keys))\n                ]\n            )\n            + \"\\n})\\n\"\n        )",
  "def pin_memory(self) -> \"KeyedJaggedTensor\":\n        weights = self._weights\n        lengths = self._lengths\n        offsets = self._offsets\n        stride, stride_per_key_per_rank = (\n            (None, self._stride_per_key_per_rank)\n            if self.variable_stride_per_key()\n            else (self._stride, None)\n        )\n\n        return KeyedJaggedTensor(\n            keys=self._keys,\n            values=self._values.pin_memory(),\n            weights=weights.pin_memory() if weights is not None else None,\n            lengths=lengths.pin_memory() if lengths is not None else None,\n            offsets=offsets.pin_memory() if offsets is not None else None,\n            stride=stride,\n            stride_per_key_per_rank=stride_per_key_per_rank,\n            length_per_key=self._length_per_key,\n            offset_per_key=self._offset_per_key,\n            index_per_key=self._index_per_key,\n            jt_dict=None,\n        )",
  "def dist_labels(self) -> List[str]:\n        labels = [\"lengths\", \"values\"]\n        if self.variable_stride_per_key():\n            labels.append(\"strides\")\n        if self.weights_or_none() is not None:\n            labels.append(\"weights\")\n        return labels",
  "def dist_splits(self, key_splits: List[int]) -> List[List[int]]:\n        batch_size_per_split = _sum_by_splits(self.stride_per_key(), key_splits)\n        length_per_split = _sum_by_splits(self.length_per_key(), key_splits)\n        splits = [batch_size_per_split, length_per_split]\n        if self.variable_stride_per_key():\n            splits.append(key_splits)\n        if self.weights_or_none() is not None:\n            splits.append(length_per_split)\n        return splits",
  "def dist_tensors(self) -> List[torch.Tensor]:\n        tensors = [self.lengths(), self.values()]\n        if self.variable_stride_per_key():\n            strides = _pin_and_move(torch.tensor(self.stride_per_key()), self.device())\n            tensors.append(strides)\n        if self.weights_or_none() is not None:\n            tensors.append(self.weights())\n        return tensors",
  "def dist_init(\n        keys: List[str],\n        tensors: List[torch.Tensor],\n        variable_stride_per_key: bool,\n        num_workers: int,\n        recat: Optional[torch.Tensor],\n        stride_per_rank: Optional[List[int]],\n    ) -> \"KeyedJaggedTensor\":\n        assert len(tensors) in [2, 3, 4]\n        lengths = tensors[0]\n        values = tensors[1]\n        stride_per_rank_per_key = tensors[2] if variable_stride_per_key else None\n        weights = (\n            tensors[-1]\n            if (variable_stride_per_key and len(tensors) == 4)\n            or (not variable_stride_per_key and len(tensors) == 3)\n            else None\n        )\n\n        if variable_stride_per_key:\n            assert stride_per_rank_per_key is not None\n            stride_per_key_per_rank: List[List[int]] = stride_per_rank_per_key.view(\n                num_workers, len(keys)\n            ).T.tolist()\n            strides_cumsum: List[int] = torch.ops.fbgemm.asynchronous_complete_cumsum(\n                stride_per_rank_per_key\n            ).tolist()\n            cumsum_lengths = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)\n            length_per_key = (\n                cumsum_lengths[strides_cumsum[1:]] - cumsum_lengths[strides_cumsum[:-1]]\n            )\n            with record_function(\"## all2all_data:recat_values ##\"):\n                if recat is not None and recat.numel() > 0:\n                    (_, lengths, _,) = torch.ops.fbgemm.permute_1D_sparse_data(\n                        recat,\n                        stride_per_rank_per_key,\n                        lengths,\n                        None,\n                        None,\n                    )\n                    (_, values, weights,) = torch.ops.fbgemm.permute_1D_sparse_data(\n                        recat,\n                        length_per_key,\n                        values,\n                        weights,\n                        None,\n                    )\n            if not stride_per_key_per_rank:\n                stride_per_key_per_rank = [[0]] * len(keys)\n            kjt = KeyedJaggedTensor(\n                keys=keys,\n                values=values,\n                weights=weights,\n                lengths=lengths,\n                stride_per_key_per_rank=stride_per_key_per_rank,\n            )\n            return kjt.sync()\n        else:\n            assert stride_per_rank is not None\n            with record_function(\"## all2all_data:recat_values ##\"):\n                if recat is not None and recat.numel() > 0:\n                    stride = stride_per_rank[0]\n                    if all(s == stride for s in stride_per_rank):\n                        (\n                            lengths,\n                            values,\n                            weights,\n                        ) = torch.ops.fbgemm.permute_2D_sparse_data(\n                            recat,\n                            lengths.view(-1, stride),\n                            values,\n                            weights,\n                            values.numel(),\n                        )\n                        lengths = lengths.view(-1)\n                    else:  # variable batch size per rank\n                        (\n                            lengths,\n                            values,\n                            weights,\n                        ) = torch.ops.fbgemm.permute_1D_sparse_data(\n                            recat,\n                            lengths.view(-1),\n                            values,\n                            weights,\n                            values.numel(),\n                        )\n            kjt = KeyedJaggedTensor(\n                keys=keys,\n                values=values,\n                weights=weights,\n                lengths=lengths,\n                stride=sum(stride_per_rank),\n            )\n            return kjt.sync()",
  "def _kjt_to_str(spec: TreeSpec, child_strings: List[str]) -> str:\n        assert spec.type == KeyedJaggedTensor\n        return f\"K({json.dumps([spec.context, ','.join(child_strings)])})\"",
  "def _maybe_str_to_kjt(str_spec: str):\n        if not str_spec.startswith(\"K\"):\n            return None\n        assert str_spec[1] == \"(\"\n        assert str_spec[-1] == \")\"\n        r = json.loads(str_spec[2:-1])\n        return KeyedJaggedTensor, r[0], r[1]",
  "def __init__(\n        self,\n        keys: List[str],\n        length_per_key: List[int],\n        values: torch.Tensor,\n        key_dim: int = 1,\n        # Below exposed to ensure torch.script-able\n        offset_per_key: Optional[List[int]] = None,\n        index_per_key: Optional[Dict[str, int]] = None,\n    ) -> None:\n        self._keys = keys\n        self._length_per_key = length_per_key\n        self._values = values\n        self._key_dim = key_dim\n\n        self._offset_per_key: Optional[List[int]] = offset_per_key\n        self._index_per_key: Optional[Dict[str, int]] = index_per_key",
  "def from_tensor_list(\n        keys: List[str], tensors: List[torch.Tensor], key_dim: int = 1, cat_dim: int = 1\n    ) -> \"KeyedTensor\":\n        length_per_key = [tensor.shape[key_dim] for tensor in tensors]\n        return KeyedTensor(\n            keys=keys,\n            length_per_key=length_per_key,\n            values=torch.cat(tensors, dim=cat_dim),\n            key_dim=key_dim,\n        )",
  "def keys(self) -> List[str]:\n        return self._keys",
  "def values(self) -> torch.Tensor:\n        return self._values",
  "def key_dim(self) -> int:\n        return self._key_dim",
  "def offset_per_key(self) -> List[int]:\n        _offset_per_key = _maybe_compute_offset_per_key_kt(\n            self._length_per_key,\n            self._offset_per_key,\n        )\n        self._offset_per_key = _offset_per_key\n        return _offset_per_key",
  "def length_per_key(self) -> List[int]:\n        return self._length_per_key",
  "def _key_indices(self) -> Dict[str, int]:\n        _index_per_key = _maybe_compute_index_per_key(\n            self._keys,\n            self._index_per_key,\n        )\n        self._index_per_key = _index_per_key\n        return _index_per_key",
  "def __getitem__(self, key: str) -> torch.Tensor:\n        index = self._key_indices()[key]\n        start = self.offset_per_key()[index]\n        length = self._length_per_key[index]\n        return self._values.narrow(dim=self._key_dim, start=start, length=length)",
  "def to_dict(self) -> Dict[str, torch.Tensor]:\n        indices = self._key_indices()\n        lengths = self._length_per_key\n        split_values = self._values.split(lengths, dim=self._key_dim)\n        return {key: split_values[index] for (key, index) in indices.items()}",
  "def regroup(\n        keyed_tensors: List[\"KeyedTensor\"], groups: List[List[str]]\n    ) -> List[torch.Tensor]:\n        return _regroup_keyed_tensors(keyed_tensors, groups)",
  "def regroup_as_dict(\n        keyed_tensors: List[\"KeyedTensor\"], groups: List[List[str]], keys: List[str]\n    ) -> Dict[str, torch.Tensor]:\n        assert len(groups) == len(keys), \"Groups and keys should have same length\"\n        embeddings_list = _regroup_keyed_tensors(keyed_tensors, groups)\n        embeddings_dict: Dict[str, torch.Tensor] = {}\n        for i, key in enumerate(keys):\n            embeddings_dict[key] = embeddings_list[i]\n        return embeddings_dict",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n        self._values.record_stream(stream)",
  "def to(self, device: torch.device, non_blocking: bool = False) -> \"KeyedTensor\":\n        return KeyedTensor(\n            keys=self._keys,\n            length_per_key=self._length_per_key,\n            values=self._values.to(device, non_blocking=non_blocking),\n            key_dim=self._key_dim,\n            offset_per_key=self._offset_per_key,\n            index_per_key=self._index_per_key,\n        )",
  "def __str__(self) -> str:\n        if len(self._keys) == 0:\n            return \"KeyedTensor()\\n\"\n\n        return (\n            \"KeyedTensor({\\n\"\n            + \",\\n\".join(\n                [\n                    '    \"{}\": '.format(key) + _keyed_values_string(self[key])\n                    for key in self._keys\n                ]\n            )\n            + \"\\n})\\n\"\n        )",
  "def set_ec_index_dedup(val: bool) -> None:\n    global EC_INDEX_DEDUP\n    EC_INDEX_DEDUP = val",
  "def get_ec_index_dedup() -> bool:\n    global EC_INDEX_DEDUP\n    return EC_INDEX_DEDUP",
  "def create_embedding_sharding(\n    sharding_type: str,\n    sharding_infos: List[EmbeddingShardingInfo],\n    env: ShardingEnv,\n    device: Optional[torch.device] = None,\n    qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n) -> EmbeddingSharding[\n    SequenceShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n]:\n    if sharding_type == ShardingType.TABLE_WISE.value:\n        return TwSequenceEmbeddingSharding(\n            sharding_infos=sharding_infos,\n            env=env,\n            device=device,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n    elif sharding_type == ShardingType.ROW_WISE.value:\n        return RwSequenceEmbeddingSharding(\n            sharding_infos=sharding_infos,\n            env=env,\n            device=device,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n    elif sharding_type == ShardingType.DATA_PARALLEL.value:\n        return DpSequenceEmbeddingSharding(\n            sharding_infos=sharding_infos,\n            env=env,\n            device=device,\n        )\n    elif sharding_type == ShardingType.COLUMN_WISE.value:\n        return CwSequenceEmbeddingSharding(\n            sharding_infos=sharding_infos,\n            env=env,\n            device=device,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n    else:\n        raise ValueError(f\"Sharding not supported {sharding_type}\")",
  "def create_sharding_infos_by_sharding(\n    module: EmbeddingCollectionInterface,\n    table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n    fused_params: Optional[Dict[str, Any]],\n) -> Dict[str, List[EmbeddingShardingInfo]]:\n\n    if fused_params is None:\n        fused_params = {}\n\n    sharding_type_to_sharding_infos: Dict[str, List[EmbeddingShardingInfo]] = {}\n    # state_dict returns parameter.Tensor, which loses parameter level attributes\n    parameter_by_name = dict(module.named_parameters())\n    # QuantEBC registers weights as buffers (since they are INT8), and so we need to grab it there\n    state_dict = module.state_dict()\n\n    for (\n        config,\n        embedding_names,\n    ) in zip(module.embedding_configs(), module.embedding_names_by_table()):\n        table_name = config.name\n        assert table_name in table_name_to_parameter_sharding\n\n        parameter_sharding = table_name_to_parameter_sharding[table_name]\n        if parameter_sharding.compute_kernel not in [\n            kernel.value for kernel in EmbeddingComputeKernel\n        ]:\n            raise ValueError(\n                f\"Compute kernel not supported {parameter_sharding.compute_kernel}\"\n            )\n\n        param_name = \"embeddings.\" + config.name + \".weight\"\n        assert param_name in parameter_by_name or param_name in state_dict\n        param = parameter_by_name.get(param_name, state_dict[param_name])\n\n        if parameter_sharding.sharding_type not in sharding_type_to_sharding_infos:\n            sharding_type_to_sharding_infos[parameter_sharding.sharding_type] = []\n\n        optimizer_params = getattr(param, \"_optimizer_kwargs\", [{}])\n        optimizer_classes = getattr(param, \"_optimizer_classes\", [None])\n\n        assert (\n            len(optimizer_classes) == 1 and len(optimizer_params) == 1\n        ), f\"Only support 1 optimizer, given {len(optimizer_classes)}\"\n\n        optimizer_class = optimizer_classes[0]\n        optimizer_params = optimizer_params[0]\n        if optimizer_class:\n            optimizer_params[\"optimizer\"] = optimizer_type_to_emb_opt_type(\n                optimizer_class\n            )\n\n        per_table_fused_params = merge_fused_params(fused_params, optimizer_params)\n        per_table_fused_params = add_params_from_parameter_sharding(\n            per_table_fused_params, parameter_sharding\n        )\n        per_table_fused_params = convert_to_fbgemm_types(per_table_fused_params)\n\n        sharding_type_to_sharding_infos[parameter_sharding.sharding_type].append(\n            (\n                EmbeddingShardingInfo(\n                    embedding_config=EmbeddingTableConfig(\n                        num_embeddings=config.num_embeddings,\n                        embedding_dim=config.embedding_dim,\n                        name=config.name,\n                        data_type=config.data_type,\n                        feature_names=copy.deepcopy(config.feature_names),\n                        pooling=PoolingType.NONE,\n                        is_weighted=False,\n                        has_feature_processor=False,\n                        embedding_names=embedding_names,\n                        weight_init_max=config.weight_init_max,\n                        weight_init_min=config.weight_init_min,\n                    ),\n                    param_sharding=parameter_sharding,\n                    param=param,\n                    fused_params=per_table_fused_params,\n                )\n            )\n        )\n    return sharding_type_to_sharding_infos",
  "def _construct_jagged_tensors(\n    embeddings: torch.Tensor,\n    features: KeyedJaggedTensor,\n    embedding_names: List[str],\n    need_indices: bool = False,\n    features_to_permute_indices: Optional[Dict[str, List[int]]] = None,\n    original_features: Optional[KeyedJaggedTensor] = None,\n    reverse_indices: Optional[torch.Tensor] = None,\n) -> Dict[str, JaggedTensor]:\n    with record_function(\"## _construct_jagged_tensors ##\"):\n        if original_features is not None:\n            features = original_features\n        if reverse_indices is not None:\n            embeddings = torch.index_select(\n                embeddings, 0, reverse_indices.to(torch.int32)\n            )\n\n        ret: Dict[str, JaggedTensor] = {}\n        stride = features.stride()\n        length_per_key = features.length_per_key()\n        values = features.values()\n\n        lengths = features.lengths().view(-1, stride)\n        lengths_tuple = torch.unbind(lengths.view(-1, stride), dim=0)\n        embeddings_list = torch.split(embeddings, length_per_key, dim=0)\n        values_list = torch.split(values, length_per_key) if need_indices else None\n\n        key_indices = defaultdict(list)\n        for i, key in enumerate(embedding_names):\n            key_indices[key].append(i)\n        for key, indices in key_indices.items():\n            # combines outputs in correct order for CW sharding\n            indices = (\n                _permute_indices(indices, features_to_permute_indices[key])\n                if features_to_permute_indices and key in features_to_permute_indices\n                else indices\n            )\n            ret[key] = JaggedTensor(\n                lengths=lengths_tuple[indices[0]],\n                values=embeddings_list[indices[0]]\n                if len(indices) == 1\n                else torch.cat([embeddings_list[i] for i in indices], dim=1),\n                weights=values_list[indices[0]] if values_list else None,\n            )\n        return ret",
  "def _permute_indices(indices: List[int], permute: List[int]) -> List[int]:\n    permuted_indices = [0] * len(indices)\n    for i, permuted_index in enumerate(permute):\n        permuted_indices[i] = indices[permuted_index]\n    return permuted_indices",
  "class EmbeddingCollectionContext(Multistreamable):\n    sharding_contexts: List[SequenceShardingContext] = field(default_factory=list)\n    input_features: List[KeyedJaggedTensor] = field(default_factory=list)\n    reverse_indices: List[torch.Tensor] = field(default_factory=list)\n\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for ctx in self.sharding_contexts:\n            ctx.record_stream(stream)\n        for f in self.input_features:\n            f.record_stream(stream)\n        for r in self.reverse_indices:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            r.record_stream(stream)",
  "class EmbeddingCollectionAwaitable(LazyAwaitable[Dict[str, JaggedTensor]]):\n    def __init__(\n        self,\n        awaitables_per_sharding: List[Awaitable[torch.Tensor]],\n        features_per_sharding: List[KeyedJaggedTensor],\n        embedding_names_per_sharding: List[List[str]],\n        ctx: EmbeddingCollectionContext,\n        need_indices: bool = False,\n        features_to_permute_indices: Optional[Dict[str, List[int]]] = None,\n    ) -> None:\n        super().__init__()\n        self._awaitables_per_sharding = awaitables_per_sharding\n        self._features_per_sharding = features_per_sharding\n        self._need_indices = need_indices\n        self._features_to_permute_indices = features_to_permute_indices\n        self._embedding_names_per_sharding = embedding_names_per_sharding\n        self._ctx = ctx\n\n    def _wait_impl(self) -> Dict[str, JaggedTensor]:\n        jt_dict: Dict[str, JaggedTensor] = {}\n        for i, (w, f, e) in enumerate(\n            zip(\n                self._awaitables_per_sharding,\n                self._features_per_sharding,\n                self._embedding_names_per_sharding,\n            )\n        ):\n            original_features = (\n                None\n                if i >= len(self._ctx.input_features)\n                else self._ctx.input_features[i]\n            )\n            reverse_indices = (\n                None\n                if i >= len(self._ctx.reverse_indices)\n                else self._ctx.reverse_indices[i]\n            )\n            jt_dict.update(\n                _construct_jagged_tensors(\n                    embeddings=w.wait(),\n                    features=f,\n                    embedding_names=e,\n                    need_indices=self._need_indices,\n                    features_to_permute_indices=self._features_to_permute_indices,\n                    original_features=original_features,\n                    reverse_indices=reverse_indices,\n                )\n            )\n        return jt_dict",
  "class ShardedEmbeddingCollection(\n    ShardedEmbeddingModule[\n        KJTList,\n        List[torch.Tensor],\n        Dict[str, JaggedTensor],\n        EmbeddingCollectionContext,\n    ],\n    # TODO remove after compute_kernel X sharding decoupling\n    FusedOptimizerModule,\n):\n    \"\"\"\n    Sharded implementation of `EmbeddingCollection`.\n    This is part of the public API to allow for manual data dist pipelining.\n    \"\"\"\n\n    def __init__(\n        self,\n        module: EmbeddingCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._embedding_configs: List[EmbeddingConfig] = module.embedding_configs()\n        self._table_names: List[str] = [\n            config.name for config in self._embedding_configs\n        ]\n        self.module_sharding_plan: EmbeddingModuleShardingPlan = cast(\n            EmbeddingModuleShardingPlan,\n            {\n                table_name: parameter_sharding\n                for table_name, parameter_sharding in table_name_to_parameter_sharding.items()\n                if table_name in self._table_names\n            },\n        )\n\n        self._env = env\n        sharding_type_to_sharding_infos = create_sharding_infos_by_sharding(\n            module,\n            table_name_to_parameter_sharding,\n            fused_params,\n        )\n        self._sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                SequenceShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n            ],\n        ] = {\n            sharding_type: create_embedding_sharding(\n                sharding_type=sharding_type,\n                sharding_infos=embedding_confings,\n                env=env,\n                device=device,\n                qcomm_codecs_registry=self.qcomm_codecs_registry,\n            )\n            for sharding_type, embedding_confings in sharding_type_to_sharding_infos.items()\n        }\n\n        self._device = device\n        self._input_dists: List[nn.Module] = []\n        self._lookups: List[nn.Module] = []\n        self._create_lookups()\n        self._output_dists: List[nn.Module] = []\n        self._create_output_dist()\n\n        self._feature_splits: List[int] = []\n        self._features_order: List[int] = []\n\n        self._has_uninitialized_input_dist: bool = True\n        self._ec_index_dedup: bool = get_ec_index_dedup()\n        logger.info(f\"EC index dedup enabled: {self._ec_index_dedup}.\")\n\n        # Get all fused optimizers and combine them.\n        optims = []\n        for lookup in self._lookups:\n            for _, m in lookup.named_modules():\n                if isinstance(m, FusedOptimizerModule):\n                    # modify param keys to match EmbeddingCollection\n                    params: MutableMapping[str, Union[torch.Tensor, ShardedTensor]] = {}\n                    for param_key, weight in m.fused_optimizer.params.items():\n                        params[\"embeddings.\" + param_key] = weight\n                    m.fused_optimizer.params = params\n                    optims.append((\"\", m.fused_optimizer))\n        self._optim: CombinedOptimizer = CombinedOptimizer(optims)\n        self._embedding_dim: int = module.embedding_dim()\n        self._embedding_names_per_sharding: List[List[str]] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._embedding_names_per_sharding.append(sharding.embedding_names())\n        self._local_embedding_dim: int = self._embedding_dim\n        self._features_to_permute_indices: Dict[str, List[int]] = {}\n        if ShardingType.COLUMN_WISE.value in self._sharding_type_to_sharding:\n            sharding = self._sharding_type_to_sharding[ShardingType.COLUMN_WISE.value]\n            # CW partition must be same for all CW sharded parameters\n            self._local_embedding_dim = cast(\n                ShardMetadata, sharding.embedding_shard_metadata()[0]\n            ).shard_sizes[1]\n            self._generate_permute_indices_per_feature(\n                module.embedding_configs(), table_name_to_parameter_sharding\n            )\n        self._need_indices: bool = module.need_indices()\n\n        for index, (sharding, lookup) in enumerate(\n            zip(\n                self._sharding_type_to_sharding.values(),\n                self._lookups,\n            )\n        ):\n            # TODO: can move this into DpPooledEmbeddingSharding once all modules are composable\n            if isinstance(sharding, DpSequenceEmbeddingSharding):\n                self._lookups[index] = DistributedDataParallel(\n                    module=lookup,\n                    device_ids=[device]\n                    if self._device and self._device.type == \"gpu\"\n                    else None,\n                    process_group=env.process_group,\n                    gradient_as_bucket_view=True,\n                    broadcast_buffers=True,\n                    static_graph=True,\n                )\n        self._initialize_torch_state()\n\n        if module.device != torch.device(\"meta\"):\n            self.load_state_dict(module.state_dict())\n\n    @staticmethod\n    def _pre_load_state_dict_hook(\n        self: \"ShardedEmbeddingCollection\",\n        state_dict: Dict[str, Any],\n        prefix: str,\n        *args: Any,\n    ) -> None:\n        \"\"\"\n        Modify the destination state_dict for model parallel\n        to transform from ShardedTensors into tensors\n        \"\"\"\n        for (\n            table_name,\n            model_shards,\n        ) in self._model_parallel_name_to_local_shards.items():\n            key = f\"{prefix}embeddings.{table_name}.weight\"\n\n            # If state_dict[key] is already a ShardedTensor, use its local shards\n            if isinstance(state_dict[key], ShardedTensor):\n                local_shards = state_dict[key].local_shards()\n                # If no local shards, create an empty tensor\n                if len(local_shards) == 0:\n                    state_dict[key] = torch.empty(0)\n                else:\n                    dim = state_dict[key].metadata().shards_metadata[0].shard_sizes[1]\n                    # CW multiple shards are merged\n                    if len(local_shards) > 1:\n                        state_dict[key] = torch.cat(\n                            [s.tensor.view(-1) for s in local_shards], dim=0\n                        ).view(-1, dim)\n                    else:\n                        state_dict[key] = local_shards[0].tensor.view(-1, dim)\n            else:\n                local_shards = []\n                for shard in model_shards:\n                    # Extract shard size and offsets for splicing\n                    shard_sizes = shard.metadata.shard_sizes\n                    shard_offsets = shard.metadata.shard_offsets\n\n                    # Prepare tensor by splicing and placing on appropriate device\n                    spliced_tensor = state_dict[key][\n                        shard_offsets[0] : shard_offsets[0] + shard_sizes[0],\n                        shard_offsets[1] : shard_offsets[1] + shard_sizes[1],\n                    ].to(shard.tensor.get_device())\n\n                    # Append spliced tensor into local shards\n                    local_shards.append(spliced_tensor)\n\n                state_dict[key] = (\n                    torch.empty(0)\n                    if not local_shards\n                    else torch.cat(local_shards, dim=0)\n                )\n\n    def _initialize_torch_state(self) -> None:  # noqa\n        \"\"\"\n        This provides consistency between this class and the EmbeddingCollection's\n        nn.Module API calls (state_dict, named_modules, etc)\n        \"\"\"\n\n        self.embeddings: nn.ModuleDict = nn.ModuleDict()\n        for table_name in self._table_names:\n            self.embeddings[table_name] = nn.Module()\n        self._model_parallel_name_to_local_shards = OrderedDict()\n        self._model_parallel_name_to_sharded_tensor = OrderedDict()\n        model_parallel_name_to_compute_kernel: Dict[str, str] = {}\n        for (\n            table_name,\n            parameter_sharding,\n        ) in self.module_sharding_plan.items():\n            if parameter_sharding.sharding_type == ShardingType.DATA_PARALLEL.value:\n                continue\n            self._model_parallel_name_to_local_shards[table_name] = []\n            model_parallel_name_to_compute_kernel[\n                table_name\n            ] = parameter_sharding.compute_kernel\n\n        self._name_to_table_size = {}\n        for table in self._embedding_configs:\n            self._name_to_table_size[table.name] = (\n                table.num_embeddings,\n                table.embedding_dim,\n            )\n\n        for sharding_type, lookup in zip(\n            self._sharding_type_to_sharding.keys(), self._lookups\n        ):\n            if sharding_type == ShardingType.DATA_PARALLEL.value:\n                # unwrap DDP\n                lookup = lookup.module\n            else:\n                # save local_shards for transforming MP params to shardedTensor\n                for key, v in lookup.state_dict().items():\n                    table_name = key[: -len(\".weight\")]\n                    self._model_parallel_name_to_local_shards[table_name].extend(\n                        v.local_shards()\n                    )\n            for (\n                table_name,\n                tbe_slice,\n            ) in lookup.named_parameters_by_table():\n                self.embeddings[table_name].register_parameter(\"weight\", tbe_slice)\n        for (\n            table_name,\n            local_shards,\n        ) in self._model_parallel_name_to_local_shards.items():\n            # for shards that don't exist on this rank, register with empty tensor\n            if not hasattr(self.embeddings[table_name], \"weight\"):\n                self.embeddings[table_name].register_parameter(\n                    \"weight\", nn.Parameter(torch.empty(0))\n                )\n                if (\n                    model_parallel_name_to_compute_kernel[table_name]\n                    != EmbeddingComputeKernel.DENSE.value\n                ):\n                    self.embeddings[table_name].weight._in_backward_optimizers = [\n                        EmptyFusedOptimizer()\n                    ]\n            # created ShardedTensors once in init, use in post_state_dict_hook\n            self._model_parallel_name_to_sharded_tensor[\n                table_name\n            ] = ShardedTensor._init_from_local_shards(\n                local_shards,\n                self._name_to_table_size[table_name],\n                process_group=self._env.process_group,\n            )\n\n        def post_state_dict_hook(\n            module: ShardedEmbeddingCollection,\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            # Adjust dense MP\n            for (\n                table_name,\n                sharded_t,\n            ) in module._model_parallel_name_to_sharded_tensor.items():\n                destination_key = f\"{prefix}embeddings.{table_name}.weight\"\n                destination[destination_key] = sharded_t\n\n        self._register_state_dict_hook(post_state_dict_hook)\n        self._register_load_state_dict_pre_hook(\n            self._pre_load_state_dict_hook, with_module=True\n        )\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        if self._device and self._device.type == \"meta\":\n            return\n        # Initialize embedding weights with init_fn\n        for table_config in self._embedding_configs:\n            assert table_config.init_fn is not None\n            param = self.embeddings[f\"{table_config.name}\"].weight\n            # pyre-ignore\n            table_config.init_fn(param)\n\n    def _generate_permute_indices_per_feature(\n        self,\n        embedding_configs: List[EmbeddingConfig],\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n    ) -> None:\n        \"\"\"\n        Generates permute indices per feature for column-wise sharding.\n\n        Since outputs are stored in order of rank, column-wise shards of a table on the\n        same rank will be seen as adjacent, which may not be correct.\n\n        The permute indices store the correct ordering of outputs relative to the\n        provided ordering.\n\n        Example::\n            rank_0 = [f_0(shard_0), f_0(shard_2)]\n            rank_1 = [f_0(shard_1)]\n            output = [f_0(shard_0), f_0(shard_2), f_0(shard_1)]\n\n            shard_ranks = [0, 1, 0]\n            output_ranks = [0, 0, 1]\n\n            # To get the correct order from output_ranks -> shard_ranks\n            permute_indices = [0, 2, 1]\n        \"\"\"\n        shared_feature: Dict[str, bool] = {}\n        for table in embedding_configs:\n            for feature_name in table.feature_names:\n                if feature_name not in shared_feature:\n                    shared_feature[feature_name] = False\n                else:\n                    shared_feature[feature_name] = True\n\n        for table in embedding_configs:\n            sharding = table_name_to_parameter_sharding[table.name]\n            if sharding.sharding_type != ShardingType.COLUMN_WISE.value:\n                continue\n            ranks = cast(List[int], sharding.ranks)\n            rank_to_indices = defaultdict(deque)\n            for i, rank in enumerate(sorted(ranks)):\n                rank_to_indices[rank].append(i)\n            permute_indices = [rank_to_indices[rank].popleft() for rank in ranks]\n            for feature_name in table.feature_names:\n                if shared_feature[feature_name]:\n                    self._features_to_permute_indices[\n                        feature_name + \"@\" + table.name\n                    ] = permute_indices\n                else:\n                    self._features_to_permute_indices[feature_name] = permute_indices\n\n    def _create_hash_size_info(\n        self,\n        feature_names: List[str],\n    ) -> None:\n        feature_index = 0\n        for i, sharding in enumerate(self._sharding_type_to_sharding.values()):\n            feature_hash_size: List[int] = []\n            feature_hash_size_lengths: List[int] = []\n            for table in sharding.embedding_tables():\n                table_hash_size = [0] * table.num_features()\n                table_hash_size[-1] = table.num_embeddings\n                feature_hash_size.extend(table_hash_size)\n\n                table_hash_size = [0] * table.num_features()\n                table_hash_size[0] = table.num_features()\n                feature_hash_size_lengths.extend(table_hash_size)\n\n                # Sanity check for feature orders\n                for f in range(table.num_features()):\n                    assert feature_names[feature_index + f] == table.feature_names[f]\n                feature_index += table.num_features()\n\n            feature_hash_size_cumsum: List[int] = [0] + list(\n                accumulate(feature_hash_size)\n            )\n            feature_hash_size_offset: List[int] = [0] + list(\n                accumulate(feature_hash_size_lengths)\n            )\n\n            # Register buffers for this shard\n            self.register_buffer(\n                f\"_hash_size_cumsum_tensor_{i}\",\n                torch.tensor(\n                    feature_hash_size_cumsum, device=self._device, dtype=torch.int64\n                ),\n                persistent=False,\n            )\n            self.register_buffer(\n                f\"_hash_size_offset_tensor_{i}\",\n                torch.tensor(\n                    feature_hash_size_offset, device=self._device, dtype=torch.int64\n                ),\n                persistent=False,\n            )\n\n    def _create_input_dist(\n        self,\n        input_feature_names: List[str],\n    ) -> None:\n        feature_names: List[str] = []\n        self._feature_splits: List[int] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._input_dists.append(sharding.create_input_dist())\n            feature_names.extend(sharding.feature_names())\n            self._feature_splits.append(len(sharding.feature_names()))\n        self._features_order: List[int] = []\n        for f in feature_names:\n            self._features_order.append(input_feature_names.index(f))\n        self._features_order = (\n            []\n            if self._features_order == list(range(len(self._features_order)))\n            else self._features_order\n        )\n        self.register_buffer(\n            \"_features_order_tensor\",\n            torch.tensor(self._features_order, device=self._device, dtype=torch.int32),\n            persistent=False,\n        )\n\n        if self._ec_index_dedup:\n            self._create_hash_size_info(feature_names)\n\n    def _create_lookups(self) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._lookups.append(sharding.create_lookup())\n\n    def _create_output_dist(\n        self,\n    ) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._output_dists.append(sharding.create_output_dist())\n\n    # pyre-ignore [14]\n    def input_dist(\n        self,\n        ctx: EmbeddingCollectionContext,\n        features: KeyedJaggedTensor,\n    ) -> Awaitable[Awaitable[KJTList]]:\n        if features.variable_stride_per_key():\n            raise ValueError(\n                \"Variable batch per feature is not supported with EmbeddingCollection\"\n            )\n        if self._has_uninitialized_input_dist:\n            self._create_input_dist(input_feature_names=features.keys())\n            self._has_uninitialized_input_dist = False\n        with torch.no_grad():\n            if self._features_order:\n                features = features.permute(\n                    self._features_order,\n                    self._features_order_tensor,\n                )\n\n            input_feature_splits = features.split(\n                self._feature_splits,\n            )\n\n            if not self._ec_index_dedup:\n                features_by_shards = input_feature_splits\n            else:\n                with record_function(\"## dedup_ec_indices ##\"):\n                    features_by_shards = []\n                    for i, input_feature in enumerate(input_feature_splits):\n                        hash_size_cumsum = self.get_buffer(\n                            f\"_hash_size_cumsum_tensor_{i}\"\n                        )\n                        hash_size_offset = self.get_buffer(\n                            f\"_hash_size_offset_tensor_{i}\"\n                        )\n                        (\n                            lengths,\n                            offsets,\n                            unique_indices,\n                            reverse_indices,\n                        ) = torch.ops.fbgemm.jagged_unique_indices(\n                            hash_size_cumsum,\n                            hash_size_offset,\n                            input_feature.offsets().to(torch.int64),\n                            input_feature.values().to(torch.int64),\n                        )\n                        dedup_features = KeyedJaggedTensor(\n                            keys=input_feature.keys(),\n                            lengths=lengths,\n                            offsets=offsets,\n                            values=unique_indices,\n                        )\n\n                        ctx.input_features.append(input_feature)\n                        ctx.reverse_indices.append(reverse_indices)\n                        features_by_shards.append(dedup_features)\n\n            awaitables = []\n            for input_dist, features in zip(self._input_dists, features_by_shards):\n                awaitables.append(input_dist(features))\n                ctx.sharding_contexts.append(\n                    SequenceShardingContext(\n                        features_before_input_dist=features,\n                        unbucketize_permute_tensor=input_dist.unbucketize_permute_tensor\n                        if isinstance(input_dist, RwSparseFeaturesDist)\n                        else None,\n                    )\n                )\n        return KJTListSplitsAwaitable(awaitables, ctx)\n\n    def compute(\n        self, ctx: EmbeddingCollectionContext, dist_input: KJTList\n    ) -> List[torch.Tensor]:\n        ret: List[torch.Tensor] = []\n        for lookup, features, sharding_ctx, sharding_type in zip(\n            self._lookups,\n            dist_input,\n            ctx.sharding_contexts,\n            self._sharding_type_to_sharding,\n        ):\n            sharding_ctx.lengths_after_input_dist = features.lengths().view(\n                -1, features.stride()\n            )\n            embedding_dim = self._embedding_dim_for_sharding_type(sharding_type)\n            ret.append(lookup(features).view(-1, embedding_dim))\n        return ret\n\n    def output_dist(\n        self, ctx: EmbeddingCollectionContext, output: List[torch.Tensor]\n    ) -> LazyAwaitable[Dict[str, JaggedTensor]]:\n        awaitables_per_sharding: List[Awaitable[torch.Tensor]] = []\n        features_before_all2all_per_sharding: List[KeyedJaggedTensor] = []\n        for odist, embeddings, sharding_ctx in zip(\n            self._output_dists,\n            output,\n            ctx.sharding_contexts,\n        ):\n            awaitables_per_sharding.append(odist(embeddings, sharding_ctx))\n            features_before_all2all_per_sharding.append(\n                # pyre-fixme[6]: For 1st argument expected `KeyedJaggedTensor` but\n                #  got `Optional[KeyedJaggedTensor]`.\n                sharding_ctx.features_before_input_dist\n            )\n        return EmbeddingCollectionAwaitable(\n            awaitables_per_sharding=awaitables_per_sharding,\n            features_per_sharding=features_before_all2all_per_sharding,\n            embedding_names_per_sharding=self._embedding_names_per_sharding,\n            need_indices=self._need_indices,\n            features_to_permute_indices=self._features_to_permute_indices,\n            ctx=ctx,\n        )\n\n    def compute_and_output_dist(\n        self, ctx: EmbeddingCollectionContext, input: KJTList\n    ) -> LazyAwaitable[Dict[str, JaggedTensor]]:\n        awaitables_per_sharding: List[Awaitable[torch.Tensor]] = []\n        features_before_all2all_per_sharding: List[KeyedJaggedTensor] = []\n        for lookup, odist, features, sharding_ctx, sharding_type in zip(\n            self._lookups,\n            self._output_dists,\n            input,\n            ctx.sharding_contexts,\n            self._sharding_type_to_sharding,\n        ):\n            sharding_ctx.lengths_after_input_dist = features.lengths().view(\n                -1, features.stride()\n            )\n            embedding_dim = self._embedding_dim_for_sharding_type(sharding_type)\n            awaitables_per_sharding.append(\n                odist(lookup(features).view(-1, embedding_dim), sharding_ctx)\n            )\n            features_before_all2all_per_sharding.append(\n                # pyre-fixme[6]: For 1st argument expected `KeyedJaggedTensor` but\n                #  got `Optional[KeyedJaggedTensor]`.\n                sharding_ctx.features_before_input_dist\n            )\n        return EmbeddingCollectionAwaitable(\n            awaitables_per_sharding=awaitables_per_sharding,\n            features_per_sharding=features_before_all2all_per_sharding,\n            embedding_names_per_sharding=self._embedding_names_per_sharding,\n            need_indices=self._need_indices,\n            features_to_permute_indices=self._features_to_permute_indices,\n            ctx=ctx,\n        )\n\n    def _embedding_dim_for_sharding_type(self, sharding_type: str) -> int:\n        return (\n            self._local_embedding_dim\n            if sharding_type == ShardingType.COLUMN_WISE.value\n            else self._embedding_dim\n        )\n\n    @property\n    def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim\n\n    def create_context(self) -> EmbeddingCollectionContext:\n        return EmbeddingCollectionContext(sharding_contexts=[])",
  "class EmbeddingCollectionSharder(BaseEmbeddingSharder[EmbeddingCollection]):\n    \"\"\"\n    This implementation uses non-fused EmbeddingCollection\n    \"\"\"\n\n    def shard(\n        self,\n        module: EmbeddingCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingCollection:\n        return ShardedEmbeddingCollection(\n            module,\n            params,\n            env,\n            self.fused_params,\n            device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )\n\n    def shardable_parameters(\n        self, module: EmbeddingCollection\n    ) -> Dict[str, nn.Parameter]:\n        return {\n            name.split(\".\")[0]: param\n            for name, param in module.embeddings.named_parameters()\n        }\n\n    def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.DATA_PARALLEL.value,\n            ShardingType.TABLE_WISE.value,\n            ShardingType.COLUMN_WISE.value,\n            ShardingType.ROW_WISE.value,\n        ]\n        return types\n\n    @property\n    def module_type(self) -> Type[EmbeddingCollection]:\n        return EmbeddingCollection",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for ctx in self.sharding_contexts:\n            ctx.record_stream(stream)\n        for f in self.input_features:\n            f.record_stream(stream)\n        for r in self.reverse_indices:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            r.record_stream(stream)",
  "def __init__(\n        self,\n        awaitables_per_sharding: List[Awaitable[torch.Tensor]],\n        features_per_sharding: List[KeyedJaggedTensor],\n        embedding_names_per_sharding: List[List[str]],\n        ctx: EmbeddingCollectionContext,\n        need_indices: bool = False,\n        features_to_permute_indices: Optional[Dict[str, List[int]]] = None,\n    ) -> None:\n        super().__init__()\n        self._awaitables_per_sharding = awaitables_per_sharding\n        self._features_per_sharding = features_per_sharding\n        self._need_indices = need_indices\n        self._features_to_permute_indices = features_to_permute_indices\n        self._embedding_names_per_sharding = embedding_names_per_sharding\n        self._ctx = ctx",
  "def _wait_impl(self) -> Dict[str, JaggedTensor]:\n        jt_dict: Dict[str, JaggedTensor] = {}\n        for i, (w, f, e) in enumerate(\n            zip(\n                self._awaitables_per_sharding,\n                self._features_per_sharding,\n                self._embedding_names_per_sharding,\n            )\n        ):\n            original_features = (\n                None\n                if i >= len(self._ctx.input_features)\n                else self._ctx.input_features[i]\n            )\n            reverse_indices = (\n                None\n                if i >= len(self._ctx.reverse_indices)\n                else self._ctx.reverse_indices[i]\n            )\n            jt_dict.update(\n                _construct_jagged_tensors(\n                    embeddings=w.wait(),\n                    features=f,\n                    embedding_names=e,\n                    need_indices=self._need_indices,\n                    features_to_permute_indices=self._features_to_permute_indices,\n                    original_features=original_features,\n                    reverse_indices=reverse_indices,\n                )\n            )\n        return jt_dict",
  "def __init__(\n        self,\n        module: EmbeddingCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._embedding_configs: List[EmbeddingConfig] = module.embedding_configs()\n        self._table_names: List[str] = [\n            config.name for config in self._embedding_configs\n        ]\n        self.module_sharding_plan: EmbeddingModuleShardingPlan = cast(\n            EmbeddingModuleShardingPlan,\n            {\n                table_name: parameter_sharding\n                for table_name, parameter_sharding in table_name_to_parameter_sharding.items()\n                if table_name in self._table_names\n            },\n        )\n\n        self._env = env\n        sharding_type_to_sharding_infos = create_sharding_infos_by_sharding(\n            module,\n            table_name_to_parameter_sharding,\n            fused_params,\n        )\n        self._sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                SequenceShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n            ],\n        ] = {\n            sharding_type: create_embedding_sharding(\n                sharding_type=sharding_type,\n                sharding_infos=embedding_confings,\n                env=env,\n                device=device,\n                qcomm_codecs_registry=self.qcomm_codecs_registry,\n            )\n            for sharding_type, embedding_confings in sharding_type_to_sharding_infos.items()\n        }\n\n        self._device = device\n        self._input_dists: List[nn.Module] = []\n        self._lookups: List[nn.Module] = []\n        self._create_lookups()\n        self._output_dists: List[nn.Module] = []\n        self._create_output_dist()\n\n        self._feature_splits: List[int] = []\n        self._features_order: List[int] = []\n\n        self._has_uninitialized_input_dist: bool = True\n        self._ec_index_dedup: bool = get_ec_index_dedup()\n        logger.info(f\"EC index dedup enabled: {self._ec_index_dedup}.\")\n\n        # Get all fused optimizers and combine them.\n        optims = []\n        for lookup in self._lookups:\n            for _, m in lookup.named_modules():\n                if isinstance(m, FusedOptimizerModule):\n                    # modify param keys to match EmbeddingCollection\n                    params: MutableMapping[str, Union[torch.Tensor, ShardedTensor]] = {}\n                    for param_key, weight in m.fused_optimizer.params.items():\n                        params[\"embeddings.\" + param_key] = weight\n                    m.fused_optimizer.params = params\n                    optims.append((\"\", m.fused_optimizer))\n        self._optim: CombinedOptimizer = CombinedOptimizer(optims)\n        self._embedding_dim: int = module.embedding_dim()\n        self._embedding_names_per_sharding: List[List[str]] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._embedding_names_per_sharding.append(sharding.embedding_names())\n        self._local_embedding_dim: int = self._embedding_dim\n        self._features_to_permute_indices: Dict[str, List[int]] = {}\n        if ShardingType.COLUMN_WISE.value in self._sharding_type_to_sharding:\n            sharding = self._sharding_type_to_sharding[ShardingType.COLUMN_WISE.value]\n            # CW partition must be same for all CW sharded parameters\n            self._local_embedding_dim = cast(\n                ShardMetadata, sharding.embedding_shard_metadata()[0]\n            ).shard_sizes[1]\n            self._generate_permute_indices_per_feature(\n                module.embedding_configs(), table_name_to_parameter_sharding\n            )\n        self._need_indices: bool = module.need_indices()\n\n        for index, (sharding, lookup) in enumerate(\n            zip(\n                self._sharding_type_to_sharding.values(),\n                self._lookups,\n            )\n        ):\n            # TODO: can move this into DpPooledEmbeddingSharding once all modules are composable\n            if isinstance(sharding, DpSequenceEmbeddingSharding):\n                self._lookups[index] = DistributedDataParallel(\n                    module=lookup,\n                    device_ids=[device]\n                    if self._device and self._device.type == \"gpu\"\n                    else None,\n                    process_group=env.process_group,\n                    gradient_as_bucket_view=True,\n                    broadcast_buffers=True,\n                    static_graph=True,\n                )\n        self._initialize_torch_state()\n\n        if module.device != torch.device(\"meta\"):\n            self.load_state_dict(module.state_dict())",
  "def _pre_load_state_dict_hook(\n        self: \"ShardedEmbeddingCollection\",\n        state_dict: Dict[str, Any],\n        prefix: str,\n        *args: Any,\n    ) -> None:\n        \"\"\"\n        Modify the destination state_dict for model parallel\n        to transform from ShardedTensors into tensors\n        \"\"\"\n        for (\n            table_name,\n            model_shards,\n        ) in self._model_parallel_name_to_local_shards.items():\n            key = f\"{prefix}embeddings.{table_name}.weight\"\n\n            # If state_dict[key] is already a ShardedTensor, use its local shards\n            if isinstance(state_dict[key], ShardedTensor):\n                local_shards = state_dict[key].local_shards()\n                # If no local shards, create an empty tensor\n                if len(local_shards) == 0:\n                    state_dict[key] = torch.empty(0)\n                else:\n                    dim = state_dict[key].metadata().shards_metadata[0].shard_sizes[1]\n                    # CW multiple shards are merged\n                    if len(local_shards) > 1:\n                        state_dict[key] = torch.cat(\n                            [s.tensor.view(-1) for s in local_shards], dim=0\n                        ).view(-1, dim)\n                    else:\n                        state_dict[key] = local_shards[0].tensor.view(-1, dim)\n            else:\n                local_shards = []\n                for shard in model_shards:\n                    # Extract shard size and offsets for splicing\n                    shard_sizes = shard.metadata.shard_sizes\n                    shard_offsets = shard.metadata.shard_offsets\n\n                    # Prepare tensor by splicing and placing on appropriate device\n                    spliced_tensor = state_dict[key][\n                        shard_offsets[0] : shard_offsets[0] + shard_sizes[0],\n                        shard_offsets[1] : shard_offsets[1] + shard_sizes[1],\n                    ].to(shard.tensor.get_device())\n\n                    # Append spliced tensor into local shards\n                    local_shards.append(spliced_tensor)\n\n                state_dict[key] = (\n                    torch.empty(0)\n                    if not local_shards\n                    else torch.cat(local_shards, dim=0)\n                )",
  "def _initialize_torch_state(self) -> None:  # noqa\n        \"\"\"\n        This provides consistency between this class and the EmbeddingCollection's\n        nn.Module API calls (state_dict, named_modules, etc)\n        \"\"\"\n\n        self.embeddings: nn.ModuleDict = nn.ModuleDict()\n        for table_name in self._table_names:\n            self.embeddings[table_name] = nn.Module()\n        self._model_parallel_name_to_local_shards = OrderedDict()\n        self._model_parallel_name_to_sharded_tensor = OrderedDict()\n        model_parallel_name_to_compute_kernel: Dict[str, str] = {}\n        for (\n            table_name,\n            parameter_sharding,\n        ) in self.module_sharding_plan.items():\n            if parameter_sharding.sharding_type == ShardingType.DATA_PARALLEL.value:\n                continue\n            self._model_parallel_name_to_local_shards[table_name] = []\n            model_parallel_name_to_compute_kernel[\n                table_name\n            ] = parameter_sharding.compute_kernel\n\n        self._name_to_table_size = {}\n        for table in self._embedding_configs:\n            self._name_to_table_size[table.name] = (\n                table.num_embeddings,\n                table.embedding_dim,\n            )\n\n        for sharding_type, lookup in zip(\n            self._sharding_type_to_sharding.keys(), self._lookups\n        ):\n            if sharding_type == ShardingType.DATA_PARALLEL.value:\n                # unwrap DDP\n                lookup = lookup.module\n            else:\n                # save local_shards for transforming MP params to shardedTensor\n                for key, v in lookup.state_dict().items():\n                    table_name = key[: -len(\".weight\")]\n                    self._model_parallel_name_to_local_shards[table_name].extend(\n                        v.local_shards()\n                    )\n            for (\n                table_name,\n                tbe_slice,\n            ) in lookup.named_parameters_by_table():\n                self.embeddings[table_name].register_parameter(\"weight\", tbe_slice)\n        for (\n            table_name,\n            local_shards,\n        ) in self._model_parallel_name_to_local_shards.items():\n            # for shards that don't exist on this rank, register with empty tensor\n            if not hasattr(self.embeddings[table_name], \"weight\"):\n                self.embeddings[table_name].register_parameter(\n                    \"weight\", nn.Parameter(torch.empty(0))\n                )\n                if (\n                    model_parallel_name_to_compute_kernel[table_name]\n                    != EmbeddingComputeKernel.DENSE.value\n                ):\n                    self.embeddings[table_name].weight._in_backward_optimizers = [\n                        EmptyFusedOptimizer()\n                    ]\n            # created ShardedTensors once in init, use in post_state_dict_hook\n            self._model_parallel_name_to_sharded_tensor[\n                table_name\n            ] = ShardedTensor._init_from_local_shards(\n                local_shards,\n                self._name_to_table_size[table_name],\n                process_group=self._env.process_group,\n            )\n\n        def post_state_dict_hook(\n            module: ShardedEmbeddingCollection,\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            # Adjust dense MP\n            for (\n                table_name,\n                sharded_t,\n            ) in module._model_parallel_name_to_sharded_tensor.items():\n                destination_key = f\"{prefix}embeddings.{table_name}.weight\"\n                destination[destination_key] = sharded_t\n\n        self._register_state_dict_hook(post_state_dict_hook)\n        self._register_load_state_dict_pre_hook(\n            self._pre_load_state_dict_hook, with_module=True\n        )\n\n        self.reset_parameters()",
  "def reset_parameters(self) -> None:\n        if self._device and self._device.type == \"meta\":\n            return\n        # Initialize embedding weights with init_fn\n        for table_config in self._embedding_configs:\n            assert table_config.init_fn is not None\n            param = self.embeddings[f\"{table_config.name}\"].weight\n            # pyre-ignore\n            table_config.init_fn(param)",
  "def _generate_permute_indices_per_feature(\n        self,\n        embedding_configs: List[EmbeddingConfig],\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n    ) -> None:\n        \"\"\"\n        Generates permute indices per feature for column-wise sharding.\n\n        Since outputs are stored in order of rank, column-wise shards of a table on the\n        same rank will be seen as adjacent, which may not be correct.\n\n        The permute indices store the correct ordering of outputs relative to the\n        provided ordering.\n\n        Example::\n            rank_0 = [f_0(shard_0), f_0(shard_2)]\n            rank_1 = [f_0(shard_1)]\n            output = [f_0(shard_0), f_0(shard_2), f_0(shard_1)]\n\n            shard_ranks = [0, 1, 0]\n            output_ranks = [0, 0, 1]\n\n            # To get the correct order from output_ranks -> shard_ranks\n            permute_indices = [0, 2, 1]\n        \"\"\"\n        shared_feature: Dict[str, bool] = {}\n        for table in embedding_configs:\n            for feature_name in table.feature_names:\n                if feature_name not in shared_feature:\n                    shared_feature[feature_name] = False\n                else:\n                    shared_feature[feature_name] = True\n\n        for table in embedding_configs:\n            sharding = table_name_to_parameter_sharding[table.name]\n            if sharding.sharding_type != ShardingType.COLUMN_WISE.value:\n                continue\n            ranks = cast(List[int], sharding.ranks)\n            rank_to_indices = defaultdict(deque)\n            for i, rank in enumerate(sorted(ranks)):\n                rank_to_indices[rank].append(i)\n            permute_indices = [rank_to_indices[rank].popleft() for rank in ranks]\n            for feature_name in table.feature_names:\n                if shared_feature[feature_name]:\n                    self._features_to_permute_indices[\n                        feature_name + \"@\" + table.name\n                    ] = permute_indices\n                else:\n                    self._features_to_permute_indices[feature_name] = permute_indices",
  "def _create_hash_size_info(\n        self,\n        feature_names: List[str],\n    ) -> None:\n        feature_index = 0\n        for i, sharding in enumerate(self._sharding_type_to_sharding.values()):\n            feature_hash_size: List[int] = []\n            feature_hash_size_lengths: List[int] = []\n            for table in sharding.embedding_tables():\n                table_hash_size = [0] * table.num_features()\n                table_hash_size[-1] = table.num_embeddings\n                feature_hash_size.extend(table_hash_size)\n\n                table_hash_size = [0] * table.num_features()\n                table_hash_size[0] = table.num_features()\n                feature_hash_size_lengths.extend(table_hash_size)\n\n                # Sanity check for feature orders\n                for f in range(table.num_features()):\n                    assert feature_names[feature_index + f] == table.feature_names[f]\n                feature_index += table.num_features()\n\n            feature_hash_size_cumsum: List[int] = [0] + list(\n                accumulate(feature_hash_size)\n            )\n            feature_hash_size_offset: List[int] = [0] + list(\n                accumulate(feature_hash_size_lengths)\n            )\n\n            # Register buffers for this shard\n            self.register_buffer(\n                f\"_hash_size_cumsum_tensor_{i}\",\n                torch.tensor(\n                    feature_hash_size_cumsum, device=self._device, dtype=torch.int64\n                ),\n                persistent=False,\n            )\n            self.register_buffer(\n                f\"_hash_size_offset_tensor_{i}\",\n                torch.tensor(\n                    feature_hash_size_offset, device=self._device, dtype=torch.int64\n                ),\n                persistent=False,\n            )",
  "def _create_input_dist(\n        self,\n        input_feature_names: List[str],\n    ) -> None:\n        feature_names: List[str] = []\n        self._feature_splits: List[int] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._input_dists.append(sharding.create_input_dist())\n            feature_names.extend(sharding.feature_names())\n            self._feature_splits.append(len(sharding.feature_names()))\n        self._features_order: List[int] = []\n        for f in feature_names:\n            self._features_order.append(input_feature_names.index(f))\n        self._features_order = (\n            []\n            if self._features_order == list(range(len(self._features_order)))\n            else self._features_order\n        )\n        self.register_buffer(\n            \"_features_order_tensor\",\n            torch.tensor(self._features_order, device=self._device, dtype=torch.int32),\n            persistent=False,\n        )\n\n        if self._ec_index_dedup:\n            self._create_hash_size_info(feature_names)",
  "def _create_lookups(self) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._lookups.append(sharding.create_lookup())",
  "def _create_output_dist(\n        self,\n    ) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._output_dists.append(sharding.create_output_dist())",
  "def input_dist(\n        self,\n        ctx: EmbeddingCollectionContext,\n        features: KeyedJaggedTensor,\n    ) -> Awaitable[Awaitable[KJTList]]:\n        if features.variable_stride_per_key():\n            raise ValueError(\n                \"Variable batch per feature is not supported with EmbeddingCollection\"\n            )\n        if self._has_uninitialized_input_dist:\n            self._create_input_dist(input_feature_names=features.keys())\n            self._has_uninitialized_input_dist = False\n        with torch.no_grad():\n            if self._features_order:\n                features = features.permute(\n                    self._features_order,\n                    self._features_order_tensor,\n                )\n\n            input_feature_splits = features.split(\n                self._feature_splits,\n            )\n\n            if not self._ec_index_dedup:\n                features_by_shards = input_feature_splits\n            else:\n                with record_function(\"## dedup_ec_indices ##\"):\n                    features_by_shards = []\n                    for i, input_feature in enumerate(input_feature_splits):\n                        hash_size_cumsum = self.get_buffer(\n                            f\"_hash_size_cumsum_tensor_{i}\"\n                        )\n                        hash_size_offset = self.get_buffer(\n                            f\"_hash_size_offset_tensor_{i}\"\n                        )\n                        (\n                            lengths,\n                            offsets,\n                            unique_indices,\n                            reverse_indices,\n                        ) = torch.ops.fbgemm.jagged_unique_indices(\n                            hash_size_cumsum,\n                            hash_size_offset,\n                            input_feature.offsets().to(torch.int64),\n                            input_feature.values().to(torch.int64),\n                        )\n                        dedup_features = KeyedJaggedTensor(\n                            keys=input_feature.keys(),\n                            lengths=lengths,\n                            offsets=offsets,\n                            values=unique_indices,\n                        )\n\n                        ctx.input_features.append(input_feature)\n                        ctx.reverse_indices.append(reverse_indices)\n                        features_by_shards.append(dedup_features)\n\n            awaitables = []\n            for input_dist, features in zip(self._input_dists, features_by_shards):\n                awaitables.append(input_dist(features))\n                ctx.sharding_contexts.append(\n                    SequenceShardingContext(\n                        features_before_input_dist=features,\n                        unbucketize_permute_tensor=input_dist.unbucketize_permute_tensor\n                        if isinstance(input_dist, RwSparseFeaturesDist)\n                        else None,\n                    )\n                )\n        return KJTListSplitsAwaitable(awaitables, ctx)",
  "def compute(\n        self, ctx: EmbeddingCollectionContext, dist_input: KJTList\n    ) -> List[torch.Tensor]:\n        ret: List[torch.Tensor] = []\n        for lookup, features, sharding_ctx, sharding_type in zip(\n            self._lookups,\n            dist_input,\n            ctx.sharding_contexts,\n            self._sharding_type_to_sharding,\n        ):\n            sharding_ctx.lengths_after_input_dist = features.lengths().view(\n                -1, features.stride()\n            )\n            embedding_dim = self._embedding_dim_for_sharding_type(sharding_type)\n            ret.append(lookup(features).view(-1, embedding_dim))\n        return ret",
  "def output_dist(\n        self, ctx: EmbeddingCollectionContext, output: List[torch.Tensor]\n    ) -> LazyAwaitable[Dict[str, JaggedTensor]]:\n        awaitables_per_sharding: List[Awaitable[torch.Tensor]] = []\n        features_before_all2all_per_sharding: List[KeyedJaggedTensor] = []\n        for odist, embeddings, sharding_ctx in zip(\n            self._output_dists,\n            output,\n            ctx.sharding_contexts,\n        ):\n            awaitables_per_sharding.append(odist(embeddings, sharding_ctx))\n            features_before_all2all_per_sharding.append(\n                # pyre-fixme[6]: For 1st argument expected `KeyedJaggedTensor` but\n                #  got `Optional[KeyedJaggedTensor]`.\n                sharding_ctx.features_before_input_dist\n            )\n        return EmbeddingCollectionAwaitable(\n            awaitables_per_sharding=awaitables_per_sharding,\n            features_per_sharding=features_before_all2all_per_sharding,\n            embedding_names_per_sharding=self._embedding_names_per_sharding,\n            need_indices=self._need_indices,\n            features_to_permute_indices=self._features_to_permute_indices,\n            ctx=ctx,\n        )",
  "def compute_and_output_dist(\n        self, ctx: EmbeddingCollectionContext, input: KJTList\n    ) -> LazyAwaitable[Dict[str, JaggedTensor]]:\n        awaitables_per_sharding: List[Awaitable[torch.Tensor]] = []\n        features_before_all2all_per_sharding: List[KeyedJaggedTensor] = []\n        for lookup, odist, features, sharding_ctx, sharding_type in zip(\n            self._lookups,\n            self._output_dists,\n            input,\n            ctx.sharding_contexts,\n            self._sharding_type_to_sharding,\n        ):\n            sharding_ctx.lengths_after_input_dist = features.lengths().view(\n                -1, features.stride()\n            )\n            embedding_dim = self._embedding_dim_for_sharding_type(sharding_type)\n            awaitables_per_sharding.append(\n                odist(lookup(features).view(-1, embedding_dim), sharding_ctx)\n            )\n            features_before_all2all_per_sharding.append(\n                # pyre-fixme[6]: For 1st argument expected `KeyedJaggedTensor` but\n                #  got `Optional[KeyedJaggedTensor]`.\n                sharding_ctx.features_before_input_dist\n            )\n        return EmbeddingCollectionAwaitable(\n            awaitables_per_sharding=awaitables_per_sharding,\n            features_per_sharding=features_before_all2all_per_sharding,\n            embedding_names_per_sharding=self._embedding_names_per_sharding,\n            need_indices=self._need_indices,\n            features_to_permute_indices=self._features_to_permute_indices,\n            ctx=ctx,\n        )",
  "def _embedding_dim_for_sharding_type(self, sharding_type: str) -> int:\n        return (\n            self._local_embedding_dim\n            if sharding_type == ShardingType.COLUMN_WISE.value\n            else self._embedding_dim\n        )",
  "def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim",
  "def create_context(self) -> EmbeddingCollectionContext:\n        return EmbeddingCollectionContext(sharding_contexts=[])",
  "def shard(\n        self,\n        module: EmbeddingCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingCollection:\n        return ShardedEmbeddingCollection(\n            module,\n            params,\n            env,\n            self.fused_params,\n            device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def shardable_parameters(\n        self, module: EmbeddingCollection\n    ) -> Dict[str, nn.Parameter]:\n        return {\n            name.split(\".\")[0]: param\n            for name, param in module.embeddings.named_parameters()\n        }",
  "def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.DATA_PARALLEL.value,\n            ShardingType.TABLE_WISE.value,\n            ShardingType.COLUMN_WISE.value,\n            ShardingType.ROW_WISE.value,\n        ]\n        return types",
  "def module_type(self) -> Type[EmbeddingCollection]:\n        return EmbeddingCollection",
  "def post_state_dict_hook(\n            module: ShardedEmbeddingCollection,\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            # Adjust dense MP\n            for (\n                table_name,\n                sharded_t,\n            ) in module._model_parallel_name_to_sharded_tensor.items():\n                destination_key = f\"{prefix}embeddings.{table_name}.weight\"\n                destination[destination_key] = sharded_t",
  "def none_throws(optional: Optional[_T], message: str = \"Unexpected `None`\") -> _T:\n    \"\"\"Convert an optional to its value. Raises an `AssertionError` if the\n    value is `None`\"\"\"\n    if optional is None:\n        raise AssertionError(message)\n    return optional",
  "def append_prefix(prefix: str, name: str) -> str:\n    \"\"\"\n    Appends provided prefix to provided name.\n    \"\"\"\n\n    if prefix != \"\" and name != \"\":\n        return prefix + \".\" + name\n    else:\n        return prefix + name",
  "def filter_state_dict(\n    state_dict: \"OrderedDict[str, torch.Tensor]\", name: str\n) -> \"OrderedDict[str, torch.Tensor]\":\n    \"\"\"\n    Filters state dict for keys that start with provided name.\n    Strips provided name from beginning of key in the resulting state dict.\n\n    Args:\n        state_dict (OrderedDict[str, torch.Tensor]): input state dict to filter.\n        name (str): name to filter from state dict keys.\n\n    Returns:\n        OrderedDict[str, torch.Tensor]: filtered state dict.\n    \"\"\"\n\n    filtered_state_dict = OrderedDict()\n    for key, value in state_dict.items():\n        if key.startswith(name + \".\"):\n            # + 1 to length is to remove the '.' after the key\n            filtered_state_dict[key[len(name) + 1 :]] = value\n    return filtered_state_dict",
  "def add_prefix_to_state_dict(state_dict: Dict[str, Any], prefix: str) -> None:\n    \"\"\"\n    Adds prefix to all keys in state dict, in place.\n\n    Args:\n        state_dict (Dict[str, Any]): input state dict to update.\n        prefix (str): name to filter from state dict keys.\n\n    Returns:\n        None.\n    \"\"\"\n    keys = sorted(state_dict.keys())\n    for key in keys:\n        state_dict[prefix + key] = state_dict.pop(key)\n\n    if \"_metadata\" in state_dict:\n        metadata = state_dict[\"_metadata\"]\n        for key in list(metadata.keys()):\n            if len(key) == 0:\n                continue\n            metadata[prefix + key] = metadata.pop(key)",
  "def _get_unsharded_module_names_helper(\n    model: torch.nn.Module,\n    path: str,\n    unsharded_module_names: Set[str],\n) -> bool:\n    sharded_children = set()\n    for name, child in model.named_children():\n        curr_path = path + name\n        if isinstance(child, ShardedModule):\n            sharded_children.add(name)\n        else:\n            child_sharded = _get_unsharded_module_names_helper(\n                child,\n                curr_path + \".\",\n                unsharded_module_names,\n            )\n            if child_sharded:\n                sharded_children.add(name)\n\n    if len(sharded_children) > 0:\n        for name, _ in model.named_children():\n            if name not in sharded_children:\n                unsharded_module_names.add(path + name)\n\n    return len(sharded_children) > 0",
  "def get_unsharded_module_names(model: torch.nn.Module) -> List[str]:\n    \"\"\"\n    Retrieves names of top level modules that do not contain any sharded sub-modules.\n\n    Args:\n        model (torch.nn.Module): model to retrieve unsharded module names from.\n\n    Returns:\n        List[str]: list of names of modules that don't have sharded sub-modules.\n    \"\"\"\n\n    unsharded_module_names: Set[str] = set()\n    _get_unsharded_module_names_helper(\n        model,\n        \"\",\n        unsharded_module_names,\n    )\n    return list(unsharded_module_names)",
  "class sharded_model_copy:\n    \"\"\"\n    Allows copying of DistributedModelParallel module to a target device.\n\n    Example::\n\n        # Copying model to CPU.\n\n        m = DistributedModelParallel(m)\n        with sharded_model_copy(\"cpu\"):\n            m_cpu = copy.deepcopy(m)\n    \"\"\"\n\n    def __init__(self, device: Optional[Union[str, int, torch.device]]) -> None:\n        self.device = device\n\n    def __enter__(self) -> None:\n        # pyre-ignore [16]\n        self.t_copy_save_ = torch.Tensor.__deepcopy__\n        # pyre-ignore [16]\n        self.p_copy_save_ = torch.nn.Parameter.__deepcopy__\n\n        device = self.device\n\n        # pyre-ignore [2, 3, 53]\n        def _tensor_copy(tensor, memo):\n            if tensor.device != device:\n                return tensor.detach().to(device)\n            else:\n                return tensor.detach().clone()\n\n        # pyre-ignore [2, 3]\n        def _no_copy(obj, memo):\n            return obj\n\n        _copy_or_not = _tensor_copy if self.device is not None else _no_copy\n\n        # pyre-ignore [2, 3, 53]\n        def _param_copy(param, memo):\n            return torch.nn.Parameter(\n                _copy_or_not(param, memo), requires_grad=param.requires_grad\n            )\n\n        torch.Tensor.__deepcopy__ = _copy_or_not\n        torch.nn.Parameter.__deepcopy__ = _param_copy\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.ProcessGroupNCCL.__deepcopy__ = _no_copy\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.ProcessGroupGloo.__deepcopy__ = _no_copy\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.Work.__deepcopy__ = _no_copy\n        # pyre-ignore [16]\n        torch.cuda.streams.Stream.__deepcopy__ = _no_copy\n\n    # pyre-ignore [2]\n    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n        # pyre-ignore [16]\n        torch.Tensor.__deepcopy__ = self.t_copy_save_\n        # pyre-ignore [16]\n        torch.nn.Parameter.__deepcopy__ = self.p_copy_save_\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.ProcessGroupNCCL.__deepcopy__ = None\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.ProcessGroupGloo.__deepcopy__ = None\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.Work.__deepcopy__ = None\n        # pyre-ignore [16]\n        torch.cuda.streams.Stream.__deepcopy__ = None",
  "def copy_to_device(\n    module: nn.Module,\n    current_device: torch.device,\n    to_device: torch.device,\n) -> nn.Module:\n\n    with sharded_model_copy(device=None):\n        copy_module = copy.deepcopy(module)\n\n    # Copy only weights with matching device.\n    def _copy_if_device_match(tensor: torch.Tensor) -> torch.Tensor:\n        if tensor.device == current_device:\n            return tensor.to(to_device)\n        return tensor\n\n    # if this is a sharded module, customize the copy\n    if isinstance(copy_module, CopyMixIn):\n        return copy_module.copy(to_device)\n    copied_param = {\n        name: torch.nn.Parameter(\n            _copy_if_device_match(param.data), requires_grad=param.requires_grad\n        )\n        for name, param in copy_module.named_parameters(recurse=False)\n    }\n    copied_buffer = {\n        name: _copy_if_device_match(buffer)\n        for name, buffer in copy_module.named_buffers(recurse=False)\n    }\n    for name, param in copied_param.items():\n        copy_module.register_parameter(name, param)\n    for name, buffer in copied_buffer.items():\n        copy_module.register_buffer(name, buffer)\n    for child_name, child in copy_module.named_children():\n        if not any([isinstance(submodule, CopyMixIn) for submodule in child.modules()]):\n            child_copy = child._apply(_copy_if_device_match)\n        else:\n            child_copy = copy_to_device(child, current_device, to_device)\n        copy_module.register_module(child_name, child_copy)\n    return copy_module",
  "class CopyableMixin(nn.Module):\n    \"\"\"\n    Allows copying of module to a target device.\n\n    Example::\n\n        class MyModule(CopyableMixin):\n            ...\n\n    Args:\n        device : torch.device to copy to\n\n    Returns\n        nn.Module on new device\n    \"\"\"\n\n    def copy(\n        self,\n        device: torch.device,\n    ) -> nn.Module:\n        return copy_to_device(\n            self,\n            current_device=torch.device(\"cpu\"),\n            to_device=device,\n        )",
  "def optimizer_type_to_emb_opt_type(\n    optimizer_class: Type[torch.optim.Optimizer],\n) -> Optional[EmbOptimType]:\n    # TODO add more optimizers to be in parity with ones provided by FBGEMM\n    # TODO kwargs accepted by fbgemm and and canonical optimizers are different\n    # may need to add special handling for them\n    lookup = {\n        torch.optim.SGD: EmbOptimType.EXACT_SGD,\n        torch.optim.Adagrad: EmbOptimType.EXACT_ADAGRAD,\n        torch.optim.Adam: EmbOptimType.ADAM,\n        # below are torchrec wrappers over these optims.\n        # they accept an **unused kwargs portion, that let us set FBGEMM specific args such as\n        # max gradient, etc\n        trec_optim.SGD: EmbOptimType.EXACT_SGD,\n        trec_optim.LarsSGD: EmbOptimType.LARS_SGD,\n        trec_optim.LAMB: EmbOptimType.LAMB,\n        trec_optim.PartialRowWiseLAMB: EmbOptimType.PARTIAL_ROWWISE_LAMB,\n        trec_optim.Adam: EmbOptimType.ADAM,\n        trec_optim.PartialRowWiseAdam: EmbOptimType.PARTIAL_ROWWISE_ADAM,\n        trec_optim.Adagrad: EmbOptimType.EXACT_ADAGRAD,\n        trec_optim.RowWiseAdagrad: EmbOptimType.EXACT_ROWWISE_ADAGRAD,\n    }\n    if optimizer_class not in lookup:\n        raise ValueError(f\"Cannot cast {optimizer_class} to an EmbOptimType\")\n    return lookup[optimizer_class]",
  "def merge_fused_params(\n    fused_params: Optional[Dict[str, Any]] = None,\n    param_fused_params: Optional[Dict[str, Any]] = None,\n) -> Dict[str, Any]:\n\n    \"\"\"\n    Configure the fused_params including cache_precision if the value is not preset.\n\n    Values set in table_level_fused_params take precidence over the global fused_params\n\n    Args:\n        fused_params (Optional[Dict[str, Any]]): the original fused_params\n        grouped_fused_params\n\n    Returns:\n        [Dict[str, Any]]: a non-null configured fused_params dictionary to be\n        used to configure the embedding lookup kernel\n    \"\"\"\n\n    if fused_params is None:\n        fused_params = {}\n    if param_fused_params is None:\n        param_fused_params = {}\n    if \"lr\" in param_fused_params:\n        param_fused_params[\"learning_rate\"] = param_fused_params.pop(\"lr\")\n\n    _fused_params = copy.deepcopy(fused_params)\n    _fused_params.update(param_fused_params)\n    return _fused_params",
  "def add_params_from_parameter_sharding(\n    fused_params: Optional[Dict[str, Any]],\n    parameter_sharding: ParameterSharding,\n) -> Dict[str, Any]:\n    \"\"\"\n    Extract params from parameter sharding and then add them to fused_params.\n\n    Params from parameter sharding will override the ones in fused_params if they\n    exist already.\n\n    Args:\n        fused_params (Optional[Dict[str, Any]]): the existing fused_params\n        parameter_sharding (ParameterSharding): the parameter sharding to use\n\n    Returns:\n        [Dict[str, Any]]: the fused_params dictionary with params from parameter\n        sharding added.\n\n    \"\"\"\n    if fused_params is None:\n        fused_params = {}\n\n    # update fused_params using params from parameter_sharding\n    # this will take precidence over the fused_params provided from sharders\n    if parameter_sharding.cache_params is not None:\n        cache_params = parameter_sharding.cache_params\n        if cache_params.algorithm is not None:\n            fused_params[\"cache_algorithm\"] = cache_params.algorithm\n        if cache_params.load_factor is not None:\n            fused_params[\"cache_load_factor\"] = cache_params.load_factor\n        if cache_params.reserved_memory is not None:\n            fused_params[\"cache_reserved_memory\"] = cache_params.reserved_memory\n        if cache_params.precision is not None:\n            fused_params[\"cache_precision\"] = cache_params.precision\n\n    if parameter_sharding.enforce_hbm is not None:\n        fused_params[\"enforce_hbm\"] = parameter_sharding.enforce_hbm\n\n    if parameter_sharding.stochastic_rounding is not None:\n        fused_params[\"stochastic_rounding\"] = parameter_sharding.stochastic_rounding\n\n    if parameter_sharding.bounds_check_mode is not None:\n        fused_params[\"bounds_check_mode\"] = parameter_sharding.bounds_check_mode\n\n    # print warning if sharding_type is data_parallel or kernel is dense\n    if parameter_sharding.sharding_type == ShardingType.DATA_PARALLEL.value:\n        logger.warning(\n            f\"Sharding Type is {parameter_sharding.sharding_type}, \"\n            \"caching params will be ignored\"\n        )\n    elif parameter_sharding.compute_kernel == EmbeddingComputeKernel.DENSE.value:\n        logger.warning(\n            f\"Compute Kernel is {parameter_sharding.compute_kernel}, \"\n            \"caching params will be ignored\"\n        )\n\n    return fused_params",
  "def convert_to_fbgemm_types(fused_params: Dict[str, Any]) -> Dict[str, Any]:\n    if \"cache_precision\" in fused_params:\n        if isinstance(fused_params[\"cache_precision\"], DataType):\n            fused_params[\"cache_precision\"] = data_type_to_sparse_type(\n                fused_params[\"cache_precision\"]\n            )\n\n    if \"cache_algorithm\" in fused_params:\n        if isinstance(fused_params[\"cache_algorithm\"], CacheAlgorithm):\n            fused_params[\"cache_algorithm\"] = to_fbgemm_cache_algorithm(\n                fused_params[\"cache_algorithm\"]\n            )\n\n    if \"bounds_check_mode\" in fused_params:\n        if isinstance(fused_params[\"bounds_check_mode\"], BoundsCheckMode):\n            fused_params[\"bounds_check_mode\"] = to_fbgemm_bounds_check_mode(\n                fused_params[\"bounds_check_mode\"]\n            )\n\n    return fused_params",
  "def init_parameters(module: nn.Module, device: torch.device) -> None:\n    @torch.no_grad()\n    def init_parameters(module: nn.Module) -> None:\n        # Allocate parameters and buffers if over 'meta' device.\n        has_meta_param = False\n        for name, param in module._parameters.items():\n            if isinstance(param, torch.Tensor) and param.device.type == \"meta\":\n                module._parameters[name] = nn.Parameter(\n                    torch.empty_like(param, device=device),\n                    requires_grad=param.requires_grad,\n                )\n                has_meta_param = True\n        for name, buffer in module._buffers.items():\n            if isinstance(buffer, torch.Tensor) and buffer.device.type == \"meta\":\n                module._buffers[name] = torch.empty_like(buffer, device=device)\n\n        # Init parameters if at least one parameter is over 'meta' device.\n        if has_meta_param and hasattr(module, \"reset_parameters\"):\n            module.reset_parameters()\n\n    module.apply(init_parameters)",
  "def __init__(self, device: Optional[Union[str, int, torch.device]]) -> None:\n        self.device = device",
  "def __enter__(self) -> None:\n        # pyre-ignore [16]\n        self.t_copy_save_ = torch.Tensor.__deepcopy__\n        # pyre-ignore [16]\n        self.p_copy_save_ = torch.nn.Parameter.__deepcopy__\n\n        device = self.device\n\n        # pyre-ignore [2, 3, 53]\n        def _tensor_copy(tensor, memo):\n            if tensor.device != device:\n                return tensor.detach().to(device)\n            else:\n                return tensor.detach().clone()\n\n        # pyre-ignore [2, 3]\n        def _no_copy(obj, memo):\n            return obj\n\n        _copy_or_not = _tensor_copy if self.device is not None else _no_copy\n\n        # pyre-ignore [2, 3, 53]\n        def _param_copy(param, memo):\n            return torch.nn.Parameter(\n                _copy_or_not(param, memo), requires_grad=param.requires_grad\n            )\n\n        torch.Tensor.__deepcopy__ = _copy_or_not\n        torch.nn.Parameter.__deepcopy__ = _param_copy\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.ProcessGroupNCCL.__deepcopy__ = _no_copy\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.ProcessGroupGloo.__deepcopy__ = _no_copy\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.Work.__deepcopy__ = _no_copy\n        # pyre-ignore [16]\n        torch.cuda.streams.Stream.__deepcopy__ = _no_copy",
  "def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n        # pyre-ignore [16]\n        torch.Tensor.__deepcopy__ = self.t_copy_save_\n        # pyre-ignore [16]\n        torch.nn.Parameter.__deepcopy__ = self.p_copy_save_\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.ProcessGroupNCCL.__deepcopy__ = None\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.ProcessGroupGloo.__deepcopy__ = None\n        # pyre-fixme[16]: `Type` has no attribute `__deepcopy__`.\n        torch._C._distributed_c10d.Work.__deepcopy__ = None\n        # pyre-ignore [16]\n        torch.cuda.streams.Stream.__deepcopy__ = None",
  "def _copy_if_device_match(tensor: torch.Tensor) -> torch.Tensor:\n        if tensor.device == current_device:\n            return tensor.to(to_device)\n        return tensor",
  "def copy(\n        self,\n        device: torch.device,\n    ) -> nn.Module:\n        return copy_to_device(\n            self,\n            current_device=torch.device(\"cpu\"),\n            to_device=device,\n        )",
  "def init_parameters(module: nn.Module) -> None:\n        # Allocate parameters and buffers if over 'meta' device.\n        has_meta_param = False\n        for name, param in module._parameters.items():\n            if isinstance(param, torch.Tensor) and param.device.type == \"meta\":\n                module._parameters[name] = nn.Parameter(\n                    torch.empty_like(param, device=device),\n                    requires_grad=param.requires_grad,\n                )\n                has_meta_param = True\n        for name, buffer in module._buffers.items():\n            if isinstance(buffer, torch.Tensor) and buffer.device.type == \"meta\":\n                module._buffers[name] = torch.empty_like(buffer, device=device)\n\n        # Init parameters if at least one parameter is over 'meta' device.\n        if has_meta_param and hasattr(module, \"reset_parameters\"):\n            module.reset_parameters()",
  "def _tensor_copy(tensor, memo):\n            if tensor.device != device:\n                return tensor.detach().to(device)\n            else:\n                return tensor.detach().clone()",
  "def _no_copy(obj, memo):\n            return obj",
  "def _param_copy(param, memo):\n            return torch.nn.Parameter(\n                _copy_or_not(param, memo), requires_grad=param.requires_grad\n            )",
  "class ManagedCollisionEmbeddingBagCollectionContext(EmbeddingBagCollectionContext):\n    evictions_per_table: Optional[Dict[str, Optional[torch.Tensor]]] = None\n    remapped_kjt: Optional[KJTList] = None\n\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        super().record_stream(stream)\n        if self.evictions_per_table:\n            #  pyre-ignore\n            for value in self.evictions_per_table.values():\n                if value is None:\n                    continue\n                value.record_stream(stream)\n        if self.remapped_kjt is not None:\n            self.remapped_kjt.record_stream(stream)",
  "class ShardedManagedCollisionEmbeddingBagCollection(\n    ShardedEmbeddingModule[\n        KJTList,\n        List[torch.Tensor],\n        Tuple[LazyAwaitable[KeyedTensor], LazyAwaitable[Optional[KeyedJaggedTensor]]],\n        ManagedCollisionEmbeddingBagCollectionContext,\n    ]\n):\n    def __init__(\n        self,\n        module: ManagedCollisionEmbeddingBagCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        ebc_sharder: EmbeddingBagCollectionSharder,\n        mc_sharder: ManagedCollisionCollectionSharder,\n        # TODO - maybe we need this to manage unsharded/sharded consistency/state consistency\n        env: ShardingEnv,\n        device: torch.device,\n    ) -> None:\n        super().__init__()\n\n        self._device = device\n        self._env = env\n\n        self._embedding_bag_collection: ShardedEmbeddingBagCollection = (\n            ebc_sharder.shard(\n                module._embedding_bag_collection,\n                table_name_to_parameter_sharding,\n                env=env,\n                device=device,\n            )\n        )\n        self._managed_collision_collection: ShardedManagedCollisionCollection = mc_sharder.shard(\n            module._managed_collision_collection,\n            table_name_to_parameter_sharding,\n            env=env,\n            device=device,\n            sharding_type_to_sharding=self._embedding_bag_collection._sharding_type_to_sharding,\n        )\n        self._return_remapped_features: bool = module._return_remapped_features\n\n        # pyre-ignore\n        self._table_to_tbe_and_index = {}\n        for lookup in self._embedding_bag_collection._lookups:\n            for emb_module in lookup._emb_modules:\n                for table_idx, table in enumerate(emb_module._config.embedding_tables):\n                    self._table_to_tbe_and_index[table.name] = (\n                        emb_module._emb_module,\n                        torch.tensor([table_idx], dtype=torch.int, device=self._device),\n                    )\n        self._buffer_ids: torch.Tensor = torch.tensor(\n            [0], device=self._device, dtype=torch.int\n        )\n\n    # pyre-ignore\n    def input_dist(\n        self,\n        ctx: ManagedCollisionEmbeddingBagCollectionContext,\n        features: KeyedJaggedTensor,\n        force_insert: bool = False,\n    ) -> Awaitable[Awaitable[KJTList]]:\n        # TODO: resolve incompatiblity with different contexts\n        return self._managed_collision_collection.input_dist(\n            # pyre-fixme [6]\n            ctx,\n            features,\n            force_insert,\n        )\n\n    def _evict(self, evictions_per_table: Dict[str, Optional[torch.Tensor]]) -> None:\n        for table, evictions_indices_for_table in evictions_per_table.items():\n            if evictions_indices_for_table is not None:\n                (tbe, logical_table_ids) = self._table_to_tbe_and_index[table]\n                pruned_indices_offsets = torch.tensor(\n                    [0, evictions_indices_for_table.shape[0]],\n                    dtype=torch.long,\n                    device=self._device,\n                )\n                logger.info(\n                    f\"Evicting {evictions_indices_for_table.numel()} ids from {table}\"\n                )\n                with torch.no_grad():\n                    # embeddings, and optimizer state will be reset\n                    tbe.reset_embedding_weight_momentum(\n                        pruned_indices=evictions_indices_for_table.long(),\n                        pruned_indices_offsets=pruned_indices_offsets,\n                        logical_table_ids=logical_table_ids,\n                        buffer_ids=self._buffer_ids,\n                    )\n                    table_weight_param = (\n                        self._embedding_bag_collection.embedding_bags.get_parameter(\n                            f\"{table}.weight\"\n                        )\n                    )\n\n                    init_fn = self._embedding_bag_collection._table_name_to_config[\n                        table\n                    ].init_fn\n\n                    # pyre-ignore\n                    # Set evicted indices to original init_fn instead of all zeros\n                    table_weight_param[evictions_indices_for_table] = init_fn(\n                        table_weight_param[evictions_indices_for_table]\n                    )\n\n    def compute(\n        self,\n        ctx: ManagedCollisionEmbeddingBagCollectionContext,\n        dist_input: KJTList,\n    ) -> List[torch.Tensor]:\n        with record_function(\"## compute:mcc ##\"):\n            remapped_kjt = self._managed_collision_collection.compute(\n                # pyre-fixme [6]\n                ctx,\n                dist_input,\n            )\n            evictions_per_table = self._managed_collision_collection.evict()\n\n            self._evict(evictions_per_table)\n            ctx.remapped_kjt = remapped_kjt\n            ctx.evictions_per_table = evictions_per_table\n\n            return self._embedding_bag_collection.compute(ctx, remapped_kjt)\n\n    # pyre-ignore\n    def output_dist(\n        self,\n        ctx: ManagedCollisionEmbeddingBagCollectionContext,\n        output: List[torch.Tensor],\n    ) -> Tuple[LazyAwaitable[KeyedTensor], LazyAwaitable[Optional[KeyedJaggedTensor]]]:\n\n        ebc_awaitable = self._embedding_bag_collection.output_dist(ctx, output)\n\n        if self._return_remapped_features:\n            kjt_awaitable = self._managed_collision_collection.output_dist(\n                # pyre-fixme [6]\n                ctx,\n                # pyre-ignore [6]\n                ctx.remapped_kjt,\n            )\n        else:\n            kjt_awaitable = NoWait(None)\n\n        # pyre-ignore\n        return ebc_awaitable, kjt_awaitable\n\n    def create_context(self) -> ManagedCollisionEmbeddingBagCollectionContext:\n        return ManagedCollisionEmbeddingBagCollectionContext(sharding_contexts=[])\n\n    def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        for fqn, _ in self.named_parameters():\n            yield append_prefix(prefix, fqn)\n        for fqn, _ in self.named_buffers():\n            yield append_prefix(prefix, fqn)",
  "class ManagedCollisionEmbeddingBagCollectionSharder(\n    BaseEmbeddingSharder[ManagedCollisionEmbeddingBagCollection]\n):\n    def __init__(\n        self,\n        ebc_sharder: Optional[EmbeddingBagCollectionSharder] = None,\n        mc_sharder: Optional[ManagedCollisionCollectionSharder] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._ebc_sharder: EmbeddingBagCollectionSharder = (\n            ebc_sharder or EmbeddingBagCollectionSharder(self.qcomm_codecs_registry)\n        )\n        self._mc_sharder: ManagedCollisionCollectionSharder = (\n            mc_sharder or ManagedCollisionCollectionSharder()\n        )\n\n    def shard(\n        self,\n        module: ManagedCollisionEmbeddingBagCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedManagedCollisionEmbeddingBagCollection:\n\n        if device is None:\n            device = torch.device(\"cuda\")\n\n        return ShardedManagedCollisionEmbeddingBagCollection(\n            module,\n            params,\n            ebc_sharder=self._ebc_sharder,\n            mc_sharder=self._mc_sharder,\n            env=env,\n            device=device,\n        )\n\n    def shardable_parameters(\n        self, module: ManagedCollisionEmbeddingBagCollection\n    ) -> Dict[str, torch.nn.Parameter]:\n        return self._ebc_sharder.shardable_parameters(module._embedding_bag_collection)\n\n    @property\n    def module_type(self) -> Type[ManagedCollisionEmbeddingBagCollection]:\n        return ManagedCollisionEmbeddingBagCollection\n\n    def compute_kernels(\n        self,\n        sharding_type: str,\n        compute_device_type: str,\n    ) -> List[str]:\n        return [EmbeddingComputeKernel.FUSED.value]\n\n    def sharding_types(self, compute_device_type: str) -> List[str]:\n        return list(\n            set.intersection(\n                set(self._ebc_sharder.sharding_types(compute_device_type)),\n                set(self._mc_sharder.sharding_types(compute_device_type)),\n            )\n        )",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        super().record_stream(stream)\n        if self.evictions_per_table:\n            #  pyre-ignore\n            for value in self.evictions_per_table.values():\n                if value is None:\n                    continue\n                value.record_stream(stream)\n        if self.remapped_kjt is not None:\n            self.remapped_kjt.record_stream(stream)",
  "def __init__(\n        self,\n        module: ManagedCollisionEmbeddingBagCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        ebc_sharder: EmbeddingBagCollectionSharder,\n        mc_sharder: ManagedCollisionCollectionSharder,\n        # TODO - maybe we need this to manage unsharded/sharded consistency/state consistency\n        env: ShardingEnv,\n        device: torch.device,\n    ) -> None:\n        super().__init__()\n\n        self._device = device\n        self._env = env\n\n        self._embedding_bag_collection: ShardedEmbeddingBagCollection = (\n            ebc_sharder.shard(\n                module._embedding_bag_collection,\n                table_name_to_parameter_sharding,\n                env=env,\n                device=device,\n            )\n        )\n        self._managed_collision_collection: ShardedManagedCollisionCollection = mc_sharder.shard(\n            module._managed_collision_collection,\n            table_name_to_parameter_sharding,\n            env=env,\n            device=device,\n            sharding_type_to_sharding=self._embedding_bag_collection._sharding_type_to_sharding,\n        )\n        self._return_remapped_features: bool = module._return_remapped_features\n\n        # pyre-ignore\n        self._table_to_tbe_and_index = {}\n        for lookup in self._embedding_bag_collection._lookups:\n            for emb_module in lookup._emb_modules:\n                for table_idx, table in enumerate(emb_module._config.embedding_tables):\n                    self._table_to_tbe_and_index[table.name] = (\n                        emb_module._emb_module,\n                        torch.tensor([table_idx], dtype=torch.int, device=self._device),\n                    )\n        self._buffer_ids: torch.Tensor = torch.tensor(\n            [0], device=self._device, dtype=torch.int\n        )",
  "def input_dist(\n        self,\n        ctx: ManagedCollisionEmbeddingBagCollectionContext,\n        features: KeyedJaggedTensor,\n        force_insert: bool = False,\n    ) -> Awaitable[Awaitable[KJTList]]:\n        # TODO: resolve incompatiblity with different contexts\n        return self._managed_collision_collection.input_dist(\n            # pyre-fixme [6]\n            ctx,\n            features,\n            force_insert,\n        )",
  "def _evict(self, evictions_per_table: Dict[str, Optional[torch.Tensor]]) -> None:\n        for table, evictions_indices_for_table in evictions_per_table.items():\n            if evictions_indices_for_table is not None:\n                (tbe, logical_table_ids) = self._table_to_tbe_and_index[table]\n                pruned_indices_offsets = torch.tensor(\n                    [0, evictions_indices_for_table.shape[0]],\n                    dtype=torch.long,\n                    device=self._device,\n                )\n                logger.info(\n                    f\"Evicting {evictions_indices_for_table.numel()} ids from {table}\"\n                )\n                with torch.no_grad():\n                    # embeddings, and optimizer state will be reset\n                    tbe.reset_embedding_weight_momentum(\n                        pruned_indices=evictions_indices_for_table.long(),\n                        pruned_indices_offsets=pruned_indices_offsets,\n                        logical_table_ids=logical_table_ids,\n                        buffer_ids=self._buffer_ids,\n                    )\n                    table_weight_param = (\n                        self._embedding_bag_collection.embedding_bags.get_parameter(\n                            f\"{table}.weight\"\n                        )\n                    )\n\n                    init_fn = self._embedding_bag_collection._table_name_to_config[\n                        table\n                    ].init_fn\n\n                    # pyre-ignore\n                    # Set evicted indices to original init_fn instead of all zeros\n                    table_weight_param[evictions_indices_for_table] = init_fn(\n                        table_weight_param[evictions_indices_for_table]\n                    )",
  "def compute(\n        self,\n        ctx: ManagedCollisionEmbeddingBagCollectionContext,\n        dist_input: KJTList,\n    ) -> List[torch.Tensor]:\n        with record_function(\"## compute:mcc ##\"):\n            remapped_kjt = self._managed_collision_collection.compute(\n                # pyre-fixme [6]\n                ctx,\n                dist_input,\n            )\n            evictions_per_table = self._managed_collision_collection.evict()\n\n            self._evict(evictions_per_table)\n            ctx.remapped_kjt = remapped_kjt\n            ctx.evictions_per_table = evictions_per_table\n\n            return self._embedding_bag_collection.compute(ctx, remapped_kjt)",
  "def output_dist(\n        self,\n        ctx: ManagedCollisionEmbeddingBagCollectionContext,\n        output: List[torch.Tensor],\n    ) -> Tuple[LazyAwaitable[KeyedTensor], LazyAwaitable[Optional[KeyedJaggedTensor]]]:\n\n        ebc_awaitable = self._embedding_bag_collection.output_dist(ctx, output)\n\n        if self._return_remapped_features:\n            kjt_awaitable = self._managed_collision_collection.output_dist(\n                # pyre-fixme [6]\n                ctx,\n                # pyre-ignore [6]\n                ctx.remapped_kjt,\n            )\n        else:\n            kjt_awaitable = NoWait(None)\n\n        # pyre-ignore\n        return ebc_awaitable, kjt_awaitable",
  "def create_context(self) -> ManagedCollisionEmbeddingBagCollectionContext:\n        return ManagedCollisionEmbeddingBagCollectionContext(sharding_contexts=[])",
  "def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        for fqn, _ in self.named_parameters():\n            yield append_prefix(prefix, fqn)\n        for fqn, _ in self.named_buffers():\n            yield append_prefix(prefix, fqn)",
  "def __init__(\n        self,\n        ebc_sharder: Optional[EmbeddingBagCollectionSharder] = None,\n        mc_sharder: Optional[ManagedCollisionCollectionSharder] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._ebc_sharder: EmbeddingBagCollectionSharder = (\n            ebc_sharder or EmbeddingBagCollectionSharder(self.qcomm_codecs_registry)\n        )\n        self._mc_sharder: ManagedCollisionCollectionSharder = (\n            mc_sharder or ManagedCollisionCollectionSharder()\n        )",
  "def shard(\n        self,\n        module: ManagedCollisionEmbeddingBagCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedManagedCollisionEmbeddingBagCollection:\n\n        if device is None:\n            device = torch.device(\"cuda\")\n\n        return ShardedManagedCollisionEmbeddingBagCollection(\n            module,\n            params,\n            ebc_sharder=self._ebc_sharder,\n            mc_sharder=self._mc_sharder,\n            env=env,\n            device=device,\n        )",
  "def shardable_parameters(\n        self, module: ManagedCollisionEmbeddingBagCollection\n    ) -> Dict[str, torch.nn.Parameter]:\n        return self._ebc_sharder.shardable_parameters(module._embedding_bag_collection)",
  "def module_type(self) -> Type[ManagedCollisionEmbeddingBagCollection]:\n        return ManagedCollisionEmbeddingBagCollection",
  "def compute_kernels(\n        self,\n        sharding_type: str,\n        compute_device_type: str,\n    ) -> List[str]:\n        return [EmbeddingComputeKernel.FUSED.value]",
  "def sharding_types(self, compute_device_type: str) -> List[str]:\n        return list(\n            set.intersection(\n                set(self._ebc_sharder.sharding_types(compute_device_type)),\n                set(self._mc_sharder.sharding_types(compute_device_type)),\n            )\n        )",
  "def _replace_sharding_with_intra_node(\n    table_name_to_parameter_sharding: Dict[str, ParameterSharding], local_size: int\n) -> None:\n    for _, value in table_name_to_parameter_sharding.items():\n        if value.sharding_type == ShardingType.TABLE_ROW_WISE.value:\n            value.sharding_type = ShardingType.ROW_WISE.value\n        elif value.sharding_type == ShardingType.TABLE_COLUMN_WISE.value:\n            value.sharding_type = ShardingType.COLUMN_WISE.value\n        else:\n            raise ValueError(f\"Sharding type not supported {value.sharding_type}\")\n        if value.ranks:\n            value.ranks = [rank % local_size for rank in value.ranks]\n        if value.sharding_spec:\n            # pyre-ignore [6, 16]\n            for (shard, rank) in zip(value.sharding_spec.shards, value.ranks):\n                shard.placement._rank = rank",
  "class TowerLazyAwaitable(LazyAwaitable[torch.Tensor]):\n    def __init__(\n        self,\n        awaitable: PooledEmbeddingsAwaitable,\n    ) -> None:\n        super().__init__()\n        self._awaitable = awaitable\n\n    def _wait_impl(self) -> torch.Tensor:\n        return self._awaitable.wait()",
  "class EmbeddingTowerCollectionContext(Multistreamable):\n    embedding_contexts: List[NullShardedModuleContext] = field(default_factory=list)\n\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for ctx in self.embedding_contexts:\n            ctx.record_stream(stream)",
  "class ShardedEmbeddingTower(\n    ShardedEmbeddingModule[\n        KJTList,\n        torch.Tensor,\n        torch.Tensor,\n        NullShardedModuleContext,\n    ],\n    FusedOptimizerModule,\n):\n    def __init__(\n        self,\n        module: EmbeddingTower,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        embedding_sharder: BaseEmbeddingSharder[nn.Module],\n        kjt_features: List[str],\n        wkjt_features: List[str],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        intra_pg, cross_pg = intra_and_cross_node_pg(device)\n        self._intra_pg: Optional[dist.ProcessGroup] = intra_pg\n        self._cross_pg: Optional[dist.ProcessGroup] = cross_pg\n        self._device = device\n        self._output_dist: Optional[PooledEmbeddingsAllToAll] = None\n        self._cross_pg_global_batch_size: int = 0\n        self._cross_pg_world_size: int = dist.get_world_size(self._cross_pg)\n\n        self._has_uninitialized_output_dist = True\n\n        # make sure all sharding on single physical node\n        devices_per_host = dist.get_world_size(intra_pg)\n        tower_devices = set()\n        for sharding in table_name_to_parameter_sharding.values():\n            # pyre-ignore [6]\n            tower_devices.update(sharding.ranks)\n        host = {tower_device // devices_per_host for tower_device in tower_devices}\n        assert len(host) == 1, f\"{tower_devices}, {table_name_to_parameter_sharding}\"\n        self._tower_node: int = next(iter(host))\n        self._active_device: bool = {dist.get_rank() // devices_per_host} == host\n\n        # input_dist\n        self._kjt_feature_names: List[str] = kjt_features\n        self._wkjt_feature_names: List[str] = wkjt_features\n        self._has_uninitialized_input_dist: bool = True\n        self._cross_dist: nn.Module = nn.Module()\n        self._weighted_cross_dist: nn.Module = nn.Module()\n        self._kjt_features_order: List[int] = []\n        self._wkjt_features_order: List[int] = []\n        self._has_kjt_features_permute: bool = False\n        self._has_wkjt_features_permute: bool = False\n\n        self.embedding: Optional[nn.Module] = None\n        self.interaction: Optional[nn.Module] = None\n        if self._active_device:\n            _replace_sharding_with_intra_node(\n                table_name_to_parameter_sharding,\n                dist.get_world_size(self._intra_pg),\n            )\n            intra_env: ShardingEnv = ShardingEnv(\n                world_size=dist.get_world_size(self._intra_pg),\n                rank=dist.get_rank(self._intra_pg),\n                pg=self._intra_pg,\n            )\n            # shard embedding module\n            self.embedding = embedding_sharder.shard(\n                module.embedding,\n                table_name_to_parameter_sharding,\n                intra_env,\n                device,\n            )\n            # Hierarchical DDP\n            self.interaction = DistributedDataParallel(\n                module=module.interaction.to(self._device),\n                device_ids=[self._device],\n                process_group=self._intra_pg,\n                gradient_as_bucket_view=True,\n                broadcast_buffers=False,\n            )\n\n        # Setup output dists for quantized comms\n        self._output_dists: nn.ModuleList = (\n            self.embedding._output_dists if self.embedding else nn.ModuleList()\n        )\n\n    def _create_input_dist(\n        self,\n        kjt_feature_names: List[str],\n        wkjt_feature_names: List[str],\n    ) -> None:\n        if self._kjt_feature_names != kjt_feature_names:\n            self._has_kjt_features_permute = True\n            for f in self._kjt_feature_names:\n                self._kjt_features_order.append(kjt_feature_names.index(f))\n            self.register_buffer(\n                \"_kjt_features_order_tensor\",\n                torch.tensor(\n                    self._kjt_features_order, device=self._device, dtype=torch.int32\n                ),\n                persistent=False,\n            )\n\n        if self._wkjt_feature_names != wkjt_feature_names:\n            self._has_wkjt_features_permute = True\n            for f in self._wkjt_feature_names:\n                self._wkjt_features_order.append(wkjt_feature_names.index(f))\n            self.register_buffer(\n                \"_wkjt_features_order_tensor\",\n                torch.tensor(\n                    self._wkjt_features_order, device=self._device, dtype=torch.int32\n                ),\n                persistent=False,\n            )\n\n        node_count = dist.get_world_size(self._cross_pg)\n        kjt_features_per_node = [\n            len(self._kjt_feature_names) if node == self._tower_node else 0\n            for node in range(node_count)\n        ]\n        wkjt_features_per_node = [\n            len(self._wkjt_feature_names) if node == self._tower_node else 0\n            for node in range(node_count)\n        ]\n        self._cross_dist = KJTAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._cross_pg,\n            kjt_features_per_node,\n        )\n        self._weighted_cross_dist = KJTAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._cross_pg,\n            wkjt_features_per_node,\n        )\n\n    # pyre-ignore[14]\n    def input_dist(\n        self,\n        ctx: NullShardedModuleContext,\n        features: KeyedJaggedTensor,\n        optional_features: Optional[KeyedJaggedTensor] = None,\n    ) -> Awaitable[Awaitable[KJTList]]:\n\n        # optional_features are populated only if both kjt and weighted kjt present in tower\n        if self._wkjt_feature_names and self._kjt_feature_names:\n            kjt_features = features\n            wkjt_features = optional_features\n        elif self._wkjt_feature_names:\n            kjt_features = None\n            wkjt_features = features\n        else:\n            kjt_features = features\n            wkjt_features = None\n\n        if self._has_uninitialized_input_dist:\n            self._cross_pg_global_batch_size = (\n                features.stride() * self._cross_pg_world_size\n            )\n            self._create_input_dist(\n                kjt_features.keys() if kjt_features else [],\n                wkjt_features.keys() if wkjt_features else [],\n            )\n            self._has_uninitialized_input_dist = False\n\n        with torch.no_grad():\n            if self._has_kjt_features_permute:\n                # pyre-ignore [16]\n                kjt_features = kjt_features.permute(\n                    self._kjt_features_order,\n                    self._kjt_features_order_tensor,\n                )\n            if self._has_wkjt_features_permute:\n                wkjt_features = wkjt_features.permute(\n                    self._wkjt_features_order,\n                    self._wkjt_features_order_tensor,\n                )\n\n            awaitables = []\n            if kjt_features is not None:\n                awaitables.append(self._cross_dist(kjt_features))\n            if wkjt_features is not None:\n                awaitables.append(self._weighted_cross_dist(wkjt_features))\n\n            return KJTListSplitsAwaitable(awaitables, ctx)\n\n    def compute(\n        self, ctx: NullShardedModuleContext, dist_input: KJTList\n    ) -> torch.Tensor:\n        if self._active_device:\n            if len(dist_input) == 2:\n                kjt_features = dist_input[0]\n                wkjt_features = dist_input[1]\n                # pyre-ignore [29]\n                embeddings = self.embedding(kjt_features, wkjt_features)\n            else:\n                # pyre-ignore [29]\n                embeddings = self.embedding(dist_input[0])\n            # pyre-ignore [29]\n            output = self.interaction(embeddings)\n        else:\n            output = torch.empty(\n                [self._cross_pg_global_batch_size, 0],\n                device=self._device,\n                requires_grad=True,\n            )\n        return output\n\n    def _create_output_dist(\n        self, ctx: NullShardedModuleContext, output: torch.Tensor\n    ) -> None:\n        # Determine the output_dist splits and the all_to_all output size\n        assert len(output.shape) == 2\n        local_dim_sum = torch.tensor(\n            [\n                output.shape[1],\n            ],\n            dtype=torch.int64,\n            device=self._device,\n        )\n        dim_sum_per_rank = [\n            torch.zeros(\n                1,\n                dtype=torch.int64,\n                device=self._device,\n            )\n            for i in range(dist.get_world_size(self._cross_pg))\n        ]\n        dist.all_gather(\n            dim_sum_per_rank,\n            local_dim_sum,\n            group=self._cross_pg,\n        )\n        dim_sum_per_rank = [x.item() for x in dim_sum_per_rank]\n        self._output_dist = PooledEmbeddingsAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            pg=self._cross_pg,\n            # pyre-fixme[6]: For 2nd param expected `List[int]` but got\n            #  `List[Union[bool, float, int]]`.\n            dim_sum_per_rank=dim_sum_per_rank,\n            device=self._device,\n            codecs=self.qcomm_codecs_registry.get(\n                CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name, None\n            )\n            if self.qcomm_codecs_registry\n            else None,\n        )\n\n    def output_dist(\n        self, ctx: NullShardedModuleContext, output: torch.Tensor\n    ) -> LazyAwaitable[torch.Tensor]:\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(ctx, output)\n            self._has_uninitialized_output_dist = False\n        # pyre-ignore [29]\n        return TowerLazyAwaitable(self._output_dist(output))\n\n    # pyre-ignore [14]\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n        if self._active_device:\n            # pyre-ignore [16]\n            self.embedding.state_dict(destination, prefix + \"embedding.\", keep_vars)\n            # pyre-ignore [16]\n            self.interaction.module.state_dict(\n                destination, prefix + \"interaction.\", keep_vars\n            )\n        return destination\n\n    @property\n    def fused_optimizer(self) -> KeyedOptimizer:\n        if self.embedding:\n            return self.embedding.fused_optimizer\n        else:\n            return CombinedOptimizer([])\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        if self._active_device:\n            # pyre-ignore[16]\n            yield from self.embedding.named_parameters(\n                append_prefix(prefix, \"embedding\"), recurse\n            )\n            # pyre-ignore[16]\n            yield from self.interaction.module.named_parameters(\n                append_prefix(prefix, \"interaction\"), recurse\n            )\n        else:\n            yield from ()\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        if self._active_device:\n            # pyre-ignore[16]\n            yield from self.embedding.named_buffers(\n                append_prefix(prefix, \"embedding\"), recurse\n            )\n            # pyre-ignore[16]\n            yield from self.interaction.module.named_buffers(\n                append_prefix(prefix, \"interaction\"), recurse\n            )\n        yield from ()\n\n    def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        if self._active_device:\n            # pyre-ignore[16]\n            yield from self.embedding.sharded_parameter_names(\n                append_prefix(prefix, \"embedding\")\n            )\n            # pyre-ignore[16]\n            for name, _ in self.interaction.module.named_parameters(\n                append_prefix(prefix, \"interaction\")\n            ):\n                yield name\n        else:\n            yield from ()\n\n    def named_modules(\n        self,\n        memo: Optional[Set[nn.Module]] = None,\n        prefix: str = \"\",\n        remove_duplicate: bool = True,\n    ) -> Iterator[Tuple[str, nn.Module]]:\n        yield from [(prefix, self)]\n\n    def create_context(self) -> NullShardedModuleContext:\n        return NullShardedModuleContext()",
  "class ShardedEmbeddingTowerCollection(\n    ShardedEmbeddingModule[\n        KJTList,\n        torch.Tensor,\n        torch.Tensor,\n        EmbeddingTowerCollectionContext,\n    ],\n    FusedOptimizerModule,\n):\n    def __init__(\n        self,\n        module: EmbeddingTowerCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        tower_sharder: BaseEmbeddingSharder[EmbeddingTower],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n\n        intra_pg, cross_pg = intra_and_cross_node_pg(device)\n        self._intra_pg: Optional[dist.ProcessGroup] = intra_pg\n        self._cross_pg: Optional[dist.ProcessGroup] = cross_pg\n        self._cross_pg_world_size: int = dist.get_world_size(self._cross_pg)\n        self._intra_pg_world_size: int = dist.get_world_size(self._intra_pg)\n        self._device = device\n        self._tower_id: int = dist.get_rank() // self._intra_pg_world_size\n        self._output_dist: Optional[PooledEmbeddingsAllToAll] = None\n        self._cross_pg_global_batch_size: int = 0\n        self._is_weighted: bool = False\n        self._has_uninitialized_input_dist: bool = True\n        self._has_uninitialized_output_dist: bool = True\n        self._kjt_features_order: List[int] = []\n        self._wkjt_features_order: List[int] = []\n        self._kjt_feature_names: List[str] = []\n        self._wkjt_feature_names: List[str] = []\n        self._kjt_num_features_per_pt: List[int] = []\n        self._wkjt_num_features_per_pt: List[int] = []\n        self._has_kjt_features_permute: bool = False\n        self._has_wkjt_features_permute: bool = False\n        self.embeddings: nn.ModuleDict = nn.ModuleDict()\n        self.interactions: nn.ModuleDict = nn.ModuleDict()\n        self.input_dist_params: List[Tuple[bool, bool]] = []\n        self._cross_dist: nn.Module = nn.Module()\n        self._weighted_cross_dist: nn.Module = nn.Module()\n\n        # groups parameter sharding into physical towers\n        tables_per_pt: List[Set[str]] = [\n            set() for _ in range(self._cross_pg_world_size)\n        ]\n        [\n            tables_per_pt[i].add(k)\n            for i in range(self._cross_pg_world_size)\n            for k, v in table_name_to_parameter_sharding.items()\n            # pyre-ignore [16]\n            if v.ranks[0] // self._intra_pg_world_size == i\n        ]\n\n        # create mapping of logical towers to physical towers\n        tables_per_lt: List[Set[str]] = []\n        for tower in module.towers:\n            lt_tables = set(tower_sharder.shardable_parameters(tower).keys())\n            tables_per_lt.append(lt_tables)\n            # check the tables in a logical tower are on same physical tower\n            found_physical_tower = False\n            for pt_tables in tables_per_pt:\n                if lt_tables.issubset(pt_tables):\n                    found_physical_tower = True\n                    break\n            assert (\n                found_physical_tower\n            ), f\"tables in a logical tower must be in the same physical tower, logical tower tables: {lt_tables}, tables_per_pt: {tables_per_pt}\"\n\n        logical_to_physical_order: List[List[int]] = [\n            [] for _ in range(self._cross_pg_world_size)\n        ]\n        feature_names_by_pt: List[Tuple[List[str], List[str]]] = [\n            ([], []) for _ in range(self._cross_pg_world_size)\n        ]\n\n        for i, pt_tables in enumerate(tables_per_pt):\n            found = False\n            for j, lt_tables in enumerate(tables_per_lt):\n                if lt_tables.issubset(pt_tables):\n                    logical_to_physical_order[i].append(j)\n                    found = True\n            if not found and pt_tables:\n                raise RuntimeError(\n                    f\"Could not find any towers with features: {pt_tables}\"\n                )\n\n        for pt_index, lt_on_pt in enumerate(logical_to_physical_order):\n            for lt_index in lt_on_pt:\n                # pyre-ignore [16]\n                kjt_features, wkjt_features = tower_sharder.embedding_feature_names(\n                    module.towers[lt_index]\n                )\n                feature_names_by_pt[pt_index][0].extend(kjt_features)\n                feature_names_by_pt[pt_index][1].extend(wkjt_features)\n\n        for kjt_names, wkjt_names in feature_names_by_pt:\n            self._kjt_feature_names.extend(kjt_names)\n            self._wkjt_feature_names.extend(wkjt_names)\n            self._kjt_num_features_per_pt.append(len(kjt_names))\n            self._wkjt_num_features_per_pt.append(len(wkjt_names))\n\n        local_towers: List[Tuple[str, EmbeddingTower]] = [\n            (str(i), tower)\n            for i, tower in enumerate(module.towers)\n            if i in logical_to_physical_order[self._tower_id]\n        ]\n\n        if local_towers:\n            _replace_sharding_with_intra_node(\n                table_name_to_parameter_sharding,\n                dist.get_world_size(self._intra_pg),\n            )\n            intra_env: ShardingEnv = ShardingEnv(\n                world_size=dist.get_world_size(self._intra_pg),\n                rank=dist.get_rank(self._intra_pg),\n                pg=self._intra_pg,\n            )\n            for i, tower in local_towers:\n                # pyre-ignore [16]\n                self.embeddings[i] = tower_sharder.embedding_sharder(tower).shard(\n                    tower.embedding,\n                    table_name_to_parameter_sharding,\n                    intra_env,\n                    device,\n                )\n                self.input_dist_params.append(tower_input_params(tower.embedding))\n                # Hierarchical DDP\n                self.interactions[i] = DistributedDataParallel(\n                    module=tower.interaction.to(self._device),\n                    device_ids=[self._device],\n                    process_group=self._intra_pg,\n                    gradient_as_bucket_view=True,\n                    broadcast_buffers=False,\n                    static_graph=True,\n                )\n\n        # Setup output dists for quantized comms\n        output_dists = nn.ModuleList()\n        for embedding in self.embeddings.values():\n            output_dists.extend(embedding._output_dists)\n        self._output_dists: nn.ModuleList = output_dists\n\n    def _create_input_dist(\n        self,\n        kjt_feature_names: List[str],\n        wkjt_feature_names: List[str],\n    ) -> None:\n\n        if self._kjt_feature_names != kjt_feature_names:\n            self._has_kjt_features_permute = True\n            for f in self._kjt_feature_names:\n                self._kjt_features_order.append(kjt_feature_names.index(f))\n            self.register_buffer(\n                \"_kjt_features_order_tensor\",\n                torch.tensor(\n                    self._kjt_features_order, device=self._device, dtype=torch.int32\n                ),\n            )\n\n        if self._wkjt_feature_names != wkjt_feature_names:\n            self._has_wkjt_features_permute = True\n            for f in self._wkjt_feature_names:\n                self._wkjt_features_order.append(wkjt_feature_names.index(f))\n            self.register_buffer(\n                \"_wkjt_features_order_tensor\",\n                torch.tensor(\n                    self._wkjt_features_order, device=self._device, dtype=torch.int32\n                ),\n            )\n\n        self._cross_dist = KJTAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._cross_pg,\n            self._kjt_num_features_per_pt,\n        )\n        self._weighted_cross_dist = KJTAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._cross_pg,\n            self._wkjt_num_features_per_pt,\n        )\n\n    # pyre-ignore [14]\n    def input_dist(\n        self,\n        ctx: EmbeddingTowerCollectionContext,\n        kjt_features: Optional[KeyedJaggedTensor] = None,\n        wkjt_features: Optional[KeyedJaggedTensor] = None,\n    ) -> Awaitable[Awaitable[KJTList]]:\n        if self._has_uninitialized_input_dist:\n            # pyre-ignore [16]\n            stride = kjt_features.stride() if kjt_features else wkjt_features.stride()\n            self._cross_pg_global_batch_size = stride * self._cross_pg_world_size\n            self._create_input_dist(\n                kjt_features.keys() if kjt_features else [],\n                wkjt_features.keys() if wkjt_features else [],\n            )\n            self._has_uninitialized_input_dist = False\n        with torch.no_grad():\n            if self._has_kjt_features_permute:\n                kjt_features = kjt_features.permute(  # pyre-ignore [16]\n                    self._kjt_features_order,\n                    cast(torch.Tensor, self._kjt_features_order_tensor),\n                )\n            if self._has_wkjt_features_permute:\n                wkjt_features = wkjt_features.permute(\n                    self._wkjt_features_order,\n                    cast(torch.Tensor, self._wkjt_features_order_tensor),\n                )\n            awaitables = []\n            if kjt_features is not None:\n                awaitables.append(self._cross_dist(kjt_features))\n            if wkjt_features is not None:\n                awaitables.append(self._weighted_cross_dist(wkjt_features))\n        return KJTListSplitsAwaitable(awaitables, ctx)\n\n    def compute(\n        self, ctx: EmbeddingTowerCollectionContext, dist_input: KJTList\n    ) -> torch.Tensor:\n        if self.embeddings:\n            embeddings = []\n            for embedding, input_dist_params in zip(\n                self.embeddings.values(), self.input_dist_params\n            ):\n                kjt_param, wkjt_param = input_dist_params\n                if kjt_param and wkjt_param:\n                    assert len(dist_input) == 2\n                    embeddings.append(embedding(dist_input[0], dist_input[1]))\n                elif wkjt_param and len(dist_input) == 2:\n                    embeddings.append(embedding(dist_input[1]))\n                else:\n                    embeddings.append(embedding(dist_input[0]))\n            output = torch.cat(\n                [\n                    interaction(embedding)\n                    for embedding, interaction in zip(\n                        embeddings,\n                        self.interactions.values(),\n                    )\n                ],\n                dim=1,\n            )\n        else:\n            output = torch.empty(\n                [self._cross_pg_global_batch_size, 0],\n                device=self._device,\n                requires_grad=True,\n            )\n\n        return output\n\n    def _create_output_dist(self, output: torch.Tensor) -> None:\n        # Determine the output_dist splits and the all_to_all output size\n        assert len(output.shape) == 2\n        local_dim_sum = torch.tensor(\n            [\n                output.shape[1],\n            ],\n            dtype=torch.int64,\n            device=self._device,\n        )\n        dim_sum_per_rank = [\n            torch.zeros(\n                1,\n                dtype=torch.int64,\n                device=self._device,\n            )\n            for i in range(dist.get_world_size(self._cross_pg))\n        ]\n        dist.all_gather(\n            dim_sum_per_rank,\n            local_dim_sum,\n            group=self._cross_pg,\n        )\n        dim_sum_per_rank = [x.item() for x in dim_sum_per_rank]\n        self._output_dist = PooledEmbeddingsAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            pg=self._cross_pg,\n            # pyre-ignore\n            dim_sum_per_rank=dim_sum_per_rank,\n            device=self._device,\n            codecs=self.qcomm_codecs_registry.get(\n                CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name, None\n            )\n            if self.qcomm_codecs_registry\n            else None,\n        )\n\n    def output_dist(\n        self, ctx: EmbeddingTowerCollectionContext, output: torch.Tensor\n    ) -> LazyAwaitable[torch.Tensor]:\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(output)\n            self._has_uninitialized_output_dist = False\n        # pyre-ignore [29]\n        return TowerLazyAwaitable(self._output_dist(output))\n\n    def create_context(self) -> EmbeddingTowerCollectionContext:\n        return EmbeddingTowerCollectionContext(embedding_contexts=[])\n\n    # pyre-ignore [14]\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n        for i, embedding in self.embeddings.items():\n            embedding.state_dict(\n                destination, prefix + f\"towers.{i}.embedding.\", keep_vars\n            )\n        for i, interaction in self.interactions.items():\n            interaction.module.state_dict(\n                destination, prefix + f\"towers.{i}.interaction.\", keep_vars\n            )\n        return destination\n\n    @property\n    def fused_optimizer(self) -> KeyedOptimizer:\n        return CombinedOptimizer(\n            [\n                (f\"towers.{tower_index}.embedding\", embedding.fused_optimizer)\n                for tower_index, embedding in self.embeddings.items()\n            ],\n        )\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        for i, embedding in self.embeddings.items():\n            yield from (\n                embedding.named_parameters(\n                    append_prefix(prefix, f\"towers.{i}.embedding\"), recurse\n                )\n            )\n        for i, interaction in self.interactions.items():\n            yield from (\n                interaction.module.named_parameters(\n                    append_prefix(prefix, f\"towers.{i}.interaction\"), recurse\n                )\n            )\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        for i, embedding in self.embeddings.items():\n            yield from (\n                embedding.named_buffers(\n                    append_prefix(prefix, f\"towers.{i}.embedding\"), recurse\n                )\n            )\n        for i, interaction in self.interactions.items():\n            yield from (\n                interaction.module.named_buffers(\n                    append_prefix(prefix, f\"towers.{i}.interaction\"), recurse\n                )\n            )\n\n    def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        for i, embedding in self.embeddings.items():\n            yield from (\n                embedding.sharded_parameter_names(\n                    append_prefix(prefix, f\"towers.{i}.embedding\")\n                )\n            )\n        for i, interaction in self.interactions.items():\n            yield from (\n                key\n                for key, _ in interaction.module.named_parameters(\n                    append_prefix(prefix, f\"towers.{i}.interaction\")\n                )\n            )\n\n    def named_modules(\n        self,\n        memo: Optional[Set[nn.Module]] = None,\n        prefix: str = \"\",\n        remove_duplicate: bool = True,\n    ) -> Iterator[Tuple[str, nn.Module]]:\n        yield from [(prefix, self)]",
  "class EmbeddingTowerSharder(BaseEmbeddingSharder[EmbeddingTower]):\n    def shard(\n        self,\n        module: EmbeddingTower,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingTower:\n        kjt_features, wkjt_features = self.embedding_feature_names(module)\n\n        return ShardedEmbeddingTower(\n            module=module,\n            table_name_to_parameter_sharding=params,\n            embedding_sharder=self.embedding_sharder(module),\n            kjt_features=kjt_features,\n            wkjt_features=wkjt_features,\n            env=env,\n            fused_params=self.fused_params,\n            device=device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )\n\n    def sharding_types(self, compute_device_type: str) -> List[str]:\n        \"\"\"\n        List of supported sharding types. See ShardingType for well-known examples.\n        \"\"\"\n        return [\n            ShardingType.TABLE_ROW_WISE.value,\n            ShardingType.TABLE_COLUMN_WISE.value,\n        ]\n\n    def shardable_parameters(self, module: EmbeddingTower) -> Dict[str, nn.Parameter]:\n        \"\"\"\n        List of parameters, which can be sharded.\n        \"\"\"\n        return self.embedding_sharder(module).shardable_parameters(module.embedding)\n\n    @property\n    def module_type(self) -> Type[EmbeddingTower]:\n        return EmbeddingTower\n\n    def embedding_sharder(\n        self, module: EmbeddingTower\n    ) -> BaseEmbeddingSharder[nn.Module]:\n        embedding: nn.Module = module.embedding\n        if isinstance(embedding, EmbeddingBagCollection):\n            # pyre-ignore [7]\n            return EmbeddingBagCollectionSharder(\n                self.fused_params, qcomm_codecs_registry=self.qcomm_codecs_registry\n            )\n        elif isinstance(embedding, EmbeddingCollection):\n            # pyre-ignore [7]\n            return EmbeddingCollectionSharder(\n                self.fused_params, qcomm_codecs_registry=self.qcomm_codecs_registry\n            )\n        else:\n            raise RuntimeError(f\"Unsupported embedding type: {type(module)}\")\n\n    def embedding_feature_names(\n        self, module: EmbeddingTower\n    ) -> Tuple[List[str], List[str]]:\n        embedding: nn.Module = module.embedding\n        if not (\n            isinstance(embedding, EmbeddingBagCollection)\n            or isinstance(embedding, EmbeddingCollection)\n        ):\n            raise RuntimeError(f\"unsupported embedding type: {type(module)}\")\n\n        kjt_features: List[str] = []\n        wkjt_features: List[str] = []\n        configs = []\n\n        weighted = False\n        if isinstance(embedding, EmbeddingBagCollection):\n            configs = embedding.embedding_bag_configs()\n            weighted = embedding.is_weighted()\n        elif isinstance(embedding, EmbeddingCollection):\n            configs = embedding.embedding_configs()\n\n        for config in configs:\n            if getattr(config, \"weighted\", weighted):\n                wkjt_features.extend(config.feature_names)\n            else:\n                kjt_features.extend(config.feature_names)\n        return kjt_features, wkjt_features",
  "class EmbeddingTowerCollectionSharder(BaseEmbeddingSharder[EmbeddingTowerCollection]):\n    def __init__(\n        self,\n        fused_params: Optional[Dict[str, Any]] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n\n        super().__init__(\n            fused_params=fused_params, qcomm_codecs_registry=qcomm_codecs_registry\n        )\n        self._tower_sharder = EmbeddingTowerSharder(\n            self.fused_params, qcomm_codecs_registry=qcomm_codecs_registry\n        )\n\n    def shard(\n        self,\n        module: EmbeddingTowerCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingTowerCollection:\n\n        return ShardedEmbeddingTowerCollection(\n            module=module,\n            table_name_to_parameter_sharding=params,\n            tower_sharder=self._tower_sharder,\n            env=env,\n            fused_params=self.fused_params,\n            device=device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )\n\n    def sharding_types(self, compute_device_type: str) -> List[str]:\n        \"\"\"\n        List of supported sharding types. See ShardingType for well-known examples.\n        \"\"\"\n        return [\n            ShardingType.TABLE_ROW_WISE.value,\n            ShardingType.TABLE_COLUMN_WISE.value,\n        ]\n\n    def shardable_parameters(\n        self, module: EmbeddingTowerCollection\n    ) -> Dict[str, nn.Parameter]:\n        \"\"\"\n        List of parameters, which can be sharded.\n        \"\"\"\n\n        named_parameters: Dict[str, nn.Parameter] = {}\n        for tower in module.towers:\n            named_parameters.update(self._tower_sharder.shardable_parameters(tower))\n        return named_parameters\n\n    @property\n    def module_type(self) -> Type[EmbeddingTowerCollection]:\n        return EmbeddingTowerCollection",
  "def __init__(\n        self,\n        awaitable: PooledEmbeddingsAwaitable,\n    ) -> None:\n        super().__init__()\n        self._awaitable = awaitable",
  "def _wait_impl(self) -> torch.Tensor:\n        return self._awaitable.wait()",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for ctx in self.embedding_contexts:\n            ctx.record_stream(stream)",
  "def __init__(\n        self,\n        module: EmbeddingTower,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        embedding_sharder: BaseEmbeddingSharder[nn.Module],\n        kjt_features: List[str],\n        wkjt_features: List[str],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        intra_pg, cross_pg = intra_and_cross_node_pg(device)\n        self._intra_pg: Optional[dist.ProcessGroup] = intra_pg\n        self._cross_pg: Optional[dist.ProcessGroup] = cross_pg\n        self._device = device\n        self._output_dist: Optional[PooledEmbeddingsAllToAll] = None\n        self._cross_pg_global_batch_size: int = 0\n        self._cross_pg_world_size: int = dist.get_world_size(self._cross_pg)\n\n        self._has_uninitialized_output_dist = True\n\n        # make sure all sharding on single physical node\n        devices_per_host = dist.get_world_size(intra_pg)\n        tower_devices = set()\n        for sharding in table_name_to_parameter_sharding.values():\n            # pyre-ignore [6]\n            tower_devices.update(sharding.ranks)\n        host = {tower_device // devices_per_host for tower_device in tower_devices}\n        assert len(host) == 1, f\"{tower_devices}, {table_name_to_parameter_sharding}\"\n        self._tower_node: int = next(iter(host))\n        self._active_device: bool = {dist.get_rank() // devices_per_host} == host\n\n        # input_dist\n        self._kjt_feature_names: List[str] = kjt_features\n        self._wkjt_feature_names: List[str] = wkjt_features\n        self._has_uninitialized_input_dist: bool = True\n        self._cross_dist: nn.Module = nn.Module()\n        self._weighted_cross_dist: nn.Module = nn.Module()\n        self._kjt_features_order: List[int] = []\n        self._wkjt_features_order: List[int] = []\n        self._has_kjt_features_permute: bool = False\n        self._has_wkjt_features_permute: bool = False\n\n        self.embedding: Optional[nn.Module] = None\n        self.interaction: Optional[nn.Module] = None\n        if self._active_device:\n            _replace_sharding_with_intra_node(\n                table_name_to_parameter_sharding,\n                dist.get_world_size(self._intra_pg),\n            )\n            intra_env: ShardingEnv = ShardingEnv(\n                world_size=dist.get_world_size(self._intra_pg),\n                rank=dist.get_rank(self._intra_pg),\n                pg=self._intra_pg,\n            )\n            # shard embedding module\n            self.embedding = embedding_sharder.shard(\n                module.embedding,\n                table_name_to_parameter_sharding,\n                intra_env,\n                device,\n            )\n            # Hierarchical DDP\n            self.interaction = DistributedDataParallel(\n                module=module.interaction.to(self._device),\n                device_ids=[self._device],\n                process_group=self._intra_pg,\n                gradient_as_bucket_view=True,\n                broadcast_buffers=False,\n            )\n\n        # Setup output dists for quantized comms\n        self._output_dists: nn.ModuleList = (\n            self.embedding._output_dists if self.embedding else nn.ModuleList()\n        )",
  "def _create_input_dist(\n        self,\n        kjt_feature_names: List[str],\n        wkjt_feature_names: List[str],\n    ) -> None:\n        if self._kjt_feature_names != kjt_feature_names:\n            self._has_kjt_features_permute = True\n            for f in self._kjt_feature_names:\n                self._kjt_features_order.append(kjt_feature_names.index(f))\n            self.register_buffer(\n                \"_kjt_features_order_tensor\",\n                torch.tensor(\n                    self._kjt_features_order, device=self._device, dtype=torch.int32\n                ),\n                persistent=False,\n            )\n\n        if self._wkjt_feature_names != wkjt_feature_names:\n            self._has_wkjt_features_permute = True\n            for f in self._wkjt_feature_names:\n                self._wkjt_features_order.append(wkjt_feature_names.index(f))\n            self.register_buffer(\n                \"_wkjt_features_order_tensor\",\n                torch.tensor(\n                    self._wkjt_features_order, device=self._device, dtype=torch.int32\n                ),\n                persistent=False,\n            )\n\n        node_count = dist.get_world_size(self._cross_pg)\n        kjt_features_per_node = [\n            len(self._kjt_feature_names) if node == self._tower_node else 0\n            for node in range(node_count)\n        ]\n        wkjt_features_per_node = [\n            len(self._wkjt_feature_names) if node == self._tower_node else 0\n            for node in range(node_count)\n        ]\n        self._cross_dist = KJTAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._cross_pg,\n            kjt_features_per_node,\n        )\n        self._weighted_cross_dist = KJTAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._cross_pg,\n            wkjt_features_per_node,\n        )",
  "def input_dist(\n        self,\n        ctx: NullShardedModuleContext,\n        features: KeyedJaggedTensor,\n        optional_features: Optional[KeyedJaggedTensor] = None,\n    ) -> Awaitable[Awaitable[KJTList]]:\n\n        # optional_features are populated only if both kjt and weighted kjt present in tower\n        if self._wkjt_feature_names and self._kjt_feature_names:\n            kjt_features = features\n            wkjt_features = optional_features\n        elif self._wkjt_feature_names:\n            kjt_features = None\n            wkjt_features = features\n        else:\n            kjt_features = features\n            wkjt_features = None\n\n        if self._has_uninitialized_input_dist:\n            self._cross_pg_global_batch_size = (\n                features.stride() * self._cross_pg_world_size\n            )\n            self._create_input_dist(\n                kjt_features.keys() if kjt_features else [],\n                wkjt_features.keys() if wkjt_features else [],\n            )\n            self._has_uninitialized_input_dist = False\n\n        with torch.no_grad():\n            if self._has_kjt_features_permute:\n                # pyre-ignore [16]\n                kjt_features = kjt_features.permute(\n                    self._kjt_features_order,\n                    self._kjt_features_order_tensor,\n                )\n            if self._has_wkjt_features_permute:\n                wkjt_features = wkjt_features.permute(\n                    self._wkjt_features_order,\n                    self._wkjt_features_order_tensor,\n                )\n\n            awaitables = []\n            if kjt_features is not None:\n                awaitables.append(self._cross_dist(kjt_features))\n            if wkjt_features is not None:\n                awaitables.append(self._weighted_cross_dist(wkjt_features))\n\n            return KJTListSplitsAwaitable(awaitables, ctx)",
  "def compute(\n        self, ctx: NullShardedModuleContext, dist_input: KJTList\n    ) -> torch.Tensor:\n        if self._active_device:\n            if len(dist_input) == 2:\n                kjt_features = dist_input[0]\n                wkjt_features = dist_input[1]\n                # pyre-ignore [29]\n                embeddings = self.embedding(kjt_features, wkjt_features)\n            else:\n                # pyre-ignore [29]\n                embeddings = self.embedding(dist_input[0])\n            # pyre-ignore [29]\n            output = self.interaction(embeddings)\n        else:\n            output = torch.empty(\n                [self._cross_pg_global_batch_size, 0],\n                device=self._device,\n                requires_grad=True,\n            )\n        return output",
  "def _create_output_dist(\n        self, ctx: NullShardedModuleContext, output: torch.Tensor\n    ) -> None:\n        # Determine the output_dist splits and the all_to_all output size\n        assert len(output.shape) == 2\n        local_dim_sum = torch.tensor(\n            [\n                output.shape[1],\n            ],\n            dtype=torch.int64,\n            device=self._device,\n        )\n        dim_sum_per_rank = [\n            torch.zeros(\n                1,\n                dtype=torch.int64,\n                device=self._device,\n            )\n            for i in range(dist.get_world_size(self._cross_pg))\n        ]\n        dist.all_gather(\n            dim_sum_per_rank,\n            local_dim_sum,\n            group=self._cross_pg,\n        )\n        dim_sum_per_rank = [x.item() for x in dim_sum_per_rank]\n        self._output_dist = PooledEmbeddingsAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            pg=self._cross_pg,\n            # pyre-fixme[6]: For 2nd param expected `List[int]` but got\n            #  `List[Union[bool, float, int]]`.\n            dim_sum_per_rank=dim_sum_per_rank,\n            device=self._device,\n            codecs=self.qcomm_codecs_registry.get(\n                CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name, None\n            )\n            if self.qcomm_codecs_registry\n            else None,\n        )",
  "def output_dist(\n        self, ctx: NullShardedModuleContext, output: torch.Tensor\n    ) -> LazyAwaitable[torch.Tensor]:\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(ctx, output)\n            self._has_uninitialized_output_dist = False\n        # pyre-ignore [29]\n        return TowerLazyAwaitable(self._output_dist(output))",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n        if self._active_device:\n            # pyre-ignore [16]\n            self.embedding.state_dict(destination, prefix + \"embedding.\", keep_vars)\n            # pyre-ignore [16]\n            self.interaction.module.state_dict(\n                destination, prefix + \"interaction.\", keep_vars\n            )\n        return destination",
  "def fused_optimizer(self) -> KeyedOptimizer:\n        if self.embedding:\n            return self.embedding.fused_optimizer\n        else:\n            return CombinedOptimizer([])",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        if self._active_device:\n            # pyre-ignore[16]\n            yield from self.embedding.named_parameters(\n                append_prefix(prefix, \"embedding\"), recurse\n            )\n            # pyre-ignore[16]\n            yield from self.interaction.module.named_parameters(\n                append_prefix(prefix, \"interaction\"), recurse\n            )\n        else:\n            yield from ()",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        if self._active_device:\n            # pyre-ignore[16]\n            yield from self.embedding.named_buffers(\n                append_prefix(prefix, \"embedding\"), recurse\n            )\n            # pyre-ignore[16]\n            yield from self.interaction.module.named_buffers(\n                append_prefix(prefix, \"interaction\"), recurse\n            )\n        yield from ()",
  "def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        if self._active_device:\n            # pyre-ignore[16]\n            yield from self.embedding.sharded_parameter_names(\n                append_prefix(prefix, \"embedding\")\n            )\n            # pyre-ignore[16]\n            for name, _ in self.interaction.module.named_parameters(\n                append_prefix(prefix, \"interaction\")\n            ):\n                yield name\n        else:\n            yield from ()",
  "def named_modules(\n        self,\n        memo: Optional[Set[nn.Module]] = None,\n        prefix: str = \"\",\n        remove_duplicate: bool = True,\n    ) -> Iterator[Tuple[str, nn.Module]]:\n        yield from [(prefix, self)]",
  "def create_context(self) -> NullShardedModuleContext:\n        return NullShardedModuleContext()",
  "def __init__(\n        self,\n        module: EmbeddingTowerCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        tower_sharder: BaseEmbeddingSharder[EmbeddingTower],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n\n        intra_pg, cross_pg = intra_and_cross_node_pg(device)\n        self._intra_pg: Optional[dist.ProcessGroup] = intra_pg\n        self._cross_pg: Optional[dist.ProcessGroup] = cross_pg\n        self._cross_pg_world_size: int = dist.get_world_size(self._cross_pg)\n        self._intra_pg_world_size: int = dist.get_world_size(self._intra_pg)\n        self._device = device\n        self._tower_id: int = dist.get_rank() // self._intra_pg_world_size\n        self._output_dist: Optional[PooledEmbeddingsAllToAll] = None\n        self._cross_pg_global_batch_size: int = 0\n        self._is_weighted: bool = False\n        self._has_uninitialized_input_dist: bool = True\n        self._has_uninitialized_output_dist: bool = True\n        self._kjt_features_order: List[int] = []\n        self._wkjt_features_order: List[int] = []\n        self._kjt_feature_names: List[str] = []\n        self._wkjt_feature_names: List[str] = []\n        self._kjt_num_features_per_pt: List[int] = []\n        self._wkjt_num_features_per_pt: List[int] = []\n        self._has_kjt_features_permute: bool = False\n        self._has_wkjt_features_permute: bool = False\n        self.embeddings: nn.ModuleDict = nn.ModuleDict()\n        self.interactions: nn.ModuleDict = nn.ModuleDict()\n        self.input_dist_params: List[Tuple[bool, bool]] = []\n        self._cross_dist: nn.Module = nn.Module()\n        self._weighted_cross_dist: nn.Module = nn.Module()\n\n        # groups parameter sharding into physical towers\n        tables_per_pt: List[Set[str]] = [\n            set() for _ in range(self._cross_pg_world_size)\n        ]\n        [\n            tables_per_pt[i].add(k)\n            for i in range(self._cross_pg_world_size)\n            for k, v in table_name_to_parameter_sharding.items()\n            # pyre-ignore [16]\n            if v.ranks[0] // self._intra_pg_world_size == i\n        ]\n\n        # create mapping of logical towers to physical towers\n        tables_per_lt: List[Set[str]] = []\n        for tower in module.towers:\n            lt_tables = set(tower_sharder.shardable_parameters(tower).keys())\n            tables_per_lt.append(lt_tables)\n            # check the tables in a logical tower are on same physical tower\n            found_physical_tower = False\n            for pt_tables in tables_per_pt:\n                if lt_tables.issubset(pt_tables):\n                    found_physical_tower = True\n                    break\n            assert (\n                found_physical_tower\n            ), f\"tables in a logical tower must be in the same physical tower, logical tower tables: {lt_tables}, tables_per_pt: {tables_per_pt}\"\n\n        logical_to_physical_order: List[List[int]] = [\n            [] for _ in range(self._cross_pg_world_size)\n        ]\n        feature_names_by_pt: List[Tuple[List[str], List[str]]] = [\n            ([], []) for _ in range(self._cross_pg_world_size)\n        ]\n\n        for i, pt_tables in enumerate(tables_per_pt):\n            found = False\n            for j, lt_tables in enumerate(tables_per_lt):\n                if lt_tables.issubset(pt_tables):\n                    logical_to_physical_order[i].append(j)\n                    found = True\n            if not found and pt_tables:\n                raise RuntimeError(\n                    f\"Could not find any towers with features: {pt_tables}\"\n                )\n\n        for pt_index, lt_on_pt in enumerate(logical_to_physical_order):\n            for lt_index in lt_on_pt:\n                # pyre-ignore [16]\n                kjt_features, wkjt_features = tower_sharder.embedding_feature_names(\n                    module.towers[lt_index]\n                )\n                feature_names_by_pt[pt_index][0].extend(kjt_features)\n                feature_names_by_pt[pt_index][1].extend(wkjt_features)\n\n        for kjt_names, wkjt_names in feature_names_by_pt:\n            self._kjt_feature_names.extend(kjt_names)\n            self._wkjt_feature_names.extend(wkjt_names)\n            self._kjt_num_features_per_pt.append(len(kjt_names))\n            self._wkjt_num_features_per_pt.append(len(wkjt_names))\n\n        local_towers: List[Tuple[str, EmbeddingTower]] = [\n            (str(i), tower)\n            for i, tower in enumerate(module.towers)\n            if i in logical_to_physical_order[self._tower_id]\n        ]\n\n        if local_towers:\n            _replace_sharding_with_intra_node(\n                table_name_to_parameter_sharding,\n                dist.get_world_size(self._intra_pg),\n            )\n            intra_env: ShardingEnv = ShardingEnv(\n                world_size=dist.get_world_size(self._intra_pg),\n                rank=dist.get_rank(self._intra_pg),\n                pg=self._intra_pg,\n            )\n            for i, tower in local_towers:\n                # pyre-ignore [16]\n                self.embeddings[i] = tower_sharder.embedding_sharder(tower).shard(\n                    tower.embedding,\n                    table_name_to_parameter_sharding,\n                    intra_env,\n                    device,\n                )\n                self.input_dist_params.append(tower_input_params(tower.embedding))\n                # Hierarchical DDP\n                self.interactions[i] = DistributedDataParallel(\n                    module=tower.interaction.to(self._device),\n                    device_ids=[self._device],\n                    process_group=self._intra_pg,\n                    gradient_as_bucket_view=True,\n                    broadcast_buffers=False,\n                    static_graph=True,\n                )\n\n        # Setup output dists for quantized comms\n        output_dists = nn.ModuleList()\n        for embedding in self.embeddings.values():\n            output_dists.extend(embedding._output_dists)\n        self._output_dists: nn.ModuleList = output_dists",
  "def _create_input_dist(\n        self,\n        kjt_feature_names: List[str],\n        wkjt_feature_names: List[str],\n    ) -> None:\n\n        if self._kjt_feature_names != kjt_feature_names:\n            self._has_kjt_features_permute = True\n            for f in self._kjt_feature_names:\n                self._kjt_features_order.append(kjt_feature_names.index(f))\n            self.register_buffer(\n                \"_kjt_features_order_tensor\",\n                torch.tensor(\n                    self._kjt_features_order, device=self._device, dtype=torch.int32\n                ),\n            )\n\n        if self._wkjt_feature_names != wkjt_feature_names:\n            self._has_wkjt_features_permute = True\n            for f in self._wkjt_feature_names:\n                self._wkjt_features_order.append(wkjt_feature_names.index(f))\n            self.register_buffer(\n                \"_wkjt_features_order_tensor\",\n                torch.tensor(\n                    self._wkjt_features_order, device=self._device, dtype=torch.int32\n                ),\n            )\n\n        self._cross_dist = KJTAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._cross_pg,\n            self._kjt_num_features_per_pt,\n        )\n        self._weighted_cross_dist = KJTAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._cross_pg,\n            self._wkjt_num_features_per_pt,\n        )",
  "def input_dist(\n        self,\n        ctx: EmbeddingTowerCollectionContext,\n        kjt_features: Optional[KeyedJaggedTensor] = None,\n        wkjt_features: Optional[KeyedJaggedTensor] = None,\n    ) -> Awaitable[Awaitable[KJTList]]:\n        if self._has_uninitialized_input_dist:\n            # pyre-ignore [16]\n            stride = kjt_features.stride() if kjt_features else wkjt_features.stride()\n            self._cross_pg_global_batch_size = stride * self._cross_pg_world_size\n            self._create_input_dist(\n                kjt_features.keys() if kjt_features else [],\n                wkjt_features.keys() if wkjt_features else [],\n            )\n            self._has_uninitialized_input_dist = False\n        with torch.no_grad():\n            if self._has_kjt_features_permute:\n                kjt_features = kjt_features.permute(  # pyre-ignore [16]\n                    self._kjt_features_order,\n                    cast(torch.Tensor, self._kjt_features_order_tensor),\n                )\n            if self._has_wkjt_features_permute:\n                wkjt_features = wkjt_features.permute(\n                    self._wkjt_features_order,\n                    cast(torch.Tensor, self._wkjt_features_order_tensor),\n                )\n            awaitables = []\n            if kjt_features is not None:\n                awaitables.append(self._cross_dist(kjt_features))\n            if wkjt_features is not None:\n                awaitables.append(self._weighted_cross_dist(wkjt_features))\n        return KJTListSplitsAwaitable(awaitables, ctx)",
  "def compute(\n        self, ctx: EmbeddingTowerCollectionContext, dist_input: KJTList\n    ) -> torch.Tensor:\n        if self.embeddings:\n            embeddings = []\n            for embedding, input_dist_params in zip(\n                self.embeddings.values(), self.input_dist_params\n            ):\n                kjt_param, wkjt_param = input_dist_params\n                if kjt_param and wkjt_param:\n                    assert len(dist_input) == 2\n                    embeddings.append(embedding(dist_input[0], dist_input[1]))\n                elif wkjt_param and len(dist_input) == 2:\n                    embeddings.append(embedding(dist_input[1]))\n                else:\n                    embeddings.append(embedding(dist_input[0]))\n            output = torch.cat(\n                [\n                    interaction(embedding)\n                    for embedding, interaction in zip(\n                        embeddings,\n                        self.interactions.values(),\n                    )\n                ],\n                dim=1,\n            )\n        else:\n            output = torch.empty(\n                [self._cross_pg_global_batch_size, 0],\n                device=self._device,\n                requires_grad=True,\n            )\n\n        return output",
  "def _create_output_dist(self, output: torch.Tensor) -> None:\n        # Determine the output_dist splits and the all_to_all output size\n        assert len(output.shape) == 2\n        local_dim_sum = torch.tensor(\n            [\n                output.shape[1],\n            ],\n            dtype=torch.int64,\n            device=self._device,\n        )\n        dim_sum_per_rank = [\n            torch.zeros(\n                1,\n                dtype=torch.int64,\n                device=self._device,\n            )\n            for i in range(dist.get_world_size(self._cross_pg))\n        ]\n        dist.all_gather(\n            dim_sum_per_rank,\n            local_dim_sum,\n            group=self._cross_pg,\n        )\n        dim_sum_per_rank = [x.item() for x in dim_sum_per_rank]\n        self._output_dist = PooledEmbeddingsAllToAll(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            pg=self._cross_pg,\n            # pyre-ignore\n            dim_sum_per_rank=dim_sum_per_rank,\n            device=self._device,\n            codecs=self.qcomm_codecs_registry.get(\n                CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name, None\n            )\n            if self.qcomm_codecs_registry\n            else None,\n        )",
  "def output_dist(\n        self, ctx: EmbeddingTowerCollectionContext, output: torch.Tensor\n    ) -> LazyAwaitable[torch.Tensor]:\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(output)\n            self._has_uninitialized_output_dist = False\n        # pyre-ignore [29]\n        return TowerLazyAwaitable(self._output_dist(output))",
  "def create_context(self) -> EmbeddingTowerCollectionContext:\n        return EmbeddingTowerCollectionContext(embedding_contexts=[])",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n        for i, embedding in self.embeddings.items():\n            embedding.state_dict(\n                destination, prefix + f\"towers.{i}.embedding.\", keep_vars\n            )\n        for i, interaction in self.interactions.items():\n            interaction.module.state_dict(\n                destination, prefix + f\"towers.{i}.interaction.\", keep_vars\n            )\n        return destination",
  "def fused_optimizer(self) -> KeyedOptimizer:\n        return CombinedOptimizer(\n            [\n                (f\"towers.{tower_index}.embedding\", embedding.fused_optimizer)\n                for tower_index, embedding in self.embeddings.items()\n            ],\n        )",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        for i, embedding in self.embeddings.items():\n            yield from (\n                embedding.named_parameters(\n                    append_prefix(prefix, f\"towers.{i}.embedding\"), recurse\n                )\n            )\n        for i, interaction in self.interactions.items():\n            yield from (\n                interaction.module.named_parameters(\n                    append_prefix(prefix, f\"towers.{i}.interaction\"), recurse\n                )\n            )",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        for i, embedding in self.embeddings.items():\n            yield from (\n                embedding.named_buffers(\n                    append_prefix(prefix, f\"towers.{i}.embedding\"), recurse\n                )\n            )\n        for i, interaction in self.interactions.items():\n            yield from (\n                interaction.module.named_buffers(\n                    append_prefix(prefix, f\"towers.{i}.interaction\"), recurse\n                )\n            )",
  "def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        for i, embedding in self.embeddings.items():\n            yield from (\n                embedding.sharded_parameter_names(\n                    append_prefix(prefix, f\"towers.{i}.embedding\")\n                )\n            )\n        for i, interaction in self.interactions.items():\n            yield from (\n                key\n                for key, _ in interaction.module.named_parameters(\n                    append_prefix(prefix, f\"towers.{i}.interaction\")\n                )\n            )",
  "def named_modules(\n        self,\n        memo: Optional[Set[nn.Module]] = None,\n        prefix: str = \"\",\n        remove_duplicate: bool = True,\n    ) -> Iterator[Tuple[str, nn.Module]]:\n        yield from [(prefix, self)]",
  "def shard(\n        self,\n        module: EmbeddingTower,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingTower:\n        kjt_features, wkjt_features = self.embedding_feature_names(module)\n\n        return ShardedEmbeddingTower(\n            module=module,\n            table_name_to_parameter_sharding=params,\n            embedding_sharder=self.embedding_sharder(module),\n            kjt_features=kjt_features,\n            wkjt_features=wkjt_features,\n            env=env,\n            fused_params=self.fused_params,\n            device=device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def sharding_types(self, compute_device_type: str) -> List[str]:\n        \"\"\"\n        List of supported sharding types. See ShardingType for well-known examples.\n        \"\"\"\n        return [\n            ShardingType.TABLE_ROW_WISE.value,\n            ShardingType.TABLE_COLUMN_WISE.value,\n        ]",
  "def shardable_parameters(self, module: EmbeddingTower) -> Dict[str, nn.Parameter]:\n        \"\"\"\n        List of parameters, which can be sharded.\n        \"\"\"\n        return self.embedding_sharder(module).shardable_parameters(module.embedding)",
  "def module_type(self) -> Type[EmbeddingTower]:\n        return EmbeddingTower",
  "def embedding_sharder(\n        self, module: EmbeddingTower\n    ) -> BaseEmbeddingSharder[nn.Module]:\n        embedding: nn.Module = module.embedding\n        if isinstance(embedding, EmbeddingBagCollection):\n            # pyre-ignore [7]\n            return EmbeddingBagCollectionSharder(\n                self.fused_params, qcomm_codecs_registry=self.qcomm_codecs_registry\n            )\n        elif isinstance(embedding, EmbeddingCollection):\n            # pyre-ignore [7]\n            return EmbeddingCollectionSharder(\n                self.fused_params, qcomm_codecs_registry=self.qcomm_codecs_registry\n            )\n        else:\n            raise RuntimeError(f\"Unsupported embedding type: {type(module)}\")",
  "def embedding_feature_names(\n        self, module: EmbeddingTower\n    ) -> Tuple[List[str], List[str]]:\n        embedding: nn.Module = module.embedding\n        if not (\n            isinstance(embedding, EmbeddingBagCollection)\n            or isinstance(embedding, EmbeddingCollection)\n        ):\n            raise RuntimeError(f\"unsupported embedding type: {type(module)}\")\n\n        kjt_features: List[str] = []\n        wkjt_features: List[str] = []\n        configs = []\n\n        weighted = False\n        if isinstance(embedding, EmbeddingBagCollection):\n            configs = embedding.embedding_bag_configs()\n            weighted = embedding.is_weighted()\n        elif isinstance(embedding, EmbeddingCollection):\n            configs = embedding.embedding_configs()\n\n        for config in configs:\n            if getattr(config, \"weighted\", weighted):\n                wkjt_features.extend(config.feature_names)\n            else:\n                kjt_features.extend(config.feature_names)\n        return kjt_features, wkjt_features",
  "def __init__(\n        self,\n        fused_params: Optional[Dict[str, Any]] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n\n        super().__init__(\n            fused_params=fused_params, qcomm_codecs_registry=qcomm_codecs_registry\n        )\n        self._tower_sharder = EmbeddingTowerSharder(\n            self.fused_params, qcomm_codecs_registry=qcomm_codecs_registry\n        )",
  "def shard(\n        self,\n        module: EmbeddingTowerCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingTowerCollection:\n\n        return ShardedEmbeddingTowerCollection(\n            module=module,\n            table_name_to_parameter_sharding=params,\n            tower_sharder=self._tower_sharder,\n            env=env,\n            fused_params=self.fused_params,\n            device=device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def sharding_types(self, compute_device_type: str) -> List[str]:\n        \"\"\"\n        List of supported sharding types. See ShardingType for well-known examples.\n        \"\"\"\n        return [\n            ShardingType.TABLE_ROW_WISE.value,\n            ShardingType.TABLE_COLUMN_WISE.value,\n        ]",
  "def shardable_parameters(\n        self, module: EmbeddingTowerCollection\n    ) -> Dict[str, nn.Parameter]:\n        \"\"\"\n        List of parameters, which can be sharded.\n        \"\"\"\n\n        named_parameters: Dict[str, nn.Parameter] = {}\n        for tower in module.towers:\n            named_parameters.update(self._tower_sharder.shardable_parameters(tower))\n        return named_parameters",
  "def module_type(self) -> Type[EmbeddingTowerCollection]:\n        return EmbeddingTowerCollection",
  "class BaseEmbedding(abc.ABC, nn.Module):\n    \"\"\"\n    Abstract base class for grouped `nn.Embedding` and `nn.EmbeddingBag`\n    \"\"\"\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor):\n        Returns:\n            torch.Tensor: sparse gradient parameter names.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def config(self) -> GroupedEmbeddingConfig:\n        pass",
  "def get_state_dict(\n    embedding_tables: List[ShardedEmbeddingTable],\n    params: Union[\n        nn.ModuleList,\n        List[Union[nn.Module, torch.Tensor]],\n        List[torch.Tensor],\n        List[Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]],\n    ],\n    pg: Optional[dist.ProcessGroup] = None,\n    destination: Optional[Dict[str, Any]] = None,\n    prefix: str = \"\",\n) -> Dict[str, Any]:\n    if destination is None:\n        destination = OrderedDict()\n        # pyre-ignore [16]\n        destination._metadata = OrderedDict()\n    \"\"\"\n    It is possible for there to be multiple shards from a table on a single rank.\n    We accumulate them in key_to_local_shards. Repeat shards should have identical\n    global ShardedTensorMetadata.\n    \"\"\"\n    key_to_local_shards: Dict[str, List[Shard]] = defaultdict(list)\n    key_to_global_metadata: Dict[str, ShardedTensorMetadata] = {}\n\n    def get_key_from_embedding_table(embedding_table: ShardedEmbeddingTable) -> str:\n        return prefix + f\"{embedding_table.name}.weight\"\n\n    for embedding_table, param in zip(embedding_tables, params):\n        key = get_key_from_embedding_table(embedding_table)\n        is_quant = embedding_table.compute_kernel in [\n            EmbeddingComputeKernel.QUANT,\n            EmbeddingComputeKernel.QUANT_UVM,\n            EmbeddingComputeKernel.QUANT_UVM_CACHING,\n        ]\n        qscale = None\n        qbias = None\n        if is_quant:\n            # For QUANT* param is Tuple[torch.Tensor, Optional[torch.Tensor]] where first argument is the weight table, the second is optional quantization extra information, depending on quantization type. e.g. for fbgemm rowwise quantization this is scale and shift for each row.\n            assert isinstance(param, tuple)\n            qscale = param[1]\n            qbias = param[2]\n            param = param[0]\n\n        assert embedding_table.local_rows == param.size(0)  # pyre-ignore[16]\n\n        if qscale is not None:\n            assert embedding_table.local_cols == param.size(1)  # pyre-ignore[16]\n\n        if embedding_table.global_metadata is not None and pg is not None:\n            # set additional field of sharded tensor based on local tensor properties\n            embedding_table.global_metadata.tensor_properties.dtype = (\n                param.dtype  # pyre-ignore[16]\n            )\n            embedding_table.global_metadata.tensor_properties.requires_grad = (\n                param.requires_grad  # pyre-ignore[16]\n            )\n            key_to_global_metadata[key] = embedding_table.global_metadata\n\n            key_to_local_shards[key].append(\n                # pyre-fixme[6]: For 1st argument expected `Tensor` but got\n                #  `Union[Module, Tensor]`.\n                # pyre-fixme[6]: For 2nd argument expected `ShardMetadata` but got\n                #  `Optional[ShardMetadata]`.\n                Shard(param, embedding_table.local_metadata)\n            )\n        else:\n            destination[key] = param\n            if qscale is not None:\n                destination[f\"{key}_qscale\"] = qscale\n            if qbias is not None:\n                destination[f\"{key}_qbias\"] = qbias\n\n    if pg is not None:\n        # Populate the remaining destinations that have a global metadata\n        for key in key_to_local_shards:\n            global_metadata = key_to_global_metadata[key]\n            destination[\n                key\n            ] = ShardedTensor._init_from_local_shards_and_global_metadata(\n                local_shards=key_to_local_shards[key],\n                sharded_tensor_metadata=global_metadata,\n                process_group=pg,\n            )\n\n    return destination",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (KeyedJaggedTensor):\n        Returns:\n            torch.Tensor: sparse gradient parameter names.\n        \"\"\"\n        pass",
  "def config(self) -> GroupedEmbeddingConfig:\n        pass",
  "def get_key_from_embedding_table(embedding_table: ShardedEmbeddingTable) -> str:\n        return prefix + f\"{embedding_table.name}.weight\"",
  "def fx_wrap_tensor_view2d(x: torch.Tensor, dim0: int, dim1: int) -> torch.Tensor:\n    return x.view(dim0, dim1)",
  "def _load_state_dict(\n    emb_modules: \"nn.ModuleList\",\n    state_dict: \"OrderedDict[str, Union[torch.Tensor, ShardedTensor]]\",\n) -> Tuple[List[str], List[str]]:\n    missing_keys = []\n    unexpected_keys = list(state_dict.keys())\n    for emb_module in emb_modules:\n        for key, dst_param in emb_module.state_dict().items():\n            if key in state_dict:\n                src_param = state_dict[key]\n                if isinstance(dst_param, ShardedTensor):\n                    assert isinstance(src_param, ShardedTensor)\n                    assert len(dst_param.local_shards()) == len(\n                        src_param.local_shards()\n                    )\n                    for dst_local_shard, src_local_shard in zip(\n                        dst_param.local_shards(), src_param.local_shards()\n                    ):\n                        assert (\n                            dst_local_shard.metadata.shard_offsets\n                            == src_local_shard.metadata.shard_offsets\n                        )\n                        assert (\n                            dst_local_shard.metadata.shard_sizes\n                            == src_local_shard.metadata.shard_sizes\n                        )\n\n                        dst_local_shard.tensor.detach().copy_(src_local_shard.tensor)\n                else:\n                    assert isinstance(src_param, torch.Tensor) and isinstance(\n                        dst_param, torch.Tensor\n                    )\n                    dst_param.detach().copy_(src_param)\n                unexpected_keys.remove(key)\n            else:\n                missing_keys.append(cast(str, key))\n    return missing_keys, unexpected_keys",
  "def embeddings_cat_empty_rank_handle(\n    embeddings: List[torch.Tensor],\n    dummy_embs_tensor: torch.Tensor,\n    dim: int = 0,\n) -> torch.Tensor:\n    if len(embeddings) == 0:\n        # a hack for empty ranks\n        return dummy_embs_tensor\n    elif len(embeddings) == 1:\n        return embeddings[0]\n    else:\n        return torch.cat(embeddings, dim=dim)",
  "class GroupedEmbeddingsLookup(BaseEmbeddingLookup[KeyedJaggedTensor, torch.Tensor]):\n    \"\"\"\n    Lookup modules for Sequence embeddings (i.e Embeddings)\n    \"\"\"\n\n    def __init__(\n        self,\n        grouped_configs: List[GroupedEmbeddingConfig],\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        # TODO rename to _create_embedding_kernel\n        def _create_lookup(\n            config: GroupedEmbeddingConfig,\n        ) -> BaseEmbedding:\n            if config.compute_kernel == EmbeddingComputeKernel.DENSE:\n                return BatchedDenseEmbedding(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            elif config.compute_kernel == EmbeddingComputeKernel.FUSED:\n                return BatchedFusedEmbedding(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            else:\n                raise ValueError(\n                    f\"Compute kernel not supported {config.compute_kernel}\"\n                )\n\n        super().__init__()\n        self._emb_modules: nn.ModuleList = nn.ModuleList()\n        for config in grouped_configs:\n            self._emb_modules.append(_create_lookup(config))\n\n        self._feature_splits: List[int] = []\n        for config in grouped_configs:\n            self._feature_splits.append(config.num_features())\n\n        # return a dummy empty tensor when grouped_configs is empty\n        self.register_buffer(\n            \"_dummy_embs_tensor\",\n            torch.empty(\n                [0],\n                dtype=torch.float32,\n                device=device,\n                requires_grad=True,\n            ),\n        )\n\n        self.grouped_configs = grouped_configs\n\n    def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        embeddings: List[torch.Tensor] = []\n        features_by_group = sparse_features.split(\n            self._feature_splits,\n        )\n        for emb_op, features in zip(self._emb_modules, features_by_group):\n            embeddings.append(emb_op(features).view(-1))\n\n        return embeddings_cat_empty_rank_handle(embeddings, self._dummy_embs_tensor)\n\n    # pyre-fixme[14]: `state_dict` overrides method defined in `Module` inconsistently.\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n\n        for emb_module in self._emb_modules:\n            emb_module.state_dict(destination, prefix, keep_vars)\n\n        return destination\n\n    # pyre-fixme[14]: `load_state_dict` overrides method defined in `Module`\n    #  inconsistently.\n    def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, Union[torch.Tensor, ShardedTensor]]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        m, u = _load_state_dict(self._emb_modules, state_dict)\n        return _IncompatibleKeys(missing_keys=m, unexpected_keys=u)\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_parameters for\"\n            \"GroupedEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_parameters(prefix, recurse)\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_buffers for\"\n            \"GroupedEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_buffers(prefix, recurse)\n\n    def named_parameters_by_table(\n        self,\n    ) -> Iterator[Tuple[str, TableBatchedEmbeddingSlice]]:\n        \"\"\"\n        Like named_parameters(), but yields table_name and embedding_weights which are wrapped in TableBatchedEmbeddingSlice.\n        For a single table with multiple shards (i.e CW) these are combined into one table/weight.\n        Used in composability.\n        \"\"\"\n        for embedding_kernel in self._emb_modules:\n            for (\n                table_name,\n                tbe_slice,\n            ) in embedding_kernel.named_parameters_by_table():\n                yield (table_name, tbe_slice)",
  "class CommOpGradientScaling(torch.autograd.Function):\n    @staticmethod\n    # pyre-ignore\n    def forward(\n        ctx: FunctionCtx, input_tensor: torch.Tensor, scale_gradient_factor: int\n    ) -> torch.Tensor:\n        # pyre-ignore\n        ctx.scale_gradient_factor = scale_gradient_factor\n        return input_tensor\n\n    @staticmethod\n    # pyre-ignore[14]: `forward` overrides method defined in `Function` inconsistently.\n    def backward(\n        ctx: FunctionCtx, grad_output: torch.Tensor\n    ) -> Tuple[torch.Tensor, None]:\n        # When gradient division is on, we scale down the gradient by world size\n        # at alltoall backward for model parallelism. However weights\n        # is controlled by DDP so it already has gradient division, so we scale\n        # the gradient back up\n        # pyre-ignore[16]: `FunctionCtx` has no attribute `scale_gradient_factor`\n        grad_output.mul_(ctx.scale_gradient_factor)\n        return grad_output, None",
  "class GroupedPooledEmbeddingsLookup(\n    BaseEmbeddingLookup[KeyedJaggedTensor, torch.Tensor]\n):\n    \"\"\"\n    Lookup modules for Pooled embeddings (i.e EmbeddingBags)\n    \"\"\"\n\n    def __init__(\n        self,\n        grouped_configs: List[GroupedEmbeddingConfig],\n        device: Optional[torch.device] = None,\n        pg: Optional[dist.ProcessGroup] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n        scale_weight_gradients: bool = True,\n    ) -> None:\n        # TODO rename to _create_embedding_kernel\n        def _create_lookup(\n            config: GroupedEmbeddingConfig,\n            device: Optional[torch.device] = None,\n        ) -> BaseEmbedding:\n            for table in config.embedding_tables:\n                if table.compute_kernel == EmbeddingComputeKernel.FUSED_UVM_CACHING:\n                    self._need_prefetch = True\n            if config.compute_kernel == EmbeddingComputeKernel.DENSE:\n                return BatchedDenseEmbeddingBag(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            elif config.compute_kernel == EmbeddingComputeKernel.FUSED:\n                return BatchedFusedEmbeddingBag(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            else:\n                raise ValueError(\n                    f\"Compute kernel not supported {config.compute_kernel}\"\n                )\n\n        super().__init__()\n        self._emb_modules: nn.ModuleList = nn.ModuleList()\n        self._need_prefetch = False\n        for config in grouped_configs:\n            self._emb_modules.append(_create_lookup(config, device))\n\n        self._feature_splits: List[int] = []\n        for config in grouped_configs:\n            self._feature_splits.append(config.num_features())\n\n        # return a dummy empty tensor when grouped_configs is empty\n        self.register_buffer(\n            \"_dummy_embs_tensor\",\n            torch.empty(\n                [0],\n                dtype=torch.float32,\n                device=device,\n                requires_grad=True,\n            ),\n        )\n\n        self.grouped_configs = grouped_configs\n        self._feature_processor = feature_processor\n\n        self._scale_gradient_factor: int = (\n            dist.get_world_size(pg)\n            if scale_weight_gradients and get_gradient_division()\n            else 1\n        )\n\n    def prefetch(\n        self,\n        sparse_features: KeyedJaggedTensor,\n        forward_stream: Optional[torch.cuda.Stream] = None,\n    ) -> None:\n        if not self._need_prefetch:\n            return\n        if len(self._emb_modules) > 0:\n            assert sparse_features is not None\n            features_by_group = sparse_features.split(\n                self._feature_splits,\n            )\n            for emb_op, features in zip(self._emb_modules, features_by_group):\n                if (\n                    isinstance(emb_op.emb_module, SplitTableBatchedEmbeddingBagsCodegen)\n                    and not emb_op.emb_module.prefetch_pipeline\n                ):\n                    logging.error(\n                        \"Invalid setting on SplitTableBatchedEmbeddingBagsCodegen modules. prefetch_pipeline must be set to True.\\n\"\n                        \"If you don\u2019t turn on prefetch_pipeline, cache locations might be wrong in backward and can cause wrong results.\\n\"\n                    )\n                if hasattr(emb_op.emb_module, \"prefetch\"):\n                    emb_op.emb_module.prefetch(\n                        indices=features.values(),\n                        offsets=features.offsets(),\n                        forward_stream=forward_stream,\n                    )\n\n    def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        embeddings: List[torch.Tensor] = []\n        if len(self._emb_modules) > 0:\n            assert sparse_features is not None\n            features_by_group = sparse_features.split(\n                self._feature_splits,\n            )\n            for config, emb_op, features in zip(\n                self.grouped_configs, self._emb_modules, features_by_group\n            ):\n                if (\n                    config.has_feature_processor\n                    and self._feature_processor is not None\n                    and isinstance(self._feature_processor, BaseGroupedFeatureProcessor)\n                ):\n                    features = self._feature_processor(features)\n\n                if config.is_weighted:\n                    features._weights = CommOpGradientScaling.apply(\n                        features._weights, self._scale_gradient_factor\n                    )\n\n                embeddings.append(emb_op(features))\n\n        dummy_embedding = (\n            self._dummy_embs_tensor\n            if sparse_features.variable_stride_per_key()\n            else fx_wrap_tensor_view2d(\n                self._dummy_embs_tensor, sparse_features.stride(), 0\n            )\n        )\n        return embeddings_cat_empty_rank_handle(\n            embeddings,\n            dummy_embedding,\n            dim=1,\n        )\n\n    # pyre-fixme[14]: `state_dict` overrides method defined in `Module` inconsistently.\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n\n        for emb_module in self._emb_modules:\n            emb_module.state_dict(destination, prefix, keep_vars)\n\n        return destination\n\n    # pyre-fixme[14]: `load_state_dict` overrides method defined in `Module`\n    #  inconsistently.\n    def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, Union[ShardedTensor, torch.Tensor]]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        m, u = _load_state_dict(self._emb_modules, state_dict)\n        return _IncompatibleKeys(missing_keys=m, unexpected_keys=u)\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_parameters for\"\n            \"GroupedPooledEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_parameters(prefix, recurse)\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_buffers for\"\n            \"GroupedPooledEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_buffers(prefix, recurse)\n\n    def named_parameters_by_table(\n        self,\n    ) -> Iterator[Tuple[str, TableBatchedEmbeddingSlice]]:\n        \"\"\"\n        Like named_parameters(), but yields table_name and embedding_weights which are wrapped in TableBatchedEmbeddingSlice.\n        For a single table with multiple shards (i.e CW) these are combined into one table/weight.\n        Used in composability.\n        \"\"\"\n        for embedding_kernel in self._emb_modules:\n            for (\n                table_name,\n                tbe_slice,\n            ) in embedding_kernel.named_parameters_by_table():\n                yield (table_name, tbe_slice)",
  "class MetaInferGroupedEmbeddingsLookup(\n    BaseEmbeddingLookup[KeyedJaggedTensor, torch.Tensor], TBEToRegisterMixIn\n):\n    \"\"\"\n    meta embedding lookup module for inference since inference lookup has references\n    for multiple TBE ops over all gpu workers.\n    inference grouped embedding lookup module contains meta modules allocated over gpu workers.\n    \"\"\"\n\n    def __init__(\n        self,\n        grouped_configs: List[GroupedEmbeddingConfig],\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        # TODO rename to _create_embedding_kernel\n        def _create_lookup(\n            config: GroupedEmbeddingConfig,\n            device: Optional[torch.device] = None,\n            fused_params: Optional[Dict[str, Any]] = None,\n        ) -> BaseBatchedEmbedding[\n            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]\n        ]:\n            return QuantBatchedEmbedding(\n                config=config,\n                device=device,\n                fused_params=fused_params,\n            )\n\n        super().__init__()\n        self._emb_modules: nn.ModuleList = nn.ModuleList()\n        for config in grouped_configs:\n            self._emb_modules.append(_create_lookup(config, device, fused_params))\n\n        self._feature_splits: List[int] = [\n            config.num_features() for config in grouped_configs\n        ]\n\n        # return a dummy empty tensor when grouped_configs is empty\n        self.register_buffer(\n            \"_dummy_embs_tensor\",\n            torch.empty(\n                [0],\n                dtype=torch.float32,\n                device=device,\n            ),\n        )\n\n        self.grouped_configs = grouped_configs\n\n    def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return get_tbes_to_register_from_iterable(self._emb_modules)\n\n    def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        embeddings: List[torch.Tensor] = []\n        features_by_group = sparse_features.split(\n            self._feature_splits,\n        )\n        for i in range(len(self._emb_modules)):\n            embeddings.append(\n                self._emb_modules[i].forward(features_by_group[i]).view(-1)\n            )\n\n        return embeddings_cat_empty_rank_handle(embeddings, self._dummy_embs_tensor)\n\n    # pyre-ignore [14]\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n\n        for emb_module in self._emb_modules:\n            emb_module.state_dict(destination, prefix, keep_vars)\n\n        return destination\n\n    # pyre-fixme[14]: `load_state_dict` overrides method defined in `Module`\n    #  inconsistently.\n    def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, Union[ShardedTensor, torch.Tensor]]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        m, u = _load_state_dict(self._emb_modules, state_dict)\n        return _IncompatibleKeys(missing_keys=m, unexpected_keys=u)\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_buffers for\"\n            \"MetaInferGroupedEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_parameters(prefix, recurse)\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_buffers for\"\n            \"MetaInferGroupedEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_buffers(prefix, recurse)",
  "class MetaInferGroupedPooledEmbeddingsLookup(\n    BaseEmbeddingLookup[KeyedJaggedTensor, torch.Tensor], TBEToRegisterMixIn\n):\n    \"\"\"\n    meta embedding bag lookup module for inference since inference lookup has references\n    for multiple TBE ops over all gpu workers.\n    inference grouped embedding bag lookup module contains meta modules allocated over gpu workers.\n    \"\"\"\n\n    def __init__(\n        self,\n        grouped_configs: List[GroupedEmbeddingConfig],\n        device: Optional[torch.device] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        # TODO rename to _create_embedding_kernel\n        def _create_lookup(\n            config: GroupedEmbeddingConfig,\n            device: Optional[torch.device] = None,\n            fused_params: Optional[Dict[str, Any]] = None,\n        ) -> BaseBatchedEmbeddingBag[\n            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]\n        ]:\n            return QuantBatchedEmbeddingBag(\n                config=config,\n                device=device,\n                fused_params=fused_params,\n            )\n\n        super().__init__()\n        self._emb_modules: nn.ModuleList = nn.ModuleList()\n        for config in grouped_configs:\n            self._emb_modules.append(_create_lookup(config, device, fused_params))\n\n        self._feature_splits: List[int] = [\n            config.num_features() for config in grouped_configs\n        ]\n\n        # return a dummy empty tensor when grouped_configs is empty\n        self.register_buffer(\n            \"_dummy_embs_tensor\",\n            torch.empty(\n                [0],\n                dtype=torch.float16,\n                device=device,\n            ),\n        )\n\n        self.grouped_configs = grouped_configs\n        self._feature_processor = feature_processor\n\n    def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return get_tbes_to_register_from_iterable(self._emb_modules)\n\n    def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        embeddings: List[torch.Tensor] = []\n        features_by_group = sparse_features.split(\n            self._feature_splits,\n        )\n        # syntax for torchscript\n        for i, (config, emb_op) in enumerate(\n            zip(self.grouped_configs, self._emb_modules)\n        ):\n            features = features_by_group[i]\n            if (\n                config.has_feature_processor\n                and self._feature_processor is not None\n                and isinstance(self._feature_processor, BaseGroupedFeatureProcessor)\n            ):\n                features = self._feature_processor(features)\n            embeddings.append(emb_op.forward(features))\n\n        return embeddings_cat_empty_rank_handle(\n            embeddings,\n            fx_wrap_tensor_view2d(self._dummy_embs_tensor, sparse_features.stride(), 0),\n            dim=1,\n        )\n\n    # pyre-ignore [14]\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n\n        for emb_module in self._emb_modules:\n            emb_module.state_dict(destination, prefix, keep_vars)\n\n        return destination\n\n    # pyre-fixme[14]: `load_state_dict` overrides method defined in `Module`\n    #  inconsistently.\n    def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, Union[ShardedTensor, torch.Tensor]]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        m, u = _load_state_dict(self._emb_modules, state_dict)\n        return _IncompatibleKeys(missing_keys=m, unexpected_keys=u)\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_parameters for\"\n            \"MetaInferGroupedPooledEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_parameters(prefix, recurse)\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_buffers for\"\n            \"MetaInferGroupedPooledEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_buffers(prefix, recurse)",
  "class InferGroupedLookupMixin(ABC):\n    def forward(\n        self,\n        sparse_features: KJTList,\n    ) -> List[torch.Tensor]:\n        embeddings: List[torch.Tensor] = []\n        # syntax for torchscript\n        for i, embedding_lookup in enumerate(\n            # pyre-fixme[16]\n            self._embedding_lookups_per_rank,\n        ):\n            sparse_features_rank = sparse_features[i]\n            embeddings.append(embedding_lookup.forward(sparse_features_rank))\n        return embeddings\n\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n\n        # pyre-fixme[16]\n        for rank_modules in self._embedding_lookups_per_rank:\n            rank_modules.state_dict(destination, prefix, keep_vars)\n\n        return destination\n\n    def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, torch.Tensor]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        missing_keys = []\n        unexpected_keys = []\n        # pyre-fixme[16]\n        for rank_modules in self._embedding_lookups_per_rank:\n            incompatible_keys = rank_modules.load_state_dict(state_dict)\n            missing_keys.extend(incompatible_keys.missing_keys)\n            unexpected_keys.extend(incompatible_keys.unexpected_keys)\n        return _IncompatibleKeys(\n            missing_keys=missing_keys, unexpected_keys=unexpected_keys\n        )\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        # pyre-fixme[16]\n        for rank_modules in self._embedding_lookups_per_rank:\n            yield from rank_modules.named_parameters(prefix, recurse)\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        # pyre-fixme[16]\n        for rank_modules in self._embedding_lookups_per_rank:\n            yield from rank_modules.named_buffers(prefix, recurse)",
  "class InferGroupedPooledEmbeddingsLookup(\n    InferGroupedLookupMixin,\n    BaseEmbeddingLookup[KJTList, List[torch.Tensor]],\n    TBEToRegisterMixIn,\n):\n    def __init__(\n        self,\n        grouped_configs_per_rank: List[List[GroupedEmbeddingConfig]],\n        world_size: int,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._embedding_lookups_per_rank: List[\n            MetaInferGroupedPooledEmbeddingsLookup\n        ] = []\n\n        device_type = \"meta\" if device is not None and device.type == \"meta\" else \"cuda\"\n        for rank in range(world_size):\n            self._embedding_lookups_per_rank.append(\n                # TODO add position weighted module support\n                MetaInferGroupedPooledEmbeddingsLookup(\n                    grouped_configs=grouped_configs_per_rank[rank],\n                    # syntax for torchscript\n                    device=torch.device(type=device_type, index=rank),\n                    fused_params=fused_params,\n                )\n            )\n\n    def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return get_tbes_to_register_from_iterable(self._embedding_lookups_per_rank)",
  "class InferGroupedEmbeddingsLookup(\n    InferGroupedLookupMixin,\n    BaseEmbeddingLookup[KJTList, List[torch.Tensor]],\n    TBEToRegisterMixIn,\n):\n    def __init__(\n        self,\n        grouped_configs_per_rank: List[List[GroupedEmbeddingConfig]],\n        world_size: int,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._embedding_lookups_per_rank: List[MetaInferGroupedEmbeddingsLookup] = []\n\n        device_type = \"meta\" if device is not None and device.type == \"meta\" else \"cuda\"\n        for rank in range(world_size):\n            self._embedding_lookups_per_rank.append(\n                MetaInferGroupedEmbeddingsLookup(\n                    grouped_configs=grouped_configs_per_rank[rank],\n                    # syntax for torchscript\n                    device=torch.device(type=device_type, index=rank),\n                    fused_params=fused_params,\n                )\n            )\n\n    def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return get_tbes_to_register_from_iterable(self._embedding_lookups_per_rank)",
  "def __init__(\n        self,\n        grouped_configs: List[GroupedEmbeddingConfig],\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        # TODO rename to _create_embedding_kernel\n        def _create_lookup(\n            config: GroupedEmbeddingConfig,\n        ) -> BaseEmbedding:\n            if config.compute_kernel == EmbeddingComputeKernel.DENSE:\n                return BatchedDenseEmbedding(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            elif config.compute_kernel == EmbeddingComputeKernel.FUSED:\n                return BatchedFusedEmbedding(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            else:\n                raise ValueError(\n                    f\"Compute kernel not supported {config.compute_kernel}\"\n                )\n\n        super().__init__()\n        self._emb_modules: nn.ModuleList = nn.ModuleList()\n        for config in grouped_configs:\n            self._emb_modules.append(_create_lookup(config))\n\n        self._feature_splits: List[int] = []\n        for config in grouped_configs:\n            self._feature_splits.append(config.num_features())\n\n        # return a dummy empty tensor when grouped_configs is empty\n        self.register_buffer(\n            \"_dummy_embs_tensor\",\n            torch.empty(\n                [0],\n                dtype=torch.float32,\n                device=device,\n                requires_grad=True,\n            ),\n        )\n\n        self.grouped_configs = grouped_configs",
  "def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        embeddings: List[torch.Tensor] = []\n        features_by_group = sparse_features.split(\n            self._feature_splits,\n        )\n        for emb_op, features in zip(self._emb_modules, features_by_group):\n            embeddings.append(emb_op(features).view(-1))\n\n        return embeddings_cat_empty_rank_handle(embeddings, self._dummy_embs_tensor)",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n\n        for emb_module in self._emb_modules:\n            emb_module.state_dict(destination, prefix, keep_vars)\n\n        return destination",
  "def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, Union[torch.Tensor, ShardedTensor]]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        m, u = _load_state_dict(self._emb_modules, state_dict)\n        return _IncompatibleKeys(missing_keys=m, unexpected_keys=u)",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_parameters for\"\n            \"GroupedEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_parameters(prefix, recurse)",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_buffers for\"\n            \"GroupedEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_buffers(prefix, recurse)",
  "def named_parameters_by_table(\n        self,\n    ) -> Iterator[Tuple[str, TableBatchedEmbeddingSlice]]:\n        \"\"\"\n        Like named_parameters(), but yields table_name and embedding_weights which are wrapped in TableBatchedEmbeddingSlice.\n        For a single table with multiple shards (i.e CW) these are combined into one table/weight.\n        Used in composability.\n        \"\"\"\n        for embedding_kernel in self._emb_modules:\n            for (\n                table_name,\n                tbe_slice,\n            ) in embedding_kernel.named_parameters_by_table():\n                yield (table_name, tbe_slice)",
  "def forward(\n        ctx: FunctionCtx, input_tensor: torch.Tensor, scale_gradient_factor: int\n    ) -> torch.Tensor:\n        # pyre-ignore\n        ctx.scale_gradient_factor = scale_gradient_factor\n        return input_tensor",
  "def backward(\n        ctx: FunctionCtx, grad_output: torch.Tensor\n    ) -> Tuple[torch.Tensor, None]:\n        # When gradient division is on, we scale down the gradient by world size\n        # at alltoall backward for model parallelism. However weights\n        # is controlled by DDP so it already has gradient division, so we scale\n        # the gradient back up\n        # pyre-ignore[16]: `FunctionCtx` has no attribute `scale_gradient_factor`\n        grad_output.mul_(ctx.scale_gradient_factor)\n        return grad_output, None",
  "def __init__(\n        self,\n        grouped_configs: List[GroupedEmbeddingConfig],\n        device: Optional[torch.device] = None,\n        pg: Optional[dist.ProcessGroup] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n        scale_weight_gradients: bool = True,\n    ) -> None:\n        # TODO rename to _create_embedding_kernel\n        def _create_lookup(\n            config: GroupedEmbeddingConfig,\n            device: Optional[torch.device] = None,\n        ) -> BaseEmbedding:\n            for table in config.embedding_tables:\n                if table.compute_kernel == EmbeddingComputeKernel.FUSED_UVM_CACHING:\n                    self._need_prefetch = True\n            if config.compute_kernel == EmbeddingComputeKernel.DENSE:\n                return BatchedDenseEmbeddingBag(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            elif config.compute_kernel == EmbeddingComputeKernel.FUSED:\n                return BatchedFusedEmbeddingBag(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            else:\n                raise ValueError(\n                    f\"Compute kernel not supported {config.compute_kernel}\"\n                )\n\n        super().__init__()\n        self._emb_modules: nn.ModuleList = nn.ModuleList()\n        self._need_prefetch = False\n        for config in grouped_configs:\n            self._emb_modules.append(_create_lookup(config, device))\n\n        self._feature_splits: List[int] = []\n        for config in grouped_configs:\n            self._feature_splits.append(config.num_features())\n\n        # return a dummy empty tensor when grouped_configs is empty\n        self.register_buffer(\n            \"_dummy_embs_tensor\",\n            torch.empty(\n                [0],\n                dtype=torch.float32,\n                device=device,\n                requires_grad=True,\n            ),\n        )\n\n        self.grouped_configs = grouped_configs\n        self._feature_processor = feature_processor\n\n        self._scale_gradient_factor: int = (\n            dist.get_world_size(pg)\n            if scale_weight_gradients and get_gradient_division()\n            else 1\n        )",
  "def prefetch(\n        self,\n        sparse_features: KeyedJaggedTensor,\n        forward_stream: Optional[torch.cuda.Stream] = None,\n    ) -> None:\n        if not self._need_prefetch:\n            return\n        if len(self._emb_modules) > 0:\n            assert sparse_features is not None\n            features_by_group = sparse_features.split(\n                self._feature_splits,\n            )\n            for emb_op, features in zip(self._emb_modules, features_by_group):\n                if (\n                    isinstance(emb_op.emb_module, SplitTableBatchedEmbeddingBagsCodegen)\n                    and not emb_op.emb_module.prefetch_pipeline\n                ):\n                    logging.error(\n                        \"Invalid setting on SplitTableBatchedEmbeddingBagsCodegen modules. prefetch_pipeline must be set to True.\\n\"\n                        \"If you don\u2019t turn on prefetch_pipeline, cache locations might be wrong in backward and can cause wrong results.\\n\"\n                    )\n                if hasattr(emb_op.emb_module, \"prefetch\"):\n                    emb_op.emb_module.prefetch(\n                        indices=features.values(),\n                        offsets=features.offsets(),\n                        forward_stream=forward_stream,\n                    )",
  "def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        embeddings: List[torch.Tensor] = []\n        if len(self._emb_modules) > 0:\n            assert sparse_features is not None\n            features_by_group = sparse_features.split(\n                self._feature_splits,\n            )\n            for config, emb_op, features in zip(\n                self.grouped_configs, self._emb_modules, features_by_group\n            ):\n                if (\n                    config.has_feature_processor\n                    and self._feature_processor is not None\n                    and isinstance(self._feature_processor, BaseGroupedFeatureProcessor)\n                ):\n                    features = self._feature_processor(features)\n\n                if config.is_weighted:\n                    features._weights = CommOpGradientScaling.apply(\n                        features._weights, self._scale_gradient_factor\n                    )\n\n                embeddings.append(emb_op(features))\n\n        dummy_embedding = (\n            self._dummy_embs_tensor\n            if sparse_features.variable_stride_per_key()\n            else fx_wrap_tensor_view2d(\n                self._dummy_embs_tensor, sparse_features.stride(), 0\n            )\n        )\n        return embeddings_cat_empty_rank_handle(\n            embeddings,\n            dummy_embedding,\n            dim=1,\n        )",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n\n        for emb_module in self._emb_modules:\n            emb_module.state_dict(destination, prefix, keep_vars)\n\n        return destination",
  "def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, Union[ShardedTensor, torch.Tensor]]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        m, u = _load_state_dict(self._emb_modules, state_dict)\n        return _IncompatibleKeys(missing_keys=m, unexpected_keys=u)",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_parameters for\"\n            \"GroupedPooledEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_parameters(prefix, recurse)",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_buffers for\"\n            \"GroupedPooledEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_buffers(prefix, recurse)",
  "def named_parameters_by_table(\n        self,\n    ) -> Iterator[Tuple[str, TableBatchedEmbeddingSlice]]:\n        \"\"\"\n        Like named_parameters(), but yields table_name and embedding_weights which are wrapped in TableBatchedEmbeddingSlice.\n        For a single table with multiple shards (i.e CW) these are combined into one table/weight.\n        Used in composability.\n        \"\"\"\n        for embedding_kernel in self._emb_modules:\n            for (\n                table_name,\n                tbe_slice,\n            ) in embedding_kernel.named_parameters_by_table():\n                yield (table_name, tbe_slice)",
  "def __init__(\n        self,\n        grouped_configs: List[GroupedEmbeddingConfig],\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        # TODO rename to _create_embedding_kernel\n        def _create_lookup(\n            config: GroupedEmbeddingConfig,\n            device: Optional[torch.device] = None,\n            fused_params: Optional[Dict[str, Any]] = None,\n        ) -> BaseBatchedEmbedding[\n            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]\n        ]:\n            return QuantBatchedEmbedding(\n                config=config,\n                device=device,\n                fused_params=fused_params,\n            )\n\n        super().__init__()\n        self._emb_modules: nn.ModuleList = nn.ModuleList()\n        for config in grouped_configs:\n            self._emb_modules.append(_create_lookup(config, device, fused_params))\n\n        self._feature_splits: List[int] = [\n            config.num_features() for config in grouped_configs\n        ]\n\n        # return a dummy empty tensor when grouped_configs is empty\n        self.register_buffer(\n            \"_dummy_embs_tensor\",\n            torch.empty(\n                [0],\n                dtype=torch.float32,\n                device=device,\n            ),\n        )\n\n        self.grouped_configs = grouped_configs",
  "def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return get_tbes_to_register_from_iterable(self._emb_modules)",
  "def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        embeddings: List[torch.Tensor] = []\n        features_by_group = sparse_features.split(\n            self._feature_splits,\n        )\n        for i in range(len(self._emb_modules)):\n            embeddings.append(\n                self._emb_modules[i].forward(features_by_group[i]).view(-1)\n            )\n\n        return embeddings_cat_empty_rank_handle(embeddings, self._dummy_embs_tensor)",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n\n        for emb_module in self._emb_modules:\n            emb_module.state_dict(destination, prefix, keep_vars)\n\n        return destination",
  "def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, Union[ShardedTensor, torch.Tensor]]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        m, u = _load_state_dict(self._emb_modules, state_dict)\n        return _IncompatibleKeys(missing_keys=m, unexpected_keys=u)",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_buffers for\"\n            \"MetaInferGroupedEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_parameters(prefix, recurse)",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_buffers for\"\n            \"MetaInferGroupedEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_buffers(prefix, recurse)",
  "def __init__(\n        self,\n        grouped_configs: List[GroupedEmbeddingConfig],\n        device: Optional[torch.device] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        # TODO rename to _create_embedding_kernel\n        def _create_lookup(\n            config: GroupedEmbeddingConfig,\n            device: Optional[torch.device] = None,\n            fused_params: Optional[Dict[str, Any]] = None,\n        ) -> BaseBatchedEmbeddingBag[\n            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]\n        ]:\n            return QuantBatchedEmbeddingBag(\n                config=config,\n                device=device,\n                fused_params=fused_params,\n            )\n\n        super().__init__()\n        self._emb_modules: nn.ModuleList = nn.ModuleList()\n        for config in grouped_configs:\n            self._emb_modules.append(_create_lookup(config, device, fused_params))\n\n        self._feature_splits: List[int] = [\n            config.num_features() for config in grouped_configs\n        ]\n\n        # return a dummy empty tensor when grouped_configs is empty\n        self.register_buffer(\n            \"_dummy_embs_tensor\",\n            torch.empty(\n                [0],\n                dtype=torch.float16,\n                device=device,\n            ),\n        )\n\n        self.grouped_configs = grouped_configs\n        self._feature_processor = feature_processor",
  "def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return get_tbes_to_register_from_iterable(self._emb_modules)",
  "def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> torch.Tensor:\n        embeddings: List[torch.Tensor] = []\n        features_by_group = sparse_features.split(\n            self._feature_splits,\n        )\n        # syntax for torchscript\n        for i, (config, emb_op) in enumerate(\n            zip(self.grouped_configs, self._emb_modules)\n        ):\n            features = features_by_group[i]\n            if (\n                config.has_feature_processor\n                and self._feature_processor is not None\n                and isinstance(self._feature_processor, BaseGroupedFeatureProcessor)\n            ):\n                features = self._feature_processor(features)\n            embeddings.append(emb_op.forward(features))\n\n        return embeddings_cat_empty_rank_handle(\n            embeddings,\n            fx_wrap_tensor_view2d(self._dummy_embs_tensor, sparse_features.stride(), 0),\n            dim=1,\n        )",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n\n        for emb_module in self._emb_modules:\n            emb_module.state_dict(destination, prefix, keep_vars)\n\n        return destination",
  "def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, Union[ShardedTensor, torch.Tensor]]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        m, u = _load_state_dict(self._emb_modules, state_dict)\n        return _IncompatibleKeys(missing_keys=m, unexpected_keys=u)",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_parameters for\"\n            \"MetaInferGroupedPooledEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_parameters(prefix, recurse)",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert remove_duplicate, (\n            \"remove_duplicate=False in named_buffers for\"\n            \"MetaInferGroupedPooledEmbeddingsLookup is not supported\"\n        )\n        for emb_module in self._emb_modules:\n            yield from emb_module.named_buffers(prefix, recurse)",
  "def forward(\n        self,\n        sparse_features: KJTList,\n    ) -> List[torch.Tensor]:\n        embeddings: List[torch.Tensor] = []\n        # syntax for torchscript\n        for i, embedding_lookup in enumerate(\n            # pyre-fixme[16]\n            self._embedding_lookups_per_rank,\n        ):\n            sparse_features_rank = sparse_features[i]\n            embeddings.append(embedding_lookup.forward(sparse_features_rank))\n        return embeddings",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n\n        # pyre-fixme[16]\n        for rank_modules in self._embedding_lookups_per_rank:\n            rank_modules.state_dict(destination, prefix, keep_vars)\n\n        return destination",
  "def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, torch.Tensor]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        missing_keys = []\n        unexpected_keys = []\n        # pyre-fixme[16]\n        for rank_modules in self._embedding_lookups_per_rank:\n            incompatible_keys = rank_modules.load_state_dict(state_dict)\n            missing_keys.extend(incompatible_keys.missing_keys)\n            unexpected_keys.extend(incompatible_keys.unexpected_keys)\n        return _IncompatibleKeys(\n            missing_keys=missing_keys, unexpected_keys=unexpected_keys\n        )",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        # pyre-fixme[16]\n        for rank_modules in self._embedding_lookups_per_rank:\n            yield from rank_modules.named_parameters(prefix, recurse)",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        # pyre-fixme[16]\n        for rank_modules in self._embedding_lookups_per_rank:\n            yield from rank_modules.named_buffers(prefix, recurse)",
  "def __init__(\n        self,\n        grouped_configs_per_rank: List[List[GroupedEmbeddingConfig]],\n        world_size: int,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._embedding_lookups_per_rank: List[\n            MetaInferGroupedPooledEmbeddingsLookup\n        ] = []\n\n        device_type = \"meta\" if device is not None and device.type == \"meta\" else \"cuda\"\n        for rank in range(world_size):\n            self._embedding_lookups_per_rank.append(\n                # TODO add position weighted module support\n                MetaInferGroupedPooledEmbeddingsLookup(\n                    grouped_configs=grouped_configs_per_rank[rank],\n                    # syntax for torchscript\n                    device=torch.device(type=device_type, index=rank),\n                    fused_params=fused_params,\n                )\n            )",
  "def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return get_tbes_to_register_from_iterable(self._embedding_lookups_per_rank)",
  "def __init__(\n        self,\n        grouped_configs_per_rank: List[List[GroupedEmbeddingConfig]],\n        world_size: int,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._embedding_lookups_per_rank: List[MetaInferGroupedEmbeddingsLookup] = []\n\n        device_type = \"meta\" if device is not None and device.type == \"meta\" else \"cuda\"\n        for rank in range(world_size):\n            self._embedding_lookups_per_rank.append(\n                MetaInferGroupedEmbeddingsLookup(\n                    grouped_configs=grouped_configs_per_rank[rank],\n                    # syntax for torchscript\n                    device=torch.device(type=device_type, index=rank),\n                    fused_params=fused_params,\n                )\n            )",
  "def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return get_tbes_to_register_from_iterable(self._embedding_lookups_per_rank)",
  "def _create_lookup(\n            config: GroupedEmbeddingConfig,\n        ) -> BaseEmbedding:\n            if config.compute_kernel == EmbeddingComputeKernel.DENSE:\n                return BatchedDenseEmbedding(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            elif config.compute_kernel == EmbeddingComputeKernel.FUSED:\n                return BatchedFusedEmbedding(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            else:\n                raise ValueError(\n                    f\"Compute kernel not supported {config.compute_kernel}\"\n                )",
  "def _create_lookup(\n            config: GroupedEmbeddingConfig,\n            device: Optional[torch.device] = None,\n        ) -> BaseEmbedding:\n            for table in config.embedding_tables:\n                if table.compute_kernel == EmbeddingComputeKernel.FUSED_UVM_CACHING:\n                    self._need_prefetch = True\n            if config.compute_kernel == EmbeddingComputeKernel.DENSE:\n                return BatchedDenseEmbeddingBag(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            elif config.compute_kernel == EmbeddingComputeKernel.FUSED:\n                return BatchedFusedEmbeddingBag(\n                    config=config,\n                    pg=pg,\n                    device=device,\n                )\n            else:\n                raise ValueError(\n                    f\"Compute kernel not supported {config.compute_kernel}\"\n                )",
  "def _create_lookup(\n            config: GroupedEmbeddingConfig,\n            device: Optional[torch.device] = None,\n            fused_params: Optional[Dict[str, Any]] = None,\n        ) -> BaseBatchedEmbedding[\n            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]\n        ]:\n            return QuantBatchedEmbedding(\n                config=config,\n                device=device,\n                fused_params=fused_params,\n            )",
  "def _create_lookup(\n            config: GroupedEmbeddingConfig,\n            device: Optional[torch.device] = None,\n            fused_params: Optional[Dict[str, Any]] = None,\n        ) -> BaseBatchedEmbeddingBag[\n            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]\n        ]:\n            return QuantBatchedEmbeddingBag(\n                config=config,\n                device=device,\n                fused_params=fused_params,\n            )",
  "class ManagedCollisionCollectionAwaitable(LazyAwaitable[KeyedJaggedTensor]):\n    def __init__(\n        self,\n        awaitables_per_sharding: List[Awaitable[torch.Tensor]],\n        features_per_sharding: List[KeyedJaggedTensor],\n        embedding_names_per_sharding: List[List[str]],\n        need_indices: bool = False,\n        features_to_permute_indices: Optional[Dict[str, List[int]]] = None,\n    ) -> None:\n        super().__init__()\n        self._awaitables_per_sharding = awaitables_per_sharding\n        self._features_per_sharding = features_per_sharding\n        self._need_indices = need_indices\n        self._features_to_permute_indices = features_to_permute_indices\n        self._embedding_names_per_sharding = embedding_names_per_sharding\n\n    def _wait_impl(self) -> KeyedJaggedTensor:\n        jt_dict: Dict[str, JaggedTensor] = {}\n        for w, f, e in zip(\n            self._awaitables_per_sharding,\n            self._features_per_sharding,\n            self._embedding_names_per_sharding,\n        ):\n            jt_dict.update(\n                _construct_jagged_tensors(\n                    embeddings=w.wait(),\n                    features=f,\n                    embedding_names=e,\n                    need_indices=self._need_indices,\n                    features_to_permute_indices=self._features_to_permute_indices,\n                )\n            )\n            # TODO: find better solution\n            for jt in jt_dict.values():\n                jt._values = jt.values().flatten()\n        return KeyedJaggedTensor.from_jt_dict(jt_dict)",
  "class ManagedCollisionCollectionContext(EmbeddingCollectionContext):\n    pass",
  "def create_mc_sharding(\n    sharding_type: str,\n    sharding_infos: List[EmbeddingShardingInfo],\n    env: ShardingEnv,\n    device: Optional[torch.device] = None,\n) -> EmbeddingSharding[\n    SequenceShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n]:\n    if sharding_type == ShardingType.ROW_WISE.value:\n        return RwSequenceEmbeddingSharding(\n            sharding_infos=sharding_infos,\n            env=env,\n            device=device,\n        )\n    else:\n        raise ValueError(f\"Sharding not supported {sharding_type}\")",
  "class ShardedManagedCollisionCollection(\n    ShardedModule[\n        KJTList,\n        KJTList,\n        KeyedJaggedTensor,\n        ManagedCollisionCollectionContext,\n    ]\n):\n    def __init__(\n        self,\n        module: ManagedCollisionCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: torch.device,\n        sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                EmbeddingShardingContext,\n                KeyedJaggedTensor,\n                torch.Tensor,\n                torch.Tensor,\n            ],\n        ],\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n\n        self._device = device\n        self._env = env\n        self._table_name_to_parameter_sharding: Dict[\n            str, ParameterSharding\n        ] = copy.deepcopy(table_name_to_parameter_sharding)\n        # TODO: create a MCSharding type instead of leveraging EmbeddingSharding\n        self._sharding_type_to_sharding = sharding_type_to_sharding\n\n        self._embedding_names_per_sharding: List[List[str]] = []\n        for sharding_type, sharding in self._sharding_type_to_sharding.items():\n            # TODO: support TWRW sharding\n            assert (\n                sharding_type == ShardingType.ROW_WISE.value\n            ), \"Only ROW_WISE sharding is supported.\"\n            self._embedding_names_per_sharding.append(sharding.embedding_names())\n\n        self._feature_to_table: Dict[str, str] = module._feature_to_table\n        self._table_to_features: Dict[str, List[str]] = module._table_to_features\n        self._has_uninitialized_input_dists: bool = True\n        self._input_dists: List[nn.Module] = []\n        self._managed_collision_modules = nn.ModuleDict()\n        self._create_managed_collision_modules(module)\n        self._output_dists: List[nn.Module] = []\n        self._create_output_dists()\n\n        self._initialize_torch_state()\n\n    def _initialize_torch_state(self) -> None:\n        self._model_parallel_mc_buffer_name_to_sharded_tensor = OrderedDict()\n        for table_name, mc_module in self._managed_collision_modules.items():\n            assert (\n                self._table_name_to_parameter_sharding[table_name].sharding_type\n                == ShardingType.ROW_WISE.value\n            )\n            mc_module_state_dict = mc_module.state_dict(prefix=table_name + \".\")\n            shardable_buffers = set.intersection(\n                {name for name, _ in mc_module.named_buffers(prefix=table_name)},\n                set(mc_module_state_dict.keys()),\n            )\n            shard_offset, shard_size, global_size = self._mc_module_name_shard_metadata[\n                table_name\n            ]\n            for name, tensor in mc_module_state_dict.items():\n\n                if name not in shardable_buffers:\n                    continue\n\n                self._model_parallel_mc_buffer_name_to_sharded_tensor[\n                    name\n                ] = ShardedTensor._init_from_local_shards(\n                    [\n                        Shard(\n                            tensor=tensor,\n                            metadata=ShardMetadata(\n                                # pyre-ignore [6]\n                                shard_offsets=[shard_offset],\n                                # pyre-ignore [6]\n                                shard_sizes=[shard_size],\n                                placement=(\n                                    f\"rank:{self._env.rank}/cuda:\"\n                                    f\"{get_local_rank(self._env.world_size, self._env.rank)}\"\n                                ),\n                            ),\n                        )\n                    ],\n                    # pyre-ignore [6]\n                    torch.Size([global_size]),\n                    process_group=self._env.process_group,\n                )\n\n        def _post_state_dict_hook(\n            module: ShardedManagedCollisionCollection,\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            for (\n                mc_buffer_name,\n                sharded_tensor,\n            ) in module._model_parallel_mc_buffer_name_to_sharded_tensor.items():\n                destination_key = f\"{prefix}_managed_collision_modules.{mc_buffer_name}\"\n                destination[destination_key] = sharded_tensor\n\n        def _load_state_dict_pre_hook(\n            module: \"ShardedManagedCollisionCollection\",\n            state_dict: Dict[str, Any],\n            prefix: str,\n            *args: Any,\n        ) -> None:\n            for (\n                mc_buffer_name,\n                _sharded_tensor,\n            ) in module._model_parallel_mc_buffer_name_to_sharded_tensor.items():\n                key = f\"{prefix}_managed_collision_modules.{mc_buffer_name}\"\n                if key in state_dict:\n                    if isinstance(state_dict[key], ShardedTensor):\n                        local_shards = state_dict[key].local_shards()\n                        state_dict[key] = local_shards[0].tensor\n                    else:\n                        raise RuntimeError(\n                            f\"Unexpected state_dict key type {type(state_dict[key])} found for {key}\"\n                        )\n\n        self._register_state_dict_hook(_post_state_dict_hook)\n        self._register_load_state_dict_pre_hook(\n            _load_state_dict_pre_hook, with_module=True\n        )\n\n    def _create_managed_collision_modules(\n        self, module: ManagedCollisionCollection\n    ) -> None:\n\n        self._mc_module_name_shard_metadata: DefaultDict[\n            str, DefaultDict[str, List[int]]\n        ] = defaultdict(lambda: defaultdict(list))\n        self._feature_to_offset: Dict[str, int] = {}\n\n        for sharding_type, sharding in self._sharding_type_to_sharding.items():\n            if sharding_type == ShardingType.ROW_WISE.value:\n                assert isinstance(sharding, BaseRwEmbeddingSharding)\n\n                grouped_embedding_configs: List[\n                    GroupedEmbeddingConfig\n                ] = sharding._grouped_embedding_configs\n                for group_config in grouped_embedding_configs:\n                    for table in group_config.embedding_tables:\n                        # pyre-ignore [16]\n                        new_min_output_id = table.local_metadata.shard_offsets[0]\n                        # pyre-ignore [16]\n                        new_range_size = table.local_metadata.shard_sizes[0]\n\n                        mc_module = module._managed_collision_modules[table.name]\n                        # current_max_output_id = mc_module._max_output_id\n\n                        # TODO:\n                        #  1) need to make TBE accept global indices for now force to local indices\n                        #  2) MCH is particularly nasty with a portion of each shard; ideally dont do this\n                        #  3) now create a feature_to_offset and pass into awaitable callbacks to act as raw id adder\n                        self._managed_collision_modules[\n                            table.name\n                        ] = mc_module.rebuild_with_output_id_range(\n                            output_id_range=(\n                                0,  # new_min_output_id,\n                                new_range_size,  # new_min_output_id + new_range_size,\n                            ),\n                            device=self._device,\n                        )\n                        zch_size = self._managed_collision_modules[table.name]._zch_size\n\n                        self._mc_module_name_shard_metadata[table.name] = (\n                            zch_size * self._env.rank,  # new_min_output_id,\n                            zch_size,  # new_range_size,\n                            zch_size * (self._env.world_size),  # current_max_output_id,\n                        )\n                        for feature in table.feature_names:\n                            self._feature_to_offset[feature] = new_min_output_id\n\n    def _create_input_dists(\n        self,\n        input_feature_names: List[str],\n    ) -> None:\n        feature_names: List[str] = []\n        self._feature_splits: List[int] = []\n        for sharding_type, sharding in self._sharding_type_to_sharding.items():\n            if sharding_type == ShardingType.ROW_WISE.value:\n                feature_hash_sizes: List[int] = [\n                    self._managed_collision_modules[\n                        self._feature_to_table[f]\n                    ].input_size()\n                    for f in sharding.feature_names()\n                ]\n\n                input_dist = RwSparseFeaturesDist(\n                    # pyre-ignore [16]\n                    pg=sharding._pg,\n                    # pyre-ignore [16]\n                    num_features=sharding._get_num_features(),\n                    feature_hash_sizes=feature_hash_sizes,\n                    # pyre-ignore [16]\n                    device=sharding._device,\n                    is_sequence=True,\n                    # pyre-ignore [16]\n                    has_feature_processor=sharding._has_feature_processor,\n                    need_pos=False,\n                )\n                self._input_dists.append(input_dist)\n                feature_names.extend(sharding.feature_names())\n                self._feature_splits.append(len(sharding.feature_names()))\n\n        self._features_order: List[int] = []\n        for f in feature_names:\n            self._features_order.append(input_feature_names.index(f))\n        self._features_order = (\n            []\n            if self._features_order == list(range(len(self._features_order)))\n            else self._features_order\n        )\n        self.register_buffer(\n            \"_features_order_tensor\",\n            torch.tensor(self._features_order, device=self._device, dtype=torch.int32),\n            persistent=False,\n        )\n\n    def _create_output_dists(\n        self,\n    ) -> None:\n        for sharding_type, sharding in self._sharding_type_to_sharding.items():\n            if sharding_type == ShardingType.ROW_WISE.value:\n                self._output_dists.append(\n                    RwSequenceEmbeddingDist(\n                        # pyre-ignore [16]\n                        sharding._pg,\n                        # pyre-ignore [16]\n                        sharding._get_num_features(),\n                        # pyre-ignore [16]\n                        sharding._device,\n                    )\n                )\n\n    # pyre-ignore [14]\n    def input_dist(\n        self,\n        ctx: ManagedCollisionCollectionContext,\n        features: KeyedJaggedTensor,\n        force_insert: bool = False,\n    ) -> Awaitable[Awaitable[KJTList]]:\n        self._force_insert = force_insert\n        if self._has_uninitialized_input_dists:\n            self._create_input_dists(input_feature_names=features.keys())\n            self._has_uninitialized_input_dists = False\n\n        with torch.no_grad():\n            # NOTE shared features not currently supported\n            features = KeyedJaggedTensor.from_jt_dict(\n                apply_mc_method_to_jt_dict(\n                    \"preprocess\",\n                    features.to_dict(),\n                    self._table_to_features,\n                    self._managed_collision_modules,\n                )\n            )\n            if self._features_order:\n                features = features.permute(\n                    self._features_order,\n                    self._features_order_tensor,\n                )\n            features_by_sharding = features.split(\n                self._feature_splits,\n            )\n            awaitables = []\n            for input_dist, features in zip(self._input_dists, features_by_sharding):\n                awaitables.append(input_dist(features))\n                ctx.sharding_contexts.append(\n                    SequenceShardingContext(\n                        features_before_input_dist=features,\n                        unbucketize_permute_tensor=input_dist.unbucketize_permute_tensor\n                        if isinstance(input_dist, RwSparseFeaturesDist)\n                        else None,\n                    )\n                )\n\n        return KJTListSplitsAwaitable(awaitables, ctx)\n\n    def _kjt_list_to_tensor_list(\n        self,\n        kjt_list: KJTList,\n        feature_to_offset: Dict[str, int],\n    ) -> List[torch.Tensor]:\n        remapped_ids_ret: List[torch.Tensor] = []\n        # TODO: find a better solution\n        for kjt in kjt_list:\n            jt_dict = kjt.to_dict()\n            for feature, jt in jt_dict.items():\n                offset = feature_to_offset[feature]\n                jt._values = jt.values().add(offset)\n            new_kjt = KeyedJaggedTensor.from_jt_dict(jt_dict)\n            remapped_ids_ret.append(new_kjt.values().view(-1, 1))\n        return remapped_ids_ret\n\n    def compute(\n        self,\n        ctx: ManagedCollisionCollectionContext,\n        dist_input: KJTList,\n    ) -> KJTList:\n        remapped_kjts: List[KeyedJaggedTensor] = []\n\n        for features, sharding_ctx in zip(\n            dist_input,\n            ctx.sharding_contexts,\n        ):\n            sharding_ctx.lengths_after_input_dist = features.lengths().view(\n                -1, features.stride()\n            )\n            features_dict = features.to_dict()\n            features_dict = apply_mc_method_to_jt_dict(\n                \"profile\",\n                features_dict=features_dict,\n                table_to_features=self._table_to_features,\n                managed_collisions=self._managed_collision_modules,\n                force_insert=self._force_insert,\n            )\n            features_dict = apply_mc_method_to_jt_dict(\n                \"remap\",\n                features_dict=features_dict,\n                table_to_features=self._table_to_features,\n                managed_collisions=self._managed_collision_modules,\n            )\n            remapped_kjts.append(KeyedJaggedTensor.from_jt_dict(features_dict))\n\n        return KJTList(remapped_kjts)\n\n    def evict(self) -> Dict[str, Optional[torch.Tensor]]:\n        evictions: Dict[str, Optional[torch.Tensor]] = {}\n        for (\n            table,\n            managed_collision_module,\n        ) in self._managed_collision_modules.items():\n            evictions[table] = managed_collision_module.evict()\n        return evictions\n\n    def output_dist(\n        self,\n        ctx: ManagedCollisionCollectionContext,\n        output: KJTList,\n    ) -> LazyAwaitable[KeyedJaggedTensor]:\n\n        global_remapped = self._kjt_list_to_tensor_list(output, self._feature_to_offset)\n        awaitables_per_sharding: List[Awaitable[torch.Tensor]] = []\n        features_before_all2all_per_sharding: List[KeyedJaggedTensor] = []\n        for odist, remapped_ids, sharding_ctx in zip(\n            self._output_dists,\n            global_remapped,\n            ctx.sharding_contexts,\n        ):\n            awaitables_per_sharding.append(odist(remapped_ids, sharding_ctx))\n            features_before_all2all_per_sharding.append(\n                # pyre-fixme[6]: For 1st argument expected `KeyedJaggedTensor` but\n                #  got `Optional[KeyedJaggedTensor]`.\n                sharding_ctx.features_before_input_dist\n            )\n        return ManagedCollisionCollectionAwaitable(\n            awaitables_per_sharding=awaitables_per_sharding,\n            features_per_sharding=features_before_all2all_per_sharding,\n            embedding_names_per_sharding=self._embedding_names_per_sharding,\n            need_indices=False,\n            features_to_permute_indices=None,\n        )\n\n    def create_context(self) -> ManagedCollisionCollectionContext:\n        return ManagedCollisionCollectionContext(sharding_contexts=[])\n\n    def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        for fqn, _ in self.named_buffers():\n            yield append_prefix(prefix, fqn)",
  "class ManagedCollisionCollectionSharder(\n    BaseEmbeddingSharder[ManagedCollisionCollection]\n):\n    def __init__(\n        self,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n\n    def shard(\n        self,\n        module: ManagedCollisionCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                EmbeddingShardingContext,\n                KeyedJaggedTensor,\n                torch.Tensor,\n                torch.Tensor,\n            ],\n        ],\n        device: Optional[torch.device] = None,\n    ) -> ShardedManagedCollisionCollection:\n\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        return ShardedManagedCollisionCollection(\n            module,\n            params,\n            env=env,\n            device=device,\n            sharding_type_to_sharding=sharding_type_to_sharding,\n        )\n\n    def shardable_parameters(\n        self, module: ManagedCollisionCollection\n    ) -> Dict[str, torch.nn.Parameter]:\n        # TODO: standalone sharding\n        raise NotImplementedError()\n\n    @property\n    def module_type(self) -> Type[ManagedCollisionCollection]:\n        return ManagedCollisionCollection\n\n    def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.ROW_WISE.value,\n        ]\n        return types",
  "def __init__(\n        self,\n        awaitables_per_sharding: List[Awaitable[torch.Tensor]],\n        features_per_sharding: List[KeyedJaggedTensor],\n        embedding_names_per_sharding: List[List[str]],\n        need_indices: bool = False,\n        features_to_permute_indices: Optional[Dict[str, List[int]]] = None,\n    ) -> None:\n        super().__init__()\n        self._awaitables_per_sharding = awaitables_per_sharding\n        self._features_per_sharding = features_per_sharding\n        self._need_indices = need_indices\n        self._features_to_permute_indices = features_to_permute_indices\n        self._embedding_names_per_sharding = embedding_names_per_sharding",
  "def _wait_impl(self) -> KeyedJaggedTensor:\n        jt_dict: Dict[str, JaggedTensor] = {}\n        for w, f, e in zip(\n            self._awaitables_per_sharding,\n            self._features_per_sharding,\n            self._embedding_names_per_sharding,\n        ):\n            jt_dict.update(\n                _construct_jagged_tensors(\n                    embeddings=w.wait(),\n                    features=f,\n                    embedding_names=e,\n                    need_indices=self._need_indices,\n                    features_to_permute_indices=self._features_to_permute_indices,\n                )\n            )\n            # TODO: find better solution\n            for jt in jt_dict.values():\n                jt._values = jt.values().flatten()\n        return KeyedJaggedTensor.from_jt_dict(jt_dict)",
  "def __init__(\n        self,\n        module: ManagedCollisionCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: torch.device,\n        sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                EmbeddingShardingContext,\n                KeyedJaggedTensor,\n                torch.Tensor,\n                torch.Tensor,\n            ],\n        ],\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n\n        self._device = device\n        self._env = env\n        self._table_name_to_parameter_sharding: Dict[\n            str, ParameterSharding\n        ] = copy.deepcopy(table_name_to_parameter_sharding)\n        # TODO: create a MCSharding type instead of leveraging EmbeddingSharding\n        self._sharding_type_to_sharding = sharding_type_to_sharding\n\n        self._embedding_names_per_sharding: List[List[str]] = []\n        for sharding_type, sharding in self._sharding_type_to_sharding.items():\n            # TODO: support TWRW sharding\n            assert (\n                sharding_type == ShardingType.ROW_WISE.value\n            ), \"Only ROW_WISE sharding is supported.\"\n            self._embedding_names_per_sharding.append(sharding.embedding_names())\n\n        self._feature_to_table: Dict[str, str] = module._feature_to_table\n        self._table_to_features: Dict[str, List[str]] = module._table_to_features\n        self._has_uninitialized_input_dists: bool = True\n        self._input_dists: List[nn.Module] = []\n        self._managed_collision_modules = nn.ModuleDict()\n        self._create_managed_collision_modules(module)\n        self._output_dists: List[nn.Module] = []\n        self._create_output_dists()\n\n        self._initialize_torch_state()",
  "def _initialize_torch_state(self) -> None:\n        self._model_parallel_mc_buffer_name_to_sharded_tensor = OrderedDict()\n        for table_name, mc_module in self._managed_collision_modules.items():\n            assert (\n                self._table_name_to_parameter_sharding[table_name].sharding_type\n                == ShardingType.ROW_WISE.value\n            )\n            mc_module_state_dict = mc_module.state_dict(prefix=table_name + \".\")\n            shardable_buffers = set.intersection(\n                {name for name, _ in mc_module.named_buffers(prefix=table_name)},\n                set(mc_module_state_dict.keys()),\n            )\n            shard_offset, shard_size, global_size = self._mc_module_name_shard_metadata[\n                table_name\n            ]\n            for name, tensor in mc_module_state_dict.items():\n\n                if name not in shardable_buffers:\n                    continue\n\n                self._model_parallel_mc_buffer_name_to_sharded_tensor[\n                    name\n                ] = ShardedTensor._init_from_local_shards(\n                    [\n                        Shard(\n                            tensor=tensor,\n                            metadata=ShardMetadata(\n                                # pyre-ignore [6]\n                                shard_offsets=[shard_offset],\n                                # pyre-ignore [6]\n                                shard_sizes=[shard_size],\n                                placement=(\n                                    f\"rank:{self._env.rank}/cuda:\"\n                                    f\"{get_local_rank(self._env.world_size, self._env.rank)}\"\n                                ),\n                            ),\n                        )\n                    ],\n                    # pyre-ignore [6]\n                    torch.Size([global_size]),\n                    process_group=self._env.process_group,\n                )\n\n        def _post_state_dict_hook(\n            module: ShardedManagedCollisionCollection,\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            for (\n                mc_buffer_name,\n                sharded_tensor,\n            ) in module._model_parallel_mc_buffer_name_to_sharded_tensor.items():\n                destination_key = f\"{prefix}_managed_collision_modules.{mc_buffer_name}\"\n                destination[destination_key] = sharded_tensor\n\n        def _load_state_dict_pre_hook(\n            module: \"ShardedManagedCollisionCollection\",\n            state_dict: Dict[str, Any],\n            prefix: str,\n            *args: Any,\n        ) -> None:\n            for (\n                mc_buffer_name,\n                _sharded_tensor,\n            ) in module._model_parallel_mc_buffer_name_to_sharded_tensor.items():\n                key = f\"{prefix}_managed_collision_modules.{mc_buffer_name}\"\n                if key in state_dict:\n                    if isinstance(state_dict[key], ShardedTensor):\n                        local_shards = state_dict[key].local_shards()\n                        state_dict[key] = local_shards[0].tensor\n                    else:\n                        raise RuntimeError(\n                            f\"Unexpected state_dict key type {type(state_dict[key])} found for {key}\"\n                        )\n\n        self._register_state_dict_hook(_post_state_dict_hook)\n        self._register_load_state_dict_pre_hook(\n            _load_state_dict_pre_hook, with_module=True\n        )",
  "def _create_managed_collision_modules(\n        self, module: ManagedCollisionCollection\n    ) -> None:\n\n        self._mc_module_name_shard_metadata: DefaultDict[\n            str, DefaultDict[str, List[int]]\n        ] = defaultdict(lambda: defaultdict(list))\n        self._feature_to_offset: Dict[str, int] = {}\n\n        for sharding_type, sharding in self._sharding_type_to_sharding.items():\n            if sharding_type == ShardingType.ROW_WISE.value:\n                assert isinstance(sharding, BaseRwEmbeddingSharding)\n\n                grouped_embedding_configs: List[\n                    GroupedEmbeddingConfig\n                ] = sharding._grouped_embedding_configs\n                for group_config in grouped_embedding_configs:\n                    for table in group_config.embedding_tables:\n                        # pyre-ignore [16]\n                        new_min_output_id = table.local_metadata.shard_offsets[0]\n                        # pyre-ignore [16]\n                        new_range_size = table.local_metadata.shard_sizes[0]\n\n                        mc_module = module._managed_collision_modules[table.name]\n                        # current_max_output_id = mc_module._max_output_id\n\n                        # TODO:\n                        #  1) need to make TBE accept global indices for now force to local indices\n                        #  2) MCH is particularly nasty with a portion of each shard; ideally dont do this\n                        #  3) now create a feature_to_offset and pass into awaitable callbacks to act as raw id adder\n                        self._managed_collision_modules[\n                            table.name\n                        ] = mc_module.rebuild_with_output_id_range(\n                            output_id_range=(\n                                0,  # new_min_output_id,\n                                new_range_size,  # new_min_output_id + new_range_size,\n                            ),\n                            device=self._device,\n                        )\n                        zch_size = self._managed_collision_modules[table.name]._zch_size\n\n                        self._mc_module_name_shard_metadata[table.name] = (\n                            zch_size * self._env.rank,  # new_min_output_id,\n                            zch_size,  # new_range_size,\n                            zch_size * (self._env.world_size),  # current_max_output_id,\n                        )\n                        for feature in table.feature_names:\n                            self._feature_to_offset[feature] = new_min_output_id",
  "def _create_input_dists(\n        self,\n        input_feature_names: List[str],\n    ) -> None:\n        feature_names: List[str] = []\n        self._feature_splits: List[int] = []\n        for sharding_type, sharding in self._sharding_type_to_sharding.items():\n            if sharding_type == ShardingType.ROW_WISE.value:\n                feature_hash_sizes: List[int] = [\n                    self._managed_collision_modules[\n                        self._feature_to_table[f]\n                    ].input_size()\n                    for f in sharding.feature_names()\n                ]\n\n                input_dist = RwSparseFeaturesDist(\n                    # pyre-ignore [16]\n                    pg=sharding._pg,\n                    # pyre-ignore [16]\n                    num_features=sharding._get_num_features(),\n                    feature_hash_sizes=feature_hash_sizes,\n                    # pyre-ignore [16]\n                    device=sharding._device,\n                    is_sequence=True,\n                    # pyre-ignore [16]\n                    has_feature_processor=sharding._has_feature_processor,\n                    need_pos=False,\n                )\n                self._input_dists.append(input_dist)\n                feature_names.extend(sharding.feature_names())\n                self._feature_splits.append(len(sharding.feature_names()))\n\n        self._features_order: List[int] = []\n        for f in feature_names:\n            self._features_order.append(input_feature_names.index(f))\n        self._features_order = (\n            []\n            if self._features_order == list(range(len(self._features_order)))\n            else self._features_order\n        )\n        self.register_buffer(\n            \"_features_order_tensor\",\n            torch.tensor(self._features_order, device=self._device, dtype=torch.int32),\n            persistent=False,\n        )",
  "def _create_output_dists(\n        self,\n    ) -> None:\n        for sharding_type, sharding in self._sharding_type_to_sharding.items():\n            if sharding_type == ShardingType.ROW_WISE.value:\n                self._output_dists.append(\n                    RwSequenceEmbeddingDist(\n                        # pyre-ignore [16]\n                        sharding._pg,\n                        # pyre-ignore [16]\n                        sharding._get_num_features(),\n                        # pyre-ignore [16]\n                        sharding._device,\n                    )\n                )",
  "def input_dist(\n        self,\n        ctx: ManagedCollisionCollectionContext,\n        features: KeyedJaggedTensor,\n        force_insert: bool = False,\n    ) -> Awaitable[Awaitable[KJTList]]:\n        self._force_insert = force_insert\n        if self._has_uninitialized_input_dists:\n            self._create_input_dists(input_feature_names=features.keys())\n            self._has_uninitialized_input_dists = False\n\n        with torch.no_grad():\n            # NOTE shared features not currently supported\n            features = KeyedJaggedTensor.from_jt_dict(\n                apply_mc_method_to_jt_dict(\n                    \"preprocess\",\n                    features.to_dict(),\n                    self._table_to_features,\n                    self._managed_collision_modules,\n                )\n            )\n            if self._features_order:\n                features = features.permute(\n                    self._features_order,\n                    self._features_order_tensor,\n                )\n            features_by_sharding = features.split(\n                self._feature_splits,\n            )\n            awaitables = []\n            for input_dist, features in zip(self._input_dists, features_by_sharding):\n                awaitables.append(input_dist(features))\n                ctx.sharding_contexts.append(\n                    SequenceShardingContext(\n                        features_before_input_dist=features,\n                        unbucketize_permute_tensor=input_dist.unbucketize_permute_tensor\n                        if isinstance(input_dist, RwSparseFeaturesDist)\n                        else None,\n                    )\n                )\n\n        return KJTListSplitsAwaitable(awaitables, ctx)",
  "def _kjt_list_to_tensor_list(\n        self,\n        kjt_list: KJTList,\n        feature_to_offset: Dict[str, int],\n    ) -> List[torch.Tensor]:\n        remapped_ids_ret: List[torch.Tensor] = []\n        # TODO: find a better solution\n        for kjt in kjt_list:\n            jt_dict = kjt.to_dict()\n            for feature, jt in jt_dict.items():\n                offset = feature_to_offset[feature]\n                jt._values = jt.values().add(offset)\n            new_kjt = KeyedJaggedTensor.from_jt_dict(jt_dict)\n            remapped_ids_ret.append(new_kjt.values().view(-1, 1))\n        return remapped_ids_ret",
  "def compute(\n        self,\n        ctx: ManagedCollisionCollectionContext,\n        dist_input: KJTList,\n    ) -> KJTList:\n        remapped_kjts: List[KeyedJaggedTensor] = []\n\n        for features, sharding_ctx in zip(\n            dist_input,\n            ctx.sharding_contexts,\n        ):\n            sharding_ctx.lengths_after_input_dist = features.lengths().view(\n                -1, features.stride()\n            )\n            features_dict = features.to_dict()\n            features_dict = apply_mc_method_to_jt_dict(\n                \"profile\",\n                features_dict=features_dict,\n                table_to_features=self._table_to_features,\n                managed_collisions=self._managed_collision_modules,\n                force_insert=self._force_insert,\n            )\n            features_dict = apply_mc_method_to_jt_dict(\n                \"remap\",\n                features_dict=features_dict,\n                table_to_features=self._table_to_features,\n                managed_collisions=self._managed_collision_modules,\n            )\n            remapped_kjts.append(KeyedJaggedTensor.from_jt_dict(features_dict))\n\n        return KJTList(remapped_kjts)",
  "def evict(self) -> Dict[str, Optional[torch.Tensor]]:\n        evictions: Dict[str, Optional[torch.Tensor]] = {}\n        for (\n            table,\n            managed_collision_module,\n        ) in self._managed_collision_modules.items():\n            evictions[table] = managed_collision_module.evict()\n        return evictions",
  "def output_dist(\n        self,\n        ctx: ManagedCollisionCollectionContext,\n        output: KJTList,\n    ) -> LazyAwaitable[KeyedJaggedTensor]:\n\n        global_remapped = self._kjt_list_to_tensor_list(output, self._feature_to_offset)\n        awaitables_per_sharding: List[Awaitable[torch.Tensor]] = []\n        features_before_all2all_per_sharding: List[KeyedJaggedTensor] = []\n        for odist, remapped_ids, sharding_ctx in zip(\n            self._output_dists,\n            global_remapped,\n            ctx.sharding_contexts,\n        ):\n            awaitables_per_sharding.append(odist(remapped_ids, sharding_ctx))\n            features_before_all2all_per_sharding.append(\n                # pyre-fixme[6]: For 1st argument expected `KeyedJaggedTensor` but\n                #  got `Optional[KeyedJaggedTensor]`.\n                sharding_ctx.features_before_input_dist\n            )\n        return ManagedCollisionCollectionAwaitable(\n            awaitables_per_sharding=awaitables_per_sharding,\n            features_per_sharding=features_before_all2all_per_sharding,\n            embedding_names_per_sharding=self._embedding_names_per_sharding,\n            need_indices=False,\n            features_to_permute_indices=None,\n        )",
  "def create_context(self) -> ManagedCollisionCollectionContext:\n        return ManagedCollisionCollectionContext(sharding_contexts=[])",
  "def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        for fqn, _ in self.named_buffers():\n            yield append_prefix(prefix, fqn)",
  "def __init__(\n        self,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)",
  "def shard(\n        self,\n        module: ManagedCollisionCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                EmbeddingShardingContext,\n                KeyedJaggedTensor,\n                torch.Tensor,\n                torch.Tensor,\n            ],\n        ],\n        device: Optional[torch.device] = None,\n    ) -> ShardedManagedCollisionCollection:\n\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        return ShardedManagedCollisionCollection(\n            module,\n            params,\n            env=env,\n            device=device,\n            sharding_type_to_sharding=sharding_type_to_sharding,\n        )",
  "def shardable_parameters(\n        self, module: ManagedCollisionCollection\n    ) -> Dict[str, torch.nn.Parameter]:\n        # TODO: standalone sharding\n        raise NotImplementedError()",
  "def module_type(self) -> Type[ManagedCollisionCollection]:\n        return ManagedCollisionCollection",
  "def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.ROW_WISE.value,\n        ]\n        return types",
  "def _post_state_dict_hook(\n            module: ShardedManagedCollisionCollection,\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            for (\n                mc_buffer_name,\n                sharded_tensor,\n            ) in module._model_parallel_mc_buffer_name_to_sharded_tensor.items():\n                destination_key = f\"{prefix}_managed_collision_modules.{mc_buffer_name}\"\n                destination[destination_key] = sharded_tensor",
  "def _load_state_dict_pre_hook(\n            module: \"ShardedManagedCollisionCollection\",\n            state_dict: Dict[str, Any],\n            prefix: str,\n            *args: Any,\n        ) -> None:\n            for (\n                mc_buffer_name,\n                _sharded_tensor,\n            ) in module._model_parallel_mc_buffer_name_to_sharded_tensor.items():\n                key = f\"{prefix}_managed_collision_modules.{mc_buffer_name}\"\n                if key in state_dict:\n                    if isinstance(state_dict[key], ShardedTensor):\n                        local_shards = state_dict[key].local_shards()\n                        state_dict[key] = local_shards[0].tensor\n                    else:\n                        raise RuntimeError(\n                            f\"Unexpected state_dict key type {type(state_dict[key])} found for {key}\"\n                        )",
  "def _copy_config(\n    original: GroupedEmbeddingConfig,\n    data_type: DataType,\n    sparse_type: SparseType,\n    device: torch.device,\n) -> GroupedEmbeddingConfig:\n    # Adjust config to quantized version.\n    # This obviously doesn't work for column-wise sharding.\n    config = copy.deepcopy(original)\n    config.data_type = data_type\n    for table in config.embedding_tables:\n        row_alignment = 16 if device.type == \"cuda\" else 1\n        table.local_cols = rounded_row_size_in_bytes(\n            table.local_cols, sparse_type, row_alignment\n        )\n        if table.local_metadata is not None:\n            table.local_metadata.shard_sizes = [\n                table.local_rows,\n                table.local_cols,\n            ]\n\n        global_metadata = table.global_metadata\n        if global_metadata is not None:\n            for shard_meta in global_metadata.shards_metadata:\n                if shard_meta != table.local_metadata:\n                    shard_meta.shard_sizes = [\n                        shard_meta.shard_sizes[0],\n                        rounded_row_size_in_bytes(\n                            shard_meta.shard_sizes[1], sparse_type, row_alignment\n                        ),\n                    ]\n            global_metadata.size = torch.Size(\n                [\n                    global_metadata.size[0],\n                    sum(\n                        shard_meta.shard_sizes[1]\n                        for shard_meta in global_metadata.shards_metadata\n                    ),\n                ]\n            )\n    return config",
  "def _quantize_weight(\n    state_dict: Dict[str, torch.Tensor],\n    data_type: DataType,\n) -> List[Tuple[torch.Tensor, Optional[torch.Tensor]]]:\n    quant_weight_list = []\n    for weight in state_dict.values():\n        if weight.dtype == torch.float or weight.dtype == torch.float16:\n            quantized_weights = (\n                torch.ops.fbgemm.FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf(\n                    weight, DATA_TYPE_NUM_BITS[data_type]\n                )\n            )\n        else:\n            raise Exception(\"Unsupported dtype: {weight.dtype}\")\n\n        # weight and 4 byte scale shift (2xfp16)\n        quant_weight = quantized_weights[:, :-4]\n        scale_shift = quantized_weights[:, -4:]\n\n        quant_weight_list.append((quant_weight, scale_shift))\n    return quant_weight_list",
  "def _unwrap_kjt(\n    features: KeyedJaggedTensor,\n) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n    return features.values().int(), features.offsets().int(), features.weights_or_none()",
  "class QuantBatchedEmbeddingBag(\n    BaseBatchedEmbeddingBag[\n        Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]\n    ],\n    TBEToRegisterMixIn,\n):\n    def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        managed: List[EmbeddingLocation] = []\n        for table in config.embedding_tables:\n            if device is not None and device.type == \"cuda\":\n                managed.append(\n                    compute_kernel_to_embedding_location(table.compute_kernel)\n                )\n            else:\n                managed.append(EmbeddingLocation.HOST)\n        self._config: GroupedEmbeddingConfig = config\n        self._emb_module_registered: bool = is_fused_param_register_tbe(fused_params)\n        self._quant_state_dict_split_scale_bias: bool = (\n            is_fused_param_quant_state_dict_split_scale_bias(fused_params)\n        )\n        self._emb_module: IntNBitTableBatchedEmbeddingBagsCodegen = IntNBitTableBatchedEmbeddingBagsCodegen(\n            embedding_specs=[\n                (\n                    table.name,\n                    local_rows,\n                    local_cols\n                    if self._quant_state_dict_split_scale_bias\n                    else table.embedding_dim,\n                    data_type_to_sparse_type(config.data_type),\n                    location,\n                )\n                for local_rows, local_cols, table, location in zip(\n                    self._local_rows, self._local_cols, config.embedding_tables, managed\n                )\n            ],\n            device=device,\n            pooling_mode=self._pooling,\n            feature_table_map=self._feature_table_map,\n            row_alignment=16,\n            uvm_host_mapped=True,  # Use cudaHostAlloc for UVM CACHING to fix imbalance numa memory issue\n            **(tbe_fused_params(fused_params) or {}),\n        )\n        if device is not None:\n            self._emb_module.initialize_weights()\n\n    def init_parameters(self) -> None:\n        pass\n\n    @property\n    def emb_module(\n        self,\n    ) -> IntNBitTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module\n\n    def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return {self._emb_module: self._config}\n\n    def forward(self, features: KeyedJaggedTensor) -> torch.Tensor:\n        indices, offsets, per_sample_weights = _unwrap_kjt(features)\n        # Conditional call of .forward function for FX:\n        # emb_module() can go through FX only if emb_module is registered in named_modules (FX node call_module)\n        # emb_module.forward() does not require registering emb_module in named_modules (FX node call_function)\n        # For some post processing that requires TBE emb_module copied in fx.GraphModule we need to be call_module, as it will copies this module inside fx.GraphModule unchanged.\n        if self._emb_module_registered:\n            return self.emb_module(\n                indices=indices,\n                offsets=offsets,\n                per_sample_weights=per_sample_weights,\n            )\n        else:\n            return self.emb_module.forward(\n                indices=indices,\n                offsets=offsets,\n                per_sample_weights=per_sample_weights,\n            )\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert (\n            remove_duplicate\n        ), \"remove_duplicate=False not supported in QuantBatchedEmbeddingBag.named_split_embedding_weights\"\n        for config, (weight, weight_qscale, weight_qbias) in zip(\n            self._config.embedding_tables,\n            self.emb_module.split_embedding_weights_with_scale_bias(\n                split_scale_bias_mode=2\n                if self._quant_state_dict_split_scale_bias\n                else 0\n            ),\n        ):\n            yield append_prefix(prefix, f\"{config.name}.weight\"), weight\n            if self._quant_state_dict_split_scale_bias:\n                yield append_prefix(\n                    prefix, f\"{config.name}.weight_qscale\"\n                ), weight_qscale\n                yield append_prefix(prefix, f\"{config.name}.weight_qbias\"), weight_qbias\n\n    def split_embedding_weights(\n        self,\n    ) -> List[Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]]:\n        return [\n            (weight, qscale, qbias)\n            for weight, qscale, qbias in self.emb_module.split_embedding_weights_with_scale_bias(\n                split_scale_bias_mode=2\n                if self._quant_state_dict_split_scale_bias\n                else 0\n            )\n        ]\n\n    @classmethod\n    def from_float(cls, module: BaseEmbedding) -> \"QuantBatchedEmbeddingBag\":\n        assert hasattr(\n            module, \"qconfig\"\n        ), \"BaseEmbedding input float module must have qconfig defined\"\n\n        data_type = dtype_to_data_type(module.qconfig.weight().dtype)\n        sparse_type = data_type_to_sparse_type(data_type)\n\n        # TODO Can we simplify this with state_dict = module.state_dict()?\n        state_dict = (\n            dict(module.named_split_embedding_weights())\n            if isinstance(module, BatchedDenseEmbeddingBag)\n            else dict(module.named_parameters())\n        )\n        device = next(iter(state_dict.values())).device\n\n        config = _copy_config(module.config, data_type, sparse_type, device)\n        ret = QuantBatchedEmbeddingBag(config=config, device=device)\n\n        # pyre-ignore\n        quant_weight_list = _quantize_weight(state_dict, data_type)\n        ret.emb_module.assign_embedding_weights(quant_weight_list)\n\n        return ret",
  "class QuantBatchedEmbedding(\n    BaseBatchedEmbedding[\n        Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]\n    ],\n    TBEToRegisterMixIn,\n):\n    def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        managed: List[EmbeddingLocation] = []\n        for table in config.embedding_tables:\n            if device is not None and device.type == \"cuda\":\n                managed.append(\n                    compute_kernel_to_embedding_location(table.compute_kernel)\n                )\n            else:\n                managed.append(EmbeddingLocation.HOST)\n        self._config: GroupedEmbeddingConfig = config\n        self._emb_module_registered: bool = is_fused_param_register_tbe(fused_params)\n        self._quant_state_dict_split_scale_bias: bool = (\n            is_fused_param_quant_state_dict_split_scale_bias(fused_params)\n        )\n        self._emb_module: IntNBitTableBatchedEmbeddingBagsCodegen = IntNBitTableBatchedEmbeddingBagsCodegen(\n            embedding_specs=[\n                (\n                    table.name,\n                    local_rows,\n                    local_cols\n                    if self._quant_state_dict_split_scale_bias\n                    else table.embedding_dim,\n                    data_type_to_sparse_type(config.data_type),\n                    location,\n                )\n                for local_rows, local_cols, table, location in zip(\n                    self._local_rows, self._local_cols, config.embedding_tables, managed\n                )\n            ],\n            device=device,\n            pooling_mode=PoolingMode.NONE,\n            feature_table_map=self._feature_table_map,\n            row_alignment=16,\n            uvm_host_mapped=True,  # Use cudaHostAlloc for UVM CACHING to fix imbalance numa memory issue\n            **(tbe_fused_params(fused_params) or {}),\n        )\n        if device is not None:\n            self._emb_module.initialize_weights()\n\n    @property\n    def emb_module(\n        self,\n    ) -> IntNBitTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module\n\n    def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return {self._emb_module: self._config}\n\n    def split_embedding_weights(\n        self,\n    ) -> List[Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]]:\n        return [\n            (weight, qscale, qbias)\n            for weight, qscale, qbias in self.emb_module.split_embedding_weights_with_scale_bias(\n                split_scale_bias_mode=2\n                if self._quant_state_dict_split_scale_bias\n                else 0\n            )\n        ]\n\n    def forward(self, features: KeyedJaggedTensor) -> torch.Tensor:\n        values, offsets, _ = _unwrap_kjt(features)\n        if self._emb_module_registered:\n            return self.emb_module(\n                indices=values,\n                offsets=offsets,\n            )\n        else:\n            return self.emb_module.forward(\n                indices=values,\n                offsets=offsets,\n            )\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        for config, (weight, weight_qscale, weight_qbias) in zip(\n            self._config.embedding_tables,\n            self.emb_module.split_embedding_weights_with_scale_bias(\n                split_scale_bias_mode=2\n                if self._quant_state_dict_split_scale_bias\n                else 0\n            ),\n        ):\n            yield append_prefix(prefix, f\"{config.name}.weight\"), weight\n            if self._quant_state_dict_split_scale_bias:\n                yield append_prefix(\n                    prefix, f\"{config.name}.weight_qscale\"\n                ), weight_qscale\n                yield append_prefix(prefix, f\"{config.name}.weight_qbias\"), weight_qbias\n\n    @classmethod\n    def from_float(cls, module: BaseEmbedding) -> \"QuantBatchedEmbedding\":\n        assert hasattr(\n            module, \"qconfig\"\n        ), \"BaseEmbedding input float module must have qconfig defined\"\n\n        data_type = dtype_to_data_type(module.qconfig.weight().dtype)\n        sparse_type = data_type_to_sparse_type(data_type)\n\n        # TODO Can we simplify this with state_dict = module.state_dict()?\n        state_dict = (\n            dict(module.named_split_embedding_weights())\n            if isinstance(module, BatchedDenseEmbedding)\n            else dict(module.named_parameters())\n        )\n        device = next(iter(state_dict.values())).device\n\n        config = _copy_config(module.config, data_type, sparse_type, device)\n        ret = QuantBatchedEmbedding(config=config, device=device)\n\n        # pyre-ignore\n        quant_weight_list = _quantize_weight(state_dict, data_type)\n        ret.emb_module.assign_embedding_weights(quant_weight_list)\n\n        return ret",
  "def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        managed: List[EmbeddingLocation] = []\n        for table in config.embedding_tables:\n            if device is not None and device.type == \"cuda\":\n                managed.append(\n                    compute_kernel_to_embedding_location(table.compute_kernel)\n                )\n            else:\n                managed.append(EmbeddingLocation.HOST)\n        self._config: GroupedEmbeddingConfig = config\n        self._emb_module_registered: bool = is_fused_param_register_tbe(fused_params)\n        self._quant_state_dict_split_scale_bias: bool = (\n            is_fused_param_quant_state_dict_split_scale_bias(fused_params)\n        )\n        self._emb_module: IntNBitTableBatchedEmbeddingBagsCodegen = IntNBitTableBatchedEmbeddingBagsCodegen(\n            embedding_specs=[\n                (\n                    table.name,\n                    local_rows,\n                    local_cols\n                    if self._quant_state_dict_split_scale_bias\n                    else table.embedding_dim,\n                    data_type_to_sparse_type(config.data_type),\n                    location,\n                )\n                for local_rows, local_cols, table, location in zip(\n                    self._local_rows, self._local_cols, config.embedding_tables, managed\n                )\n            ],\n            device=device,\n            pooling_mode=self._pooling,\n            feature_table_map=self._feature_table_map,\n            row_alignment=16,\n            uvm_host_mapped=True,  # Use cudaHostAlloc for UVM CACHING to fix imbalance numa memory issue\n            **(tbe_fused_params(fused_params) or {}),\n        )\n        if device is not None:\n            self._emb_module.initialize_weights()",
  "def init_parameters(self) -> None:\n        pass",
  "def emb_module(\n        self,\n    ) -> IntNBitTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module",
  "def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return {self._emb_module: self._config}",
  "def forward(self, features: KeyedJaggedTensor) -> torch.Tensor:\n        indices, offsets, per_sample_weights = _unwrap_kjt(features)\n        # Conditional call of .forward function for FX:\n        # emb_module() can go through FX only if emb_module is registered in named_modules (FX node call_module)\n        # emb_module.forward() does not require registering emb_module in named_modules (FX node call_function)\n        # For some post processing that requires TBE emb_module copied in fx.GraphModule we need to be call_module, as it will copies this module inside fx.GraphModule unchanged.\n        if self._emb_module_registered:\n            return self.emb_module(\n                indices=indices,\n                offsets=offsets,\n                per_sample_weights=per_sample_weights,\n            )\n        else:\n            return self.emb_module.forward(\n                indices=indices,\n                offsets=offsets,\n                per_sample_weights=per_sample_weights,\n            )",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert (\n            remove_duplicate\n        ), \"remove_duplicate=False not supported in QuantBatchedEmbeddingBag.named_split_embedding_weights\"\n        for config, (weight, weight_qscale, weight_qbias) in zip(\n            self._config.embedding_tables,\n            self.emb_module.split_embedding_weights_with_scale_bias(\n                split_scale_bias_mode=2\n                if self._quant_state_dict_split_scale_bias\n                else 0\n            ),\n        ):\n            yield append_prefix(prefix, f\"{config.name}.weight\"), weight\n            if self._quant_state_dict_split_scale_bias:\n                yield append_prefix(\n                    prefix, f\"{config.name}.weight_qscale\"\n                ), weight_qscale\n                yield append_prefix(prefix, f\"{config.name}.weight_qbias\"), weight_qbias",
  "def split_embedding_weights(\n        self,\n    ) -> List[Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]]:\n        return [\n            (weight, qscale, qbias)\n            for weight, qscale, qbias in self.emb_module.split_embedding_weights_with_scale_bias(\n                split_scale_bias_mode=2\n                if self._quant_state_dict_split_scale_bias\n                else 0\n            )\n        ]",
  "def from_float(cls, module: BaseEmbedding) -> \"QuantBatchedEmbeddingBag\":\n        assert hasattr(\n            module, \"qconfig\"\n        ), \"BaseEmbedding input float module must have qconfig defined\"\n\n        data_type = dtype_to_data_type(module.qconfig.weight().dtype)\n        sparse_type = data_type_to_sparse_type(data_type)\n\n        # TODO Can we simplify this with state_dict = module.state_dict()?\n        state_dict = (\n            dict(module.named_split_embedding_weights())\n            if isinstance(module, BatchedDenseEmbeddingBag)\n            else dict(module.named_parameters())\n        )\n        device = next(iter(state_dict.values())).device\n\n        config = _copy_config(module.config, data_type, sparse_type, device)\n        ret = QuantBatchedEmbeddingBag(config=config, device=device)\n\n        # pyre-ignore\n        quant_weight_list = _quantize_weight(state_dict, data_type)\n        ret.emb_module.assign_embedding_weights(quant_weight_list)\n\n        return ret",
  "def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        managed: List[EmbeddingLocation] = []\n        for table in config.embedding_tables:\n            if device is not None and device.type == \"cuda\":\n                managed.append(\n                    compute_kernel_to_embedding_location(table.compute_kernel)\n                )\n            else:\n                managed.append(EmbeddingLocation.HOST)\n        self._config: GroupedEmbeddingConfig = config\n        self._emb_module_registered: bool = is_fused_param_register_tbe(fused_params)\n        self._quant_state_dict_split_scale_bias: bool = (\n            is_fused_param_quant_state_dict_split_scale_bias(fused_params)\n        )\n        self._emb_module: IntNBitTableBatchedEmbeddingBagsCodegen = IntNBitTableBatchedEmbeddingBagsCodegen(\n            embedding_specs=[\n                (\n                    table.name,\n                    local_rows,\n                    local_cols\n                    if self._quant_state_dict_split_scale_bias\n                    else table.embedding_dim,\n                    data_type_to_sparse_type(config.data_type),\n                    location,\n                )\n                for local_rows, local_cols, table, location in zip(\n                    self._local_rows, self._local_cols, config.embedding_tables, managed\n                )\n            ],\n            device=device,\n            pooling_mode=PoolingMode.NONE,\n            feature_table_map=self._feature_table_map,\n            row_alignment=16,\n            uvm_host_mapped=True,  # Use cudaHostAlloc for UVM CACHING to fix imbalance numa memory issue\n            **(tbe_fused_params(fused_params) or {}),\n        )\n        if device is not None:\n            self._emb_module.initialize_weights()",
  "def emb_module(\n        self,\n    ) -> IntNBitTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module",
  "def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        return {self._emb_module: self._config}",
  "def split_embedding_weights(\n        self,\n    ) -> List[Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]]:\n        return [\n            (weight, qscale, qbias)\n            for weight, qscale, qbias in self.emb_module.split_embedding_weights_with_scale_bias(\n                split_scale_bias_mode=2\n                if self._quant_state_dict_split_scale_bias\n                else 0\n            )\n        ]",
  "def forward(self, features: KeyedJaggedTensor) -> torch.Tensor:\n        values, offsets, _ = _unwrap_kjt(features)\n        if self._emb_module_registered:\n            return self.emb_module(\n                indices=values,\n                offsets=offsets,\n            )\n        else:\n            return self.emb_module.forward(\n                indices=values,\n                offsets=offsets,\n            )",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        for config, (weight, weight_qscale, weight_qbias) in zip(\n            self._config.embedding_tables,\n            self.emb_module.split_embedding_weights_with_scale_bias(\n                split_scale_bias_mode=2\n                if self._quant_state_dict_split_scale_bias\n                else 0\n            ),\n        ):\n            yield append_prefix(prefix, f\"{config.name}.weight\"), weight\n            if self._quant_state_dict_split_scale_bias:\n                yield append_prefix(\n                    prefix, f\"{config.name}.weight_qscale\"\n                ), weight_qscale\n                yield append_prefix(prefix, f\"{config.name}.weight_qbias\"), weight_qbias",
  "def from_float(cls, module: BaseEmbedding) -> \"QuantBatchedEmbedding\":\n        assert hasattr(\n            module, \"qconfig\"\n        ), \"BaseEmbedding input float module must have qconfig defined\"\n\n        data_type = dtype_to_data_type(module.qconfig.weight().dtype)\n        sparse_type = data_type_to_sparse_type(data_type)\n\n        # TODO Can we simplify this with state_dict = module.state_dict()?\n        state_dict = (\n            dict(module.named_split_embedding_weights())\n            if isinstance(module, BatchedDenseEmbedding)\n            else dict(module.named_parameters())\n        )\n        device = next(iter(state_dict.values())).device\n\n        config = _copy_config(module.config, data_type, sparse_type, device)\n        ret = QuantBatchedEmbedding(config=config, device=device)\n\n        # pyre-ignore\n        quant_weight_list = _quantize_weight(state_dict, data_type)\n        ret.emb_module.assign_embedding_weights(quant_weight_list)\n\n        return ret",
  "def replace_placement_with_meta_device(\n    sharding_infos: List[EmbeddingShardingInfo],\n) -> None:\n    \"\"\"Placement device and tensor device could be unmatched in some\n    scenarios, e.g. passing meta device to DMP and passing cuda\n    to EmbeddingShardingPlanner. We need to make device consistent\n    after getting sharding planner.\n    \"\"\"\n    for info in sharding_infos:\n        sharding_spec = info.param_sharding.sharding_spec\n        if sharding_spec is None:\n            continue\n        if isinstance(sharding_spec, EnumerableShardingSpec):\n            for shard_metadata in sharding_spec.shards:\n                placement = shard_metadata.placement\n                if isinstance(placement, str):\n                    placement = torch.distributed._remote_device(placement)\n                assert isinstance(placement, torch.distributed._remote_device)\n                placement._device = torch.device(\"meta\")\n                shard_metadata.placement = placement\n        else:\n            # We only support EnumerableShardingSpec at present.\n            raise RuntimeError(\n                f\"Unsupported ShardingSpec {type(sharding_spec)} with meta device\"\n            )",
  "def create_embedding_bag_sharding(\n    sharding_type: str,\n    sharding_infos: List[EmbeddingShardingInfo],\n    env: ShardingEnv,\n    device: Optional[torch.device] = None,\n    permute_embeddings: bool = False,\n    qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n) -> EmbeddingSharding[\n    EmbeddingShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n]:\n    if device is not None and device.type == \"meta\":\n        replace_placement_with_meta_device(sharding_infos)\n    if sharding_type == ShardingType.TABLE_WISE.value:\n        return TwPooledEmbeddingSharding(\n            sharding_infos,\n            env,\n            device,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n    elif sharding_type == ShardingType.ROW_WISE.value:\n        return RwPooledEmbeddingSharding(\n            sharding_infos,\n            env,\n            device,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n    elif sharding_type == ShardingType.DATA_PARALLEL.value:\n        return DpPooledEmbeddingSharding(sharding_infos, env, device)\n    elif sharding_type == ShardingType.TABLE_ROW_WISE.value:\n        return TwRwPooledEmbeddingSharding(\n            sharding_infos,\n            env,\n            device,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n    elif sharding_type == ShardingType.COLUMN_WISE.value:\n        return CwPooledEmbeddingSharding(\n            sharding_infos,\n            env,\n            device,\n            permute_embeddings=permute_embeddings,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n    elif sharding_type == ShardingType.TABLE_COLUMN_WISE.value:\n        return TwCwPooledEmbeddingSharding(\n            sharding_infos,\n            env,\n            device,\n            permute_embeddings=permute_embeddings,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n    else:\n        raise ValueError(f\"Sharding type not supported {sharding_type}\")",
  "def create_sharding_infos_by_sharding(\n    module: EmbeddingBagCollectionInterface,\n    table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n    prefix: str,\n    fused_params: Optional[Dict[str, Any]],\n    suffix: Optional[str] = \"weight\",\n) -> Dict[str, List[EmbeddingShardingInfo]]:\n\n    if fused_params is None:\n        fused_params = {}\n\n    shared_feature: Dict[str, bool] = {}\n    for embedding_config in module.embedding_bag_configs():\n        if not embedding_config.feature_names:\n            embedding_config.feature_names = [embedding_config.name]\n        for feature_name in embedding_config.feature_names:\n            if feature_name not in shared_feature:\n                shared_feature[feature_name] = False\n            else:\n                shared_feature[feature_name] = True\n\n    sharding_type_to_sharding_infos: Dict[str, List[EmbeddingShardingInfo]] = {}\n\n    # state_dict returns parameter.Tensor, which loses parameter level attributes\n    parameter_by_name = dict(module.named_parameters())\n    # QuantEBC registers weights as buffers (since they are INT8), and so we need to grab it there\n    state_dict = module.state_dict()\n\n    for config in module.embedding_bag_configs():\n        table_name = config.name\n        assert (\n            table_name in table_name_to_parameter_sharding\n        ), f\"{table_name} not in table_name_to_parameter_sharding\"\n        parameter_sharding = table_name_to_parameter_sharding[table_name]\n        if parameter_sharding.compute_kernel not in [\n            kernel.value for kernel in EmbeddingComputeKernel\n        ]:\n            raise ValueError(\n                f\"Compute kernel not supported {parameter_sharding.compute_kernel}\"\n            )\n        embedding_names: List[str] = []\n        for feature_name in config.feature_names:\n            if shared_feature[feature_name]:\n                embedding_names.append(feature_name + \"@\" + config.name)\n            else:\n                embedding_names.append(feature_name)\n\n        param_name = prefix + table_name\n        if suffix is not None:\n            param_name = f\"{param_name}.{suffix}\"\n\n        assert param_name in parameter_by_name or param_name in state_dict\n        param = parameter_by_name.get(param_name, state_dict[param_name])\n\n        if parameter_sharding.sharding_type not in sharding_type_to_sharding_infos:\n            sharding_type_to_sharding_infos[parameter_sharding.sharding_type] = []\n\n        optimizer_params = getattr(param, \"_optimizer_kwargs\", [{}])\n        optimizer_classes = getattr(param, \"_optimizer_classes\", [None])\n\n        assert (\n            len(optimizer_classes) == 1 and len(optimizer_params) == 1\n        ), f\"Only support 1 optimizer, given {len(optimizer_classes)} optimizer classes \\\n        and {len(optimizer_params)} optimizer kwargs.\"\n\n        optimizer_class = optimizer_classes[0]\n        optimizer_params = optimizer_params[0]\n        if optimizer_class:\n            optimizer_params[\"optimizer\"] = optimizer_type_to_emb_opt_type(\n                optimizer_class\n            )\n\n        per_table_fused_params = merge_fused_params(fused_params, optimizer_params)\n        per_table_fused_params = add_params_from_parameter_sharding(\n            per_table_fused_params, parameter_sharding\n        )\n        per_table_fused_params = convert_to_fbgemm_types(per_table_fused_params)\n\n        sharding_type_to_sharding_infos[parameter_sharding.sharding_type].append(\n            EmbeddingShardingInfo(\n                embedding_config=EmbeddingTableConfig(\n                    num_embeddings=config.num_embeddings,\n                    embedding_dim=config.embedding_dim,\n                    name=config.name,\n                    data_type=config.data_type,\n                    feature_names=copy.deepcopy(config.feature_names),\n                    pooling=config.pooling,\n                    is_weighted=module.is_weighted(),\n                    has_feature_processor=False,\n                    embedding_names=embedding_names,\n                    weight_init_max=config.weight_init_max,\n                    weight_init_min=config.weight_init_min,\n                ),\n                param_sharding=parameter_sharding,\n                param=param,\n                fused_params=per_table_fused_params,\n            )\n        )\n    return sharding_type_to_sharding_infos",
  "def construct_output_kt(\n    embeddings: List[torch.Tensor],\n    embedding_names: List[str],\n    embedding_dims: List[int],\n) -> KeyedTensor:\n    cat_embeddings: torch.Tensor\n    if len(embeddings) == 1:\n        cat_embeddings = embeddings[0]\n    else:\n        cat_embeddings = torch.cat(embeddings, dim=1)\n    return KeyedTensor(\n        keys=embedding_names,\n        length_per_key=embedding_dims,\n        values=cat_embeddings,\n        key_dim=1,\n    )",
  "class EmbeddingBagCollectionAwaitable(LazyAwaitable[KeyedTensor]):\n    def __init__(\n        self,\n        awaitables: List[Awaitable[torch.Tensor]],\n        embedding_dims: List[int],\n        embedding_names: List[str],\n    ) -> None:\n        super().__init__()\n        self._awaitables = awaitables\n        self._embedding_dims = embedding_dims\n        self._embedding_names = embedding_names\n\n    def _wait_impl(self) -> KeyedTensor:\n        return construct_output_kt(\n            embeddings=[w.wait() for w in self._awaitables],\n            embedding_names=self._embedding_names,\n            embedding_dims=self._embedding_dims,\n        )",
  "class EmbeddingBagCollectionContext(Multistreamable):\n    sharding_contexts: List[Optional[EmbeddingShardingContext]] = field(\n        default_factory=list\n    )\n\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for ctx in self.sharding_contexts:\n            if ctx:\n                ctx.record_stream(stream)",
  "class ShardedEmbeddingBagCollection(\n    ShardedEmbeddingModule[\n        KJTList,\n        List[torch.Tensor],\n        KeyedTensor,\n        EmbeddingBagCollectionContext,\n    ],\n    # TODO remove after compute_kernel X sharding decoupling\n    FusedOptimizerModule,\n):\n    \"\"\"\n    Sharded implementation of EmbeddingBagCollection.\n    This is part of the public API to allow for manual data dist pipelining.\n    \"\"\"\n\n    def __init__(\n        self,\n        module: EmbeddingBagCollectionInterface,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._embedding_bag_configs: List[\n            EmbeddingBagConfig\n        ] = module.embedding_bag_configs()\n        self._table_names: List[str] = [\n            config.name for config in self._embedding_bag_configs\n        ]\n\n        self._table_name_to_config: Dict[str, EmbeddingBagConfig] = {\n            config.name: config for config in self._embedding_bag_configs\n        }\n\n        self.module_sharding_plan: EmbeddingModuleShardingPlan = cast(\n            EmbeddingModuleShardingPlan,\n            {\n                table_name: parameter_sharding\n                for table_name, parameter_sharding in table_name_to_parameter_sharding.items()\n                if table_name in self._table_names\n            },\n        )\n        self._env = env\n\n        sharding_type_to_sharding_infos = create_sharding_infos_by_sharding(\n            module,\n            table_name_to_parameter_sharding,\n            \"embedding_bags.\",\n            fused_params,\n        )\n        self._sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                EmbeddingShardingContext,\n                KeyedJaggedTensor,\n                torch.Tensor,\n                torch.Tensor,\n            ],\n        ] = {\n            sharding_type: create_embedding_bag_sharding(\n                sharding_type,\n                embedding_configs,\n                env,\n                device,\n                permute_embeddings=True,\n                qcomm_codecs_registry=self.qcomm_codecs_registry,\n            )\n            for sharding_type, embedding_configs in sharding_type_to_sharding_infos.items()\n        }\n\n        self._is_weighted: bool = module.is_weighted()\n        self._device = device\n        self._input_dists: List[nn.Module] = []\n        self._lookups: List[nn.Module] = []\n        self._create_lookups()\n        self._output_dists: List[nn.Module] = []\n        self._embedding_names: List[str] = []\n        self._embedding_dims: List[int] = []\n        self._feature_splits: List[int] = []\n        self._features_order: List[int] = []\n        # to support the FP16 hook\n        self._create_output_dist()\n\n        # forward pass flow control\n        self._has_uninitialized_input_dist: bool = True\n        self._has_features_permute: bool = True\n        # Get all fused optimizers and combine them.\n        optims = []\n        for lookup in self._lookups:\n            for _, tbe_module in lookup.named_modules():\n                if isinstance(tbe_module, FusedOptimizerModule):\n                    # modify param keys to match EmbeddingBagCollection\n                    params: Mapping[str, Union[torch.Tensor, ShardedTensor]] = {}\n                    for param_key, weight in tbe_module.fused_optimizer.params.items():\n                        # pyre-fixme[16]: `Mapping` has no attribute `__setitem__`\n                        params[\"embedding_bags.\" + param_key] = weight\n                    tbe_module.fused_optimizer.params = params\n                    optims.append((\"\", tbe_module.fused_optimizer))\n        self._optim: CombinedOptimizer = CombinedOptimizer(optims)\n\n        for index, (sharding, lookup) in enumerate(\n            zip(\n                self._sharding_type_to_sharding.values(),\n                self._lookups,\n            )\n        ):\n            # TODO: can move this into DpPooledEmbeddingSharding once all modules are composable\n            if isinstance(sharding, DpPooledEmbeddingSharding):\n                self._lookups[index] = DistributedDataParallel(\n                    module=lookup,\n                    device_ids=[device]\n                    if self._device and self._device.type == \"gpu\"\n                    else None,\n                    process_group=env.process_group,\n                    gradient_as_bucket_view=True,\n                    broadcast_buffers=True,\n                    static_graph=True,\n                )\n        self._initialize_torch_state()\n\n        # TODO[zainhuda]: support module device coming from CPU\n        if module.device not in [\"meta\", \"cpu\"] and module.device.type not in [\n            \"meta\",\n            \"cpu\",\n        ]:\n            self.load_state_dict(module.state_dict(), strict=False)\n\n    @staticmethod\n    def _pre_load_state_dict_hook(\n        self: \"ShardedEmbeddingBagCollection\",\n        state_dict: Dict[str, Any],\n        prefix: str,\n        *args: Any,\n    ) -> None:\n        \"\"\"\n        Modify the destination state_dict for model parallel\n        to transform from ShardedTensors into tensors\n        \"\"\"\n        for (\n            table_name,\n            model_shards,\n        ) in self._model_parallel_name_to_local_shards.items():\n            key = f\"{prefix}embedding_bags.{table_name}.weight\"\n\n            # If state_dict[key] is already a ShardedTensor, use its local shards\n            if isinstance(state_dict[key], ShardedTensor):\n                local_shards = state_dict[key].local_shards()\n                if len(local_shards) == 0:\n                    state_dict[key] = torch.empty(0)\n                else:\n                    dim = state_dict[key].metadata().shards_metadata[0].shard_sizes[1]\n                    # CW multiple shards are merged\n                    if len(local_shards) > 1:\n                        state_dict[key] = torch.cat(\n                            [s.tensor.view(-1) for s in local_shards], dim=0\n                        ).view(-1, dim)\n                    else:\n                        state_dict[key] = local_shards[0].tensor.view(-1, dim)\n            elif isinstance(state_dict[key], torch.Tensor):\n                local_shards = []\n                for shard in model_shards:\n                    # Extract shard size and offsets for splicing\n                    shard_sizes = shard.metadata.shard_sizes\n                    shard_offsets = shard.metadata.shard_offsets\n\n                    # Prepare tensor by splicing and placing on appropriate device\n                    spliced_tensor = state_dict[key][\n                        shard_offsets[0] : shard_offsets[0] + shard_sizes[0],\n                        shard_offsets[1] : shard_offsets[1] + shard_sizes[1],\n                    ]\n\n                    # Append spliced tensor into local shards\n                    local_shards.append(spliced_tensor)\n                state_dict[key] = (\n                    torch.empty(0)\n                    if not local_shards\n                    else torch.cat(local_shards, dim=0)\n                )\n            else:\n                raise RuntimeError(\n                    f\"Unexpected state_dict key type {type(state_dict[key])} found for {key}\"\n                )\n\n    def _initialize_torch_state(self) -> None:  # noqa\n        \"\"\"\n        This provides consistency between this class and the EmbeddingBagCollection's\n        nn.Module API calls (state_dict, named_modules, etc)\n        \"\"\"\n        self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n        for table_name in self._table_names:\n            self.embedding_bags[table_name] = nn.Module()\n        self._model_parallel_name_to_local_shards = OrderedDict()\n        self._model_parallel_name_to_sharded_tensor = OrderedDict()\n        model_parallel_name_to_compute_kernel: Dict[str, str] = {}\n        for (\n            table_name,\n            parameter_sharding,\n        ) in self.module_sharding_plan.items():\n            if parameter_sharding.sharding_type == ShardingType.DATA_PARALLEL.value:\n                continue\n            self._model_parallel_name_to_local_shards[table_name] = []\n            model_parallel_name_to_compute_kernel[\n                table_name\n            ] = parameter_sharding.compute_kernel\n\n        self._name_to_table_size = {}\n        for table in self._embedding_bag_configs:\n            self._name_to_table_size[table.name] = (\n                table.num_embeddings,\n                table.embedding_dim,\n            )\n\n        for sharding_type, lookup in zip(\n            self._sharding_type_to_sharding.keys(), self._lookups\n        ):\n            if sharding_type == ShardingType.DATA_PARALLEL.value:\n                # unwrap DDP\n                lookup = lookup.module\n            else:\n                # save local_shards for transforming MP params to shardedTensor\n                for key, v in lookup.state_dict().items():\n                    table_name = key[: -len(\".weight\")]\n                    self._model_parallel_name_to_local_shards[table_name].extend(\n                        v.local_shards()\n                    )\n            for (\n                table_name,\n                tbe_slice,\n            ) in lookup.named_parameters_by_table():\n                self.embedding_bags[table_name].register_parameter(\"weight\", tbe_slice)\n        for (\n            table_name,\n            local_shards,\n        ) in self._model_parallel_name_to_local_shards.items():\n            # for shards that don't exist on this rank, register with empty tensor\n            if not hasattr(self.embedding_bags[table_name], \"weight\"):\n                self.embedding_bags[table_name].register_parameter(\n                    \"weight\", nn.Parameter(torch.empty(0))\n                )\n                if (\n                    model_parallel_name_to_compute_kernel[table_name]\n                    != EmbeddingComputeKernel.DENSE.value\n                ):\n                    self.embedding_bags[table_name].weight._in_backward_optimizers = [\n                        EmptyFusedOptimizer()\n                    ]\n            # created ShardedTensors once in init, use in post_state_dict_hook\n            self._model_parallel_name_to_sharded_tensor[\n                table_name\n            ] = ShardedTensor._init_from_local_shards(\n                local_shards,\n                self._name_to_table_size[table_name],\n                process_group=self._env.process_group,\n            )\n\n        def post_state_dict_hook(\n            module: ShardedEmbeddingBagCollection,\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            # Adjust dense MP\n            for (\n                table_name,\n                sharded_t,\n            ) in module._model_parallel_name_to_sharded_tensor.items():\n                destination_key = f\"{prefix}embedding_bags.{table_name}.weight\"\n                destination[destination_key] = sharded_t\n\n        self._register_state_dict_hook(post_state_dict_hook)\n        self._register_load_state_dict_pre_hook(\n            self._pre_load_state_dict_hook, with_module=True\n        )\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        if self._device and self._device.type == \"meta\":\n            return\n\n        # Initialize embedding bags weights with init_fn\n        for table_config in self._embedding_bag_configs:\n            assert table_config.init_fn is not None\n            param = self.embedding_bags[f\"{table_config.name}\"].weight\n            # pyre-ignore\n            table_config.init_fn(param)\n\n    def _create_input_dist(\n        self,\n        input_feature_names: List[str],\n    ) -> None:\n        feature_names: List[str] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._input_dists.append(sharding.create_input_dist())\n            feature_names.extend(sharding.feature_names())\n            self._feature_splits.append(len(sharding.feature_names()))\n\n        if feature_names == input_feature_names:\n            self._has_features_permute = False\n        else:\n            for f in feature_names:\n                self._features_order.append(input_feature_names.index(f))\n            self.register_buffer(\n                \"_features_order_tensor\",\n                torch.tensor(\n                    self._features_order, device=self._device, dtype=torch.int32\n                ),\n                persistent=False,\n            )\n\n    def _create_lookups(\n        self,\n    ) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._lookups.append(sharding.create_lookup())\n\n    def _create_output_dist(self) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._output_dists.append(sharding.create_output_dist(device=self._device))\n            self._embedding_names.extend(sharding.embedding_names())\n            self._embedding_dims.extend(sharding.embedding_dims())\n\n    # pyre-ignore [14]\n    def input_dist(\n        self, ctx: EmbeddingBagCollectionContext, features: KeyedJaggedTensor\n    ) -> Awaitable[Awaitable[KJTList]]:\n        if self._has_uninitialized_input_dist:\n            self._create_input_dist(features.keys())\n            self._has_uninitialized_input_dist = False\n        with torch.no_grad():\n            if self._has_features_permute:\n                features = features.permute(\n                    self._features_order,\n                    self._features_order_tensor,\n                )\n            features_by_shards = features.split(\n                self._feature_splits,\n            )\n            awaitables = []\n            for input_dist, features_by_shard in zip(\n                self._input_dists, features_by_shards\n            ):\n                awaitables.append(input_dist(features_by_shard))\n                ctx.sharding_contexts.append(EmbeddingShardingContext())\n            return KJTListSplitsAwaitable(awaitables, ctx)\n\n    def compute(\n        self,\n        ctx: EmbeddingBagCollectionContext,\n        dist_input: KJTList,\n    ) -> List[torch.Tensor]:\n        return [lookup(features) for lookup, features in zip(self._lookups, dist_input)]\n\n    def output_dist(\n        self,\n        ctx: EmbeddingBagCollectionContext,\n        output: List[torch.Tensor],\n    ) -> LazyAwaitable[KeyedTensor]:\n        return EmbeddingBagCollectionAwaitable(\n            awaitables=[\n                dist(embeddings, sharding_ctx)\n                for dist, sharding_ctx, embeddings in zip(\n                    self._output_dists,\n                    ctx.sharding_contexts,\n                    output,\n                )\n            ],\n            embedding_dims=self._embedding_dims,\n            embedding_names=self._embedding_names,\n        )\n\n    def compute_and_output_dist(\n        self, ctx: EmbeddingBagCollectionContext, input: KJTList\n    ) -> LazyAwaitable[KeyedTensor]:\n        return EmbeddingBagCollectionAwaitable(\n            awaitables=[\n                dist(lookup(features), sharding_ctx)\n                for lookup, dist, sharding_ctx, features in zip(\n                    self._lookups,\n                    self._output_dists,\n                    ctx.sharding_contexts,\n                    input,\n                )\n            ],\n            embedding_dims=self._embedding_dims,\n            embedding_names=self._embedding_names,\n        )\n\n    @property\n    def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim\n\n    def create_context(self) -> EmbeddingBagCollectionContext:\n        return EmbeddingBagCollectionContext()",
  "class EmbeddingBagCollectionSharder(BaseEmbeddingSharder[EmbeddingBagCollection]):\n    \"\"\"\n    This implementation uses non-fused `EmbeddingBagCollection`\n    \"\"\"\n\n    def shard(\n        self,\n        module: EmbeddingBagCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingBagCollection:\n        return ShardedEmbeddingBagCollection(\n            module=module,\n            table_name_to_parameter_sharding=params,\n            env=env,\n            fused_params=self.fused_params,\n            device=device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )\n\n    def shardable_parameters(\n        self, module: EmbeddingBagCollection\n    ) -> Dict[str, nn.Parameter]:\n        return {\n            name.split(\".\")[0]: param\n            for name, param in module.embedding_bags.named_parameters()\n        }\n\n    @property\n    def module_type(self) -> Type[EmbeddingBagCollection]:\n        return EmbeddingBagCollection",
  "class EmbeddingAwaitable(LazyAwaitable[torch.Tensor]):\n    def __init__(\n        self,\n        awaitable: Awaitable[torch.Tensor],\n    ) -> None:\n        super().__init__()\n        self._awaitable = awaitable\n\n    def _wait_impl(self) -> torch.Tensor:\n        embedding = self._awaitable.wait()\n        return embedding",
  "class ShardedEmbeddingBag(\n    ShardedEmbeddingModule[\n        KeyedJaggedTensor, torch.Tensor, torch.Tensor, NullShardedModuleContext\n    ],\n    FusedOptimizerModule,\n):\n    \"\"\"\n    Sharded implementation of `nn.EmbeddingBag`.\n    This is part of the public API to allow for manual data dist pipelining.\n    \"\"\"\n\n    def __init__(\n        self,\n        module: nn.EmbeddingBag,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n\n        assert (\n            len(table_name_to_parameter_sharding) == 1\n        ), \"expect 1 table, but got len(table_name_to_parameter_sharding)\"\n        assert module.mode == \"sum\", \"ShardedEmbeddingBag only supports sum pooling\"\n\n        self._dummy_embedding_table_name = \"dummy_embedding_table_name\"\n        self._dummy_feature_name = \"dummy_feature_name\"\n        self.parameter_sharding: ParameterSharding = next(\n            iter(table_name_to_parameter_sharding.values())\n        )\n        embedding_table_config = EmbeddingTableConfig(\n            num_embeddings=module.num_embeddings,\n            embedding_dim=module.embedding_dim,\n            name=self._dummy_embedding_table_name,\n            feature_names=[self._dummy_feature_name],\n            pooling=PoolingType.SUM,\n            # We set is_weighted to True for now,\n            # if per_sample_weights is None in forward(),\n            # we could assign a all-one vector to per_sample_weights\n            is_weighted=True,\n            embedding_names=[self._dummy_feature_name],\n        )\n\n        if self.parameter_sharding.sharding_type == ShardingType.TABLE_WISE.value:\n            # TODO: enable it with correct semantics, see T104397332\n            raise RuntimeError(\n                \"table-wise sharding on a single EmbeddingBag is not supported yet\"\n            )\n\n        self._embedding_sharding: EmbeddingSharding[\n            EmbeddingShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n        ] = create_embedding_bag_sharding(\n            sharding_type=self.parameter_sharding.sharding_type,\n            sharding_infos=[\n                EmbeddingShardingInfo(\n                    embedding_config=embedding_table_config,\n                    param_sharding=self.parameter_sharding,\n                    param=next(iter(module.parameters())),\n                    fused_params=fused_params,\n                ),\n            ],\n            env=env,\n            device=device,\n            permute_embeddings=True,\n        )\n        self._input_dist: nn.Module = self._embedding_sharding.create_input_dist()\n        self._lookup: nn.Module = self._embedding_sharding.create_lookup()\n        self._output_dist: nn.Module = self._embedding_sharding.create_output_dist()\n\n        # Get all fused optimizers and combine them.\n        optims = []\n        for _, module in self._lookup.named_modules():\n            if isinstance(module, FusedOptimizerModule):\n                # modify param keys to match EmbeddingBag\n                params: Mapping[str, Union[torch.Tensor, ShardedTensor]] = {}\n                for param_key, weight in module.fused_optimizer.params.items():\n                    # pyre-fixme[16]: `Mapping` has no attribute `__setitem__`.\n                    params[param_key.split(\".\")[-1]] = weight\n                module.fused_optimizer.params = params\n                optims.append((\"\", module.fused_optimizer))\n        self._optim: CombinedOptimizer = CombinedOptimizer(optims)\n\n    # pyre-ignore [14]\n    def input_dist(\n        self,\n        ctx: NullShardedModuleContext,\n        input: Tensor,\n        offsets: Optional[Tensor] = None,\n        per_sample_weights: Optional[Tensor] = None,\n    ) -> Awaitable[Awaitable[KeyedJaggedTensor]]:\n        if per_sample_weights is None:\n            per_sample_weights = torch.ones_like(input, dtype=torch.float)\n        features = KeyedJaggedTensor(\n            keys=[self._dummy_feature_name],\n            values=input,\n            offsets=offsets,\n            weights=per_sample_weights,\n        )\n        return self._input_dist(features)\n\n    def compute(\n        self, ctx: NullShardedModuleContext, dist_input: KeyedJaggedTensor\n    ) -> torch.Tensor:\n        return self._lookup(dist_input)\n\n    def output_dist(\n        self, ctx: NullShardedModuleContext, output: torch.Tensor\n    ) -> LazyAwaitable[torch.Tensor]:\n        return EmbeddingAwaitable(\n            awaitable=self._output_dist(output),\n        )\n\n    # pyre-fixme[14]: `state_dict` overrides method defined in `Module` inconsistently.\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n        # pyre-fixme[19]: Expected 0 positional arguments.\n        lookup_state_dict = self._lookup.state_dict(None, \"\", keep_vars)\n        # update key to match embeddingBag state_dict key\n        for key, item in lookup_state_dict.items():\n            new_key = prefix + key.split(\".\")[-1]\n            destination[new_key] = item\n        return destination\n\n    def named_modules(\n        self,\n        memo: Optional[Set[nn.Module]] = None,\n        prefix: str = \"\",\n        remove_duplicate: bool = True,\n    ) -> Iterator[Tuple[str, nn.Module]]:\n        yield from [(prefix, self)]\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        # TODO: add remove_duplicate\n        for name, parameter in self._lookup.named_parameters(\"\", recurse):\n            # update name to match embeddingBag parameter name\n            yield append_prefix(prefix, name.split(\".\")[-1]), parameter\n\n    def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        if self.parameter_sharding.sharding_type == ShardingType.DATA_PARALLEL.value:\n            yield from []\n        else:\n            for name, _ in self._lookup.named_parameters(\"\"):\n                yield append_prefix(prefix, name.split(\".\")[-1])\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        # TODO: add remove_duplicate\n        for name, buffer in self._lookup.named_buffers(\"\", recurse):\n            yield append_prefix(prefix, name.split(\".\")[-1]), buffer\n\n    # pyre-fixme[14]: `load_state_dict` overrides method defined in `Module`\n    #  inconsistently.\n    def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, torch.Tensor]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        missing_keys = []\n        unexpected_keys = []\n        # update key to match  embeddingBag state_dict key\n        for key, value in state_dict.items():\n            new_key = \".\".join([self._dummy_embedding_table_name, key])\n            state_dict[new_key] = value\n            state_dict.pop(key)\n        missing, unexpected = self._lookup.load_state_dict(\n            state_dict,\n            strict,\n        )\n        missing_keys.extend(missing)\n        unexpected_keys.extend(unexpected)\n\n        return _IncompatibleKeys(\n            missing_keys=missing_keys, unexpected_keys=unexpected_keys\n        )\n\n    @property\n    def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim\n\n    def create_context(self) -> NullShardedModuleContext:\n        return NullShardedModuleContext()",
  "class EmbeddingBagSharder(BaseEmbeddingSharder[nn.EmbeddingBag]):\n    \"\"\"\n    This implementation uses non-fused `nn.EmbeddingBag`\n    \"\"\"\n\n    def shard(\n        self,\n        module: nn.EmbeddingBag,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingBag:\n        return ShardedEmbeddingBag(module, params, env, self.fused_params, device)\n\n    def shardable_parameters(self, module: nn.EmbeddingBag) -> Dict[str, nn.Parameter]:\n        return {name: param for name, param in module.named_parameters()}\n\n    @property\n    def module_type(self) -> Type[nn.EmbeddingBag]:\n        return nn.EmbeddingBag",
  "def __init__(\n        self,\n        awaitables: List[Awaitable[torch.Tensor]],\n        embedding_dims: List[int],\n        embedding_names: List[str],\n    ) -> None:\n        super().__init__()\n        self._awaitables = awaitables\n        self._embedding_dims = embedding_dims\n        self._embedding_names = embedding_names",
  "def _wait_impl(self) -> KeyedTensor:\n        return construct_output_kt(\n            embeddings=[w.wait() for w in self._awaitables],\n            embedding_names=self._embedding_names,\n            embedding_dims=self._embedding_dims,\n        )",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for ctx in self.sharding_contexts:\n            if ctx:\n                ctx.record_stream(stream)",
  "def __init__(\n        self,\n        module: EmbeddingBagCollectionInterface,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._embedding_bag_configs: List[\n            EmbeddingBagConfig\n        ] = module.embedding_bag_configs()\n        self._table_names: List[str] = [\n            config.name for config in self._embedding_bag_configs\n        ]\n\n        self._table_name_to_config: Dict[str, EmbeddingBagConfig] = {\n            config.name: config for config in self._embedding_bag_configs\n        }\n\n        self.module_sharding_plan: EmbeddingModuleShardingPlan = cast(\n            EmbeddingModuleShardingPlan,\n            {\n                table_name: parameter_sharding\n                for table_name, parameter_sharding in table_name_to_parameter_sharding.items()\n                if table_name in self._table_names\n            },\n        )\n        self._env = env\n\n        sharding_type_to_sharding_infos = create_sharding_infos_by_sharding(\n            module,\n            table_name_to_parameter_sharding,\n            \"embedding_bags.\",\n            fused_params,\n        )\n        self._sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                EmbeddingShardingContext,\n                KeyedJaggedTensor,\n                torch.Tensor,\n                torch.Tensor,\n            ],\n        ] = {\n            sharding_type: create_embedding_bag_sharding(\n                sharding_type,\n                embedding_configs,\n                env,\n                device,\n                permute_embeddings=True,\n                qcomm_codecs_registry=self.qcomm_codecs_registry,\n            )\n            for sharding_type, embedding_configs in sharding_type_to_sharding_infos.items()\n        }\n\n        self._is_weighted: bool = module.is_weighted()\n        self._device = device\n        self._input_dists: List[nn.Module] = []\n        self._lookups: List[nn.Module] = []\n        self._create_lookups()\n        self._output_dists: List[nn.Module] = []\n        self._embedding_names: List[str] = []\n        self._embedding_dims: List[int] = []\n        self._feature_splits: List[int] = []\n        self._features_order: List[int] = []\n        # to support the FP16 hook\n        self._create_output_dist()\n\n        # forward pass flow control\n        self._has_uninitialized_input_dist: bool = True\n        self._has_features_permute: bool = True\n        # Get all fused optimizers and combine them.\n        optims = []\n        for lookup in self._lookups:\n            for _, tbe_module in lookup.named_modules():\n                if isinstance(tbe_module, FusedOptimizerModule):\n                    # modify param keys to match EmbeddingBagCollection\n                    params: Mapping[str, Union[torch.Tensor, ShardedTensor]] = {}\n                    for param_key, weight in tbe_module.fused_optimizer.params.items():\n                        # pyre-fixme[16]: `Mapping` has no attribute `__setitem__`\n                        params[\"embedding_bags.\" + param_key] = weight\n                    tbe_module.fused_optimizer.params = params\n                    optims.append((\"\", tbe_module.fused_optimizer))\n        self._optim: CombinedOptimizer = CombinedOptimizer(optims)\n\n        for index, (sharding, lookup) in enumerate(\n            zip(\n                self._sharding_type_to_sharding.values(),\n                self._lookups,\n            )\n        ):\n            # TODO: can move this into DpPooledEmbeddingSharding once all modules are composable\n            if isinstance(sharding, DpPooledEmbeddingSharding):\n                self._lookups[index] = DistributedDataParallel(\n                    module=lookup,\n                    device_ids=[device]\n                    if self._device and self._device.type == \"gpu\"\n                    else None,\n                    process_group=env.process_group,\n                    gradient_as_bucket_view=True,\n                    broadcast_buffers=True,\n                    static_graph=True,\n                )\n        self._initialize_torch_state()\n\n        # TODO[zainhuda]: support module device coming from CPU\n        if module.device not in [\"meta\", \"cpu\"] and module.device.type not in [\n            \"meta\",\n            \"cpu\",\n        ]:\n            self.load_state_dict(module.state_dict(), strict=False)",
  "def _pre_load_state_dict_hook(\n        self: \"ShardedEmbeddingBagCollection\",\n        state_dict: Dict[str, Any],\n        prefix: str,\n        *args: Any,\n    ) -> None:\n        \"\"\"\n        Modify the destination state_dict for model parallel\n        to transform from ShardedTensors into tensors\n        \"\"\"\n        for (\n            table_name,\n            model_shards,\n        ) in self._model_parallel_name_to_local_shards.items():\n            key = f\"{prefix}embedding_bags.{table_name}.weight\"\n\n            # If state_dict[key] is already a ShardedTensor, use its local shards\n            if isinstance(state_dict[key], ShardedTensor):\n                local_shards = state_dict[key].local_shards()\n                if len(local_shards) == 0:\n                    state_dict[key] = torch.empty(0)\n                else:\n                    dim = state_dict[key].metadata().shards_metadata[0].shard_sizes[1]\n                    # CW multiple shards are merged\n                    if len(local_shards) > 1:\n                        state_dict[key] = torch.cat(\n                            [s.tensor.view(-1) for s in local_shards], dim=0\n                        ).view(-1, dim)\n                    else:\n                        state_dict[key] = local_shards[0].tensor.view(-1, dim)\n            elif isinstance(state_dict[key], torch.Tensor):\n                local_shards = []\n                for shard in model_shards:\n                    # Extract shard size and offsets for splicing\n                    shard_sizes = shard.metadata.shard_sizes\n                    shard_offsets = shard.metadata.shard_offsets\n\n                    # Prepare tensor by splicing and placing on appropriate device\n                    spliced_tensor = state_dict[key][\n                        shard_offsets[0] : shard_offsets[0] + shard_sizes[0],\n                        shard_offsets[1] : shard_offsets[1] + shard_sizes[1],\n                    ]\n\n                    # Append spliced tensor into local shards\n                    local_shards.append(spliced_tensor)\n                state_dict[key] = (\n                    torch.empty(0)\n                    if not local_shards\n                    else torch.cat(local_shards, dim=0)\n                )\n            else:\n                raise RuntimeError(\n                    f\"Unexpected state_dict key type {type(state_dict[key])} found for {key}\"\n                )",
  "def _initialize_torch_state(self) -> None:  # noqa\n        \"\"\"\n        This provides consistency between this class and the EmbeddingBagCollection's\n        nn.Module API calls (state_dict, named_modules, etc)\n        \"\"\"\n        self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n        for table_name in self._table_names:\n            self.embedding_bags[table_name] = nn.Module()\n        self._model_parallel_name_to_local_shards = OrderedDict()\n        self._model_parallel_name_to_sharded_tensor = OrderedDict()\n        model_parallel_name_to_compute_kernel: Dict[str, str] = {}\n        for (\n            table_name,\n            parameter_sharding,\n        ) in self.module_sharding_plan.items():\n            if parameter_sharding.sharding_type == ShardingType.DATA_PARALLEL.value:\n                continue\n            self._model_parallel_name_to_local_shards[table_name] = []\n            model_parallel_name_to_compute_kernel[\n                table_name\n            ] = parameter_sharding.compute_kernel\n\n        self._name_to_table_size = {}\n        for table in self._embedding_bag_configs:\n            self._name_to_table_size[table.name] = (\n                table.num_embeddings,\n                table.embedding_dim,\n            )\n\n        for sharding_type, lookup in zip(\n            self._sharding_type_to_sharding.keys(), self._lookups\n        ):\n            if sharding_type == ShardingType.DATA_PARALLEL.value:\n                # unwrap DDP\n                lookup = lookup.module\n            else:\n                # save local_shards for transforming MP params to shardedTensor\n                for key, v in lookup.state_dict().items():\n                    table_name = key[: -len(\".weight\")]\n                    self._model_parallel_name_to_local_shards[table_name].extend(\n                        v.local_shards()\n                    )\n            for (\n                table_name,\n                tbe_slice,\n            ) in lookup.named_parameters_by_table():\n                self.embedding_bags[table_name].register_parameter(\"weight\", tbe_slice)\n        for (\n            table_name,\n            local_shards,\n        ) in self._model_parallel_name_to_local_shards.items():\n            # for shards that don't exist on this rank, register with empty tensor\n            if not hasattr(self.embedding_bags[table_name], \"weight\"):\n                self.embedding_bags[table_name].register_parameter(\n                    \"weight\", nn.Parameter(torch.empty(0))\n                )\n                if (\n                    model_parallel_name_to_compute_kernel[table_name]\n                    != EmbeddingComputeKernel.DENSE.value\n                ):\n                    self.embedding_bags[table_name].weight._in_backward_optimizers = [\n                        EmptyFusedOptimizer()\n                    ]\n            # created ShardedTensors once in init, use in post_state_dict_hook\n            self._model_parallel_name_to_sharded_tensor[\n                table_name\n            ] = ShardedTensor._init_from_local_shards(\n                local_shards,\n                self._name_to_table_size[table_name],\n                process_group=self._env.process_group,\n            )\n\n        def post_state_dict_hook(\n            module: ShardedEmbeddingBagCollection,\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            # Adjust dense MP\n            for (\n                table_name,\n                sharded_t,\n            ) in module._model_parallel_name_to_sharded_tensor.items():\n                destination_key = f\"{prefix}embedding_bags.{table_name}.weight\"\n                destination[destination_key] = sharded_t\n\n        self._register_state_dict_hook(post_state_dict_hook)\n        self._register_load_state_dict_pre_hook(\n            self._pre_load_state_dict_hook, with_module=True\n        )\n        self.reset_parameters()",
  "def reset_parameters(self) -> None:\n        if self._device and self._device.type == \"meta\":\n            return\n\n        # Initialize embedding bags weights with init_fn\n        for table_config in self._embedding_bag_configs:\n            assert table_config.init_fn is not None\n            param = self.embedding_bags[f\"{table_config.name}\"].weight\n            # pyre-ignore\n            table_config.init_fn(param)",
  "def _create_input_dist(\n        self,\n        input_feature_names: List[str],\n    ) -> None:\n        feature_names: List[str] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._input_dists.append(sharding.create_input_dist())\n            feature_names.extend(sharding.feature_names())\n            self._feature_splits.append(len(sharding.feature_names()))\n\n        if feature_names == input_feature_names:\n            self._has_features_permute = False\n        else:\n            for f in feature_names:\n                self._features_order.append(input_feature_names.index(f))\n            self.register_buffer(\n                \"_features_order_tensor\",\n                torch.tensor(\n                    self._features_order, device=self._device, dtype=torch.int32\n                ),\n                persistent=False,\n            )",
  "def _create_lookups(\n        self,\n    ) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._lookups.append(sharding.create_lookup())",
  "def _create_output_dist(self) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._output_dists.append(sharding.create_output_dist(device=self._device))\n            self._embedding_names.extend(sharding.embedding_names())\n            self._embedding_dims.extend(sharding.embedding_dims())",
  "def input_dist(\n        self, ctx: EmbeddingBagCollectionContext, features: KeyedJaggedTensor\n    ) -> Awaitable[Awaitable[KJTList]]:\n        if self._has_uninitialized_input_dist:\n            self._create_input_dist(features.keys())\n            self._has_uninitialized_input_dist = False\n        with torch.no_grad():\n            if self._has_features_permute:\n                features = features.permute(\n                    self._features_order,\n                    self._features_order_tensor,\n                )\n            features_by_shards = features.split(\n                self._feature_splits,\n            )\n            awaitables = []\n            for input_dist, features_by_shard in zip(\n                self._input_dists, features_by_shards\n            ):\n                awaitables.append(input_dist(features_by_shard))\n                ctx.sharding_contexts.append(EmbeddingShardingContext())\n            return KJTListSplitsAwaitable(awaitables, ctx)",
  "def compute(\n        self,\n        ctx: EmbeddingBagCollectionContext,\n        dist_input: KJTList,\n    ) -> List[torch.Tensor]:\n        return [lookup(features) for lookup, features in zip(self._lookups, dist_input)]",
  "def output_dist(\n        self,\n        ctx: EmbeddingBagCollectionContext,\n        output: List[torch.Tensor],\n    ) -> LazyAwaitable[KeyedTensor]:\n        return EmbeddingBagCollectionAwaitable(\n            awaitables=[\n                dist(embeddings, sharding_ctx)\n                for dist, sharding_ctx, embeddings in zip(\n                    self._output_dists,\n                    ctx.sharding_contexts,\n                    output,\n                )\n            ],\n            embedding_dims=self._embedding_dims,\n            embedding_names=self._embedding_names,\n        )",
  "def compute_and_output_dist(\n        self, ctx: EmbeddingBagCollectionContext, input: KJTList\n    ) -> LazyAwaitable[KeyedTensor]:\n        return EmbeddingBagCollectionAwaitable(\n            awaitables=[\n                dist(lookup(features), sharding_ctx)\n                for lookup, dist, sharding_ctx, features in zip(\n                    self._lookups,\n                    self._output_dists,\n                    ctx.sharding_contexts,\n                    input,\n                )\n            ],\n            embedding_dims=self._embedding_dims,\n            embedding_names=self._embedding_names,\n        )",
  "def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim",
  "def create_context(self) -> EmbeddingBagCollectionContext:\n        return EmbeddingBagCollectionContext()",
  "def shard(\n        self,\n        module: EmbeddingBagCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingBagCollection:\n        return ShardedEmbeddingBagCollection(\n            module=module,\n            table_name_to_parameter_sharding=params,\n            env=env,\n            fused_params=self.fused_params,\n            device=device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def shardable_parameters(\n        self, module: EmbeddingBagCollection\n    ) -> Dict[str, nn.Parameter]:\n        return {\n            name.split(\".\")[0]: param\n            for name, param in module.embedding_bags.named_parameters()\n        }",
  "def module_type(self) -> Type[EmbeddingBagCollection]:\n        return EmbeddingBagCollection",
  "def __init__(\n        self,\n        awaitable: Awaitable[torch.Tensor],\n    ) -> None:\n        super().__init__()\n        self._awaitable = awaitable",
  "def _wait_impl(self) -> torch.Tensor:\n        embedding = self._awaitable.wait()\n        return embedding",
  "def __init__(\n        self,\n        module: nn.EmbeddingBag,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n\n        assert (\n            len(table_name_to_parameter_sharding) == 1\n        ), \"expect 1 table, but got len(table_name_to_parameter_sharding)\"\n        assert module.mode == \"sum\", \"ShardedEmbeddingBag only supports sum pooling\"\n\n        self._dummy_embedding_table_name = \"dummy_embedding_table_name\"\n        self._dummy_feature_name = \"dummy_feature_name\"\n        self.parameter_sharding: ParameterSharding = next(\n            iter(table_name_to_parameter_sharding.values())\n        )\n        embedding_table_config = EmbeddingTableConfig(\n            num_embeddings=module.num_embeddings,\n            embedding_dim=module.embedding_dim,\n            name=self._dummy_embedding_table_name,\n            feature_names=[self._dummy_feature_name],\n            pooling=PoolingType.SUM,\n            # We set is_weighted to True for now,\n            # if per_sample_weights is None in forward(),\n            # we could assign a all-one vector to per_sample_weights\n            is_weighted=True,\n            embedding_names=[self._dummy_feature_name],\n        )\n\n        if self.parameter_sharding.sharding_type == ShardingType.TABLE_WISE.value:\n            # TODO: enable it with correct semantics, see T104397332\n            raise RuntimeError(\n                \"table-wise sharding on a single EmbeddingBag is not supported yet\"\n            )\n\n        self._embedding_sharding: EmbeddingSharding[\n            EmbeddingShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n        ] = create_embedding_bag_sharding(\n            sharding_type=self.parameter_sharding.sharding_type,\n            sharding_infos=[\n                EmbeddingShardingInfo(\n                    embedding_config=embedding_table_config,\n                    param_sharding=self.parameter_sharding,\n                    param=next(iter(module.parameters())),\n                    fused_params=fused_params,\n                ),\n            ],\n            env=env,\n            device=device,\n            permute_embeddings=True,\n        )\n        self._input_dist: nn.Module = self._embedding_sharding.create_input_dist()\n        self._lookup: nn.Module = self._embedding_sharding.create_lookup()\n        self._output_dist: nn.Module = self._embedding_sharding.create_output_dist()\n\n        # Get all fused optimizers and combine them.\n        optims = []\n        for _, module in self._lookup.named_modules():\n            if isinstance(module, FusedOptimizerModule):\n                # modify param keys to match EmbeddingBag\n                params: Mapping[str, Union[torch.Tensor, ShardedTensor]] = {}\n                for param_key, weight in module.fused_optimizer.params.items():\n                    # pyre-fixme[16]: `Mapping` has no attribute `__setitem__`.\n                    params[param_key.split(\".\")[-1]] = weight\n                module.fused_optimizer.params = params\n                optims.append((\"\", module.fused_optimizer))\n        self._optim: CombinedOptimizer = CombinedOptimizer(optims)",
  "def input_dist(\n        self,\n        ctx: NullShardedModuleContext,\n        input: Tensor,\n        offsets: Optional[Tensor] = None,\n        per_sample_weights: Optional[Tensor] = None,\n    ) -> Awaitable[Awaitable[KeyedJaggedTensor]]:\n        if per_sample_weights is None:\n            per_sample_weights = torch.ones_like(input, dtype=torch.float)\n        features = KeyedJaggedTensor(\n            keys=[self._dummy_feature_name],\n            values=input,\n            offsets=offsets,\n            weights=per_sample_weights,\n        )\n        return self._input_dist(features)",
  "def compute(\n        self, ctx: NullShardedModuleContext, dist_input: KeyedJaggedTensor\n    ) -> torch.Tensor:\n        return self._lookup(dist_input)",
  "def output_dist(\n        self, ctx: NullShardedModuleContext, output: torch.Tensor\n    ) -> LazyAwaitable[torch.Tensor]:\n        return EmbeddingAwaitable(\n            awaitable=self._output_dist(output),\n        )",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n        # pyre-fixme[19]: Expected 0 positional arguments.\n        lookup_state_dict = self._lookup.state_dict(None, \"\", keep_vars)\n        # update key to match embeddingBag state_dict key\n        for key, item in lookup_state_dict.items():\n            new_key = prefix + key.split(\".\")[-1]\n            destination[new_key] = item\n        return destination",
  "def named_modules(\n        self,\n        memo: Optional[Set[nn.Module]] = None,\n        prefix: str = \"\",\n        remove_duplicate: bool = True,\n    ) -> Iterator[Tuple[str, nn.Module]]:\n        yield from [(prefix, self)]",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        # TODO: add remove_duplicate\n        for name, parameter in self._lookup.named_parameters(\"\", recurse):\n            # update name to match embeddingBag parameter name\n            yield append_prefix(prefix, name.split(\".\")[-1]), parameter",
  "def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        if self.parameter_sharding.sharding_type == ShardingType.DATA_PARALLEL.value:\n            yield from []\n        else:\n            for name, _ in self._lookup.named_parameters(\"\"):\n                yield append_prefix(prefix, name.split(\".\")[-1])",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        # TODO: add remove_duplicate\n        for name, buffer in self._lookup.named_buffers(\"\", recurse):\n            yield append_prefix(prefix, name.split(\".\")[-1]), buffer",
  "def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, torch.Tensor]\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        missing_keys = []\n        unexpected_keys = []\n        # update key to match  embeddingBag state_dict key\n        for key, value in state_dict.items():\n            new_key = \".\".join([self._dummy_embedding_table_name, key])\n            state_dict[new_key] = value\n            state_dict.pop(key)\n        missing, unexpected = self._lookup.load_state_dict(\n            state_dict,\n            strict,\n        )\n        missing_keys.extend(missing)\n        unexpected_keys.extend(unexpected)\n\n        return _IncompatibleKeys(\n            missing_keys=missing_keys, unexpected_keys=unexpected_keys\n        )",
  "def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim",
  "def create_context(self) -> NullShardedModuleContext:\n        return NullShardedModuleContext()",
  "def shard(\n        self,\n        module: nn.EmbeddingBag,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingBag:\n        return ShardedEmbeddingBag(module, params, env, self.fused_params, device)",
  "def shardable_parameters(self, module: nn.EmbeddingBag) -> Dict[str, nn.Parameter]:\n        return {name: param for name, param in module.named_parameters()}",
  "def module_type(self) -> Type[nn.EmbeddingBag]:\n        return nn.EmbeddingBag",
  "def post_state_dict_hook(\n            module: ShardedEmbeddingBagCollection,\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            # Adjust dense MP\n            for (\n                table_name,\n                sharded_t,\n            ) in module._model_parallel_name_to_sharded_tensor.items():\n                destination_key = f\"{prefix}embedding_bags.{table_name}.weight\"\n                destination[destination_key] = sharded_t",
  "class ShardedFusedEmbeddingBagCollection(\n    ShardedEmbeddingBagCollection,\n):\n    def __init__(\n        self,\n        module: FusedEmbeddingBagCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        optimizer_type = module.optimizer_type()\n        optimizer_kwargs = module.optimizer_kwargs()\n\n        fused_params = {}\n        emb_opt_type_and_kwargs = convert_optimizer_type_and_kwargs(\n            optimizer_type, optimizer_kwargs\n        )\n        assert emb_opt_type_and_kwargs is not None\n        (emb_optim_type, emb_opt_kwargs) = emb_opt_type_and_kwargs\n\n        fused_params[\"optimizer\"] = emb_optim_type\n        fused_params.update(emb_opt_kwargs)\n\n        super().__init__(\n            module=module,\n            table_name_to_parameter_sharding=table_name_to_parameter_sharding,\n            fused_params=fused_params,\n            env=env,\n            device=device,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n\n        for index, (sharding, lookup) in enumerate(\n            zip(self._sharding_type_to_sharding.values(), self._lookups)\n        ):\n            if isinstance(sharding, DpPooledEmbeddingSharding):\n                self._lookups[index] = DistributedDataParallel(\n                    module=lookup,\n                    device_ids=[device],\n                    process_group=env.process_group,\n                    gradient_as_bucket_view=True,\n                    broadcast_buffers=False,\n                    static_graph=True,\n                )\n                self._lookups[index]._register_fused_optim(\n                    optimizer_type, **optimizer_kwargs\n                )",
  "class FusedEmbeddingBagCollectionSharder(\n    BaseEmbeddingSharder[FusedEmbeddingBagCollection]\n):\n    def shard(\n        self,\n        module: FusedEmbeddingBagCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingBagCollection:\n\n        return ShardedFusedEmbeddingBagCollection(\n            module,\n            params,\n            env,\n            device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )\n\n    def shardable_parameters(\n        self, module: FusedEmbeddingBagCollection\n    ) -> Dict[str, nn.Parameter]:\n\n        params = {\n            name.split(\".\")[-2]: param\n            for name, param in module.state_dict().items()\n            if name.endswith(\".weight\")\n        }\n        return params\n\n    @property\n    def module_type(self) -> Type[FusedEmbeddingBagCollection]:\n        return FusedEmbeddingBagCollection\n\n    def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.DATA_PARALLEL.value,\n            ShardingType.TABLE_WISE.value,\n            ShardingType.COLUMN_WISE.value,\n            ShardingType.TABLE_COLUMN_WISE.value,\n        ]\n        if compute_device_type in {\"cuda\"}:\n            types += [\n                ShardingType.ROW_WISE.value,\n                ShardingType.TABLE_ROW_WISE.value,\n            ]\n\n        return types\n\n    def compute_kernels(\n        self, sharding_type: str, compute_device_type: str\n    ) -> List[str]:\n        ret = []\n        if sharding_type != ShardingType.DATA_PARALLEL.value:\n            ret += [\n                EmbeddingComputeKernel.FUSED.value,\n            ]\n            if compute_device_type in {\"cuda\"}:\n                ret += [\n                    EmbeddingComputeKernel.FUSED_UVM.value,\n                    EmbeddingComputeKernel.FUSED_UVM_CACHING.value,\n                ]\n        else:\n            ret.append(EmbeddingComputeKernel.DENSE.value)\n        return ret",
  "def __init__(\n        self,\n        module: FusedEmbeddingBagCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        optimizer_type = module.optimizer_type()\n        optimizer_kwargs = module.optimizer_kwargs()\n\n        fused_params = {}\n        emb_opt_type_and_kwargs = convert_optimizer_type_and_kwargs(\n            optimizer_type, optimizer_kwargs\n        )\n        assert emb_opt_type_and_kwargs is not None\n        (emb_optim_type, emb_opt_kwargs) = emb_opt_type_and_kwargs\n\n        fused_params[\"optimizer\"] = emb_optim_type\n        fused_params.update(emb_opt_kwargs)\n\n        super().__init__(\n            module=module,\n            table_name_to_parameter_sharding=table_name_to_parameter_sharding,\n            fused_params=fused_params,\n            env=env,\n            device=device,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n\n        for index, (sharding, lookup) in enumerate(\n            zip(self._sharding_type_to_sharding.values(), self._lookups)\n        ):\n            if isinstance(sharding, DpPooledEmbeddingSharding):\n                self._lookups[index] = DistributedDataParallel(\n                    module=lookup,\n                    device_ids=[device],\n                    process_group=env.process_group,\n                    gradient_as_bucket_view=True,\n                    broadcast_buffers=False,\n                    static_graph=True,\n                )\n                self._lookups[index]._register_fused_optim(\n                    optimizer_type, **optimizer_kwargs\n                )",
  "def shard(\n        self,\n        module: FusedEmbeddingBagCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedEmbeddingBagCollection:\n\n        return ShardedFusedEmbeddingBagCollection(\n            module,\n            params,\n            env,\n            device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def shardable_parameters(\n        self, module: FusedEmbeddingBagCollection\n    ) -> Dict[str, nn.Parameter]:\n\n        params = {\n            name.split(\".\")[-2]: param\n            for name, param in module.state_dict().items()\n            if name.endswith(\".weight\")\n        }\n        return params",
  "def module_type(self) -> Type[FusedEmbeddingBagCollection]:\n        return FusedEmbeddingBagCollection",
  "def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.DATA_PARALLEL.value,\n            ShardingType.TABLE_WISE.value,\n            ShardingType.COLUMN_WISE.value,\n            ShardingType.TABLE_COLUMN_WISE.value,\n        ]\n        if compute_device_type in {\"cuda\"}:\n            types += [\n                ShardingType.ROW_WISE.value,\n                ShardingType.TABLE_ROW_WISE.value,\n            ]\n\n        return types",
  "def compute_kernels(\n        self, sharding_type: str, compute_device_type: str\n    ) -> List[str]:\n        ret = []\n        if sharding_type != ShardingType.DATA_PARALLEL.value:\n            ret += [\n                EmbeddingComputeKernel.FUSED.value,\n            ]\n            if compute_device_type in {\"cuda\"}:\n                ret += [\n                    EmbeddingComputeKernel.FUSED_UVM.value,\n                    EmbeddingComputeKernel.FUSED_UVM_CACHING.value,\n                ]\n        else:\n            ret.append(EmbeddingComputeKernel.DENSE.value)\n        return ret",
  "class TBEToRegisterMixIn:\n    def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        raise NotImplementedError",
  "def get_tbes_to_register_from_iterable(\n    iterable: Iterable[torch.nn.Module],\n) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n    tbes: Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig] = {}\n    for m in iterable:\n        if isinstance(m, TBEToRegisterMixIn):\n            tbes.update(m.get_tbes_to_register())\n    return tbes",
  "def is_fused_param_register_tbe(fused_params: Optional[Dict[str, Any]]) -> bool:\n    return (\n        fused_params\n        and FUSED_PARAM_REGISTER_TBE_BOOL in fused_params\n        and fused_params[FUSED_PARAM_REGISTER_TBE_BOOL]\n    )",
  "def is_fused_param_quant_state_dict_split_scale_bias(\n    fused_params: Optional[Dict[str, Any]]\n) -> bool:\n    return (\n        fused_params\n        and FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS in fused_params\n        and fused_params[FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS]\n    )",
  "def tbe_fused_params(\n    fused_params: Optional[Dict[str, Any]]\n) -> Optional[Dict[str, Any]]:\n    if not fused_params:\n        return None\n\n    fused_params_for_tbe = dict(fused_params)\n    if FUSED_PARAM_REGISTER_TBE_BOOL in fused_params_for_tbe:\n        fused_params_for_tbe.pop(FUSED_PARAM_REGISTER_TBE_BOOL)\n    if FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS in fused_params_for_tbe:\n        fused_params_for_tbe.pop(FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS)\n\n    return fused_params_for_tbe",
  "def get_tbes_to_register(\n        self,\n    ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:\n        raise NotImplementedError",
  "class TrainPipeline(abc.ABC, Generic[In, Out]):\n    @abc.abstractmethod\n    def progress(self, dataloader_iter: Iterator[In]) -> Out:\n        pass",
  "def _to_device(batch: In, device: torch.device, non_blocking: bool) -> In:\n    assert isinstance(\n        batch, (torch.Tensor, Pipelineable)\n    ), f\"{type(batch)} must implement Pipelineable interface\"\n    return cast(In, batch.to(device=device, non_blocking=non_blocking))",
  "def _wait_for_batch(batch: In, stream: Optional[torch.cuda.streams.Stream]) -> None:\n    if stream is None:\n        return\n    torch.cuda.current_stream().wait_stream(stream)\n    \"\"\"\n    As mentioned in\n    https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html, PyTorch\n    uses the \"caching allocator\" for memory allocation for tensors. When a tensor is\n    freed, its memory is likely to be reused by newly constructed tenosrs. By default,\n    this allocator traces whether a tensor is still in use by only the CUDA stream where\n    it was created. When a tensor is used by additional CUDA streams, we need to call\n    `record_stream` to tell the allocator about these streams. Otherwise, the allocator\n    might free the underlying memory of the tensor once it is no longer used by the\n    creator stream. This is a notable programming trick when we write programs using\n    multiple CUDA streams.\n    \"\"\"\n\n    cur_stream = torch.cuda.current_stream()\n    assert isinstance(\n        batch, (torch.Tensor, Multistreamable)\n    ), f\"{type(batch)} must implement Multistreamable interface\"\n    batch.record_stream(cur_stream)",
  "class TrainPipelineBase(TrainPipeline[In, Out]):\n    \"\"\"\n    This class runs training iterations using a pipeline of two stages, each as a CUDA\n    stream, namely, the current (default) stream and `self._memcpy_stream`. For each\n    iteration, `self._memcpy_stream` moves the input from host (CPU) memory to GPU\n    memory, and the default stream runs forward, backward, and optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        device: torch.device,\n    ) -> None:\n        self._model = model\n        self._optimizer = optimizer\n        self._device = device\n        self._memcpy_stream: Optional[torch.cuda.streams.Stream] = (\n            torch.cuda.Stream() if device.type == \"cuda\" else None\n        )\n        self._cur_batch: Optional[In] = None\n        self._connected = False\n\n    def _connect(self, dataloader_iter: Iterator[In]) -> None:\n        cur_batch = next(dataloader_iter)\n        self._cur_batch = cur_batch\n        with torch.cuda.stream(self._memcpy_stream):\n            self._cur_batch = _to_device(cur_batch, self._device, non_blocking=True)\n        self._connected = True\n\n    def progress(self, dataloader_iter: Iterator[In]) -> Out:\n        if not self._connected:\n            self._connect(dataloader_iter)\n\n        # Fetch next batch\n        with record_function(\"## next_batch ##\"):\n            next_batch = next(dataloader_iter)\n        cur_batch = self._cur_batch\n        assert cur_batch is not None\n\n        if self._model.training:\n            with record_function(\"## zero_grad ##\"):\n                self._optimizer.zero_grad()\n\n        with record_function(\"## wait_for_batch ##\"):\n            _wait_for_batch(cur_batch, self._memcpy_stream)\n\n        with record_function(\"## forward ##\"):\n            (losses, output) = self._model(cur_batch)\n\n        if self._model.training:\n            with record_function(\"## backward ##\"):\n                torch.sum(losses, dim=0).backward()\n\n        # Copy the next batch to GPU\n        self._cur_batch = cur_batch = next_batch\n        with record_function(\"## copy_batch_to_gpu ##\"):\n            with torch.cuda.stream(self._memcpy_stream):\n                self._cur_batch = _to_device(cur_batch, self._device, non_blocking=True)\n\n        # Update\n        if self._model.training:\n            with record_function(\"## optimizer ##\"):\n                self._optimizer.step()\n\n        return output",
  "class Tracer(torch.fx.Tracer):\n    \"\"\"\n    Disables proxying buffers during tracing. Ideally, proxying buffers would be\n    disabled, but some models are currently mutating buffer values, which causes errors\n    during tracing. If those models can be rewritten to not do that, we can likely\n    remove this line.\n    \"\"\"\n\n    proxy_buffer_attributes = False\n\n    def __init__(self, leaf_modules: Optional[List[str]] = None) -> None:\n        super().__init__()\n        self._leaf_modules: List[str] = leaf_modules if leaf_modules is not None else []\n\n    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:\n        if (\n            isinstance(m, ShardedModule)\n            or module_qualified_name in self._leaf_modules\n            or isinstance(m, FSDP)\n        ):\n            return True\n        return super().is_leaf_module(m, module_qualified_name)",
  "class SplitsAllToAllAwaitable(Awaitable[List[List[int]]]):\n    def __init__(\n        self,\n        input_tensors: List[torch.Tensor],\n        pg: dist.ProcessGroup,\n    ) -> None:\n        super().__init__()\n        self.num_workers: int = pg.size()\n\n        with record_function(\"## all2all_data:kjt splits ##\"):\n            self._output_tensor: torch.Tensor = torch.empty(\n                [self.num_workers * len(input_tensors)],\n                device=input_tensors[0].device,\n                dtype=input_tensors[0].dtype,\n            )\n            input_tensor = torch.stack(input_tensors, dim=1).flatten()\n            self._splits_awaitable: dist.Work = dist.all_to_all_single(\n                output=self._output_tensor,\n                input=input_tensor,\n                group=pg,\n                async_op=True,\n            )\n\n    def _wait_impl(self) -> List[List[int]]:\n        self._splits_awaitable.wait()\n        return self._output_tensor.view(self.num_workers, -1).T.tolist()",
  "def _set_sharding_context_intra_a2a(\n    tensors_awaitables: List[Awaitable[KeyedJaggedTensor]],\n    ctx: C,\n) -> None:\n    for awaitable, sharding_context in zip(\n        tensors_awaitables,\n        getattr(ctx, \"sharding_contexts\", []),\n    ):\n        if isinstance(awaitable, KJTAllToAllTensorsAwaitable):\n            if hasattr(sharding_context, \"input_splits\"):\n                sharding_context.input_splits = awaitable._input_splits[\"values\"]\n            if hasattr(sharding_context, \"output_splits\"):\n                sharding_context.output_splits = awaitable._output_splits[\"values\"]\n            if hasattr(sharding_context, \"sparse_features_recat\"):\n                sharding_context.sparse_features_recat = awaitable._recat\n            if (\n                hasattr(sharding_context, \"batch_size_per_rank\")\n                and awaitable._stride_per_rank is not None\n            ):\n                sharding_context.batch_size_per_rank = awaitable._stride_per_rank",
  "def _set_sharding_context_pre_a2a(\n    awaitables: List[Awaitable[Awaitable[KeyedJaggedTensor]]],\n    ctx: C,\n) -> None:\n    for awaitable, sharding_context in zip(\n        awaitables,\n        getattr(ctx, \"sharding_contexts\", []),\n    ):\n        kjt = (\n            awaitable._obj._obj\n            if isinstance(awaitable, NoWait)\n            else awaitable._input  # pyre-ignore[16]: KJTAllToAllSplitsAwaitable or KJTSplitsAllToAllMeta\n        )\n        if hasattr(sharding_context, \"batch_size_per_feature_pre_a2a\"):\n            sharding_context.batch_size_per_feature_pre_a2a = kjt.stride_per_key()\n        if hasattr(sharding_context, \"variable_batch_per_feature\"):\n            sharding_context.variable_batch_per_feature = kjt.variable_stride_per_key()",
  "class KJTSplitsAllToAllMeta:\n    pg: dist.ProcessGroup\n    _input: KeyedJaggedTensor\n    splits: List[int]\n    splits_tensors: List[torch.Tensor]\n    input_splits: List[List[int]]\n    input_tensors: List[torch.Tensor]\n    labels: List[str]\n    keys: List[str]\n    device: torch.device\n    stagger: int",
  "def _split(flat_list: List[T], splits: List[int]) -> List[List[T]]:\n    return [\n        flat_list[sum(splits[:i]) : sum(splits[:i]) + n] for i, n in enumerate(splits)\n    ]",
  "class FusedKJTListSplitsAwaitable(Awaitable[List[KJTListAwaitable]]):\n    def __init__(\n        self,\n        requests: List[KJTListSplitsAwaitable[C]],\n        contexts: List[C],\n        pg: Optional[dist.ProcessGroup],\n    ) -> None:\n        super().__init__()\n        self._contexts = contexts\n        self._awaitables: List[\n            Union[KJTSplitsAllToAllMeta, Awaitable[Awaitable[KeyedJaggedTensor]]]\n        ] = [awaitable for request in requests for awaitable in request.awaitables]\n        for req, ctx in zip(requests, self._contexts):\n            _set_sharding_context_pre_a2a(req.awaitables, ctx)\n        self._output_lengths: List[int] = [\n            len(request.awaitables) for request in requests\n        ]\n        self._lengths: List[int] = [\n            len(awaitable.splits_tensors)\n            if isinstance(awaitable, KJTSplitsAllToAllMeta)\n            else 0\n            for awaitable in self._awaitables\n        ]\n        splits_tensors = [\n            splits_tensor\n            for awaitable in self._awaitables\n            for splits_tensor in (\n                awaitable.splits_tensors\n                if isinstance(awaitable, KJTSplitsAllToAllMeta)\n                else []\n            )\n        ]\n        self._splits_awaitable: Optional[SplitsAllToAllAwaitable] = (\n            SplitsAllToAllAwaitable(\n                input_tensors=splits_tensors,\n                pg=pg,\n            )\n            if splits_tensors and pg is not None\n            else None\n        )\n\n    def _wait_impl(self) -> List[KJTListAwaitable]:\n        if self._splits_awaitable:\n            splits_list = self._splits_awaitable.wait()\n            splits_per_awaitable = _split(splits_list, self._lengths)\n        else:\n            splits_per_awaitable = [[] for _ in range(len(self._lengths))]\n        tensors_awaitables = []\n        for splits, awaitable in zip(splits_per_awaitable, self._awaitables):\n            if not splits:  # NoWait\n                assert isinstance(awaitable, Awaitable)\n                tensors_awaitables.append(awaitable.wait())\n                continue\n            assert isinstance(awaitable, KJTSplitsAllToAllMeta)\n            if awaitable._input.variable_stride_per_key():\n                output_splits = splits\n                stride_per_rank = None\n            else:\n                output_splits = splits[:-1]\n                stride_per_rank = splits[-1]\n            tensors_awaitables.append(\n                KJTAllToAllTensorsAwaitable(\n                    pg=awaitable.pg,\n                    input=awaitable._input,\n                    splits=awaitable.splits,\n                    input_splits=awaitable.input_splits,\n                    output_splits=output_splits,\n                    input_tensors=awaitable.input_tensors,\n                    labels=awaitable.labels,\n                    keys=awaitable.keys,\n                    device=awaitable.device,\n                    stagger=awaitable.stagger,\n                    stride_per_rank=stride_per_rank,\n                )\n            )\n        output = []\n        awaitables_per_output = _split(tensors_awaitables, self._output_lengths)\n        for awaitables, ctx in zip(awaitables_per_output, self._contexts):\n            _set_sharding_context_intra_a2a(awaitables, ctx)\n            output.append(KJTListAwaitable(awaitables, ctx))\n        return output",
  "class TrainPipelineContext:\n    \"\"\"\n    Context information for a `TrainPipelineSparseDist` instance.\n\n    Attributes:\n        input_dist_splits_requests (Dict[str, Awaitable[Any]]): Stores input dist\n            requests in the splits awaitable stage, which occurs after starting the\n            input dist.\n        input_dist_tensors_requests (Dict[str, Awaitable[Any]]): Stores input dist\n            requests in the tensors awaitable stage, which occurs after calling `wait()`\n            on the splits awaitable.\n        module_contexts (Dict[str, Multistreamable]): Stores module contexts from the\n            input dist for the current batch.\n        module_contexts_next_batch (Dict[str, Multistreamable]): Stores module contexts\n            from the input dist for the next batch.\n        fused_splits_awaitables (List[Tuple[List[str], FusedKJTListSplitsAwaitable]]):\n            List of fused splits input dist awaitable and the corresponding module names\n            of each awaitable.\n    \"\"\"\n\n    # pyre-ignore [4]\n    input_dist_splits_requests: Dict[str, Awaitable[Any]] = field(default_factory=dict)\n    # pyre-ignore [4]\n    input_dist_tensors_requests: Dict[str, Awaitable[Any]] = field(default_factory=dict)\n    module_contexts: Dict[str, Multistreamable] = field(default_factory=dict)\n    module_contexts_next_batch: Dict[str, Multistreamable] = field(default_factory=dict)\n    fused_splits_awaitables: List[\n        Tuple[List[str], FusedKJTListSplitsAwaitable]\n    ] = field(default_factory=list)",
  "class PrefetchTrainPipelineContext(TrainPipelineContext):\n    module_input_post_prefetch: Dict[str, Multistreamable] = field(default_factory=dict)\n    module_contexts_post_prefetch: Dict[str, Multistreamable] = field(\n        default_factory=dict\n    )",
  "class ArgInfo:\n    \"\"\"\n    Representation of args from a node.\n\n    Attributes:\n        input_attrs (List[str]): attributes of input batch,\n            e.g. `batch.attr1.attr2` will produce [\"attr1\", \"attr2\"].\n        is_getitems (List[bool]): `batch[attr1].attr2` will produce [True, False].\n        name (Optional[str]): name for kwarg of pipelined forward() call or None for a\n            positional arg.\n    \"\"\"\n\n    input_attrs: List[str]\n    is_getitems: List[bool]\n    name: Optional[str]",
  "class BaseForward:\n    def __init__(\n        self,\n        name: str,\n        args: List[ArgInfo],\n        module: ShardedModule,\n        context: TrainPipelineContext,\n        stream: Optional[torch.cuda.streams.Stream],\n    ) -> None:\n        self._name = name\n        self._args = args\n        self._module = module\n        self._context = context\n        self._stream = stream\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    @property\n    def args(self) -> List[ArgInfo]:\n        return self._args",
  "class PipelinedForward(BaseForward):\n    # pyre-ignore [2, 24]\n    def __call__(self, *input, **kwargs) -> Awaitable:\n        assert self._name in self._context.input_dist_tensors_requests\n        request = self._context.input_dist_tensors_requests[self._name]\n        assert isinstance(request, Awaitable)\n        with record_function(\"## wait_sparse_data_dist ##\"):\n            # Finish waiting on the dist_stream,\n            # in case some delayed stream scheduling happens during the wait() call.\n            with torch.cuda.stream(self._stream):\n                data = request.wait()\n\n        # Make sure that both result of input_dist and context\n        # are properly transferred to the current stream.\n        if self._stream is not None:\n            torch.cuda.current_stream().wait_stream(self._stream)\n            cur_stream = torch.cuda.current_stream()\n\n            assert isinstance(\n                data, (torch.Tensor, Multistreamable)\n            ), f\"{type(data)} must implement Multistreamable interface\"\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            data.record_stream(cur_stream)\n\n            ctx = self._context.module_contexts[self._name]\n            ctx.record_stream(cur_stream)\n\n        return self._module.compute_and_output_dist(\n            self._context.module_contexts[self._name], data\n        )",
  "class PrefetchPipelinedForward(BaseForward):\n    def __init__(\n        self,\n        name: str,\n        args: List[ArgInfo],\n        module: ShardedModule,\n        context: PrefetchTrainPipelineContext,\n        prefetch_stream: Optional[torch.cuda.streams.Stream],\n    ) -> None:\n        super().__init__(\n            name=name,\n            args=args,\n            module=module,\n            context=context,\n            stream=prefetch_stream,\n        )\n        self._context: PrefetchTrainPipelineContext = self._context\n\n    # pyre-ignore [2, 24]\n    def __call__(self, *input, **kwargs) -> Awaitable:\n        assert self._name in self._context.module_input_post_prefetch\n        data = self._context.module_input_post_prefetch[self._name]\n\n        # Make sure that both result of input_dist and context\n        # are properly transferred to the current stream.\n        if self._stream is not None:\n            torch.cuda.current_stream().wait_stream(self._stream)\n            cur_stream = torch.cuda.current_stream()\n\n            assert isinstance(\n                data, (torch.Tensor, Multistreamable)\n            ), f\"{type(data)} must implement Multistreamable interface\"\n            data.record_stream(cur_stream)\n\n            ctx = self._context.module_contexts_post_prefetch[self._name]\n            ctx.record_stream(cur_stream)\n\n        return self._module.compute_and_output_dist(\n            self._context.module_contexts_post_prefetch[self._name], data\n        )",
  "class KJTAllToAllForward:\n    def __init__(\n        self, pg: dist.ProcessGroup, splits: List[int], stagger: int = 1\n    ) -> None:\n        self._pg = pg\n        self._splits = splits\n        self._stagger = stagger\n        self._splits_cumsum: List[int] = [0] + list(itertools.accumulate(splits))\n\n    def __call__(self, input: KeyedJaggedTensor) -> KJTSplitsAllToAllMeta:\n        with torch.no_grad():\n            assert len(input.keys()) == sum(self._splits)\n            rank = dist.get_rank(self._pg)\n            local_keys = input.keys()[\n                self._splits_cumsum[rank] : self._splits_cumsum[rank + 1]\n            ]\n            input_splits = input.dist_splits(self._splits)\n            device = input.values().device\n            splits_tensors = [\n                torch.tensor(splits, device=device) for splits in input_splits\n            ]\n            if not input.variable_stride_per_key():\n                splits_tensors.append(\n                    torch.tensor([input.stride()] * self._pg.size(), device=device)\n                )\n            return KJTSplitsAllToAllMeta(\n                pg=self._pg,\n                _input=input,\n                splits=self._splits,\n                splits_tensors=splits_tensors,\n                input_splits=input_splits,\n                input_tensors=input.dist_tensors(),\n                labels=input.dist_labels(),\n                keys=local_keys,\n                device=device,\n                stagger=self._stagger,\n            )",
  "def _start_data_dist(\n    pipelined_modules: List[ShardedModule],\n    batch: In,\n    context: TrainPipelineContext,\n) -> None:\n    context.input_dist_splits_requests.clear()\n    context.module_contexts_next_batch.clear()\n    context.fused_splits_awaitables.clear()\n    for module in pipelined_modules:\n        forward = module.forward\n        assert isinstance(forward, PipelinedForward) or isinstance(\n            forward, PrefetchPipelinedForward\n        )\n\n        # Retrieve argument for the input_dist of EBC\n        # is_getitem True means this argument could be retrieved by a list\n        # False means this argument is getting while getattr\n        # and this info was done in the _rewrite_model by tracing the\n        # entire model to get the arg_info_list\n        args = []\n        kwargs = {}\n        for arg_info in forward.args:\n            if arg_info.input_attrs:\n                arg = batch\n                for attr, is_getitem in zip(arg_info.input_attrs, arg_info.is_getitems):\n                    if is_getitem:\n                        arg = arg[attr]\n                    else:\n                        arg = getattr(arg, attr)\n                if arg_info.name:\n                    kwargs[arg_info.name] = arg\n                else:\n                    args.append(arg)\n            else:\n                args.append(None)\n        # Start input distribution.\n        module_ctx = module.create_context()\n        context.module_contexts_next_batch[forward.name] = module_ctx\n        context.input_dist_splits_requests[forward.name] = module.input_dist(\n            module_ctx, *args, **kwargs\n        )\n    _fuse_input_dist_splits(context)",
  "def _fuse_input_dist_splits(context: TrainPipelineContext) -> None:\n    names_per_pg = defaultdict(list)\n    for name, request in context.input_dist_splits_requests.items():\n        pg = None\n        if isinstance(request, KJTListSplitsAwaitable):\n            for awaitable in request.awaitables:\n                if isinstance(awaitable, KJTSplitsAllToAllMeta):\n                    pg = awaitable.pg\n                    break\n        names_per_pg[pg].append(name)\n\n    for pg, names in names_per_pg.items():\n        context.fused_splits_awaitables.append(\n            (\n                names,\n                FusedKJTListSplitsAwaitable(\n                    # pyre-ignore[6]\n                    requests=[\n                        context.input_dist_splits_requests[name] for name in names\n                    ],\n                    contexts=[\n                        context.module_contexts_next_batch[name] for name in names\n                    ],\n                    pg=pg,\n                ),\n            )\n        )",
  "def _get_node_args_helper(\n    # pyre-ignore\n    arguments,\n    num_found: int,\n) -> Tuple[List[ArgInfo], int]:\n    \"\"\"\n    Goes through the args/kwargs of a node and arranges them into a list of `ArgInfo`s.\n    It also counts the number of (args + kwargs) found.\n    \"\"\"\n\n    arg_info_list = [ArgInfo([], [], None) for _ in range(len(arguments))]\n    for arg, arg_info in zip(arguments, arg_info_list):\n        if arg is None:\n            num_found += 1\n            continue\n        while True:\n            if not isinstance(arg, torch.fx.Node):\n                break\n            child_node = arg\n\n            if child_node.op == \"placeholder\":\n                if hasattr(child_node, \"ph_key\"):\n                    # pyre-ignore[16]\n                    arg_info.input_attrs.insert(0, child_node.ph_key)\n                    arg_info.is_getitems.insert(0, False)\n                num_found += 1\n                break\n            elif (\n                child_node.op == \"call_function\"\n                and child_node.target.__module__ == \"builtins\"\n                # pyre-ignore[16]\n                and child_node.target.__name__ == \"getattr\"\n            ):\n                # pyre-fixme[6]: For 2nd argument expected `str` but got\n                #  `Union[None, Dict[str, typing.Any], List[typing.Any], Node, bool,\n                #  complex, float, int, range, slice, str, device, dtype, layout,\n                #  memory_format, Tensor, typing.Tuple[typing.Any, ...]]`.\n                arg_info.input_attrs.insert(0, child_node.args[1])\n                arg_info.is_getitems.insert(0, False)\n                arg = child_node.args[0]\n            elif (\n                child_node.op == \"call_function\"\n                and child_node.target.__module__ == \"_operator\"\n                # pyre-ignore[16]\n                and child_node.target.__name__ == \"getitem\"\n            ):\n                # pyre-fixme[6]: For 2nd argument expected `str` but got\n                #  `Union[None, Dict[str, typing.Any], List[typing.Any], Node, bool,\n                #  complex, float, int, range, slice, str, device, dtype, layout,\n                #  memory_format, Tensor, typing.Tuple[typing.Any, ...]]`.\n                arg_info.input_attrs.insert(0, child_node.args[1])\n                arg_info.is_getitems.insert(0, True)\n                arg = child_node.args[0]\n            elif (\n                child_node.op == \"call_function\"\n                and child_node.target.__module__ == \"torch.utils._pytree\"\n                # pyre-ignore[16]\n                and child_node.target.__name__ == \"tree_unflatten\"\n            ):\n                \"\"\"\n                This is for the PT2 export path where we unflatten the input to reconstruct\n                the structure with the recorded tree spec.\n                \"\"\"\n                assert arg_info.is_getitems[0]\n                # pyre-fixme[16]\n                arg = child_node.args[0][arg_info.input_attrs[0]]\n            elif (\n                child_node.op == \"call_function\"\n                and child_node.target.__module__ == \"torchrec.sparse.jagged_tensor\"\n                # pyre-fixme[16]\n                and child_node.target.__name__ == \"KeyedJaggedTensor\"\n            ):\n                if \"values\" in child_node.kwargs:\n                    arg = child_node.kwargs[\"values\"]\n                else:\n                    arg = child_node.args[1]\n            else:\n                break\n    return arg_info_list, num_found",
  "def _get_node_args(\n    node: Node,\n) -> Tuple[List[ArgInfo], int]:\n    num_found = 0\n    pos_arg_info_list, num_found = _get_node_args_helper(node.args, num_found)\n    kwargs_arg_info_list, num_found = _get_node_args_helper(\n        node.kwargs.values(), num_found\n    )\n\n    # Replace with proper names for kwargs\n    for name, arg_info_list in zip(node.kwargs, kwargs_arg_info_list):\n        arg_info_list.name = name\n\n    arg_info_list = pos_arg_info_list + kwargs_arg_info_list\n    return arg_info_list, num_found",
  "def _get_leaf_module_names_helper(\n    model: torch.nn.Module,\n    path: str,\n    leaf_module_names: Set[str],\n) -> bool:\n    sharded_children = set()\n    for name, child in model.named_children():\n        curr_path = path + name\n        if isinstance(child, ShardedModule):\n            sharded_children.add(name)\n        else:\n            child_sharded = _get_leaf_module_names_helper(\n                child,\n                curr_path + \".\",\n                leaf_module_names,\n            )\n            if child_sharded:\n                sharded_children.add(name)\n\n    if len(sharded_children) > 0:\n        for name, child in model.named_children():\n            if name in sharded_children:\n                continue\n            # assume module is leaf node unless annotated otherwise\n            if not getattr(child, \"_is_pytorch_fx_traceable\", False):\n                leaf_module_names.add(path + name)\n    return len(sharded_children) > 0",
  "def _get_leaf_module_names(model: torch.nn.Module) -> List[str]:\n    \"\"\"\n    Returns a list of top level modules to be used as leaf modules for FX tracing.\n    This is a shallow FX trace that only goes the minimum depth required to pipeline\n    the model unless child modules are explicitly tagged as `_is_pytorch_fx_traceable`.\n    \"\"\"\n\n    leaf_module_names: Set[str] = set()\n    _get_leaf_module_names_helper(\n        model,\n        \"\",\n        leaf_module_names,\n    )\n    return list(leaf_module_names)",
  "def _jit_modules(module: torch.nn.Module, path: str, optional: bool = True) -> bool:\n    sharded_children = set()\n    for name, child in module.named_children():\n        curr_path = path + name\n        if isinstance(child, ShardedModule):\n            sharded_children.add(name)\n        else:\n            child_sharded = _jit_modules(child, curr_path + \".\", optional)\n            if child_sharded:\n                sharded_children.add(name)\n\n    if len(sharded_children) > 0:\n        for name, child in module.named_children():\n            if name not in sharded_children:\n                try:\n                    jit_child = torch.jit.script(child)\n                    setattr(module, name, jit_child)\n                    logger.info(f\"jit.script applied to {path + name}.\")\n                except Exception as error:\n                    if not optional:\n                        raise\n                    else:\n                        logger.info(\n                            f\"Warning: failed to jit.script {path + name}: {error}.\"\n                        )\n\n    return len(sharded_children) > 0",
  "def _rewrite_model(  # noqa C901\n    model: torch.nn.Module,\n    context: TrainPipelineContext,\n    dist_stream: Optional[torch.cuda.streams.Stream],\n    batch: Optional[In] = None,\n    apply_jit: bool = False,\n    pipelined_forward: Type[BaseForward] = PipelinedForward,\n) -> Tuple[List[ShardedModule], torch.nn.Module]:\n    input_model = model\n    # Get underlying nn.Module\n    if isinstance(model, DistributedModelParallel):\n        model = model.module\n\n    # Collect a list of sharded modules.\n    sharded_modules = {}\n    for name, m in model.named_modules():\n        if isinstance(m, ShardedModule):\n            sharded_modules[name] = m\n\n    # Trace a model.\n    concrete_args = {}\n    if batch:\n        if hasattr(batch, \"to_proxy\"):\n            # for some special models, it requires using \"input\"\n            # as the key for input\n            # pyre-ignore[16]: Variable[In (bound to Pipelineable)] has no attribute to_proxy.\n            concrete_args[\"inputs\"] = copy.copy(batch).to_proxy()\n        elif hasattr(batch, \"to_proxy_tuple\"):\n            # when the model is pre-fx traced or dynamo exported, the\n            # inputs are already flattened, and therefore we use\n            # tuple as concrete args that fx.trace will automatically\n            # match with the argument names.\n            # We pass in the model for the caller side to customize\n            # the batch\n            # pyre-ignore[16]: Variable[In (bound to Pipelineable)] has no attribute to_proxy_tuple.\n            concrete_args = batch.to_proxy_tuple(model)\n\n    tracer = Tracer(leaf_modules=_get_leaf_module_names(model))\n    graph = tracer.trace(model, concrete_args=concrete_args)\n\n    # Select sharded modules, which are top-level in the forward call graph,\n    # i.e. don't have input transformations, i.e. rely only on 'builtins.getattr'.\n    pipelined_forwards = []\n    for node in graph.nodes:\n        if node.op == \"call_module\" and node.target in sharded_modules:\n            total_num_args = len(node.args) + len(node.kwargs)\n            if total_num_args == 0:\n                continue\n            arg_info_list, num_found = _get_node_args(node)\n\n            if num_found == total_num_args:\n                logger.info(f\"Module '{node.target}'' will be pipelined\")\n                child = sharded_modules[node.target]\n                child.forward = pipelined_forward(\n                    node.target,\n                    arg_info_list,\n                    child,\n                    context,\n                    dist_stream,\n                )\n                pipelined_forwards.append(child)\n\n    # JIT script unsharded modules if applicable.\n    if apply_jit:\n        graph_model = torch.fx.GraphModule(model, graph)\n        _jit_modules(graph_model, \"\")\n        if isinstance(input_model, DistributedModelParallel):\n            input_model.module = graph_model\n\n    return pipelined_forwards, input_model",
  "def _override_input_dist_forwards(pipelined_modules: List[ShardedModule]) -> None:\n    \"\"\"\n    Overrides each input dist forward to support fusing the splits collective.\n    NOTE: this can only be called after the input dists are initialized.\n    \"\"\"\n    for module in pipelined_modules:\n        for child_fqn, child_module in module.named_modules():\n            if hasattr(child_module, \"_has_uninitialized_input_dist\"):\n                assert (\n                    not child_module._has_uninitialized_input_dist\n                ), f\"{child_fqn} has uninitialized input dist\"\n\n            if not hasattr(child_module, \"_input_dists\"):\n                continue\n\n            for input_dist in child_module._input_dists:\n                if hasattr(input_dist, \"_dist\"):\n                    assert isinstance(input_dist._dist, KJTAllToAll)\n                    input_dist._dist.forward = KJTAllToAllForward(\n                        pg=input_dist._dist._pg,\n                        splits=input_dist._dist._splits,\n                        stagger=input_dist._dist._stagger,\n                    )",
  "class TrainPipelineSparseDist(TrainPipeline[In, Out]):\n    \"\"\"\n    This pipeline overlaps device transfer, and `ShardedModule.input_dist()` with\n    forward and backward. This helps hide the all2all latency while preserving the\n    training forward / backward ordering.\n\n    stage 3: forward, backward - uses default CUDA stream\n    stage 2: ShardedModule.input_dist() - uses data_dist CUDA stream\n    stage 1: device transfer - uses memcpy CUDA stream\n\n    `ShardedModule.input_dist()` is only done for top-level modules in the call graph.\n    To be considered a top-level module, a module can only depend on 'getattr' calls on\n    input.\n\n    Input model must be symbolically traceable with the exception of `ShardedModule` and\n    `DistributedDataParallel` modules.\n\n    Args:\n        model (torch.nn.Module): model to pipeline.\n        optimizer (torch.optim.Optimizer): optimizer to use.\n        device (torch.device): device where device transfer, sparse data dist, and\n            forward/backward pass will happen.\n        execute_all_batches (bool): executes remaining batches in pipeline after\n            exhausting dataloader iterator.\n        apply_jit (bool): apply torch.jit.script to non-pipelined (unsharded) modules.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        device: torch.device,\n        execute_all_batches: bool = True,\n        apply_jit: bool = False,\n    ) -> None:\n        self._model = model\n        self._optimizer = optimizer\n        self._device = device\n        self._execute_all_batches = execute_all_batches\n        self._apply_jit = apply_jit\n        # use two data streams to support two concurrent batches\n        if device.type == \"cuda\":\n            self._memcpy_stream: Optional[\n                torch.cuda.streams.Stream\n            ] = torch.cuda.Stream(priority=-1)\n            self._data_dist_stream: Optional[\n                torch.cuda.streams.Stream\n            ] = torch.cuda.Stream(priority=-1)\n        else:\n            self._memcpy_stream: Optional[torch.cuda.streams.Stream] = None\n            self._data_dist_stream: Optional[torch.cuda.streams.Stream] = None\n        self._batch_i: Optional[In] = None\n        self._batch_ip1: Optional[In] = None\n        self._batch_ip2: Optional[In] = None\n        self._context = TrainPipelineContext()\n        self._pipelined_modules: List[ShardedModule] = []\n\n    def _fill_pipeline(self, dataloader_iter: Iterator[In]) -> None:\n        # pipeline is already filled\n        if self._batch_i and self._batch_ip1:\n            return\n        # executes last batch in pipeline\n        if self._batch_i and self._execute_all_batches:\n            return\n\n        # batch 1\n        self._batch_i = self._copy_batch_to_gpu(dataloader_iter)\n        if self._batch_i is None:\n            raise StopIteration\n\n        self._init_pipelined_modules(self._batch_i)\n        self._start_sparse_data_dist(self._batch_i)\n        self._wait_sparse_data_dist()\n\n        # batch 2\n        self._batch_ip1 = self._copy_batch_to_gpu(dataloader_iter)\n\n    def progress(self, dataloader_iter: Iterator[In]) -> Out:\n        self._fill_pipeline(dataloader_iter)\n\n        if self._model.training:\n            with record_function(\"## zero_grad ##\"):\n                self._optimizer.zero_grad()\n\n        with record_function(\"## wait_for_batch ##\"):\n            _wait_for_batch(cast(In, self._batch_i), self._data_dist_stream)\n\n        self._start_sparse_data_dist(self._batch_ip1)\n\n        self._batch_ip2 = self._copy_batch_to_gpu(dataloader_iter)\n\n        # forward\n        with record_function(\"## forward ##\"):\n            losses, output = cast(Tuple[torch.Tensor, Out], self._model(self._batch_i))\n\n        self._wait_sparse_data_dist()\n\n        if self._model.training:\n            # backward\n            with record_function(\"## backward ##\"):\n                torch.sum(losses, dim=0).backward()\n\n            # update\n            with record_function(\"## optimizer ##\"):\n                self._optimizer.step()\n\n        self._batch_i = self._batch_ip1\n        self._batch_ip1 = self._batch_ip2\n\n        return output\n\n    def _init_pipelined_modules(self, batch: In) -> None:\n        \"\"\"\n        Retrieves the pipelined modules after overriding their forwards, initializes the\n        modules' input dists, and overrides the input dist forwards to support fusing\n        the splits collective in the input dist.\n        \"\"\"\n        if self._pipelined_modules:\n            return\n        self._pipelined_modules, self._model = _rewrite_model(\n            model=self._model,\n            context=self._context,\n            dist_stream=self._data_dist_stream,\n            batch=self._batch_i,\n            apply_jit=self._apply_jit,\n        )\n        # initializes input dist, so we can override input dist forwards\n        self._start_sparse_data_dist(self._batch_i)\n        _override_input_dist_forwards(self._pipelined_modules)\n\n    def _copy_batch_to_gpu(self, dataloader_iter: Iterator[In]) -> Optional[In]:\n        \"\"\"\n        Retrieves batch from dataloader and moves it to the provided device.\n\n        Raises:\n            StopIteration: if the dataloader iterator is exhausted; unless\n                `self._execute_all_batches=True`, then returns None.\n        \"\"\"\n        with record_function(\"## copy_batch_to_gpu ##\"):\n            with torch.cuda.stream(self._memcpy_stream):\n                batch = next(dataloader_iter, None)\n                if batch is not None:\n                    batch = _to_device(batch, self._device, non_blocking=True)\n                elif not self._execute_all_batches:\n                    raise StopIteration\n                return batch\n\n    def _start_sparse_data_dist(self, batch: Optional[In]) -> None:\n        \"\"\"\n        Waits for batch to finish getting copied to GPU, then starts the input dist.\n        \"\"\"\n        if batch is None:\n            return\n        with record_function(\"## start_sparse_data_dist ##\"):\n            with torch.cuda.stream(self._data_dist_stream):\n                _wait_for_batch(batch, self._memcpy_stream)\n                _start_data_dist(self._pipelined_modules, batch, self._context)\n\n    def _wait_sparse_data_dist(self) -> None:\n        \"\"\"\n        Waits on the input dist splits requests to get the input dist tensors requests,\n        and populates the context with them.\n        \"\"\"\n        with record_function(\"## wait_sparse_data_dist ##\"):\n            with torch.cuda.stream(self._data_dist_stream):\n                self._context.module_contexts = (\n                    self._context.module_contexts_next_batch.copy()\n                )\n                self._context.input_dist_tensors_requests.clear()\n                for names, awaitable in self._context.fused_splits_awaitables:\n                    for name, request in zip(names, awaitable.wait()):\n                        self._context.input_dist_tensors_requests[name] = request",
  "class PrefetchTrainPipelineSparseDist(TrainPipelineSparseDist[In, Out]):\n    \"\"\"\n    This pipeline overlaps device transfer, `ShardedModule.input_dist()`, and cache\n    prefetching with forward and backward. This helps hide the all2all latency while\n    preserving the training forward / backward ordering.\n\n    stage 4: forward, backward - uses default CUDA stream\n    stage 3: prefetch - uses prefetch CUDA stream\n    stage 2: ShardedModule.input_dist() - uses data_dist CUDA stream\n    stage 1: device transfer - uses memcpy CUDA stream\n\n    `ShardedModule.input_dist()` is only done for top-level modules in the call graph.\n    To be considered a top-level module, a module can only depend on 'getattr' calls on\n    input.\n\n    Input model must be symbolically traceable with the exception of `ShardedModule` and\n    `DistributedDataParallel` modules.\n\n    Args:\n        model (torch.nn.Module): model to pipeline.\n        optimizer (torch.optim.Optimizer): optimizer to use.\n        device (torch.device): device where device transfer, sparse data dist, prefetch,\n            and forward/backward pass will happen.\n        execute_all_batches (bool): executes remaining batches in pipeline after\n            exhausting dataloader iterator.\n        apply_jit (bool): apply torch.jit.script to non-pipelined (unsharded) modules.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        device: torch.device,\n        execute_all_batches: bool = True,\n        apply_jit: bool = False,\n    ) -> None:\n        super().__init__(\n            model=model,\n            optimizer=optimizer,\n            device=device,\n            execute_all_batches=execute_all_batches,\n            apply_jit=apply_jit,\n        )\n        self._context = PrefetchTrainPipelineContext()\n        if self._device.type == \"cuda\":\n            self._prefetch_stream: Optional[\n                torch.cuda.streams.Stream\n            ] = torch.cuda.Stream()\n            self._default_stream: Optional[\n                torch.cuda.streams.Stream\n            ] = torch.cuda.current_stream()\n        else:\n            self._prefetch_stream: Optional[torch.cuda.streams.Stream] = None\n            self._default_stream: Optional[torch.cuda.streams.Stream] = None\n        self._batch_ip3: Optional[In] = None\n\n    def _fill_pipeline(self, dataloader_iter: Iterator[In]) -> None:\n        # pipeline is already filled\n        if self._batch_i and self._batch_ip1 and self._batch_ip2:\n            return\n        # executes last batch in pipeline\n        if self._execute_all_batches and (self._batch_i or self._batch_ip1):\n            return\n\n        # batch 1\n        self._batch_i = self._copy_batch_to_gpu(dataloader_iter)\n        if self._batch_i is None:\n            raise StopIteration\n\n        self._init_pipelined_modules(self._batch_i)\n        self._start_sparse_data_dist(self._batch_i)\n        self._wait_sparse_data_dist()\n        self._prefetch(self._batch_i)\n\n        # batch 2\n        self._batch_ip1 = self._copy_batch_to_gpu(dataloader_iter)\n        self._start_sparse_data_dist(self._batch_ip1)\n        self._wait_sparse_data_dist()\n\n        # batch 3\n        self._batch_ip2 = self._copy_batch_to_gpu(dataloader_iter)\n\n    def progress(self, dataloader_iter: Iterator[In]) -> Out:\n        self._fill_pipeline(dataloader_iter)\n\n        if self._model.training:\n            with record_function(\"## zero_grad ##\"):\n                self._optimizer.zero_grad()\n\n        with record_function(\"## wait_for_batch ##\"):\n            _wait_for_batch(cast(In, self._batch_i), self._prefetch_stream)\n\n        self._start_sparse_data_dist(self._batch_ip2)\n\n        self._batch_ip3 = self._copy_batch_to_gpu(dataloader_iter)\n\n        # forward\n        with record_function(\"## forward ##\"):\n            losses, output = cast(Tuple[torch.Tensor, Out], self._model(self._batch_i))\n\n        self._prefetch(self._batch_ip1)\n\n        self._wait_sparse_data_dist()\n\n        if self._model.training:\n            # backward\n            with record_function(\"## backward ##\"):\n                torch.sum(losses, dim=0).backward()\n\n            # update\n            with record_function(\"## optimizer ##\"):\n                self._optimizer.step()\n\n        self._batch_i = self._batch_ip1\n        self._batch_ip1 = self._batch_ip2\n        self._batch_ip2 = self._batch_ip3\n\n        return output\n\n    def _init_pipelined_modules(self, batch: In) -> None:\n        \"\"\"\n        Retrieves the pipelined modules after overriding their forwards, initializes the\n        modules' input dists, and overrides the input dist forwards to support fusing\n        the splits collective in the input dist.\n        \"\"\"\n        if self._pipelined_modules:\n            return\n        self._pipelined_modules, self._model = _rewrite_model(\n            model=self._model,\n            context=self._context,\n            dist_stream=self._data_dist_stream,\n            batch=self._batch_i,\n            apply_jit=self._apply_jit,\n            pipelined_forward=PrefetchPipelinedForward,\n        )\n\n        # initializes input dist, so we can override input dist forwards\n        self._start_sparse_data_dist(self._batch_i)\n        _override_input_dist_forwards(self._pipelined_modules)\n\n    def _prefetch(self, batch: Optional[In]) -> None:\n        \"\"\"\n        Waits for input dist to finish, then prefetches data.\n        \"\"\"\n        if batch is None:\n            return\n        self._context.module_input_post_prefetch.clear()\n        self._context.module_contexts_post_prefetch.clear()\n\n        with record_function(\"## sharded_module_prefetch ##\"):\n            with torch.cuda.stream(self._prefetch_stream):\n                batch.record_stream(torch.cuda.current_stream())\n                for sharded_module in self._pipelined_modules:\n                    forward = sharded_module.forward\n                    assert isinstance(forward, PrefetchPipelinedForward)\n\n                    assert forward._name in self._context.input_dist_tensors_requests\n                    request = self._context.input_dist_tensors_requests[forward._name]\n                    assert isinstance(request, Awaitable)\n                    with record_function(\"## wait_sparse_data_dist ##\"):\n                        # Finish waiting on the dist_stream,\n                        # in case some delayed stream scheduling happens during the wait() call.\n                        with torch.cuda.stream(self._data_dist_stream):\n                            data = request.wait()\n\n                    # Make sure that both result of input_dist and context\n                    # are properly transferred to the current stream.\n                    if self._data_dist_stream is not None:\n                        torch.cuda.current_stream().wait_stream(self._data_dist_stream)\n                        cur_stream = torch.cuda.current_stream()\n\n                        assert isinstance(\n                            data, (torch.Tensor, Multistreamable)\n                        ), f\"{type(data)} must implement Multistreamable interface\"\n                        data.record_stream(cur_stream)\n                        data.record_stream(self._default_stream)\n\n                        ctx = self._context.module_contexts[forward._name]\n                        ctx.record_stream(cur_stream)\n                        ctx.record_stream(self._default_stream)\n\n                    sharded_module.prefetch(\n                        dist_input=data, forward_stream=self._default_stream\n                    )\n                    self._context.module_input_post_prefetch[forward._name] = data\n                    self._context.module_contexts_post_prefetch[\n                        forward._name\n                    ] = self._context.module_contexts[forward._name]",
  "def progress(self, dataloader_iter: Iterator[In]) -> Out:\n        pass",
  "def __init__(\n        self,\n        model: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        device: torch.device,\n    ) -> None:\n        self._model = model\n        self._optimizer = optimizer\n        self._device = device\n        self._memcpy_stream: Optional[torch.cuda.streams.Stream] = (\n            torch.cuda.Stream() if device.type == \"cuda\" else None\n        )\n        self._cur_batch: Optional[In] = None\n        self._connected = False",
  "def _connect(self, dataloader_iter: Iterator[In]) -> None:\n        cur_batch = next(dataloader_iter)\n        self._cur_batch = cur_batch\n        with torch.cuda.stream(self._memcpy_stream):\n            self._cur_batch = _to_device(cur_batch, self._device, non_blocking=True)\n        self._connected = True",
  "def progress(self, dataloader_iter: Iterator[In]) -> Out:\n        if not self._connected:\n            self._connect(dataloader_iter)\n\n        # Fetch next batch\n        with record_function(\"## next_batch ##\"):\n            next_batch = next(dataloader_iter)\n        cur_batch = self._cur_batch\n        assert cur_batch is not None\n\n        if self._model.training:\n            with record_function(\"## zero_grad ##\"):\n                self._optimizer.zero_grad()\n\n        with record_function(\"## wait_for_batch ##\"):\n            _wait_for_batch(cur_batch, self._memcpy_stream)\n\n        with record_function(\"## forward ##\"):\n            (losses, output) = self._model(cur_batch)\n\n        if self._model.training:\n            with record_function(\"## backward ##\"):\n                torch.sum(losses, dim=0).backward()\n\n        # Copy the next batch to GPU\n        self._cur_batch = cur_batch = next_batch\n        with record_function(\"## copy_batch_to_gpu ##\"):\n            with torch.cuda.stream(self._memcpy_stream):\n                self._cur_batch = _to_device(cur_batch, self._device, non_blocking=True)\n\n        # Update\n        if self._model.training:\n            with record_function(\"## optimizer ##\"):\n                self._optimizer.step()\n\n        return output",
  "def __init__(self, leaf_modules: Optional[List[str]] = None) -> None:\n        super().__init__()\n        self._leaf_modules: List[str] = leaf_modules if leaf_modules is not None else []",
  "def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:\n        if (\n            isinstance(m, ShardedModule)\n            or module_qualified_name in self._leaf_modules\n            or isinstance(m, FSDP)\n        ):\n            return True\n        return super().is_leaf_module(m, module_qualified_name)",
  "def __init__(\n        self,\n        input_tensors: List[torch.Tensor],\n        pg: dist.ProcessGroup,\n    ) -> None:\n        super().__init__()\n        self.num_workers: int = pg.size()\n\n        with record_function(\"## all2all_data:kjt splits ##\"):\n            self._output_tensor: torch.Tensor = torch.empty(\n                [self.num_workers * len(input_tensors)],\n                device=input_tensors[0].device,\n                dtype=input_tensors[0].dtype,\n            )\n            input_tensor = torch.stack(input_tensors, dim=1).flatten()\n            self._splits_awaitable: dist.Work = dist.all_to_all_single(\n                output=self._output_tensor,\n                input=input_tensor,\n                group=pg,\n                async_op=True,\n            )",
  "def _wait_impl(self) -> List[List[int]]:\n        self._splits_awaitable.wait()\n        return self._output_tensor.view(self.num_workers, -1).T.tolist()",
  "def __init__(\n        self,\n        requests: List[KJTListSplitsAwaitable[C]],\n        contexts: List[C],\n        pg: Optional[dist.ProcessGroup],\n    ) -> None:\n        super().__init__()\n        self._contexts = contexts\n        self._awaitables: List[\n            Union[KJTSplitsAllToAllMeta, Awaitable[Awaitable[KeyedJaggedTensor]]]\n        ] = [awaitable for request in requests for awaitable in request.awaitables]\n        for req, ctx in zip(requests, self._contexts):\n            _set_sharding_context_pre_a2a(req.awaitables, ctx)\n        self._output_lengths: List[int] = [\n            len(request.awaitables) for request in requests\n        ]\n        self._lengths: List[int] = [\n            len(awaitable.splits_tensors)\n            if isinstance(awaitable, KJTSplitsAllToAllMeta)\n            else 0\n            for awaitable in self._awaitables\n        ]\n        splits_tensors = [\n            splits_tensor\n            for awaitable in self._awaitables\n            for splits_tensor in (\n                awaitable.splits_tensors\n                if isinstance(awaitable, KJTSplitsAllToAllMeta)\n                else []\n            )\n        ]\n        self._splits_awaitable: Optional[SplitsAllToAllAwaitable] = (\n            SplitsAllToAllAwaitable(\n                input_tensors=splits_tensors,\n                pg=pg,\n            )\n            if splits_tensors and pg is not None\n            else None\n        )",
  "def _wait_impl(self) -> List[KJTListAwaitable]:\n        if self._splits_awaitable:\n            splits_list = self._splits_awaitable.wait()\n            splits_per_awaitable = _split(splits_list, self._lengths)\n        else:\n            splits_per_awaitable = [[] for _ in range(len(self._lengths))]\n        tensors_awaitables = []\n        for splits, awaitable in zip(splits_per_awaitable, self._awaitables):\n            if not splits:  # NoWait\n                assert isinstance(awaitable, Awaitable)\n                tensors_awaitables.append(awaitable.wait())\n                continue\n            assert isinstance(awaitable, KJTSplitsAllToAllMeta)\n            if awaitable._input.variable_stride_per_key():\n                output_splits = splits\n                stride_per_rank = None\n            else:\n                output_splits = splits[:-1]\n                stride_per_rank = splits[-1]\n            tensors_awaitables.append(\n                KJTAllToAllTensorsAwaitable(\n                    pg=awaitable.pg,\n                    input=awaitable._input,\n                    splits=awaitable.splits,\n                    input_splits=awaitable.input_splits,\n                    output_splits=output_splits,\n                    input_tensors=awaitable.input_tensors,\n                    labels=awaitable.labels,\n                    keys=awaitable.keys,\n                    device=awaitable.device,\n                    stagger=awaitable.stagger,\n                    stride_per_rank=stride_per_rank,\n                )\n            )\n        output = []\n        awaitables_per_output = _split(tensors_awaitables, self._output_lengths)\n        for awaitables, ctx in zip(awaitables_per_output, self._contexts):\n            _set_sharding_context_intra_a2a(awaitables, ctx)\n            output.append(KJTListAwaitable(awaitables, ctx))\n        return output",
  "def __init__(\n        self,\n        name: str,\n        args: List[ArgInfo],\n        module: ShardedModule,\n        context: TrainPipelineContext,\n        stream: Optional[torch.cuda.streams.Stream],\n    ) -> None:\n        self._name = name\n        self._args = args\n        self._module = module\n        self._context = context\n        self._stream = stream",
  "def name(self) -> str:\n        return self._name",
  "def args(self) -> List[ArgInfo]:\n        return self._args",
  "def __call__(self, *input, **kwargs) -> Awaitable:\n        assert self._name in self._context.input_dist_tensors_requests\n        request = self._context.input_dist_tensors_requests[self._name]\n        assert isinstance(request, Awaitable)\n        with record_function(\"## wait_sparse_data_dist ##\"):\n            # Finish waiting on the dist_stream,\n            # in case some delayed stream scheduling happens during the wait() call.\n            with torch.cuda.stream(self._stream):\n                data = request.wait()\n\n        # Make sure that both result of input_dist and context\n        # are properly transferred to the current stream.\n        if self._stream is not None:\n            torch.cuda.current_stream().wait_stream(self._stream)\n            cur_stream = torch.cuda.current_stream()\n\n            assert isinstance(\n                data, (torch.Tensor, Multistreamable)\n            ), f\"{type(data)} must implement Multistreamable interface\"\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            data.record_stream(cur_stream)\n\n            ctx = self._context.module_contexts[self._name]\n            ctx.record_stream(cur_stream)\n\n        return self._module.compute_and_output_dist(\n            self._context.module_contexts[self._name], data\n        )",
  "def __init__(\n        self,\n        name: str,\n        args: List[ArgInfo],\n        module: ShardedModule,\n        context: PrefetchTrainPipelineContext,\n        prefetch_stream: Optional[torch.cuda.streams.Stream],\n    ) -> None:\n        super().__init__(\n            name=name,\n            args=args,\n            module=module,\n            context=context,\n            stream=prefetch_stream,\n        )\n        self._context: PrefetchTrainPipelineContext = self._context",
  "def __call__(self, *input, **kwargs) -> Awaitable:\n        assert self._name in self._context.module_input_post_prefetch\n        data = self._context.module_input_post_prefetch[self._name]\n\n        # Make sure that both result of input_dist and context\n        # are properly transferred to the current stream.\n        if self._stream is not None:\n            torch.cuda.current_stream().wait_stream(self._stream)\n            cur_stream = torch.cuda.current_stream()\n\n            assert isinstance(\n                data, (torch.Tensor, Multistreamable)\n            ), f\"{type(data)} must implement Multistreamable interface\"\n            data.record_stream(cur_stream)\n\n            ctx = self._context.module_contexts_post_prefetch[self._name]\n            ctx.record_stream(cur_stream)\n\n        return self._module.compute_and_output_dist(\n            self._context.module_contexts_post_prefetch[self._name], data\n        )",
  "def __init__(\n        self, pg: dist.ProcessGroup, splits: List[int], stagger: int = 1\n    ) -> None:\n        self._pg = pg\n        self._splits = splits\n        self._stagger = stagger\n        self._splits_cumsum: List[int] = [0] + list(itertools.accumulate(splits))",
  "def __call__(self, input: KeyedJaggedTensor) -> KJTSplitsAllToAllMeta:\n        with torch.no_grad():\n            assert len(input.keys()) == sum(self._splits)\n            rank = dist.get_rank(self._pg)\n            local_keys = input.keys()[\n                self._splits_cumsum[rank] : self._splits_cumsum[rank + 1]\n            ]\n            input_splits = input.dist_splits(self._splits)\n            device = input.values().device\n            splits_tensors = [\n                torch.tensor(splits, device=device) for splits in input_splits\n            ]\n            if not input.variable_stride_per_key():\n                splits_tensors.append(\n                    torch.tensor([input.stride()] * self._pg.size(), device=device)\n                )\n            return KJTSplitsAllToAllMeta(\n                pg=self._pg,\n                _input=input,\n                splits=self._splits,\n                splits_tensors=splits_tensors,\n                input_splits=input_splits,\n                input_tensors=input.dist_tensors(),\n                labels=input.dist_labels(),\n                keys=local_keys,\n                device=device,\n                stagger=self._stagger,\n            )",
  "def __init__(\n        self,\n        model: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        device: torch.device,\n        execute_all_batches: bool = True,\n        apply_jit: bool = False,\n    ) -> None:\n        self._model = model\n        self._optimizer = optimizer\n        self._device = device\n        self._execute_all_batches = execute_all_batches\n        self._apply_jit = apply_jit\n        # use two data streams to support two concurrent batches\n        if device.type == \"cuda\":\n            self._memcpy_stream: Optional[\n                torch.cuda.streams.Stream\n            ] = torch.cuda.Stream(priority=-1)\n            self._data_dist_stream: Optional[\n                torch.cuda.streams.Stream\n            ] = torch.cuda.Stream(priority=-1)\n        else:\n            self._memcpy_stream: Optional[torch.cuda.streams.Stream] = None\n            self._data_dist_stream: Optional[torch.cuda.streams.Stream] = None\n        self._batch_i: Optional[In] = None\n        self._batch_ip1: Optional[In] = None\n        self._batch_ip2: Optional[In] = None\n        self._context = TrainPipelineContext()\n        self._pipelined_modules: List[ShardedModule] = []",
  "def _fill_pipeline(self, dataloader_iter: Iterator[In]) -> None:\n        # pipeline is already filled\n        if self._batch_i and self._batch_ip1:\n            return\n        # executes last batch in pipeline\n        if self._batch_i and self._execute_all_batches:\n            return\n\n        # batch 1\n        self._batch_i = self._copy_batch_to_gpu(dataloader_iter)\n        if self._batch_i is None:\n            raise StopIteration\n\n        self._init_pipelined_modules(self._batch_i)\n        self._start_sparse_data_dist(self._batch_i)\n        self._wait_sparse_data_dist()\n\n        # batch 2\n        self._batch_ip1 = self._copy_batch_to_gpu(dataloader_iter)",
  "def progress(self, dataloader_iter: Iterator[In]) -> Out:\n        self._fill_pipeline(dataloader_iter)\n\n        if self._model.training:\n            with record_function(\"## zero_grad ##\"):\n                self._optimizer.zero_grad()\n\n        with record_function(\"## wait_for_batch ##\"):\n            _wait_for_batch(cast(In, self._batch_i), self._data_dist_stream)\n\n        self._start_sparse_data_dist(self._batch_ip1)\n\n        self._batch_ip2 = self._copy_batch_to_gpu(dataloader_iter)\n\n        # forward\n        with record_function(\"## forward ##\"):\n            losses, output = cast(Tuple[torch.Tensor, Out], self._model(self._batch_i))\n\n        self._wait_sparse_data_dist()\n\n        if self._model.training:\n            # backward\n            with record_function(\"## backward ##\"):\n                torch.sum(losses, dim=0).backward()\n\n            # update\n            with record_function(\"## optimizer ##\"):\n                self._optimizer.step()\n\n        self._batch_i = self._batch_ip1\n        self._batch_ip1 = self._batch_ip2\n\n        return output",
  "def _init_pipelined_modules(self, batch: In) -> None:\n        \"\"\"\n        Retrieves the pipelined modules after overriding their forwards, initializes the\n        modules' input dists, and overrides the input dist forwards to support fusing\n        the splits collective in the input dist.\n        \"\"\"\n        if self._pipelined_modules:\n            return\n        self._pipelined_modules, self._model = _rewrite_model(\n            model=self._model,\n            context=self._context,\n            dist_stream=self._data_dist_stream,\n            batch=self._batch_i,\n            apply_jit=self._apply_jit,\n        )\n        # initializes input dist, so we can override input dist forwards\n        self._start_sparse_data_dist(self._batch_i)\n        _override_input_dist_forwards(self._pipelined_modules)",
  "def _copy_batch_to_gpu(self, dataloader_iter: Iterator[In]) -> Optional[In]:\n        \"\"\"\n        Retrieves batch from dataloader and moves it to the provided device.\n\n        Raises:\n            StopIteration: if the dataloader iterator is exhausted; unless\n                `self._execute_all_batches=True`, then returns None.\n        \"\"\"\n        with record_function(\"## copy_batch_to_gpu ##\"):\n            with torch.cuda.stream(self._memcpy_stream):\n                batch = next(dataloader_iter, None)\n                if batch is not None:\n                    batch = _to_device(batch, self._device, non_blocking=True)\n                elif not self._execute_all_batches:\n                    raise StopIteration\n                return batch",
  "def _start_sparse_data_dist(self, batch: Optional[In]) -> None:\n        \"\"\"\n        Waits for batch to finish getting copied to GPU, then starts the input dist.\n        \"\"\"\n        if batch is None:\n            return\n        with record_function(\"## start_sparse_data_dist ##\"):\n            with torch.cuda.stream(self._data_dist_stream):\n                _wait_for_batch(batch, self._memcpy_stream)\n                _start_data_dist(self._pipelined_modules, batch, self._context)",
  "def _wait_sparse_data_dist(self) -> None:\n        \"\"\"\n        Waits on the input dist splits requests to get the input dist tensors requests,\n        and populates the context with them.\n        \"\"\"\n        with record_function(\"## wait_sparse_data_dist ##\"):\n            with torch.cuda.stream(self._data_dist_stream):\n                self._context.module_contexts = (\n                    self._context.module_contexts_next_batch.copy()\n                )\n                self._context.input_dist_tensors_requests.clear()\n                for names, awaitable in self._context.fused_splits_awaitables:\n                    for name, request in zip(names, awaitable.wait()):\n                        self._context.input_dist_tensors_requests[name] = request",
  "def __init__(\n        self,\n        model: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        device: torch.device,\n        execute_all_batches: bool = True,\n        apply_jit: bool = False,\n    ) -> None:\n        super().__init__(\n            model=model,\n            optimizer=optimizer,\n            device=device,\n            execute_all_batches=execute_all_batches,\n            apply_jit=apply_jit,\n        )\n        self._context = PrefetchTrainPipelineContext()\n        if self._device.type == \"cuda\":\n            self._prefetch_stream: Optional[\n                torch.cuda.streams.Stream\n            ] = torch.cuda.Stream()\n            self._default_stream: Optional[\n                torch.cuda.streams.Stream\n            ] = torch.cuda.current_stream()\n        else:\n            self._prefetch_stream: Optional[torch.cuda.streams.Stream] = None\n            self._default_stream: Optional[torch.cuda.streams.Stream] = None\n        self._batch_ip3: Optional[In] = None",
  "def _fill_pipeline(self, dataloader_iter: Iterator[In]) -> None:\n        # pipeline is already filled\n        if self._batch_i and self._batch_ip1 and self._batch_ip2:\n            return\n        # executes last batch in pipeline\n        if self._execute_all_batches and (self._batch_i or self._batch_ip1):\n            return\n\n        # batch 1\n        self._batch_i = self._copy_batch_to_gpu(dataloader_iter)\n        if self._batch_i is None:\n            raise StopIteration\n\n        self._init_pipelined_modules(self._batch_i)\n        self._start_sparse_data_dist(self._batch_i)\n        self._wait_sparse_data_dist()\n        self._prefetch(self._batch_i)\n\n        # batch 2\n        self._batch_ip1 = self._copy_batch_to_gpu(dataloader_iter)\n        self._start_sparse_data_dist(self._batch_ip1)\n        self._wait_sparse_data_dist()\n\n        # batch 3\n        self._batch_ip2 = self._copy_batch_to_gpu(dataloader_iter)",
  "def progress(self, dataloader_iter: Iterator[In]) -> Out:\n        self._fill_pipeline(dataloader_iter)\n\n        if self._model.training:\n            with record_function(\"## zero_grad ##\"):\n                self._optimizer.zero_grad()\n\n        with record_function(\"## wait_for_batch ##\"):\n            _wait_for_batch(cast(In, self._batch_i), self._prefetch_stream)\n\n        self._start_sparse_data_dist(self._batch_ip2)\n\n        self._batch_ip3 = self._copy_batch_to_gpu(dataloader_iter)\n\n        # forward\n        with record_function(\"## forward ##\"):\n            losses, output = cast(Tuple[torch.Tensor, Out], self._model(self._batch_i))\n\n        self._prefetch(self._batch_ip1)\n\n        self._wait_sparse_data_dist()\n\n        if self._model.training:\n            # backward\n            with record_function(\"## backward ##\"):\n                torch.sum(losses, dim=0).backward()\n\n            # update\n            with record_function(\"## optimizer ##\"):\n                self._optimizer.step()\n\n        self._batch_i = self._batch_ip1\n        self._batch_ip1 = self._batch_ip2\n        self._batch_ip2 = self._batch_ip3\n\n        return output",
  "def _init_pipelined_modules(self, batch: In) -> None:\n        \"\"\"\n        Retrieves the pipelined modules after overriding their forwards, initializes the\n        modules' input dists, and overrides the input dist forwards to support fusing\n        the splits collective in the input dist.\n        \"\"\"\n        if self._pipelined_modules:\n            return\n        self._pipelined_modules, self._model = _rewrite_model(\n            model=self._model,\n            context=self._context,\n            dist_stream=self._data_dist_stream,\n            batch=self._batch_i,\n            apply_jit=self._apply_jit,\n            pipelined_forward=PrefetchPipelinedForward,\n        )\n\n        # initializes input dist, so we can override input dist forwards\n        self._start_sparse_data_dist(self._batch_i)\n        _override_input_dist_forwards(self._pipelined_modules)",
  "def _prefetch(self, batch: Optional[In]) -> None:\n        \"\"\"\n        Waits for input dist to finish, then prefetches data.\n        \"\"\"\n        if batch is None:\n            return\n        self._context.module_input_post_prefetch.clear()\n        self._context.module_contexts_post_prefetch.clear()\n\n        with record_function(\"## sharded_module_prefetch ##\"):\n            with torch.cuda.stream(self._prefetch_stream):\n                batch.record_stream(torch.cuda.current_stream())\n                for sharded_module in self._pipelined_modules:\n                    forward = sharded_module.forward\n                    assert isinstance(forward, PrefetchPipelinedForward)\n\n                    assert forward._name in self._context.input_dist_tensors_requests\n                    request = self._context.input_dist_tensors_requests[forward._name]\n                    assert isinstance(request, Awaitable)\n                    with record_function(\"## wait_sparse_data_dist ##\"):\n                        # Finish waiting on the dist_stream,\n                        # in case some delayed stream scheduling happens during the wait() call.\n                        with torch.cuda.stream(self._data_dist_stream):\n                            data = request.wait()\n\n                    # Make sure that both result of input_dist and context\n                    # are properly transferred to the current stream.\n                    if self._data_dist_stream is not None:\n                        torch.cuda.current_stream().wait_stream(self._data_dist_stream)\n                        cur_stream = torch.cuda.current_stream()\n\n                        assert isinstance(\n                            data, (torch.Tensor, Multistreamable)\n                        ), f\"{type(data)} must implement Multistreamable interface\"\n                        data.record_stream(cur_stream)\n                        data.record_stream(self._default_stream)\n\n                        ctx = self._context.module_contexts[forward._name]\n                        ctx.record_stream(cur_stream)\n                        ctx.record_stream(self._default_stream)\n\n                    sharded_module.prefetch(\n                        dist_input=data, forward_stream=self._default_stream\n                    )\n                    self._context.module_input_post_prefetch[forward._name] = data\n                    self._context.module_contexts_post_prefetch[\n                        forward._name\n                    ] = self._context.module_contexts[forward._name]",
  "def _tabulate(\n    table: List[List[Union[str, int]]], headers: Optional[List[str]] = None\n) -> str:\n    \"\"\"\n    Format a table as a string.\n    Parameters:\n        table (list of lists or list of tuples): The data to be formatted as a table.\n        headers (list of strings, optional): The column headers for the table. If not provided, the first row of the table will be used as the headers.\n    Returns:\n        str: A string representation of the table.\n    \"\"\"\n    if headers is None:\n        headers = table[0]\n        table = table[1:]\n    headers = cast(List[str], headers)\n    rows = []\n    # Determine the maximum width of each column\n    col_widths = [max([len(str(item)) for item in column]) for column in zip(*table)]\n    col_widths = [max(i, len(j)) for i, j in zip(col_widths, headers)]\n    # Format each row of the table\n    for row in table:\n        row_str = \" | \".join(\n            [str(item).ljust(width) for item, width in zip(row, col_widths)]\n        )\n        rows.append(row_str)\n    # Add the header row and the separator line\n    rows.insert(\n        0,\n        \" | \".join(\n            [header.center(width) for header, width in zip(headers, col_widths)]\n        ),\n    )\n    rows.insert(1, \" | \".join([\"-\" * width for width in col_widths]))\n    return \"\\n\".join(rows)",
  "class BoundsCheckMode(Enum):\n    # Raise an exception (CPU) or device-side assert (CUDA)\n    FATAL = 0\n    # Log the first out-of-bounds instance per kernel, and set to zero.\n    WARNING = 1\n    # Set to zero.\n    IGNORE = 2\n    # No bounds checks.\n    NONE = 3",
  "class CacheAlgorithm(Enum):\n    LRU = 0\n    LFU = 1",
  "class DataType(Enum):\n    \"\"\"\n    Our fusion implementation supports only certain types of data\n    so it makes sense to retrict in a non-fused version as well.\n    \"\"\"\n\n    FP32 = \"FP32\"\n    FP16 = \"FP16\"\n    BF16 = \"BF16\"\n    INT64 = \"INT64\"\n    INT32 = \"INT32\"\n    INT8 = \"INT8\"\n    UINT8 = \"UINT8\"\n    INT4 = \"INT4\"\n    INT2 = \"INT2\"\n\n    def __str__(self) -> str:\n        return self.value",
  "class ShardingType(Enum):\n    \"\"\"\n    Well-known sharding types, used by inter-module optimizations.\n    \"\"\"\n\n    # Replicated on all ranks\n    DATA_PARALLEL = \"data_parallel\"\n    # Placed on a single rank\n    TABLE_WISE = \"table_wise\"\n    # Placed on multiple ranks as different sharded tables\n    COLUMN_WISE = \"column_wise\"\n    # Range-split on the first dimension across all ranks\n    ROW_WISE = \"row_wise\"\n    # Row-wise on the same node and table-wise across nodes\n    # Useful when having multiple ranks per node\n    # and comms within a single node are more efficient than across nodes.\n    TABLE_ROW_WISE = \"table_row_wise\"\n    # Column-wise on the same node and table-wise across nodes\n    TABLE_COLUMN_WISE = \"table_column_wise\"",
  "class ParameterStorage(Enum):\n    \"\"\"\n    Well-known physical resources, which can be used as constraints by ShardingPlanner.\n    \"\"\"\n\n    # GPU-attached memory\n    HBM = \"hbm\"\n    # CPU-attached memory\n    DDR = \"ddr\"",
  "class ComputeKernel(Enum):\n    DEFAULT = \"default\"",
  "class QuantizedCommCodec(Generic[QuantizationContext]):\n    \"\"\"\n    Provide an implementation to quantized, or apply mixed precision, to the tensors used in collective calls (pooled_all_to_all, reduce_scatter, etc).\n    The dtype is the dtype of the tensor called from encode.\n\n    This makes the assumption that the input tensor has type torch.float32\n\n    >>>\n        quantized_tensor = quantized_comm_codec.encode(input_tensor)\n        quantized_tensor.dtype == quantized_comm_codec.quantized_dtype\n        collective_call(output_tensors, input_tensors=tensor)\n        output_tensor = decode(output_tensors)\n\n        torch.assert_close(input_tensors, output_tensor)\n\n    \"\"\"\n\n    def encode(\n        self, input_tensor: torch.Tensor, ctx: Optional[QuantizationContext] = None\n    ) -> torch.Tensor:\n        ...\n\n    def decode(\n        self, input_grad: torch.Tensor, ctx: Optional[QuantizationContext] = None\n    ) -> torch.Tensor:\n        ...\n\n    @property\n    def quantized_dtype(self) -> torch.dtype:\n        \"\"\"\n        tensor.dtype of the resultant encode(input_tensor)\n        \"\"\"\n        ...\n\n    def calc_quantized_size(\n        self,\n        input_len: int,\n        ctx: Optional[QuantizationContext] = None,\n    ) -> int:\n        \"\"\"\n        Given the length of input tensor, returns the length of tensor after\n        quantization. Used by INT8 codecs where the quantized tensor have\n        some additional parameters. For other cases, the quantized tensor should\n        have the same length with input.\n        \"\"\"\n        ...\n\n    def create_context(self) -> Optional[QuantizationContext]:\n        \"\"\"\n        Create a context object that can be used to carry session-based\n        parameters between encoder and decoder.\n        \"\"\"\n        ...",
  "class NoOpQuantizedCommCodec(Generic[QuantizationContext]):\n    \"\"\"\n    Default No-Op implementation of QuantizedCommCodec\n    \"\"\"\n\n    def encode(\n        self,\n        input_tensor: torch.Tensor,\n        ctx: Optional[QuantizationContext] = None,\n    ) -> torch.Tensor:\n        return input_tensor\n\n    def decode(\n        self,\n        input_grad: torch.Tensor,\n        ctx: Optional[QuantizationContext] = None,\n    ) -> torch.Tensor:\n        return input_grad\n\n    def quantized_dtype(self) -> torch.dtype:\n        return torch.float\n\n    def calc_quantized_size(\n        self,\n        input_len: int,\n        ctx: Optional[QuantizationContext] = None,\n    ) -> int:\n        return input_len\n\n    def create_context(self) -> Optional[QuantizationContext]:\n        return None",
  "class QuantizedCommCodecs:\n    \"\"\"\n    The quantization codecs to use for the forward and backward pass respectively of a comm op (e.g. pooled_all_to_all, reduce_scatter, sequence_all_to_all).\n    \"\"\"\n\n    # pyre-ignore\n    forward: QuantizedCommCodec = NoOpQuantizedCommCodec()\n    # pyre-ignore\n    backward: QuantizedCommCodec = NoOpQuantizedCommCodec()",
  "class CommOp(Enum):\n    # For detailed descriptions of each of these, see their doc strings in dist_data.\n    # These are commonly used inside of a QuantizedCommsRegistry\n    POOLED_EMBEDDINGS_ALL_TO_ALL = \"pooled_embeddings_all_to_all\"\n    POOLED_EMBEDDINGS_REDUCE_SCATTER = \"pooled_embeddings_reduce_scatter\"\n    SEQUENCE_EMBEDDINGS_ALL_TO_ALL = \"sequence_embeddings_all_to_all\"",
  "class Awaitable(abc.ABC, Generic[W]):\n    def __init__(self) -> None:\n        self._callbacks: List[Callable[[W], W]] = []\n\n    @abc.abstractmethod\n    def _wait_impl(self) -> W:\n        pass\n\n    def wait(self) -> W:\n        with record_function(f\"## {self.__class__.__name__} wait() ##\"):\n            ret: W = self._wait_impl()\n            for callback in self.callbacks:\n                ret = callback(ret)\n        return ret\n\n    @property\n    def callbacks(self) -> List[Callable[[W], W]]:\n        return self._callbacks",
  "class NoWait(Awaitable[W]):\n    def __init__(self, obj: W) -> None:\n        super().__init__()\n        self._obj = obj\n\n    def _wait_impl(self) -> W:\n        return self._obj",
  "class _LazyAwaitableMeta(\n    GenericMeta, abc.ABCMeta, torch.fx._symbolic_trace.ProxyableClassMeta\n):\n\n    \"\"\"\n    The _LazyAwaitableMeta class that inherits both ABCMeta and ProxyableClassMeta\n    This is because ABCMeta/ProxyableClassMeta are both non-trival metaclasses\n    Declaring this separately to ensure the init order of metaclasses\n\n    XXX: Generics are non-trival metaclass before python 3.7 but are removed\n    afterwards. we add GenericsMeta here to support version before 3.7.\n    \"\"\"\n\n    pass",
  "class LazyAwaitable(Awaitable[W], metaclass=_LazyAwaitableMeta):\n    \"\"\"\n    The LazyAwaitable type which exposes a `wait()` API, concrete types\n    can control how to initialize and how the `wait()` behavior should\n    be in order to achieve specific async operation.\n\n    This base LazyAwaitable type is a \"lazy\" async type, which means it will\n    delay `wait()` as late as possible, see details in `__torch_function__`\n    below. This could help the model automatically enable computation and\n    communication overlap, model author doesn't need to manually call\n    `wait()` if the results is used by a pytorch function, or by other python\n    operations (NOTE: need to implement corresponding magic methods\n    like __getattr__ below)\n\n    Some caveats:\n\n    * This works with Pytorch functions, but not any generic method, if\n      you would like to do arbitary python operations, you need to\n      implement the corresponding magic methods\n\n    * In the case that one function have two or more arguments are LazyAwaitable,\n      the lazy wait mechanism can't ensure perfect computation/communication\n      overlap (i.e. quickly waited the first one but long wait on the second)\n    \"\"\"\n\n    def __init__(\n        self,\n    ) -> None:\n        super().__init__()\n        # _result is used to cache the results after the wait() is called.\n        self._result: Optional[W] = None\n\n    @staticmethod\n    # pyre-ignore [2, 3]\n    def _wait_async(obj: Any) -> Any:\n        \"\"\"\n        This method is used internally to automatically wait when necessary\n        and cache the results of the `LazyAwaitable.wait()` call\n        \"\"\"\n        if isinstance(obj, LazyAwaitable):\n            if obj._result is None:\n                obj._result = obj.wait()\n            return obj._result\n        else:\n            return obj\n\n    # pyre-ignore [2, 3]\n    def __torch_function__(self, func, types, args=(), kwargs=None):\n        \"\"\"\n        The LazyAwaitable type has a `__torch_function__` implementation.\n        This means when this type is seens as an argument to a PyTorch\n        function in a position where it expects a W, the PyTorch's\n        dispatcher will call into this function for special handling\n\n        Our `__torch_function__` implementation goes through all of the\n        args and kwargs and checks if any of them are `LazyAwaitable`.\n        If it is, it will call `wait()` on it and replace the LazyAwaitable\n        type object with the result of wait. In this way, async values\n        are waited on when the concrete value is first needed and without\n        the user having to write an explicit `wait()` call.\n        \"\"\"\n        kwargs = kwargs or {}\n\n        # wait() on all LazyAwaitable args/kwargs and replace\n        # them with the resulting value.\n        new_args = torch.fx.node.map_aggregate(args, LazyAwaitable._wait_async)\n        new_kwargs = torch.fx.node.map_aggregate(kwargs, LazyAwaitable._wait_async)\n\n        return func(*new_args, **new_kwargs)\n\n    # pyre-ignore [2, 3]\n    def __getattr__(self, name):\n        \"\"\"\n        Overrides __getattr__ to allow LazyAwaitable to first wait and then call getattr\n        on the wait results.\n        \"\"\"\n        if name == \"_result\":\n            raise RuntimeError(\n                f\"LazyAwaitable type {type(self)} has not been initialized properly, \"\n                f\"did you forget to call 'super()'?\"\n            )\n\n        res = LazyAwaitable._wait_async(self)\n        return getattr(res, name)",
  "class LazyNoWait(LazyAwaitable[W]):\n    def __init__(self, obj: W) -> None:\n        super().__init__()\n        self._obj = obj\n\n    def _wait_impl(self) -> W:\n        return self._obj",
  "class ModuleShardingPlan:\n    pass",
  "class CacheParams:\n    algorithm: Optional[CacheAlgorithm] = None\n    load_factor: Optional[float] = None\n    reserved_memory: Optional[float] = None\n    precision: Optional[DataType] = None",
  "class ParameterSharding:\n    \"\"\"\n        Describes the sharding of the parameter.\n\n        sharding_type (str): how this parameter is sharded. See ShardingType for well-known\n            types.\n        compute_kernel (str): compute kernel to be used by this parameter.\n        ranks (Optional[List[int]]): rank of each shard.\n        sharding_spec (Optional[ShardingSpec]): list of ShardMetadata for each shard.\n        cache_params (Optional[CacheParams]): cache params for embedding lookup.\n        enforce_hbm (Optional[bool]): whether to use HBM.\n        stochastic_rounding (Optional[bool]): whether to use stochastic rounding.\n        bounds_check_mode (Optional[BoundsCheckMode]): bounds check mode.\n\n    NOTE:\n      ShardingType.TABLE_WISE - rank where this embedding is placed\n      ShardingType.COLUMN_WISE - rank where the embedding shards are placed, seen as\n      individual tables\n      ShardingType.TABLE_ROW_WISE  - first rank when this embedding is placed\n      ShardingType.ROW_WISE, ShardingType.DATA_PARALLEL - unused\n\n    \"\"\"\n\n    sharding_type: str\n    compute_kernel: str\n    ranks: Optional[List[int]] = None\n    sharding_spec: Optional[ShardingSpec] = None\n    cache_params: Optional[CacheParams] = None\n    enforce_hbm: Optional[bool] = None\n    stochastic_rounding: Optional[bool] = None\n    bounds_check_mode: Optional[BoundsCheckMode] = None",
  "class EmbeddingModuleShardingPlan(ModuleShardingPlan, Dict[str, ParameterSharding]):\n    \"\"\"\n    Map of ParameterSharding per parameter (usually a table). This describes the sharding plan for a torchrec module (e.g. `EmbeddingBagCollection`)\n    \"\"\"\n\n    def __str__(self) -> str:\n        out = \"\"\n        param_table = []\n        shard_table = []\n        for param_name, param_sharding in self.items():\n            param_table.append(\n                [\n                    param_name,\n                    param_sharding.sharding_type,\n                    param_sharding.compute_kernel,\n                    param_sharding.ranks,\n                ]\n            )\n            if isinstance(param_sharding.sharding_spec, EnumerableShardingSpec):\n                shards = param_sharding.sharding_spec.shards\n                if shards is not None:\n                    for shard in shards:\n                        shard_table.append(\n                            [\n                                param_name,\n                                shard.shard_offsets,\n                                shard.shard_sizes,\n                                shard.placement,\n                            ]\n                        )\n        out += \"\\n\\n\" + _tabulate(\n            param_table, [\"param\", \"sharding type\", \"compute kernel\", \"ranks\"]\n        )\n        out += \"\\n\\n\" + _tabulate(\n            shard_table, [\"param\", \"shard offsets\", \"shard sizes\", \"placement\"]\n        )\n        return out",
  "class ShardingPlan:\n    \"\"\"\n    Representation of sharding plan. This uses the FQN of the larger wrapped model (i.e the model that is wrapped using `DistributedModelParallel`)\n    EmbeddingModuleShardingPlan should be used when TorchRec composability is desired.\n\n    Attributes:\n        plan (Dict[str, EmbeddingModuleShardingPlan]): dict keyed by module path of\n            dict of parameter sharding specs keyed by parameter name.\n    \"\"\"\n\n    plan: Dict[str, ModuleShardingPlan]\n\n    def get_plan_for_module(self, module_path: str) -> Optional[ModuleShardingPlan]:\n        \"\"\"\n        Args:\n            module_path (str):\n\n        Returns:\n            Optional[ModuleShardingPlan]: dict of parameter sharding specs keyed by parameter name. None if sharding specs do not exist for given module_path.\n        \"\"\"\n        return self.plan.get(module_path, None)\n\n    def __str__(self) -> str:\n        out = \"\"\n        for i, (module_path, module_plan) in enumerate(self.plan.items()):\n            if i > 0:\n                out += \"\\n\\n\"\n            out += \"module: \" + module_path\n            out += str(module_plan)\n        return out",
  "class NullShardedModuleContext(Multistreamable):\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        pass\n\n    # pyre-ignore [2]\n    def __setattr__(self, key: str, value: Any) -> None:\n        raise NotImplementedError()",
  "class ShardingEnv:\n    \"\"\"\n    Provides an abstraction over `torch.distributed.ProcessGroup`, which practically\n    enables `DistributedModelParallel` to be used during inference.\n    \"\"\"\n\n    def __init__(\n        self,\n        world_size: int,\n        rank: int,\n        pg: Optional[dist.ProcessGroup] = None,\n    ) -> None:\n        self.world_size = world_size\n        self.rank = rank\n        self.process_group: Optional[dist.ProcessGroup] = pg\n\n    @classmethod\n    def from_process_group(cls, pg: dist.ProcessGroup) -> \"ShardingEnv\":\n        \"\"\"\n        Creates ProcessGroup-based sharding environment.\n\n        NOTE:\n            Typically used during training.\n        \"\"\"\n        return cls(dist.get_world_size(pg), dist.get_rank(pg), pg)\n\n    @classmethod\n    def from_local(cls, world_size: int, rank: int) -> \"ShardingEnv\":\n        \"\"\"\n        Creates a local host-based sharding environment.\n\n        NOTE:\n            Typically used during single host inference.\n        \"\"\"\n        return cls(world_size, rank, None)",
  "class NullShardingContext(Multistreamable):\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        pass",
  "class ShardedModule(\n    abc.ABC,\n    nn.Module,\n    Generic[CompIn, DistOut, Out, ShrdCtx],\n    ModuleNoCopyMixin,\n):\n    \"\"\"\n    All model-parallel modules implement this interface.\n    Inputs and outputs are data-parallel.\n\n    Args::\n        qcomm_codecs_registry (Optional[Dict[str, QuantizedCommCodecs]]) : Mapping of CommOp name to QuantizedCommCodecs\n\n    NOTE:\n        'input_dist' / 'output_dist' are responsible of transforming inputs / outputs\n        from data-parallel to model parallel and vise-versa.\n    \"\"\"\n\n    @abc.abstractmethod\n    def __init__(\n        self, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None\n    ) -> None:\n\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.distributed.{self.__class__.__name__}\")\n\n        if qcomm_codecs_registry is None:\n            qcomm_codecs_registry = {}\n        self._qcomm_codecs_registry = qcomm_codecs_registry\n\n    @abc.abstractmethod\n    def create_context(self) -> ShrdCtx:\n        pass\n\n    @property\n    def qcomm_codecs_registry(self) -> Optional[Dict[str, QuantizedCommCodecs]]:\n        return self._qcomm_codecs_registry\n\n    @abc.abstractmethod\n    def input_dist(\n        self,\n        ctx: ShrdCtx,\n        # pyre-ignore[2]\n        *input,\n        # pyre-ignore[2]\n        **kwargs,\n    ) -> Awaitable[Awaitable[CompIn]]:\n        pass\n\n    @abc.abstractmethod\n    def compute(self, ctx: ShrdCtx, dist_input: CompIn) -> DistOut:\n        pass\n\n    @abc.abstractmethod\n    def output_dist(self, ctx: ShrdCtx, output: DistOut) -> LazyAwaitable[Out]:\n        pass\n\n    def compute_and_output_dist(\n        self, ctx: ShrdCtx, input: CompIn\n    ) -> LazyAwaitable[Out]:\n        \"\"\"\n        In case of multiple output distributions it makes sense to override this method\n        and initiate the output distibution as soon as the corresponding compute\n        completes.\n        \"\"\"\n        output = self.compute(ctx, input)\n        return self.output_dist(ctx, output)\n\n    # pyre-ignore[2]\n    def forward(self, *input, **kwargs) -> LazyAwaitable[Out]:\n        \"\"\"\n        Executes the input dist, compute, and output dist steps.\n\n        Args:\n            *input: input.\n            **kwargs: keyword arguments.\n\n        Returns:\n            LazyAwaitable[Out]: awaitable of output from output dist.\n        \"\"\"\n        ctx = self.create_context()\n        dist_input = self.input_dist(ctx, *input, **kwargs).wait().wait()\n        return self.compute_and_output_dist(ctx, dist_input)\n\n    def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        for key, _ in self.named_parameters(prefix):\n            yield key",
  "class ShardableModule(abc.ABC, nn.Module):\n    @abc.abstractmethod\n    # pyre-ignore [3]\n    def shard(\n        self,\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedModule[Any, Any, Any, Any]:\n        ...\n\n    def _initialize_torch_state(self) -> None:\n        \"\"\"\n        This provides consistency between this class and the ShardedModule's\n        nn.Module API calls (state_dict, named_modules, etc)\n        \"\"\"\n        pass",
  "class ModuleSharder(abc.ABC, Generic[M]):\n    \"\"\"\n    `ModuleSharder` is per each module, which supports sharding,\n    e.g. `EmbeddingBagCollection`.\n\n    Args::\n        qcomm_codecs_registry (Optional[Dict[str, QuantizedCommCodecs]]) : Mapping of CommOp name to QuantizedCommCodecs\n    \"\"\"\n\n    def __init__(\n        self, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None\n    ) -> None:\n        torch._C._log_api_usage_once(f\"torchrec.distributed.{self.__class__.__name__}\")\n        self._qcomm_codecs_registry = qcomm_codecs_registry\n\n    @abc.abstractclassmethod\n    # pyre-ignore [3]\n    def shard(\n        self,\n        module: M,\n        params: EmbeddingModuleShardingPlan,\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedModule[Any, Any, Any, Any]:\n        \"\"\"\n        Does the actual sharding. It will allocate parameters on the requested locations\n        as specified by corresponding ParameterSharding.\n\n        Default implementation is data-parallel replication.\n\n        Args:\n            module (M): module to shard.\n            params (EmbeddingModuleShardingPlan): dict of fully qualified parameter names\n                (module path + parameter name, '.'-separated) to its sharding spec.\n            env (ShardingEnv): sharding environment that has the process group.\n            device (torch.device): compute device.\n\n        Returns:\n            ShardedModule[Any, Any, Any]: sharded module implementation.\n        \"\"\"\n        ...\n\n    @property\n    @abc.abstractmethod\n    def module_type(self) -> Type[M]:\n        ...\n\n    @property\n    def qcomm_codecs_registry(self) -> Optional[Dict[str, QuantizedCommCodecs]]:\n        return self._qcomm_codecs_registry\n\n    def shardable_parameters(self, module: M) -> Dict[str, nn.Parameter]:\n        \"\"\"\n        List of parameters that can be sharded.\n        \"\"\"\n        return dict(module.named_parameters())\n\n    def sharding_types(self, compute_device_type: str) -> List[str]:\n        \"\"\"\n        List of supported sharding types. See `ShardingType` for well-known examples.\n        \"\"\"\n        return [ShardingType.DATA_PARALLEL.value]\n\n    def compute_kernels(\n        self, sharding_type: str, compute_device_type: str\n    ) -> List[str]:\n        \"\"\"\n        List of supported compute kernels for a given sharding type and compute device.\n        \"\"\"\n\n        return [ComputeKernel.DEFAULT.value]\n\n    def storage_usage(\n        self, tensor: torch.Tensor, compute_device_type: str, compute_kernel: str\n    ) -> Dict[str, int]:\n        \"\"\"\n        List of system resources and corresponding usage given a compute device and\n        compute kernel.\n        \"\"\"\n\n        assert compute_device_type in {\"cuda\", \"cpu\"}\n        storage_map = {\"cuda\": ParameterStorage.HBM, \"cpu\": ParameterStorage.DDR}\n        return {\n            storage_map[compute_device_type].value: tensor.element_size()\n            * tensor.nelement()\n        }",
  "class ShardingPlanner(abc.ABC):\n    \"\"\"\n    Plans sharding.\n    This plan can be saved and re-used to ensure sharding stability.\n    \"\"\"\n\n    @abc.abstractmethod\n    def plan(\n        self,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n    ) -> ShardingPlan:\n        \"\"\"\n        Plans sharding for provided module and given sharders.\n\n        Args:\n            module (nn.Module): module that sharding is planned for.\n            sharders (List[ModuleSharder[nn.Module]]): provided sharders for module.\n\n        Returns:\n            ShardingPlan: the computed sharding plan.\n        \"\"\"\n        ...\n\n    @abc.abstractmethod\n    def collective_plan(\n        self,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n    ) -> ShardingPlan:\n        \"\"\"\n        Calls self.plan(...) on rank 0 and broadcasts.\n\n        Args:\n            module (nn.Module): module that sharding is planned for.\n            sharders (List[ModuleSharder[nn.Module]]): provided sharders for module.\n\n        Returns:\n            ShardingPlan: the computed sharding plan.\n        \"\"\"\n        ...",
  "def __str__(self) -> str:\n        return self.value",
  "def encode(\n        self, input_tensor: torch.Tensor, ctx: Optional[QuantizationContext] = None\n    ) -> torch.Tensor:\n        ...",
  "def decode(\n        self, input_grad: torch.Tensor, ctx: Optional[QuantizationContext] = None\n    ) -> torch.Tensor:\n        ...",
  "def quantized_dtype(self) -> torch.dtype:\n        \"\"\"\n        tensor.dtype of the resultant encode(input_tensor)\n        \"\"\"\n        ...",
  "def calc_quantized_size(\n        self,\n        input_len: int,\n        ctx: Optional[QuantizationContext] = None,\n    ) -> int:\n        \"\"\"\n        Given the length of input tensor, returns the length of tensor after\n        quantization. Used by INT8 codecs where the quantized tensor have\n        some additional parameters. For other cases, the quantized tensor should\n        have the same length with input.\n        \"\"\"\n        ...",
  "def create_context(self) -> Optional[QuantizationContext]:\n        \"\"\"\n        Create a context object that can be used to carry session-based\n        parameters between encoder and decoder.\n        \"\"\"\n        ...",
  "def encode(\n        self,\n        input_tensor: torch.Tensor,\n        ctx: Optional[QuantizationContext] = None,\n    ) -> torch.Tensor:\n        return input_tensor",
  "def decode(\n        self,\n        input_grad: torch.Tensor,\n        ctx: Optional[QuantizationContext] = None,\n    ) -> torch.Tensor:\n        return input_grad",
  "def quantized_dtype(self) -> torch.dtype:\n        return torch.float",
  "def calc_quantized_size(\n        self,\n        input_len: int,\n        ctx: Optional[QuantizationContext] = None,\n    ) -> int:\n        return input_len",
  "def create_context(self) -> Optional[QuantizationContext]:\n        return None",
  "def __init__(self) -> None:\n        self._callbacks: List[Callable[[W], W]] = []",
  "def _wait_impl(self) -> W:\n        pass",
  "def wait(self) -> W:\n        with record_function(f\"## {self.__class__.__name__} wait() ##\"):\n            ret: W = self._wait_impl()\n            for callback in self.callbacks:\n                ret = callback(ret)\n        return ret",
  "def callbacks(self) -> List[Callable[[W], W]]:\n        return self._callbacks",
  "def __init__(self, obj: W) -> None:\n        super().__init__()\n        self._obj = obj",
  "def _wait_impl(self) -> W:\n        return self._obj",
  "def __init__(\n        self,\n    ) -> None:\n        super().__init__()\n        # _result is used to cache the results after the wait() is called.\n        self._result: Optional[W] = None",
  "def _wait_async(obj: Any) -> Any:\n        \"\"\"\n        This method is used internally to automatically wait when necessary\n        and cache the results of the `LazyAwaitable.wait()` call\n        \"\"\"\n        if isinstance(obj, LazyAwaitable):\n            if obj._result is None:\n                obj._result = obj.wait()\n            return obj._result\n        else:\n            return obj",
  "def __torch_function__(self, func, types, args=(), kwargs=None):\n        \"\"\"\n        The LazyAwaitable type has a `__torch_function__` implementation.\n        This means when this type is seens as an argument to a PyTorch\n        function in a position where it expects a W, the PyTorch's\n        dispatcher will call into this function for special handling\n\n        Our `__torch_function__` implementation goes through all of the\n        args and kwargs and checks if any of them are `LazyAwaitable`.\n        If it is, it will call `wait()` on it and replace the LazyAwaitable\n        type object with the result of wait. In this way, async values\n        are waited on when the concrete value is first needed and without\n        the user having to write an explicit `wait()` call.\n        \"\"\"\n        kwargs = kwargs or {}\n\n        # wait() on all LazyAwaitable args/kwargs and replace\n        # them with the resulting value.\n        new_args = torch.fx.node.map_aggregate(args, LazyAwaitable._wait_async)\n        new_kwargs = torch.fx.node.map_aggregate(kwargs, LazyAwaitable._wait_async)\n\n        return func(*new_args, **new_kwargs)",
  "def __getattr__(self, name):\n        \"\"\"\n        Overrides __getattr__ to allow LazyAwaitable to first wait and then call getattr\n        on the wait results.\n        \"\"\"\n        if name == \"_result\":\n            raise RuntimeError(\n                f\"LazyAwaitable type {type(self)} has not been initialized properly, \"\n                f\"did you forget to call 'super()'?\"\n            )\n\n        res = LazyAwaitable._wait_async(self)\n        return getattr(res, name)",
  "def __init__(self, obj: W) -> None:\n        super().__init__()\n        self._obj = obj",
  "def _wait_impl(self) -> W:\n        return self._obj",
  "def scope(method):\n        def impl(*args, **kwargs):\n            lhs = args[0]\n            op_fn = getattr(operator, method)\n            if len(args) == 1:\n                return op_fn(LazyAwaitable._wait_async(lhs))\n            elif len(args) == 2:\n                rhs = args[1]\n                return op_fn(\n                    LazyAwaitable._wait_async(lhs), LazyAwaitable._wait_async(rhs)\n                )\n            else:\n                raise RuntimeError(f\"magic method {as_magic} not supported!\")\n\n        impl.__name__ = as_magic\n        setattr(LazyAwaitable, as_magic, impl)",
  "def scope(method):\n        # pyre-ignore [2, 3, 53]\n        def impl(self, rhs):\n            op_fn = getattr(operator, method)\n            return op_fn(\n                LazyAwaitable._wait_async(rhs), LazyAwaitable._wait_async(self)\n            )\n\n        impl.__name__ = as_magic\n        impl.__qualname__ = as_magic\n        setattr(LazyAwaitable, as_magic, impl)",
  "def __str__(self) -> str:\n        out = \"\"\n        param_table = []\n        shard_table = []\n        for param_name, param_sharding in self.items():\n            param_table.append(\n                [\n                    param_name,\n                    param_sharding.sharding_type,\n                    param_sharding.compute_kernel,\n                    param_sharding.ranks,\n                ]\n            )\n            if isinstance(param_sharding.sharding_spec, EnumerableShardingSpec):\n                shards = param_sharding.sharding_spec.shards\n                if shards is not None:\n                    for shard in shards:\n                        shard_table.append(\n                            [\n                                param_name,\n                                shard.shard_offsets,\n                                shard.shard_sizes,\n                                shard.placement,\n                            ]\n                        )\n        out += \"\\n\\n\" + _tabulate(\n            param_table, [\"param\", \"sharding type\", \"compute kernel\", \"ranks\"]\n        )\n        out += \"\\n\\n\" + _tabulate(\n            shard_table, [\"param\", \"shard offsets\", \"shard sizes\", \"placement\"]\n        )\n        return out",
  "def get_plan_for_module(self, module_path: str) -> Optional[ModuleShardingPlan]:\n        \"\"\"\n        Args:\n            module_path (str):\n\n        Returns:\n            Optional[ModuleShardingPlan]: dict of parameter sharding specs keyed by parameter name. None if sharding specs do not exist for given module_path.\n        \"\"\"\n        return self.plan.get(module_path, None)",
  "def __str__(self) -> str:\n        out = \"\"\n        for i, (module_path, module_plan) in enumerate(self.plan.items()):\n            if i > 0:\n                out += \"\\n\\n\"\n            out += \"module: \" + module_path\n            out += str(module_plan)\n        return out",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        pass",
  "def __setattr__(self, key: str, value: Any) -> None:\n        raise NotImplementedError()",
  "def __init__(\n        self,\n        world_size: int,\n        rank: int,\n        pg: Optional[dist.ProcessGroup] = None,\n    ) -> None:\n        self.world_size = world_size\n        self.rank = rank\n        self.process_group: Optional[dist.ProcessGroup] = pg",
  "def from_process_group(cls, pg: dist.ProcessGroup) -> \"ShardingEnv\":\n        \"\"\"\n        Creates ProcessGroup-based sharding environment.\n\n        NOTE:\n            Typically used during training.\n        \"\"\"\n        return cls(dist.get_world_size(pg), dist.get_rank(pg), pg)",
  "def from_local(cls, world_size: int, rank: int) -> \"ShardingEnv\":\n        \"\"\"\n        Creates a local host-based sharding environment.\n\n        NOTE:\n            Typically used during single host inference.\n        \"\"\"\n        return cls(world_size, rank, None)",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        pass",
  "def __init__(\n        self, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None\n    ) -> None:\n\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.distributed.{self.__class__.__name__}\")\n\n        if qcomm_codecs_registry is None:\n            qcomm_codecs_registry = {}\n        self._qcomm_codecs_registry = qcomm_codecs_registry",
  "def create_context(self) -> ShrdCtx:\n        pass",
  "def qcomm_codecs_registry(self) -> Optional[Dict[str, QuantizedCommCodecs]]:\n        return self._qcomm_codecs_registry",
  "def input_dist(\n        self,\n        ctx: ShrdCtx,\n        # pyre-ignore[2]\n        *input,\n        # pyre-ignore[2]\n        **kwargs,\n    ) -> Awaitable[Awaitable[CompIn]]:\n        pass",
  "def compute(self, ctx: ShrdCtx, dist_input: CompIn) -> DistOut:\n        pass",
  "def output_dist(self, ctx: ShrdCtx, output: DistOut) -> LazyAwaitable[Out]:\n        pass",
  "def compute_and_output_dist(\n        self, ctx: ShrdCtx, input: CompIn\n    ) -> LazyAwaitable[Out]:\n        \"\"\"\n        In case of multiple output distributions it makes sense to override this method\n        and initiate the output distibution as soon as the corresponding compute\n        completes.\n        \"\"\"\n        output = self.compute(ctx, input)\n        return self.output_dist(ctx, output)",
  "def forward(self, *input, **kwargs) -> LazyAwaitable[Out]:\n        \"\"\"\n        Executes the input dist, compute, and output dist steps.\n\n        Args:\n            *input: input.\n            **kwargs: keyword arguments.\n\n        Returns:\n            LazyAwaitable[Out]: awaitable of output from output dist.\n        \"\"\"\n        ctx = self.create_context()\n        dist_input = self.input_dist(ctx, *input, **kwargs).wait().wait()\n        return self.compute_and_output_dist(ctx, dist_input)",
  "def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        for key, _ in self.named_parameters(prefix):\n            yield key",
  "def shard(\n        self,\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedModule[Any, Any, Any, Any]:\n        ...",
  "def _initialize_torch_state(self) -> None:\n        \"\"\"\n        This provides consistency between this class and the ShardedModule's\n        nn.Module API calls (state_dict, named_modules, etc)\n        \"\"\"\n        pass",
  "def __init__(\n        self, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None\n    ) -> None:\n        torch._C._log_api_usage_once(f\"torchrec.distributed.{self.__class__.__name__}\")\n        self._qcomm_codecs_registry = qcomm_codecs_registry",
  "def shard(\n        self,\n        module: M,\n        params: EmbeddingModuleShardingPlan,\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedModule[Any, Any, Any, Any]:\n        \"\"\"\n        Does the actual sharding. It will allocate parameters on the requested locations\n        as specified by corresponding ParameterSharding.\n\n        Default implementation is data-parallel replication.\n\n        Args:\n            module (M): module to shard.\n            params (EmbeddingModuleShardingPlan): dict of fully qualified parameter names\n                (module path + parameter name, '.'-separated) to its sharding spec.\n            env (ShardingEnv): sharding environment that has the process group.\n            device (torch.device): compute device.\n\n        Returns:\n            ShardedModule[Any, Any, Any]: sharded module implementation.\n        \"\"\"\n        ...",
  "def module_type(self) -> Type[M]:\n        ...",
  "def qcomm_codecs_registry(self) -> Optional[Dict[str, QuantizedCommCodecs]]:\n        return self._qcomm_codecs_registry",
  "def shardable_parameters(self, module: M) -> Dict[str, nn.Parameter]:\n        \"\"\"\n        List of parameters that can be sharded.\n        \"\"\"\n        return dict(module.named_parameters())",
  "def sharding_types(self, compute_device_type: str) -> List[str]:\n        \"\"\"\n        List of supported sharding types. See `ShardingType` for well-known examples.\n        \"\"\"\n        return [ShardingType.DATA_PARALLEL.value]",
  "def compute_kernels(\n        self, sharding_type: str, compute_device_type: str\n    ) -> List[str]:\n        \"\"\"\n        List of supported compute kernels for a given sharding type and compute device.\n        \"\"\"\n\n        return [ComputeKernel.DEFAULT.value]",
  "def storage_usage(\n        self, tensor: torch.Tensor, compute_device_type: str, compute_kernel: str\n    ) -> Dict[str, int]:\n        \"\"\"\n        List of system resources and corresponding usage given a compute device and\n        compute kernel.\n        \"\"\"\n\n        assert compute_device_type in {\"cuda\", \"cpu\"}\n        storage_map = {\"cuda\": ParameterStorage.HBM, \"cpu\": ParameterStorage.DDR}\n        return {\n            storage_map[compute_device_type].value: tensor.element_size()\n            * tensor.nelement()\n        }",
  "def plan(\n        self,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n    ) -> ShardingPlan:\n        \"\"\"\n        Plans sharding for provided module and given sharders.\n\n        Args:\n            module (nn.Module): module that sharding is planned for.\n            sharders (List[ModuleSharder[nn.Module]]): provided sharders for module.\n\n        Returns:\n            ShardingPlan: the computed sharding plan.\n        \"\"\"\n        ...",
  "def collective_plan(\n        self,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n    ) -> ShardingPlan:\n        \"\"\"\n        Calls self.plan(...) on rank 0 and broadcasts.\n\n        Args:\n            module (nn.Module): module that sharding is planned for.\n            sharders (List[ModuleSharder[nn.Module]]): provided sharders for module.\n\n        Returns:\n            ShardingPlan: the computed sharding plan.\n        \"\"\"\n        ...",
  "class GenericMeta(type):\n        pass",
  "def impl(*args, **kwargs):\n            lhs = args[0]\n            op_fn = getattr(operator, method)\n            if len(args) == 1:\n                return op_fn(LazyAwaitable._wait_async(lhs))\n            elif len(args) == 2:\n                rhs = args[1]\n                return op_fn(\n                    LazyAwaitable._wait_async(lhs), LazyAwaitable._wait_async(rhs)\n                )\n            else:\n                raise RuntimeError(f\"magic method {as_magic} not supported!\")",
  "def impl(self, rhs):\n            op_fn = getattr(operator, method)\n            return op_fn(\n                LazyAwaitable._wait_async(rhs), LazyAwaitable._wait_async(self)\n            )",
  "def _fx_wrap_tensor_to_device_dtype(\n    t: torch.Tensor, tensor_device_dtype: torch.Tensor\n) -> torch.Tensor:\n    return t.to(device=tensor_device_dtype.device, dtype=tensor_device_dtype.dtype)",
  "def bucketize_kjt_before_all2all(\n    kjt: KeyedJaggedTensor,\n    num_buckets: int,\n    block_sizes: torch.Tensor,\n    output_permute: bool = False,\n    bucketize_pos: bool = False,\n) -> Tuple[KeyedJaggedTensor, Optional[torch.Tensor]]:\n    \"\"\"\n    Bucketizes the `values` in KeyedJaggedTensor into `num_buckets` buckets,\n    `lengths` are readjusted based on the bucketization results.\n\n    Note: This function should be used only for row-wise sharding before calling\n    `KJTAllToAll`.\n\n    Args:\n        num_buckets (int): number of buckets to bucketize the values into.\n        block_sizes: (torch.Tensor): bucket sizes for the keyed dimension.\n        output_permute (bool): output the memory location mapping from the unbucketized\n            values to bucketized values or not.\n        bucketize_pos (bool): output the changed position of the bucketized values or\n            not.\n\n    Returns:\n        Tuple[KeyedJaggedTensor, Optional[torch.Tensor]]: the bucketized `KeyedJaggedTensor` and the optional permute mapping from the unbucketized values to bucketized value.\n    \"\"\"\n\n    num_features = len(kjt.keys())\n    assert_fx_safe(\n        block_sizes.numel() == num_features,\n        f\"Expecting block sizes for {num_features} features, but {block_sizes.numel()} received.\",\n    )\n    block_sizes_new_type = _fx_wrap_tensor_to_device_dtype(block_sizes, kjt.values())\n    (\n        bucketized_lengths,\n        bucketized_indices,\n        bucketized_weights,\n        pos,\n        unbucketize_permute,\n    ) = torch.ops.fbgemm.block_bucketize_sparse_features(\n        kjt.lengths().view(-1),\n        kjt.values(),\n        bucketize_pos=bucketize_pos,\n        sequence=output_permute,\n        block_sizes=block_sizes_new_type,\n        my_size=num_buckets,\n        weights=kjt.weights_or_none(),\n    )\n\n    return (\n        KeyedJaggedTensor(\n            # duplicate keys will be resolved by AllToAll\n            keys=kjt.keys() * num_buckets,\n            values=bucketized_indices,\n            weights=pos if bucketize_pos else bucketized_weights,\n            lengths=bucketized_lengths.view(-1),\n            offsets=None,\n            stride=kjt.stride(),\n            length_per_key=None,\n            offset_per_key=None,\n            index_per_key=None,\n        ),\n        unbucketize_permute,\n    )",
  "def group_tables(\n    tables_per_rank: List[List[ShardedEmbeddingTable]],\n) -> List[List[GroupedEmbeddingConfig]]:\n    \"\"\"\n    Groups tables by `DataType`, `PoolingType`, and `EmbeddingComputeKernel`.\n\n    Args:\n        tables_per_rank (List[List[ShardedEmbeddingTable]]): list of sharded embedding\n            tables per rank with consistent weightedness.\n\n    Returns:\n        List[List[GroupedEmbeddingConfig]]: per rank list of GroupedEmbeddingConfig for features.\n    \"\"\"\n\n    def _group_tables_per_rank(\n        embedding_tables: List[ShardedEmbeddingTable],\n    ) -> List[GroupedEmbeddingConfig]:\n        grouped_embedding_configs: List[GroupedEmbeddingConfig] = []\n\n        # add fused params:\n        fused_params_groups = []\n        for table in embedding_tables:\n            if table.fused_params is None:\n                table.fused_params = {}\n            if table.fused_params not in fused_params_groups:\n                fused_params_groups.append(table.fused_params)\n\n        compute_kernels = [\n            EmbeddingComputeKernel.DENSE,\n            EmbeddingComputeKernel.FUSED,\n            EmbeddingComputeKernel.QUANT,\n        ]\n\n        for data_type in DataType:\n            for pooling in PoolingType:\n                # remove this when finishing migration\n                for has_feature_processor in [False, True]:\n                    for fused_params_group in fused_params_groups:\n                        for compute_kernel in compute_kernels:\n                            grouped_tables: List[ShardedEmbeddingTable] = []\n                            is_weighted = False\n                            for table in embedding_tables:\n                                compute_kernel_type = table.compute_kernel\n                                is_weighted = table.is_weighted\n                                if table.compute_kernel in [\n                                    EmbeddingComputeKernel.FUSED_UVM,\n                                    EmbeddingComputeKernel.FUSED_UVM_CACHING,\n                                ]:\n                                    compute_kernel_type = EmbeddingComputeKernel.FUSED\n                                elif table.compute_kernel in [\n                                    EmbeddingComputeKernel.QUANT_UVM,\n                                    EmbeddingComputeKernel.QUANT_UVM_CACHING,\n                                ]:\n                                    compute_kernel_type = EmbeddingComputeKernel.QUANT\n                                if (\n                                    table.data_type == data_type\n                                    and table.pooling.value == pooling.value\n                                    and table.has_feature_processor\n                                    == has_feature_processor\n                                    and compute_kernel_type == compute_kernel\n                                    and table.fused_params == fused_params_group\n                                ):\n                                    grouped_tables.append(table)\n\n                            if fused_params_group is None:\n                                fused_params_group = {}\n\n                            if grouped_tables:\n                                grouped_embedding_configs.append(\n                                    GroupedEmbeddingConfig(\n                                        data_type=data_type,\n                                        pooling=pooling,\n                                        is_weighted=is_weighted,\n                                        has_feature_processor=has_feature_processor,\n                                        compute_kernel=compute_kernel,\n                                        embedding_tables=grouped_tables,\n                                        fused_params={\n                                            k: v\n                                            for k, v in fused_params_group.items()\n                                            if k\n                                            not in [\n                                                \"_batch_key\"\n                                            ]  # drop '_batch_key' not a native fused param\n                                        },\n                                    )\n                                )\n        return grouped_embedding_configs\n\n    table_weightedness = [\n        table.is_weighted for tables in tables_per_rank for table in tables\n    ]\n    assert all(table_weightedness) or not any(table_weightedness)\n\n    grouped_embedding_configs_by_rank: List[List[GroupedEmbeddingConfig]] = []\n    for tables in tables_per_rank:\n        grouped_embedding_configs = _group_tables_per_rank(tables)\n        grouped_embedding_configs_by_rank.append(grouped_embedding_configs)\n\n    return grouped_embedding_configs_by_rank",
  "class KJTListAwaitable(Awaitable[KJTList]):\n    \"\"\"\n    Awaitable of KJTList.\n\n    Args:\n        awaitables (List[Awaitable[KeyedJaggedTensor]]): list of `Awaitable` of sparse\n            features.\n        ctx (C): sharding context to save the batch size info from the KJT for the\n            embedding AlltoAll.\n    \"\"\"\n\n    def __init__(\n        self,\n        awaitables: List[Awaitable[KeyedJaggedTensor]],\n        ctx: C,\n    ) -> None:\n        super().__init__()\n        self.awaitables = awaitables\n        self.ctx = ctx\n\n    def _wait_impl(self) -> KJTList:\n        \"\"\"\n        Syncs KJTs in `KJTList`.\n\n        Returns:\n            KJTList: synced `KJTList`.\n        \"\"\"\n        kjts = [w.wait() for w in self.awaitables]\n        _set_sharding_context_post_a2a(kjts, self.ctx)\n        return KJTList(kjts)",
  "def _set_sharding_context_post_a2a(\n    kjts: List[KeyedJaggedTensor],\n    ctx: C,\n) -> None:\n    for kjt, sharding_context in zip(kjts, getattr(ctx, \"sharding_contexts\", [])):\n        if (\n            hasattr(sharding_context, \"batch_size_per_rank_per_feature\")\n            and kjt.variable_stride_per_key()\n            and kjt.stride_per_key_per_rank()\n        ):\n            sharding_context.batch_size_per_rank_per_feature = [\n                [\n                    kjt.stride_per_key_per_rank()[i][j]\n                    for i in range(len(kjt.stride_per_key_per_rank()))\n                ]\n                for j in range(len(kjt.stride_per_key_per_rank()[0]))\n            ]",
  "def _set_sharding_context_intra_a2a(\n    tensors_awaitables: List[Awaitable[KeyedJaggedTensor]],\n    ctx: C,\n) -> None:\n    for awaitable, sharding_context in zip(\n        tensors_awaitables,\n        getattr(ctx, \"sharding_contexts\", []),\n    ):\n        if isinstance(awaitable, KJTAllToAllTensorsAwaitable):\n            if hasattr(sharding_context, \"input_splits\"):\n                sharding_context.input_splits = awaitable._input_splits[\"values\"]\n            if hasattr(sharding_context, \"output_splits\"):\n                sharding_context.output_splits = awaitable._output_splits[\"values\"]\n            if hasattr(sharding_context, \"sparse_features_recat\"):\n                sharding_context.sparse_features_recat = awaitable._recat\n            if (\n                hasattr(sharding_context, \"batch_size_per_rank\")\n                and awaitable._stride_per_rank is not None\n            ):\n                sharding_context.batch_size_per_rank = awaitable._stride_per_rank",
  "def _set_sharding_context_pre_a2a(\n    awaitables: List[Awaitable[Awaitable[KeyedJaggedTensor]]],\n    ctx: C,\n) -> None:\n    for awaitable, sharding_context in zip(\n        awaitables,\n        getattr(ctx, \"sharding_contexts\", []),\n    ):\n        kjt = (\n            awaitable._obj._obj\n            if isinstance(awaitable, NoWait)\n            else awaitable._input  # pyre-ignore[16]: KJTAllToAllSplitsAwaitable or KJTSplitsAllToAllMeta\n        )\n        if hasattr(sharding_context, \"batch_size_per_feature_pre_a2a\"):\n            sharding_context.batch_size_per_feature_pre_a2a = kjt.stride_per_key()\n        if hasattr(sharding_context, \"variable_batch_per_feature\"):\n            sharding_context.variable_batch_per_feature = kjt.variable_stride_per_key()",
  "def _split(flat_list: List[T], splits: List[int]) -> List[List[T]]:\n    return [\n        flat_list[sum(splits[:i]) : sum(splits[:i]) + n] for i, n in enumerate(splits)\n    ]",
  "class KJTListSplitsAwaitable(Awaitable[Awaitable[KJTList]], Generic[C]):\n    \"\"\"\n    Awaitable of Awaitable of KJTList.\n\n    Args:\n        awaitables (List[Awaitable[Awaitable[KeyedJaggedTensor]]]): result from calling\n            forward on `KJTAllToAll` with sparse features to redistribute.\n        ctx (C): sharding context to save the metadata from the input dist to for the\n            embedding AlltoAll.\n    \"\"\"\n\n    def __init__(\n        self,\n        awaitables: List[Awaitable[Awaitable[KeyedJaggedTensor]]],\n        ctx: C,\n    ) -> None:\n        super().__init__()\n        self.awaitables = awaitables\n        self.ctx = ctx\n        _set_sharding_context_pre_a2a(self.awaitables, self.ctx)\n\n    def _wait_impl(self) -> KJTListAwaitable:\n        \"\"\"\n        Calls first wait on the awaitable of awaitable of sparse features and updates\n        the context with metadata from the tensors awaitable.\n\n        The first wait gets the result of splits AlltoAll and returns the tensors\n        awaitable.\n\n        Returns:\n            KJTListAwaitable: awaitables for tensors of the sparse features.\n        \"\"\"\n        tensors_awaitables = [w.wait() for w in self.awaitables]\n        _set_sharding_context_intra_a2a(tensors_awaitables, self.ctx)\n        return KJTListAwaitable(tensors_awaitables, self.ctx)",
  "class KJTSplitsAllToAllMeta:\n    pg: dist.ProcessGroup\n    _input: KeyedJaggedTensor\n    splits: List[int]\n    splits_tensors: List[torch.Tensor]\n    input_splits: List[List[int]]\n    input_tensors: List[torch.Tensor]\n    labels: List[str]\n    keys: List[str]\n    device: torch.device\n    stagger: int\n    splits_cumsum: List[int]",
  "class FusedKJTListSplitsAwaitable(Awaitable[List[KJTListAwaitable]]):\n    def __init__(\n        self,\n        requests: List[KJTListSplitsAwaitable[C]],\n        contexts: List[C],\n        pg: Optional[dist.ProcessGroup],\n    ) -> None:\n        super().__init__()\n        self._contexts = contexts\n        self._awaitables: List[\n            Union[KJTSplitsAllToAllMeta, Awaitable[Awaitable[KeyedJaggedTensor]]]\n        ] = [awaitable for request in requests for awaitable in request.awaitables]\n        for req, ctx in zip(requests, self._contexts):\n            _set_sharding_context_pre_a2a(req.awaitables, ctx)\n        self._output_lengths: List[int] = [\n            len(request.awaitables) for request in requests\n        ]\n        self._lengths: List[int] = [\n            len(awaitable.splits_tensors)\n            if isinstance(awaitable, KJTSplitsAllToAllMeta)\n            else 0\n            for awaitable in self._awaitables\n        ]\n        splits_tensors = [\n            splits_tensor\n            for awaitable in self._awaitables\n            for splits_tensor in (\n                awaitable.splits_tensors\n                if isinstance(awaitable, KJTSplitsAllToAllMeta)\n                else []\n            )\n        ]\n        self._splits_awaitable: Optional[SplitsAllToAllAwaitable] = (\n            SplitsAllToAllAwaitable(\n                input_tensors=splits_tensors,\n                pg=pg,\n            )\n            if splits_tensors and pg is not None\n            else None\n        )\n\n    def _wait_impl(self) -> List[KJTListAwaitable]:\n        if self._splits_awaitable:\n            splits_list = self._splits_awaitable.wait()\n            splits_per_awaitable = _split(splits_list, self._lengths)\n        else:\n            splits_per_awaitable = [[] for _ in range(len(self._lengths))]\n        tensors_awaitables = []\n        for splits, awaitable in zip(splits_per_awaitable, self._awaitables):\n            if not splits:  # NoWait\n                assert isinstance(awaitable, Awaitable)\n                tensors_awaitables.append(awaitable.wait())\n                continue\n            assert isinstance(awaitable, KJTSplitsAllToAllMeta)\n            if awaitable._input.variable_stride_per_key():\n                output_splits = splits\n                stride_per_rank = None\n            else:\n                output_splits = splits[:-1]\n                stride_per_rank = splits[-1]\n            tensors_awaitables.append(\n                KJTAllToAllTensorsAwaitable(\n                    pg=awaitable.pg,\n                    input=awaitable._input,\n                    splits=awaitable.splits,\n                    input_splits=awaitable.input_splits,\n                    output_splits=output_splits,\n                    input_tensors=awaitable.input_tensors,\n                    labels=awaitable.labels,\n                    keys=awaitable.keys,\n                    device=awaitable.device,\n                    stagger=awaitable.stagger,\n                    stride_per_rank=stride_per_rank,\n                )\n            )\n        output = []\n        awaitables_per_output = _split(tensors_awaitables, self._output_lengths)\n        for awaitables, ctx in zip(awaitables_per_output, self._contexts):\n            _set_sharding_context_intra_a2a(awaitables, ctx)\n            output.append(KJTListAwaitable(awaitables, ctx))\n        return output",
  "class ListOfKJTListAwaitable(Awaitable[ListOfKJTList]):\n    \"\"\"\n    This module handles the tables-wise sharding input features distribution for\n    inference.\n\n    Args:\n        awaitables (List[Awaitable[KJTList]]): list of `Awaitable` of `KJTList`.\n    \"\"\"\n\n    def __init__(\n        self,\n        awaitables: List[Awaitable[KJTList]],\n    ) -> None:\n        super().__init__()\n        self.awaitables = awaitables\n\n    def _wait_impl(self) -> ListOfKJTList:\n        \"\"\"\n        Syncs sparse features in list of KJTList.\n\n        Returns:\n            ListOfKJTList: synced `ListOfKJTList`.\n\n        \"\"\"\n        return ListOfKJTList([w.wait() for w in self.awaitables])",
  "class ListOfKJTListSplitsAwaitable(Awaitable[Awaitable[ListOfKJTList]]):\n    \"\"\"\n    Awaitable of Awaitable of ListOfKJTList.\n\n    Args:\n        awaitables (List[Awaitable[Awaitable[KJTList]]]): list of `Awaitable`\n            of `Awaitable` of sparse features list.\n    \"\"\"\n\n    def __init__(\n        self,\n        awaitables: List[Awaitable[Awaitable[KJTList]]],\n    ) -> None:\n        super().__init__()\n        self.awaitables = awaitables\n\n    def _wait_impl(self) -> Awaitable[ListOfKJTList]:\n        \"\"\"\n        Calls first wait on the awaitable of awaitable of ListOfKJTList.\n\n        Returns:\n            Awaitable[ListOfKJTList]: awaitable of `ListOfKJTList`.\n\n        \"\"\"\n        return ListOfKJTListAwaitable([w.wait() for w in self.awaitables])",
  "class EmbeddingShardingContext(Multistreamable):\n    batch_size_per_rank: List[int] = field(default_factory=list)\n    batch_size_per_rank_per_feature: List[List[int]] = field(default_factory=list)\n    batch_size_per_feature_pre_a2a: List[int] = field(default_factory=list)\n    variable_batch_per_feature: bool = False\n\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        pass",
  "class BaseSparseFeaturesDist(abc.ABC, nn.Module, Generic[F]):\n    \"\"\"\n    Converts input from data-parallel to model-parallel.\n    \"\"\"\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> Union[Awaitable[Awaitable[F]], F]:\n        pass",
  "class BaseEmbeddingDist(abc.ABC, nn.Module, Generic[C, T, W]):\n    \"\"\"\n    Converts output of EmbeddingLookup from model-parallel to data-parallel.\n    \"\"\"\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        local_embs: T,\n        sharding_ctx: Optional[C] = None,\n    ) -> Union[Awaitable[W], W]:\n        pass",
  "class EmbeddingSharding(abc.ABC, Generic[C, F, T, W], FeatureShardingMixIn):\n    \"\"\"\n    Used to implement different sharding types for `EmbeddingBagCollection`, e.g.\n    table_wise.\n    \"\"\"\n\n    def __init__(\n        self, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None\n    ) -> None:\n\n        self._qcomm_codecs_registry = qcomm_codecs_registry\n\n    @property\n    def qcomm_codecs_registry(self) -> Optional[Dict[str, QuantizedCommCodecs]]:\n        return self._qcomm_codecs_registry\n\n    @abc.abstractmethod\n    def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[F]:\n        pass\n\n    @abc.abstractmethod\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[C, T, W]:\n        pass\n\n    @abc.abstractmethod\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[F, T]:\n        pass\n\n    @abc.abstractmethod\n    def embedding_dims(self) -> List[int]:\n        pass\n\n    @abc.abstractmethod\n    def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        pass\n\n    @abc.abstractmethod\n    def embedding_names(self) -> List[str]:\n        pass\n\n    @abc.abstractmethod\n    def embedding_names_per_rank(self) -> List[List[str]]:\n        pass\n\n    def embedding_tables(self) -> List[ShardedEmbeddingTable]:\n        raise NotImplementedError",
  "class EmbeddingShardingInfo:\n    embedding_config: EmbeddingTableConfig\n    param_sharding: ParameterSharding\n    param: torch.Tensor\n    fused_params: Optional[Dict[str, Any]] = None",
  "def _group_tables_per_rank(\n        embedding_tables: List[ShardedEmbeddingTable],\n    ) -> List[GroupedEmbeddingConfig]:\n        grouped_embedding_configs: List[GroupedEmbeddingConfig] = []\n\n        # add fused params:\n        fused_params_groups = []\n        for table in embedding_tables:\n            if table.fused_params is None:\n                table.fused_params = {}\n            if table.fused_params not in fused_params_groups:\n                fused_params_groups.append(table.fused_params)\n\n        compute_kernels = [\n            EmbeddingComputeKernel.DENSE,\n            EmbeddingComputeKernel.FUSED,\n            EmbeddingComputeKernel.QUANT,\n        ]\n\n        for data_type in DataType:\n            for pooling in PoolingType:\n                # remove this when finishing migration\n                for has_feature_processor in [False, True]:\n                    for fused_params_group in fused_params_groups:\n                        for compute_kernel in compute_kernels:\n                            grouped_tables: List[ShardedEmbeddingTable] = []\n                            is_weighted = False\n                            for table in embedding_tables:\n                                compute_kernel_type = table.compute_kernel\n                                is_weighted = table.is_weighted\n                                if table.compute_kernel in [\n                                    EmbeddingComputeKernel.FUSED_UVM,\n                                    EmbeddingComputeKernel.FUSED_UVM_CACHING,\n                                ]:\n                                    compute_kernel_type = EmbeddingComputeKernel.FUSED\n                                elif table.compute_kernel in [\n                                    EmbeddingComputeKernel.QUANT_UVM,\n                                    EmbeddingComputeKernel.QUANT_UVM_CACHING,\n                                ]:\n                                    compute_kernel_type = EmbeddingComputeKernel.QUANT\n                                if (\n                                    table.data_type == data_type\n                                    and table.pooling.value == pooling.value\n                                    and table.has_feature_processor\n                                    == has_feature_processor\n                                    and compute_kernel_type == compute_kernel\n                                    and table.fused_params == fused_params_group\n                                ):\n                                    grouped_tables.append(table)\n\n                            if fused_params_group is None:\n                                fused_params_group = {}\n\n                            if grouped_tables:\n                                grouped_embedding_configs.append(\n                                    GroupedEmbeddingConfig(\n                                        data_type=data_type,\n                                        pooling=pooling,\n                                        is_weighted=is_weighted,\n                                        has_feature_processor=has_feature_processor,\n                                        compute_kernel=compute_kernel,\n                                        embedding_tables=grouped_tables,\n                                        fused_params={\n                                            k: v\n                                            for k, v in fused_params_group.items()\n                                            if k\n                                            not in [\n                                                \"_batch_key\"\n                                            ]  # drop '_batch_key' not a native fused param\n                                        },\n                                    )\n                                )\n        return grouped_embedding_configs",
  "def __init__(\n        self,\n        awaitables: List[Awaitable[KeyedJaggedTensor]],\n        ctx: C,\n    ) -> None:\n        super().__init__()\n        self.awaitables = awaitables\n        self.ctx = ctx",
  "def _wait_impl(self) -> KJTList:\n        \"\"\"\n        Syncs KJTs in `KJTList`.\n\n        Returns:\n            KJTList: synced `KJTList`.\n        \"\"\"\n        kjts = [w.wait() for w in self.awaitables]\n        _set_sharding_context_post_a2a(kjts, self.ctx)\n        return KJTList(kjts)",
  "def __init__(\n        self,\n        awaitables: List[Awaitable[Awaitable[KeyedJaggedTensor]]],\n        ctx: C,\n    ) -> None:\n        super().__init__()\n        self.awaitables = awaitables\n        self.ctx = ctx\n        _set_sharding_context_pre_a2a(self.awaitables, self.ctx)",
  "def _wait_impl(self) -> KJTListAwaitable:\n        \"\"\"\n        Calls first wait on the awaitable of awaitable of sparse features and updates\n        the context with metadata from the tensors awaitable.\n\n        The first wait gets the result of splits AlltoAll and returns the tensors\n        awaitable.\n\n        Returns:\n            KJTListAwaitable: awaitables for tensors of the sparse features.\n        \"\"\"\n        tensors_awaitables = [w.wait() for w in self.awaitables]\n        _set_sharding_context_intra_a2a(tensors_awaitables, self.ctx)\n        return KJTListAwaitable(tensors_awaitables, self.ctx)",
  "def __init__(\n        self,\n        requests: List[KJTListSplitsAwaitable[C]],\n        contexts: List[C],\n        pg: Optional[dist.ProcessGroup],\n    ) -> None:\n        super().__init__()\n        self._contexts = contexts\n        self._awaitables: List[\n            Union[KJTSplitsAllToAllMeta, Awaitable[Awaitable[KeyedJaggedTensor]]]\n        ] = [awaitable for request in requests for awaitable in request.awaitables]\n        for req, ctx in zip(requests, self._contexts):\n            _set_sharding_context_pre_a2a(req.awaitables, ctx)\n        self._output_lengths: List[int] = [\n            len(request.awaitables) for request in requests\n        ]\n        self._lengths: List[int] = [\n            len(awaitable.splits_tensors)\n            if isinstance(awaitable, KJTSplitsAllToAllMeta)\n            else 0\n            for awaitable in self._awaitables\n        ]\n        splits_tensors = [\n            splits_tensor\n            for awaitable in self._awaitables\n            for splits_tensor in (\n                awaitable.splits_tensors\n                if isinstance(awaitable, KJTSplitsAllToAllMeta)\n                else []\n            )\n        ]\n        self._splits_awaitable: Optional[SplitsAllToAllAwaitable] = (\n            SplitsAllToAllAwaitable(\n                input_tensors=splits_tensors,\n                pg=pg,\n            )\n            if splits_tensors and pg is not None\n            else None\n        )",
  "def _wait_impl(self) -> List[KJTListAwaitable]:\n        if self._splits_awaitable:\n            splits_list = self._splits_awaitable.wait()\n            splits_per_awaitable = _split(splits_list, self._lengths)\n        else:\n            splits_per_awaitable = [[] for _ in range(len(self._lengths))]\n        tensors_awaitables = []\n        for splits, awaitable in zip(splits_per_awaitable, self._awaitables):\n            if not splits:  # NoWait\n                assert isinstance(awaitable, Awaitable)\n                tensors_awaitables.append(awaitable.wait())\n                continue\n            assert isinstance(awaitable, KJTSplitsAllToAllMeta)\n            if awaitable._input.variable_stride_per_key():\n                output_splits = splits\n                stride_per_rank = None\n            else:\n                output_splits = splits[:-1]\n                stride_per_rank = splits[-1]\n            tensors_awaitables.append(\n                KJTAllToAllTensorsAwaitable(\n                    pg=awaitable.pg,\n                    input=awaitable._input,\n                    splits=awaitable.splits,\n                    input_splits=awaitable.input_splits,\n                    output_splits=output_splits,\n                    input_tensors=awaitable.input_tensors,\n                    labels=awaitable.labels,\n                    keys=awaitable.keys,\n                    device=awaitable.device,\n                    stagger=awaitable.stagger,\n                    stride_per_rank=stride_per_rank,\n                )\n            )\n        output = []\n        awaitables_per_output = _split(tensors_awaitables, self._output_lengths)\n        for awaitables, ctx in zip(awaitables_per_output, self._contexts):\n            _set_sharding_context_intra_a2a(awaitables, ctx)\n            output.append(KJTListAwaitable(awaitables, ctx))\n        return output",
  "def __init__(\n        self,\n        awaitables: List[Awaitable[KJTList]],\n    ) -> None:\n        super().__init__()\n        self.awaitables = awaitables",
  "def _wait_impl(self) -> ListOfKJTList:\n        \"\"\"\n        Syncs sparse features in list of KJTList.\n\n        Returns:\n            ListOfKJTList: synced `ListOfKJTList`.\n\n        \"\"\"\n        return ListOfKJTList([w.wait() for w in self.awaitables])",
  "def __init__(\n        self,\n        awaitables: List[Awaitable[Awaitable[KJTList]]],\n    ) -> None:\n        super().__init__()\n        self.awaitables = awaitables",
  "def _wait_impl(self) -> Awaitable[ListOfKJTList]:\n        \"\"\"\n        Calls first wait on the awaitable of awaitable of ListOfKJTList.\n\n        Returns:\n            Awaitable[ListOfKJTList]: awaitable of `ListOfKJTList`.\n\n        \"\"\"\n        return ListOfKJTListAwaitable([w.wait() for w in self.awaitables])",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        pass",
  "def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> Union[Awaitable[Awaitable[F]], F]:\n        pass",
  "def forward(\n        self,\n        local_embs: T,\n        sharding_ctx: Optional[C] = None,\n    ) -> Union[Awaitable[W], W]:\n        pass",
  "def __init__(\n        self, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None\n    ) -> None:\n\n        self._qcomm_codecs_registry = qcomm_codecs_registry",
  "def qcomm_codecs_registry(self) -> Optional[Dict[str, QuantizedCommCodecs]]:\n        return self._qcomm_codecs_registry",
  "def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[F]:\n        pass",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[C, T, W]:\n        pass",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[F, T]:\n        pass",
  "def embedding_dims(self) -> List[int]:\n        pass",
  "def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        pass",
  "def embedding_names(self) -> List[str]:\n        pass",
  "def embedding_names_per_rank(self) -> List[List[str]]:\n        pass",
  "def embedding_tables(self) -> List[ShardedEmbeddingTable]:\n        raise NotImplementedError",
  "def _append_table_shard(\n    d: Dict[str, List[Shard]], table_name: str, shard: Shard\n) -> None:\n    if table_name not in d:\n        d[table_name] = []\n    d[table_name].append(shard)",
  "class ShardedQuantEmbeddingModuleState(\n    ShardedEmbeddingModule[CompIn, DistOut, Out, ShrdCtx]\n):\n    def _initialize_torch_state(  # noqa: C901\n        # Union[ShardedQuantEmbeddingBagCollection, ShardedQuantEmbeddingCollection]\n        self,\n        tbes: Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig],\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        tables_weights_prefix: str,  # \"embedding_bags\" or \"embeddings\"\n    ) -> None:  # noqa\n        # State is prepared only in \"quant_state_dict_split_scale_bias\" mode\n        assert (\n            tables_weights_prefix == \"embedding_bags\"\n            or tables_weights_prefix == \"embeddings\"\n        )\n\n        # weight\n        self._table_name_to_local_shards: Dict[str, List[Shard]] = {}\n        self._table_name_to_sharded_tensor: Dict[\n            str, Union[torch.Tensor, ShardedTensorBase]\n        ] = {}\n\n        # weight_qscale\n        self._table_name_to_local_shards_qscale: Dict[str, List[Shard]] = {}\n        self._table_name_to_sharded_tensor_qscale: Dict[\n            str, Union[torch.Tensor, ShardedTensorBase]\n        ] = {}\n        self._table_name_to_tensors_list_qscale: Dict[str, List[torch.Tensor]] = {}\n\n        # weight_qbias\n        self._table_name_to_local_shards_qbias: Dict[str, List[Shard]] = {}\n        self._table_name_to_sharded_tensor_qbias: Dict[\n            str, Union[torch.Tensor, ShardedTensorBase]\n        ] = {}\n        self._table_name_to_tensors_list_qbias: Dict[str, List[torch.Tensor]] = {}\n\n        for tbe, config in tbes.items():\n            for (tbe_split_w, tbe_split_qscale, tbe_split_qbias), table in zip(\n                tbe.split_embedding_weights_with_scale_bias(split_scale_bias_mode=2),\n                config.embedding_tables,\n            ):\n                # weight shards section:\n                assert table.local_metadata\n                metadata: ShardMetadata = copy.deepcopy(table.local_metadata)\n                metadata.shard_sizes = [tbe_split_w.size(0), tbe_split_w.size(1)]\n\n                # TODO(ivankobzarev): \"meta\" sharding support: cleanup when copy to \"meta\" moves all tensors to \"meta\"\n                # pyre-ignore\n                if metadata.placement.device != tbe_split_w.device:\n                    metadata.placement = _remote_device(tbe_split_w.device)\n                _append_table_shard(\n                    self._table_name_to_local_shards,\n                    table.name,\n                    Shard(tensor=tbe_split_w, metadata=metadata),\n                )\n                # end of weight shards section\n\n                # weight_qscale & weight_qbias section:\n                # For RW - ShardedTensorBase\n                # For CW - List[Tensor] that logically corresponds to the same unsharded Tensor, but present on each sharded rank\n                for (\n                    tbe_split_qparam,\n                    table_name_to_local_shards,\n                    table_name_to_tensors_list,\n                ) in [\n                    (\n                        tbe_split_qscale,\n                        self._table_name_to_local_shards_qscale,\n                        self._table_name_to_tensors_list_qscale,\n                    ),\n                    (\n                        tbe_split_qbias,\n                        self._table_name_to_local_shards_qbias,\n                        self._table_name_to_tensors_list_qbias,\n                    ),\n                ]:\n                    assert table.local_metadata\n                    metadata: ShardMetadata = copy.deepcopy(table.local_metadata)\n                    shard_sizes = metadata.shard_sizes\n                    shard_offsets = metadata.shard_offsets\n\n                    shard_sizes_cols = shard_sizes[1]\n                    shard_offsets_cols = shard_offsets[1]\n\n                    parameter_sharding: ParameterSharding = (\n                        table_name_to_parameter_sharding[table.name]\n                    )\n                    sharding_type: str = parameter_sharding.sharding_type\n\n                    if sharding_type == ShardingType.COLUMN_WISE.value:\n                        if table.name not in table_name_to_tensors_list:\n                            assert parameter_sharding.ranks\n                            num_shards: int = len(parameter_sharding.ranks)\n                            table_name_to_tensors_list[table.name] = [\n                                torch.empty([])\n                            ] * num_shards\n\n                        column_idx = int(shard_offsets_cols / shard_sizes_cols)\n                        table_name_to_tensors_list[table.name][\n                            column_idx\n                        ] = tbe_split_qparam\n                    else:\n                        qmetadata = ShardMetadata(\n                            shard_offsets=metadata.shard_offsets,\n                            shard_sizes=[\n                                tbe_split_qparam.shape[0],\n                                tbe_split_qparam.shape[1],\n                            ],\n                            # pyre-ignore\n                            placement=table.local_metadata.placement,\n                        )\n                        # TODO(ivankobzarev): \"meta\" sharding support: cleanup when copy to \"meta\" moves all tensors to \"meta\"\n                        if qmetadata.placement.device != tbe_split_qparam.device:\n                            qmetadata.placement = _remote_device(\n                                tbe_split_qparam.device\n                            )\n                        _append_table_shard(\n                            table_name_to_local_shards,\n                            table.name,\n                            Shard(tensor=tbe_split_qparam, metadata=qmetadata),\n                        )\n                    # end of weight_qscale & weight_qbias section\n\n        for table_name_to_local_shards, table_name_to_sharded_tensor in [\n            (self._table_name_to_local_shards, self._table_name_to_sharded_tensor),\n            (\n                self._table_name_to_local_shards_qscale,\n                self._table_name_to_sharded_tensor_qscale,\n            ),\n            (\n                self._table_name_to_local_shards_qbias,\n                self._table_name_to_sharded_tensor_qbias,\n            ),\n        ]:\n            for table_name, local_shards in table_name_to_local_shards.items():\n                if len(local_shards) == 1:\n                    # Single Tensor per table (TW sharding)\n                    table_name_to_sharded_tensor[table_name] = local_shards[0].tensor\n                    continue\n\n                # ShardedTensor per table\n                global_rows = max(\n                    [\n                        ls.metadata.shard_offsets[0] + ls.metadata.shard_sizes[0]\n                        for ls in local_shards\n                    ]\n                )\n                global_cols = max(\n                    [\n                        ls.metadata.shard_offsets[1] + ls.metadata.shard_sizes[1]\n                        for ls in local_shards\n                    ]\n                )\n                global_metadata: ShardedTensorMetadata = ShardedTensorMetadata(\n                    shards_metadata=[ls.metadata for ls in local_shards],\n                    size=torch.Size([global_rows, global_cols]),\n                )\n                table_name_to_sharded_tensor[\n                    table_name\n                ] = ShardedTensorBase._init_from_local_shards_and_global_metadata(\n                    local_shards=local_shards,\n                    sharded_tensor_metadata=global_metadata,\n                )\n\n        def post_state_dict_hook(\n            # Union[\"ShardedQuantEmbeddingBagCollection\", \"ShardedQuantEmbeddingCollection\"]\n            module: ShardedQuantEmbeddingModuleState[CompIn, DistOut, Out, ShrdCtx],\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            for (\n                table_name,\n                sharded_t,\n            ) in module._table_name_to_sharded_tensor.items():\n                destination[\n                    f\"{prefix}{tables_weights_prefix}.{table_name}.weight\"\n                ] = sharded_t\n\n            for sfx, dict_sharded_t, dict_t_list in [\n                (\n                    \"qscale\",\n                    module._table_name_to_sharded_tensor_qscale,\n                    module._table_name_to_tensors_list_qscale,\n                ),\n                (\n                    \"qbias\",\n                    module._table_name_to_sharded_tensor_qbias,\n                    module._table_name_to_tensors_list_qbias,\n                ),\n            ]:\n                for (\n                    table_name,\n                    sharded_t,\n                ) in dict_sharded_t.items():\n                    destination[\n                        f\"{prefix}{tables_weights_prefix}.{table_name}.weight_{sfx}\"\n                    ] = sharded_t\n                for (\n                    table_name,\n                    t_list,\n                ) in dict_t_list.items():\n                    destination[\n                        f\"{prefix}{tables_weights_prefix}.{table_name}.weight_{sfx}\"\n                    ] = t_list\n\n        self._register_state_dict_hook(post_state_dict_hook)\n\n    def _load_from_state_dict(\n        # Union[\"ShardedQuantEmbeddingBagCollection\", \"ShardedQuantEmbeddingCollection\"]\n        self,\n        state_dict: Mapping[str, Any],\n        prefix: str,\n        # pyre-ignore\n        local_metadata,\n        strict: bool,\n        missing_keys: List[str],\n        unexpected_keys: List[str],\n        error_msgs: List[str],\n    ) -> None:\n        dst_state_dict = self.state_dict()\n        _missing_keys: List[str] = []\n        _unexpected_keys: List[str] = list(state_dict.keys())\n        for name, dst_tensor in dst_state_dict.items():\n            src_state_dict_name = prefix + name\n            if src_state_dict_name not in state_dict:\n                _missing_keys.append(src_state_dict_name)\n                continue\n\n            src_tensor = state_dict[src_state_dict_name]\n            if isinstance(dst_tensor, ShardedTensorBase) and isinstance(\n                src_tensor, ShardedTensorBase\n            ):\n                # sharded to sharded model, only identically sharded\n                for dst_local_shard in dst_tensor.local_shards():\n                    copied: bool = False\n                    for src_local_shard in src_tensor.local_shards():\n                        if (\n                            dst_local_shard.metadata.shard_offsets\n                            == src_local_shard.metadata.shard_offsets\n                            and dst_local_shard.metadata.shard_sizes\n                            == src_local_shard.metadata.shard_sizes\n                        ):\n                            dst_local_shard.tensor.copy_(src_local_shard.tensor)\n                            copied = True\n                            break\n                    assert copied, \"Incompatible state_dict\"\n            elif isinstance(dst_tensor, ShardedTensorBase) and isinstance(\n                src_tensor, torch.Tensor\n            ):\n                # non_sharded to sharded model\n                for dst_local_shard in dst_tensor.local_shards():\n                    dst_tensor = dst_local_shard.tensor\n                    assert src_tensor.ndim == dst_tensor.ndim\n                    meta = dst_local_shard.metadata\n                    t = src_tensor.detach()\n                    rows_from = meta.shard_offsets[0]\n                    rows_to = rows_from + meta.shard_sizes[0]\n                    if t.ndim == 1:\n                        dst_tensor.copy_(t[rows_from:rows_to])\n                    elif t.ndim == 2:\n                        cols_from = meta.shard_offsets[1]\n                        cols_to = cols_from + meta.shard_sizes[1]\n                        dst_tensor.copy_(\n                            t[\n                                rows_from:rows_to,\n                                cols_from:cols_to,\n                            ]\n                        )\n                    else:\n                        raise RuntimeError(\"Tensors with ndim > 2 are not supported\")\n            elif isinstance(dst_tensor, list) and isinstance(src_tensor, torch.Tensor):\n                # non_sharded to CW columns qscale, qbias (one to many)\n                for t in dst_tensor:\n                    assert isinstance(t, torch.Tensor)\n                    t.copy_(src_tensor)\n            else:\n                dst_tensor.copy_(src_tensor)\n\n            _unexpected_keys.remove(src_state_dict_name)\n        missing_keys.extend(_missing_keys)\n        unexpected_keys.extend(_unexpected_keys)",
  "def _initialize_torch_state(  # noqa: C901\n        # Union[ShardedQuantEmbeddingBagCollection, ShardedQuantEmbeddingCollection]\n        self,\n        tbes: Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig],\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        tables_weights_prefix: str,  # \"embedding_bags\" or \"embeddings\"\n    ) -> None:  # noqa\n        # State is prepared only in \"quant_state_dict_split_scale_bias\" mode\n        assert (\n            tables_weights_prefix == \"embedding_bags\"\n            or tables_weights_prefix == \"embeddings\"\n        )\n\n        # weight\n        self._table_name_to_local_shards: Dict[str, List[Shard]] = {}\n        self._table_name_to_sharded_tensor: Dict[\n            str, Union[torch.Tensor, ShardedTensorBase]\n        ] = {}\n\n        # weight_qscale\n        self._table_name_to_local_shards_qscale: Dict[str, List[Shard]] = {}\n        self._table_name_to_sharded_tensor_qscale: Dict[\n            str, Union[torch.Tensor, ShardedTensorBase]\n        ] = {}\n        self._table_name_to_tensors_list_qscale: Dict[str, List[torch.Tensor]] = {}\n\n        # weight_qbias\n        self._table_name_to_local_shards_qbias: Dict[str, List[Shard]] = {}\n        self._table_name_to_sharded_tensor_qbias: Dict[\n            str, Union[torch.Tensor, ShardedTensorBase]\n        ] = {}\n        self._table_name_to_tensors_list_qbias: Dict[str, List[torch.Tensor]] = {}\n\n        for tbe, config in tbes.items():\n            for (tbe_split_w, tbe_split_qscale, tbe_split_qbias), table in zip(\n                tbe.split_embedding_weights_with_scale_bias(split_scale_bias_mode=2),\n                config.embedding_tables,\n            ):\n                # weight shards section:\n                assert table.local_metadata\n                metadata: ShardMetadata = copy.deepcopy(table.local_metadata)\n                metadata.shard_sizes = [tbe_split_w.size(0), tbe_split_w.size(1)]\n\n                # TODO(ivankobzarev): \"meta\" sharding support: cleanup when copy to \"meta\" moves all tensors to \"meta\"\n                # pyre-ignore\n                if metadata.placement.device != tbe_split_w.device:\n                    metadata.placement = _remote_device(tbe_split_w.device)\n                _append_table_shard(\n                    self._table_name_to_local_shards,\n                    table.name,\n                    Shard(tensor=tbe_split_w, metadata=metadata),\n                )\n                # end of weight shards section\n\n                # weight_qscale & weight_qbias section:\n                # For RW - ShardedTensorBase\n                # For CW - List[Tensor] that logically corresponds to the same unsharded Tensor, but present on each sharded rank\n                for (\n                    tbe_split_qparam,\n                    table_name_to_local_shards,\n                    table_name_to_tensors_list,\n                ) in [\n                    (\n                        tbe_split_qscale,\n                        self._table_name_to_local_shards_qscale,\n                        self._table_name_to_tensors_list_qscale,\n                    ),\n                    (\n                        tbe_split_qbias,\n                        self._table_name_to_local_shards_qbias,\n                        self._table_name_to_tensors_list_qbias,\n                    ),\n                ]:\n                    assert table.local_metadata\n                    metadata: ShardMetadata = copy.deepcopy(table.local_metadata)\n                    shard_sizes = metadata.shard_sizes\n                    shard_offsets = metadata.shard_offsets\n\n                    shard_sizes_cols = shard_sizes[1]\n                    shard_offsets_cols = shard_offsets[1]\n\n                    parameter_sharding: ParameterSharding = (\n                        table_name_to_parameter_sharding[table.name]\n                    )\n                    sharding_type: str = parameter_sharding.sharding_type\n\n                    if sharding_type == ShardingType.COLUMN_WISE.value:\n                        if table.name not in table_name_to_tensors_list:\n                            assert parameter_sharding.ranks\n                            num_shards: int = len(parameter_sharding.ranks)\n                            table_name_to_tensors_list[table.name] = [\n                                torch.empty([])\n                            ] * num_shards\n\n                        column_idx = int(shard_offsets_cols / shard_sizes_cols)\n                        table_name_to_tensors_list[table.name][\n                            column_idx\n                        ] = tbe_split_qparam\n                    else:\n                        qmetadata = ShardMetadata(\n                            shard_offsets=metadata.shard_offsets,\n                            shard_sizes=[\n                                tbe_split_qparam.shape[0],\n                                tbe_split_qparam.shape[1],\n                            ],\n                            # pyre-ignore\n                            placement=table.local_metadata.placement,\n                        )\n                        # TODO(ivankobzarev): \"meta\" sharding support: cleanup when copy to \"meta\" moves all tensors to \"meta\"\n                        if qmetadata.placement.device != tbe_split_qparam.device:\n                            qmetadata.placement = _remote_device(\n                                tbe_split_qparam.device\n                            )\n                        _append_table_shard(\n                            table_name_to_local_shards,\n                            table.name,\n                            Shard(tensor=tbe_split_qparam, metadata=qmetadata),\n                        )\n                    # end of weight_qscale & weight_qbias section\n\n        for table_name_to_local_shards, table_name_to_sharded_tensor in [\n            (self._table_name_to_local_shards, self._table_name_to_sharded_tensor),\n            (\n                self._table_name_to_local_shards_qscale,\n                self._table_name_to_sharded_tensor_qscale,\n            ),\n            (\n                self._table_name_to_local_shards_qbias,\n                self._table_name_to_sharded_tensor_qbias,\n            ),\n        ]:\n            for table_name, local_shards in table_name_to_local_shards.items():\n                if len(local_shards) == 1:\n                    # Single Tensor per table (TW sharding)\n                    table_name_to_sharded_tensor[table_name] = local_shards[0].tensor\n                    continue\n\n                # ShardedTensor per table\n                global_rows = max(\n                    [\n                        ls.metadata.shard_offsets[0] + ls.metadata.shard_sizes[0]\n                        for ls in local_shards\n                    ]\n                )\n                global_cols = max(\n                    [\n                        ls.metadata.shard_offsets[1] + ls.metadata.shard_sizes[1]\n                        for ls in local_shards\n                    ]\n                )\n                global_metadata: ShardedTensorMetadata = ShardedTensorMetadata(\n                    shards_metadata=[ls.metadata for ls in local_shards],\n                    size=torch.Size([global_rows, global_cols]),\n                )\n                table_name_to_sharded_tensor[\n                    table_name\n                ] = ShardedTensorBase._init_from_local_shards_and_global_metadata(\n                    local_shards=local_shards,\n                    sharded_tensor_metadata=global_metadata,\n                )\n\n        def post_state_dict_hook(\n            # Union[\"ShardedQuantEmbeddingBagCollection\", \"ShardedQuantEmbeddingCollection\"]\n            module: ShardedQuantEmbeddingModuleState[CompIn, DistOut, Out, ShrdCtx],\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            for (\n                table_name,\n                sharded_t,\n            ) in module._table_name_to_sharded_tensor.items():\n                destination[\n                    f\"{prefix}{tables_weights_prefix}.{table_name}.weight\"\n                ] = sharded_t\n\n            for sfx, dict_sharded_t, dict_t_list in [\n                (\n                    \"qscale\",\n                    module._table_name_to_sharded_tensor_qscale,\n                    module._table_name_to_tensors_list_qscale,\n                ),\n                (\n                    \"qbias\",\n                    module._table_name_to_sharded_tensor_qbias,\n                    module._table_name_to_tensors_list_qbias,\n                ),\n            ]:\n                for (\n                    table_name,\n                    sharded_t,\n                ) in dict_sharded_t.items():\n                    destination[\n                        f\"{prefix}{tables_weights_prefix}.{table_name}.weight_{sfx}\"\n                    ] = sharded_t\n                for (\n                    table_name,\n                    t_list,\n                ) in dict_t_list.items():\n                    destination[\n                        f\"{prefix}{tables_weights_prefix}.{table_name}.weight_{sfx}\"\n                    ] = t_list\n\n        self._register_state_dict_hook(post_state_dict_hook)",
  "def _load_from_state_dict(\n        # Union[\"ShardedQuantEmbeddingBagCollection\", \"ShardedQuantEmbeddingCollection\"]\n        self,\n        state_dict: Mapping[str, Any],\n        prefix: str,\n        # pyre-ignore\n        local_metadata,\n        strict: bool,\n        missing_keys: List[str],\n        unexpected_keys: List[str],\n        error_msgs: List[str],\n    ) -> None:\n        dst_state_dict = self.state_dict()\n        _missing_keys: List[str] = []\n        _unexpected_keys: List[str] = list(state_dict.keys())\n        for name, dst_tensor in dst_state_dict.items():\n            src_state_dict_name = prefix + name\n            if src_state_dict_name not in state_dict:\n                _missing_keys.append(src_state_dict_name)\n                continue\n\n            src_tensor = state_dict[src_state_dict_name]\n            if isinstance(dst_tensor, ShardedTensorBase) and isinstance(\n                src_tensor, ShardedTensorBase\n            ):\n                # sharded to sharded model, only identically sharded\n                for dst_local_shard in dst_tensor.local_shards():\n                    copied: bool = False\n                    for src_local_shard in src_tensor.local_shards():\n                        if (\n                            dst_local_shard.metadata.shard_offsets\n                            == src_local_shard.metadata.shard_offsets\n                            and dst_local_shard.metadata.shard_sizes\n                            == src_local_shard.metadata.shard_sizes\n                        ):\n                            dst_local_shard.tensor.copy_(src_local_shard.tensor)\n                            copied = True\n                            break\n                    assert copied, \"Incompatible state_dict\"\n            elif isinstance(dst_tensor, ShardedTensorBase) and isinstance(\n                src_tensor, torch.Tensor\n            ):\n                # non_sharded to sharded model\n                for dst_local_shard in dst_tensor.local_shards():\n                    dst_tensor = dst_local_shard.tensor\n                    assert src_tensor.ndim == dst_tensor.ndim\n                    meta = dst_local_shard.metadata\n                    t = src_tensor.detach()\n                    rows_from = meta.shard_offsets[0]\n                    rows_to = rows_from + meta.shard_sizes[0]\n                    if t.ndim == 1:\n                        dst_tensor.copy_(t[rows_from:rows_to])\n                    elif t.ndim == 2:\n                        cols_from = meta.shard_offsets[1]\n                        cols_to = cols_from + meta.shard_sizes[1]\n                        dst_tensor.copy_(\n                            t[\n                                rows_from:rows_to,\n                                cols_from:cols_to,\n                            ]\n                        )\n                    else:\n                        raise RuntimeError(\"Tensors with ndim > 2 are not supported\")\n            elif isinstance(dst_tensor, list) and isinstance(src_tensor, torch.Tensor):\n                # non_sharded to CW columns qscale, qbias (one to many)\n                for t in dst_tensor:\n                    assert isinstance(t, torch.Tensor)\n                    t.copy_(src_tensor)\n            else:\n                dst_tensor.copy_(src_tensor)\n\n            _unexpected_keys.remove(src_state_dict_name)\n        missing_keys.extend(_missing_keys)\n        unexpected_keys.extend(_unexpected_keys)",
  "def post_state_dict_hook(\n            # Union[\"ShardedQuantEmbeddingBagCollection\", \"ShardedQuantEmbeddingCollection\"]\n            module: ShardedQuantEmbeddingModuleState[CompIn, DistOut, Out, ShrdCtx],\n            destination: Dict[str, torch.Tensor],\n            prefix: str,\n            _local_metadata: Dict[str, Any],\n        ) -> None:\n            for (\n                table_name,\n                sharded_t,\n            ) in module._table_name_to_sharded_tensor.items():\n                destination[\n                    f\"{prefix}{tables_weights_prefix}.{table_name}.weight\"\n                ] = sharded_t\n\n            for sfx, dict_sharded_t, dict_t_list in [\n                (\n                    \"qscale\",\n                    module._table_name_to_sharded_tensor_qscale,\n                    module._table_name_to_tensors_list_qscale,\n                ),\n                (\n                    \"qbias\",\n                    module._table_name_to_sharded_tensor_qbias,\n                    module._table_name_to_tensors_list_qbias,\n                ),\n            ]:\n                for (\n                    table_name,\n                    sharded_t,\n                ) in dict_sharded_t.items():\n                    destination[\n                        f\"{prefix}{tables_weights_prefix}.{table_name}.weight_{sfx}\"\n                    ] = sharded_t\n                for (\n                    table_name,\n                    t_list,\n                ) in dict_t_list.items():\n                    destination[\n                        f\"{prefix}{tables_weights_prefix}.{table_name}.weight_{sfx}\"\n                    ] = t_list",
  "def set_gradient_division(val: bool) -> None:\n    global GRADIENT_DIVISION\n    GRADIENT_DIVISION = val",
  "def get_gradient_division() -> bool:\n    global GRADIENT_DIVISION\n    return GRADIENT_DIVISION",
  "class Request(Awaitable[W]):\n    \"\"\"\n    Defines a collective operation request for a process group on a tensor.\n\n    Args:\n        pg (dist.ProcessGroup): The process group the request is for.\n    \"\"\"\n\n    def __init__(self, pg: dist.ProcessGroup, device: torch.device) -> None:\n        super().__init__()\n        self.pg: dist.ProcessGroup = pg\n        self.req: Optional[dist.Work] = None\n        self.tensor: Optional[W] = None\n        self.a2ai = None  # type: ignore\n        self.qcomm_ctx = None  # type: ignore\n        self.rsi = None  # type: ignore\n        self.agi = None  # type: ignore\n        self.wait_function = None  # type: ignore\n\n        # This dummy tensor is used to build the autograd graph between\n        # CommOp-Req and CommOp-Await. The actual forward tensors, and backwards gradient tensors\n        # are stored in self.tensor\n        self.dummy_tensor: torch.Tensor = torch.empty(\n            1,\n            requires_grad=True,\n            device=device,\n        )\n\n    def _wait_impl(self) -> W:\n        \"\"\"\n        Calls the wait function for this request.\n        \"\"\"\n\n        ret = self.wait_function.apply(self.pg, self, self.dummy_tensor)\n        self.req = None\n        self.tensor = None\n        return ret",
  "class All2AllPooledInfo(object):\n    \"\"\"\n    The data class that collects the attributes when calling the `alltoall_pooled`\n    operation.\n\n    Attributes:\n        batch_size_per_rank (List[int]): batch size in each rank\n        dim_sum_per_rank (List[int]): number of features (sum of dimensions) of the\n            embedding in each rank.\n        dim_sum_per_rank_tensor (Optional[Tensor]): the tensor version of\n            `dim_sum_per_rank`, this is only used by the fast kernel of\n            `_recat_pooled_embedding_grad_out`.\n        cumsum_dim_sum_per_rank_tensor (Optional[Tensor]): cumulative sum of\n            `dim_sum_per_rank`, this is only used by the fast kernel of\n            `_recat_pooled_embedding_grad_out`.\n        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.\n    \"\"\"\n\n    batch_size_per_rank: List[int]\n    dim_sum_per_rank: List[int]\n    dim_sum_per_rank_tensor: Optional[Tensor]\n    cumsum_dim_sum_per_rank_tensor: Optional[Tensor]\n    codecs: Optional[QuantizedCommCodecs] = None",
  "class VariableBatchAll2AllPooledInfo(object):\n    \"\"\"\n    The data class that collects the attributes when calling the\n    `variable_batch_alltoall_pooled` operation.\n\n    Attributes:\n        batch_size_per_rank_per_feature (List[List[int]]): batch size per rank per\n            feature.\n        batch_size_per_feature_pre_a2a (List[int]): local batch size before scattering.\n        emb_dim_per_rank_per_feature (List[List[int]]): embedding dimension per rank\n            per feature\n        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.\n        input_splits (Optional[List[int]]): input splits of tensor all to all.\n        output_splits (Optional[List[int]]): output splits of tensor all to all.\n    \"\"\"\n\n    batch_size_per_rank_per_feature: List[List[int]]\n    batch_size_per_feature_pre_a2a: List[int]\n    emb_dim_per_rank_per_feature: List[List[int]]\n    codecs: Optional[QuantizedCommCodecs] = None\n    input_splits: Optional[List[int]] = None\n    output_splits: Optional[List[int]] = None",
  "class All2AllSequenceInfo(object):\n    \"\"\"\n    The data class that collects the attributes when calling the `alltoall_sequence`\n    operation.\n\n    Attributes:\n        embedding_dim (int): embedding dimension.\n        lengths_after_sparse_data_all2all (Tensor): lengths of sparse features after\n            AlltoAll.\n        forward_recat_tensor (Optional[Tensor]): recat tensor for forward.\n        backward_recat_tensor (Tensor): recat tensor for backward.\n        input_splits (List[int]): input splits.\n        output_splits (List[int]): output splits.\n        variable_batch_size (bool): whether variable batch size is enabled.\n        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.\n        permuted_lengths_after_sparse_data_all2all (Optional[Tensor]): lengths of sparse\n            features before AlltoAll.\n    \"\"\"\n\n    embedding_dim: int\n    lengths_after_sparse_data_all2all: Tensor\n    forward_recat_tensor: Optional[Tensor]\n    backward_recat_tensor: Tensor\n    input_splits: List[int]\n    output_splits: List[int]\n    variable_batch_size: bool = False\n    codecs: Optional[QuantizedCommCodecs] = None\n    permuted_lengths_after_sparse_data_all2all: Optional[Tensor] = None",
  "class All2AllVInfo(object):\n    \"\"\"\n    The data class that collects the attributes when calling the `alltoallv` operation.\n\n    Attributes:\n        dim_sum_per_rank (List[int]): number of features (sum of dimensions) of the\n            embedding in each rank.\n        B_global (int): global batch size for each rank.\n        B_local (int): local batch size before scattering.\n        B_local_list: (List[int]): local batch sizes for each embedding table locally\n            (in my current rank).\n        D_local_list (List[int]): embedding dimension of each embedding table locally\n            (in my current rank).\n        input_split_sizes (List[int]): The input split sizes for each rank, this\n            remembers how to split the input when doing the `all_to_all_single` operation.\n        output_split_sizes (List[int]): The output split sizes for each rank, this\n            remembers how to fill the output when doing the `all_to_all_single` operation.\n    \"\"\"\n\n    dims_sum_per_rank: List[int]\n    B_global: int\n    B_local: int\n    B_local_list: List[int]\n    D_local_list: List[int]\n    input_split_sizes: List[int] = field(default_factory=list)\n    output_split_sizes: List[int] = field(default_factory=list)\n    codecs: Optional[QuantizedCommCodecs] = None",
  "class ReduceScatterInfo(object):\n    \"\"\"\n    The data class that collects the attributes when calling the `reduce_scatter_pooled`\n    operation.\n\n    Attributes:\n        input_sizes (List[torch.Size]): the sizes of the input tensors. This remembers the\n            sizes of the input tensors when running the backward pass and producing the\n            gradient.\n    \"\"\"\n\n    input_sizes: List[torch.Size]\n    codecs: Optional[QuantizedCommCodecs] = None",
  "class ReduceScatterBaseInfo(object):\n    \"\"\"\n    The data class that collects the attributes when calling the\n    `reduce_scatter_base_pooled` operation.\n\n    Attributes:\n        input_sizes (torch.Size): the sizes of the input flatten tensor.\n    \"\"\"\n\n    input_sizes: torch.Size\n    codecs: Optional[QuantizedCommCodecs] = None",
  "class AllGatherBaseInfo(object):\n    \"\"\"\n    The data class that collects the attributes when calling the\n    `all_gatther_base_pooled` operation.\n\n    Attributes:\n        input_size (int): the size of the input tensor.\n    \"\"\"\n\n    input_size: torch.Size\n    codecs: Optional[QuantizedCommCodecs] = None",
  "class ReduceScatterVInfo(object):\n    \"\"\"\n    The data class that collects the attributes when calling the `reduce_scatter_v_pooled`\n    operation.\n\n    Attributes:\n        input_sizes (List[torch.Size]): the sizes of the input tensors. This saves the\n            sizes of the input tensors when running the backward pass and producing the\n            gradient.\n        input_splits (List[int]): the splits of the input tensors along dim 0.\n        total_input_size: (List[int]): total input size.\n    \"\"\"\n\n    input_sizes: List[torch.Size]\n    input_splits: List[int]\n    equal_splits: bool\n    total_input_size: List[int]\n    codecs: Optional[QuantizedCommCodecs]",
  "class All2AllDenseInfo(object):\n    \"\"\"\n    The data class that collects the attributes when calling the `alltoall_dense`\n    operation.\n    \"\"\"\n\n    output_splits: List[int]\n    batch_size: int\n    input_shape: List[int]\n    input_splits: List[int]",
  "def _get_split_lengths_by_len(\n    world_size: int, my_rank: int, n: int\n) -> Tuple[int, List[int]]:\n    k, m = divmod(n, world_size)\n    if m == 0:\n        splits = [k] * world_size\n        my_len = k\n    else:\n        splits = [(k + 1) if i < m else k for i in range(world_size)]\n        my_len = splits[my_rank]\n    return (my_len, splits)",
  "def alltoall_pooled(\n    a2a_pooled_embs_tensor: Tensor,\n    batch_size_per_rank: List[int],\n    dim_sum_per_rank: List[int],\n    dim_sum_per_rank_tensor: Optional[Tensor] = None,\n    cumsum_dim_sum_per_rank_tensor: Optional[Tensor] = None,\n    group: Optional[dist.ProcessGroup] = None,\n    codecs: Optional[QuantizedCommCodecs] = None,\n) -> Awaitable[Tensor]:\n    \"\"\"\n    Performs AlltoAll operation for a single pooled embedding tensor. Each process\n    splits the input pooled embeddings tensor based on the world size, and then scatters\n    the split list to all processes in the group. Then concatenates the received tensors\n    from all processes in the group and returns a single output tensor.\n\n    Args:\n        a2a_pooled_embs_tensor (Tensor): input pooled embeddings. Must be pooled\n            together before passing into this function. Its shape is `B x D_local_sum`,\n            where `D_local_sum` is the dimension sum of all the local embedding tables.\n        batch_size_per_rank (List[int]): batch size in each rank.\n        dim_sum_per_rank (List[int]): number of features (sum of dimensions) of the\n            embedding in each rank.\n        dim_sum_per_rank_tensor (Optional[Tensor]): the tensor version of\n            `dim_sum_per_rank`, this is only used by the fast kernel of\n            `_recat_pooled_embedding_grad_out`.\n        cumsum_dim_sum_per_rank_tensor (Optional[Tensor]): cumulative sum of\n            `dim_sum_per_rank`, this is only used by the fast kernel of\n            `_recat_pooled_embedding_grad_out`.\n        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the\n            default process group will be used.\n        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.\n\n    Returns:\n        Awaitable[Tensor]: async work handle (`Awaitable`), which can be `wait()` later to get the resulting tensor.\n\n    .. warning::\n        `alltoall_pooled` is experimental and subject to change.\n    \"\"\"\n\n    if group is None:\n        group = dist.distributed_c10d._get_default_group()\n\n    if dist.get_world_size(group) <= 1:\n        return NoWait(a2a_pooled_embs_tensor)\n\n    myreq = Request(group, device=a2a_pooled_embs_tensor.device)\n    a2ai = All2AllPooledInfo(\n        batch_size_per_rank=batch_size_per_rank,\n        dim_sum_per_rank=dim_sum_per_rank,\n        dim_sum_per_rank_tensor=dim_sum_per_rank_tensor,\n        cumsum_dim_sum_per_rank_tensor=cumsum_dim_sum_per_rank_tensor,\n        codecs=codecs,\n    )\n    All2All_Pooled_Req.apply(group, myreq, a2ai, a2a_pooled_embs_tensor)\n    return myreq",
  "def variable_batch_alltoall_pooled(\n    a2a_pooled_embs_tensor: Tensor,\n    batch_size_per_rank_per_feature: List[List[int]],\n    batch_size_per_feature_pre_a2a: List[int],\n    emb_dim_per_rank_per_feature: List[List[int]],\n    group: Optional[dist.ProcessGroup] = None,\n    codecs: Optional[QuantizedCommCodecs] = None,\n) -> Awaitable[Tensor]:\n\n    if group is None:\n        group = dist.distributed_c10d._get_default_group()\n\n    if dist.get_world_size(group) <= 1:\n        return NoWait(a2a_pooled_embs_tensor)\n\n    myreq = Request(group, device=a2a_pooled_embs_tensor.device)\n    a2ai = VariableBatchAll2AllPooledInfo(\n        batch_size_per_rank_per_feature=batch_size_per_rank_per_feature,\n        batch_size_per_feature_pre_a2a=batch_size_per_feature_pre_a2a,\n        emb_dim_per_rank_per_feature=emb_dim_per_rank_per_feature,\n        codecs=codecs,\n    )\n    Variable_Batch_All2All_Pooled_Req.apply(group, myreq, a2ai, a2a_pooled_embs_tensor)\n    return myreq",
  "def alltoall_sequence(\n    # (T, B, L_i * D) flattened\n    a2a_sequence_embs_tensor: Tensor,\n    forward_recat_tensor: Tensor,\n    backward_recat_tensor: Tensor,\n    lengths_after_sparse_data_all2all: Tensor,\n    input_splits: List[int],\n    output_splits: List[int],\n    variable_batch_size: bool = False,\n    group: Optional[dist.ProcessGroup] = None,\n    codecs: Optional[QuantizedCommCodecs] = None,\n) -> Awaitable[Tensor]:\n    \"\"\"\n    Performs AlltoAll operation for sequence embeddings. Each process splits the input\n    tensor based on the world size, and then scatters the split list to all processes in\n    the group. Then concatenates the received tensors from all processes in the group\n    and returns a single output tensor.\n\n    NOTE:\n        AlltoAll operator for Sequence embedding tensors.\n        Does not support mixed dimensions.\n\n    Args:\n        a2a_sequence_embs_tensor (Tensor): input embeddings.\n        forward_recat_tensor (Tensor): recat tensor for forward.\n        backward_recat_tensor (Tensor): recat tensor for backward.\n        lengths_after_sparse_data_all2all (Tensor): lengths of sparse features after\n            AlltoAll.\n        input_splits (List[int]): input splits.\n        output_splits (List[int]): output splits.\n        variable_batch_size (bool): whether variable batch size is enabled.\n        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the\n            default process group will be used.\n        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.\n\n    Returns:\n        Awaitable[List[Tensor]]: async work handle (`Awaitable`), which can be `wait()` later to get the resulting tensor.\n\n    .. warning::\n        `alltoall_sequence` is experimental and subject to change.\n    \"\"\"\n\n    if group is None:\n        group = dist.distributed_c10d._get_default_group()\n\n    if dist.get_world_size(group) <= 1:\n        return NoWait(a2a_sequence_embs_tensor)\n\n    myreq = Request(group, device=a2a_sequence_embs_tensor.device)\n    a2ai = All2AllSequenceInfo(\n        embedding_dim=a2a_sequence_embs_tensor.shape[1],\n        lengths_after_sparse_data_all2all=lengths_after_sparse_data_all2all,\n        forward_recat_tensor=forward_recat_tensor,\n        backward_recat_tensor=backward_recat_tensor,\n        input_splits=input_splits,\n        output_splits=output_splits,\n        variable_batch_size=variable_batch_size,\n        codecs=codecs,\n    )\n    # sequence of embeddings, bags are definitely non-uniform\n\n    All2All_Seq_Req.apply(group, myreq, a2ai, a2a_sequence_embs_tensor)\n    return myreq",
  "def alltoallv(\n    inputs: List[Tensor],\n    out_split: Optional[List[int]] = None,\n    per_rank_split_lengths: Optional[List[int]] = None,\n    group: Optional[dist.ProcessGroup] = None,\n    codecs: Optional[QuantizedCommCodecs] = None,\n) -> Awaitable[List[Tensor]]:\n    \"\"\"\n    Performs `alltoallv` operation for a list of input embeddings. Each process scatters\n    the list to all processes in the group.\n\n    Args:\n        inputs (List[Tensor]): list of tensors to scatter, one per rank. The tensors in\n            the list usually have different lengths.\n        out_split (Optional[List[int]]): output split sizes (or dim_sum_per_rank), if\n            not specified, we will use `per_rank_split_lengths` to construct a output\n            split with the assumption that all the embs have the same dimension.\n        per_rank_split_lengths (Optional[List[int]]): split lengths per rank. If not\n            specified, the `out_split` must be specified.\n        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the\n            default process group will be used.\n        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.\n\n    Returns:\n        Awaitable[List[Tensor]]: async work handle (`Awaitable`), which can be `wait()` later to get the resulting list of tensors.\n\n    .. warning::\n        `alltoallv` is experimental and subject to change.\n    \"\"\"\n\n    if group is None:\n        group = dist.distributed_c10d._get_default_group()\n\n    world_size = dist.get_world_size(group)\n    my_rank = dist.get_rank(group)\n\n    myreq = Request(group, device=inputs[0].device)\n    B_global, _ = inputs[0].size()\n    D_local_list = [e.size()[1] for e in inputs]\n    B_local, B_local_list = _get_split_lengths_by_len(world_size, my_rank, B_global)\n\n    if out_split is not None:\n        dims_sum_per_rank = out_split\n    elif per_rank_split_lengths is not None:\n        # all the embs have the same dimension\n        dims_sum_per_rank = [s * D_local_list[0] for s in per_rank_split_lengths]\n    else:\n        raise RuntimeError(\"Need to specify either out_split or per_rank_split_lengths\")\n\n    a2ai = All2AllVInfo(\n        dims_sum_per_rank=dims_sum_per_rank,\n        B_local=B_local,\n        B_local_list=B_local_list,\n        D_local_list=D_local_list,\n        B_global=B_global,\n        codecs=codecs,\n    )\n\n    All2Allv_Req.apply(group, myreq, a2ai, inputs)\n\n    return myreq",
  "def reduce_scatter_pooled(\n    inputs: List[Tensor],\n    group: Optional[dist.ProcessGroup] = None,\n    codecs: Optional[QuantizedCommCodecs] = None,\n) -> Awaitable[Tensor]:\n    \"\"\"\n    Performs reduce-scatter operation for a pooled embeddings tensor split into world\n    size number of chunks. The result of the reduce operation gets scattered to all\n    processes in the group.\n\n    Args:\n        inputs (List[Tensor]): list of tensors to scatter, one per rank.\n        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the\n            default process group will be used.\n        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.\n\n    Returns:\n        Awaitable[Tensor]: async work handle (Awaitable), which can be `wait()` later to get the resulting tensor.\n\n    .. warning::\n        `reduce_scatter_pooled` is experimental and subject to change.\n    \"\"\"\n\n    if group is None:\n        group = dist.distributed_c10d._get_default_group()\n\n    if dist.get_world_size(group) <= 1:\n        return NoWait(inputs[dist.get_rank(group)])\n\n    myreq = Request(group, device=inputs[0].device)\n    rsi = ReduceScatterInfo(\n        input_sizes=[tensor.size() for tensor in inputs], codecs=codecs\n    )\n    ReduceScatter_Req.apply(group, myreq, rsi, *inputs)\n    return myreq",
  "def reduce_scatter_base_pooled(\n    input: Tensor,\n    group: Optional[dist.ProcessGroup] = None,\n    codecs: Optional[QuantizedCommCodecs] = None,\n) -> Awaitable[Tensor]:\n    \"\"\"\n    Reduces then scatters a flattened pooled embeddings tensor to all processes in a\n    group.\n    Input tensor is of size `output_tensor_size * world_size`.\n\n    Args:\n        input (Tensor): flattened tensor to scatter.\n        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the\n            default process group will be used.\n        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.\n\n    Returns:\n        Awaitable[Tensor]: async work handle (Awaitable), which can be `wait()` later to get the resulting tensor.\n\n    .. warning::\n        `reduce_scatter_base_pooled` is experimental and subject to change.\n    \"\"\"\n\n    if group is None:\n        group = dist.distributed_c10d._get_default_group()\n\n    if dist.get_world_size(group) <= 1:\n        return NoWait(input)\n\n    myreq = Request(group, device=input.device)\n    rsi = ReduceScatterBaseInfo(input_sizes=input.size(), codecs=codecs)\n    ReduceScatterBase_Req.apply(group, myreq, rsi, input)\n    return myreq",
  "def all_gather_base_pooled(\n    input: Tensor,\n    group: Optional[dist.ProcessGroup] = None,\n    codecs: Optional[QuantizedCommCodecs] = None,\n) -> Awaitable[Tensor]:\n    \"\"\"\n    All-gathers tensors from all processes in a group to form a flattened pooled\n    embeddings tensor.\n    Input tensor is of size `output_tensor_size / world_size`.\n\n    Args:\n        input (Tensor): tensor to gather.\n        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the\n            default process group will be used.\n\n    Returns:\n        Awaitable[Tensor]: async work handle (Awaitable), which can be `wait()` later to get the resulting tensor.\n\n    .. warning::\n        `all_gather_base_pooled` is experimental and subject to change.\n    \"\"\"\n\n    if group is None:\n        group = dist.distributed_c10d._get_default_group()\n\n    if dist.get_world_size(group) <= 1:\n        return NoWait(input)\n\n    myreq = Request(group, device=input.device)\n    agi = AllGatherBaseInfo(input_size=input.size(), codecs=codecs)\n    AllGatherBase_Req.apply(group, myreq, agi, input)\n    return myreq",
  "def reduce_scatter_v_pooled(\n    input: Tensor,\n    input_splits: List[int],\n    group: Optional[dist.ProcessGroup] = None,\n    codecs: Optional[QuantizedCommCodecs] = None,\n) -> Awaitable[Tensor]:\n    \"\"\"\n    Performs reduce-scatter-v operation for a pooled embeddings tensor split unevenly\n    into world size number of chunks. The result of the reduce operation gets scattered\n    to all processes in the group according to `input_splits`.\n\n    Args:\n        input (Tensor): tensor to scatter.\n        input_splits (List[int]): input splits.\n        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the\n            default process group will be used.\n\n    Returns:\n        Awaitable[Tensor]: async work handle (Awaitable), which can be `wait()` later to get the resulting tensor.\n\n    .. warning::\n        `reduce_scatter_v_pooled` is experimental and subject to change.\n    \"\"\"\n\n    if group is None:\n        group = dist.distributed_c10d._get_default_group()\n\n    if dist.get_world_size(group) <= 1:\n        return NoWait(input)\n\n    myreq = Request(group, device=input.device)\n    input_size = list(input.size())\n    input_sizes = [\n        torch.Size(\n            [ip_split if d == 0 else input_size[d] for d in range(len(input_size))]\n        )\n        for ip_split in input_splits\n    ]\n    equal_splits = all(ip_split == input_splits[0] for ip_split in input_splits)\n\n    rsvi = ReduceScatterVInfo(\n        input_sizes=input_sizes,\n        input_splits=input_splits,\n        equal_splits=equal_splits,\n        total_input_size=input_size,\n        codecs=codecs,\n    )\n    ReduceScatterV_Req.apply(group, myreq, rsvi, input)\n    return myreq",
  "def _recat_pooled_embedding_grad_out(\n    grad_output: Tensor, num_features_per_rank: List[int]\n) -> Tensor:\n    grad_outputs_by_rank = grad_output.split(num_features_per_rank, dim=1)\n    return torch.cat(\n        [\n            grad_output_by_rank.contiguous().view(-1)\n            for grad_output_by_rank in grad_outputs_by_rank\n        ],\n        dim=0,\n    )",
  "def _recat_seq_embedding(\n    input_embeddings: Tensor,\n    split_sizes: List[int],\n    T_local: int,\n    my_size: int,\n    forward: bool,\n) -> Tensor:\n    seq_embeddings_by_rank = input_embeddings.split(split_sizes)\n    if forward:\n        return torch.cat(\n            [\n                seq_embeddings_by_rank[t * my_size + i]\n                # .contiguous().view(-1)\n                for i in range(my_size)\n                for t in range(T_local)\n            ],\n            dim=0,\n        )\n    else:\n        return torch.cat(\n            [\n                seq_embeddings_by_rank[i * T_local + t]\n                # .contiguous()\n                # .view(-1)\n                for t in range(T_local)\n                for i in range(my_size)\n            ],\n            dim=0,\n        )",
  "class All2All_Pooled_Req(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        a2ai: All2AllPooledInfo,\n        input_embeddings: Tensor,\n    ) -> Tensor:\n        my_rank = dist.get_rank(pg)\n        (B_global, D_local_sum) = input_embeddings.shape\n\n        dim_sum_per_rank = a2ai.dim_sum_per_rank\n        batch_size_per_rank = a2ai.batch_size_per_rank\n        B_local = batch_size_per_rank[my_rank]\n\n        assert B_global == sum(batch_size_per_rank)\n\n        sharded_input_embeddings = input_embeddings.view(-1)\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.forward.create_context()\n            sharded_input_embeddings = codecs.forward.encode(\n                sharded_input_embeddings,\n                qcomm_ctx,\n            )\n            output_split_sizes = [\n                codecs.forward.calc_quantized_size(\n                    B_local * D_rank_sum,\n                    qcomm_ctx,\n                )\n                for D_rank_sum in dim_sum_per_rank\n            ]\n            input_split_sizes = [\n                codecs.forward.calc_quantized_size(\n                    D_local_sum * B_rank,\n                    qcomm_ctx,\n                )\n                for B_rank in batch_size_per_rank\n            ]\n        else:\n            output_split_sizes = [\n                B_local * D_rank_sum for D_rank_sum in dim_sum_per_rank\n            ]\n            input_split_sizes = [D_local_sum * B_rank for B_rank in batch_size_per_rank]\n            qcomm_ctx = None\n\n        sharded_output_embeddings = torch.empty(\n            sum(output_split_sizes),\n            dtype=sharded_input_embeddings.dtype,\n            device=sharded_input_embeddings.device,\n        )\n\n        with record_function(\"## alltoall_fwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_output_embeddings,\n                input=sharded_input_embeddings,\n                output_split_sizes=output_split_sizes,\n                input_split_sizes=input_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n\n        myreq.req = req\n        myreq.tensor = sharded_output_embeddings\n        myreq.qcomm_ctx = qcomm_ctx\n        myreq.a2ai = a2ai\n        myreq.wait_function = All2All_Pooled_Wait\n        ctx.myreq = myreq\n        ctx.pg = pg\n        return myreq.dummy_tensor\n\n    @staticmethod\n    # pyre-fixme[2]: Parameter must be annotated.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, *unused) -> Tuple[None, None, None, Tensor]:\n        pg = ctx.pg\n        my_rank = dist.get_rank(pg)\n        myreq = ctx.myreq\n        a2ai = myreq.a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        grad_output = myreq.tensor\n        dim_sum_per_rank = a2ai.dim_sum_per_rank\n        batch_size_per_rank = a2ai.batch_size_per_rank\n        D_local_sum = dim_sum_per_rank[my_rank]\n        B_global = sum(batch_size_per_rank)\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            grad_input = codecs.backward.decode(grad_output, myreq.qcomm_ctx)\n            grad_input = grad_input.view(B_global, D_local_sum)\n        else:\n            grad_input = grad_output.view(B_global, D_local_sum)\n        if GRADIENT_DIVISION:\n            grad_input.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, grad_input)",
  "class All2All_Pooled_Wait(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: Tensor,\n    ) -> Tensor:\n        my_rank = dist.get_rank(pg)\n        a2ai = myreq.a2ai\n        ctx.a2ai = a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        sharded_output_embeddings = myreq.tensor\n        myreq.req = None\n        myreq.tensor = None\n        ctx.pg = pg\n        ctx.myreq = myreq\n        dim_sum_per_rank = a2ai.dim_sum_per_rank\n        batch_size_per_rank = a2ai.batch_size_per_rank\n        B_local = batch_size_per_rank[my_rank]\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            sharded_output_embeddings = codecs.forward.decode(\n                sharded_output_embeddings,\n                myreq.qcomm_ctx,\n            )\n\n        outputs_by_rank = sharded_output_embeddings.split(\n            [B_local * D_rank_sum for D_rank_sum in dim_sum_per_rank]\n        )\n        result = torch.cat(\n            [output.view(B_local, -1) for output in outputs_by_rank], dim=1\n        )\n        return result\n\n    @staticmethod\n    # pyre-fixme[14]: `backward` overrides method defined in `Function` inconsistently.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        a2ai = ctx.a2ai\n        pg = ctx.pg\n        my_rank = dist.get_rank(pg)\n        dim_sum_per_rank = a2ai.dim_sum_per_rank\n        batch_size_per_rank = a2ai.batch_size_per_rank\n\n        D_local_sum = dim_sum_per_rank[my_rank]\n        (B_local, D_global_sum) = grad_output.shape\n        assert sum(dim_sum_per_rank) == D_global_sum\n\n        sharded_grad_output = _recat_pooled_embedding_grad_out(\n            grad_output.contiguous(),\n            dim_sum_per_rank,\n        )\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.backward.create_context()\n            sharded_grad_output = codecs.backward.encode(\n                sharded_grad_output,\n                qcomm_ctx,\n            )\n            input_split_sizes = [\n                codecs.backward.calc_quantized_size(\n                    B_local * D_rank_sum,\n                    qcomm_ctx,\n                )\n                for D_rank_sum in dim_sum_per_rank\n            ]\n            output_split_sizes = [\n                codecs.backward.calc_quantized_size(\n                    D_local_sum * B_rank,\n                    qcomm_ctx,\n                )\n                for B_rank in batch_size_per_rank\n            ]\n        else:\n            qcomm_ctx = None\n            input_split_sizes = [\n                B_local * D_rank_sum for D_rank_sum in dim_sum_per_rank\n            ]\n            output_split_sizes = [\n                D_local_sum * B_rank for B_rank in batch_size_per_rank\n            ]\n\n        sharded_grad_input = torch.empty(\n            sum(output_split_sizes),\n            device=sharded_grad_output.device,\n            dtype=sharded_grad_output.dtype,\n        )\n        with record_function(\"## alltoall_bwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_grad_input,\n                input=sharded_grad_output,\n                output_split_sizes=output_split_sizes,\n                input_split_sizes=input_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = sharded_grad_input\n        myreq.qcomm_ctx = qcomm_ctx\n\n        return (None, None, myreq.dummy_tensor)",
  "class Variable_Batch_All2All_Pooled_Req(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        a2ai: VariableBatchAll2AllPooledInfo,\n        input_embeddings: Tensor,\n    ) -> Tensor:\n        my_rank = dist.get_rank(pg)\n\n        # get input splits\n        world_size = dist.get_world_size(pg)\n        input_split_sizes = [0 for _ in range(world_size)]\n        if a2ai.batch_size_per_rank_per_feature:\n            for i in range(world_size):\n                curr_size = 0\n                for batch_size, emb_dim in zip(\n                    a2ai.batch_size_per_rank_per_feature[i],\n                    a2ai.emb_dim_per_rank_per_feature[my_rank],\n                ):\n                    curr_size += batch_size * emb_dim\n                input_split_sizes[i] = curr_size\n        a2ai.input_splits = input_split_sizes\n\n        # get output splits\n        output_split_sizes = [0 for _ in range(world_size)]\n        ind = 0\n        for i in range(world_size):\n            curr_size = 0\n            for emb_dim in a2ai.emb_dim_per_rank_per_feature[i]:\n                curr_size += a2ai.batch_size_per_feature_pre_a2a[ind] * emb_dim\n                ind += 1\n            output_split_sizes[i] = curr_size\n        a2ai.output_splits = output_split_sizes\n\n        sharded_input_embeddings = input_embeddings.view(-1)\n        qcomm_ctx = None\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.forward.create_context()\n            sharded_input_embeddings = codecs.forward.encode(\n                sharded_input_embeddings,\n                qcomm_ctx,\n            )\n            output_split_sizes = [\n                codecs.forward.calc_quantized_size(\n                    split,\n                    qcomm_ctx,\n                )\n                for split in output_split_sizes\n            ]\n            input_split_sizes = [\n                codecs.forward.calc_quantized_size(\n                    split,\n                    qcomm_ctx,\n                )\n                for split in input_split_sizes\n            ]\n\n        sharded_output_embeddings = torch.empty(\n            sum(output_split_sizes),\n            dtype=sharded_input_embeddings.dtype,\n            device=sharded_input_embeddings.device,\n        )\n\n        with record_function(\"## alltoall_fwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_output_embeddings,\n                input=sharded_input_embeddings,\n                output_split_sizes=output_split_sizes,\n                input_split_sizes=input_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n\n        myreq.req = req\n        myreq.tensor = sharded_output_embeddings\n        myreq.qcomm_ctx = qcomm_ctx\n        myreq.a2ai = a2ai\n        myreq.wait_function = Variable_Batch_All2All_Pooled_Wait\n        ctx.myreq = myreq\n        ctx.pg = pg\n        return myreq.dummy_tensor\n\n    @staticmethod\n    # pyre-fixme[2]: Parameter must be annotated.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, *unused) -> Tuple[None, None, None, Tensor]:\n        myreq = ctx.myreq\n        a2ai = myreq.a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        grad_output = myreq.tensor\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            grad_input = codecs.backward.decode(grad_output, myreq.qcomm_ctx)\n        else:\n            grad_input = grad_output\n        if GRADIENT_DIVISION:\n            grad_input.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, grad_input)",
  "class Variable_Batch_All2All_Pooled_Wait(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: Tensor,\n    ) -> Tensor:\n        a2ai = myreq.a2ai\n        ctx.a2ai = a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        sharded_output_embeddings = myreq.tensor\n        myreq.req = None\n        myreq.tensor = None\n        ctx.pg = pg\n        ctx.myreq = myreq\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            sharded_output_embeddings = codecs.forward.decode(\n                sharded_output_embeddings,\n                myreq.qcomm_ctx,\n            )\n        # the return result is a 1-d tensor, like: f_0_s_0, f_0_s1, ..., f_n_s_0, f_n_s_k\n        # f_0, f_1, ... , f_n are ordered by features on each rank\n        return sharded_output_embeddings\n\n    @staticmethod\n    # pyre-fixme[14]: `backward` overrides method defined in `Function` inconsistently.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        a2ai = ctx.a2ai\n        pg = ctx.pg\n\n        assert a2ai.input_splits is not None\n        assert a2ai.output_splits is not None\n        input_split_sizes = a2ai.output_splits\n        output_split_sizes = a2ai.input_splits\n\n        sharded_grad_output = grad_output.contiguous()\n        qcomm_ctx = None\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.backward.create_context()\n            sharded_grad_output = codecs.backward.encode(\n                sharded_grad_output,\n                qcomm_ctx,\n            )\n            input_split_sizes = [\n                codecs.backward.calc_quantized_size(\n                    split,\n                    qcomm_ctx,\n                )\n                for split in input_split_sizes\n            ]\n            output_split_sizes = [\n                codecs.backward.calc_quantized_size(\n                    split,\n                    qcomm_ctx,\n                )\n                for split in output_split_sizes\n            ]\n\n        sharded_grad_input = torch.empty(\n            sum(output_split_sizes),\n            device=sharded_grad_output.device,\n            dtype=sharded_grad_output.dtype,\n        )\n        with record_function(\"## alltoall_bwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_grad_input,\n                input=sharded_grad_output,\n                output_split_sizes=output_split_sizes,\n                input_split_sizes=input_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = sharded_grad_input\n        myreq.qcomm_ctx = qcomm_ctx\n\n        return (None, None, myreq.dummy_tensor)",
  "class All2All_Seq_Req(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        a2ai: All2AllSequenceInfo,\n        sharded_input_embeddings: Tensor,\n    ) -> Tensor:\n        world_size = dist.get_world_size(pg)\n        my_rank = dist.get_rank(pg)\n        D = a2ai.embedding_dim\n        forward_recat_tensor = a2ai.forward_recat_tensor\n        variable_batch_size = a2ai.variable_batch_size\n        lengths_after_sparse_data_all2all = a2ai.lengths_after_sparse_data_all2all * D\n        input_splits = [i * D for i in a2ai.output_splits]\n        output_splits = [i * D for i in a2ai.input_splits]\n\n        a2ai.input_splits = input_splits\n        a2ai.output_splits = output_splits\n\n        local_T = lengths_after_sparse_data_all2all.shape[0]\n        if local_T > 0:\n            with record_function(\"## alltoall_seq_embedding_fwd_permute ##\"):\n                if not variable_batch_size:\n                    (\n                        permuted_lengths_after_sparse_data_all2all,\n                        sharded_input_embeddings,\n                        _,\n                    ) = torch.ops.fbgemm.permute_2D_sparse_data(\n                        forward_recat_tensor,\n                        lengths_after_sparse_data_all2all.view(\n                            local_T * world_size, -1\n                        ),\n                        sharded_input_embeddings.view(-1),\n                        None,\n                        sharded_input_embeddings.numel(),\n                    )\n                else:\n                    (\n                        permuted_lengths_after_sparse_data_all2all,\n                        sharded_input_embeddings,\n                        _,\n                    ) = torch.ops.fbgemm.permute_1D_sparse_data(\n                        forward_recat_tensor,\n                        lengths_after_sparse_data_all2all.view(-1),\n                        sharded_input_embeddings.view(-1),\n                        None,\n                        sharded_input_embeddings.numel(),\n                    )\n        else:\n            permuted_lengths_after_sparse_data_all2all = None\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.forward.create_context()\n            # pyre-ignore [16]\n            sharded_input_embeddings = a2ai.codecs.forward.encode(\n                sharded_input_embeddings, qcomm_ctx\n            )\n            output_splits = [\n                a2ai.codecs.forward.calc_quantized_size(x, qcomm_ctx)\n                for x in output_splits\n            ]\n            input_splits = [\n                a2ai.codecs.forward.calc_quantized_size(x, qcomm_ctx)\n                for x in input_splits\n            ]\n        else:\n            qcomm_ctx = None\n\n        sharded_output_embeddings = torch.empty(\n            sum(output_splits),\n            dtype=sharded_input_embeddings.dtype,\n            device=sharded_input_embeddings.device,\n        )\n\n        with record_function(\"## alltoall_seq_embedding_fwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_output_embeddings,\n                input=sharded_input_embeddings,\n                output_split_sizes=output_splits,\n                input_split_sizes=input_splits,\n                group=pg,\n                async_op=True,\n            )\n        a2ai.permuted_lengths_after_sparse_data_all2all = (\n            permuted_lengths_after_sparse_data_all2all\n        )\n        myreq.req = req\n        myreq.tensor = sharded_output_embeddings\n        myreq.a2ai = a2ai\n        myreq.wait_function = All2All_Seq_Req_Wait\n        ctx.myreq = myreq\n        myreq.qcomm_ctx = qcomm_ctx\n        ctx.pg = pg\n        ctx.my_rank = my_rank\n        ctx.world_size = world_size\n        return myreq.dummy_tensor\n\n    @staticmethod\n    # pyre-fixme[2]: Parameter must be annotated.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, *unused) -> Tuple[None, None, None, Tensor]:\n        myreq = ctx.myreq\n        a2ai = myreq.a2ai\n        D = a2ai.embedding_dim\n        variable_batch_size = a2ai.variable_batch_size\n        backward_recat_tensor = a2ai.backward_recat_tensor\n        permuted_lengths_after_sparse_data_all2all = (\n            a2ai.permuted_lengths_after_sparse_data_all2all\n        )\n        assert myreq.req is not None\n        myreq.req.wait()\n        sharded_grad_input = myreq.tensor\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            sharded_grad_input = codecs.backward.decode(\n                sharded_grad_input, myreq.qcomm_ctx\n            )\n        myreq.req = None\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n\n        if permuted_lengths_after_sparse_data_all2all is not None:\n            with record_function(\"## alltoall_seq_embedding_bwd_permute ##\"):\n                if not variable_batch_size:\n                    _, sharded_grad_input, _ = torch.ops.fbgemm.permute_2D_sparse_data(\n                        backward_recat_tensor,\n                        permuted_lengths_after_sparse_data_all2all,\n                        sharded_grad_input,\n                        None,\n                        sharded_grad_input.numel(),\n                    )\n                else:\n                    _, sharded_grad_input, _ = torch.ops.fbgemm.permute_1D_sparse_data(\n                        backward_recat_tensor,\n                        permuted_lengths_after_sparse_data_all2all,\n                        sharded_grad_input,\n                        None,\n                        sharded_grad_input.numel(),\n                    )\n        if GRADIENT_DIVISION:\n            sharded_grad_input.div_(dist.get_world_size(ctx.pg))\n        return (None, None, None, sharded_grad_input.view(-1, D))",
  "class All2All_Seq_Req_Wait(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: torch.Tensor,\n    ) -> Tensor:\n        a2ai = myreq.a2ai\n        D = a2ai.embedding_dim\n        ctx.a2ai = a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        sharded_output_embeddings = myreq.tensor\n        myreq.tensor = None\n        ctx.pg = pg\n        ctx.myreq = myreq\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            sharded_output_embeddings = codecs.forward.decode(\n                sharded_output_embeddings, myreq.qcomm_ctx\n            )\n        return sharded_output_embeddings.view(-1, D)\n\n    @staticmethod\n    # pyre-fixme[14]: `backward` overrides method defined in `Function` inconsistently.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, sharded_grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        a2ai = ctx.a2ai\n        pg = ctx.pg\n        input_splits = a2ai.output_splits\n        output_splits = a2ai.input_splits\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.backward.create_context()\n            sharded_grad_output = a2ai.codecs.backward.encode(\n                sharded_grad_output, qcomm_ctx\n            )\n            output_splits = [\n                a2ai.codecs.backward.calc_quantized_size(x, qcomm_ctx)\n                for x in output_splits\n            ]\n            input_splits = [\n                a2ai.codecs.backward.calc_quantized_size(x, qcomm_ctx)\n                for x in input_splits\n            ]\n        else:\n            qcomm_ctx = None\n\n        sharded_grad_input = torch.empty(\n            sum(output_splits),\n            device=sharded_grad_output.device,\n            dtype=sharded_grad_output.dtype,\n        )\n        with record_function(\"## alltoall_seq_embedding_bwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_grad_input,\n                input=sharded_grad_output.view(-1),\n                output_split_sizes=output_splits,\n                input_split_sizes=input_splits,\n                group=pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = sharded_grad_input\n        myreq.qcomm_ctx = qcomm_ctx\n\n        return (None, None, myreq.dummy_tensor)",
  "class All2Allv_Req(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        a2ai: All2AllVInfo,\n        inputs: List[Tensor],\n    ) -> Tensor:\n        input_split_sizes = [m * sum(a2ai.D_local_list) for m in a2ai.B_local_list]\n        output_split_sizes = [a2ai.B_local * e for e in a2ai.dims_sum_per_rank]\n        input = torch.cat(inputs, dim=1).view([-1])\n        if a2ai.codecs is not None:\n            input = a2ai.codecs.forward.encode(input)\n\n        output = input.new_empty(sum(output_split_sizes))\n        with record_function(\"## alltoallv_bwd_single ##\"):\n            req = dist.all_to_all_single(\n                output,\n                input,\n                output_split_sizes,\n                input_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n\n        myreq.req = req\n        myreq.tensor = output\n        myreq.wait_function = All2Allv_Wait\n        a2ai.input_split_sizes = input_split_sizes\n        a2ai.output_split_sizes = output_split_sizes\n        myreq.a2ai = a2ai\n        ctx.a2ai = a2ai\n        ctx.myreq = myreq\n        ctx.tensor = output\n        return myreq.dummy_tensor\n\n    @staticmethod\n    # pyre-fixme[3]: Return type must be annotated.\n    # pyre-fixme[2]: Parameter must be annotated.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, *grad_output):\n        a2ai = ctx.a2ai\n        myreq = ctx.myreq\n        myreq.req.wait()\n        myreq.req = None\n        grad_input = myreq.tensor\n        if a2ai.codecs is not None:\n            grad_input = a2ai.codecs.backward.decode(grad_input)\n\n        grad_inputs = grad_input.view([a2ai.B_global, -1]).split(\n            a2ai.D_local_list, dim=1\n        )\n        grad_inputs = [gin.contiguous() for gin in grad_inputs]\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, *grad_inputs)",
  "class All2Allv_Wait(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: torch.Tensor,\n    ) -> Tuple[Tensor]:\n        a2ai = myreq.a2ai\n        ctx.a2ai = a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        output = myreq.tensor\n        myreq.tensor = None\n        ctx.pg = pg\n        ctx.myreq = myreq\n\n        if a2ai.codecs is not None:\n            output = a2ai.codecs.forward.decode(output)\n        outputs = tuple(\n            [\n                out.view([a2ai.B_local, -1])\n                for out in output.split(a2ai.output_split_sizes)\n            ]\n        )\n        return outputs\n\n    @staticmethod\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, *grad_outputs) -> Tuple[None, None, Tensor]:\n        pg = ctx.pg\n        myreq = ctx.myreq\n        a2ai = ctx.a2ai\n\n        if a2ai.codecs is not None:\n            grad_outputs = a2ai.codecs.backward.encode(grad_outputs)\n\n        grad_outputs = [gout.contiguous().view([-1]) for gout in grad_outputs]\n        grad_output = torch.cat(grad_outputs)\n        grad_input = grad_output.new_empty([a2ai.B_global * sum(a2ai.D_local_list)])\n        with record_function(\"## alltoall_bwd_single ##\"):\n            req = dist.all_to_all_single(\n                grad_input,\n                grad_output,\n                a2ai.input_split_sizes,\n                a2ai.output_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = grad_input\n        return (None, None, myreq.dummy_tensor)",
  "class ReduceScatter_Req(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        rsi: ReduceScatterInfo,\n        *inputs: Any,\n    ) -> Tensor:\n        my_rank = dist.get_rank(pg)\n\n        if rsi.codecs is not None:\n            # pyre-ignore\n            inputs = [rsi.codecs.forward.encode(input) for input in inputs]\n\n        output = inputs[my_rank].new_empty(\n            inputs[my_rank].size(),\n            dtype=inputs[my_rank].dtype,\n            device=inputs[my_rank].device,\n        )\n        with record_function(\"## reduce_scatter ##\"):\n            req = dist.reduce_scatter(output, list(inputs), group=pg, async_op=True)\n        myreq.req = req\n        myreq.tensor = output\n        myreq.wait_function = ReduceScatter_Wait\n        myreq.rsi = rsi\n        ctx.myreq = myreq\n        ctx.pg = pg\n        ctx.tensor = output\n        return myreq.dummy_tensor\n\n    @staticmethod\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, *unused: Tensor) -> Tuple[Optional[Tensor], ...]:\n        myreq = ctx.myreq\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        grad_inputs = list(myreq.tensor)\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            grad_inputs = [\n                rsi.codecs.backward.decode(grad_input) for grad_input in grad_inputs\n            ]\n        # Make it equivalent to running on a single rank.\n        if GRADIENT_DIVISION:\n            for grad_input in grad_inputs:\n                grad_input.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor\n        return (None, None, None, *grad_inputs)",
  "class ReduceScatter_Wait(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: Tensor,\n    ) -> Tensor:\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        output = myreq.tensor\n        myreq.tensor = None\n        ctx.myreq = myreq\n        ctx.pg = pg\n\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            output = rsi.codecs.forward.decode(output)\n        return output\n\n    @staticmethod\n    # pyre-fixme[14]: `backward` overrides method defined in `Function` inconsistently.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            grad_output = rsi.codecs.backward.encode(grad_output)\n\n        grad_inputs = [\n            grad_output.new_empty(\n                in_size,\n                dtype=grad_output.dtype,\n                device=grad_output.device,\n            )\n            for in_size in rsi.input_sizes\n        ]\n\n        with record_function(\"## reduce_scatter_bw (all_gather) ##\"):\n            req = dist.all_gather(\n                grad_inputs,\n                grad_output.contiguous(),\n                group=ctx.pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = grad_inputs\n        return (None, None, myreq.dummy_tensor)",
  "class ReduceScatterBase_Req(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        rsi: ReduceScatterBaseInfo,\n        inputs: Tensor,\n    ) -> Tensor:\n        my_size = dist.get_world_size(pg)\n        assert inputs.size(0) % my_size == 0\n        if rsi.codecs is not None:\n            inputs = rsi.codecs.forward.encode(inputs)\n        output = inputs.new_empty((inputs.size(0) // my_size, inputs.size(1)))\n        with record_function(\"## reduce_scatter_base ##\"):\n            req = dist._reduce_scatter_base(output, inputs, group=pg, async_op=True)\n        myreq.req = req\n        myreq.tensor = output\n        myreq.wait_function = ReduceScatterBase_Wait\n        myreq.rsi = rsi\n        myreq.tensor = output\n        ctx.myreq = myreq\n        ctx.pg = pg\n\n        return myreq.dummy_tensor\n\n    @staticmethod\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, *unused: Tensor) -> Tuple[Optional[Tensor], ...]:\n        myreq = ctx.myreq\n        myreq.req.wait()\n        myreq.req = None\n        grad_inputs = myreq.tensor\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            grad_inputs = rsi.codecs.backward.decode(grad_inputs)\n        # Make it equivalent to running on a single rank.\n        if GRADIENT_DIVISION:\n            grad_inputs.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, grad_inputs)",
  "class ReduceScatterBase_Wait(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_Tensor: Tensor,\n    ) -> Tensor:\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        output = myreq.tensor\n        myreq.tensor = None\n        ctx.myreq = myreq\n        ctx.pg = pg\n        rsi = myreq.rsi\n\n        if rsi.codecs is not None:\n            output = rsi.codecs.forward.decode(output)\n        return output\n\n    @staticmethod\n    # pyre-fixme[14]: `backward` overrides method defined in `Function` inconsistently.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        rsi = myreq.rsi\n\n        if rsi.codecs is not None:\n            grad_output = rsi.codecs.backward.encode(grad_output)\n        grad_inputs = grad_output.new_empty(rsi.input_sizes)\n        with record_function(\"## reduce_scatter_base_bw (all_gather) ##\"):\n            req = dist._all_gather_base(\n                grad_inputs,\n                grad_output.contiguous(),\n                group=ctx.pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = grad_inputs\n        return (None, None, myreq.dummy_tensor)",
  "class AllGatherBase_Req(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        agi: AllGatherBaseInfo,\n        input: Tensor,\n    ) -> Tensor:\n        my_size = dist.get_world_size(pg)\n\n        if agi.codecs is not None:\n            input = agi.codecs.forward.encode(input)\n\n        outputs = input.new_empty((input.size(0) * my_size, input.size(1)))\n        with record_function(\"## all_gather_base ##\"):\n            req = dist._all_gather_base(outputs, input, group=pg, async_op=True)\n        myreq.req = req\n        myreq.tensor = outputs\n        myreq.wait_function = AllGatherBase_Wait\n        myreq.agi = agi\n        ctx.myreq = myreq\n        ctx.pg = pg\n        return myreq.dummy_tensor\n\n    @staticmethod\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, *unused: Tensor) -> Tuple[Optional[Tensor], ...]:\n        myreq = ctx.myreq\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        agi = myreq.agi\n        grad_input = myreq.tensor\n        if agi.codecs is not None:\n            grad_input = agi.codecs.backward.decode(grad_input)\n\n        # Make it equivalent to running on a single rank.\n        if GRADIENT_DIVISION:\n            grad_input.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, grad_input)",
  "class AllGatherBase_Wait(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: Tensor,\n    ) -> Tensor:\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        outputs = myreq.tensor\n        myreq.tensor = None\n        ctx.myreq = myreq\n        ctx.pg = pg\n\n        agi = myreq.agi\n        if agi.codecs is not None:\n            outputs = agi.codecs.forward.decode(outputs)\n        return outputs\n\n    @staticmethod\n    # pyre-fixme[14]: `backward` overrides method defined in `Function` inconsistently.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, grad_outputs: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        agi = myreq.agi\n\n        if agi.codecs is not None:\n            grad_outputs = agi.codecs.backward.encode(grad_outputs)\n        grad_input = grad_outputs.new_empty(agi.input_size)\n        with record_function(\"## all_gather_base_bw (reduce_scatter) ##\"):\n            req = dist._reduce_scatter_base(\n                grad_input,\n                grad_outputs.contiguous(),\n                group=ctx.pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = grad_input\n\n        return (None, None, myreq.dummy_tensor)",
  "class ReduceScatterV_Req(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        rsi: ReduceScatterVInfo,\n        input: Tensor,\n    ) -> Tensor:\n        my_rank = dist.get_rank(pg)\n\n        if rsi.codecs is not None:\n            input = rsi.codecs.forward.encode(input)\n\n        output = input.new_empty(rsi.input_sizes[my_rank])\n\n        # Use dist._reduce_scatter_base when a vector reduce-scatter is not needed\n        # else use dist.reduce_scatter which internally supports vector reduce-scatter\n        if rsi.equal_splits:\n            with record_function(\"## reduce_scatter_base ##\"):\n                req = dist._reduce_scatter_base(output, input, group=pg, async_op=True)\n        else:\n            with record_function(\"## reduce_scatter_v ##\"):\n                req = dist.reduce_scatter(\n                    output,\n                    list(torch.split(input, rsi.input_splits)),\n                    group=pg,\n                    async_op=True,\n                )\n\n        myreq.req = req\n        myreq.tensor = output\n        myreq.wait_function = ReduceScatterV_Wait\n        myreq.rsi = rsi\n        ctx.myreq = myreq\n        ctx.pg = pg\n\n        return myreq.dummy_tensor\n\n    @staticmethod\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, *unused: Tensor) -> Tuple[Optional[Tensor], ...]:\n        myreq = ctx.myreq\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        grad_input = myreq.tensor\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            grad_input = rsi.codecs.backward.decode(grad_input)\n        # Make it equivalent to running on a single rank.\n        if GRADIENT_DIVISION:\n            grad_input.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, grad_input)",
  "class ReduceScatterV_Wait(Function):\n    @staticmethod\n    # pyre-fixme[14]: `forward` overrides method defined in `Function` inconsistently.\n    def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: Tensor,\n    ) -> Tensor:\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        # pyre-ignore\n        output: torch.Tensor = myreq.tensor\n        myreq.tensor = None\n\n        ctx.myreq = myreq\n        ctx.pg = pg\n\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            output = rsi.codecs.forward.decode(output)\n\n        return output\n\n    @staticmethod\n    # pyre-fixme[14]: `backward` overrides method defined in `Function` inconsistently.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def backward(ctx, grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            grad_output = rsi.codecs.backward.encode(grad_output)\n        grad_input = grad_output.new_empty(rsi.total_input_size)\n\n        if rsi.equal_splits:\n            with record_function(\"## reduce_scatter_base_bw (all_gather) ##\"):\n                req = dist._all_gather_base(\n                    grad_input,\n                    grad_output.contiguous(),\n                    group=ctx.pg,\n                    async_op=True,\n                )\n        else:\n            with record_function(\"## reduce_scatter_v_bw (all_gather_v) ##\"):\n                req = dist.all_gather(\n                    list(torch.split(grad_input, rsi.input_splits)),\n                    grad_output.contiguous(),\n                    group=ctx.pg,\n                    async_op=True,\n                )\n        myreq.req = req\n        myreq.tensor = grad_input\n        return (None, None, myreq.dummy_tensor)",
  "def __init__(self, pg: dist.ProcessGroup, device: torch.device) -> None:\n        super().__init__()\n        self.pg: dist.ProcessGroup = pg\n        self.req: Optional[dist.Work] = None\n        self.tensor: Optional[W] = None\n        self.a2ai = None  # type: ignore\n        self.qcomm_ctx = None  # type: ignore\n        self.rsi = None  # type: ignore\n        self.agi = None  # type: ignore\n        self.wait_function = None  # type: ignore\n\n        # This dummy tensor is used to build the autograd graph between\n        # CommOp-Req and CommOp-Await. The actual forward tensors, and backwards gradient tensors\n        # are stored in self.tensor\n        self.dummy_tensor: torch.Tensor = torch.empty(\n            1,\n            requires_grad=True,\n            device=device,\n        )",
  "def _wait_impl(self) -> W:\n        \"\"\"\n        Calls the wait function for this request.\n        \"\"\"\n\n        ret = self.wait_function.apply(self.pg, self, self.dummy_tensor)\n        self.req = None\n        self.tensor = None\n        return ret",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        a2ai: All2AllPooledInfo,\n        input_embeddings: Tensor,\n    ) -> Tensor:\n        my_rank = dist.get_rank(pg)\n        (B_global, D_local_sum) = input_embeddings.shape\n\n        dim_sum_per_rank = a2ai.dim_sum_per_rank\n        batch_size_per_rank = a2ai.batch_size_per_rank\n        B_local = batch_size_per_rank[my_rank]\n\n        assert B_global == sum(batch_size_per_rank)\n\n        sharded_input_embeddings = input_embeddings.view(-1)\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.forward.create_context()\n            sharded_input_embeddings = codecs.forward.encode(\n                sharded_input_embeddings,\n                qcomm_ctx,\n            )\n            output_split_sizes = [\n                codecs.forward.calc_quantized_size(\n                    B_local * D_rank_sum,\n                    qcomm_ctx,\n                )\n                for D_rank_sum in dim_sum_per_rank\n            ]\n            input_split_sizes = [\n                codecs.forward.calc_quantized_size(\n                    D_local_sum * B_rank,\n                    qcomm_ctx,\n                )\n                for B_rank in batch_size_per_rank\n            ]\n        else:\n            output_split_sizes = [\n                B_local * D_rank_sum for D_rank_sum in dim_sum_per_rank\n            ]\n            input_split_sizes = [D_local_sum * B_rank for B_rank in batch_size_per_rank]\n            qcomm_ctx = None\n\n        sharded_output_embeddings = torch.empty(\n            sum(output_split_sizes),\n            dtype=sharded_input_embeddings.dtype,\n            device=sharded_input_embeddings.device,\n        )\n\n        with record_function(\"## alltoall_fwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_output_embeddings,\n                input=sharded_input_embeddings,\n                output_split_sizes=output_split_sizes,\n                input_split_sizes=input_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n\n        myreq.req = req\n        myreq.tensor = sharded_output_embeddings\n        myreq.qcomm_ctx = qcomm_ctx\n        myreq.a2ai = a2ai\n        myreq.wait_function = All2All_Pooled_Wait\n        ctx.myreq = myreq\n        ctx.pg = pg\n        return myreq.dummy_tensor",
  "def backward(ctx, *unused) -> Tuple[None, None, None, Tensor]:\n        pg = ctx.pg\n        my_rank = dist.get_rank(pg)\n        myreq = ctx.myreq\n        a2ai = myreq.a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        grad_output = myreq.tensor\n        dim_sum_per_rank = a2ai.dim_sum_per_rank\n        batch_size_per_rank = a2ai.batch_size_per_rank\n        D_local_sum = dim_sum_per_rank[my_rank]\n        B_global = sum(batch_size_per_rank)\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            grad_input = codecs.backward.decode(grad_output, myreq.qcomm_ctx)\n            grad_input = grad_input.view(B_global, D_local_sum)\n        else:\n            grad_input = grad_output.view(B_global, D_local_sum)\n        if GRADIENT_DIVISION:\n            grad_input.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, grad_input)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: Tensor,\n    ) -> Tensor:\n        my_rank = dist.get_rank(pg)\n        a2ai = myreq.a2ai\n        ctx.a2ai = a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        sharded_output_embeddings = myreq.tensor\n        myreq.req = None\n        myreq.tensor = None\n        ctx.pg = pg\n        ctx.myreq = myreq\n        dim_sum_per_rank = a2ai.dim_sum_per_rank\n        batch_size_per_rank = a2ai.batch_size_per_rank\n        B_local = batch_size_per_rank[my_rank]\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            sharded_output_embeddings = codecs.forward.decode(\n                sharded_output_embeddings,\n                myreq.qcomm_ctx,\n            )\n\n        outputs_by_rank = sharded_output_embeddings.split(\n            [B_local * D_rank_sum for D_rank_sum in dim_sum_per_rank]\n        )\n        result = torch.cat(\n            [output.view(B_local, -1) for output in outputs_by_rank], dim=1\n        )\n        return result",
  "def backward(ctx, grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        a2ai = ctx.a2ai\n        pg = ctx.pg\n        my_rank = dist.get_rank(pg)\n        dim_sum_per_rank = a2ai.dim_sum_per_rank\n        batch_size_per_rank = a2ai.batch_size_per_rank\n\n        D_local_sum = dim_sum_per_rank[my_rank]\n        (B_local, D_global_sum) = grad_output.shape\n        assert sum(dim_sum_per_rank) == D_global_sum\n\n        sharded_grad_output = _recat_pooled_embedding_grad_out(\n            grad_output.contiguous(),\n            dim_sum_per_rank,\n        )\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.backward.create_context()\n            sharded_grad_output = codecs.backward.encode(\n                sharded_grad_output,\n                qcomm_ctx,\n            )\n            input_split_sizes = [\n                codecs.backward.calc_quantized_size(\n                    B_local * D_rank_sum,\n                    qcomm_ctx,\n                )\n                for D_rank_sum in dim_sum_per_rank\n            ]\n            output_split_sizes = [\n                codecs.backward.calc_quantized_size(\n                    D_local_sum * B_rank,\n                    qcomm_ctx,\n                )\n                for B_rank in batch_size_per_rank\n            ]\n        else:\n            qcomm_ctx = None\n            input_split_sizes = [\n                B_local * D_rank_sum for D_rank_sum in dim_sum_per_rank\n            ]\n            output_split_sizes = [\n                D_local_sum * B_rank for B_rank in batch_size_per_rank\n            ]\n\n        sharded_grad_input = torch.empty(\n            sum(output_split_sizes),\n            device=sharded_grad_output.device,\n            dtype=sharded_grad_output.dtype,\n        )\n        with record_function(\"## alltoall_bwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_grad_input,\n                input=sharded_grad_output,\n                output_split_sizes=output_split_sizes,\n                input_split_sizes=input_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = sharded_grad_input\n        myreq.qcomm_ctx = qcomm_ctx\n\n        return (None, None, myreq.dummy_tensor)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        a2ai: VariableBatchAll2AllPooledInfo,\n        input_embeddings: Tensor,\n    ) -> Tensor:\n        my_rank = dist.get_rank(pg)\n\n        # get input splits\n        world_size = dist.get_world_size(pg)\n        input_split_sizes = [0 for _ in range(world_size)]\n        if a2ai.batch_size_per_rank_per_feature:\n            for i in range(world_size):\n                curr_size = 0\n                for batch_size, emb_dim in zip(\n                    a2ai.batch_size_per_rank_per_feature[i],\n                    a2ai.emb_dim_per_rank_per_feature[my_rank],\n                ):\n                    curr_size += batch_size * emb_dim\n                input_split_sizes[i] = curr_size\n        a2ai.input_splits = input_split_sizes\n\n        # get output splits\n        output_split_sizes = [0 for _ in range(world_size)]\n        ind = 0\n        for i in range(world_size):\n            curr_size = 0\n            for emb_dim in a2ai.emb_dim_per_rank_per_feature[i]:\n                curr_size += a2ai.batch_size_per_feature_pre_a2a[ind] * emb_dim\n                ind += 1\n            output_split_sizes[i] = curr_size\n        a2ai.output_splits = output_split_sizes\n\n        sharded_input_embeddings = input_embeddings.view(-1)\n        qcomm_ctx = None\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.forward.create_context()\n            sharded_input_embeddings = codecs.forward.encode(\n                sharded_input_embeddings,\n                qcomm_ctx,\n            )\n            output_split_sizes = [\n                codecs.forward.calc_quantized_size(\n                    split,\n                    qcomm_ctx,\n                )\n                for split in output_split_sizes\n            ]\n            input_split_sizes = [\n                codecs.forward.calc_quantized_size(\n                    split,\n                    qcomm_ctx,\n                )\n                for split in input_split_sizes\n            ]\n\n        sharded_output_embeddings = torch.empty(\n            sum(output_split_sizes),\n            dtype=sharded_input_embeddings.dtype,\n            device=sharded_input_embeddings.device,\n        )\n\n        with record_function(\"## alltoall_fwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_output_embeddings,\n                input=sharded_input_embeddings,\n                output_split_sizes=output_split_sizes,\n                input_split_sizes=input_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n\n        myreq.req = req\n        myreq.tensor = sharded_output_embeddings\n        myreq.qcomm_ctx = qcomm_ctx\n        myreq.a2ai = a2ai\n        myreq.wait_function = Variable_Batch_All2All_Pooled_Wait\n        ctx.myreq = myreq\n        ctx.pg = pg\n        return myreq.dummy_tensor",
  "def backward(ctx, *unused) -> Tuple[None, None, None, Tensor]:\n        myreq = ctx.myreq\n        a2ai = myreq.a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        grad_output = myreq.tensor\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            grad_input = codecs.backward.decode(grad_output, myreq.qcomm_ctx)\n        else:\n            grad_input = grad_output\n        if GRADIENT_DIVISION:\n            grad_input.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, grad_input)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: Tensor,\n    ) -> Tensor:\n        a2ai = myreq.a2ai\n        ctx.a2ai = a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        sharded_output_embeddings = myreq.tensor\n        myreq.req = None\n        myreq.tensor = None\n        ctx.pg = pg\n        ctx.myreq = myreq\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            sharded_output_embeddings = codecs.forward.decode(\n                sharded_output_embeddings,\n                myreq.qcomm_ctx,\n            )\n        # the return result is a 1-d tensor, like: f_0_s_0, f_0_s1, ..., f_n_s_0, f_n_s_k\n        # f_0, f_1, ... , f_n are ordered by features on each rank\n        return sharded_output_embeddings",
  "def backward(ctx, grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        a2ai = ctx.a2ai\n        pg = ctx.pg\n\n        assert a2ai.input_splits is not None\n        assert a2ai.output_splits is not None\n        input_split_sizes = a2ai.output_splits\n        output_split_sizes = a2ai.input_splits\n\n        sharded_grad_output = grad_output.contiguous()\n        qcomm_ctx = None\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.backward.create_context()\n            sharded_grad_output = codecs.backward.encode(\n                sharded_grad_output,\n                qcomm_ctx,\n            )\n            input_split_sizes = [\n                codecs.backward.calc_quantized_size(\n                    split,\n                    qcomm_ctx,\n                )\n                for split in input_split_sizes\n            ]\n            output_split_sizes = [\n                codecs.backward.calc_quantized_size(\n                    split,\n                    qcomm_ctx,\n                )\n                for split in output_split_sizes\n            ]\n\n        sharded_grad_input = torch.empty(\n            sum(output_split_sizes),\n            device=sharded_grad_output.device,\n            dtype=sharded_grad_output.dtype,\n        )\n        with record_function(\"## alltoall_bwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_grad_input,\n                input=sharded_grad_output,\n                output_split_sizes=output_split_sizes,\n                input_split_sizes=input_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = sharded_grad_input\n        myreq.qcomm_ctx = qcomm_ctx\n\n        return (None, None, myreq.dummy_tensor)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        a2ai: All2AllSequenceInfo,\n        sharded_input_embeddings: Tensor,\n    ) -> Tensor:\n        world_size = dist.get_world_size(pg)\n        my_rank = dist.get_rank(pg)\n        D = a2ai.embedding_dim\n        forward_recat_tensor = a2ai.forward_recat_tensor\n        variable_batch_size = a2ai.variable_batch_size\n        lengths_after_sparse_data_all2all = a2ai.lengths_after_sparse_data_all2all * D\n        input_splits = [i * D for i in a2ai.output_splits]\n        output_splits = [i * D for i in a2ai.input_splits]\n\n        a2ai.input_splits = input_splits\n        a2ai.output_splits = output_splits\n\n        local_T = lengths_after_sparse_data_all2all.shape[0]\n        if local_T > 0:\n            with record_function(\"## alltoall_seq_embedding_fwd_permute ##\"):\n                if not variable_batch_size:\n                    (\n                        permuted_lengths_after_sparse_data_all2all,\n                        sharded_input_embeddings,\n                        _,\n                    ) = torch.ops.fbgemm.permute_2D_sparse_data(\n                        forward_recat_tensor,\n                        lengths_after_sparse_data_all2all.view(\n                            local_T * world_size, -1\n                        ),\n                        sharded_input_embeddings.view(-1),\n                        None,\n                        sharded_input_embeddings.numel(),\n                    )\n                else:\n                    (\n                        permuted_lengths_after_sparse_data_all2all,\n                        sharded_input_embeddings,\n                        _,\n                    ) = torch.ops.fbgemm.permute_1D_sparse_data(\n                        forward_recat_tensor,\n                        lengths_after_sparse_data_all2all.view(-1),\n                        sharded_input_embeddings.view(-1),\n                        None,\n                        sharded_input_embeddings.numel(),\n                    )\n        else:\n            permuted_lengths_after_sparse_data_all2all = None\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.forward.create_context()\n            # pyre-ignore [16]\n            sharded_input_embeddings = a2ai.codecs.forward.encode(\n                sharded_input_embeddings, qcomm_ctx\n            )\n            output_splits = [\n                a2ai.codecs.forward.calc_quantized_size(x, qcomm_ctx)\n                for x in output_splits\n            ]\n            input_splits = [\n                a2ai.codecs.forward.calc_quantized_size(x, qcomm_ctx)\n                for x in input_splits\n            ]\n        else:\n            qcomm_ctx = None\n\n        sharded_output_embeddings = torch.empty(\n            sum(output_splits),\n            dtype=sharded_input_embeddings.dtype,\n            device=sharded_input_embeddings.device,\n        )\n\n        with record_function(\"## alltoall_seq_embedding_fwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_output_embeddings,\n                input=sharded_input_embeddings,\n                output_split_sizes=output_splits,\n                input_split_sizes=input_splits,\n                group=pg,\n                async_op=True,\n            )\n        a2ai.permuted_lengths_after_sparse_data_all2all = (\n            permuted_lengths_after_sparse_data_all2all\n        )\n        myreq.req = req\n        myreq.tensor = sharded_output_embeddings\n        myreq.a2ai = a2ai\n        myreq.wait_function = All2All_Seq_Req_Wait\n        ctx.myreq = myreq\n        myreq.qcomm_ctx = qcomm_ctx\n        ctx.pg = pg\n        ctx.my_rank = my_rank\n        ctx.world_size = world_size\n        return myreq.dummy_tensor",
  "def backward(ctx, *unused) -> Tuple[None, None, None, Tensor]:\n        myreq = ctx.myreq\n        a2ai = myreq.a2ai\n        D = a2ai.embedding_dim\n        variable_batch_size = a2ai.variable_batch_size\n        backward_recat_tensor = a2ai.backward_recat_tensor\n        permuted_lengths_after_sparse_data_all2all = (\n            a2ai.permuted_lengths_after_sparse_data_all2all\n        )\n        assert myreq.req is not None\n        myreq.req.wait()\n        sharded_grad_input = myreq.tensor\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            sharded_grad_input = codecs.backward.decode(\n                sharded_grad_input, myreq.qcomm_ctx\n            )\n        myreq.req = None\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n\n        if permuted_lengths_after_sparse_data_all2all is not None:\n            with record_function(\"## alltoall_seq_embedding_bwd_permute ##\"):\n                if not variable_batch_size:\n                    _, sharded_grad_input, _ = torch.ops.fbgemm.permute_2D_sparse_data(\n                        backward_recat_tensor,\n                        permuted_lengths_after_sparse_data_all2all,\n                        sharded_grad_input,\n                        None,\n                        sharded_grad_input.numel(),\n                    )\n                else:\n                    _, sharded_grad_input, _ = torch.ops.fbgemm.permute_1D_sparse_data(\n                        backward_recat_tensor,\n                        permuted_lengths_after_sparse_data_all2all,\n                        sharded_grad_input,\n                        None,\n                        sharded_grad_input.numel(),\n                    )\n        if GRADIENT_DIVISION:\n            sharded_grad_input.div_(dist.get_world_size(ctx.pg))\n        return (None, None, None, sharded_grad_input.view(-1, D))",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: torch.Tensor,\n    ) -> Tensor:\n        a2ai = myreq.a2ai\n        D = a2ai.embedding_dim\n        ctx.a2ai = a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        sharded_output_embeddings = myreq.tensor\n        myreq.tensor = None\n        ctx.pg = pg\n        ctx.myreq = myreq\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            sharded_output_embeddings = codecs.forward.decode(\n                sharded_output_embeddings, myreq.qcomm_ctx\n            )\n        return sharded_output_embeddings.view(-1, D)",
  "def backward(ctx, sharded_grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        a2ai = ctx.a2ai\n        pg = ctx.pg\n        input_splits = a2ai.output_splits\n        output_splits = a2ai.input_splits\n\n        if a2ai.codecs is not None:\n            codecs = none_throws(a2ai.codecs)\n            qcomm_ctx = codecs.backward.create_context()\n            sharded_grad_output = a2ai.codecs.backward.encode(\n                sharded_grad_output, qcomm_ctx\n            )\n            output_splits = [\n                a2ai.codecs.backward.calc_quantized_size(x, qcomm_ctx)\n                for x in output_splits\n            ]\n            input_splits = [\n                a2ai.codecs.backward.calc_quantized_size(x, qcomm_ctx)\n                for x in input_splits\n            ]\n        else:\n            qcomm_ctx = None\n\n        sharded_grad_input = torch.empty(\n            sum(output_splits),\n            device=sharded_grad_output.device,\n            dtype=sharded_grad_output.dtype,\n        )\n        with record_function(\"## alltoall_seq_embedding_bwd_single ##\"):\n            req = dist.all_to_all_single(\n                output=sharded_grad_input,\n                input=sharded_grad_output.view(-1),\n                output_split_sizes=output_splits,\n                input_split_sizes=input_splits,\n                group=pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = sharded_grad_input\n        myreq.qcomm_ctx = qcomm_ctx\n\n        return (None, None, myreq.dummy_tensor)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        a2ai: All2AllVInfo,\n        inputs: List[Tensor],\n    ) -> Tensor:\n        input_split_sizes = [m * sum(a2ai.D_local_list) for m in a2ai.B_local_list]\n        output_split_sizes = [a2ai.B_local * e for e in a2ai.dims_sum_per_rank]\n        input = torch.cat(inputs, dim=1).view([-1])\n        if a2ai.codecs is not None:\n            input = a2ai.codecs.forward.encode(input)\n\n        output = input.new_empty(sum(output_split_sizes))\n        with record_function(\"## alltoallv_bwd_single ##\"):\n            req = dist.all_to_all_single(\n                output,\n                input,\n                output_split_sizes,\n                input_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n\n        myreq.req = req\n        myreq.tensor = output\n        myreq.wait_function = All2Allv_Wait\n        a2ai.input_split_sizes = input_split_sizes\n        a2ai.output_split_sizes = output_split_sizes\n        myreq.a2ai = a2ai\n        ctx.a2ai = a2ai\n        ctx.myreq = myreq\n        ctx.tensor = output\n        return myreq.dummy_tensor",
  "def backward(ctx, *grad_output):\n        a2ai = ctx.a2ai\n        myreq = ctx.myreq\n        myreq.req.wait()\n        myreq.req = None\n        grad_input = myreq.tensor\n        if a2ai.codecs is not None:\n            grad_input = a2ai.codecs.backward.decode(grad_input)\n\n        grad_inputs = grad_input.view([a2ai.B_global, -1]).split(\n            a2ai.D_local_list, dim=1\n        )\n        grad_inputs = [gin.contiguous() for gin in grad_inputs]\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, *grad_inputs)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: torch.Tensor,\n    ) -> Tuple[Tensor]:\n        a2ai = myreq.a2ai\n        ctx.a2ai = a2ai\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        output = myreq.tensor\n        myreq.tensor = None\n        ctx.pg = pg\n        ctx.myreq = myreq\n\n        if a2ai.codecs is not None:\n            output = a2ai.codecs.forward.decode(output)\n        outputs = tuple(\n            [\n                out.view([a2ai.B_local, -1])\n                for out in output.split(a2ai.output_split_sizes)\n            ]\n        )\n        return outputs",
  "def backward(ctx, *grad_outputs) -> Tuple[None, None, Tensor]:\n        pg = ctx.pg\n        myreq = ctx.myreq\n        a2ai = ctx.a2ai\n\n        if a2ai.codecs is not None:\n            grad_outputs = a2ai.codecs.backward.encode(grad_outputs)\n\n        grad_outputs = [gout.contiguous().view([-1]) for gout in grad_outputs]\n        grad_output = torch.cat(grad_outputs)\n        grad_input = grad_output.new_empty([a2ai.B_global * sum(a2ai.D_local_list)])\n        with record_function(\"## alltoall_bwd_single ##\"):\n            req = dist.all_to_all_single(\n                grad_input,\n                grad_output,\n                a2ai.input_split_sizes,\n                a2ai.output_split_sizes,\n                group=pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = grad_input\n        return (None, None, myreq.dummy_tensor)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        rsi: ReduceScatterInfo,\n        *inputs: Any,\n    ) -> Tensor:\n        my_rank = dist.get_rank(pg)\n\n        if rsi.codecs is not None:\n            # pyre-ignore\n            inputs = [rsi.codecs.forward.encode(input) for input in inputs]\n\n        output = inputs[my_rank].new_empty(\n            inputs[my_rank].size(),\n            dtype=inputs[my_rank].dtype,\n            device=inputs[my_rank].device,\n        )\n        with record_function(\"## reduce_scatter ##\"):\n            req = dist.reduce_scatter(output, list(inputs), group=pg, async_op=True)\n        myreq.req = req\n        myreq.tensor = output\n        myreq.wait_function = ReduceScatter_Wait\n        myreq.rsi = rsi\n        ctx.myreq = myreq\n        ctx.pg = pg\n        ctx.tensor = output\n        return myreq.dummy_tensor",
  "def backward(ctx, *unused: Tensor) -> Tuple[Optional[Tensor], ...]:\n        myreq = ctx.myreq\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        grad_inputs = list(myreq.tensor)\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            grad_inputs = [\n                rsi.codecs.backward.decode(grad_input) for grad_input in grad_inputs\n            ]\n        # Make it equivalent to running on a single rank.\n        if GRADIENT_DIVISION:\n            for grad_input in grad_inputs:\n                grad_input.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor\n        return (None, None, None, *grad_inputs)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: Tensor,\n    ) -> Tensor:\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        output = myreq.tensor\n        myreq.tensor = None\n        ctx.myreq = myreq\n        ctx.pg = pg\n\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            output = rsi.codecs.forward.decode(output)\n        return output",
  "def backward(ctx, grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            grad_output = rsi.codecs.backward.encode(grad_output)\n\n        grad_inputs = [\n            grad_output.new_empty(\n                in_size,\n                dtype=grad_output.dtype,\n                device=grad_output.device,\n            )\n            for in_size in rsi.input_sizes\n        ]\n\n        with record_function(\"## reduce_scatter_bw (all_gather) ##\"):\n            req = dist.all_gather(\n                grad_inputs,\n                grad_output.contiguous(),\n                group=ctx.pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = grad_inputs\n        return (None, None, myreq.dummy_tensor)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        rsi: ReduceScatterBaseInfo,\n        inputs: Tensor,\n    ) -> Tensor:\n        my_size = dist.get_world_size(pg)\n        assert inputs.size(0) % my_size == 0\n        if rsi.codecs is not None:\n            inputs = rsi.codecs.forward.encode(inputs)\n        output = inputs.new_empty((inputs.size(0) // my_size, inputs.size(1)))\n        with record_function(\"## reduce_scatter_base ##\"):\n            req = dist._reduce_scatter_base(output, inputs, group=pg, async_op=True)\n        myreq.req = req\n        myreq.tensor = output\n        myreq.wait_function = ReduceScatterBase_Wait\n        myreq.rsi = rsi\n        myreq.tensor = output\n        ctx.myreq = myreq\n        ctx.pg = pg\n\n        return myreq.dummy_tensor",
  "def backward(ctx, *unused: Tensor) -> Tuple[Optional[Tensor], ...]:\n        myreq = ctx.myreq\n        myreq.req.wait()\n        myreq.req = None\n        grad_inputs = myreq.tensor\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            grad_inputs = rsi.codecs.backward.decode(grad_inputs)\n        # Make it equivalent to running on a single rank.\n        if GRADIENT_DIVISION:\n            grad_inputs.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, grad_inputs)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_Tensor: Tensor,\n    ) -> Tensor:\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        output = myreq.tensor\n        myreq.tensor = None\n        ctx.myreq = myreq\n        ctx.pg = pg\n        rsi = myreq.rsi\n\n        if rsi.codecs is not None:\n            output = rsi.codecs.forward.decode(output)\n        return output",
  "def backward(ctx, grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        rsi = myreq.rsi\n\n        if rsi.codecs is not None:\n            grad_output = rsi.codecs.backward.encode(grad_output)\n        grad_inputs = grad_output.new_empty(rsi.input_sizes)\n        with record_function(\"## reduce_scatter_base_bw (all_gather) ##\"):\n            req = dist._all_gather_base(\n                grad_inputs,\n                grad_output.contiguous(),\n                group=ctx.pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = grad_inputs\n        return (None, None, myreq.dummy_tensor)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        agi: AllGatherBaseInfo,\n        input: Tensor,\n    ) -> Tensor:\n        my_size = dist.get_world_size(pg)\n\n        if agi.codecs is not None:\n            input = agi.codecs.forward.encode(input)\n\n        outputs = input.new_empty((input.size(0) * my_size, input.size(1)))\n        with record_function(\"## all_gather_base ##\"):\n            req = dist._all_gather_base(outputs, input, group=pg, async_op=True)\n        myreq.req = req\n        myreq.tensor = outputs\n        myreq.wait_function = AllGatherBase_Wait\n        myreq.agi = agi\n        ctx.myreq = myreq\n        ctx.pg = pg\n        return myreq.dummy_tensor",
  "def backward(ctx, *unused: Tensor) -> Tuple[Optional[Tensor], ...]:\n        myreq = ctx.myreq\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        agi = myreq.agi\n        grad_input = myreq.tensor\n        if agi.codecs is not None:\n            grad_input = agi.codecs.backward.decode(grad_input)\n\n        # Make it equivalent to running on a single rank.\n        if GRADIENT_DIVISION:\n            grad_input.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, grad_input)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: Tensor,\n    ) -> Tensor:\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        outputs = myreq.tensor\n        myreq.tensor = None\n        ctx.myreq = myreq\n        ctx.pg = pg\n\n        agi = myreq.agi\n        if agi.codecs is not None:\n            outputs = agi.codecs.forward.decode(outputs)\n        return outputs",
  "def backward(ctx, grad_outputs: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        agi = myreq.agi\n\n        if agi.codecs is not None:\n            grad_outputs = agi.codecs.backward.encode(grad_outputs)\n        grad_input = grad_outputs.new_empty(agi.input_size)\n        with record_function(\"## all_gather_base_bw (reduce_scatter) ##\"):\n            req = dist._reduce_scatter_base(\n                grad_input,\n                grad_outputs.contiguous(),\n                group=ctx.pg,\n                async_op=True,\n            )\n        myreq.req = req\n        myreq.tensor = grad_input\n\n        return (None, None, myreq.dummy_tensor)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        rsi: ReduceScatterVInfo,\n        input: Tensor,\n    ) -> Tensor:\n        my_rank = dist.get_rank(pg)\n\n        if rsi.codecs is not None:\n            input = rsi.codecs.forward.encode(input)\n\n        output = input.new_empty(rsi.input_sizes[my_rank])\n\n        # Use dist._reduce_scatter_base when a vector reduce-scatter is not needed\n        # else use dist.reduce_scatter which internally supports vector reduce-scatter\n        if rsi.equal_splits:\n            with record_function(\"## reduce_scatter_base ##\"):\n                req = dist._reduce_scatter_base(output, input, group=pg, async_op=True)\n        else:\n            with record_function(\"## reduce_scatter_v ##\"):\n                req = dist.reduce_scatter(\n                    output,\n                    list(torch.split(input, rsi.input_splits)),\n                    group=pg,\n                    async_op=True,\n                )\n\n        myreq.req = req\n        myreq.tensor = output\n        myreq.wait_function = ReduceScatterV_Wait\n        myreq.rsi = rsi\n        ctx.myreq = myreq\n        ctx.pg = pg\n\n        return myreq.dummy_tensor",
  "def backward(ctx, *unused: Tensor) -> Tuple[Optional[Tensor], ...]:\n        myreq = ctx.myreq\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        grad_input = myreq.tensor\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            grad_input = rsi.codecs.backward.decode(grad_input)\n        # Make it equivalent to running on a single rank.\n        if GRADIENT_DIVISION:\n            grad_input.div_(dist.get_world_size(ctx.pg))\n        myreq.tensor = None\n        myreq.dummy_tensor = None\n        return (None, None, None, grad_input)",
  "def forward(\n        # pyre-fixme[2]: Parameter must be annotated.\n        ctx,\n        pg: dist.ProcessGroup,\n        myreq: Request[Tensor],\n        *dummy_tensor: Tensor,\n    ) -> Tensor:\n        assert myreq.req is not None\n        myreq.req.wait()\n        myreq.req = None\n        # pyre-ignore\n        output: torch.Tensor = myreq.tensor\n        myreq.tensor = None\n\n        ctx.myreq = myreq\n        ctx.pg = pg\n\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            output = rsi.codecs.forward.decode(output)\n\n        return output",
  "def backward(ctx, grad_output: Tensor) -> Tuple[None, None, Tensor]:\n        myreq = ctx.myreq\n        rsi = myreq.rsi\n        if rsi.codecs is not None:\n            grad_output = rsi.codecs.backward.encode(grad_output)\n        grad_input = grad_output.new_empty(rsi.total_input_size)\n\n        if rsi.equal_splits:\n            with record_function(\"## reduce_scatter_base_bw (all_gather) ##\"):\n                req = dist._all_gather_base(\n                    grad_input,\n                    grad_output.contiguous(),\n                    group=ctx.pg,\n                    async_op=True,\n                )\n        else:\n            with record_function(\"## reduce_scatter_v_bw (all_gather_v) ##\"):\n                req = dist.all_gather(\n                    list(torch.split(grad_input, rsi.input_splits)),\n                    grad_output.contiguous(),\n                    group=ctx.pg,\n                    async_op=True,\n                )\n        myreq.req = req\n        myreq.tensor = grad_input\n        return (None, None, myreq.dummy_tensor)",
  "class EmbeddingFusedOptimizer(FusedOptimizer):\n    def __init__(  # noqa C901\n        self,\n        config: GroupedEmbeddingConfig,\n        emb_module: SplitTableBatchedEmbeddingBagsCodegen,\n        pg: Optional[dist.ProcessGroup] = None,\n        create_for_table: Optional[str] = None,\n        param_weight_for_table: Optional[nn.Parameter] = None,\n    ) -> None:\n        \"\"\"\n        Implementation of a FusedOptimizer. Designed as a base class Embedding kernels\n\n        create_for_table is an optional flag, which if passed in only creates the optimizer for a single table.\n        This optimizer shares data with the broader optimizer (one per embedding kernel)\n        and is used to share step and LR changes\n        \"\"\"\n        self._emb_module: SplitTableBatchedEmbeddingBagsCodegen = emb_module\n        self._pg = pg\n\n        @dataclass\n        class ShardParams:\n            optimizer_states: List[Optional[Tuple[torch.Tensor]]]\n            local_metadata: List[ShardMetadata]\n            embedding_weights: List[torch.Tensor]\n\n        def get_optimizer_rowwise_shard_metadata_and_global_metadata(\n            table_global_metadata: ShardedTensorMetadata,\n            optimizer_state: torch.Tensor,\n            sharding_dim: int,\n        ) -> Tuple[Dict[ShardMetadata, ShardMetadata], ShardedTensorMetadata]:\n\n            table_global_shards_metadata: List[\n                ShardMetadata\n            ] = table_global_metadata.shards_metadata\n\n            # column-wise sharding\n            # sort the metadata based on column offset and\n            # we construct the momentum tensor in row-wise sharded way\n            if sharding_dim == 1:\n                table_global_shards_metadata = sorted(\n                    table_global_shards_metadata,\n                    key=lambda shard: shard.shard_offsets[1],\n                )\n\n            table_shard_metadata_to_optimizer_shard_metadata = {}\n\n            for idx, table_shard_metadata in enumerate(table_global_shards_metadata):\n                offset = table_shard_metadata.shard_offsets[0]\n                # for column-wise sharding, we still create row-wise sharded metadata for optimizer\n                # manually create a row-wise offset\n\n                if sharding_dim == 1:\n                    offset = idx * table_shard_metadata.shard_sizes[0]\n\n                table_shard_metadata_to_optimizer_shard_metadata[\n                    table_shard_metadata\n                ] = ShardMetadata(\n                    shard_sizes=[table_shard_metadata.shard_sizes[0]],\n                    shard_offsets=[offset],\n                    placement=table_shard_metadata.placement,\n                )\n\n            tensor_properties = TensorProperties(\n                dtype=optimizer_state.dtype,\n                layout=optimizer_state.layout,\n                requires_grad=False,\n            )\n            len_rw_shards = (\n                len(table_shard_metadata_to_optimizer_shard_metadata)\n                if sharding_dim == 1\n                else 1\n            )\n            rowwise_optimizer_st_metadata = ShardedTensorMetadata(\n                shards_metadata=list(\n                    table_shard_metadata_to_optimizer_shard_metadata.values()\n                ),\n                size=torch.Size([table_global_metadata.size[0] * len_rw_shards]),\n                tensor_properties=tensor_properties,\n            )\n\n            return (\n                table_shard_metadata_to_optimizer_shard_metadata,\n                rowwise_optimizer_st_metadata,\n            )\n\n        def get_optimizer_pointwise_shard_metadata_and_global_metadata(\n            table_global_metadata: ShardedTensorMetadata,\n            optimizer_state: torch.Tensor,\n        ) -> Tuple[Dict[ShardMetadata, ShardMetadata], ShardedTensorMetadata]:\n            table_global_shards_metadata: List[\n                ShardMetadata\n            ] = table_global_metadata.shards_metadata\n\n            table_shard_metadata_to_optimizer_shard_metadata = {}\n\n            for table_shard_metadata in table_global_shards_metadata:\n                table_shard_metadata_to_optimizer_shard_metadata[\n                    table_shard_metadata\n                ] = ShardMetadata(\n                    shard_sizes=table_shard_metadata.shard_sizes,\n                    shard_offsets=table_shard_metadata.shard_offsets,\n                    placement=table_shard_metadata.placement,\n                )\n            tensor_properties = TensorProperties(\n                dtype=optimizer_state.dtype,\n                layout=optimizer_state.layout,\n                requires_grad=False,\n            )\n            pointwise_optimizer_st_metadata = ShardedTensorMetadata(\n                shards_metadata=list(\n                    table_shard_metadata_to_optimizer_shard_metadata.values()\n                ),\n                size=table_global_metadata.size,\n                tensor_properties=tensor_properties,\n            )\n\n            return (\n                table_shard_metadata_to_optimizer_shard_metadata,\n                pointwise_optimizer_st_metadata,\n            )\n\n        # pyre-ignore [33]\n        state: Dict[Any, Any] = {}\n        param_group: Dict[str, Any] = {\n            \"params\": [],\n            \"lr\": emb_module.optimizer_args.learning_rate,\n        }\n\n        params: Dict[str, Union[torch.Tensor, ShardedTensor]] = {}\n\n        # Fused optimizers use buffers (they don't use autograd) and we want to make sure\n        # that state_dict look identical to no-fused version.\n        table_to_shard_params: Dict[str, ShardParams] = {}\n\n        embedding_weights_by_table = emb_module.split_embedding_weights()\n\n        all_optimizer_states = emb_module.get_optimizer_state()\n        optimizer_states_keys_by_table: Dict[str, List[torch.Tensor]] = {}\n\n        for (table_config, optimizer_states, weight,) in itertools.zip_longest(\n            config.embedding_tables,\n            all_optimizer_states,\n            embedding_weights_by_table,\n        ):\n            # When EmbeddingFusedOptimizer is created for composability, only create state\n            if create_for_table is not None and create_for_table != table_config.name:\n                continue\n            if table_config.name not in table_to_shard_params:\n                table_to_shard_params[table_config.name] = ShardParams(\n                    optimizer_states=[], local_metadata=[], embedding_weights=[]\n                )\n            optimizer_state_values = None\n            if optimizer_states:\n                optimizer_state_values = tuple(optimizer_states.values())\n                for optimizer_state_value in optimizer_state_values:\n                    assert table_config.local_rows == optimizer_state_value.size(0)\n                optimizer_states_keys_by_table[table_config.name] = list(\n                    optimizer_states.keys()\n                )\n            local_metadata = table_config.local_metadata\n\n            table_to_shard_params[table_config.name].optimizer_states.append(\n                optimizer_state_values\n            )\n            table_to_shard_params[table_config.name].local_metadata.append(\n                local_metadata\n            )\n            table_to_shard_params[table_config.name].embedding_weights.append(weight)\n\n        seen_tables = set()\n        for table_config in config.embedding_tables:\n            if create_for_table is not None and create_for_table != table_config.name:\n                continue\n            if table_config.name in seen_tables:\n                continue\n            seen_tables.add(table_config.name)\n            table_config_global_metadata: Optional[\n                ShardedTensorMetadata\n            ] = copy.deepcopy(table_config.global_metadata)\n\n            shard_params: ShardParams = table_to_shard_params[table_config.name]\n\n            assert table_config_global_metadata is not None\n            if create_for_table is None:\n                local_weight_shards = []\n                for local_weight, local_metadata in zip(\n                    shard_params.embedding_weights, shard_params.local_metadata\n                ):\n                    local_weight_shards.append(Shard(local_weight, local_metadata))\n                    table_config_global_metadata.tensor_properties.dtype = (\n                        local_weight.dtype\n                    )\n                    table_config_global_metadata.tensor_properties.requires_grad = (\n                        local_weight.requires_grad\n                    )\n                # TODO share this logic to create the same TableBatchedEmbeddingSlice in FusedModules below\n                weight = ShardedTensor._init_from_local_shards_and_global_metadata(\n                    local_shards=local_weight_shards,\n                    sharded_tensor_metadata=table_config_global_metadata,\n                    process_group=self._pg,\n                )\n                param_key = table_config.name + \".weight\"\n            else:\n                assert (\n                    param_weight_for_table is not None\n                ), \"param_weight_for_table cannot be None when using create_for_table\"\n                weight = param_weight_for_table\n                param_key = \"\"\n\n            state[weight] = {}\n            param_group[\"params\"].append(weight)\n            params[param_key] = weight\n\n            # Setting optimizer states\n            sharding_dim: int = (\n                1 if table_config.local_cols != table_config.embedding_dim else 0\n            )\n\n            if all(\n                [opt_state is not None for opt_state in shard_params.optimizer_states]\n            ):\n                # pyre-ignore\n                def get_sharded_optim_state(momentum_idx: int) -> ShardedTensor:\n                    assert momentum_idx > 0\n                    momentum_local_shards: List[Shard] = []\n                    optimizer_sharded_tensor_metadata: ShardedTensorMetadata\n\n                    is_rowwise_optimizer_state: bool = (\n                        # pyre-ignore\n                        shard_params.optimizer_states[0][momentum_idx - 1].dim()\n                        == 1\n                    )\n\n                    if is_rowwise_optimizer_state:\n                        (\n                            table_shard_metadata_to_optimizer_shard_metadata,\n                            optimizer_sharded_tensor_metadata,\n                        ) = get_optimizer_rowwise_shard_metadata_and_global_metadata(\n                            table_config.global_metadata,\n                            shard_params.optimizer_states[0][momentum_idx - 1],\n                            sharding_dim,\n                        )\n                    else:\n                        (\n                            table_shard_metadata_to_optimizer_shard_metadata,\n                            optimizer_sharded_tensor_metadata,\n                        ) = get_optimizer_pointwise_shard_metadata_and_global_metadata(\n                            table_config.global_metadata,\n                            shard_params.optimizer_states[0][momentum_idx - 1],\n                        )\n\n                    for (optimizer_state, table_shard_local_metadata) in zip(\n                        shard_params.optimizer_states, shard_params.local_metadata\n                    ):\n                        local_optimizer_shard_metadata = (\n                            table_shard_metadata_to_optimizer_shard_metadata[\n                                table_shard_local_metadata\n                            ]\n                        )\n                        momentum_local_shards.append(\n                            Shard(\n                                optimizer_state[momentum_idx - 1],\n                                local_optimizer_shard_metadata,\n                            )\n                        )\n\n                    # TODO we should be creating this in SPMD fashion (e.g. init_from_local_shards), and let it derive global metadata.\n                    return ShardedTensor._init_from_local_shards_and_global_metadata(\n                        local_shards=momentum_local_shards,\n                        sharded_tensor_metadata=optimizer_sharded_tensor_metadata,\n                        process_group=self._pg,\n                    )\n\n                num_states: int = min(\n                    # pyre-ignore\n                    [len(opt_state) for opt_state in shard_params.optimizer_states]\n                )\n                optimizer_state_keys = []\n                if num_states > 0:\n                    optimizer_state_keys = optimizer_states_keys_by_table[\n                        table_config.name\n                    ]\n                for cur_state_idx in range(0, num_states):\n                    if cur_state_idx == 0:\n                        # for backward compatibility\n                        cur_state_key = \"momentum1\"\n                    else:\n                        cur_state_key = optimizer_state_keys[cur_state_idx]\n\n                    state[weight][\n                        f\"{table_config.name}.{cur_state_key}\"\n                    ] = get_sharded_optim_state(cur_state_idx + 1)\n\n        super().__init__(params, state, [param_group])\n\n    def zero_grad(self, set_to_none: bool = False) -> None:\n        # pyre-ignore [16]\n        self._emb_module.set_learning_rate(self.param_groups[0][\"lr\"])\n\n    # pyre-ignore [2]\n    def step(self, closure: Any = None) -> None:\n        # pyre-ignore [16]\n        self._emb_module.set_learning_rate(self.param_groups[0][\"lr\"])",
  "def _gen_named_parameters_by_table_fused(\n    emb_module: SplitTableBatchedEmbeddingBagsCodegen,\n    table_name_to_count: Dict[str, int],\n    config: GroupedEmbeddingConfig,\n    pg: Optional[dist.ProcessGroup] = None,\n) -> Iterator[Tuple[str, TableBatchedEmbeddingSlice]]:\n    # TODO: move logic to FBGEMM to avoid accessing fbgemm internals\n    for t_idx, (rows, dim, location, _) in enumerate(emb_module.embedding_specs):\n        table_name = config.embedding_tables[t_idx].name\n        if table_name not in table_name_to_count:\n            continue\n        table_count = table_name_to_count.pop(table_name)\n        if emb_module.weights_precision == SparseType.INT8:\n            dim += emb_module.int8_emb_row_dim_offset\n        offset = emb_module.weights_physical_offsets[t_idx]\n        weights: torch.Tensor\n        if location == EmbeddingLocation.DEVICE.value:\n            weights = emb_module.weights_dev\n        elif location == EmbeddingLocation.HOST.value:\n            weights = emb_module.weights_host\n        else:\n            weights = emb_module.weights_uvm\n        weight = TableBatchedEmbeddingSlice(\n            data=weights,\n            start_offset=offset,\n            end_offset=offset + table_count * rows * dim,\n            num_embeddings=-1,\n            embedding_dim=dim,\n        )\n        # this reuses logic in EmbeddingFusedOptimizer but is per table\n        # pyre-ignore\n        weight._in_backward_optimizers = [\n            EmbeddingFusedOptimizer(\n                config=config,\n                emb_module=emb_module,\n                pg=pg,\n                create_for_table=table_name,\n                param_weight_for_table=weight,\n            )\n        ]\n        yield (table_name, weight)",
  "def _gen_named_parameters_by_table_dense(\n    emb_module: DenseTableBatchedEmbeddingBagsCodegen,\n    table_name_to_count: Dict[str, int],\n    config: GroupedEmbeddingConfig,\n) -> Iterator[Tuple[str, TableBatchedEmbeddingSlice]]:\n    # TODO: move logic to FBGEMM to avoid accessing fbgemm internals\n    for t_idx, (rows, dim) in enumerate(emb_module.embedding_specs):\n        table_name = config.embedding_tables[t_idx].name\n        if table_name not in table_name_to_count:\n            continue\n        table_count = table_name_to_count.pop(table_name)\n        offset = emb_module.weights_physical_offsets[t_idx]\n        weight = TableBatchedEmbeddingSlice(\n            data=emb_module.weights,\n            start_offset=offset,\n            end_offset=offset + table_count * rows * dim,\n            num_embeddings=-1,\n            embedding_dim=dim,\n        )\n        yield (table_name, weight)",
  "class BaseBatchedEmbedding(BaseEmbedding, Generic[SplitWeightType]):\n    def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.distributed.{self.__class__.__name__}\")\n        self._config = config\n        self._pg = pg\n\n        self._local_rows: List[int] = []\n        self._weight_init_mins: List[float] = []\n        self._weight_init_maxs: List[float] = []\n        self._num_embeddings: List[int] = []\n        self._local_cols: List[int] = []\n        self._feature_table_map: List[int] = []\n        self.table_name_to_count: Dict[str, int] = {}\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = {}\n\n        for idx, config in enumerate(self._config.embedding_tables):\n            self._local_rows.append(config.local_rows)\n            self._weight_init_mins.append(config.get_weight_init_min())\n            self._weight_init_maxs.append(config.get_weight_init_max())\n            self._num_embeddings.append(config.num_embeddings)\n            self._local_cols.append(config.local_cols)\n            self._feature_table_map.extend([idx] * config.num_features())\n            if config.name not in self.table_name_to_count:\n                self.table_name_to_count[config.name] = 0\n            self.table_name_to_count[config.name] += 1\n\n    def init_parameters(self) -> None:\n        # initialize embedding weights\n        assert len(self._num_embeddings) == len(self.split_embedding_weights())\n        for (rows, emb_dim, weight_init_min, weight_init_max, param) in zip(\n            self._local_rows,\n            self._local_cols,\n            self._weight_init_mins,\n            self._weight_init_maxs,\n            self.split_embedding_weights(),\n        ):\n            assert param.shape == (rows, emb_dim)  # pyre-ignore[16]\n            param.data.uniform_(  # pyre-ignore[16]\n                weight_init_min,\n                weight_init_max,\n            )\n\n    def forward(self, features: KeyedJaggedTensor) -> torch.Tensor:\n        return self.emb_module(\n            indices=features.values().long(),\n            offsets=features.offsets().long(),\n        )\n\n    # pyre-fixme[14]: `state_dict` overrides method defined in `Module` inconsistently.\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        self.flush()\n        return get_state_dict(\n            self._config.embedding_tables,\n            # pyre-ignore\n            self.split_embedding_weights(),\n            self._pg,\n            destination,\n            prefix,\n        )\n\n    def split_embedding_weights(self) -> List[SplitWeightType]:\n        return self.emb_module.split_embedding_weights()\n\n    @property\n    @abc.abstractmethod\n    def emb_module(\n        self,\n    ) -> Union[\n        DenseTableBatchedEmbeddingBagsCodegen,\n        SplitTableBatchedEmbeddingBagsCodegen,\n        IntNBitTableBatchedEmbeddingBagsCodegen,\n    ]:\n        ...\n\n    @property\n    def config(self) -> GroupedEmbeddingConfig:\n        return self._config\n\n    def flush(self) -> None:\n        pass\n\n    def named_split_embedding_weights(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert (\n            remove_duplicate\n        ), \"remove_duplicate=False not supported in BaseBatchedEmbedding.named_split_embedding_weights\"\n        for config, param in zip(\n            self._config.embedding_tables,\n            self.emb_module.split_embedding_weights(),\n        ):\n            key = append_prefix(prefix, f\"{config.name}.weight\")\n            yield key, param\n\n    def named_parameters_by_table(\n        self,\n    ) -> Iterator[Tuple[str, TableBatchedEmbeddingSlice]]:\n        \"\"\"\n        Like named_parameters(), but yields table_name and embedding_weights which are wrapped in TableBatchedEmbeddingSlice.\n        For a single table with multiple shards (i.e CW) these are combined into one table/weight.\n        Used in composability.\n        \"\"\"\n        for name, param in self._param_per_table.items():\n            yield name, param",
  "class BatchedFusedEmbedding(BaseBatchedEmbedding[torch.Tensor], FusedOptimizerModule):\n    def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        managed: List[EmbeddingLocation] = []\n        compute_devices: List[ComputeDevice] = []\n        for table in config.embedding_tables:\n            if device is not None and device.type == \"cuda\":\n                compute_devices.append(ComputeDevice.CUDA)\n                managed.append(\n                    compute_kernel_to_embedding_location(table.compute_kernel)\n                )\n            else:\n                compute_devices.append(ComputeDevice.CPU)\n                managed.append(EmbeddingLocation.HOST)\n\n        weights_precision = data_type_to_sparse_type(config.data_type)\n\n        fused_params = config.fused_params or {}\n        if \"cache_precision\" not in fused_params:\n            fused_params[\"cache_precision\"] = weights_precision\n\n        self._emb_module: SplitTableBatchedEmbeddingBagsCodegen = (\n            SplitTableBatchedEmbeddingBagsCodegen(\n                embedding_specs=list(\n                    zip(self._local_rows, self._local_cols, managed, compute_devices)\n                ),\n                feature_table_map=self._feature_table_map,\n                pooling_mode=PoolingMode.NONE,\n                weights_precision=weights_precision,\n                device=device,\n                **fused_params,\n            )\n        )\n        self._optim: EmbeddingFusedOptimizer = EmbeddingFusedOptimizer(\n            config,\n            self._emb_module,\n            pg,\n        )\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = dict(\n            _gen_named_parameters_by_table_fused(\n                emb_module=self._emb_module,\n                table_name_to_count=self.table_name_to_count.copy(),\n                config=self._config,\n                pg=pg,\n            )\n        )\n        self.init_parameters()\n\n    @property\n    def emb_module(\n        self,\n    ) -> SplitTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module\n\n    @property\n    def fused_optimizer(self) -> FusedOptimizer:\n        return self._optim\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        \"\"\"\n        By convention, fused parameters are designated as buffers because they no longer\n        have gradients available to external optimizers.\n        \"\"\"\n        # TODO can delete this override once SEA is removed\n        yield from ()\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        for name, tensor in self.named_split_embedding_weights(\n            prefix, recurse, remove_duplicate\n        ):\n            # hack before we support optimizer on sharded parameter level\n            # can delete after SEA deprecation\n            param = nn.Parameter(tensor)\n            # pyre-ignore\n            param._in_backward_optimizers = [EmptyFusedOptimizer()]\n            yield name, param\n\n    def flush(self) -> None:\n        self._emb_module.flush()",
  "class BatchedDenseEmbedding(BaseBatchedEmbedding[torch.Tensor]):\n    def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        weights_precision = data_type_to_sparse_type(config.data_type)\n        self._emb_module: DenseTableBatchedEmbeddingBagsCodegen = (\n            DenseTableBatchedEmbeddingBagsCodegen(\n                list(zip(self._local_rows, self._local_cols)),\n                feature_table_map=self._feature_table_map,\n                pooling_mode=PoolingMode.NONE,\n                use_cpu=device is None\n                or device.type == \"cpu\"\n                or not torch.cuda.is_available(),\n                weights_precision=weights_precision,\n            )\n        )\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = dict(\n            _gen_named_parameters_by_table_dense(\n                self._emb_module, self.table_name_to_count.copy(), self._config\n            )\n        )\n        self.init_parameters()\n\n    @property\n    def emb_module(\n        self,\n    ) -> DenseTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        yield from ()\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        combined_key = \"/\".join(\n            [config.name for config in self._config.embedding_tables]\n        )\n        yield append_prefix(prefix, f\"{combined_key}.weight\"), cast(\n            nn.Parameter, self._emb_module.weights\n        )",
  "class BaseBatchedEmbeddingBag(BaseEmbedding, Generic[SplitWeightType]):\n    def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.distributed.{self.__class__.__name__}\")\n        self._config = config\n        self._pg = pg\n\n        self._pooling: PoolingMode = pooling_type_to_pooling_mode(config.pooling)\n\n        self._local_rows: List[int] = []\n        self._weight_init_mins: List[float] = []\n        self._weight_init_maxs: List[float] = []\n        self._num_embeddings: List[int] = []\n        self._local_cols: List[int] = []\n        self._feature_table_map: List[int] = []\n        self._emb_names: List[str] = []\n        self._lengths_per_emb: List[int] = []\n        self.table_name_to_count: Dict[str, int] = {}\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = {}\n\n        for idx, config in enumerate(self._config.embedding_tables):\n            self._local_rows.append(config.local_rows)\n            self._weight_init_mins.append(config.get_weight_init_min())\n            self._weight_init_maxs.append(config.get_weight_init_max())\n            self._num_embeddings.append(config.num_embeddings)\n            self._local_cols.append(config.local_cols)\n            self._feature_table_map.extend([idx] * config.num_features())\n            if config.name not in self.table_name_to_count:\n                self.table_name_to_count[config.name] = 0\n            self.table_name_to_count[config.name] += 1\n\n    def init_parameters(self) -> None:\n        # initialize embedding weights\n        assert len(self._num_embeddings) == len(self.split_embedding_weights())\n        for (rows, emb_dim, weight_init_min, weight_init_max, param) in zip(\n            self._local_rows,\n            self._local_cols,\n            self._weight_init_mins,\n            self._weight_init_maxs,\n            self.split_embedding_weights(),\n        ):\n            assert param.shape == (rows, emb_dim)  # pyre-ignore[16]\n            param.data.uniform_(  # pyre-ignore[16]\n                weight_init_min,\n                weight_init_max,\n            )\n\n    def forward(self, features: KeyedJaggedTensor) -> torch.Tensor:\n        weights = features.weights_or_none()\n        if weights is not None and not torch.is_floating_point(weights):\n            weights = None\n        if features.variable_stride_per_key() and isinstance(\n            self.emb_module, SplitTableBatchedEmbeddingBagsCodegen\n        ):\n            return self.emb_module(\n                indices=features.values().long(),\n                offsets=features.offsets().long(),\n                per_sample_weights=weights,\n                batch_size_per_feature_per_rank=features.stride_per_key_per_rank(),\n            )\n        else:\n            return self.emb_module(\n                indices=features.values().long(),\n                offsets=features.offsets().long(),\n                per_sample_weights=weights,\n            )\n\n    # pyre-fixme[14]: `state_dict` overrides method defined in `Module` inconsistently.\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        self.flush()\n        return get_state_dict(\n            self._config.embedding_tables,\n            # pyre-ignore\n            self.split_embedding_weights(),\n            self._pg,\n            destination,\n            prefix,\n        )\n\n    def split_embedding_weights(self) -> List[SplitWeightType]:\n        return self.emb_module.split_embedding_weights()\n\n    @property\n    @abc.abstractmethod\n    def emb_module(\n        self,\n    ) -> Union[\n        DenseTableBatchedEmbeddingBagsCodegen,\n        SplitTableBatchedEmbeddingBagsCodegen,\n        IntNBitTableBatchedEmbeddingBagsCodegen,\n    ]:\n        ...\n\n    @property\n    def config(self) -> GroupedEmbeddingConfig:\n        return self._config\n\n    def flush(self) -> None:\n        pass\n\n    def named_split_embedding_weights(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert (\n            remove_duplicate\n        ), \"remove_duplicate=False not supported in BaseBatchedEmbedding.named_split_embedding_weights\"\n        for config, tensor in zip(\n            self._config.embedding_tables,\n            self.emb_module.split_embedding_weights(),\n        ):\n            key = append_prefix(prefix, f\"{config.name}.weight\")\n            yield key, tensor\n\n    def named_parameters_by_table(\n        self,\n    ) -> Iterator[Tuple[str, TableBatchedEmbeddingSlice]]:\n        \"\"\"\n        Like named_parameters(), but yields table_name and embedding_weights which are wrapped in TableBatchedEmbeddingSlice.\n        For a single table with multiple shards (i.e CW) these are combined into one table/weight.\n        Used in composability.\n        \"\"\"\n        for name, param in self._param_per_table.items():\n            yield name, param",
  "class BatchedFusedEmbeddingBag(\n    BaseBatchedEmbeddingBag[torch.Tensor], FusedOptimizerModule\n):\n    def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        managed: List[EmbeddingLocation] = []\n        compute_devices: List[ComputeDevice] = []\n        for table in config.embedding_tables:\n            assert table.local_cols % 4 == 0, (\n                f\"table {table.name} has local_cols={table.local_cols} \"\n                \"not divisible by 4. \"\n            )\n            if device is not None and device.type == \"cuda\":\n                compute_devices.append(ComputeDevice.CUDA)\n                managed.append(\n                    compute_kernel_to_embedding_location(table.compute_kernel)\n                )\n            else:\n                compute_devices.append(ComputeDevice.CPU)\n                managed.append(EmbeddingLocation.HOST)\n\n        weights_precision = data_type_to_sparse_type(config.data_type)\n        fused_params = config.fused_params or {}\n        if \"cache_precision\" not in fused_params:\n            fused_params[\"cache_precision\"] = weights_precision\n\n        self._emb_module: SplitTableBatchedEmbeddingBagsCodegen = (\n            SplitTableBatchedEmbeddingBagsCodegen(\n                embedding_specs=list(\n                    zip(self._local_rows, self._local_cols, managed, compute_devices)\n                ),\n                feature_table_map=self._feature_table_map,\n                pooling_mode=self._pooling,\n                weights_precision=weights_precision,\n                device=device,\n                **fused_params,\n            )\n        )\n        self._optim: EmbeddingFusedOptimizer = EmbeddingFusedOptimizer(\n            config,\n            self._emb_module,\n            pg,\n        )\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = dict(\n            _gen_named_parameters_by_table_fused(\n                emb_module=self._emb_module,\n                table_name_to_count=self.table_name_to_count.copy(),\n                config=self._config,\n                pg=pg,\n            )\n        )\n        self.init_parameters()\n\n    @property\n    def emb_module(\n        self,\n    ) -> SplitTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module\n\n    @property\n    def fused_optimizer(self) -> FusedOptimizer:\n        return self._optim\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        \"\"\"\n        By convention, fused parameters are designated as buffers because they no longer\n        have gradients available to external optimizers.\n        \"\"\"\n        # TODO can delete this override once SEA is removed\n        yield from ()\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        for name, tensor in self.named_split_embedding_weights(\n            prefix, recurse, remove_duplicate\n        ):\n            # hack before we support optimizer on sharded parameter level\n            # can delete after PEA deprecation\n            param = nn.Parameter(tensor)\n            # pyre-ignore\n            param._in_backward_optimizers = [EmptyFusedOptimizer()]\n            yield name, param\n\n    def flush(self) -> None:\n        self._emb_module.flush()",
  "class BatchedDenseEmbeddingBag(BaseBatchedEmbeddingBag[torch.Tensor]):\n    def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        weights_precision = data_type_to_sparse_type(config.data_type)\n        self._emb_module: DenseTableBatchedEmbeddingBagsCodegen = (\n            DenseTableBatchedEmbeddingBagsCodegen(\n                list(zip(self._local_rows, self._local_cols)),\n                feature_table_map=self._feature_table_map,\n                pooling_mode=self._pooling,\n                use_cpu=device is None\n                or device.type == \"cpu\"\n                or not torch.cuda.is_available(),\n                weights_precision=weights_precision,\n            )\n        )\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = dict(\n            _gen_named_parameters_by_table_dense(\n                self._emb_module, self.table_name_to_count.copy(), self._config\n            )\n        )\n        self.init_parameters()\n\n    @property\n    def emb_module(\n        self,\n    ) -> DenseTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        yield from ()\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        combined_key = \"/\".join(\n            [config.name for config in self._config.embedding_tables]\n        )\n        yield append_prefix(prefix, f\"{combined_key}.weight\"), cast(\n            nn.Parameter, self._emb_module.weights\n        )",
  "def __init__(  # noqa C901\n        self,\n        config: GroupedEmbeddingConfig,\n        emb_module: SplitTableBatchedEmbeddingBagsCodegen,\n        pg: Optional[dist.ProcessGroup] = None,\n        create_for_table: Optional[str] = None,\n        param_weight_for_table: Optional[nn.Parameter] = None,\n    ) -> None:\n        \"\"\"\n        Implementation of a FusedOptimizer. Designed as a base class Embedding kernels\n\n        create_for_table is an optional flag, which if passed in only creates the optimizer for a single table.\n        This optimizer shares data with the broader optimizer (one per embedding kernel)\n        and is used to share step and LR changes\n        \"\"\"\n        self._emb_module: SplitTableBatchedEmbeddingBagsCodegen = emb_module\n        self._pg = pg\n\n        @dataclass\n        class ShardParams:\n            optimizer_states: List[Optional[Tuple[torch.Tensor]]]\n            local_metadata: List[ShardMetadata]\n            embedding_weights: List[torch.Tensor]\n\n        def get_optimizer_rowwise_shard_metadata_and_global_metadata(\n            table_global_metadata: ShardedTensorMetadata,\n            optimizer_state: torch.Tensor,\n            sharding_dim: int,\n        ) -> Tuple[Dict[ShardMetadata, ShardMetadata], ShardedTensorMetadata]:\n\n            table_global_shards_metadata: List[\n                ShardMetadata\n            ] = table_global_metadata.shards_metadata\n\n            # column-wise sharding\n            # sort the metadata based on column offset and\n            # we construct the momentum tensor in row-wise sharded way\n            if sharding_dim == 1:\n                table_global_shards_metadata = sorted(\n                    table_global_shards_metadata,\n                    key=lambda shard: shard.shard_offsets[1],\n                )\n\n            table_shard_metadata_to_optimizer_shard_metadata = {}\n\n            for idx, table_shard_metadata in enumerate(table_global_shards_metadata):\n                offset = table_shard_metadata.shard_offsets[0]\n                # for column-wise sharding, we still create row-wise sharded metadata for optimizer\n                # manually create a row-wise offset\n\n                if sharding_dim == 1:\n                    offset = idx * table_shard_metadata.shard_sizes[0]\n\n                table_shard_metadata_to_optimizer_shard_metadata[\n                    table_shard_metadata\n                ] = ShardMetadata(\n                    shard_sizes=[table_shard_metadata.shard_sizes[0]],\n                    shard_offsets=[offset],\n                    placement=table_shard_metadata.placement,\n                )\n\n            tensor_properties = TensorProperties(\n                dtype=optimizer_state.dtype,\n                layout=optimizer_state.layout,\n                requires_grad=False,\n            )\n            len_rw_shards = (\n                len(table_shard_metadata_to_optimizer_shard_metadata)\n                if sharding_dim == 1\n                else 1\n            )\n            rowwise_optimizer_st_metadata = ShardedTensorMetadata(\n                shards_metadata=list(\n                    table_shard_metadata_to_optimizer_shard_metadata.values()\n                ),\n                size=torch.Size([table_global_metadata.size[0] * len_rw_shards]),\n                tensor_properties=tensor_properties,\n            )\n\n            return (\n                table_shard_metadata_to_optimizer_shard_metadata,\n                rowwise_optimizer_st_metadata,\n            )\n\n        def get_optimizer_pointwise_shard_metadata_and_global_metadata(\n            table_global_metadata: ShardedTensorMetadata,\n            optimizer_state: torch.Tensor,\n        ) -> Tuple[Dict[ShardMetadata, ShardMetadata], ShardedTensorMetadata]:\n            table_global_shards_metadata: List[\n                ShardMetadata\n            ] = table_global_metadata.shards_metadata\n\n            table_shard_metadata_to_optimizer_shard_metadata = {}\n\n            for table_shard_metadata in table_global_shards_metadata:\n                table_shard_metadata_to_optimizer_shard_metadata[\n                    table_shard_metadata\n                ] = ShardMetadata(\n                    shard_sizes=table_shard_metadata.shard_sizes,\n                    shard_offsets=table_shard_metadata.shard_offsets,\n                    placement=table_shard_metadata.placement,\n                )\n            tensor_properties = TensorProperties(\n                dtype=optimizer_state.dtype,\n                layout=optimizer_state.layout,\n                requires_grad=False,\n            )\n            pointwise_optimizer_st_metadata = ShardedTensorMetadata(\n                shards_metadata=list(\n                    table_shard_metadata_to_optimizer_shard_metadata.values()\n                ),\n                size=table_global_metadata.size,\n                tensor_properties=tensor_properties,\n            )\n\n            return (\n                table_shard_metadata_to_optimizer_shard_metadata,\n                pointwise_optimizer_st_metadata,\n            )\n\n        # pyre-ignore [33]\n        state: Dict[Any, Any] = {}\n        param_group: Dict[str, Any] = {\n            \"params\": [],\n            \"lr\": emb_module.optimizer_args.learning_rate,\n        }\n\n        params: Dict[str, Union[torch.Tensor, ShardedTensor]] = {}\n\n        # Fused optimizers use buffers (they don't use autograd) and we want to make sure\n        # that state_dict look identical to no-fused version.\n        table_to_shard_params: Dict[str, ShardParams] = {}\n\n        embedding_weights_by_table = emb_module.split_embedding_weights()\n\n        all_optimizer_states = emb_module.get_optimizer_state()\n        optimizer_states_keys_by_table: Dict[str, List[torch.Tensor]] = {}\n\n        for (table_config, optimizer_states, weight,) in itertools.zip_longest(\n            config.embedding_tables,\n            all_optimizer_states,\n            embedding_weights_by_table,\n        ):\n            # When EmbeddingFusedOptimizer is created for composability, only create state\n            if create_for_table is not None and create_for_table != table_config.name:\n                continue\n            if table_config.name not in table_to_shard_params:\n                table_to_shard_params[table_config.name] = ShardParams(\n                    optimizer_states=[], local_metadata=[], embedding_weights=[]\n                )\n            optimizer_state_values = None\n            if optimizer_states:\n                optimizer_state_values = tuple(optimizer_states.values())\n                for optimizer_state_value in optimizer_state_values:\n                    assert table_config.local_rows == optimizer_state_value.size(0)\n                optimizer_states_keys_by_table[table_config.name] = list(\n                    optimizer_states.keys()\n                )\n            local_metadata = table_config.local_metadata\n\n            table_to_shard_params[table_config.name].optimizer_states.append(\n                optimizer_state_values\n            )\n            table_to_shard_params[table_config.name].local_metadata.append(\n                local_metadata\n            )\n            table_to_shard_params[table_config.name].embedding_weights.append(weight)\n\n        seen_tables = set()\n        for table_config in config.embedding_tables:\n            if create_for_table is not None and create_for_table != table_config.name:\n                continue\n            if table_config.name in seen_tables:\n                continue\n            seen_tables.add(table_config.name)\n            table_config_global_metadata: Optional[\n                ShardedTensorMetadata\n            ] = copy.deepcopy(table_config.global_metadata)\n\n            shard_params: ShardParams = table_to_shard_params[table_config.name]\n\n            assert table_config_global_metadata is not None\n            if create_for_table is None:\n                local_weight_shards = []\n                for local_weight, local_metadata in zip(\n                    shard_params.embedding_weights, shard_params.local_metadata\n                ):\n                    local_weight_shards.append(Shard(local_weight, local_metadata))\n                    table_config_global_metadata.tensor_properties.dtype = (\n                        local_weight.dtype\n                    )\n                    table_config_global_metadata.tensor_properties.requires_grad = (\n                        local_weight.requires_grad\n                    )\n                # TODO share this logic to create the same TableBatchedEmbeddingSlice in FusedModules below\n                weight = ShardedTensor._init_from_local_shards_and_global_metadata(\n                    local_shards=local_weight_shards,\n                    sharded_tensor_metadata=table_config_global_metadata,\n                    process_group=self._pg,\n                )\n                param_key = table_config.name + \".weight\"\n            else:\n                assert (\n                    param_weight_for_table is not None\n                ), \"param_weight_for_table cannot be None when using create_for_table\"\n                weight = param_weight_for_table\n                param_key = \"\"\n\n            state[weight] = {}\n            param_group[\"params\"].append(weight)\n            params[param_key] = weight\n\n            # Setting optimizer states\n            sharding_dim: int = (\n                1 if table_config.local_cols != table_config.embedding_dim else 0\n            )\n\n            if all(\n                [opt_state is not None for opt_state in shard_params.optimizer_states]\n            ):\n                # pyre-ignore\n                def get_sharded_optim_state(momentum_idx: int) -> ShardedTensor:\n                    assert momentum_idx > 0\n                    momentum_local_shards: List[Shard] = []\n                    optimizer_sharded_tensor_metadata: ShardedTensorMetadata\n\n                    is_rowwise_optimizer_state: bool = (\n                        # pyre-ignore\n                        shard_params.optimizer_states[0][momentum_idx - 1].dim()\n                        == 1\n                    )\n\n                    if is_rowwise_optimizer_state:\n                        (\n                            table_shard_metadata_to_optimizer_shard_metadata,\n                            optimizer_sharded_tensor_metadata,\n                        ) = get_optimizer_rowwise_shard_metadata_and_global_metadata(\n                            table_config.global_metadata,\n                            shard_params.optimizer_states[0][momentum_idx - 1],\n                            sharding_dim,\n                        )\n                    else:\n                        (\n                            table_shard_metadata_to_optimizer_shard_metadata,\n                            optimizer_sharded_tensor_metadata,\n                        ) = get_optimizer_pointwise_shard_metadata_and_global_metadata(\n                            table_config.global_metadata,\n                            shard_params.optimizer_states[0][momentum_idx - 1],\n                        )\n\n                    for (optimizer_state, table_shard_local_metadata) in zip(\n                        shard_params.optimizer_states, shard_params.local_metadata\n                    ):\n                        local_optimizer_shard_metadata = (\n                            table_shard_metadata_to_optimizer_shard_metadata[\n                                table_shard_local_metadata\n                            ]\n                        )\n                        momentum_local_shards.append(\n                            Shard(\n                                optimizer_state[momentum_idx - 1],\n                                local_optimizer_shard_metadata,\n                            )\n                        )\n\n                    # TODO we should be creating this in SPMD fashion (e.g. init_from_local_shards), and let it derive global metadata.\n                    return ShardedTensor._init_from_local_shards_and_global_metadata(\n                        local_shards=momentum_local_shards,\n                        sharded_tensor_metadata=optimizer_sharded_tensor_metadata,\n                        process_group=self._pg,\n                    )\n\n                num_states: int = min(\n                    # pyre-ignore\n                    [len(opt_state) for opt_state in shard_params.optimizer_states]\n                )\n                optimizer_state_keys = []\n                if num_states > 0:\n                    optimizer_state_keys = optimizer_states_keys_by_table[\n                        table_config.name\n                    ]\n                for cur_state_idx in range(0, num_states):\n                    if cur_state_idx == 0:\n                        # for backward compatibility\n                        cur_state_key = \"momentum1\"\n                    else:\n                        cur_state_key = optimizer_state_keys[cur_state_idx]\n\n                    state[weight][\n                        f\"{table_config.name}.{cur_state_key}\"\n                    ] = get_sharded_optim_state(cur_state_idx + 1)\n\n        super().__init__(params, state, [param_group])",
  "def zero_grad(self, set_to_none: bool = False) -> None:\n        # pyre-ignore [16]\n        self._emb_module.set_learning_rate(self.param_groups[0][\"lr\"])",
  "def step(self, closure: Any = None) -> None:\n        # pyre-ignore [16]\n        self._emb_module.set_learning_rate(self.param_groups[0][\"lr\"])",
  "def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.distributed.{self.__class__.__name__}\")\n        self._config = config\n        self._pg = pg\n\n        self._local_rows: List[int] = []\n        self._weight_init_mins: List[float] = []\n        self._weight_init_maxs: List[float] = []\n        self._num_embeddings: List[int] = []\n        self._local_cols: List[int] = []\n        self._feature_table_map: List[int] = []\n        self.table_name_to_count: Dict[str, int] = {}\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = {}\n\n        for idx, config in enumerate(self._config.embedding_tables):\n            self._local_rows.append(config.local_rows)\n            self._weight_init_mins.append(config.get_weight_init_min())\n            self._weight_init_maxs.append(config.get_weight_init_max())\n            self._num_embeddings.append(config.num_embeddings)\n            self._local_cols.append(config.local_cols)\n            self._feature_table_map.extend([idx] * config.num_features())\n            if config.name not in self.table_name_to_count:\n                self.table_name_to_count[config.name] = 0\n            self.table_name_to_count[config.name] += 1",
  "def init_parameters(self) -> None:\n        # initialize embedding weights\n        assert len(self._num_embeddings) == len(self.split_embedding_weights())\n        for (rows, emb_dim, weight_init_min, weight_init_max, param) in zip(\n            self._local_rows,\n            self._local_cols,\n            self._weight_init_mins,\n            self._weight_init_maxs,\n            self.split_embedding_weights(),\n        ):\n            assert param.shape == (rows, emb_dim)  # pyre-ignore[16]\n            param.data.uniform_(  # pyre-ignore[16]\n                weight_init_min,\n                weight_init_max,\n            )",
  "def forward(self, features: KeyedJaggedTensor) -> torch.Tensor:\n        return self.emb_module(\n            indices=features.values().long(),\n            offsets=features.offsets().long(),\n        )",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        self.flush()\n        return get_state_dict(\n            self._config.embedding_tables,\n            # pyre-ignore\n            self.split_embedding_weights(),\n            self._pg,\n            destination,\n            prefix,\n        )",
  "def split_embedding_weights(self) -> List[SplitWeightType]:\n        return self.emb_module.split_embedding_weights()",
  "def emb_module(\n        self,\n    ) -> Union[\n        DenseTableBatchedEmbeddingBagsCodegen,\n        SplitTableBatchedEmbeddingBagsCodegen,\n        IntNBitTableBatchedEmbeddingBagsCodegen,\n    ]:\n        ...",
  "def config(self) -> GroupedEmbeddingConfig:\n        return self._config",
  "def flush(self) -> None:\n        pass",
  "def named_split_embedding_weights(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert (\n            remove_duplicate\n        ), \"remove_duplicate=False not supported in BaseBatchedEmbedding.named_split_embedding_weights\"\n        for config, param in zip(\n            self._config.embedding_tables,\n            self.emb_module.split_embedding_weights(),\n        ):\n            key = append_prefix(prefix, f\"{config.name}.weight\")\n            yield key, param",
  "def named_parameters_by_table(\n        self,\n    ) -> Iterator[Tuple[str, TableBatchedEmbeddingSlice]]:\n        \"\"\"\n        Like named_parameters(), but yields table_name and embedding_weights which are wrapped in TableBatchedEmbeddingSlice.\n        For a single table with multiple shards (i.e CW) these are combined into one table/weight.\n        Used in composability.\n        \"\"\"\n        for name, param in self._param_per_table.items():\n            yield name, param",
  "def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        managed: List[EmbeddingLocation] = []\n        compute_devices: List[ComputeDevice] = []\n        for table in config.embedding_tables:\n            if device is not None and device.type == \"cuda\":\n                compute_devices.append(ComputeDevice.CUDA)\n                managed.append(\n                    compute_kernel_to_embedding_location(table.compute_kernel)\n                )\n            else:\n                compute_devices.append(ComputeDevice.CPU)\n                managed.append(EmbeddingLocation.HOST)\n\n        weights_precision = data_type_to_sparse_type(config.data_type)\n\n        fused_params = config.fused_params or {}\n        if \"cache_precision\" not in fused_params:\n            fused_params[\"cache_precision\"] = weights_precision\n\n        self._emb_module: SplitTableBatchedEmbeddingBagsCodegen = (\n            SplitTableBatchedEmbeddingBagsCodegen(\n                embedding_specs=list(\n                    zip(self._local_rows, self._local_cols, managed, compute_devices)\n                ),\n                feature_table_map=self._feature_table_map,\n                pooling_mode=PoolingMode.NONE,\n                weights_precision=weights_precision,\n                device=device,\n                **fused_params,\n            )\n        )\n        self._optim: EmbeddingFusedOptimizer = EmbeddingFusedOptimizer(\n            config,\n            self._emb_module,\n            pg,\n        )\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = dict(\n            _gen_named_parameters_by_table_fused(\n                emb_module=self._emb_module,\n                table_name_to_count=self.table_name_to_count.copy(),\n                config=self._config,\n                pg=pg,\n            )\n        )\n        self.init_parameters()",
  "def emb_module(\n        self,\n    ) -> SplitTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module",
  "def fused_optimizer(self) -> FusedOptimizer:\n        return self._optim",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        \"\"\"\n        By convention, fused parameters are designated as buffers because they no longer\n        have gradients available to external optimizers.\n        \"\"\"\n        # TODO can delete this override once SEA is removed\n        yield from ()",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        for name, tensor in self.named_split_embedding_weights(\n            prefix, recurse, remove_duplicate\n        ):\n            # hack before we support optimizer on sharded parameter level\n            # can delete after SEA deprecation\n            param = nn.Parameter(tensor)\n            # pyre-ignore\n            param._in_backward_optimizers = [EmptyFusedOptimizer()]\n            yield name, param",
  "def flush(self) -> None:\n        self._emb_module.flush()",
  "def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        weights_precision = data_type_to_sparse_type(config.data_type)\n        self._emb_module: DenseTableBatchedEmbeddingBagsCodegen = (\n            DenseTableBatchedEmbeddingBagsCodegen(\n                list(zip(self._local_rows, self._local_cols)),\n                feature_table_map=self._feature_table_map,\n                pooling_mode=PoolingMode.NONE,\n                use_cpu=device is None\n                or device.type == \"cpu\"\n                or not torch.cuda.is_available(),\n                weights_precision=weights_precision,\n            )\n        )\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = dict(\n            _gen_named_parameters_by_table_dense(\n                self._emb_module, self.table_name_to_count.copy(), self._config\n            )\n        )\n        self.init_parameters()",
  "def emb_module(\n        self,\n    ) -> DenseTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        yield from ()",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        combined_key = \"/\".join(\n            [config.name for config in self._config.embedding_tables]\n        )\n        yield append_prefix(prefix, f\"{combined_key}.weight\"), cast(\n            nn.Parameter, self._emb_module.weights\n        )",
  "def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.distributed.{self.__class__.__name__}\")\n        self._config = config\n        self._pg = pg\n\n        self._pooling: PoolingMode = pooling_type_to_pooling_mode(config.pooling)\n\n        self._local_rows: List[int] = []\n        self._weight_init_mins: List[float] = []\n        self._weight_init_maxs: List[float] = []\n        self._num_embeddings: List[int] = []\n        self._local_cols: List[int] = []\n        self._feature_table_map: List[int] = []\n        self._emb_names: List[str] = []\n        self._lengths_per_emb: List[int] = []\n        self.table_name_to_count: Dict[str, int] = {}\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = {}\n\n        for idx, config in enumerate(self._config.embedding_tables):\n            self._local_rows.append(config.local_rows)\n            self._weight_init_mins.append(config.get_weight_init_min())\n            self._weight_init_maxs.append(config.get_weight_init_max())\n            self._num_embeddings.append(config.num_embeddings)\n            self._local_cols.append(config.local_cols)\n            self._feature_table_map.extend([idx] * config.num_features())\n            if config.name not in self.table_name_to_count:\n                self.table_name_to_count[config.name] = 0\n            self.table_name_to_count[config.name] += 1",
  "def init_parameters(self) -> None:\n        # initialize embedding weights\n        assert len(self._num_embeddings) == len(self.split_embedding_weights())\n        for (rows, emb_dim, weight_init_min, weight_init_max, param) in zip(\n            self._local_rows,\n            self._local_cols,\n            self._weight_init_mins,\n            self._weight_init_maxs,\n            self.split_embedding_weights(),\n        ):\n            assert param.shape == (rows, emb_dim)  # pyre-ignore[16]\n            param.data.uniform_(  # pyre-ignore[16]\n                weight_init_min,\n                weight_init_max,\n            )",
  "def forward(self, features: KeyedJaggedTensor) -> torch.Tensor:\n        weights = features.weights_or_none()\n        if weights is not None and not torch.is_floating_point(weights):\n            weights = None\n        if features.variable_stride_per_key() and isinstance(\n            self.emb_module, SplitTableBatchedEmbeddingBagsCodegen\n        ):\n            return self.emb_module(\n                indices=features.values().long(),\n                offsets=features.offsets().long(),\n                per_sample_weights=weights,\n                batch_size_per_feature_per_rank=features.stride_per_key_per_rank(),\n            )\n        else:\n            return self.emb_module(\n                indices=features.values().long(),\n                offsets=features.offsets().long(),\n                per_sample_weights=weights,\n            )",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        self.flush()\n        return get_state_dict(\n            self._config.embedding_tables,\n            # pyre-ignore\n            self.split_embedding_weights(),\n            self._pg,\n            destination,\n            prefix,\n        )",
  "def split_embedding_weights(self) -> List[SplitWeightType]:\n        return self.emb_module.split_embedding_weights()",
  "def emb_module(\n        self,\n    ) -> Union[\n        DenseTableBatchedEmbeddingBagsCodegen,\n        SplitTableBatchedEmbeddingBagsCodegen,\n        IntNBitTableBatchedEmbeddingBagsCodegen,\n    ]:\n        ...",
  "def config(self) -> GroupedEmbeddingConfig:\n        return self._config",
  "def flush(self) -> None:\n        pass",
  "def named_split_embedding_weights(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        assert (\n            remove_duplicate\n        ), \"remove_duplicate=False not supported in BaseBatchedEmbedding.named_split_embedding_weights\"\n        for config, tensor in zip(\n            self._config.embedding_tables,\n            self.emb_module.split_embedding_weights(),\n        ):\n            key = append_prefix(prefix, f\"{config.name}.weight\")\n            yield key, tensor",
  "def named_parameters_by_table(\n        self,\n    ) -> Iterator[Tuple[str, TableBatchedEmbeddingSlice]]:\n        \"\"\"\n        Like named_parameters(), but yields table_name and embedding_weights which are wrapped in TableBatchedEmbeddingSlice.\n        For a single table with multiple shards (i.e CW) these are combined into one table/weight.\n        Used in composability.\n        \"\"\"\n        for name, param in self._param_per_table.items():\n            yield name, param",
  "def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        managed: List[EmbeddingLocation] = []\n        compute_devices: List[ComputeDevice] = []\n        for table in config.embedding_tables:\n            assert table.local_cols % 4 == 0, (\n                f\"table {table.name} has local_cols={table.local_cols} \"\n                \"not divisible by 4. \"\n            )\n            if device is not None and device.type == \"cuda\":\n                compute_devices.append(ComputeDevice.CUDA)\n                managed.append(\n                    compute_kernel_to_embedding_location(table.compute_kernel)\n                )\n            else:\n                compute_devices.append(ComputeDevice.CPU)\n                managed.append(EmbeddingLocation.HOST)\n\n        weights_precision = data_type_to_sparse_type(config.data_type)\n        fused_params = config.fused_params or {}\n        if \"cache_precision\" not in fused_params:\n            fused_params[\"cache_precision\"] = weights_precision\n\n        self._emb_module: SplitTableBatchedEmbeddingBagsCodegen = (\n            SplitTableBatchedEmbeddingBagsCodegen(\n                embedding_specs=list(\n                    zip(self._local_rows, self._local_cols, managed, compute_devices)\n                ),\n                feature_table_map=self._feature_table_map,\n                pooling_mode=self._pooling,\n                weights_precision=weights_precision,\n                device=device,\n                **fused_params,\n            )\n        )\n        self._optim: EmbeddingFusedOptimizer = EmbeddingFusedOptimizer(\n            config,\n            self._emb_module,\n            pg,\n        )\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = dict(\n            _gen_named_parameters_by_table_fused(\n                emb_module=self._emb_module,\n                table_name_to_count=self.table_name_to_count.copy(),\n                config=self._config,\n                pg=pg,\n            )\n        )\n        self.init_parameters()",
  "def emb_module(\n        self,\n    ) -> SplitTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module",
  "def fused_optimizer(self) -> FusedOptimizer:\n        return self._optim",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        \"\"\"\n        By convention, fused parameters are designated as buffers because they no longer\n        have gradients available to external optimizers.\n        \"\"\"\n        # TODO can delete this override once SEA is removed\n        yield from ()",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        for name, tensor in self.named_split_embedding_weights(\n            prefix, recurse, remove_duplicate\n        ):\n            # hack before we support optimizer on sharded parameter level\n            # can delete after PEA deprecation\n            param = nn.Parameter(tensor)\n            # pyre-ignore\n            param._in_backward_optimizers = [EmptyFusedOptimizer()]\n            yield name, param",
  "def flush(self) -> None:\n        self._emb_module.flush()",
  "def __init__(\n        self,\n        config: GroupedEmbeddingConfig,\n        pg: Optional[dist.ProcessGroup] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__(config, pg, device)\n\n        weights_precision = data_type_to_sparse_type(config.data_type)\n        self._emb_module: DenseTableBatchedEmbeddingBagsCodegen = (\n            DenseTableBatchedEmbeddingBagsCodegen(\n                list(zip(self._local_rows, self._local_cols)),\n                feature_table_map=self._feature_table_map,\n                pooling_mode=self._pooling,\n                use_cpu=device is None\n                or device.type == \"cpu\"\n                or not torch.cuda.is_available(),\n                weights_precision=weights_precision,\n            )\n        )\n        self._param_per_table: Dict[str, TableBatchedEmbeddingSlice] = dict(\n            _gen_named_parameters_by_table_dense(\n                self._emb_module, self.table_name_to_count.copy(), self._config\n            )\n        )\n        self.init_parameters()",
  "def emb_module(\n        self,\n    ) -> DenseTableBatchedEmbeddingBagsCodegen:\n        return self._emb_module",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        yield from ()",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        combined_key = \"/\".join(\n            [config.name for config in self._config.embedding_tables]\n        )\n        yield append_prefix(prefix, f\"{combined_key}.weight\"), cast(\n            nn.Parameter, self._emb_module.weights\n        )",
  "class ShardParams:\n            optimizer_states: List[Optional[Tuple[torch.Tensor]]]\n            local_metadata: List[ShardMetadata]\n            embedding_weights: List[torch.Tensor]",
  "def get_optimizer_rowwise_shard_metadata_and_global_metadata(\n            table_global_metadata: ShardedTensorMetadata,\n            optimizer_state: torch.Tensor,\n            sharding_dim: int,\n        ) -> Tuple[Dict[ShardMetadata, ShardMetadata], ShardedTensorMetadata]:\n\n            table_global_shards_metadata: List[\n                ShardMetadata\n            ] = table_global_metadata.shards_metadata\n\n            # column-wise sharding\n            # sort the metadata based on column offset and\n            # we construct the momentum tensor in row-wise sharded way\n            if sharding_dim == 1:\n                table_global_shards_metadata = sorted(\n                    table_global_shards_metadata,\n                    key=lambda shard: shard.shard_offsets[1],\n                )\n\n            table_shard_metadata_to_optimizer_shard_metadata = {}\n\n            for idx, table_shard_metadata in enumerate(table_global_shards_metadata):\n                offset = table_shard_metadata.shard_offsets[0]\n                # for column-wise sharding, we still create row-wise sharded metadata for optimizer\n                # manually create a row-wise offset\n\n                if sharding_dim == 1:\n                    offset = idx * table_shard_metadata.shard_sizes[0]\n\n                table_shard_metadata_to_optimizer_shard_metadata[\n                    table_shard_metadata\n                ] = ShardMetadata(\n                    shard_sizes=[table_shard_metadata.shard_sizes[0]],\n                    shard_offsets=[offset],\n                    placement=table_shard_metadata.placement,\n                )\n\n            tensor_properties = TensorProperties(\n                dtype=optimizer_state.dtype,\n                layout=optimizer_state.layout,\n                requires_grad=False,\n            )\n            len_rw_shards = (\n                len(table_shard_metadata_to_optimizer_shard_metadata)\n                if sharding_dim == 1\n                else 1\n            )\n            rowwise_optimizer_st_metadata = ShardedTensorMetadata(\n                shards_metadata=list(\n                    table_shard_metadata_to_optimizer_shard_metadata.values()\n                ),\n                size=torch.Size([table_global_metadata.size[0] * len_rw_shards]),\n                tensor_properties=tensor_properties,\n            )\n\n            return (\n                table_shard_metadata_to_optimizer_shard_metadata,\n                rowwise_optimizer_st_metadata,\n            )",
  "def get_optimizer_pointwise_shard_metadata_and_global_metadata(\n            table_global_metadata: ShardedTensorMetadata,\n            optimizer_state: torch.Tensor,\n        ) -> Tuple[Dict[ShardMetadata, ShardMetadata], ShardedTensorMetadata]:\n            table_global_shards_metadata: List[\n                ShardMetadata\n            ] = table_global_metadata.shards_metadata\n\n            table_shard_metadata_to_optimizer_shard_metadata = {}\n\n            for table_shard_metadata in table_global_shards_metadata:\n                table_shard_metadata_to_optimizer_shard_metadata[\n                    table_shard_metadata\n                ] = ShardMetadata(\n                    shard_sizes=table_shard_metadata.shard_sizes,\n                    shard_offsets=table_shard_metadata.shard_offsets,\n                    placement=table_shard_metadata.placement,\n                )\n            tensor_properties = TensorProperties(\n                dtype=optimizer_state.dtype,\n                layout=optimizer_state.layout,\n                requires_grad=False,\n            )\n            pointwise_optimizer_st_metadata = ShardedTensorMetadata(\n                shards_metadata=list(\n                    table_shard_metadata_to_optimizer_shard_metadata.values()\n                ),\n                size=table_global_metadata.size,\n                tensor_properties=tensor_properties,\n            )\n\n            return (\n                table_shard_metadata_to_optimizer_shard_metadata,\n                pointwise_optimizer_st_metadata,\n            )",
  "def get_sharded_optim_state(momentum_idx: int) -> ShardedTensor:\n                    assert momentum_idx > 0\n                    momentum_local_shards: List[Shard] = []\n                    optimizer_sharded_tensor_metadata: ShardedTensorMetadata\n\n                    is_rowwise_optimizer_state: bool = (\n                        # pyre-ignore\n                        shard_params.optimizer_states[0][momentum_idx - 1].dim()\n                        == 1\n                    )\n\n                    if is_rowwise_optimizer_state:\n                        (\n                            table_shard_metadata_to_optimizer_shard_metadata,\n                            optimizer_sharded_tensor_metadata,\n                        ) = get_optimizer_rowwise_shard_metadata_and_global_metadata(\n                            table_config.global_metadata,\n                            shard_params.optimizer_states[0][momentum_idx - 1],\n                            sharding_dim,\n                        )\n                    else:\n                        (\n                            table_shard_metadata_to_optimizer_shard_metadata,\n                            optimizer_sharded_tensor_metadata,\n                        ) = get_optimizer_pointwise_shard_metadata_and_global_metadata(\n                            table_config.global_metadata,\n                            shard_params.optimizer_states[0][momentum_idx - 1],\n                        )\n\n                    for (optimizer_state, table_shard_local_metadata) in zip(\n                        shard_params.optimizer_states, shard_params.local_metadata\n                    ):\n                        local_optimizer_shard_metadata = (\n                            table_shard_metadata_to_optimizer_shard_metadata[\n                                table_shard_local_metadata\n                            ]\n                        )\n                        momentum_local_shards.append(\n                            Shard(\n                                optimizer_state[momentum_idx - 1],\n                                local_optimizer_shard_metadata,\n                            )\n                        )\n\n                    # TODO we should be creating this in SPMD fashion (e.g. init_from_local_shards), and let it derive global metadata.\n                    return ShardedTensor._init_from_local_shards_and_global_metadata(\n                        local_shards=momentum_local_shards,\n                        sharded_tensor_metadata=optimizer_sharded_tensor_metadata,\n                        process_group=self._pg,\n                    )",
  "def param_dp_sync(kt: KeyedTensor, no_op_tensor: torch.Tensor) -> KeyedTensor:\n    kt._values.add_(no_op_tensor)\n    return kt",
  "class ShardedFeatureProcessedEmbeddingBagCollection(\n    ShardedEmbeddingModule[\n        KJTList, List[torch.Tensor], KeyedTensor, EmbeddingBagCollectionContext\n    ]\n):\n    def __init__(\n        self,\n        module: FeatureProcessedEmbeddingBagCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        ebc_sharder: EmbeddingBagCollectionSharder,\n        env: ShardingEnv,\n        device: torch.device,\n    ) -> None:\n        super().__init__()\n\n        self._device = device\n        self._env = env\n\n        self._embedding_bag_collection: ShardedEmbeddingBagCollection = (\n            ebc_sharder.shard(\n                module._embedding_bag_collection,\n                table_name_to_parameter_sharding,\n                env=env,\n                device=device,\n            )\n        )\n\n        self._lookups: List[nn.Module] = self._embedding_bag_collection._lookups\n\n        self._is_collection: bool = False\n        self._feature_processors: Union[nn.ModuleDict, FeatureProcessorsCollection]\n        if isinstance(module._feature_processors, FeatureProcessorsCollection):\n            self._feature_processors = module._feature_processors.to(device)\n            self._is_collection = True\n        else:\n            self._feature_processors = torch.nn.ModuleDict(\n                {key: fp.to(device) for key, fp in module._feature_processors.items()}\n            )\n            self._is_collection = False\n\n        self._no_op_zero: torch.Tensor = torch.zeros((1,), device=self._device)\n\n    # pyre-ignore\n    def input_dist(\n        self, ctx: EmbeddingBagCollectionContext, features: KeyedJaggedTensor\n    ) -> Awaitable[Awaitable[KJTList]]:\n        return self._embedding_bag_collection.input_dist(ctx, features)\n\n    def apply_feature_processors_to_kjt_list(self, dist_input: KJTList) -> KJTList:\n        kjt_list = []\n        for features in dist_input:\n            if self._is_collection:\n                kjt_list.append(self._feature_processors(features))\n            else:\n                kjt_list.append(\n                    apply_feature_processors_to_kjt(\n                        features,\n                        self._feature_processors,\n                    )\n                )\n        return KJTList(kjt_list)\n\n    def compute(\n        self,\n        ctx: EmbeddingBagCollectionContext,\n        dist_input: KJTList,\n    ) -> List[torch.Tensor]:\n\n        fp_features = self.apply_feature_processors_to_kjt_list(dist_input)\n        return self._embedding_bag_collection.compute(ctx, fp_features)\n\n    def output_dist(\n        self,\n        ctx: EmbeddingBagCollectionContext,\n        output: List[torch.Tensor],\n    ) -> LazyAwaitable[KeyedTensor]:\n        lazy_awaitable_kt = self._embedding_bag_collection.output_dist(ctx, output)\n        return self.add_fp_params_grad_sync_callback(lazy_awaitable_kt)\n\n    def compute_and_output_dist(\n        self, ctx: EmbeddingBagCollectionContext, input: KJTList\n    ) -> LazyAwaitable[KeyedTensor]:\n        fp_features = self.apply_feature_processors_to_kjt_list(input)\n        lazy_awaitable_kt = self._embedding_bag_collection.compute_and_output_dist(\n            ctx, fp_features\n        )\n        return self.add_fp_params_grad_sync_callback(lazy_awaitable_kt)\n\n    def add_fp_params_grad_sync_callback(\n        self, lazy_awaitable_kt: LazyAwaitable[KeyedTensor]\n    ) -> LazyAwaitable[KeyedTensor]:\n        # This will ensure that all feature processor parameters participate in the\n        # autograd graph across all ranks. This will protect from mismatched collective\n        # calls order when using DistributedDataParallel over feature processors.\n        no_op_tensor = (\n            self._no_op_zero\n            * torch.cat(\n                [x.flatten() for x in self._feature_processors.parameters()]\n            ).sum()\n        )\n        lazy_awaitable_kt.callbacks.append(\n            partial(param_dp_sync, no_op_tensor=no_op_tensor)\n        )\n        return lazy_awaitable_kt\n\n    def create_context(self) -> EmbeddingBagCollectionContext:\n        return self._embedding_bag_collection.create_context()\n\n    def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        for fqn, _ in self.named_parameters():\n            if \"_embedding_bag_collection\" in fqn:\n                yield append_prefix(prefix, fqn)",
  "class FeatureProcessedEmbeddingBagCollectionSharder(\n    BaseEmbeddingSharder[FeatureProcessedEmbeddingBagCollection]\n):\n    def __init__(\n        self,\n        ebc_sharder: Optional[EmbeddingBagCollectionSharder] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._ebc_sharder: EmbeddingBagCollectionSharder = (\n            ebc_sharder or EmbeddingBagCollectionSharder(self.qcomm_codecs_registry)\n        )\n\n    def shard(\n        self,\n        module: FeatureProcessedEmbeddingBagCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedFeatureProcessedEmbeddingBagCollection:\n\n        if device is None:\n            device = torch.device(\"cuda\")\n\n        return ShardedFeatureProcessedEmbeddingBagCollection(\n            module,\n            params,\n            ebc_sharder=self._ebc_sharder,\n            env=env,\n            device=device,\n        )\n\n    def shardable_parameters(\n        self, module: FeatureProcessedEmbeddingBagCollection\n    ) -> Dict[str, torch.nn.Parameter]:\n        return self._ebc_sharder.shardable_parameters(module._embedding_bag_collection)\n\n    @property\n    def module_type(self) -> Type[FeatureProcessedEmbeddingBagCollection]:\n        return FeatureProcessedEmbeddingBagCollection\n\n    def sharding_types(self, compute_device_type: str) -> List[str]:\n        # No row wise because position weighted FP and RW don't play well together.\n        types = [\n            ShardingType.DATA_PARALLEL.value,\n            ShardingType.TABLE_WISE.value,\n            ShardingType.COLUMN_WISE.value,\n            ShardingType.TABLE_COLUMN_WISE.value,\n        ]\n\n        return types",
  "def __init__(\n        self,\n        module: FeatureProcessedEmbeddingBagCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        ebc_sharder: EmbeddingBagCollectionSharder,\n        env: ShardingEnv,\n        device: torch.device,\n    ) -> None:\n        super().__init__()\n\n        self._device = device\n        self._env = env\n\n        self._embedding_bag_collection: ShardedEmbeddingBagCollection = (\n            ebc_sharder.shard(\n                module._embedding_bag_collection,\n                table_name_to_parameter_sharding,\n                env=env,\n                device=device,\n            )\n        )\n\n        self._lookups: List[nn.Module] = self._embedding_bag_collection._lookups\n\n        self._is_collection: bool = False\n        self._feature_processors: Union[nn.ModuleDict, FeatureProcessorsCollection]\n        if isinstance(module._feature_processors, FeatureProcessorsCollection):\n            self._feature_processors = module._feature_processors.to(device)\n            self._is_collection = True\n        else:\n            self._feature_processors = torch.nn.ModuleDict(\n                {key: fp.to(device) for key, fp in module._feature_processors.items()}\n            )\n            self._is_collection = False\n\n        self._no_op_zero: torch.Tensor = torch.zeros((1,), device=self._device)",
  "def input_dist(\n        self, ctx: EmbeddingBagCollectionContext, features: KeyedJaggedTensor\n    ) -> Awaitable[Awaitable[KJTList]]:\n        return self._embedding_bag_collection.input_dist(ctx, features)",
  "def apply_feature_processors_to_kjt_list(self, dist_input: KJTList) -> KJTList:\n        kjt_list = []\n        for features in dist_input:\n            if self._is_collection:\n                kjt_list.append(self._feature_processors(features))\n            else:\n                kjt_list.append(\n                    apply_feature_processors_to_kjt(\n                        features,\n                        self._feature_processors,\n                    )\n                )\n        return KJTList(kjt_list)",
  "def compute(\n        self,\n        ctx: EmbeddingBagCollectionContext,\n        dist_input: KJTList,\n    ) -> List[torch.Tensor]:\n\n        fp_features = self.apply_feature_processors_to_kjt_list(dist_input)\n        return self._embedding_bag_collection.compute(ctx, fp_features)",
  "def output_dist(\n        self,\n        ctx: EmbeddingBagCollectionContext,\n        output: List[torch.Tensor],\n    ) -> LazyAwaitable[KeyedTensor]:\n        lazy_awaitable_kt = self._embedding_bag_collection.output_dist(ctx, output)\n        return self.add_fp_params_grad_sync_callback(lazy_awaitable_kt)",
  "def compute_and_output_dist(\n        self, ctx: EmbeddingBagCollectionContext, input: KJTList\n    ) -> LazyAwaitable[KeyedTensor]:\n        fp_features = self.apply_feature_processors_to_kjt_list(input)\n        lazy_awaitable_kt = self._embedding_bag_collection.compute_and_output_dist(\n            ctx, fp_features\n        )\n        return self.add_fp_params_grad_sync_callback(lazy_awaitable_kt)",
  "def add_fp_params_grad_sync_callback(\n        self, lazy_awaitable_kt: LazyAwaitable[KeyedTensor]\n    ) -> LazyAwaitable[KeyedTensor]:\n        # This will ensure that all feature processor parameters participate in the\n        # autograd graph across all ranks. This will protect from mismatched collective\n        # calls order when using DistributedDataParallel over feature processors.\n        no_op_tensor = (\n            self._no_op_zero\n            * torch.cat(\n                [x.flatten() for x in self._feature_processors.parameters()]\n            ).sum()\n        )\n        lazy_awaitable_kt.callbacks.append(\n            partial(param_dp_sync, no_op_tensor=no_op_tensor)\n        )\n        return lazy_awaitable_kt",
  "def create_context(self) -> EmbeddingBagCollectionContext:\n        return self._embedding_bag_collection.create_context()",
  "def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        for fqn, _ in self.named_parameters():\n            if \"_embedding_bag_collection\" in fqn:\n                yield append_prefix(prefix, fqn)",
  "def __init__(\n        self,\n        ebc_sharder: Optional[EmbeddingBagCollectionSharder] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._ebc_sharder: EmbeddingBagCollectionSharder = (\n            ebc_sharder or EmbeddingBagCollectionSharder(self.qcomm_codecs_registry)\n        )",
  "def shard(\n        self,\n        module: FeatureProcessedEmbeddingBagCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedFeatureProcessedEmbeddingBagCollection:\n\n        if device is None:\n            device = torch.device(\"cuda\")\n\n        return ShardedFeatureProcessedEmbeddingBagCollection(\n            module,\n            params,\n            ebc_sharder=self._ebc_sharder,\n            env=env,\n            device=device,\n        )",
  "def shardable_parameters(\n        self, module: FeatureProcessedEmbeddingBagCollection\n    ) -> Dict[str, torch.nn.Parameter]:\n        return self._ebc_sharder.shardable_parameters(module._embedding_bag_collection)",
  "def module_type(self) -> Type[FeatureProcessedEmbeddingBagCollection]:\n        return FeatureProcessedEmbeddingBagCollection",
  "def sharding_types(self, compute_device_type: str) -> List[str]:\n        # No row wise because position weighted FP and RW don't play well together.\n        types = [\n            ShardingType.DATA_PARALLEL.value,\n            ShardingType.TABLE_WISE.value,\n            ShardingType.COLUMN_WISE.value,\n            ShardingType.TABLE_COLUMN_WISE.value,\n        ]\n\n        return types",
  "class GroupedPositionWeightedModule(BaseGroupedFeatureProcessor):\n    def __init__(\n        self, max_feature_lengths: Dict[str, int], device: Optional[torch.device] = None\n    ) -> None:\n        super().__init__()\n        self.max_feature_lengths = max_feature_lengths\n        for length in self.max_feature_lengths.values():\n            if length <= 0:\n                raise\n        self.position_weights: nn.ParameterDict = nn.ParameterDict()\n        for key, length in max_feature_lengths.items():\n            self.position_weights[key] = nn.Parameter(\n                torch.empty([length], device=device).fill_(1.0)\n            )\n        self.register_buffer(\n            \"_dummy_weights\",\n            torch.tensor(\n                max(self.max_feature_lengths.values()),\n                device=device,\n            ).fill_(1.0),\n        )\n\n    def forward(self, features: KeyedJaggedTensor) -> KeyedJaggedTensor:\n        if features.weights_or_none() is None:\n            cat_seq = torch.ops.fbgemm.offsets_range(\n                features.offsets().long(), torch.numel(features.values())\n            )\n        else:\n            # for row-wise sharding\n            cat_seq = features.weights().long()\n        seqs = torch.split(cat_seq, features.length_per_key())\n        weights_list = []\n        for key, seq in zip(features.keys(), seqs):\n            if key in self.max_feature_lengths:\n                weights_list.append(\n                    torch.gather(self.position_weights[key], dim=0, index=seq)\n                )\n            else:\n                weights_list.append(\n                    self._dummy_weights[: self.max_feature_lengths[key]]\n                )\n        weights = torch.cat(weights_list)\n\n        return KeyedJaggedTensor(\n            keys=features.keys(),\n            values=features.values(),\n            weights=weights,\n            lengths=features.lengths(),\n            offsets=features.offsets(),\n            stride=features.stride(),\n            length_per_key=features.length_per_key(),\n        )\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        for name, param in self.position_weights.items():\n            yield append_prefix(prefix, f\"position_weights.{name}\"), param\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        yield from ()\n\n    # pyre-fixme[14]: `state_dict` overrides method defined in `Module` inconsistently.\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n        for name, param in self.position_weights.items():\n            destination[prefix + f\"position_weights.{name}\"] = (\n                param if keep_vars else param.detach()\n            )\n        return destination",
  "def __init__(\n        self, max_feature_lengths: Dict[str, int], device: Optional[torch.device] = None\n    ) -> None:\n        super().__init__()\n        self.max_feature_lengths = max_feature_lengths\n        for length in self.max_feature_lengths.values():\n            if length <= 0:\n                raise\n        self.position_weights: nn.ParameterDict = nn.ParameterDict()\n        for key, length in max_feature_lengths.items():\n            self.position_weights[key] = nn.Parameter(\n                torch.empty([length], device=device).fill_(1.0)\n            )\n        self.register_buffer(\n            \"_dummy_weights\",\n            torch.tensor(\n                max(self.max_feature_lengths.values()),\n                device=device,\n            ).fill_(1.0),\n        )",
  "def forward(self, features: KeyedJaggedTensor) -> KeyedJaggedTensor:\n        if features.weights_or_none() is None:\n            cat_seq = torch.ops.fbgemm.offsets_range(\n                features.offsets().long(), torch.numel(features.values())\n            )\n        else:\n            # for row-wise sharding\n            cat_seq = features.weights().long()\n        seqs = torch.split(cat_seq, features.length_per_key())\n        weights_list = []\n        for key, seq in zip(features.keys(), seqs):\n            if key in self.max_feature_lengths:\n                weights_list.append(\n                    torch.gather(self.position_weights[key], dim=0, index=seq)\n                )\n            else:\n                weights_list.append(\n                    self._dummy_weights[: self.max_feature_lengths[key]]\n                )\n        weights = torch.cat(weights_list)\n\n        return KeyedJaggedTensor(\n            keys=features.keys(),\n            values=features.values(),\n            weights=weights,\n            lengths=features.lengths(),\n            offsets=features.offsets(),\n            stride=features.stride(),\n            length_per_key=features.length_per_key(),\n        )",
  "def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, nn.Parameter]]:\n        for name, param in self.position_weights.items():\n            yield append_prefix(prefix, f\"position_weights.{name}\"), param",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        yield from ()",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        if destination is None:\n            destination = OrderedDict()\n            # pyre-ignore [16]\n            destination._metadata = OrderedDict()\n        for name, param in self.position_weights.items():\n            destination[prefix + f\"position_weights.{name}\"] = (\n                param if keep_vars else param.detach()\n            )\n        return destination",
  "def _get_recat(\n    local_split: int,\n    num_splits: int,\n    stagger: int = 1,\n    device: Optional[torch.device] = None,\n    batch_size_per_rank: Optional[List[int]] = None,\n) -> Optional[torch.Tensor]:\n    \"\"\"\n    Calculates relevant recat indices required to reorder AlltoAll collective.\n\n    Args:\n        local_split (int): number of features in local split.\n        num_splits (int): number of splits (typically WORLD_SIZE).\n        stagger (int): secondary reordering, (typically 1, but\n            `WORLD_SIZE/LOCAL_WORLD_SIZE` for TWRW).\n        device (Optional[torch.device]): device on which buffer will be allocated.\n        batch_size_per_rank (Optional[List[int]]): batch size per rank, needed for\n            variable batch size.\n\n    Returns:\n        Optional[torch.Tensor]: recat tensor, None if local rank is empty.\n\n    Example::\n\n        _recat(2, 4, 1)\n            # [0, 2, 4, 6, 1, 3, 5, 7]\n        _recat(2, 4, 2)\n            # [0, 4, 2, 6, 1, 5, 3, 7]\n        _recat(0, 4, 2)\n            # None\n    \"\"\"\n    with record_function(\"## all2all_data:recat_permute_gen ##\"):\n        if local_split == 0:\n            return None\n\n        recat: List[int] = []\n\n        feature_order: List[int] = [\n            x + num_splits // stagger * y\n            for x in range(num_splits // stagger)\n            for y in range(stagger)\n        ]\n\n        for i in range(local_split):\n            for j in feature_order:  # range(num_splits):\n                recat.append(i + j * local_split)\n\n        # variable batch size\n        if batch_size_per_rank is not None and any(\n            bs != batch_size_per_rank[0] for bs in batch_size_per_rank\n        ):\n            batch_size_per_feature = list(\n                itertools.chain.from_iterable(\n                    itertools.repeat(x, local_split) for x in batch_size_per_rank\n                )\n            )\n            permuted_batch_size_per_feature = [batch_size_per_feature[r] for r in recat]\n            input_offset = [0] + list(itertools.accumulate(batch_size_per_feature))\n            output_offset = [0] + list(\n                itertools.accumulate(permuted_batch_size_per_feature)\n            )\n            recat_tensor = torch.tensor(\n                recat,\n                device=device,\n                dtype=torch.int32,\n            )\n            input_offset_tensor = torch.tensor(\n                input_offset,\n                device=device,\n                dtype=torch.int32,\n            )\n            output_offset_tensor = torch.tensor(\n                output_offset,\n                device=device,\n                dtype=torch.int32,\n            )\n            recat = torch.ops.fbgemm.expand_into_jagged_permute(\n                recat_tensor,\n                input_offset_tensor,\n                output_offset_tensor,\n                output_offset[-1],\n            )\n            return recat\n        else:\n            return torch.tensor(recat, device=device, dtype=torch.int32)",
  "class SplitsAllToAllAwaitable(Awaitable[List[List[int]]]):\n    \"\"\"\n    Awaitable for splits AlltoAll.\n\n    Args:\n        input_tensors (List[torch.Tensor]): tensor of splits to redistribute.\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_tensors: List[torch.Tensor],\n        pg: dist.ProcessGroup,\n    ) -> None:\n        super().__init__()\n        self.num_workers: int = pg.size()\n\n        with record_function(\"## all2all_data:kjt splits ##\"):\n            self._output_tensor: torch.Tensor = torch.empty(\n                [self.num_workers * len(input_tensors)],\n                device=input_tensors[0].device,\n                dtype=input_tensors[0].dtype,\n            )\n            input_tensor = torch.stack(input_tensors, dim=1).flatten()\n            self._splits_awaitable: dist.Work = dist.all_to_all_single(\n                output=self._output_tensor,\n                input=input_tensor,\n                group=pg,\n                async_op=True,\n            )\n\n    def _wait_impl(self) -> List[List[int]]:\n        self._splits_awaitable.wait()\n        return self._output_tensor.view(self.num_workers, -1).T.tolist()",
  "class KJTAllToAllTensorsAwaitable(Awaitable[KeyedJaggedTensor]):\n    \"\"\"\n    Awaitable for KJT tensors AlltoAll.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n        input (KeyedJaggedTensor): input KJT.\n        splits (List[int]): list of len(pg.size()) which indicates how many features to\n            send to each pg.rank(). It is assumed the `KeyedJaggedTensor` is ordered by\n            destination rank. Same for all ranks.\n        input_splits (List[List[int]]): input splits (number of values each rank will\n            get) for each tensor in AlltoAll.\n        output_splits (List[List[int]]): output splits (number of values per rank in\n            output) for each tensor in AlltoAll.\n        input_tensors (List[torch.Tensor]): provided KJT tensors (ie. lengths, values)\n            to redistribute according to splits.\n        labels (List[str]): labels for each provided tensor.\n        keys (List[str]): KJT keys after AlltoAll.\n        device (torch.device): device on which buffers will be allocated.\n        stagger (int): stagger value to apply to recat tensor.\n        stride_per_rank (Optional[List[int]]): stride per rank in the non variable\n            batch per feature case.\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        input: KeyedJaggedTensor,\n        splits: List[int],\n        input_splits: List[List[int]],\n        output_splits: List[List[int]],\n        input_tensors: List[torch.Tensor],\n        labels: List[str],\n        keys: List[str],\n        device: torch.device,\n        stagger: int,\n        stride_per_rank: Optional[List[int]],\n    ) -> None:\n        super().__init__()\n        self._workers: int = pg.size()\n        self._pg: dist.ProcessGroup = pg\n        self._device: torch.device = device\n        self._input = input\n        self._splits = splits\n        self._input_splits: Dict[str, List[int]] = dict(zip(labels, input_splits))\n        self._output_splits: Dict[str, List[int]] = dict(zip(labels, output_splits))\n        self._keys = keys\n        self._stagger = stagger\n        self._stride_per_rank = stride_per_rank\n        self._recat: Optional[torch.Tensor] = _get_recat(\n            local_split=splits[pg.rank()],\n            num_splits=len(splits),\n            stagger=stagger,\n            device=device,\n            batch_size_per_rank=self._stride_per_rank,\n        )\n        if self._workers == 1:\n            return\n\n        self._output_tensors: List[torch.Tensor] = []\n        self._awaitables: List[dist.Work] = []\n\n        for input_split, output_split, input_tensor, label in zip(\n            input_splits,\n            output_splits,\n            input_tensors,\n            labels,\n        ):\n            output_tensor = torch.empty(\n                sum(output_split), device=self._device, dtype=input_tensor.dtype\n            )\n            with record_function(f\"## all2all_data:kjt {label} ##\"):\n                awaitable = dist.all_to_all_single(\n                    output=output_tensor,\n                    input=input_tensor,\n                    output_split_sizes=output_split,\n                    input_split_sizes=input_split,\n                    group=self._pg,\n                    async_op=True,\n                )\n\n            self._output_tensors.append(output_tensor)\n            self._awaitables.append(awaitable)\n\n    def _wait_impl(self) -> KeyedJaggedTensor:\n        \"\"\"\n        Overwrites wait function as we don't handle callbacks here.\n\n        Returns:\n            KeyedJaggedTensor: Synced KJT after AlltoAll.\n        \"\"\"\n\n        if self._workers == 1:\n            self._input.sync()\n            return self._input\n\n        for awaitable in self._awaitables:\n            awaitable.wait()\n\n        return type(self._input).dist_init(\n            keys=self._keys,\n            tensors=self._output_tensors,\n            variable_stride_per_key=self._input.variable_stride_per_key(),\n            num_workers=self._workers,\n            recat=self._recat,\n            stride_per_rank=self._stride_per_rank,\n        )",
  "class KJTAllToAllSplitsAwaitable(Awaitable[KJTAllToAllTensorsAwaitable]):\n    \"\"\"\n    Awaitable for KJT tensors splits AlltoAll.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n        input (KeyedJaggedTensor): input KJT.\n        splits (List[int]): list of len(pg.size()) which indicates how many features to\n            send to each pg.rank(). It is assumed the `KeyedJaggedTensor` is ordered by\n            destination rank. Same for all ranks.\n        tensor_splits (Dict[str, List[int]]): tensor splits provided by input KJT.\n        input_tensors (List[torch.Tensor]): provided KJT tensors (ie. lengths, values)\n            to redistribute according to splits.\n        keys (List[str]): KJT keys after AlltoAll.\n        device (torch.device): device on which buffers will be allocated.\n        stagger (int): stagger value to apply to recat tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        input: KeyedJaggedTensor,\n        splits: List[int],\n        labels: List[str],\n        tensor_splits: List[List[int]],\n        input_tensors: List[torch.Tensor],\n        keys: List[str],\n        device: torch.device,\n        stagger: int,\n    ) -> None:\n        super().__init__()\n        self._workers: int = pg.size()\n        self._pg: dist.ProcessGroup = pg\n        self._device: torch.device = device\n        self._input = input\n        self._splits = splits\n        self._labels = labels\n        self._input_splits = tensor_splits\n        self._input_tensors = input_tensors\n        self._keys = keys\n        self._stagger = stagger\n        self._output_splits: List[List[int]] = self._input_splits\n        self._stride_per_rank: Optional[List[int]] = (\n            None\n            if self._input.variable_stride_per_key()\n            else [self._input.stride()] * self._workers\n        )\n        if self._workers == 1:\n            return\n\n        input_tensors = [\n            torch.tensor(splits, device=device) for splits in self._input_splits\n        ]\n        if not self._input.variable_stride_per_key():\n            input_tensors.append(\n                torch.tensor([input.stride()] * self._workers, device=device)\n            )\n\n        self._splits_awaitable = SplitsAllToAllAwaitable(\n            input_tensors,\n            self._pg,\n        )\n\n    def _wait_impl(self) -> KJTAllToAllTensorsAwaitable:\n        \"\"\"\n        Overwrites wait function as we don't handle callbacks here.\n\n        Returns:\n            KJTAllToAllTensorsAwaitable.\n        \"\"\"\n\n        if self._workers > 1:\n            output_list = self._splits_awaitable.wait()\n            if self._input.variable_stride_per_key():\n                self._output_splits = output_list\n            else:\n                self._output_splits = output_list[:-1]\n                self._stride_per_rank = output_list[-1]\n\n        return KJTAllToAllTensorsAwaitable(\n            pg=self._pg,\n            input=self._input,\n            splits=self._splits,\n            input_splits=self._input_splits,\n            output_splits=self._output_splits,\n            input_tensors=self._input_tensors,\n            labels=self._labels,\n            keys=self._keys,\n            device=self._device,\n            stagger=self._stagger,\n            stride_per_rank=self._stride_per_rank,\n        )",
  "class KJTAllToAll(nn.Module):\n    \"\"\"\n    Redistributes `KeyedJaggedTensor` to a `ProcessGroup` according to splits.\n\n    Implementation utilizes AlltoAll collective as part of torch.distributed.\n\n    The input provides the necessary tensors and input splits to distribute.\n    The first collective call in `KJTAllToAllSplitsAwaitable` will transmit output\n    splits (to allocate correct space for tensors) and batch size per rank. The\n    following collective calls in `KJTAllToAllTensorsAwaitable` will transmit the actual\n    tensors asynchronously.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n        splits (List[int]): List of len(pg.size()) which indicates how many features to\n            send to each pg.rank(). It is assumed the `KeyedJaggedTensor` is ordered by\n            destination rank. Same for all ranks.\n        stagger (int): stagger value to apply to recat tensor, see `_get_recat` function\n            for more detail.\n\n    Example::\n\n        keys=['A','B','C']\n        splits=[2,1]\n        kjtA2A = KJTAllToAll(pg, splits)\n        awaitable = kjtA2A(rank0_input)\n\n        # where:\n        # rank0_input is KeyedJaggedTensor holding\n\n        #         0           1           2\n        # 'A'    [A.V0]       None        [A.V1, A.V2]\n        # 'B'    None         [B.V0]      [B.V1]\n        # 'C'    [C.V0]       [C.V1]      None\n\n        # rank1_input is KeyedJaggedTensor holding\n\n        #         0           1           2\n        # 'A'     [A.V3]      [A.V4]      None\n        # 'B'     None        [B.V2]      [B.V3, B.V4]\n        # 'C'     [C.V2]      [C.V3]      None\n\n        rank0_output = awaitable.wait()\n\n        # where:\n        # rank0_output is KeyedJaggedTensor holding\n\n        #         0           1           2           3           4           5\n        # 'A'     [A.V0]      None      [A.V1, A.V2]  [A.V3]      [A.V4]      None\n        # 'B'     None        [B.V0]    [B.V1]        None        [B.V2]      [B.V3, B.V4]\n\n        # rank1_output is KeyedJaggedTensor holding\n        #         0           1           2           3           4           5\n        # 'C'     [C.V0]      [C.V1]      None        [C.V2]      [C.V3]      None\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        splits: List[int],\n        stagger: int = 1,\n    ) -> None:\n        super().__init__()\n        assert len(splits) == pg.size()\n        self._pg: dist.ProcessGroup = pg\n        self._splits = splits\n        self._splits_cumsum: List[int] = [0] + list(itertools.accumulate(splits))\n        self._stagger = stagger\n\n    def forward(\n        self, input: KeyedJaggedTensor\n    ) -> Awaitable[KJTAllToAllTensorsAwaitable]:\n        \"\"\"\n        Sends input to relevant `ProcessGroup` ranks.\n\n        The first wait will get the output splits for the provided tensors and issue\n        tensors AlltoAll. The second wait will get the tensors.\n\n        Args:\n            input (KeyedJaggedTensor): `KeyedJaggedTensor` of values to distribute.\n\n        Returns:\n            Awaitable[KJTAllToAllTensorsAwaitable]: awaitable of a `KJTAllToAllTensorsAwaitable`.\n        \"\"\"\n\n        with torch.no_grad():\n            assert len(input.keys()) == sum(self._splits)\n            rank = dist.get_rank(self._pg)\n            local_keys = input.keys()[\n                self._splits_cumsum[rank] : self._splits_cumsum[rank + 1]\n            ]\n\n            return KJTAllToAllSplitsAwaitable(\n                pg=self._pg,\n                input=input,\n                splits=self._splits,\n                labels=input.dist_labels(),\n                tensor_splits=input.dist_splits(self._splits),\n                input_tensors=input.dist_tensors(),\n                keys=local_keys,\n                device=input.device(),\n                stagger=self._stagger,\n            )",
  "class KJTOneToAll(nn.Module):\n    \"\"\"\n    Redistributes `KeyedJaggedTensor` to all devices.\n\n    Implementation utilizes OnetoAll function, which essentially P2P copies the feature\n    to the devices.\n\n    Args:\n        splits (List[int]): lengths of features to split the `KeyJaggedTensor` features\n            into before copying them.\n        world_size (int): number of devices in the topology.\n        device (torch.device): the device on which the KJTs will be allocated.\n    \"\"\"\n\n    def __init__(\n        self,\n        splits: List[int],\n        world_size: int,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._splits = splits\n        self._world_size = world_size\n        self._device_type = (\n            \"meta\" if device is not None and device.type == \"meta\" else \"cuda\"\n        )\n        assert self._world_size == len(splits)\n\n    def forward(self, kjt: KeyedJaggedTensor) -> KJTList:\n        \"\"\"\n        Splits features first and then sends the slices to the corresponding devices.\n\n        Args:\n            kjt (KeyedJaggedTensor): the input features.\n\n        Returns:\n            Awaitable[List[KeyedJaggedTensor]]: awaitable of `KeyedJaggedTensor` splits.\n        \"\"\"\n        fx_marker(\"KJT_ONE_TO_ALL_FORWARD_BEGIN\", kjt)\n        kjts: List[KeyedJaggedTensor] = kjt.split(self._splits)\n        dist_kjts = [\n            kjts[rank]\n            if self._device_type == \"meta\"\n            else kjts[rank].to(torch.device(self._device_type, rank), non_blocking=True)\n            for rank in range(self._world_size)\n        ]\n        ret = KJTList(dist_kjts)\n        fx_marker(\"KJT_ONE_TO_ALL_FORWARD_END\", kjt)\n        return ret",
  "class PooledEmbeddingsAwaitable(Awaitable[torch.Tensor]):\n    \"\"\"\n    Awaitable for pooled embeddings after collective operation.\n\n    Args:\n        tensor_awaitable (Awaitable[torch.Tensor]): awaitable of concatenated tensors\n            from all the processes in the group after collective.\n    \"\"\"\n\n    def __init__(\n        self,\n        tensor_awaitable: Awaitable[torch.Tensor],\n    ) -> None:\n        super().__init__()\n        self._tensor_awaitable = tensor_awaitable\n\n    def _wait_impl(self) -> torch.Tensor:\n        \"\"\"\n        Syncs pooled embeddings after collective operation.\n\n        Returns:\n            torch.Tensor: synced pooled embeddings.\n        \"\"\"\n\n        ret = self._tensor_awaitable.wait()\n        return ret\n\n    @property\n    def callbacks(self) -> List[Callable[[torch.Tensor], torch.Tensor]]:\n        return self._callbacks",
  "class PooledEmbeddingsAllToAll(nn.Module):\n    # TODO: potentially refactor to take KT instead of torch.Tensor: D29174501\n    \"\"\"\n    Shards batches and collects keys of tensor with a `ProcessGroup` according to\n    `dim_sum_per_rank`.\n\n    Implementation utilizes `alltoall_pooled` operation.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n        dim_sum_per_rank (List[int]): number of features (sum of dimensions) of the\n            embedding in each rank.\n        device (Optional[torch.device]): device on which buffers will be allocated.\n        callbacks (Optional[List[Callable[[torch.Tensor], torch.Tensor]]]): callback\n            functions.\n        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.\n\n    Example::\n\n        dim_sum_per_rank = [2, 1]\n        a2a = PooledEmbeddingsAllToAll(pg, dim_sum_per_rank, device)\n\n        t0 = torch.rand((6, 2))\n        t1 = torch.rand((6, 1))\n        rank0_output = a2a(t0).wait()\n        rank1_output = a2a(t1).wait()\n        print(rank0_output.size())\n            # torch.Size([3, 3])\n        print(rank1_output.size())\n            # torch.Size([3, 3])\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        dim_sum_per_rank: List[int],\n        device: Optional[torch.device] = None,\n        callbacks: Optional[List[Callable[[torch.Tensor], torch.Tensor]]] = None,\n        codecs: Optional[QuantizedCommCodecs] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._callbacks: List[Callable[[torch.Tensor], torch.Tensor]] = []\n        if callbacks is not None:\n            self._callbacks = callbacks\n        self._dim_sum_per_rank = dim_sum_per_rank\n        self._codecs = codecs\n        self.register_buffer(\n            \"_dim_sum_per_rank_tensor\",\n            torch.tensor(dim_sum_per_rank, device=device, dtype=torch.int),\n        )\n        cumsum_dim_sum_per_rank = list(itertools.accumulate(dim_sum_per_rank))\n        self.register_buffer(\n            \"_cumsum_dim_sum_per_rank_tensor\",\n            torch.tensor(cumsum_dim_sum_per_rank, device=device, dtype=torch.int),\n        )\n\n    def forward(\n        self,\n        local_embs: torch.Tensor,\n        batch_size_per_rank: Optional[List[int]] = None,\n    ) -> PooledEmbeddingsAwaitable:\n        \"\"\"\n        Performs AlltoAll pooled operation on pooled embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n            batch_size_per_rank (Optional[List[int]]): batch size per rank, to support\n                variable batch size.\n\n        Returns:\n            PooledEmbeddingsAwaitable: awaitable of pooled embeddings.\n        \"\"\"\n\n        if not batch_size_per_rank:\n            B_global = local_embs.size(0)\n            assert (\n                B_global % self._pg.size() == 0\n            ), f\"num of ranks {self._pg.size()} doesn't divide global batch size {B_global}\"\n            B_local = B_global // self._pg.size()\n            batch_size_per_rank = [B_local] * self._pg.size()\n        tensor_awaitable = alltoall_pooled(\n            a2a_pooled_embs_tensor=local_embs,\n            batch_size_per_rank=batch_size_per_rank,\n            dim_sum_per_rank=self._dim_sum_per_rank,\n            dim_sum_per_rank_tensor=self._dim_sum_per_rank_tensor,\n            cumsum_dim_sum_per_rank_tensor=self._cumsum_dim_sum_per_rank_tensor,\n            group=self._pg,\n            codecs=self._codecs,\n        )\n\n        pooled_embedding_awaitable = PooledEmbeddingsAwaitable(\n            tensor_awaitable=tensor_awaitable,\n        )\n        pooled_embedding_awaitable.callbacks.extend(self._callbacks)\n\n        return pooled_embedding_awaitable\n\n    @property\n    def callbacks(self) -> List[Callable[[torch.Tensor], torch.Tensor]]:\n        return self._callbacks",
  "class VariableBatchPooledEmbeddingsAllToAll(nn.Module):\n    \"\"\"\n    Shards batches and collects keys of tensor with a `ProcessGroup` according to\n    `dim_sum_per_rank`.\n\n    Implementation utilizes `variable_batch_alltoall_pooled` operation.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n        emb_dim_per_rank_per_feature (List[List[int]]): embedding dimensions per rank\n            per feature.\n        device (Optional[torch.device]): device on which buffers will be allocated.\n        callbacks (Optional[List[Callable[[torch.Tensor], torch.Tensor]]]): callback\n            functions.\n        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.\n\n    Example::\n\n        emb_dim_per_rank_per_feature = [[2], [3, 3]]\n        a2a = VariableBatchPooledEmbeddingsAllToAll(\n            pg, emb_dim_per_rank_per_feature, device\n        )\n\n        t0 = torch.rand(6) # 2 * (2 + 1)\n        t1 = torch.rand(24) # 3 * (1 + 3) + 3 * (2 + 2)\n        r0_batch_size_per_rank_per_feature = [[2, 1]]\n        r1_batch_size_per_rank_per_feature = [[1, 3], [2, 2]]\n        r0_batch_size_per_feature_pre_a2a = [2, 1, 3]\n        r1_batch_size_per_feature_pre_a2a = [1, 2, 2]\n\n        rank0_output = a2a(\n            t0, r0_batch_size_per_rank_per_feature, r0_batch_size_per_feature_pre_a2a\n        ).wait()\n        rank1_output = a2a(\n            t1, r1_batch_size_per_rank_per_feature, r1_batch_size_per_feature_pre_a2a\n        ).wait()\n\n        # input splits:\n        #   r0: [2*2, 1*1]\n        #   r1: [1*3 + 3*3, 2*3 + 2*3]\n\n        # output splits:\n        #   r0: [2*2, 1*3 + 3*3]\n        #   r1: [1*2, 2*3 + 2*3]\n\n        print(rank0_output.size())\n            # torch.Size([16])\n            # 2*2 + 1*3 + 3*3\n        print(rank1_output.size())\n            # torch.Size([14])\n            # 1*2 + 2*3 + 2*3\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        emb_dim_per_rank_per_feature: List[List[int]],\n        device: Optional[torch.device] = None,\n        callbacks: Optional[List[Callable[[torch.Tensor], torch.Tensor]]] = None,\n        codecs: Optional[QuantizedCommCodecs] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._emb_dim_per_rank_per_feature = emb_dim_per_rank_per_feature\n        self._callbacks: List[Callable[[torch.Tensor], torch.Tensor]] = []\n        if callbacks is not None:\n            self._callbacks = callbacks\n        self._codecs = codecs\n\n    def forward(\n        self,\n        local_embs: torch.Tensor,\n        batch_size_per_rank_per_feature: List[List[int]],\n        batch_size_per_feature_pre_a2a: List[int],\n    ) -> PooledEmbeddingsAwaitable:\n        \"\"\"\n        Performs AlltoAll pooled operation with variable batch size per feature on a\n        pooled embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n            batch_size_per_rank_per_feature (List[List[int]]): batch size per rank per\n                feature, post a2a. Used to get the input splits.\n            batch_size_per_feature_pre_a2a (List[int]): local batch size before\n                scattering, used to get the output splits.\n                Ordered by rank_0 feature, rank_1 feature, ...\n\n        Returns:\n            PooledEmbeddingsAwaitable: awaitable of pooled embeddings.\n        \"\"\"\n\n        tensor_awaitable = variable_batch_alltoall_pooled(\n            a2a_pooled_embs_tensor=local_embs,\n            batch_size_per_rank_per_feature=batch_size_per_rank_per_feature,\n            batch_size_per_feature_pre_a2a=batch_size_per_feature_pre_a2a,\n            emb_dim_per_rank_per_feature=self._emb_dim_per_rank_per_feature,\n            group=self._pg,\n            codecs=self._codecs,\n        )\n\n        pooled_embedding_awaitable = PooledEmbeddingsAwaitable(\n            tensor_awaitable=tensor_awaitable,\n        )\n        pooled_embedding_awaitable.callbacks.extend(self._callbacks)\n\n        return pooled_embedding_awaitable\n\n    @property\n    def callbacks(self) -> List[Callable[[torch.Tensor], torch.Tensor]]:\n        return self._callbacks",
  "class EmbeddingsAllToOneReduce(nn.Module):\n    \"\"\"\n    Merges the pooled/sequence embedding tensor on each device into single tensor.\n\n    Args:\n        device (torch.device): device on which buffer will be allocated.\n        world_size (int): number of devices in the topology.\n        cat_dim (int): which dimension you would like to concatenate on.\n            For pooled embedding it is 1; for sequence embedding it is 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n        cat_dim: int,\n    ) -> None:\n        super().__init__()\n        self._device = device\n        self._world_size = world_size\n        self._cat_dim = cat_dim\n\n    def forward(\n        self,\n        tensors: List[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Performs AlltoOne operation with Reduce on pooled/sequence embeddings tensors.\n\n        Args:\n            tensors (List[torch.Tensor]): list of embedding tensors.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of the reduced embeddings.\n        \"\"\"\n        assert len(tensors) == self._world_size\n        return torch.ops.fbgemm.sum_reduce_to_one(\n            tensors,\n            self._device,\n        )",
  "class EmbeddingsAllToOne(nn.Module):\n    \"\"\"\n    Merges the pooled/sequence embedding tensor on each device into single tensor.\n\n    Args:\n        device (torch.device): device on which buffer will be allocated.\n        world_size (int): number of devices in the topology.\n        cat_dim (int): which dimension you would like to concatenate on.\n            For pooled embedding it is 1; for sequence embedding it is 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n        cat_dim: int,\n    ) -> None:\n        super().__init__()\n        self._device = device\n        self._world_size = world_size\n        self._cat_dim = cat_dim\n\n    # This method can be used by an inference runtime to update the\n    # device information for this module.\n    @torch.jit.export\n    def set_device(self, device_str: str) -> None:\n        self._device = torch.device(device_str)\n\n    def forward(self, tensors: List[torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Performs AlltoOne operation on pooled/sequence embeddings tensors.\n\n        Args:\n            tensors (List[torch.Tensor]): list of embedding tensors.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of the merged embeddings.\n        \"\"\"\n        assert len(tensors) == self._world_size\n        is_target_device_cpu: bool = self._device.type == \"cpu\"\n\n        non_cat_size = tensors[0].size(1 - self._cat_dim)\n        # if src device is cuda, target device is cpu:\n        # 1. merge on first tensor device\n        # 2. move to cpu\n        device = self._device if not is_target_device_cpu else tensors[0].device\n        merge = torch.ops.fbgemm.merge_pooled_embeddings(\n            tensors,\n            non_cat_size,\n            device,\n            self._cat_dim,\n        )\n\n        return merge if not is_target_device_cpu else merge.to(self._device)",
  "class SeqEmbeddingsAllToOne(nn.Module):\n    \"\"\"\n    Merges the pooled/sequence embedding tensor on each device into single tensor.\n\n    Args:\n        device (torch.device): device on which buffer will be allocated\n        world_size (int): number of devices in the topology.\n        cat_dim (int): which dimension you like to concate on.\n            For pooled embedding it is 1; for sequence embedding it is 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._device = device\n        self._world_size = world_size\n\n    # This method can be used by an inference runtime to update the\n    # device information for this module.\n    @torch.jit.export\n    def set_device(self, device_str: str) -> None:\n        self._device = torch.device(device_str)\n\n    def forward(self, tensors: List[torch.Tensor]) -> List[torch.Tensor]:\n        \"\"\"\n        Performs AlltoOne operation on pooled embeddings tensors.\n\n        Args:\n            tensors (List[torch.Tensor]): list of pooled embedding tensors.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of the merged pooled embeddings.\n        \"\"\"\n\n        assert len(tensors) == self._world_size\n        return torch.ops.fbgemm.all_to_one_device(\n            tensors,\n            self._device,\n        )",
  "class PooledEmbeddingsReduceScatter(nn.Module):\n    \"\"\"\n    The module class that wraps reduce-scatter communication primitives for pooled\n    embedding communication in row-wise and twrw sharding.\n\n    For pooled embeddings, we have a local model-parallel output tensor with a layout of\n    `[num_buckets x batch_size, dimension]`. We need to sum over `num_buckets` dimension\n    across batches. We split the tensor along the first dimension into unequal chunks\n    (tensor slices of different buckets) according to `input_splits` and reduce them\n    into the output tensor and scatter the results for corresponding ranks.\n\n    The class returns the async `Awaitable` handle for pooled embeddings tensor.\n    The `reduce-scatter-v` operation is only available for NCCL backend.\n\n    Args:\n        pg (dist.ProcessGroup): the process group that the reduce-scatter communication\n            happens within.\n        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.\n\n     Example::\n\n        init_distributed(rank=rank, size=2, backend=\"nccl\")\n        pg = dist.new_group(backend=\"nccl\")\n        input = torch.randn(2 * 2, 2)\n        input_splits = [1,3]\n        m = PooledEmbeddingsReduceScatter(pg)\n        output = m(input, input_splits)\n        tensor = output.wait()\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        codecs: Optional[QuantizedCommCodecs] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._codecs = codecs\n\n    def forward(\n        self, local_embs: torch.Tensor, input_splits: Optional[List[int]] = None\n    ) -> PooledEmbeddingsAwaitable:\n        \"\"\"\n        Performs reduce scatter operation on pooled embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of shape\n                `[num_buckets * batch_size, dimension]`.\n            input_splits (Optional[List[int]]): list of splits for `local_embs` dim 0.\n\n        Returns:\n            PooledEmbeddingsAwaitable: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].\n        \"\"\"\n\n        if input_splits and len(set(input_splits)) > 1:\n            tensor_awaitable = reduce_scatter_v_pooled(\n                local_embs, input_splits, self._pg, codecs=self._codecs\n            )\n        else:\n            tensor_awaitable = reduce_scatter_base_pooled(\n                local_embs, self._pg, codecs=self._codecs\n            )\n        return PooledEmbeddingsAwaitable(tensor_awaitable=tensor_awaitable)",
  "class PooledEmbeddingsAllGather(nn.Module):\n    \"\"\"\n    The module class that wraps the all-gather communication primitive for pooled\n    embedding communication.\n\n    Provided a local input tensor with a layout of `[batch_size, dimension]`, we want to\n    gather input tensors from all ranks into a flattened output tensor.\n\n    The class returns the async `Awaitable` handle for pooled embeddings tensor.\n    The all-gather is only available for NCCL backend.\n\n    Args:\n        pg (dist.ProcessGroup): the process group that the all-gather communication\n            happens within.\n\n    Example::\n\n        init_distributed(rank=rank, size=2, backend=\"nccl\")\n        pg = dist.new_group(backend=\"nccl\")\n        input = torch.randn(2, 2)\n        m = PooledEmbeddingsAllGather(pg)\n        output = m(input)\n        tensor = output.wait()\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        codecs: Optional[QuantizedCommCodecs] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._codecs = codecs\n\n    def forward(self, local_emb: torch.Tensor) -> PooledEmbeddingsAwaitable:\n        \"\"\"\n        Performs reduce scatter operation on pooled embeddings tensor.\n\n        Args:\n            local_emb (torch.Tensor): tensor of shape\n                `[num_buckets x batch_size, dimension]`.\n\n        Returns:\n            PooledEmbeddingsAwaitable: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].\n        \"\"\"\n\n        tensor_awaitable = all_gather_base_pooled(\n            local_emb, self._pg, codecs=self._codecs\n        )\n        return PooledEmbeddingsAwaitable(tensor_awaitable=tensor_awaitable)",
  "class SequenceEmbeddingsAwaitable(Awaitable[torch.Tensor]):\n    \"\"\"\n    Awaitable for sequence embeddings after collective operation.\n\n    Args:\n        tensor_awaitable (Awaitable[torch.Tensor]): awaitable of concatenated tensors\n            from all the processes in the group after collective.\n        unbucketize_permute_tensor (Optional[torch.Tensor]): stores the permute order of\n            KJT bucketize (for row-wise sharding only).\n        embedding_dim (int): embedding dimension.\n    \"\"\"\n\n    def __init__(\n        self,\n        tensor_awaitable: Awaitable[torch.Tensor],\n        unbucketize_permute_tensor: Optional[torch.Tensor],\n        embedding_dim: int,\n    ) -> None:\n        super().__init__()\n        self._tensor_awaitable = tensor_awaitable\n\n        if unbucketize_permute_tensor is not None:\n            self.callbacks.append(\n                lambda ret: torch.index_select(\n                    ret.view(-1, embedding_dim),\n                    0,\n                    unbucketize_permute_tensor,\n                )\n            )\n\n    def _wait_impl(self) -> torch.Tensor:\n        \"\"\"\n        Syncs sequence embeddings after collective operation.\n\n        Returns:\n            torch.Tensor: synced sequence embeddings.\n        \"\"\"\n\n        ret = self._tensor_awaitable.wait()\n        return ret",
  "class SequenceEmbeddingsAllToAll(nn.Module):\n    \"\"\"\n    Redistributes sequence embedding to a `ProcessGroup` according to splits.\n\n    Args:\n        pg (dist.ProcessGroup): the process group that the AlltoAll communication\n            happens within.\n        features_per_rank (List[int]): list of number of features per rank.\n        device (Optional[torch.device]): device on which buffers will be allocated.\n\n    Example::\n\n        init_distributed(rank=rank, size=2, backend=\"nccl\")\n        pg = dist.new_group(backend=\"nccl\")\n        features_per_rank = [4, 4]\n        m = SequenceEmbeddingsAllToAll(pg, features_per_rank)\n        local_embs = torch.rand((6, 2))\n        sharding_ctx: SequenceShardingContext\n        output = m(\n            local_embs=local_embs,\n            lengths=sharding_ctx.lengths_after_input_dist,\n            input_splits=sharding_ctx.input_splits,\n            output_splits=sharding_ctx.output_splits,\n            unbucketize_permute_tensor=None,\n        )\n        tensor = output.wait()\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        features_per_rank: List[int],\n        device: Optional[torch.device] = None,\n        codecs: Optional[QuantizedCommCodecs] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._local_split: int = features_per_rank[self._pg.rank()]\n        self._num_splits: int = self._pg.size()\n\n        forward_recat = []\n        for j in range(self._num_splits):\n            for i in range(self._local_split):\n                forward_recat.append(j + i * self._num_splits)\n        self.register_buffer(\n            \"_forward_recat_tensor\",\n            torch.tensor(forward_recat, device=device, dtype=torch.int),\n        )\n        backward_recat = []\n        for i in range(self._local_split):\n            for j in range(self._num_splits):\n                backward_recat.append(i + j * self._local_split)\n        self.register_buffer(\n            \"_backward_recat_tensor\",\n            torch.tensor(backward_recat, device=device, dtype=torch.int),\n        )\n        self._codecs = codecs\n\n    def forward(\n        self,\n        local_embs: torch.Tensor,\n        lengths: torch.Tensor,\n        input_splits: List[int],\n        output_splits: List[int],\n        unbucketize_permute_tensor: Optional[torch.Tensor] = None,\n        batch_size_per_rank: Optional[List[int]] = None,\n        sparse_features_recat: Optional[torch.Tensor] = None,\n    ) -> SequenceEmbeddingsAwaitable:\n        \"\"\"\n        Performs AlltoAll operation on sequence embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): input embeddings tensor.\n            lengths (torch.Tensor): lengths of sparse features after AlltoAll.\n            input_splits (List[int]): input splits of AlltoAll.\n            output_splits (List[int]): output splits of AlltoAll.\n            unbucketize_permute_tensor (Optional[torch.Tensor]): stores the permute\n                order of the KJT bucketize (for row-wise sharding only).\n            batch_size_per_rank: (Optional[List[int]]): batch size per rank.\n            sparse_features_recat (Optional[torch.Tensor]): recat tensor used for sparse\n                feature input dist. Must be provided if using variable batch size.\n\n        Returns:\n            SequenceEmbeddingsAwaitable: awaitable of sequence embeddings.\n        \"\"\"\n\n        variable_batch_size = (\n            batch_size_per_rank is not None and len(set(batch_size_per_rank)) > 1\n        )\n\n        if sparse_features_recat is not None:\n            forward_recat_tensor = torch.ops.fbgemm.invert_permute(\n                sparse_features_recat\n            )\n            backward_recat_tensor = sparse_features_recat\n        else:\n            forward_recat_tensor = self._forward_recat_tensor\n            backward_recat_tensor = self._backward_recat_tensor\n\n        tensor_awaitable = alltoall_sequence(\n            a2a_sequence_embs_tensor=local_embs,\n            forward_recat_tensor=forward_recat_tensor,\n            backward_recat_tensor=backward_recat_tensor,\n            lengths_after_sparse_data_all2all=lengths,\n            input_splits=input_splits,\n            output_splits=output_splits,\n            variable_batch_size=variable_batch_size,\n            group=self._pg,\n            codecs=self._codecs,\n        )\n        return SequenceEmbeddingsAwaitable(\n            tensor_awaitable=tensor_awaitable,\n            unbucketize_permute_tensor=unbucketize_permute_tensor,\n            embedding_dim=local_embs.shape[1],\n        )",
  "def __init__(\n        self,\n        input_tensors: List[torch.Tensor],\n        pg: dist.ProcessGroup,\n    ) -> None:\n        super().__init__()\n        self.num_workers: int = pg.size()\n\n        with record_function(\"## all2all_data:kjt splits ##\"):\n            self._output_tensor: torch.Tensor = torch.empty(\n                [self.num_workers * len(input_tensors)],\n                device=input_tensors[0].device,\n                dtype=input_tensors[0].dtype,\n            )\n            input_tensor = torch.stack(input_tensors, dim=1).flatten()\n            self._splits_awaitable: dist.Work = dist.all_to_all_single(\n                output=self._output_tensor,\n                input=input_tensor,\n                group=pg,\n                async_op=True,\n            )",
  "def _wait_impl(self) -> List[List[int]]:\n        self._splits_awaitable.wait()\n        return self._output_tensor.view(self.num_workers, -1).T.tolist()",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        input: KeyedJaggedTensor,\n        splits: List[int],\n        input_splits: List[List[int]],\n        output_splits: List[List[int]],\n        input_tensors: List[torch.Tensor],\n        labels: List[str],\n        keys: List[str],\n        device: torch.device,\n        stagger: int,\n        stride_per_rank: Optional[List[int]],\n    ) -> None:\n        super().__init__()\n        self._workers: int = pg.size()\n        self._pg: dist.ProcessGroup = pg\n        self._device: torch.device = device\n        self._input = input\n        self._splits = splits\n        self._input_splits: Dict[str, List[int]] = dict(zip(labels, input_splits))\n        self._output_splits: Dict[str, List[int]] = dict(zip(labels, output_splits))\n        self._keys = keys\n        self._stagger = stagger\n        self._stride_per_rank = stride_per_rank\n        self._recat: Optional[torch.Tensor] = _get_recat(\n            local_split=splits[pg.rank()],\n            num_splits=len(splits),\n            stagger=stagger,\n            device=device,\n            batch_size_per_rank=self._stride_per_rank,\n        )\n        if self._workers == 1:\n            return\n\n        self._output_tensors: List[torch.Tensor] = []\n        self._awaitables: List[dist.Work] = []\n\n        for input_split, output_split, input_tensor, label in zip(\n            input_splits,\n            output_splits,\n            input_tensors,\n            labels,\n        ):\n            output_tensor = torch.empty(\n                sum(output_split), device=self._device, dtype=input_tensor.dtype\n            )\n            with record_function(f\"## all2all_data:kjt {label} ##\"):\n                awaitable = dist.all_to_all_single(\n                    output=output_tensor,\n                    input=input_tensor,\n                    output_split_sizes=output_split,\n                    input_split_sizes=input_split,\n                    group=self._pg,\n                    async_op=True,\n                )\n\n            self._output_tensors.append(output_tensor)\n            self._awaitables.append(awaitable)",
  "def _wait_impl(self) -> KeyedJaggedTensor:\n        \"\"\"\n        Overwrites wait function as we don't handle callbacks here.\n\n        Returns:\n            KeyedJaggedTensor: Synced KJT after AlltoAll.\n        \"\"\"\n\n        if self._workers == 1:\n            self._input.sync()\n            return self._input\n\n        for awaitable in self._awaitables:\n            awaitable.wait()\n\n        return type(self._input).dist_init(\n            keys=self._keys,\n            tensors=self._output_tensors,\n            variable_stride_per_key=self._input.variable_stride_per_key(),\n            num_workers=self._workers,\n            recat=self._recat,\n            stride_per_rank=self._stride_per_rank,\n        )",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        input: KeyedJaggedTensor,\n        splits: List[int],\n        labels: List[str],\n        tensor_splits: List[List[int]],\n        input_tensors: List[torch.Tensor],\n        keys: List[str],\n        device: torch.device,\n        stagger: int,\n    ) -> None:\n        super().__init__()\n        self._workers: int = pg.size()\n        self._pg: dist.ProcessGroup = pg\n        self._device: torch.device = device\n        self._input = input\n        self._splits = splits\n        self._labels = labels\n        self._input_splits = tensor_splits\n        self._input_tensors = input_tensors\n        self._keys = keys\n        self._stagger = stagger\n        self._output_splits: List[List[int]] = self._input_splits\n        self._stride_per_rank: Optional[List[int]] = (\n            None\n            if self._input.variable_stride_per_key()\n            else [self._input.stride()] * self._workers\n        )\n        if self._workers == 1:\n            return\n\n        input_tensors = [\n            torch.tensor(splits, device=device) for splits in self._input_splits\n        ]\n        if not self._input.variable_stride_per_key():\n            input_tensors.append(\n                torch.tensor([input.stride()] * self._workers, device=device)\n            )\n\n        self._splits_awaitable = SplitsAllToAllAwaitable(\n            input_tensors,\n            self._pg,\n        )",
  "def _wait_impl(self) -> KJTAllToAllTensorsAwaitable:\n        \"\"\"\n        Overwrites wait function as we don't handle callbacks here.\n\n        Returns:\n            KJTAllToAllTensorsAwaitable.\n        \"\"\"\n\n        if self._workers > 1:\n            output_list = self._splits_awaitable.wait()\n            if self._input.variable_stride_per_key():\n                self._output_splits = output_list\n            else:\n                self._output_splits = output_list[:-1]\n                self._stride_per_rank = output_list[-1]\n\n        return KJTAllToAllTensorsAwaitable(\n            pg=self._pg,\n            input=self._input,\n            splits=self._splits,\n            input_splits=self._input_splits,\n            output_splits=self._output_splits,\n            input_tensors=self._input_tensors,\n            labels=self._labels,\n            keys=self._keys,\n            device=self._device,\n            stagger=self._stagger,\n            stride_per_rank=self._stride_per_rank,\n        )",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        splits: List[int],\n        stagger: int = 1,\n    ) -> None:\n        super().__init__()\n        assert len(splits) == pg.size()\n        self._pg: dist.ProcessGroup = pg\n        self._splits = splits\n        self._splits_cumsum: List[int] = [0] + list(itertools.accumulate(splits))\n        self._stagger = stagger",
  "def forward(\n        self, input: KeyedJaggedTensor\n    ) -> Awaitable[KJTAllToAllTensorsAwaitable]:\n        \"\"\"\n        Sends input to relevant `ProcessGroup` ranks.\n\n        The first wait will get the output splits for the provided tensors and issue\n        tensors AlltoAll. The second wait will get the tensors.\n\n        Args:\n            input (KeyedJaggedTensor): `KeyedJaggedTensor` of values to distribute.\n\n        Returns:\n            Awaitable[KJTAllToAllTensorsAwaitable]: awaitable of a `KJTAllToAllTensorsAwaitable`.\n        \"\"\"\n\n        with torch.no_grad():\n            assert len(input.keys()) == sum(self._splits)\n            rank = dist.get_rank(self._pg)\n            local_keys = input.keys()[\n                self._splits_cumsum[rank] : self._splits_cumsum[rank + 1]\n            ]\n\n            return KJTAllToAllSplitsAwaitable(\n                pg=self._pg,\n                input=input,\n                splits=self._splits,\n                labels=input.dist_labels(),\n                tensor_splits=input.dist_splits(self._splits),\n                input_tensors=input.dist_tensors(),\n                keys=local_keys,\n                device=input.device(),\n                stagger=self._stagger,\n            )",
  "def __init__(\n        self,\n        splits: List[int],\n        world_size: int,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._splits = splits\n        self._world_size = world_size\n        self._device_type = (\n            \"meta\" if device is not None and device.type == \"meta\" else \"cuda\"\n        )\n        assert self._world_size == len(splits)",
  "def forward(self, kjt: KeyedJaggedTensor) -> KJTList:\n        \"\"\"\n        Splits features first and then sends the slices to the corresponding devices.\n\n        Args:\n            kjt (KeyedJaggedTensor): the input features.\n\n        Returns:\n            Awaitable[List[KeyedJaggedTensor]]: awaitable of `KeyedJaggedTensor` splits.\n        \"\"\"\n        fx_marker(\"KJT_ONE_TO_ALL_FORWARD_BEGIN\", kjt)\n        kjts: List[KeyedJaggedTensor] = kjt.split(self._splits)\n        dist_kjts = [\n            kjts[rank]\n            if self._device_type == \"meta\"\n            else kjts[rank].to(torch.device(self._device_type, rank), non_blocking=True)\n            for rank in range(self._world_size)\n        ]\n        ret = KJTList(dist_kjts)\n        fx_marker(\"KJT_ONE_TO_ALL_FORWARD_END\", kjt)\n        return ret",
  "def __init__(\n        self,\n        tensor_awaitable: Awaitable[torch.Tensor],\n    ) -> None:\n        super().__init__()\n        self._tensor_awaitable = tensor_awaitable",
  "def _wait_impl(self) -> torch.Tensor:\n        \"\"\"\n        Syncs pooled embeddings after collective operation.\n\n        Returns:\n            torch.Tensor: synced pooled embeddings.\n        \"\"\"\n\n        ret = self._tensor_awaitable.wait()\n        return ret",
  "def callbacks(self) -> List[Callable[[torch.Tensor], torch.Tensor]]:\n        return self._callbacks",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        dim_sum_per_rank: List[int],\n        device: Optional[torch.device] = None,\n        callbacks: Optional[List[Callable[[torch.Tensor], torch.Tensor]]] = None,\n        codecs: Optional[QuantizedCommCodecs] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._callbacks: List[Callable[[torch.Tensor], torch.Tensor]] = []\n        if callbacks is not None:\n            self._callbacks = callbacks\n        self._dim_sum_per_rank = dim_sum_per_rank\n        self._codecs = codecs\n        self.register_buffer(\n            \"_dim_sum_per_rank_tensor\",\n            torch.tensor(dim_sum_per_rank, device=device, dtype=torch.int),\n        )\n        cumsum_dim_sum_per_rank = list(itertools.accumulate(dim_sum_per_rank))\n        self.register_buffer(\n            \"_cumsum_dim_sum_per_rank_tensor\",\n            torch.tensor(cumsum_dim_sum_per_rank, device=device, dtype=torch.int),\n        )",
  "def forward(\n        self,\n        local_embs: torch.Tensor,\n        batch_size_per_rank: Optional[List[int]] = None,\n    ) -> PooledEmbeddingsAwaitable:\n        \"\"\"\n        Performs AlltoAll pooled operation on pooled embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n            batch_size_per_rank (Optional[List[int]]): batch size per rank, to support\n                variable batch size.\n\n        Returns:\n            PooledEmbeddingsAwaitable: awaitable of pooled embeddings.\n        \"\"\"\n\n        if not batch_size_per_rank:\n            B_global = local_embs.size(0)\n            assert (\n                B_global % self._pg.size() == 0\n            ), f\"num of ranks {self._pg.size()} doesn't divide global batch size {B_global}\"\n            B_local = B_global // self._pg.size()\n            batch_size_per_rank = [B_local] * self._pg.size()\n        tensor_awaitable = alltoall_pooled(\n            a2a_pooled_embs_tensor=local_embs,\n            batch_size_per_rank=batch_size_per_rank,\n            dim_sum_per_rank=self._dim_sum_per_rank,\n            dim_sum_per_rank_tensor=self._dim_sum_per_rank_tensor,\n            cumsum_dim_sum_per_rank_tensor=self._cumsum_dim_sum_per_rank_tensor,\n            group=self._pg,\n            codecs=self._codecs,\n        )\n\n        pooled_embedding_awaitable = PooledEmbeddingsAwaitable(\n            tensor_awaitable=tensor_awaitable,\n        )\n        pooled_embedding_awaitable.callbacks.extend(self._callbacks)\n\n        return pooled_embedding_awaitable",
  "def callbacks(self) -> List[Callable[[torch.Tensor], torch.Tensor]]:\n        return self._callbacks",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        emb_dim_per_rank_per_feature: List[List[int]],\n        device: Optional[torch.device] = None,\n        callbacks: Optional[List[Callable[[torch.Tensor], torch.Tensor]]] = None,\n        codecs: Optional[QuantizedCommCodecs] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._emb_dim_per_rank_per_feature = emb_dim_per_rank_per_feature\n        self._callbacks: List[Callable[[torch.Tensor], torch.Tensor]] = []\n        if callbacks is not None:\n            self._callbacks = callbacks\n        self._codecs = codecs",
  "def forward(\n        self,\n        local_embs: torch.Tensor,\n        batch_size_per_rank_per_feature: List[List[int]],\n        batch_size_per_feature_pre_a2a: List[int],\n    ) -> PooledEmbeddingsAwaitable:\n        \"\"\"\n        Performs AlltoAll pooled operation with variable batch size per feature on a\n        pooled embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n            batch_size_per_rank_per_feature (List[List[int]]): batch size per rank per\n                feature, post a2a. Used to get the input splits.\n            batch_size_per_feature_pre_a2a (List[int]): local batch size before\n                scattering, used to get the output splits.\n                Ordered by rank_0 feature, rank_1 feature, ...\n\n        Returns:\n            PooledEmbeddingsAwaitable: awaitable of pooled embeddings.\n        \"\"\"\n\n        tensor_awaitable = variable_batch_alltoall_pooled(\n            a2a_pooled_embs_tensor=local_embs,\n            batch_size_per_rank_per_feature=batch_size_per_rank_per_feature,\n            batch_size_per_feature_pre_a2a=batch_size_per_feature_pre_a2a,\n            emb_dim_per_rank_per_feature=self._emb_dim_per_rank_per_feature,\n            group=self._pg,\n            codecs=self._codecs,\n        )\n\n        pooled_embedding_awaitable = PooledEmbeddingsAwaitable(\n            tensor_awaitable=tensor_awaitable,\n        )\n        pooled_embedding_awaitable.callbacks.extend(self._callbacks)\n\n        return pooled_embedding_awaitable",
  "def callbacks(self) -> List[Callable[[torch.Tensor], torch.Tensor]]:\n        return self._callbacks",
  "def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n        cat_dim: int,\n    ) -> None:\n        super().__init__()\n        self._device = device\n        self._world_size = world_size\n        self._cat_dim = cat_dim",
  "def forward(\n        self,\n        tensors: List[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Performs AlltoOne operation with Reduce on pooled/sequence embeddings tensors.\n\n        Args:\n            tensors (List[torch.Tensor]): list of embedding tensors.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of the reduced embeddings.\n        \"\"\"\n        assert len(tensors) == self._world_size\n        return torch.ops.fbgemm.sum_reduce_to_one(\n            tensors,\n            self._device,\n        )",
  "def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n        cat_dim: int,\n    ) -> None:\n        super().__init__()\n        self._device = device\n        self._world_size = world_size\n        self._cat_dim = cat_dim",
  "def set_device(self, device_str: str) -> None:\n        self._device = torch.device(device_str)",
  "def forward(self, tensors: List[torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Performs AlltoOne operation on pooled/sequence embeddings tensors.\n\n        Args:\n            tensors (List[torch.Tensor]): list of embedding tensors.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of the merged embeddings.\n        \"\"\"\n        assert len(tensors) == self._world_size\n        is_target_device_cpu: bool = self._device.type == \"cpu\"\n\n        non_cat_size = tensors[0].size(1 - self._cat_dim)\n        # if src device is cuda, target device is cpu:\n        # 1. merge on first tensor device\n        # 2. move to cpu\n        device = self._device if not is_target_device_cpu else tensors[0].device\n        merge = torch.ops.fbgemm.merge_pooled_embeddings(\n            tensors,\n            non_cat_size,\n            device,\n            self._cat_dim,\n        )\n\n        return merge if not is_target_device_cpu else merge.to(self._device)",
  "def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._device = device\n        self._world_size = world_size",
  "def set_device(self, device_str: str) -> None:\n        self._device = torch.device(device_str)",
  "def forward(self, tensors: List[torch.Tensor]) -> List[torch.Tensor]:\n        \"\"\"\n        Performs AlltoOne operation on pooled embeddings tensors.\n\n        Args:\n            tensors (List[torch.Tensor]): list of pooled embedding tensors.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of the merged pooled embeddings.\n        \"\"\"\n\n        assert len(tensors) == self._world_size\n        return torch.ops.fbgemm.all_to_one_device(\n            tensors,\n            self._device,\n        )",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        codecs: Optional[QuantizedCommCodecs] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._codecs = codecs",
  "def forward(\n        self, local_embs: torch.Tensor, input_splits: Optional[List[int]] = None\n    ) -> PooledEmbeddingsAwaitable:\n        \"\"\"\n        Performs reduce scatter operation on pooled embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of shape\n                `[num_buckets * batch_size, dimension]`.\n            input_splits (Optional[List[int]]): list of splits for `local_embs` dim 0.\n\n        Returns:\n            PooledEmbeddingsAwaitable: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].\n        \"\"\"\n\n        if input_splits and len(set(input_splits)) > 1:\n            tensor_awaitable = reduce_scatter_v_pooled(\n                local_embs, input_splits, self._pg, codecs=self._codecs\n            )\n        else:\n            tensor_awaitable = reduce_scatter_base_pooled(\n                local_embs, self._pg, codecs=self._codecs\n            )\n        return PooledEmbeddingsAwaitable(tensor_awaitable=tensor_awaitable)",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        codecs: Optional[QuantizedCommCodecs] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._codecs = codecs",
  "def forward(self, local_emb: torch.Tensor) -> PooledEmbeddingsAwaitable:\n        \"\"\"\n        Performs reduce scatter operation on pooled embeddings tensor.\n\n        Args:\n            local_emb (torch.Tensor): tensor of shape\n                `[num_buckets x batch_size, dimension]`.\n\n        Returns:\n            PooledEmbeddingsAwaitable: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].\n        \"\"\"\n\n        tensor_awaitable = all_gather_base_pooled(\n            local_emb, self._pg, codecs=self._codecs\n        )\n        return PooledEmbeddingsAwaitable(tensor_awaitable=tensor_awaitable)",
  "def __init__(\n        self,\n        tensor_awaitable: Awaitable[torch.Tensor],\n        unbucketize_permute_tensor: Optional[torch.Tensor],\n        embedding_dim: int,\n    ) -> None:\n        super().__init__()\n        self._tensor_awaitable = tensor_awaitable\n\n        if unbucketize_permute_tensor is not None:\n            self.callbacks.append(\n                lambda ret: torch.index_select(\n                    ret.view(-1, embedding_dim),\n                    0,\n                    unbucketize_permute_tensor,\n                )\n            )",
  "def _wait_impl(self) -> torch.Tensor:\n        \"\"\"\n        Syncs sequence embeddings after collective operation.\n\n        Returns:\n            torch.Tensor: synced sequence embeddings.\n        \"\"\"\n\n        ret = self._tensor_awaitable.wait()\n        return ret",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        features_per_rank: List[int],\n        device: Optional[torch.device] = None,\n        codecs: Optional[QuantizedCommCodecs] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._local_split: int = features_per_rank[self._pg.rank()]\n        self._num_splits: int = self._pg.size()\n\n        forward_recat = []\n        for j in range(self._num_splits):\n            for i in range(self._local_split):\n                forward_recat.append(j + i * self._num_splits)\n        self.register_buffer(\n            \"_forward_recat_tensor\",\n            torch.tensor(forward_recat, device=device, dtype=torch.int),\n        )\n        backward_recat = []\n        for i in range(self._local_split):\n            for j in range(self._num_splits):\n                backward_recat.append(i + j * self._local_split)\n        self.register_buffer(\n            \"_backward_recat_tensor\",\n            torch.tensor(backward_recat, device=device, dtype=torch.int),\n        )\n        self._codecs = codecs",
  "def forward(\n        self,\n        local_embs: torch.Tensor,\n        lengths: torch.Tensor,\n        input_splits: List[int],\n        output_splits: List[int],\n        unbucketize_permute_tensor: Optional[torch.Tensor] = None,\n        batch_size_per_rank: Optional[List[int]] = None,\n        sparse_features_recat: Optional[torch.Tensor] = None,\n    ) -> SequenceEmbeddingsAwaitable:\n        \"\"\"\n        Performs AlltoAll operation on sequence embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): input embeddings tensor.\n            lengths (torch.Tensor): lengths of sparse features after AlltoAll.\n            input_splits (List[int]): input splits of AlltoAll.\n            output_splits (List[int]): output splits of AlltoAll.\n            unbucketize_permute_tensor (Optional[torch.Tensor]): stores the permute\n                order of the KJT bucketize (for row-wise sharding only).\n            batch_size_per_rank: (Optional[List[int]]): batch size per rank.\n            sparse_features_recat (Optional[torch.Tensor]): recat tensor used for sparse\n                feature input dist. Must be provided if using variable batch size.\n\n        Returns:\n            SequenceEmbeddingsAwaitable: awaitable of sequence embeddings.\n        \"\"\"\n\n        variable_batch_size = (\n            batch_size_per_rank is not None and len(set(batch_size_per_rank)) > 1\n        )\n\n        if sparse_features_recat is not None:\n            forward_recat_tensor = torch.ops.fbgemm.invert_permute(\n                sparse_features_recat\n            )\n            backward_recat_tensor = sparse_features_recat\n        else:\n            forward_recat_tensor = self._forward_recat_tensor\n            backward_recat_tensor = self._backward_recat_tensor\n\n        tensor_awaitable = alltoall_sequence(\n            a2a_sequence_embs_tensor=local_embs,\n            forward_recat_tensor=forward_recat_tensor,\n            backward_recat_tensor=backward_recat_tensor,\n            lengths_after_sparse_data_all2all=lengths,\n            input_splits=input_splits,\n            output_splits=output_splits,\n            variable_batch_size=variable_batch_size,\n            group=self._pg,\n            codecs=self._codecs,\n        )\n        return SequenceEmbeddingsAwaitable(\n            tensor_awaitable=tensor_awaitable,\n            unbucketize_permute_tensor=unbucketize_permute_tensor,\n            embedding_dim=local_embs.shape[1],\n        )",
  "def create_infer_embedding_bag_sharding(\n    sharding_type: str,\n    sharding_infos: List[EmbeddingShardingInfo],\n    env: ShardingEnv,\n) -> EmbeddingSharding[NullShardingContext, KJTList, List[torch.Tensor], torch.Tensor]:\n    if sharding_type == ShardingType.TABLE_WISE.value:\n        return InferTwEmbeddingSharding(sharding_infos, env, device=None)\n    elif sharding_type == ShardingType.ROW_WISE.value:\n        return InferRwPooledEmbeddingSharding(sharding_infos, env, device=None)\n    elif sharding_type == ShardingType.COLUMN_WISE.value:\n        return InferCwPooledEmbeddingSharding(\n            sharding_infos, env, device=None, permute_embeddings=True\n        )\n    else:\n        raise ValueError(f\"Sharding type not supported {sharding_type}\")",
  "class ShardedQuantEmbeddingBagCollection(\n    ShardedQuantEmbeddingModuleState[\n        ListOfKJTList,\n        List[List[torch.Tensor]],\n        KeyedTensor,\n        NullShardedModuleContext,\n    ],\n):\n    \"\"\"\n    Sharded implementation of `EmbeddingBagCollection`.\n    This is part of the public API to allow for manual data dist pipelining.\n    \"\"\"\n\n    def __init__(\n        self,\n        module: EmbeddingBagCollectionInterface,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._embedding_bag_configs: List[\n            EmbeddingBagConfig\n        ] = module.embedding_bag_configs()\n        sharding_type_to_sharding_infos = create_sharding_infos_by_sharding(\n            module, table_name_to_parameter_sharding, \"embedding_bags.\", fused_params\n        )\n        self._sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                NullShardingContext,\n                KJTList,\n                List[torch.Tensor],\n                torch.Tensor,\n            ],\n        ] = {\n            sharding_type: create_infer_embedding_bag_sharding(\n                sharding_type, embedding_confings, env\n            )\n            for sharding_type, embedding_confings in sharding_type_to_sharding_infos.items()\n        }\n        self._device = device\n        self._is_weighted: bool = module.is_weighted()\n        self._input_dists: List[nn.Module] = []\n        self._lookups: List[nn.Module] = []\n        self._create_lookups(fused_params, device)\n\n        # Ensure output dist is set for post processing from an inference runtime (ie. setting device from runtime).\n        self._output_dists: torch.nn.ModuleList = torch.nn.ModuleList()\n\n        self._embedding_names: List[str] = []\n        self._embedding_dims: List[int] = []\n        self._feature_splits: List[int] = []\n        self._features_order: List[int] = []\n\n        # forward pass flow control\n        self._has_uninitialized_input_dist: bool = True\n        self._has_uninitialized_output_dist: bool = True\n        self._has_features_permute: bool = True\n\n        tbes: Dict[\n            IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig\n        ] = get_tbes_to_register_from_iterable(self._lookups)\n\n        # Optional registration of TBEs for model post processing utilities\n        if is_fused_param_register_tbe(fused_params):\n            self.tbes: torch.nn.ModuleList = torch.nn.ModuleList(tbes.keys())\n\n        quant_state_dict_split_scale_bias = (\n            is_fused_param_quant_state_dict_split_scale_bias(fused_params)\n        )\n\n        if quant_state_dict_split_scale_bias:\n            self._initialize_torch_state(\n                tbes=tbes,\n                table_name_to_parameter_sharding=table_name_to_parameter_sharding,\n                tables_weights_prefix=\"embedding_bags\",\n            )\n        else:\n            table_wise_sharded_only: bool = all(\n                [\n                    sharding_type == ShardingType.TABLE_WISE.value\n                    for sharding_type in self._sharding_type_to_sharding.keys()\n                ]\n            )\n            assert (\n                table_wise_sharded_only\n            ), \"ROW_WISE,COLUMN_WISE shardings can be used only in 'quant_state_dict_split_scale_bias' mode, specify fused_params[FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS]=True to __init__ argument\"\n\n            self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n            for table in self._embedding_bag_configs:\n                self.embedding_bags[table.name] = torch.nn.Module()\n\n            for _sharding_type, lookup in zip(\n                self._sharding_type_to_sharding.keys(), self._lookups\n            ):\n                lookup_state_dict = lookup.state_dict()\n                for key in lookup_state_dict:\n                    if key.endswith(\".weight\"):\n                        table_name = key[: -len(\".weight\")]\n                        self.embedding_bags[table_name].register_buffer(\n                            \"weight\", lookup_state_dict[key]\n                        )\n\n    def _create_input_dist(\n        self,\n        input_feature_names: List[str],\n        features_device: torch.device,\n        input_dist_device: Optional[torch.device] = None,\n    ) -> None:\n        feature_names: List[str] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._input_dists.append(\n                sharding.create_input_dist(device=input_dist_device)\n            )\n            feature_names.extend(sharding.feature_names())\n            self._feature_splits.append(len(sharding.feature_names()))\n\n        if feature_names == input_feature_names:\n            self._has_features_permute = False\n        else:\n            for f in feature_names:\n                self._features_order.append(input_feature_names.index(f))\n            self.register_buffer(\n                \"_features_order_tensor\",\n                torch.tensor(\n                    self._features_order, device=features_device, dtype=torch.int32\n                ),\n                persistent=False,\n            )\n\n    def _create_lookups(\n        self,\n        fused_params: Optional[Dict[str, Any]],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._lookups.append(\n                sharding.create_lookup(\n                    device=device,\n                    fused_params=fused_params,\n                )\n            )\n\n    def _create_output_dist(self, device: Optional[torch.device] = None) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._output_dists.append(sharding.create_output_dist(device))\n            self._embedding_names.extend(sharding.embedding_names())\n            self._embedding_dims.extend(sharding.embedding_dims())\n\n    # pyre-ignore [14]\n    # pyre-ignore\n    def input_dist(\n        self, ctx: NullShardedModuleContext, features: KeyedJaggedTensor\n    ) -> ListOfKJTList:\n        if self._has_uninitialized_input_dist:\n            self._create_input_dist(\n                features.keys(),\n                features.device(),\n                self._device,\n            )\n            self._has_uninitialized_input_dist = False\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(features.device())\n            self._has_uninitialized_output_dist = False\n        with torch.no_grad():\n            if self._has_features_permute:\n                features = features.permute(\n                    self._features_order,\n                    self._features_order_tensor,\n                )\n            features_by_shards = features.split(self._feature_splits)\n            return ListOfKJTList(\n                [\n                    self._input_dists[i].forward(features_by_shards[i])\n                    for i in range(len(self._input_dists))\n                ]\n            )\n\n    def compute(\n        self,\n        ctx: NullShardedModuleContext,\n        dist_input: ListOfKJTList,\n    ) -> List[List[torch.Tensor]]:\n        # syntax for torchscript\n        return [lookup.forward(dist_input[i]) for i, lookup in enumerate(self._lookups)]\n\n    # pyre-ignore\n    def output_dist(\n        self,\n        ctx: NullShardedModuleContext,\n        output: List[List[torch.Tensor]],\n    ) -> KeyedTensor:\n        return construct_output_kt(\n            embeddings=[\n                dist.forward(output[i]) for i, dist in enumerate(self._output_dists)\n            ],\n            embedding_dims=self._embedding_dims,\n            embedding_names=self._embedding_names,\n        )\n\n    # pyre-ignore\n    def compute_and_output_dist(\n        self, ctx: NullShardedModuleContext, input: ListOfKJTList\n    ) -> KeyedTensor:\n        return self.output_dist(ctx, self.compute(ctx, input))\n\n    # pyre-ignore\n    def forward(self, *input, **kwargs) -> KeyedTensor:\n        ctx = self.create_context()\n        dist_input = self.input_dist(ctx, *input, **kwargs)\n        return self.compute_and_output_dist(ctx, dist_input)\n\n    def copy(self, device: torch.device) -> nn.Module:\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(device)\n            self._has_uninitialized_output_dist = False\n        return super().copy(device)\n\n    @property\n    def shardings(self) -> Dict[str, FeatureShardingMixIn]:\n        # pyre-ignore [7]\n        return self._sharding_type_to_sharding\n\n    def create_context(self) -> NullShardedModuleContext:\n        return NullShardedModuleContext()",
  "class QuantEmbeddingBagCollectionSharder(\n    BaseQuantEmbeddingSharder[QuantEmbeddingBagCollection]\n):\n    def shard(\n        self,\n        module: QuantEmbeddingBagCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedQuantEmbeddingBagCollection:\n        fused_params = self.fused_params if self.fused_params else {}\n        fused_params[\"output_dtype\"] = data_type_to_sparse_type(\n            dtype_to_data_type(module.output_dtype())\n        )\n        if FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS not in fused_params:\n            fused_params[FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS] = getattr(\n                module, MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS, False\n            )\n        if FUSED_PARAM_REGISTER_TBE_BOOL not in fused_params:\n            fused_params[FUSED_PARAM_REGISTER_TBE_BOOL] = getattr(\n                module, FUSED_PARAM_REGISTER_TBE_BOOL, False\n            )\n\n        return ShardedQuantEmbeddingBagCollection(\n            module, params, env, fused_params, device=device\n        )\n\n    @property\n    def module_type(self) -> Type[QuantEmbeddingBagCollection]:\n        return QuantEmbeddingBagCollection",
  "def __init__(\n        self,\n        module: EmbeddingBagCollectionInterface,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._embedding_bag_configs: List[\n            EmbeddingBagConfig\n        ] = module.embedding_bag_configs()\n        sharding_type_to_sharding_infos = create_sharding_infos_by_sharding(\n            module, table_name_to_parameter_sharding, \"embedding_bags.\", fused_params\n        )\n        self._sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                NullShardingContext,\n                KJTList,\n                List[torch.Tensor],\n                torch.Tensor,\n            ],\n        ] = {\n            sharding_type: create_infer_embedding_bag_sharding(\n                sharding_type, embedding_confings, env\n            )\n            for sharding_type, embedding_confings in sharding_type_to_sharding_infos.items()\n        }\n        self._device = device\n        self._is_weighted: bool = module.is_weighted()\n        self._input_dists: List[nn.Module] = []\n        self._lookups: List[nn.Module] = []\n        self._create_lookups(fused_params, device)\n\n        # Ensure output dist is set for post processing from an inference runtime (ie. setting device from runtime).\n        self._output_dists: torch.nn.ModuleList = torch.nn.ModuleList()\n\n        self._embedding_names: List[str] = []\n        self._embedding_dims: List[int] = []\n        self._feature_splits: List[int] = []\n        self._features_order: List[int] = []\n\n        # forward pass flow control\n        self._has_uninitialized_input_dist: bool = True\n        self._has_uninitialized_output_dist: bool = True\n        self._has_features_permute: bool = True\n\n        tbes: Dict[\n            IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig\n        ] = get_tbes_to_register_from_iterable(self._lookups)\n\n        # Optional registration of TBEs for model post processing utilities\n        if is_fused_param_register_tbe(fused_params):\n            self.tbes: torch.nn.ModuleList = torch.nn.ModuleList(tbes.keys())\n\n        quant_state_dict_split_scale_bias = (\n            is_fused_param_quant_state_dict_split_scale_bias(fused_params)\n        )\n\n        if quant_state_dict_split_scale_bias:\n            self._initialize_torch_state(\n                tbes=tbes,\n                table_name_to_parameter_sharding=table_name_to_parameter_sharding,\n                tables_weights_prefix=\"embedding_bags\",\n            )\n        else:\n            table_wise_sharded_only: bool = all(\n                [\n                    sharding_type == ShardingType.TABLE_WISE.value\n                    for sharding_type in self._sharding_type_to_sharding.keys()\n                ]\n            )\n            assert (\n                table_wise_sharded_only\n            ), \"ROW_WISE,COLUMN_WISE shardings can be used only in 'quant_state_dict_split_scale_bias' mode, specify fused_params[FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS]=True to __init__ argument\"\n\n            self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n            for table in self._embedding_bag_configs:\n                self.embedding_bags[table.name] = torch.nn.Module()\n\n            for _sharding_type, lookup in zip(\n                self._sharding_type_to_sharding.keys(), self._lookups\n            ):\n                lookup_state_dict = lookup.state_dict()\n                for key in lookup_state_dict:\n                    if key.endswith(\".weight\"):\n                        table_name = key[: -len(\".weight\")]\n                        self.embedding_bags[table_name].register_buffer(\n                            \"weight\", lookup_state_dict[key]\n                        )",
  "def _create_input_dist(\n        self,\n        input_feature_names: List[str],\n        features_device: torch.device,\n        input_dist_device: Optional[torch.device] = None,\n    ) -> None:\n        feature_names: List[str] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._input_dists.append(\n                sharding.create_input_dist(device=input_dist_device)\n            )\n            feature_names.extend(sharding.feature_names())\n            self._feature_splits.append(len(sharding.feature_names()))\n\n        if feature_names == input_feature_names:\n            self._has_features_permute = False\n        else:\n            for f in feature_names:\n                self._features_order.append(input_feature_names.index(f))\n            self.register_buffer(\n                \"_features_order_tensor\",\n                torch.tensor(\n                    self._features_order, device=features_device, dtype=torch.int32\n                ),\n                persistent=False,\n            )",
  "def _create_lookups(\n        self,\n        fused_params: Optional[Dict[str, Any]],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._lookups.append(\n                sharding.create_lookup(\n                    device=device,\n                    fused_params=fused_params,\n                )\n            )",
  "def _create_output_dist(self, device: Optional[torch.device] = None) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._output_dists.append(sharding.create_output_dist(device))\n            self._embedding_names.extend(sharding.embedding_names())\n            self._embedding_dims.extend(sharding.embedding_dims())",
  "def input_dist(\n        self, ctx: NullShardedModuleContext, features: KeyedJaggedTensor\n    ) -> ListOfKJTList:\n        if self._has_uninitialized_input_dist:\n            self._create_input_dist(\n                features.keys(),\n                features.device(),\n                self._device,\n            )\n            self._has_uninitialized_input_dist = False\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(features.device())\n            self._has_uninitialized_output_dist = False\n        with torch.no_grad():\n            if self._has_features_permute:\n                features = features.permute(\n                    self._features_order,\n                    self._features_order_tensor,\n                )\n            features_by_shards = features.split(self._feature_splits)\n            return ListOfKJTList(\n                [\n                    self._input_dists[i].forward(features_by_shards[i])\n                    for i in range(len(self._input_dists))\n                ]\n            )",
  "def compute(\n        self,\n        ctx: NullShardedModuleContext,\n        dist_input: ListOfKJTList,\n    ) -> List[List[torch.Tensor]]:\n        # syntax for torchscript\n        return [lookup.forward(dist_input[i]) for i, lookup in enumerate(self._lookups)]",
  "def output_dist(\n        self,\n        ctx: NullShardedModuleContext,\n        output: List[List[torch.Tensor]],\n    ) -> KeyedTensor:\n        return construct_output_kt(\n            embeddings=[\n                dist.forward(output[i]) for i, dist in enumerate(self._output_dists)\n            ],\n            embedding_dims=self._embedding_dims,\n            embedding_names=self._embedding_names,\n        )",
  "def compute_and_output_dist(\n        self, ctx: NullShardedModuleContext, input: ListOfKJTList\n    ) -> KeyedTensor:\n        return self.output_dist(ctx, self.compute(ctx, input))",
  "def forward(self, *input, **kwargs) -> KeyedTensor:\n        ctx = self.create_context()\n        dist_input = self.input_dist(ctx, *input, **kwargs)\n        return self.compute_and_output_dist(ctx, dist_input)",
  "def copy(self, device: torch.device) -> nn.Module:\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(device)\n            self._has_uninitialized_output_dist = False\n        return super().copy(device)",
  "def shardings(self) -> Dict[str, FeatureShardingMixIn]:\n        # pyre-ignore [7]\n        return self._sharding_type_to_sharding",
  "def create_context(self) -> NullShardedModuleContext:\n        return NullShardedModuleContext()",
  "def shard(\n        self,\n        module: QuantEmbeddingBagCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedQuantEmbeddingBagCollection:\n        fused_params = self.fused_params if self.fused_params else {}\n        fused_params[\"output_dtype\"] = data_type_to_sparse_type(\n            dtype_to_data_type(module.output_dtype())\n        )\n        if FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS not in fused_params:\n            fused_params[FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS] = getattr(\n                module, MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS, False\n            )\n        if FUSED_PARAM_REGISTER_TBE_BOOL not in fused_params:\n            fused_params[FUSED_PARAM_REGISTER_TBE_BOOL] = getattr(\n                module, FUSED_PARAM_REGISTER_TBE_BOOL, False\n            )\n\n        return ShardedQuantEmbeddingBagCollection(\n            module, params, env, fused_params, device=device\n        )",
  "def module_type(self) -> Type[QuantEmbeddingBagCollection]:\n        return QuantEmbeddingBagCollection",
  "class EmbeddingCollectionContext(Multistreamable):\n    sharding_contexts: List[InferSequenceShardingContext]\n\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for ctx in self.sharding_contexts:\n            ctx.record_stream(stream)",
  "def create_infer_embedding_sharding(\n    sharding_type: str,\n    sharding_infos: List[EmbeddingShardingInfo],\n    env: ShardingEnv,\n    device: Optional[torch.device] = None,\n) -> EmbeddingSharding[\n    InferSequenceShardingContext,\n    KJTList,\n    List[torch.Tensor],\n    List[torch.Tensor],\n]:\n    if sharding_type == ShardingType.TABLE_WISE.value:\n        return InferTwSequenceEmbeddingSharding(sharding_infos, env, device)\n    elif sharding_type == ShardingType.COLUMN_WISE.value:\n        return InferCwSequenceEmbeddingSharding(sharding_infos, env, device)\n    elif sharding_type == ShardingType.ROW_WISE.value:\n        return InferRwSequenceEmbeddingSharding(sharding_infos, env, device)\n    else:\n        raise ValueError(f\"Sharding type not supported {sharding_type}\")",
  "def _fx_unwrap_optional_tensor(optional: Optional[torch.Tensor]) -> torch.Tensor:\n    assert optional is not None, \"Expected optional to be non-None Tensor\"\n    return optional",
  "def _construct_jagged_tensors_tw(\n    embeddings: List[torch.Tensor],\n    features: KJTList,\n    need_indices: bool,\n) -> Dict[str, JaggedTensor]:\n    ret: Dict[str, JaggedTensor] = {}\n    for i in range(len(embeddings)):\n        embeddings_i: torch.Tensor = embeddings[i]\n        features_i: KeyedJaggedTensor = features[i]\n\n        lengths = features_i.lengths().view(-1, features_i.stride())\n        values = features_i.values()\n        length_per_key = features_i.length_per_key()\n\n        embeddings_list = torch.split(embeddings_i, length_per_key, dim=0)\n        stride = features_i.stride()\n        lengths_tuple = torch.unbind(lengths.view(-1, stride), dim=0)\n        if need_indices:\n            values_list = torch.split(values, length_per_key)\n            for i, key in enumerate(features_i.keys()):\n                ret[key] = JaggedTensor(\n                    lengths=lengths_tuple[i],\n                    values=embeddings_list[i],\n                    weights=values_list[i],\n                )\n        else:\n            for i, key in enumerate(features_i.keys()):\n                ret[key] = JaggedTensor(\n                    lengths=lengths_tuple[i],\n                    values=embeddings_list[i],\n                    weights=None,\n                )\n    return ret",
  "def _construct_jagged_tensors_rw(\n    embeddings: List[torch.Tensor],\n    features_before_input_dist: KeyedJaggedTensor,\n    need_indices: bool,\n    unbucketize_tensor: torch.Tensor,\n) -> Dict[str, JaggedTensor]:\n    ret: Dict[str, JaggedTensor] = {}\n    unbucketized_embs = torch.concat(embeddings, dim=0).index_select(\n        0, unbucketize_tensor\n    )\n    embs_split_per_key = unbucketized_embs.split(\n        features_before_input_dist.length_per_key(), dim=0\n    )\n    stride = features_before_input_dist.stride()\n    lengths_list = torch.unbind(\n        features_before_input_dist.lengths().view(-1, stride), dim=0\n    )\n    values_list: List[torch.Tensor] = []\n    if need_indices:\n        # pyre-ignore\n        values_list = torch.split(\n            features_before_input_dist.values(),\n            features_before_input_dist.length_per_key(),\n        )\n    for i, key in enumerate(features_before_input_dist.keys()):\n        ret[key] = JaggedTensor(\n            values=embs_split_per_key[i],\n            lengths=lengths_list[i],\n            weights=values_list[i] if need_indices else None,\n        )\n    return ret",
  "def _construct_jagged_tensors_cw(\n    embeddings: List[torch.Tensor],\n    features: KJTList,\n    embedding_names_per_rank: List[List[str]],\n    need_indices: bool,\n    features_to_permute_indices: Dict[str, torch.Tensor],\n) -> Dict[str, JaggedTensor]:\n    ret: Dict[str, JaggedTensor] = {}\n    stride = features[0].stride()\n    lengths_lists: List[List[torch.Tensor]] = []\n    embeddings_lists: List[List[torch.Tensor]] = []\n    values_lists: List[List[torch.Tensor]] = []\n    for i in range(len(features)):\n        embedding = embeddings[i]\n        feature = features[i]\n        lengths_lists.append(torch.unbind(feature.lengths().view(-1, stride), dim=0))\n        embeddings_lists.append(\n            list(torch.split(embedding, feature.length_per_key(), dim=0))\n        )\n    if need_indices:\n        for i in range(len(features)):\n            feature = features[i]\n            values_lists.append(\n                list(torch.split(feature.values(), feature.length_per_key()))\n            )\n\n    key_to_feature_coordinates: Dict[str, List[Tuple[int, int]]] = {}\n    for rank, embedding_names in enumerate(embedding_names_per_rank):\n        for idx_in_rank, embedding_name in enumerate(embedding_names):\n            if embedding_name not in key_to_feature_coordinates:\n                key_to_feature_coordinates[embedding_name] = torch.jit.annotate(\n                    List[Tuple[int, int]], []\n                )\n            key_to_feature_coordinates[embedding_name].append((rank, idx_in_rank))\n\n    for key, coordinates in key_to_feature_coordinates.items():\n        permuted_coordinates: List[Tuple[int, int]] = coordinates\n\n        if key in features_to_permute_indices:\n            permuted_coordinates = [(-1, -1)] * len(coordinates)\n            permute_indices: List[int] = features_to_permute_indices[key].tolist()\n            for i, permute_idx in enumerate(permute_indices):\n                permuted_coordinates[i] = coordinates[permute_idx]\n\n        rank0, idx_in_rank0 = permuted_coordinates[0]\n        ret[key] = JaggedTensor(\n            lengths=lengths_lists[rank0][idx_in_rank0],\n            values=torch.cat(\n                [\n                    embeddings_lists[rank][idx_in_rank]\n                    for rank, idx_in_rank in permuted_coordinates\n                ],\n                dim=1,\n            ),\n            weights=values_lists[rank0][idx_in_rank0] if need_indices else None,\n        )\n    return ret",
  "def _construct_jagged_tensors(\n    sharding_type: str,\n    embeddings: List[torch.Tensor],\n    features: KJTList,\n    embedding_names_per_rank: List[List[str]],\n    features_before_input_dist: KeyedJaggedTensor,\n    need_indices: bool,\n    rw_unbucketize_tensor: Optional[torch.Tensor],\n    cw_features_to_permute_indices: Dict[str, torch.Tensor],\n) -> Dict[str, JaggedTensor]:\n\n    # Validating sharding type and parameters\n    valid_sharding_types = [\n        ShardingType.ROW_WISE.value,\n        ShardingType.COLUMN_WISE.value,\n        ShardingType.TABLE_WISE.value,\n    ]\n    if sharding_type not in valid_sharding_types:\n        raise ValueError(f\"Unknown sharding type {sharding_type}\")\n\n    if sharding_type == ShardingType.ROW_WISE.value and rw_unbucketize_tensor is None:\n        raise ValueError(\"rw_unbucketize_tensor is required for row-wise sharding\")\n\n    if (\n        sharding_type == ShardingType.ROW_WISE.value\n        and rw_unbucketize_tensor is not None\n    ):\n        return _construct_jagged_tensors_rw(\n            embeddings,\n            features_before_input_dist,\n            need_indices,\n            rw_unbucketize_tensor,\n        )\n    elif sharding_type == ShardingType.COLUMN_WISE.value:\n        return _construct_jagged_tensors_cw(\n            embeddings,\n            features,\n            embedding_names_per_rank,\n            need_indices,\n            cw_features_to_permute_indices,\n        )\n    else:  # sharding_type == ShardingType.TABLE_WISE.value\n        return _construct_jagged_tensors_tw(embeddings, features, need_indices)",
  "def output_jt_dict(\n    sharding_types: List[str],\n    emb_per_sharding: List[List[torch.Tensor]],\n    features_per_sharding: List[KJTList],\n    embedding_names_per_rank_per_sharding: List[List[List[str]]],\n    need_indices: bool,\n    features_before_input_dist_per_sharding: List[KeyedJaggedTensor],\n    features_to_permute_indices: Dict[str, torch.Tensor],\n    unbucketize_tensors: List[torch.Tensor],\n    unbucketize_tensor_idxs_per_sharding: List[int],\n) -> Dict[str, JaggedTensor]:\n    jt_dict: Dict[str, JaggedTensor] = {}\n    for (\n        sharding_type,\n        emb_sharding,\n        features_sharding,\n        embedding_names_per_rank,\n        unbucketize_tensor_idx,\n        features_before_input_dist,\n    ) in zip(\n        sharding_types,\n        emb_per_sharding,\n        features_per_sharding,\n        embedding_names_per_rank_per_sharding,\n        unbucketize_tensor_idxs_per_sharding,\n        features_before_input_dist_per_sharding,\n    ):\n        jt_dict.update(\n            _construct_jagged_tensors(\n                sharding_type=sharding_type,\n                embeddings=emb_sharding,\n                features=features_sharding,\n                embedding_names_per_rank=embedding_names_per_rank,\n                features_before_input_dist=features_before_input_dist,\n                need_indices=need_indices,\n                rw_unbucketize_tensor=unbucketize_tensors[unbucketize_tensor_idx]\n                if unbucketize_tensor_idx != -1\n                else None,\n                cw_features_to_permute_indices=features_to_permute_indices,\n            )\n        )\n    return jt_dict",
  "class ShardedQuantEmbeddingCollection(\n    ShardedQuantEmbeddingModuleState[\n        ListOfKJTList,\n        List[List[torch.Tensor]],\n        Dict[str, JaggedTensor],\n        EmbeddingCollectionContext,\n    ],\n):\n    \"\"\"\n    Sharded implementation of `QuantEmbeddingCollection`.\n    \"\"\"\n\n    def __init__(\n        self,\n        module: QuantEmbeddingCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n\n        self._embedding_configs: List[EmbeddingConfig] = module.embedding_configs()\n\n        sharding_type_to_sharding_infos = create_sharding_infos_by_sharding(\n            module, table_name_to_parameter_sharding, fused_params\n        )\n        self._sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                InferSequenceShardingContext,\n                KJTList,\n                List[torch.Tensor],\n                List[torch.Tensor],\n            ],\n        ] = {\n            sharding_type: create_infer_embedding_sharding(\n                sharding_type, embedding_confings, env\n            )\n            for sharding_type, embedding_confings in sharding_type_to_sharding_infos.items()\n        }\n        self._embedding_dim: int = module.embedding_dim()\n        self._local_embedding_dim: int = self._embedding_dim\n        self._embedding_names_per_sharding: List[List[str]] = []\n        self._embedding_names_per_rank_per_sharding: List[List[List[str]]] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._embedding_names_per_sharding.append(sharding.embedding_names())\n            self._embedding_names_per_rank_per_sharding.append(\n                sharding.embedding_names_per_rank()\n            )\n        self._features_to_permute_indices: Dict[str, torch.Tensor] = {}\n        if ShardingType.COLUMN_WISE.value in self._sharding_type_to_sharding:\n            sharding = self._sharding_type_to_sharding[ShardingType.COLUMN_WISE.value]\n            # CW partition must be same for all CW sharded parameters\n            self._local_embedding_dim = cast(\n                ShardMetadata, sharding.embedding_shard_metadata()[0]\n            ).shard_sizes[1]\n            self._features_to_permute_indices = (\n                self._generate_permute_indices_per_feature(\n                    module.embedding_configs(), table_name_to_parameter_sharding\n                )\n            )\n\n        self._device = device\n        self._input_dists: List[nn.Module] = []\n        self._lookups: List[nn.Module] = []\n        self._create_lookups(fused_params, device)\n\n        # Ensure output dist is set for post processing from an inference runtime (ie. setting device from runtime).\n        self._output_dists: torch.nn.ModuleList = torch.nn.ModuleList()\n\n        self._feature_splits: List[int] = []\n        self._features_order: List[int] = []\n\n        self._has_uninitialized_input_dist: bool = True\n        self._has_uninitialized_output_dist: bool = True\n\n        self._embedding_dim: int = module.embedding_dim()\n        self._need_indices: bool = module.need_indices()\n\n        self._fused_params = fused_params\n\n        tbes: Dict[\n            IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig\n        ] = get_tbes_to_register_from_iterable(self._lookups)\n\n        # Optional registration of TBEs for model post processing utilities\n        if is_fused_param_register_tbe(fused_params):\n            self.tbes: torch.nn.ModuleList = torch.nn.ModuleList(tbes.keys())\n\n        quant_state_dict_split_scale_bias = (\n            is_fused_param_quant_state_dict_split_scale_bias(fused_params)\n        )\n\n        if quant_state_dict_split_scale_bias:\n            self._initialize_torch_state(\n                tbes=tbes,\n                table_name_to_parameter_sharding=table_name_to_parameter_sharding,\n                tables_weights_prefix=\"embeddings\",\n            )\n        else:\n            table_wise_sharded_only: bool = all(\n                [\n                    sharding_type == ShardingType.TABLE_WISE.value\n                    for sharding_type in self._sharding_type_to_sharding.keys()\n                ]\n            )\n            assert (\n                table_wise_sharded_only\n            ), \"ROW_WISE,COLUMN_WISE shardings can be used only in 'quant_state_dict_split_scale_bias' mode, specify fused_params[FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS]=True to __init__ argument\"\n\n            self.embeddings: nn.ModuleDict = nn.ModuleDict()\n            for table in self._embedding_configs:\n                self.embeddings[table.name] = torch.nn.Module()\n\n            for _sharding_type, lookup in zip(\n                self._sharding_type_to_sharding.keys(), self._lookups\n            ):\n                lookup_state_dict = lookup.state_dict()\n                for key in lookup_state_dict:\n                    if key.endswith(\".weight\"):\n                        table_name = key[: -len(\".weight\")]\n                        self.embeddings[table_name].register_buffer(\n                            \"weight\", lookup_state_dict[key]\n                        )\n\n    def _generate_permute_indices_per_feature(\n        self,\n        embedding_configs: List[EmbeddingConfig],\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n    ) -> Dict[str, torch.Tensor]:\n        ret: Dict[str, torch.Tensor] = {}\n        shared_feature: Dict[str, bool] = {}\n        for table in embedding_configs:\n            for feature_name in table.feature_names:\n                if feature_name not in shared_feature:\n                    shared_feature[feature_name] = False\n                else:\n                    shared_feature[feature_name] = True\n\n        for table in embedding_configs:\n            sharding = table_name_to_parameter_sharding[table.name]\n            if sharding.sharding_type != ShardingType.COLUMN_WISE.value:\n                continue\n            ranks = cast(List[int], sharding.ranks)\n            rank_to_indices = defaultdict(deque)\n            for i, rank in enumerate(sorted(ranks)):\n                rank_to_indices[rank].append(i)\n            permute_indices = [rank_to_indices[rank].popleft() for rank in ranks]\n            tensor = torch.tensor(permute_indices, dtype=torch.int64)\n            for feature_name in table.feature_names:\n                if shared_feature[feature_name]:\n                    ret[feature_name + \"@\" + table.name] = tensor\n                else:\n                    ret[feature_name] = tensor\n        return ret\n\n    def _create_input_dist(\n        self,\n        input_feature_names: List[str],\n        device: torch.device,\n        input_dist_device: Optional[torch.device] = None,\n    ) -> None:\n        feature_names: List[str] = []\n        self._feature_splits: List[int] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._input_dists.append(\n                sharding.create_input_dist(device=input_dist_device)\n            )\n            feature_names.extend(sharding.feature_names())\n            self._feature_splits.append(len(sharding.feature_names()))\n        self._features_order: List[int] = []\n        for f in feature_names:\n            self._features_order.append(input_feature_names.index(f))\n        self._features_order = (\n            []\n            if self._features_order == list(range(len(self._features_order)))\n            else self._features_order\n        )\n        self.register_buffer(\n            \"_features_order_tensor\",\n            torch.tensor(self._features_order, device=device, dtype=torch.int32),\n            persistent=False,\n        )\n\n    def _create_lookups(\n        self,\n        fused_params: Optional[Dict[str, Any]],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._lookups.append(\n                sharding.create_lookup(fused_params=fused_params, device=device)\n            )\n\n    def _create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._output_dists.append(sharding.create_output_dist(device))\n\n    # pyre-ignore [14]\n    # pyre-ignore\n    def input_dist(\n        self,\n        ctx: EmbeddingCollectionContext,\n        features: KeyedJaggedTensor,\n    ) -> ListOfKJTList:\n        if self._has_uninitialized_input_dist:\n            self._create_input_dist(\n                input_feature_names=features.keys() if features is not None else [],\n                device=features.device(),\n                input_dist_device=self._device,\n            )\n            self._has_uninitialized_input_dist = False\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(features.device())\n            self._has_uninitialized_output_dist = False\n        ret: List[KJTList] = []\n        with torch.no_grad():\n            features_by_sharding = []\n            if self._features_order:\n                features = features.permute(\n                    self._features_order,\n                    self._features_order_tensor,\n                )\n            features_by_sharding = features.split(\n                self._feature_splits,\n            )\n\n            for i in range(len(self._input_dists)):\n                input_dist = self._input_dists[i]\n                input_dist_result = input_dist.forward(features_by_sharding[i])\n                ret.append(input_dist_result)\n                ctx.sharding_contexts.append(\n                    InferSequenceShardingContext(\n                        features=input_dist_result,\n                        features_before_input_dist=features,\n                        unbucketize_permute_tensor=input_dist.unbucketize_permute_tensor\n                        if isinstance(input_dist, InferRwSparseFeaturesDist)\n                        else None,\n                    )\n                )\n        return ListOfKJTList(ret)\n\n    def _embedding_dim_for_sharding_type(self, sharding_type: str) -> int:\n        return (\n            self._local_embedding_dim\n            if sharding_type == ShardingType.COLUMN_WISE.value\n            else self._embedding_dim\n        )\n\n    def compute(\n        self, ctx: EmbeddingCollectionContext, dist_input: ListOfKJTList\n    ) -> List[List[torch.Tensor]]:\n        ret: List[List[torch.Tensor]] = []\n\n        for lookup, features, sharding_type in zip(\n            self._lookups, dist_input, self._sharding_type_to_sharding.keys()\n        ):\n            embedding_dim = self._embedding_dim_for_sharding_type(sharding_type)\n            ret.append([o.view(-1, embedding_dim) for o in lookup.forward(features)])\n        return ret\n\n    # pyre-ignore\n    def output_dist(\n        self, ctx: EmbeddingCollectionContext, output: List[List[torch.Tensor]]\n    ) -> Dict[str, JaggedTensor]:\n        emb_per_sharding: List[List[torch.Tensor]] = []\n        features_before_input_dist_per_sharding: List[KeyedJaggedTensor] = []\n        features_per_sharding: List[KJTList] = []\n        unbucketize_tensor_idxs_per_sharding: List[int] = []\n        unbucketize_tensors: List[torch.Tensor] = []\n        for sharding_output_dist, embeddings, sharding_ctx in zip(\n            self._output_dists,\n            output,\n            ctx.sharding_contexts,\n        ):\n            sharding_output_dist_res: List[torch.Tensor] = sharding_output_dist.forward(\n                embeddings, sharding_ctx\n            )\n            emb_per_sharding.append(sharding_output_dist_res)\n            features_per_sharding.append(sharding_ctx.features)\n            if sharding_ctx.unbucketize_permute_tensor is None:\n                unbucketize_tensor_idxs_per_sharding.append(-1)\n            else:\n                unbucketize_tensors.append(\n                    _fx_unwrap_optional_tensor(sharding_ctx.unbucketize_permute_tensor)\n                )\n                unbucketize_tensor_idxs_per_sharding.append(\n                    len(unbucketize_tensors) - 1\n                )\n            features_before_input_dist_per_sharding.append(\n                # pyre-ignore\n                sharding_ctx.features_before_input_dist\n            )\n        return output_jt_dict(\n            sharding_types=list(self._sharding_type_to_sharding.keys()),\n            emb_per_sharding=emb_per_sharding,\n            features_per_sharding=features_per_sharding,\n            embedding_names_per_rank_per_sharding=self._embedding_names_per_rank_per_sharding,\n            need_indices=self._need_indices,\n            features_before_input_dist_per_sharding=features_before_input_dist_per_sharding,\n            unbucketize_tensor_idxs_per_sharding=unbucketize_tensor_idxs_per_sharding,\n            unbucketize_tensors=unbucketize_tensors,\n            features_to_permute_indices=self._features_to_permute_indices,\n        )\n\n    # pyre-ignore\n    def compute_and_output_dist(\n        self, ctx: EmbeddingCollectionContext, input: ListOfKJTList\n    ) -> Dict[str, JaggedTensor]:\n        return self.output_dist(ctx, self.compute(ctx, input))\n\n    # pyre-ignore\n    def forward(self, *input, **kwargs) -> Dict[str, JaggedTensor]:\n        ctx = self.create_context()\n        dist_input = self.input_dist(ctx, *input, **kwargs)\n        return self.compute_and_output_dist(ctx, dist_input)\n\n    def copy(self, device: torch.device) -> nn.Module:\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(device)\n            self._has_uninitialized_output_dist = False\n        return super().copy(device)\n\n    def create_context(self) -> EmbeddingCollectionContext:\n        return EmbeddingCollectionContext(sharding_contexts=[])\n\n    @property\n    def shardings(self) -> Dict[str, FeatureShardingMixIn]:\n        # pyre-ignore [7]\n        return self._sharding_type_to_sharding",
  "class QuantEmbeddingCollectionSharder(\n    BaseQuantEmbeddingSharder[QuantEmbeddingCollection]\n):\n    \"\"\"\n    This implementation uses non-fused EmbeddingCollection\n    \"\"\"\n\n    def shard(\n        self,\n        module: QuantEmbeddingCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedQuantEmbeddingCollection:\n        fused_params = self.fused_params if self.fused_params else {}\n        fused_params[\"output_dtype\"] = data_type_to_sparse_type(\n            dtype_to_data_type(module.output_dtype())\n        )\n        if FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS not in fused_params:\n            fused_params[FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS] = getattr(\n                module, MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS, False\n            )\n        if FUSED_PARAM_REGISTER_TBE_BOOL not in fused_params:\n            fused_params[FUSED_PARAM_REGISTER_TBE_BOOL] = getattr(\n                module, FUSED_PARAM_REGISTER_TBE_BOOL, False\n            )\n        return ShardedQuantEmbeddingCollection(\n            module=module,\n            table_name_to_parameter_sharding=params,\n            env=env,\n            fused_params=fused_params,\n            device=device,\n        )\n\n    @property\n    def module_type(self) -> Type[QuantEmbeddingCollection]:\n        return QuantEmbeddingCollection",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for ctx in self.sharding_contexts:\n            ctx.record_stream(stream)",
  "def __init__(\n        self,\n        module: QuantEmbeddingCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        fused_params: Optional[Dict[str, Any]] = None,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n\n        self._embedding_configs: List[EmbeddingConfig] = module.embedding_configs()\n\n        sharding_type_to_sharding_infos = create_sharding_infos_by_sharding(\n            module, table_name_to_parameter_sharding, fused_params\n        )\n        self._sharding_type_to_sharding: Dict[\n            str,\n            EmbeddingSharding[\n                InferSequenceShardingContext,\n                KJTList,\n                List[torch.Tensor],\n                List[torch.Tensor],\n            ],\n        ] = {\n            sharding_type: create_infer_embedding_sharding(\n                sharding_type, embedding_confings, env\n            )\n            for sharding_type, embedding_confings in sharding_type_to_sharding_infos.items()\n        }\n        self._embedding_dim: int = module.embedding_dim()\n        self._local_embedding_dim: int = self._embedding_dim\n        self._embedding_names_per_sharding: List[List[str]] = []\n        self._embedding_names_per_rank_per_sharding: List[List[List[str]]] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._embedding_names_per_sharding.append(sharding.embedding_names())\n            self._embedding_names_per_rank_per_sharding.append(\n                sharding.embedding_names_per_rank()\n            )\n        self._features_to_permute_indices: Dict[str, torch.Tensor] = {}\n        if ShardingType.COLUMN_WISE.value in self._sharding_type_to_sharding:\n            sharding = self._sharding_type_to_sharding[ShardingType.COLUMN_WISE.value]\n            # CW partition must be same for all CW sharded parameters\n            self._local_embedding_dim = cast(\n                ShardMetadata, sharding.embedding_shard_metadata()[0]\n            ).shard_sizes[1]\n            self._features_to_permute_indices = (\n                self._generate_permute_indices_per_feature(\n                    module.embedding_configs(), table_name_to_parameter_sharding\n                )\n            )\n\n        self._device = device\n        self._input_dists: List[nn.Module] = []\n        self._lookups: List[nn.Module] = []\n        self._create_lookups(fused_params, device)\n\n        # Ensure output dist is set for post processing from an inference runtime (ie. setting device from runtime).\n        self._output_dists: torch.nn.ModuleList = torch.nn.ModuleList()\n\n        self._feature_splits: List[int] = []\n        self._features_order: List[int] = []\n\n        self._has_uninitialized_input_dist: bool = True\n        self._has_uninitialized_output_dist: bool = True\n\n        self._embedding_dim: int = module.embedding_dim()\n        self._need_indices: bool = module.need_indices()\n\n        self._fused_params = fused_params\n\n        tbes: Dict[\n            IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig\n        ] = get_tbes_to_register_from_iterable(self._lookups)\n\n        # Optional registration of TBEs for model post processing utilities\n        if is_fused_param_register_tbe(fused_params):\n            self.tbes: torch.nn.ModuleList = torch.nn.ModuleList(tbes.keys())\n\n        quant_state_dict_split_scale_bias = (\n            is_fused_param_quant_state_dict_split_scale_bias(fused_params)\n        )\n\n        if quant_state_dict_split_scale_bias:\n            self._initialize_torch_state(\n                tbes=tbes,\n                table_name_to_parameter_sharding=table_name_to_parameter_sharding,\n                tables_weights_prefix=\"embeddings\",\n            )\n        else:\n            table_wise_sharded_only: bool = all(\n                [\n                    sharding_type == ShardingType.TABLE_WISE.value\n                    for sharding_type in self._sharding_type_to_sharding.keys()\n                ]\n            )\n            assert (\n                table_wise_sharded_only\n            ), \"ROW_WISE,COLUMN_WISE shardings can be used only in 'quant_state_dict_split_scale_bias' mode, specify fused_params[FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS]=True to __init__ argument\"\n\n            self.embeddings: nn.ModuleDict = nn.ModuleDict()\n            for table in self._embedding_configs:\n                self.embeddings[table.name] = torch.nn.Module()\n\n            for _sharding_type, lookup in zip(\n                self._sharding_type_to_sharding.keys(), self._lookups\n            ):\n                lookup_state_dict = lookup.state_dict()\n                for key in lookup_state_dict:\n                    if key.endswith(\".weight\"):\n                        table_name = key[: -len(\".weight\")]\n                        self.embeddings[table_name].register_buffer(\n                            \"weight\", lookup_state_dict[key]\n                        )",
  "def _generate_permute_indices_per_feature(\n        self,\n        embedding_configs: List[EmbeddingConfig],\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n    ) -> Dict[str, torch.Tensor]:\n        ret: Dict[str, torch.Tensor] = {}\n        shared_feature: Dict[str, bool] = {}\n        for table in embedding_configs:\n            for feature_name in table.feature_names:\n                if feature_name not in shared_feature:\n                    shared_feature[feature_name] = False\n                else:\n                    shared_feature[feature_name] = True\n\n        for table in embedding_configs:\n            sharding = table_name_to_parameter_sharding[table.name]\n            if sharding.sharding_type != ShardingType.COLUMN_WISE.value:\n                continue\n            ranks = cast(List[int], sharding.ranks)\n            rank_to_indices = defaultdict(deque)\n            for i, rank in enumerate(sorted(ranks)):\n                rank_to_indices[rank].append(i)\n            permute_indices = [rank_to_indices[rank].popleft() for rank in ranks]\n            tensor = torch.tensor(permute_indices, dtype=torch.int64)\n            for feature_name in table.feature_names:\n                if shared_feature[feature_name]:\n                    ret[feature_name + \"@\" + table.name] = tensor\n                else:\n                    ret[feature_name] = tensor\n        return ret",
  "def _create_input_dist(\n        self,\n        input_feature_names: List[str],\n        device: torch.device,\n        input_dist_device: Optional[torch.device] = None,\n    ) -> None:\n        feature_names: List[str] = []\n        self._feature_splits: List[int] = []\n        for sharding in self._sharding_type_to_sharding.values():\n            self._input_dists.append(\n                sharding.create_input_dist(device=input_dist_device)\n            )\n            feature_names.extend(sharding.feature_names())\n            self._feature_splits.append(len(sharding.feature_names()))\n        self._features_order: List[int] = []\n        for f in feature_names:\n            self._features_order.append(input_feature_names.index(f))\n        self._features_order = (\n            []\n            if self._features_order == list(range(len(self._features_order)))\n            else self._features_order\n        )\n        self.register_buffer(\n            \"_features_order_tensor\",\n            torch.tensor(self._features_order, device=device, dtype=torch.int32),\n            persistent=False,\n        )",
  "def _create_lookups(\n        self,\n        fused_params: Optional[Dict[str, Any]],\n        device: Optional[torch.device] = None,\n    ) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._lookups.append(\n                sharding.create_lookup(fused_params=fused_params, device=device)\n            )",
  "def _create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        for sharding in self._sharding_type_to_sharding.values():\n            self._output_dists.append(sharding.create_output_dist(device))",
  "def input_dist(\n        self,\n        ctx: EmbeddingCollectionContext,\n        features: KeyedJaggedTensor,\n    ) -> ListOfKJTList:\n        if self._has_uninitialized_input_dist:\n            self._create_input_dist(\n                input_feature_names=features.keys() if features is not None else [],\n                device=features.device(),\n                input_dist_device=self._device,\n            )\n            self._has_uninitialized_input_dist = False\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(features.device())\n            self._has_uninitialized_output_dist = False\n        ret: List[KJTList] = []\n        with torch.no_grad():\n            features_by_sharding = []\n            if self._features_order:\n                features = features.permute(\n                    self._features_order,\n                    self._features_order_tensor,\n                )\n            features_by_sharding = features.split(\n                self._feature_splits,\n            )\n\n            for i in range(len(self._input_dists)):\n                input_dist = self._input_dists[i]\n                input_dist_result = input_dist.forward(features_by_sharding[i])\n                ret.append(input_dist_result)\n                ctx.sharding_contexts.append(\n                    InferSequenceShardingContext(\n                        features=input_dist_result,\n                        features_before_input_dist=features,\n                        unbucketize_permute_tensor=input_dist.unbucketize_permute_tensor\n                        if isinstance(input_dist, InferRwSparseFeaturesDist)\n                        else None,\n                    )\n                )\n        return ListOfKJTList(ret)",
  "def _embedding_dim_for_sharding_type(self, sharding_type: str) -> int:\n        return (\n            self._local_embedding_dim\n            if sharding_type == ShardingType.COLUMN_WISE.value\n            else self._embedding_dim\n        )",
  "def compute(\n        self, ctx: EmbeddingCollectionContext, dist_input: ListOfKJTList\n    ) -> List[List[torch.Tensor]]:\n        ret: List[List[torch.Tensor]] = []\n\n        for lookup, features, sharding_type in zip(\n            self._lookups, dist_input, self._sharding_type_to_sharding.keys()\n        ):\n            embedding_dim = self._embedding_dim_for_sharding_type(sharding_type)\n            ret.append([o.view(-1, embedding_dim) for o in lookup.forward(features)])\n        return ret",
  "def output_dist(\n        self, ctx: EmbeddingCollectionContext, output: List[List[torch.Tensor]]\n    ) -> Dict[str, JaggedTensor]:\n        emb_per_sharding: List[List[torch.Tensor]] = []\n        features_before_input_dist_per_sharding: List[KeyedJaggedTensor] = []\n        features_per_sharding: List[KJTList] = []\n        unbucketize_tensor_idxs_per_sharding: List[int] = []\n        unbucketize_tensors: List[torch.Tensor] = []\n        for sharding_output_dist, embeddings, sharding_ctx in zip(\n            self._output_dists,\n            output,\n            ctx.sharding_contexts,\n        ):\n            sharding_output_dist_res: List[torch.Tensor] = sharding_output_dist.forward(\n                embeddings, sharding_ctx\n            )\n            emb_per_sharding.append(sharding_output_dist_res)\n            features_per_sharding.append(sharding_ctx.features)\n            if sharding_ctx.unbucketize_permute_tensor is None:\n                unbucketize_tensor_idxs_per_sharding.append(-1)\n            else:\n                unbucketize_tensors.append(\n                    _fx_unwrap_optional_tensor(sharding_ctx.unbucketize_permute_tensor)\n                )\n                unbucketize_tensor_idxs_per_sharding.append(\n                    len(unbucketize_tensors) - 1\n                )\n            features_before_input_dist_per_sharding.append(\n                # pyre-ignore\n                sharding_ctx.features_before_input_dist\n            )\n        return output_jt_dict(\n            sharding_types=list(self._sharding_type_to_sharding.keys()),\n            emb_per_sharding=emb_per_sharding,\n            features_per_sharding=features_per_sharding,\n            embedding_names_per_rank_per_sharding=self._embedding_names_per_rank_per_sharding,\n            need_indices=self._need_indices,\n            features_before_input_dist_per_sharding=features_before_input_dist_per_sharding,\n            unbucketize_tensor_idxs_per_sharding=unbucketize_tensor_idxs_per_sharding,\n            unbucketize_tensors=unbucketize_tensors,\n            features_to_permute_indices=self._features_to_permute_indices,\n        )",
  "def compute_and_output_dist(\n        self, ctx: EmbeddingCollectionContext, input: ListOfKJTList\n    ) -> Dict[str, JaggedTensor]:\n        return self.output_dist(ctx, self.compute(ctx, input))",
  "def forward(self, *input, **kwargs) -> Dict[str, JaggedTensor]:\n        ctx = self.create_context()\n        dist_input = self.input_dist(ctx, *input, **kwargs)\n        return self.compute_and_output_dist(ctx, dist_input)",
  "def copy(self, device: torch.device) -> nn.Module:\n        if self._has_uninitialized_output_dist:\n            self._create_output_dist(device)\n            self._has_uninitialized_output_dist = False\n        return super().copy(device)",
  "def create_context(self) -> EmbeddingCollectionContext:\n        return EmbeddingCollectionContext(sharding_contexts=[])",
  "def shardings(self) -> Dict[str, FeatureShardingMixIn]:\n        # pyre-ignore [7]\n        return self._sharding_type_to_sharding",
  "def shard(\n        self,\n        module: QuantEmbeddingCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedQuantEmbeddingCollection:\n        fused_params = self.fused_params if self.fused_params else {}\n        fused_params[\"output_dtype\"] = data_type_to_sparse_type(\n            dtype_to_data_type(module.output_dtype())\n        )\n        if FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS not in fused_params:\n            fused_params[FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS] = getattr(\n                module, MODULE_ATTR_QUANT_STATE_DICT_SPLIT_SCALE_BIAS, False\n            )\n        if FUSED_PARAM_REGISTER_TBE_BOOL not in fused_params:\n            fused_params[FUSED_PARAM_REGISTER_TBE_BOOL] = getattr(\n                module, FUSED_PARAM_REGISTER_TBE_BOOL, False\n            )\n        return ShardedQuantEmbeddingCollection(\n            module=module,\n            table_name_to_parameter_sharding=params,\n            env=env,\n            fused_params=fused_params,\n            device=device,\n        )",
  "def module_type(self) -> Type[QuantEmbeddingCollection]:\n        return QuantEmbeddingCollection",
  "class ShardedFusedEmbeddingCollection(\n    ShardedEmbeddingCollection,\n    FusedOptimizerModule,\n):\n    def __init__(\n        self,\n        module: FusedEmbeddingCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        optimizer_type = module.optimizer_type()\n        optimizer_kwargs = module.optimizer_kwargs()\n\n        fused_params = {}\n        emb_opt_type_and_kwargs = convert_optimizer_type_and_kwargs(\n            optimizer_type, optimizer_kwargs\n        )\n        assert emb_opt_type_and_kwargs is not None\n        (emb_optim_type, emb_opt_kwargs) = emb_opt_type_and_kwargs\n\n        fused_params[\"optimizer\"] = emb_optim_type\n        fused_params.update(emb_opt_kwargs)\n\n        super().__init__(\n            # pyre-ignore\n            module=module,\n            table_name_to_parameter_sharding=table_name_to_parameter_sharding,\n            fused_params=fused_params,\n            env=env,\n            device=device,\n        )\n\n        for index, (sharding, lookup) in enumerate(\n            zip(self._sharding_type_to_sharding.values(), self._lookups)\n        ):\n            if isinstance(sharding, DpPooledEmbeddingSharding):\n                self._lookups[index] = DistributedDataParallel(\n                    module=lookup,\n                    device_ids=[device],\n                    process_group=env.process_group,\n                    gradient_as_bucket_view=True,\n                    broadcast_buffers=False,\n                    static_graph=True,\n                )\n                self._lookups[index]._register_fused_optim(\n                    optimizer_type, **optimizer_kwargs\n                )\n                # TODO - We need a way to get this optimizer back (and add to optims) so it\n                # can be checkpointed.\n                # We need to ensure that a checkpoint from DDP and a checkpoint from a\n                # model parallel version are compatible.\n\n    def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        # different than ShardedEmbeddingCollection - we consider DDP to be \"sharded\", so that it doesn't get wrapped up in ddp again\n        # semantics of this is actually \"parameters that don't need to have their gradients reduced\"\n        for lookup, _ in zip(self._lookups, self._sharding_type_to_sharding.keys()):\n            for name, _ in lookup.named_parameters(append_prefix(prefix, \"embeddings\")):\n                yield name",
  "class FusedEmbeddingCollectionSharder(BaseEmbeddingSharder[FusedEmbeddingCollection]):\n    def shard(\n        self,\n        module: FusedEmbeddingCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedFusedEmbeddingCollection:\n\n        return ShardedFusedEmbeddingCollection(module, params, env, device)\n\n    def shardable_parameters(\n        self, module: FusedEmbeddingCollection\n    ) -> Dict[str, nn.Parameter]:\n\n        params = {\n            # state_dict value looks like model.embeddings.table_0.weights\n            name.split(\".\")[-2]: param\n            for name, param in module.state_dict().items()\n            if name.endswith(\".weight\")\n        }\n        return params\n\n    @property\n    def module_type(self) -> Type[FusedEmbeddingCollection]:\n        return FusedEmbeddingCollection\n\n    def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.DATA_PARALLEL.value,\n            ShardingType.TABLE_WISE.value,\n            ShardingType.COLUMN_WISE.value,\n            ShardingType.ROW_WISE.value,\n        ]\n\n        return types\n\n    def compute_kernels(\n        self, sharding_type: str, compute_device_type: str\n    ) -> List[str]:\n        ret = []\n        if sharding_type != ShardingType.DATA_PARALLEL.value:\n            ret += [\n                EmbeddingComputeKernel.FUSED.value,\n            ]\n            if compute_device_type in {\"cuda\"}:\n                ret += [\n                    EmbeddingComputeKernel.FUSED_UVM.value,\n                    EmbeddingComputeKernel.FUSED_UVM_CACHING.value,\n                ]\n        else:\n            ret.append(EmbeddingComputeKernel.DENSE.value)\n        return ret",
  "def __init__(\n        self,\n        module: FusedEmbeddingCollection,\n        table_name_to_parameter_sharding: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        optimizer_type = module.optimizer_type()\n        optimizer_kwargs = module.optimizer_kwargs()\n\n        fused_params = {}\n        emb_opt_type_and_kwargs = convert_optimizer_type_and_kwargs(\n            optimizer_type, optimizer_kwargs\n        )\n        assert emb_opt_type_and_kwargs is not None\n        (emb_optim_type, emb_opt_kwargs) = emb_opt_type_and_kwargs\n\n        fused_params[\"optimizer\"] = emb_optim_type\n        fused_params.update(emb_opt_kwargs)\n\n        super().__init__(\n            # pyre-ignore\n            module=module,\n            table_name_to_parameter_sharding=table_name_to_parameter_sharding,\n            fused_params=fused_params,\n            env=env,\n            device=device,\n        )\n\n        for index, (sharding, lookup) in enumerate(\n            zip(self._sharding_type_to_sharding.values(), self._lookups)\n        ):\n            if isinstance(sharding, DpPooledEmbeddingSharding):\n                self._lookups[index] = DistributedDataParallel(\n                    module=lookup,\n                    device_ids=[device],\n                    process_group=env.process_group,\n                    gradient_as_bucket_view=True,\n                    broadcast_buffers=False,\n                    static_graph=True,\n                )\n                self._lookups[index]._register_fused_optim(\n                    optimizer_type, **optimizer_kwargs\n                )",
  "def sharded_parameter_names(self, prefix: str = \"\") -> Iterator[str]:\n        # different than ShardedEmbeddingCollection - we consider DDP to be \"sharded\", so that it doesn't get wrapped up in ddp again\n        # semantics of this is actually \"parameters that don't need to have their gradients reduced\"\n        for lookup, _ in zip(self._lookups, self._sharding_type_to_sharding.keys()):\n            for name, _ in lookup.named_parameters(append_prefix(prefix, \"embeddings\")):\n                yield name",
  "def shard(\n        self,\n        module: FusedEmbeddingCollection,\n        params: Dict[str, ParameterSharding],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> ShardedFusedEmbeddingCollection:\n\n        return ShardedFusedEmbeddingCollection(module, params, env, device)",
  "def shardable_parameters(\n        self, module: FusedEmbeddingCollection\n    ) -> Dict[str, nn.Parameter]:\n\n        params = {\n            # state_dict value looks like model.embeddings.table_0.weights\n            name.split(\".\")[-2]: param\n            for name, param in module.state_dict().items()\n            if name.endswith(\".weight\")\n        }\n        return params",
  "def module_type(self) -> Type[FusedEmbeddingCollection]:\n        return FusedEmbeddingCollection",
  "def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.DATA_PARALLEL.value,\n            ShardingType.TABLE_WISE.value,\n            ShardingType.COLUMN_WISE.value,\n            ShardingType.ROW_WISE.value,\n        ]\n\n        return types",
  "def compute_kernels(\n        self, sharding_type: str, compute_device_type: str\n    ) -> List[str]:\n        ret = []\n        if sharding_type != ShardingType.DATA_PARALLEL.value:\n            ret += [\n                EmbeddingComputeKernel.FUSED.value,\n            ]\n            if compute_device_type in {\"cuda\"}:\n                ret += [\n                    EmbeddingComputeKernel.FUSED_UVM.value,\n                    EmbeddingComputeKernel.FUSED_UVM_CACHING.value,\n                ]\n        else:\n            ret.append(EmbeddingComputeKernel.DENSE.value)\n        return ret",
  "def _join_module_path(path: str, name: str) -> str:\n    return (path + \".\" + name) if path else name",
  "def shard(\n    module: nn.Module,\n    plan: Union[\n        ModuleShardingPlan,\n        Dict[str, ParameterShardingGenerator],\n        ParameterShardingGenerator,\n    ],\n    env: Optional[ShardingEnv] = None,\n    device: Optional[torch.device] = None,\n    sharder: Optional[ModuleSharder[nn.Module]] = None,\n) -> nn.Module:\n    \"\"\"\n    Replaces this module with its sharded variant\n\n    This will leave the other parts of the model unaffected.\n\n    It returns the original module\n\n    Args:\n        module (nn.Module): module to wrap.\n        env (Optional[ShardingEnv]): sharding environment that has the process group.\n        device (Optional[torch.device]): compute device, defaults to cpu.\n        plan (Union[ModuleShardingPlan, Dict[str, ParameterShardingGenerator], ParameterShardingGenerator]):\n            dict of ParameterSharding (materized plan) or ParameterShardingGenerator (which will be run to produce ParameterSharding).\n            If single ParameterShardingGenerator is supplied, it will be applied to all module parameters.\n        sharder (Optional[List[ModuleSharder[nn.Module]]]): sharder to use, default is picked from `get_default_sharders()`.\n\n    Example:\n\n        ebc = EmbeddingBagCollection()\n        sharded_ebc = shard(ebc, table_row_wise(host_index=0))\n        assert isinstance(sharded_ebc, ShardedEmbeddingBagCollection)\n    \"\"\"\n    torch._C._log_api_usage_once(\"torchrec.distributed.shard\")\n    return _shard(module, plan, env, device, sharder)",
  "def _shard(\n    module: nn.Module,\n    plan: Union[\n        ModuleShardingPlan,\n        Dict[str, ParameterShardingGenerator],\n        ParameterShardingGenerator,\n    ],\n    env: Optional[ShardingEnv] = None,\n    device: Optional[torch.device] = None,\n    sharder: Optional[ModuleSharder[nn.Module]] = None,\n) -> nn.Module:\n    \"\"\"\n    See shard\n    \"\"\"\n    if sharder is None:\n        sharder = get_module_to_default_sharders().get(type(module), None)\n    assert (\n        sharder is not None\n    ), f\"Could not find a valid sharder type for {type(module)}\"\n\n    if env is None:\n        pg = dist.GroupMember.WORLD\n        assert pg is not None, \"Process group is not initialized\"\n        env = ShardingEnv.from_process_group(pg)\n\n    if device is None:\n        if torch.cuda.is_available():\n            device = torch.device(torch.cuda.current_device())\n        else:\n            device = torch.device(\"cpu\")\n\n    if isinstance(plan, ModuleShardingPlan):\n        return sharder.shard(module, plan, env, device)\n\n    # Run sharding generators.\n    shardable_parameters = sharder.shardable_parameters(module)\n    if isinstance(plan, Callable):\n        gen = plan\n        plan = {}\n        for table_name, param in shardable_parameters.items():\n            plan[table_name] = gen(\n                param,\n                get_local_size(env.world_size),\n                env.world_size,\n                device.type,\n                sharder,\n            )\n    else:\n        for table_name, sharding in plan.items():\n            if isinstance(sharding, Callable):\n                param = shardable_parameters[table_name]\n                # pyre-fixme[6]: For 2nd argument expected `(Parameter, int, int,\n                #  str, ModuleSharder[Module]) -> ParameterSharding` but got\n                #  `ParameterSharding`.\n                plan[table_name] = sharding(\n                    param,\n                    get_local_size(env.world_size),\n                    env.world_size,\n                    device.type,\n                    sharder,\n                )\n\n    return sharder.shard(module, plan, env, device)",
  "def shard_modules(\n    module: nn.Module,\n    env: Optional[ShardingEnv] = None,\n    device: Optional[torch.device] = None,\n    plan: Optional[ShardingPlan] = None,\n    sharders: Optional[List[ModuleSharder[torch.nn.Module]]] = None,\n    init_params: bool = False,\n) -> nn.Module:\n    \"\"\"\n    Replaces all sub_modules that are embedding modules with their sharded variants. This embedding_module -> sharded_embedding_module mapping\n    is derived from the passed in sharders.\n\n    This will leave the other parts of the model unaffected.\n\n    It returns the original module\n\n    Args:\n        module (nn.Module): module to wrap.\n        env (Optional[ShardingEnv]): sharding environment that has the process group.\n        device (Optional[torch.device]): compute device, defaults to cpu.\n        plan (Optional[ShardingPlan]): plan to use when sharding, defaults to\n            `EmbeddingShardingPlanner.collective_plan()`.\n        sharders (Optional[List[ModuleSharder[nn.Module]]]): `ModuleSharders` available\n            to shard with, defaults to `get_default_sharders()`.\n        init_params: (Optional[bool]): If ``True``, will materialize parameters and\n            buffers that are on meta device, and will move module to ``device``. Note that\n            this only applies if `device.type != \"meta\"``. Default: `False`.\n\n\n    Example::\n\n        @torch.no_grad()\n        def init_weights(m):\n            if isinstance(m, nn.Linear):\n                m.weight.fill_(1.0)\n            elif isinstance(m, EmbeddingBagCollection):\n                for param in m.parameters():\n                    init.kaiming_normal_(param)\n\n        m = MyModel(device='meta')\n        m = shard(m)\n        assert isinstance(m.embedding_bag_collection, ShardedEmbeddingBagCollection)\n    \"\"\"\n\n    torch._C._log_api_usage_once(\"torchrec.distributed.shard_modules\")\n    return _shard_modules(module, env, device, plan, sharders, init_params)",
  "def _shard_modules(  # noqa: C901\n    module: nn.Module,\n    env: Optional[ShardingEnv] = None,\n    device: Optional[torch.device] = None,\n    plan: Optional[ShardingPlan] = None,\n    sharders: Optional[List[ModuleSharder[torch.nn.Module]]] = None,\n    init_params: Optional[bool] = False,\n) -> nn.Module:\n    \"\"\"\n    See shard_modules\n    \"\"\"\n\n    torch._C._log_api_usage_once(\"torchrec.distributed.shard_modules\")\n\n    if sharders is None:\n        sharders = get_default_sharders()\n\n    if env is None:\n        pg = dist.GroupMember.WORLD\n        assert pg is not None, \"Process group is not initialized\"\n        env = ShardingEnv.from_process_group(pg)\n\n    if device is None:\n        if torch.cuda.is_available():\n            device = torch.device(torch.cuda.current_device())\n        else:\n            device = torch.device(\"cpu\")\n\n    sharder_map: Dict[Type[nn.Module], ModuleSharder[nn.Module]] = {\n        sharder.module_type: sharder for sharder in sharders\n    }\n\n    if plan is None:\n        planner = EmbeddingShardingPlanner(\n            topology=Topology(\n                local_world_size=get_local_size(env.world_size),\n                world_size=env.world_size,\n                compute_device=device.type,\n            )\n        )\n        pg = env.process_group\n        if pg is not None:\n            plan = planner.collective_plan(module, sharders, pg)\n        else:\n            plan = planner.plan(module, sharders)\n\n    if type(module) in sharder_map:\n        # If the top level module is itself a shardable module, return the sharded variant.\n        # Note, we cannot do an inplace replacement in this case.\n        return sharder_map[type(module)].shard(\n            module, plan.get_plan_for_module(\"\"), env, device\n        )\n\n    def _replace(_model: nn.Module, path: str = \"\") -> None:\n        for child_name, child in _model.named_children():\n            child_path = _join_module_path(path, child_name)\n            if type(child) in sharder_map:\n                assert plan is not None\n                sharded_params = plan.get_plan_for_module(child_path)\n                if sharded_params is not None:\n                    sharded_module = sharder_map[type(child)].shard(\n                        child, sharded_params, env, device\n                    )\n                    _model.register_module(\n                        child_name,\n                        sharded_module,\n                    )\n            else:\n                _replace(child, child_path)\n\n    _replace(module)\n    if init_params and device is not None and device.type != \"meta\":\n        init_parameters(module, device)\n        module = module.to(device)\n\n    return module",
  "def _replace(_model: nn.Module, path: str = \"\") -> None:\n        for child_name, child in _model.named_children():\n            child_path = _join_module_path(path, child_name)\n            if type(child) in sharder_map:\n                assert plan is not None\n                sharded_params = plan.get_plan_for_module(child_path)\n                if sharded_params is not None:\n                    sharded_module = sharder_map[type(child)].shard(\n                        child, sharded_params, env, device\n                    )\n                    _model.register_module(\n                        child_name,\n                        sharded_module,\n                    )\n            else:\n                _replace(child, child_path)",
  "class CommType(Enum):\n    FP32 = \"fp32\"\n    FP16 = \"fp16\"\n    BF16 = \"bf16\"\n    FP8 = \"fp8\"\n    INT8 = \"int8\"\n\n    def __str__(self) -> str:\n        return self.value",
  "def comm_type_to_sparse_type(comm_type: CommType) -> SparseType:\n    return {\n        CommType.FP32: SparseType.FP32,\n        CommType.FP16: SparseType.FP16,\n        CommType.BF16: SparseType.BF16,\n        CommType.FP8: SparseType.FP8,\n        CommType.INT8: SparseType.INT8,\n    }[comm_type]",
  "class QCommsConfig:\n    \"\"\"\n    Quantization configs for the AllToAll and ReduceScatter communication modules used in sharding.\n    \"\"\"\n\n    # Quantization of comm modules in the forward pass\n    forward_precision: CommType = CommType.FP32\n    # Quantization of comm modules in the backward pass\n    backward_precision: CommType = CommType.FP32\n    # For supported precisions (currently FP16), scale the gradient of the decoder and\n    # divide the gradient of the encoder by this value. In some cases this can provide additional numerical stability.\n    forward_loss_scale: Optional[float] = None\n    backward_loss_scale: Optional[float] = None\n    fp8_quantize_dim: Optional[int] = None\n    fp8_quantize_dim_bwd: Optional[int] = None\n    fp8_bwd_uses_143: Optional[bool] = False\n\n    def __post_init__(self) -> None:\n        if (\n            self.forward_precision != CommType.FP8\n            and self.backward_precision != CommType.FP8\n            and (\n                self.fp8_quantize_dim is not None\n                or self.fp8_quantize_dim_bwd is not None\n            )\n        ):\n            raise ValueError(\n                f\"fp8_quantize_dim is set to {self.fp8_quantize_dim} and fp8_quantize_dim_bwd is set to {self.fp8_quantize_dim_bwd} but no FP8 precision is found in forward or backward precisions\"\n            )\n        if (\n            self.backward_precision == CommType.FP8\n            and self.fp8_quantize_dim_bwd is None\n        ):\n            self.fp8_quantize_dim_bwd = self.fp8_quantize_dim\n            logger.warning(\n                f\"No override of FP8 bwd row dim, using general FP8 row dim for backward: {self.fp8_quantize_dim_bwd} \"\n            )",
  "def get_qcomm_codecs(qcomms_config: Optional[QCommsConfig]) -> QuantizedCommCodecs:\n    codecs = QuantizedCommCodecs()\n    if qcomms_config is not None:\n        codecs.forward = cast(\n            QuantizedCommCodec[QuantizationContext],\n            FbgemmQuantizedCommCodec(\n                comm_precision=comm_type_to_sparse_type(\n                    qcomms_config.forward_precision\n                ),\n                loss_scale=qcomms_config.forward_loss_scale,\n                is_fwd=True,\n                row_dim=qcomms_config.fp8_quantize_dim\n                if qcomms_config.forward_precision == CommType.FP8\n                else None,\n            ),\n        )\n        codecs.backward = cast(\n            QuantizedCommCodec[QuantizationContext],\n            FbgemmQuantizedCommCodec(\n                comm_precision=comm_type_to_sparse_type(\n                    qcomms_config.backward_precision\n                ),\n                loss_scale=qcomms_config.backward_loss_scale,\n                is_fwd=True\n                if qcomms_config.fp8_bwd_uses_143\n                else False,  # if fp8_bwd_uses_143 is True, bwd will use 1-4-3\n                # if fp8_bwd_uses_143 is False/None, bwd will use 1-5-2\n                row_dim=qcomms_config.fp8_quantize_dim_bwd\n                if qcomms_config.backward_precision == CommType.FP8\n                else None,\n            ),\n        )\n    return codecs",
  "def get_qcomm_codecs_registry(\n    qcomms_config: QCommsConfig,\n    comm_ops: Optional[List[CommOp]] = None,\n    device: Optional[torch.device] = None,\n) -> Optional[Dict[str, QuantizedCommCodecs]]:\n    \"\"\"\n     This method constructs QuantizedCommCodecs from a given QCommConfig. It assumes\n     that you want to use the same QComm configs for all comm-types passed in.\n\n     Some quantization schemes are not supported for some backends (such as BF16 for gloo/cpu, and FP8 for reduce scatter on nccl).\n     This scheme will provide some fallback logic and print a warning.\n\n    Args:\n        qcomms_config (QCommsConfig): QCommsConfig to construct FBGEMMQuantizedCommCodecs from\n        comm_ops (Optional[List[CommOp]]): List of CommOps to enter into the registry\n        device (torch.device): Backend comms will run on.\n\n    Example::\n        qcomm_codces_registry = get_qcomm_codecs_registry(\n            qcomms_config=QCommsConfig(forward_precision=FP16, backward_precision=BF16),\n            device=torch.device(\"cuda\"))\n    \"\"\"\n\n    if (\n        qcomms_config.forward_precision == CommType.FP32\n        and qcomms_config.backward_precision == CommType.FP32\n    ):\n        return None\n\n    if device is None:\n        device = torch.device(\"cuda\")\n\n    qcomm_codecs_registry = {}\n    if comm_ops is None:\n        comm_ops = [\n            CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL,\n            CommOp.POOLED_EMBEDDINGS_REDUCE_SCATTER,\n            CommOp.SEQUENCE_EMBEDDINGS_ALL_TO_ALL,\n        ]\n    for comm_op in comm_ops:\n        qcomm_config_copy = copy.deepcopy(qcomms_config)\n        # TODO: On H100, FP8 types might be natively supported, in which case we should check for that arch type and not fallback.\n        if comm_op == CommOp.POOLED_EMBEDDINGS_REDUCE_SCATTER:\n            if qcomm_config_copy.forward_precision == CommType.FP8:\n                logger.warning(\n                    \"FP8 is not supported for reduce scatter's forward - falling back to FP16\"\n                )\n                qcomm_config_copy.forward_precision = CommType.FP16\n            if qcomm_config_copy.backward_precision == CommType.FP8:\n                logger.warning(\n                    \"FP8 is not supported for reduce scatter's backward - falling back to BF16\"\n                )\n                qcomm_config_copy.backward_precision = CommType.BF16\n\n        if device.type == \"cpu\":\n            if qcomm_config_copy.forward_precision == CommType.BF16:\n                logger.warning(\n                    \"BF16 is not for forward_precision is not supported on GLOO - falling back to FP16.\"\n                )\n                qcomm_config_copy.forward_precision = CommType.FP16\n\n            if qcomm_config_copy.backward_precision == CommType.BF16:\n                logger.warning(\n                    \"BF16 is not for backward_precision is not supported on GLOO - falling back to FP16.\"\n                )\n                qcomm_config_copy.backward_precision = CommType.FP16\n\n        qcomm_codecs_registry[comm_op.name] = get_qcomm_codecs(qcomm_config_copy)\n\n    return qcomm_codecs_registry",
  "def __str__(self) -> str:\n        return self.value",
  "def __post_init__(self) -> None:\n        if (\n            self.forward_precision != CommType.FP8\n            and self.backward_precision != CommType.FP8\n            and (\n                self.fp8_quantize_dim is not None\n                or self.fp8_quantize_dim_bwd is not None\n            )\n        ):\n            raise ValueError(\n                f\"fp8_quantize_dim is set to {self.fp8_quantize_dim} and fp8_quantize_dim_bwd is set to {self.fp8_quantize_dim_bwd} but no FP8 precision is found in forward or backward precisions\"\n            )\n        if (\n            self.backward_precision == CommType.FP8\n            and self.fp8_quantize_dim_bwd is None\n        ):\n            self.fp8_quantize_dim_bwd = self.fp8_quantize_dim\n            logger.warning(\n                f\"No override of FP8 bwd row dim, using general FP8 row dim for backward: {self.fp8_quantize_dim_bwd} \"\n            )",
  "def _env2int(env_list: List[str], default: int = -1) -> int:\n    for e in env_list:\n        val = int(os.environ.get(e, -1))\n        if val >= 0:\n            return val\n    return default",
  "def get_local_size(world_size: Optional[int] = None) -> int:\n    if world_size is None:\n        world_size = dist.get_world_size()\n    \"\"\"\n    Gets the local world size (see https://pytorch.org/docs/stable/elastic/run.html)\n    This is usually the size of workers on each node, or nproc_per_node\n    \"\"\"\n    local_size = _env2int(\n        [\n            \"LOCAL_WORLD_SIZE\",\n            \"MPI_LOCALNRANKS\",\n            \"OMPI_COMM_WORLD_LOCAL_SIZE\",\n            \"MV2_COMM_WORLD_LOCAL_SIZE\",\n        ],\n        8,\n    )\n\n    if local_size == -1 or world_size % local_size != 0:\n        logging.warning(\n            \"Could not determine LOCAL_WORLD_SIZE from environment, falling back to WORLD_SIZE.\"\n        )\n        local_size = world_size\n    return local_size",
  "def get_local_rank(world_size: Optional[int] = None, rank: Optional[int] = None) -> int:\n    \"\"\"\n    Gets the local rank of the local processes (see https://pytorch.org/docs/stable/elastic/run.html)\n    This is usually the rank of the worker on its node\n    \"\"\"\n    my_local_rank = _env2int(\n        [\n            \"LOCAL_RANK\",\n            \"MPI_LOCALRANKID\",\n            \"OMPI_COMM_WORLD_LOCAL_RANK\",\n            \"MV2_COMM_WORLD_LOCAL_RANK\",\n        ],\n        -1,\n    )\n    local_size = get_local_size(world_size)\n\n    if my_local_rank == -1 or my_local_rank >= local_size:\n        logging.warning(\n            \"Could not determine LOCAL_RANK from environment, falling back to GLOBAL_RANK % LOCAL_SIZE.\"\n        )\n        if rank is None:\n            rank = dist.get_rank()\n        my_local_rank = rank % local_size\n    return my_local_rank",
  "def get_group_rank(world_size: Optional[int] = None, rank: Optional[int] = None) -> int:\n    \"\"\"\n    Gets the group rank of the worker group. Also available with GROUP_RANK environment varible\n    A number between 0 and get_num_groups() (See https://pytorch.org/docs/stable/elastic/run.html)\n    \"\"\"\n    if rank is None:\n        rank = dist.get_rank()\n    return rank // get_local_size(world_size)",
  "def get_num_groups(world_size: Optional[int] = None) -> int:\n    \"\"\"\n    Gets the number of worker groups.\n    Usually equivalent to max_nnodes (See https://pytorch.org/docs/stable/elastic/run.html)\n    \"\"\"\n    if world_size is None:\n        world_size = dist.get_world_size()\n    return world_size // get_local_size(world_size)",
  "def intra_and_cross_node_pg(\n    device: Optional[torch.device] = None,\n    backend: Optional[str] = None,\n) -> Tuple[Optional[dist.ProcessGroup], Optional[dist.ProcessGroup]]:\n    \"\"\"\n    Creates sub process groups (intra and cross node)\n    \"\"\"\n    if device is not None and device.type == \"meta\":\n        return None, None\n\n    global _INTRA_PG  # intra node process group\n    global _CROSS_PG  # cross node process group\n\n    my_size = dist.get_world_size()\n    my_rank = dist.get_rank()\n    my_local_rank = get_local_rank(my_size, my_rank)\n    local_size = get_local_size(my_size)\n    my_group_rank = get_group_rank(my_size, my_rank)\n    group_count = get_num_groups(my_size)\n    if backend is None:\n        backend = dist.get_backend()\n\n    logger.info(\n        f\"[{my_rank}] my_local_rank = {my_local_rank}, local_size = {local_size},\"\n        f\"my_group_rank = {my_group_rank}, group_count = {group_count}, backend = {backend}\"\n    )\n    if _INTRA_PG is None:\n        for group_rank in range(group_count):\n            peers = [group_rank * local_size + r for r in range(local_size)]\n            curr_intra_group_pg = dist.new_group(backend=backend, ranks=peers)\n            if my_group_rank == group_rank:\n                logger.warning(\n                    \"[Connection] intra_group: [%d] -> [%s]\" % (my_rank, peers)\n                )\n                _INTRA_PG = curr_intra_group_pg\n\n    dist.barrier()\n\n    if _CROSS_PG is None:\n        for l_rank in range(local_size):\n            peers = [l_rank + g * local_size for g in range(group_count)]\n            curr_cross_group_pg = dist.new_group(backend=backend, ranks=peers)\n            if l_rank == my_local_rank:\n                logger.warning(\n                    \"[Connection] cross_group: [%d] -> [%s]\" % (my_rank, peers)\n                )\n                _CROSS_PG = curr_cross_group_pg\n\n    dist.barrier()\n\n    return _INTRA_PG, _CROSS_PG",
  "class DataParallelWrapper(abc.ABC):\n    \"\"\"\n    Interface implemented by custom data parallel wrappers.\n    \"\"\"\n\n    @abc.abstractmethod\n    def wrap(\n        self,\n        dmp: \"DistributedModelParallel\",\n        env: ShardingEnv,\n        device: torch.device,\n    ) -> None:\n        pass",
  "class DefaultDataParallelWrapper(DataParallelWrapper):\n    \"\"\"\n    Default data parallel wrapper, which applies data parallel to all unsharded modules.\n    \"\"\"\n\n    def __init__(\n        self,\n        bucket_cap_mb: int = 25,\n        static_graph: bool = True,\n        find_unused_parameters: bool = False,\n    ) -> None:\n        self._bucket_cap_mb: int = bucket_cap_mb\n        self._static_graph: bool = static_graph\n        self._find_unused_parameters: bool = find_unused_parameters\n\n    def wrap(\n        self,\n        dmp: \"DistributedModelParallel\",\n        env: ShardingEnv,\n        device: torch.device,\n    ) -> None:\n        if isinstance(dmp._dmp_wrapped_module, DistributedDataParallel) or isinstance(\n            dmp._dmp_wrapped_module, FullyShardedDataParallel\n        ):\n            return\n        pg = env.process_group\n        if pg is None:\n            raise RuntimeError(\"Can only init DDP for ProcessGroup-based ShardingEnv\")\n        sharded_parameter_names = set(\n            DistributedModelParallel._sharded_parameter_names(dmp._dmp_wrapped_module)\n        )\n        all_parameter_names = set(dict(dmp.named_parameters()).keys())\n        if len(all_parameter_names - sharded_parameter_names) == 0:\n            return\n        DistributedDataParallel._set_params_and_buffers_to_ignore_for_model(\n            module=dmp._dmp_wrapped_module,\n            params_and_buffers_to_ignore=sharded_parameter_names,\n        )\n        # initialize DDP\n        dmp._dmp_wrapped_module = cast(\n            nn.Module,\n            DistributedDataParallel(\n                module=dmp._dmp_wrapped_module.to(device),\n                device_ids=None if device.type == \"cpu\" else [device],\n                process_group=pg,\n                gradient_as_bucket_view=True,\n                broadcast_buffers=False,\n                static_graph=self._static_graph,\n                find_unused_parameters=self._find_unused_parameters,\n                bucket_cap_mb=self._bucket_cap_mb,\n            ),\n        )",
  "def get_unwrapped_module(module: nn.Module) -> nn.Module:\n    \"\"\"\n    Unwraps module wrapped by DMP, DDP, or FSDP.\n    \"\"\"\n    while (\n        isinstance(module, DistributedModelParallel)\n        or isinstance(module, DistributedDataParallel)\n        or isinstance(module, FullyShardedDataParallel)\n    ):\n        if isinstance(module, DistributedModelParallel):\n            module = module._dmp_wrapped_module\n        elif isinstance(module, FullyShardedDataParallel):\n            module = module._fsdp_wrapped_module\n        else:\n            module = module.module\n    return module",
  "def get_module(module: nn.Module) -> nn.Module:\n    \"\"\"\n    Unwraps DMP module.\n\n    Does not unwrap data parallel wrappers (i.e. DDP/FSDP), so overriding\n    implementations by the wrappers can be used.\n    \"\"\"\n    while isinstance(module, DistributedModelParallel):\n        module = module._dmp_wrapped_module\n    return module",
  "class DistributedModelParallel(nn.Module, FusedOptimizerModule):\n    \"\"\"\n    Entry point to model parallelism.\n\n    Args:\n        module (nn.Module): module to wrap.\n        env (Optional[ShardingEnv]): sharding environment that has the process group.\n        device (Optional[torch.device]): compute device, defaults to cpu.\n        plan (Optional[ShardingPlan]): plan to use when sharding, defaults to\n            `EmbeddingShardingPlanner.collective_plan()`.\n        sharders (Optional[List[ModuleSharder[nn.Module]]]): `ModuleSharders` available\n            to shard with, defaults to `EmbeddingBagCollectionSharder()`.\n        init_data_parallel (bool): data-parallel modules can be lazy, i.e. they delay\n            parameter initialization until the first forward pass. Pass `True` to delay\n            initialization of data parallel modules. Do first forward pass and then call\n            DistributedModelParallel.init_data_parallel().\n        init_parameters (bool): initialize parameters for modules still on meta device.\n        data_parallel_wrapper (Optional[DataParallelWrapper]): custom wrapper for data\n            parallel modules.\n\n    Example::\n\n        @torch.no_grad()\n        def init_weights(m):\n            if isinstance(m, nn.Linear):\n                m.weight.fill_(1.0)\n            elif isinstance(m, EmbeddingBagCollection):\n                for param in m.parameters():\n                    init.kaiming_normal_(param)\n\n        m = MyModel(device='meta')\n        m = DistributedModelParallel(m)\n        m.apply(init_weights)\n    \"\"\"\n\n    def __init__(\n        self,\n        module: nn.Module,\n        env: Optional[ShardingEnv] = None,\n        device: Optional[torch.device] = None,\n        plan: Optional[ShardingPlan] = None,\n        sharders: Optional[List[ModuleSharder[torch.nn.Module]]] = None,\n        init_data_parallel: bool = True,\n        init_parameters: bool = True,\n        data_parallel_wrapper: Optional[DataParallelWrapper] = None,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.distributed.{self.__class__.__name__}\")\n\n        self.init_parameters = init_parameters\n\n        self._ddp_wrapped: bool = False\n\n        if env is None:\n            pg = dist.GroupMember.WORLD\n            assert pg is not None, \"Process group is not initialized\"\n            env = ShardingEnv.from_process_group(pg)\n        self._env: ShardingEnv = env\n\n        if device is None:\n            device = torch.device(\"cpu\")\n        self.device: torch.device = device\n\n        if sharders is None:\n            sharders = get_default_sharders()\n\n        self._sharder_map: Dict[Type[nn.Module], ModuleSharder[nn.Module]] = {\n            sharder.module_type: sharder for sharder in sharders\n        }\n\n        if data_parallel_wrapper is None:\n            data_parallel_wrapper = DefaultDataParallelWrapper()\n        self._data_parallel_wrapper: DataParallelWrapper = data_parallel_wrapper\n\n        if plan is None:\n            planner = EmbeddingShardingPlanner(\n                topology=Topology(\n                    local_world_size=get_local_size(self._env.world_size),\n                    world_size=self._env.world_size,\n                    compute_device=self.device.type,\n                )\n            )\n            pg = self._env.process_group\n            if pg is not None:\n                plan = planner.collective_plan(module, sharders, pg)\n            else:\n                plan = planner.plan(module, sharders)\n        self._plan: ShardingPlan = plan\n        self._dmp_wrapped_module: nn.Module = self._init_dmp(module)\n        self._optim: CombinedOptimizer = self._init_optim(self._dmp_wrapped_module)\n\n        if init_parameters:\n            self._init_parameters(self.module)\n\n        if init_data_parallel:\n            self.init_data_parallel()\n\n    @property\n    def module(self) -> nn.Module:\n        \"\"\"\n        Property to directly access sharded module, which will not be wrapped in DDP,\n        FSDP, DMP, or any other parallelism wrappers.\n        \"\"\"\n        return get_unwrapped_module(self)\n\n    @module.setter\n    def module(self, value: nn.Module) -> None:\n        if isinstance(self.module, DistributedDataParallel) or isinstance(\n            self.module, FullyShardedDataParallel\n        ):\n            raise RuntimeError(\n                \"module can't be set after calling init_data_parallel(...)\"\n            )\n        else:\n            self._dmp_wrapped_module = value\n\n    # pyre-ignore [2, 3]\n    def forward(self, *args, **kwargs) -> Any:\n        return self._dmp_wrapped_module(*args, **kwargs)\n\n    def init_data_parallel(self) -> None:\n        \"\"\"\n        See init_data_parallel c-tor argument for usage.\n        It's safe to call this method multiple times.\n        \"\"\"\n        if not self._ddp_wrapped:\n            # Allocate any 'meta' tensors\n            if self.init_parameters:\n                self._init_parameters(self._dmp_wrapped_module)\n            self._data_parallel_wrapper.wrap(self, self._env, self.device)\n            self._ddp_wrapped = True\n\n    def copy(\n        self,\n        device: torch.device,\n    ) -> \"DistributedModelParallel\":\n        \"\"\"\n        Recursively copy submodules to new device by calling per-module customized copy\n        process, since some modules needs to use the original references (like\n        `ShardedModule` for inference).\n        \"\"\"\n        assert isinstance(device, torch.device)\n        # dmp code deep copy\n        with sharded_model_copy(device=None):\n            copy_dmp = copy.deepcopy(self)\n        # tensor resident module deep copy\n        copy_dmp_wrapped_module = copy_to_device(\n            self._dmp_wrapped_module, self.device, device\n        )\n        copy_dmp._dmp_wrapped_module = copy_dmp_wrapped_module\n        return copy_dmp\n\n    def _init_dmp(self, module: nn.Module) -> nn.Module:\n        return self._shard_modules_impl(module)\n\n    def _init_optim(self, module: nn.Module) -> CombinedOptimizer:\n        # pyre-ignore [6]\n        return CombinedOptimizer(self._fused_optim_impl(module, []))\n\n    def _fused_optim_impl(\n        self,\n        module: nn.Module,\n        fused_optims: List[Tuple[str, KeyedOptimizer]],\n        path: str = \"\",\n    ) -> List[Tuple[str, KeyedOptimizer]]:\n        if isinstance(module, FusedOptimizerModule):\n            fused_optims.append((path, module.fused_optimizer))\n            return fused_optims\n\n        for name, child in module.named_children():\n            self._fused_optim_impl(\n                child,\n                fused_optims,\n                path + \".\" + name if path else name,\n            )\n        return fused_optims\n\n    def _shard_modules_impl(\n        self,\n        module: nn.Module,\n        path: str = \"\",\n    ) -> nn.Module:\n\n        # pre-sharded module\n        if isinstance(module, ShardedModule):\n            return module\n\n        # shardable module\n        module_sharding_plan = self._plan.get_plan_for_module(path)\n        if module_sharding_plan:\n            sharder_key = type(module)\n            module = self._sharder_map[sharder_key].shard(\n                module,\n                module_sharding_plan,\n                self._env,\n                self.device,\n            )\n            return module\n\n        for name, child in module.named_children():\n            child = self._shard_modules_impl(\n                child,\n                path + \".\" + name if path else name,\n            )\n            setattr(module, name, child)\n\n        return module\n\n    def _init_parameters(self, module: nn.Module) -> None:\n        @torch.no_grad()\n        def init_parameters(module: nn.Module) -> None:\n            # Allocate parameters and buffers if over 'meta' device.\n            has_meta_param = False\n            for name, param in module._parameters.items():\n                if isinstance(param, torch.Tensor) and param.device.type == \"meta\":\n                    module._parameters[name] = nn.Parameter(\n                        torch.empty_like(param, device=self.device),\n                        requires_grad=param.requires_grad,\n                    )\n                    has_meta_param = True\n            for name, buffer in module._buffers.items():\n                if isinstance(buffer, torch.Tensor) and buffer.device.type == \"meta\":\n                    module._buffers[name] = torch.zeros_like(buffer, device=self.device)\n\n            # Init parameters if at least one parameter is over 'meta' device.\n            if has_meta_param and hasattr(module, \"reset_parameters\"):\n                module.reset_parameters()\n\n        module.apply(init_parameters)\n\n    def sparse_grad_parameter_names(\n        self, destination: Optional[List[str]] = None, prefix: str = \"\"\n    ) -> List[str]:\n        destination = [] if destination is None else destination\n        return self._sparse_grad_parameter_names(self.module, destination, prefix)\n\n    def _sparse_grad_parameter_names(\n        self, module: nn.Module, destination: List[str], prefix: str = \"\"\n    ) -> List[str]:\n        module = get_unwrapped_module(module)\n        if isinstance(module, ShardedModule):\n            pass\n        elif isinstance(module, nn.Embedding):\n            if module.sparse:\n                destination.append(append_prefix(prefix, \"weight\"))\n        elif isinstance(module, nn.EmbeddingBag):\n            if module.sparse:\n                destination.append(append_prefix(prefix, \"weight\"))\n        else:\n            for name, child in module.named_children():\n                self._sparse_grad_parameter_names(\n                    child, destination, append_prefix(prefix, name)\n                )\n        return destination\n\n    # pyre-ignore [14]\n    def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        state_dict = get_module(self).state_dict(\n            destination=destination, prefix=prefix, keep_vars=keep_vars\n        )\n        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n            state_dict, prefix + _DDP_STATE_DICT_PREFIX\n        )\n        add_prefix_to_state_dict(state_dict, prefix)\n        return state_dict\n\n    # pyre-fixme[14]: `load_state_dict` overrides method defined in `Module`\n    #  inconsistently.\n    def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, torch.Tensor]\",\n        prefix: str = \"\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        return self._load_state_dict(self, state_dict, prefix, strict)\n\n    def _load_state_dict(\n        self,\n        module: nn.Module,\n        state_dict: \"OrderedDict[str, torch.Tensor]\",\n        prefix: str = \"\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        missing_keys = []\n        unexpected_keys = []\n        module = get_module(module)\n        if isinstance(module, DistributedDataParallel):\n            torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n                state_dict, prefix\n            )\n            add_prefix_to_state_dict(state_dict, prefix + _DDP_STATE_DICT_PREFIX)\n        if isinstance(module, ShardedModule):\n            return module.load_state_dict(state_dict, strict=strict)\n        else:\n            module._load_from_state_dict(\n                state_dict, prefix, {}, strict, missing_keys, unexpected_keys, []\n            )\n            for name, child in module.named_children():\n                m_keys, u_keys = self._load_state_dict(\n                    child,\n                    filter_state_dict(state_dict, prefix + name),\n                    \"\",\n                    strict,\n                )\n                missing_keys.extend(m_keys)\n                unexpected_keys.extend(u_keys)\n        return _IncompatibleKeys(\n            missing_keys=missing_keys, unexpected_keys=unexpected_keys\n        )\n\n    def _named_parameters(\n        self,\n        module: nn.Module,\n        prefix: str = \"\",\n        recurse: bool = True,\n        strip_ddp: bool = True,\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        if strip_ddp:\n            module = get_unwrapped_module(module)\n        if isinstance(module, ShardedModule):\n            yield from module.named_parameters(prefix, recurse)\n        else:\n            yield from module.named_parameters(prefix, recurse=False)\n            for name, child in module.named_children():\n                yield from self._named_parameters(\n                    child,\n                    append_prefix(prefix, name),\n                    recurse,\n                    strip_ddp,\n                )\n\n    def named_parameters(\n        self,\n        prefix: str = \"\",\n        recurse: bool = True,\n        remove_duplicate: bool = True,\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        gen = self._named_parameters(\n            self.module,\n            prefix,\n            recurse,\n        )\n        memo = set()\n        for key, param in gen:\n            if param in memo:\n                continue\n            if remove_duplicate:\n                memo.add(param)\n            yield key, param\n\n    def bare_named_parameters(\n        self,\n        prefix: str = \"\",\n        recurse: bool = True,\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        gen = self._named_parameters(\n            self.module,\n            prefix,\n            recurse,\n        )\n        memo = set()\n        for key, param in gen:\n            if param in memo:\n                continue\n            memo.add(param)\n            yield key, param\n\n    @staticmethod\n    def _sharded_parameter_names(module: nn.Module, prefix: str = \"\") -> Iterator[str]:\n        module = get_unwrapped_module(module)\n        if isinstance(module, ShardedModule):\n            yield from module.sharded_parameter_names(prefix)\n        else:\n            for name, child in module.named_children():\n                yield from DistributedModelParallel._sharded_parameter_names(\n                    child, append_prefix(prefix, name)\n                )\n\n    def _named_buffers(\n        self, module: nn.Module, prefix: str = \"\", recurse: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        module = get_unwrapped_module(module)\n        if isinstance(module, ShardedModule):\n            yield from module.named_buffers(prefix, recurse)\n        else:\n            yield from module.named_buffers(prefix, recurse=False)\n            for name, child in module.named_children():\n                yield from self._named_buffers(\n                    child, append_prefix(prefix, name), recurse\n                )\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        gen = self._named_buffers(self.module, prefix, recurse)\n        memo = set()\n        for key, param in gen:\n            if param in memo:\n                continue\n            if remove_duplicate:\n                memo.add(param)\n            yield key, param\n\n    @property\n    def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim\n\n    @property\n    def plan(self) -> ShardingPlan:\n        return self._plan\n\n    @staticmethod\n    def _reset_parameters(module: nn.Module) -> None:\n        for _, m in module.named_modules():\n            if hasattr(m, \"reset_parameters\"):\n                m.reset_parameters()",
  "def wrap(\n        self,\n        dmp: \"DistributedModelParallel\",\n        env: ShardingEnv,\n        device: torch.device,\n    ) -> None:\n        pass",
  "def __init__(\n        self,\n        bucket_cap_mb: int = 25,\n        static_graph: bool = True,\n        find_unused_parameters: bool = False,\n    ) -> None:\n        self._bucket_cap_mb: int = bucket_cap_mb\n        self._static_graph: bool = static_graph\n        self._find_unused_parameters: bool = find_unused_parameters",
  "def wrap(\n        self,\n        dmp: \"DistributedModelParallel\",\n        env: ShardingEnv,\n        device: torch.device,\n    ) -> None:\n        if isinstance(dmp._dmp_wrapped_module, DistributedDataParallel) or isinstance(\n            dmp._dmp_wrapped_module, FullyShardedDataParallel\n        ):\n            return\n        pg = env.process_group\n        if pg is None:\n            raise RuntimeError(\"Can only init DDP for ProcessGroup-based ShardingEnv\")\n        sharded_parameter_names = set(\n            DistributedModelParallel._sharded_parameter_names(dmp._dmp_wrapped_module)\n        )\n        all_parameter_names = set(dict(dmp.named_parameters()).keys())\n        if len(all_parameter_names - sharded_parameter_names) == 0:\n            return\n        DistributedDataParallel._set_params_and_buffers_to_ignore_for_model(\n            module=dmp._dmp_wrapped_module,\n            params_and_buffers_to_ignore=sharded_parameter_names,\n        )\n        # initialize DDP\n        dmp._dmp_wrapped_module = cast(\n            nn.Module,\n            DistributedDataParallel(\n                module=dmp._dmp_wrapped_module.to(device),\n                device_ids=None if device.type == \"cpu\" else [device],\n                process_group=pg,\n                gradient_as_bucket_view=True,\n                broadcast_buffers=False,\n                static_graph=self._static_graph,\n                find_unused_parameters=self._find_unused_parameters,\n                bucket_cap_mb=self._bucket_cap_mb,\n            ),\n        )",
  "def __init__(\n        self,\n        module: nn.Module,\n        env: Optional[ShardingEnv] = None,\n        device: Optional[torch.device] = None,\n        plan: Optional[ShardingPlan] = None,\n        sharders: Optional[List[ModuleSharder[torch.nn.Module]]] = None,\n        init_data_parallel: bool = True,\n        init_parameters: bool = True,\n        data_parallel_wrapper: Optional[DataParallelWrapper] = None,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(f\"torchrec.distributed.{self.__class__.__name__}\")\n\n        self.init_parameters = init_parameters\n\n        self._ddp_wrapped: bool = False\n\n        if env is None:\n            pg = dist.GroupMember.WORLD\n            assert pg is not None, \"Process group is not initialized\"\n            env = ShardingEnv.from_process_group(pg)\n        self._env: ShardingEnv = env\n\n        if device is None:\n            device = torch.device(\"cpu\")\n        self.device: torch.device = device\n\n        if sharders is None:\n            sharders = get_default_sharders()\n\n        self._sharder_map: Dict[Type[nn.Module], ModuleSharder[nn.Module]] = {\n            sharder.module_type: sharder for sharder in sharders\n        }\n\n        if data_parallel_wrapper is None:\n            data_parallel_wrapper = DefaultDataParallelWrapper()\n        self._data_parallel_wrapper: DataParallelWrapper = data_parallel_wrapper\n\n        if plan is None:\n            planner = EmbeddingShardingPlanner(\n                topology=Topology(\n                    local_world_size=get_local_size(self._env.world_size),\n                    world_size=self._env.world_size,\n                    compute_device=self.device.type,\n                )\n            )\n            pg = self._env.process_group\n            if pg is not None:\n                plan = planner.collective_plan(module, sharders, pg)\n            else:\n                plan = planner.plan(module, sharders)\n        self._plan: ShardingPlan = plan\n        self._dmp_wrapped_module: nn.Module = self._init_dmp(module)\n        self._optim: CombinedOptimizer = self._init_optim(self._dmp_wrapped_module)\n\n        if init_parameters:\n            self._init_parameters(self.module)\n\n        if init_data_parallel:\n            self.init_data_parallel()",
  "def module(self) -> nn.Module:\n        \"\"\"\n        Property to directly access sharded module, which will not be wrapped in DDP,\n        FSDP, DMP, or any other parallelism wrappers.\n        \"\"\"\n        return get_unwrapped_module(self)",
  "def module(self, value: nn.Module) -> None:\n        if isinstance(self.module, DistributedDataParallel) or isinstance(\n            self.module, FullyShardedDataParallel\n        ):\n            raise RuntimeError(\n                \"module can't be set after calling init_data_parallel(...)\"\n            )\n        else:\n            self._dmp_wrapped_module = value",
  "def forward(self, *args, **kwargs) -> Any:\n        return self._dmp_wrapped_module(*args, **kwargs)",
  "def init_data_parallel(self) -> None:\n        \"\"\"\n        See init_data_parallel c-tor argument for usage.\n        It's safe to call this method multiple times.\n        \"\"\"\n        if not self._ddp_wrapped:\n            # Allocate any 'meta' tensors\n            if self.init_parameters:\n                self._init_parameters(self._dmp_wrapped_module)\n            self._data_parallel_wrapper.wrap(self, self._env, self.device)\n            self._ddp_wrapped = True",
  "def copy(\n        self,\n        device: torch.device,\n    ) -> \"DistributedModelParallel\":\n        \"\"\"\n        Recursively copy submodules to new device by calling per-module customized copy\n        process, since some modules needs to use the original references (like\n        `ShardedModule` for inference).\n        \"\"\"\n        assert isinstance(device, torch.device)\n        # dmp code deep copy\n        with sharded_model_copy(device=None):\n            copy_dmp = copy.deepcopy(self)\n        # tensor resident module deep copy\n        copy_dmp_wrapped_module = copy_to_device(\n            self._dmp_wrapped_module, self.device, device\n        )\n        copy_dmp._dmp_wrapped_module = copy_dmp_wrapped_module\n        return copy_dmp",
  "def _init_dmp(self, module: nn.Module) -> nn.Module:\n        return self._shard_modules_impl(module)",
  "def _init_optim(self, module: nn.Module) -> CombinedOptimizer:\n        # pyre-ignore [6]\n        return CombinedOptimizer(self._fused_optim_impl(module, []))",
  "def _fused_optim_impl(\n        self,\n        module: nn.Module,\n        fused_optims: List[Tuple[str, KeyedOptimizer]],\n        path: str = \"\",\n    ) -> List[Tuple[str, KeyedOptimizer]]:\n        if isinstance(module, FusedOptimizerModule):\n            fused_optims.append((path, module.fused_optimizer))\n            return fused_optims\n\n        for name, child in module.named_children():\n            self._fused_optim_impl(\n                child,\n                fused_optims,\n                path + \".\" + name if path else name,\n            )\n        return fused_optims",
  "def _shard_modules_impl(\n        self,\n        module: nn.Module,\n        path: str = \"\",\n    ) -> nn.Module:\n\n        # pre-sharded module\n        if isinstance(module, ShardedModule):\n            return module\n\n        # shardable module\n        module_sharding_plan = self._plan.get_plan_for_module(path)\n        if module_sharding_plan:\n            sharder_key = type(module)\n            module = self._sharder_map[sharder_key].shard(\n                module,\n                module_sharding_plan,\n                self._env,\n                self.device,\n            )\n            return module\n\n        for name, child in module.named_children():\n            child = self._shard_modules_impl(\n                child,\n                path + \".\" + name if path else name,\n            )\n            setattr(module, name, child)\n\n        return module",
  "def _init_parameters(self, module: nn.Module) -> None:\n        @torch.no_grad()\n        def init_parameters(module: nn.Module) -> None:\n            # Allocate parameters and buffers if over 'meta' device.\n            has_meta_param = False\n            for name, param in module._parameters.items():\n                if isinstance(param, torch.Tensor) and param.device.type == \"meta\":\n                    module._parameters[name] = nn.Parameter(\n                        torch.empty_like(param, device=self.device),\n                        requires_grad=param.requires_grad,\n                    )\n                    has_meta_param = True\n            for name, buffer in module._buffers.items():\n                if isinstance(buffer, torch.Tensor) and buffer.device.type == \"meta\":\n                    module._buffers[name] = torch.zeros_like(buffer, device=self.device)\n\n            # Init parameters if at least one parameter is over 'meta' device.\n            if has_meta_param and hasattr(module, \"reset_parameters\"):\n                module.reset_parameters()\n\n        module.apply(init_parameters)",
  "def sparse_grad_parameter_names(\n        self, destination: Optional[List[str]] = None, prefix: str = \"\"\n    ) -> List[str]:\n        destination = [] if destination is None else destination\n        return self._sparse_grad_parameter_names(self.module, destination, prefix)",
  "def _sparse_grad_parameter_names(\n        self, module: nn.Module, destination: List[str], prefix: str = \"\"\n    ) -> List[str]:\n        module = get_unwrapped_module(module)\n        if isinstance(module, ShardedModule):\n            pass\n        elif isinstance(module, nn.Embedding):\n            if module.sparse:\n                destination.append(append_prefix(prefix, \"weight\"))\n        elif isinstance(module, nn.EmbeddingBag):\n            if module.sparse:\n                destination.append(append_prefix(prefix, \"weight\"))\n        else:\n            for name, child in module.named_children():\n                self._sparse_grad_parameter_names(\n                    child, destination, append_prefix(prefix, name)\n                )\n        return destination",
  "def state_dict(\n        self,\n        destination: Optional[Dict[str, Any]] = None,\n        prefix: str = \"\",\n        keep_vars: bool = False,\n    ) -> Dict[str, Any]:\n        state_dict = get_module(self).state_dict(\n            destination=destination, prefix=prefix, keep_vars=keep_vars\n        )\n        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n            state_dict, prefix + _DDP_STATE_DICT_PREFIX\n        )\n        add_prefix_to_state_dict(state_dict, prefix)\n        return state_dict",
  "def load_state_dict(\n        self,\n        state_dict: \"OrderedDict[str, torch.Tensor]\",\n        prefix: str = \"\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        return self._load_state_dict(self, state_dict, prefix, strict)",
  "def _load_state_dict(\n        self,\n        module: nn.Module,\n        state_dict: \"OrderedDict[str, torch.Tensor]\",\n        prefix: str = \"\",\n        strict: bool = True,\n    ) -> _IncompatibleKeys:\n        missing_keys = []\n        unexpected_keys = []\n        module = get_module(module)\n        if isinstance(module, DistributedDataParallel):\n            torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n                state_dict, prefix\n            )\n            add_prefix_to_state_dict(state_dict, prefix + _DDP_STATE_DICT_PREFIX)\n        if isinstance(module, ShardedModule):\n            return module.load_state_dict(state_dict, strict=strict)\n        else:\n            module._load_from_state_dict(\n                state_dict, prefix, {}, strict, missing_keys, unexpected_keys, []\n            )\n            for name, child in module.named_children():\n                m_keys, u_keys = self._load_state_dict(\n                    child,\n                    filter_state_dict(state_dict, prefix + name),\n                    \"\",\n                    strict,\n                )\n                missing_keys.extend(m_keys)\n                unexpected_keys.extend(u_keys)\n        return _IncompatibleKeys(\n            missing_keys=missing_keys, unexpected_keys=unexpected_keys\n        )",
  "def _named_parameters(\n        self,\n        module: nn.Module,\n        prefix: str = \"\",\n        recurse: bool = True,\n        strip_ddp: bool = True,\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        if strip_ddp:\n            module = get_unwrapped_module(module)\n        if isinstance(module, ShardedModule):\n            yield from module.named_parameters(prefix, recurse)\n        else:\n            yield from module.named_parameters(prefix, recurse=False)\n            for name, child in module.named_children():\n                yield from self._named_parameters(\n                    child,\n                    append_prefix(prefix, name),\n                    recurse,\n                    strip_ddp,\n                )",
  "def named_parameters(\n        self,\n        prefix: str = \"\",\n        recurse: bool = True,\n        remove_duplicate: bool = True,\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        gen = self._named_parameters(\n            self.module,\n            prefix,\n            recurse,\n        )\n        memo = set()\n        for key, param in gen:\n            if param in memo:\n                continue\n            if remove_duplicate:\n                memo.add(param)\n            yield key, param",
  "def bare_named_parameters(\n        self,\n        prefix: str = \"\",\n        recurse: bool = True,\n    ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n        gen = self._named_parameters(\n            self.module,\n            prefix,\n            recurse,\n        )\n        memo = set()\n        for key, param in gen:\n            if param in memo:\n                continue\n            memo.add(param)\n            yield key, param",
  "def _sharded_parameter_names(module: nn.Module, prefix: str = \"\") -> Iterator[str]:\n        module = get_unwrapped_module(module)\n        if isinstance(module, ShardedModule):\n            yield from module.sharded_parameter_names(prefix)\n        else:\n            for name, child in module.named_children():\n                yield from DistributedModelParallel._sharded_parameter_names(\n                    child, append_prefix(prefix, name)\n                )",
  "def _named_buffers(\n        self, module: nn.Module, prefix: str = \"\", recurse: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        module = get_unwrapped_module(module)\n        if isinstance(module, ShardedModule):\n            yield from module.named_buffers(prefix, recurse)\n        else:\n            yield from module.named_buffers(prefix, recurse=False)\n            for name, child in module.named_children():\n                yield from self._named_buffers(\n                    child, append_prefix(prefix, name), recurse\n                )",
  "def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, torch.Tensor]]:\n        gen = self._named_buffers(self.module, prefix, recurse)\n        memo = set()\n        for key, param in gen:\n            if param in memo:\n                continue\n            if remove_duplicate:\n                memo.add(param)\n            yield key, param",
  "def fused_optimizer(self) -> KeyedOptimizer:\n        return self._optim",
  "def plan(self) -> ShardingPlan:\n        return self._plan",
  "def _reset_parameters(module: nn.Module) -> None:\n        for _, m in module.named_modules():\n            if hasattr(m, \"reset_parameters\"):\n                m.reset_parameters()",
  "def init_parameters(module: nn.Module) -> None:\n            # Allocate parameters and buffers if over 'meta' device.\n            has_meta_param = False\n            for name, param in module._parameters.items():\n                if isinstance(param, torch.Tensor) and param.device.type == \"meta\":\n                    module._parameters[name] = nn.Parameter(\n                        torch.empty_like(param, device=self.device),\n                        requires_grad=param.requires_grad,\n                    )\n                    has_meta_param = True\n            for name, buffer in module._buffers.items():\n                if isinstance(buffer, torch.Tensor) and buffer.device.type == \"meta\":\n                    module._buffers[name] = torch.zeros_like(buffer, device=self.device)\n\n            # Init parameters if at least one parameter is over 'meta' device.\n            if has_meta_param and hasattr(module, \"reset_parameters\"):\n                module.reset_parameters()",
  "class OptimType(Enum):\n    SGD = \"SGD\"\n    LARS_SGD = \"LARS_SGD\"\n    LAMB = \"LAMB\"\n    PARTIAL_ROWWISE_LAMB = \"PARTIAL_ROWWISE_LAMB\"\n    ADAM = \"ADAM\"\n    PARTIAL_ROWWISE_ADAM = \"PARTIAL_ROWWISE_ADAM\"\n    ADAGRAD = \"ADAGRAD\"\n    ROWWISE_ADAGRAD = \"ROWWISE_ADAGRAD\"\n    SHAMPOO = \"SHAMPOO\"\n    LION = \"LION\"",
  "class EmbeddingComputeKernel(Enum):\n    DENSE = \"dense\"\n    FUSED = \"fused\"\n    FUSED_UVM = \"fused_uvm\"\n    FUSED_UVM_CACHING = \"fused_uvm_caching\"\n    QUANT = \"quant\"\n    QUANT_UVM = \"quant_uvm\"\n    QUANT_UVM_CACHING = \"quant_uvm_caching\"",
  "def compute_kernel_to_embedding_location(\n    compute_kernel: EmbeddingComputeKernel,\n) -> EmbeddingLocation:\n    if compute_kernel in [\n        EmbeddingComputeKernel.DENSE,\n        EmbeddingComputeKernel.FUSED,\n        EmbeddingComputeKernel.QUANT,\n    ]:\n        return EmbeddingLocation.DEVICE\n    elif compute_kernel in [\n        EmbeddingComputeKernel.FUSED_UVM,\n        EmbeddingComputeKernel.QUANT_UVM,\n    ]:\n        return EmbeddingLocation.MANAGED\n    elif compute_kernel in [\n        EmbeddingComputeKernel.FUSED_UVM_CACHING,\n        EmbeddingComputeKernel.QUANT_UVM_CACHING,\n    ]:\n        return EmbeddingLocation.MANAGED_CACHING\n    else:\n        raise ValueError(f\"Invalid EmbeddingComputeKernel {compute_kernel}\")",
  "class KJTList(Multistreamable):\n    def __init__(self, features: List[KeyedJaggedTensor]) -> None:\n        self.features = features\n\n    def __len__(self) -> int:\n        return len(self.features)\n\n    def __setitem__(self, key: int, item: KeyedJaggedTensor) -> None:\n        self.features[key] = item\n\n    def __getitem__(self, key: int) -> KeyedJaggedTensor:\n        return self.features[key]\n\n    @torch.jit._drop\n    def __iter__(self) -> Iterator[KeyedJaggedTensor]:\n        return iter(self.features)\n\n    @torch.jit._drop\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for feature in self.features:\n            feature.record_stream(stream)\n\n    @torch.jit._drop\n    def __fx_create_arg__(self, tracer: torch.fx.Tracer) -> fx.node.Argument:\n        return tracer.create_node(\n            \"call_function\",\n            KJTList,\n            args=(tracer.create_arg(self.features),),\n            kwargs={},\n        )",
  "class ListOfKJTList(Multistreamable):\n    def __init__(self, features: List[KJTList]) -> None:\n        self.features_list = features\n\n    def __len__(self) -> int:\n        return len(self.features_list)\n\n    def __setitem__(self, key: int, item: KJTList) -> None:\n        self.features_list[key] = item\n\n    def __getitem__(self, key: int) -> KJTList:\n        return self.features_list[key]\n\n    @torch.jit._drop\n    def __iter__(self) -> Iterator[KJTList]:\n        return iter(self.features_list)\n\n    @torch.jit._drop\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for feature in self.features_list:\n            feature.record_stream(stream)\n\n    @torch.jit._drop\n    def __fx_create_arg__(self, tracer: torch.fx.Tracer) -> fx.node.Argument:\n        return tracer.create_node(\n            \"call_function\",\n            ListOfKJTList,\n            args=(tracer.create_arg(self.features_list),),\n            kwargs={},\n        )",
  "class ShardedConfig:\n    local_rows: int = 0\n    local_cols: int = 0",
  "class ShardedMetaConfig(ShardedConfig):\n    local_metadata: Optional[ShardMetadata] = None\n    global_metadata: Optional[ShardedTensorMetadata] = None",
  "class EmbeddingAttributes:\n    compute_kernel: EmbeddingComputeKernel = EmbeddingComputeKernel.DENSE",
  "class ShardedEmbeddingTable(\n    ShardedMetaConfig,\n    EmbeddingAttributes,\n    EmbeddingTableConfig,\n):\n    fused_params: Optional[Dict[str, Any]] = None",
  "class GroupedEmbeddingConfig:\n    data_type: DataType\n    pooling: PoolingType\n    is_weighted: bool\n    has_feature_processor: bool\n    compute_kernel: EmbeddingComputeKernel\n    embedding_tables: List[ShardedEmbeddingTable]\n    fused_params: Optional[Dict[str, Any]] = None\n\n    def feature_hash_sizes(self) -> List[int]:\n        feature_hash_sizes = []\n        for table in self.embedding_tables:\n            feature_hash_sizes.extend(table.num_features() * [table.num_embeddings])\n        return feature_hash_sizes\n\n    def num_features(self) -> int:\n        num_features = 0\n        for table in self.embedding_tables:\n            num_features += table.num_features()\n        return num_features\n\n    def dim_sum(self) -> int:\n        dim_sum = 0\n        for table in self.embedding_tables:\n            dim_sum += table.num_features() * table.local_cols\n        return dim_sum\n\n    def feature_names(self) -> List[str]:\n        feature_names = []\n        for table in self.embedding_tables:\n            feature_names.extend(table.feature_names)\n        return feature_names\n\n    def embedding_dims(self) -> List[int]:\n        embedding_dims = []\n        for table in self.embedding_tables:\n            embedding_dims.extend([table.local_cols] * table.num_features())\n        return embedding_dims\n\n    def embedding_names(self) -> List[str]:\n        embedding_names = []\n        for table in self.embedding_tables:\n            embedding_names.extend(table.embedding_names)\n        return embedding_names\n\n    def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        embedding_shard_metadata: List[Optional[ShardMetadata]] = []\n        for table in self.embedding_tables:\n            for _ in table.feature_names:\n                embedding_shard_metadata.append(table.local_metadata)\n        return embedding_shard_metadata",
  "class BaseEmbeddingLookup(abc.ABC, nn.Module, Generic[F, T]):\n    \"\"\"\n    Interface implemented by different embedding implementations:\n    e.g. one, which relies on `nn.EmbeddingBag` or table-batched one, etc.\n    \"\"\"\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        sparse_features: F,\n    ) -> T:\n        pass",
  "class FeatureShardingMixIn:\n    \"\"\"\n    Feature Sharding Interface to provide sharding-aware feature metadata.\n    \"\"\"\n\n    def feature_names(self) -> List[str]:\n        raise NotImplementedError\n\n    def feature_names_per_rank(self) -> List[List[str]]:\n        raise NotImplementedError\n\n    def features_per_rank(self) -> List[int]:\n        raise NotImplementedError",
  "class ModuleShardingMixIn:\n    \"\"\"\n    The interface to access a sharded module's sharding scheme.\n    \"\"\"\n\n    @property\n    def shardings(self) -> Dict[str, FeatureShardingMixIn]:\n        raise NotImplementedError",
  "class ShardedEmbeddingModule(\n    ShardedModule[CompIn, DistOut, Out, ShrdCtx],\n    ModuleShardingMixIn,\n):\n    \"\"\"\n    All model-parallel embedding modules implement this interface.\n    Inputs and outputs are data-parallel.\n\n    Args::\n        qcomm_codecs_registry (Optional[Dict[str, QuantizedCommCodecs]]) : Mapping of CommOp name to QuantizedCommCodecs\n    \"\"\"\n\n    @abc.abstractmethod\n    def __init__(\n        self, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None\n    ) -> None:\n        super().__init__(qcomm_codecs_registry)\n\n        self._input_dists: List[nn.Module] = []\n        self._lookups: List[nn.Module] = []\n        self._output_dists: List[nn.Module] = []\n\n    def prefetch(\n        self, dist_input: KJTList, forward_stream: Optional[torch.cuda.Stream] = None\n    ) -> None:\n        \"\"\"\n        Prefetch input features for each lookup module.\n        \"\"\"\n\n        for feature, emb_lookup in zip(dist_input, self._lookups):\n            emb_lookup.prefetch(sparse_features=feature, forward_stream=forward_stream)\n\n    def extra_repr(self) -> str:\n        \"\"\"\n        Pretty prints representation of the module's lookup modules, input_dists and output_dists\n        \"\"\"\n\n        def loop(key: str, modules: List[nn.Module]) -> List[str]:\n            child_lines = []\n            if len(modules) > 0:\n                child_lines.append(\"(\" + key + \"): \")\n            for module in modules:\n                mod_str = repr(module)\n                mod_str = _addindent(mod_str, 2)\n                child_lines.append(mod_str)\n            return child_lines\n\n        rep = []\n        rep.extend(loop(\"lookups\", self._lookups))\n        rep.extend(loop(\"_input_dists\", self._input_dists))\n        rep.extend(loop(\"_output_dists\", self._output_dists))\n\n        return \"\\n \".join(rep)",
  "class BaseEmbeddingSharder(ModuleSharder[M]):\n    def __init__(\n        self,\n        fused_params: Optional[Dict[str, Any]] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n\n        # TODO remove after decoupling\n        self._fused_params = fused_params\n\n    def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.DATA_PARALLEL.value,\n            ShardingType.TABLE_WISE.value,\n            ShardingType.COLUMN_WISE.value,\n            ShardingType.TABLE_COLUMN_WISE.value,\n        ]\n        if compute_device_type in {\"cuda\"}:\n            types += [\n                ShardingType.ROW_WISE.value,\n                ShardingType.TABLE_ROW_WISE.value,\n            ]\n\n        return types\n\n    def compute_kernels(\n        # TODO remove after decoupling\n        self,\n        sharding_type: str,\n        compute_device_type: str,\n    ) -> List[str]:\n        ret: List[str] = []\n        if sharding_type != ShardingType.DATA_PARALLEL.value:\n            ret += [\n                EmbeddingComputeKernel.FUSED.value,\n            ]\n            if compute_device_type in {\"cuda\"}:\n                ret += [\n                    EmbeddingComputeKernel.FUSED_UVM.value,\n                    EmbeddingComputeKernel.FUSED_UVM_CACHING.value,\n                ]\n        else:\n            # TODO re-enable model parallel and dense\n            ret += [\n                EmbeddingComputeKernel.DENSE.value,\n            ]\n        return ret\n\n    @property\n    def fused_params(self) -> Optional[Dict[str, Any]]:\n        return self._fused_params\n\n    def storage_usage(\n        self, tensor: torch.Tensor, compute_device_type: str, compute_kernel: str\n    ) -> Dict[str, int]:\n        \"\"\"\n        List of system resources and corresponding usage given a compute device and\n        compute kernel\n        \"\"\"\n        tensor_bytes = tensor.element_size() * tensor.nelement()\n        if compute_kernel in {\n            EmbeddingComputeKernel.FUSED_UVM.value,\n            EmbeddingComputeKernel.FUSED_UVM_CACHING.value,\n        }:\n            assert compute_device_type in {\"cuda\"}\n            return {ParameterStorage.DDR.value: tensor_bytes}\n        else:\n            assert compute_device_type in {\"cuda\", \"cpu\"}\n            storage_map = {\"cuda\": ParameterStorage.HBM, \"cpu\": ParameterStorage.DDR}\n            return {\n                storage_map[compute_device_type].value: tensor.element_size()\n                * tensor.nelement()\n            }",
  "class BaseGroupedFeatureProcessor(nn.Module):\n    \"\"\"\n    Abstract base class for grouped feature processor\n    \"\"\"\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedJaggedTensor:\n        pass",
  "class BaseQuantEmbeddingSharder(ModuleSharder[M]):\n    def __init__(\n        self,\n        fused_params: Optional[Dict[str, Any]] = None,\n        shardable_params: Optional[List[str]] = None,\n    ) -> None:\n        super().__init__()\n        self._fused_params = fused_params\n        if not shardable_params:\n            shardable_params = []\n        self._shardable_params: List[str] = shardable_params\n\n    def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.TABLE_WISE.value,\n            ShardingType.ROW_WISE.value,\n        ]\n\n        return types\n\n    def shardable_parameters(self, module: M) -> Dict[str, nn.Parameter]:\n\n        shardable_params: Dict[str, nn.Parameter] = {}\n        for name, param in module.state_dict().items():\n            if name.endswith(\".weight\"):\n                table_name = name.split(\".\")[-2]\n                shardable_params[table_name] = param\n\n        if self._shardable_params:\n            assert all(\n                [\n                    table_name in self._shardable_params\n                    for table_name in shardable_params.keys()\n                ]\n            ) or all(\n                [\n                    table_name not in self._shardable_params\n                    for table_name in shardable_params.keys()\n                ]\n            ), f\"Cannot partially shard {type(module)}, please check sharder kwargs\"\n            shardable_params = {\n                table_name: param\n                for table_name, param in shardable_params.items()\n                if table_name in self._shardable_params\n            }\n\n        return shardable_params\n\n    def compute_kernels(\n        self, sharding_type: str, compute_device_type: str\n    ) -> List[str]:\n        ret = [\n            EmbeddingComputeKernel.QUANT.value,\n        ]\n        if compute_device_type in {\"cuda\"}:\n            ret += [\n                EmbeddingComputeKernel.QUANT_UVM.value,\n                EmbeddingComputeKernel.QUANT_UVM_CACHING.value,\n            ]\n        return ret\n\n    @property\n    def fused_params(self) -> Optional[Dict[str, Any]]:\n        return self._fused_params\n\n    def storage_usage(\n        self, tensor: torch.Tensor, compute_device_type: str, compute_kernel: str\n    ) -> Dict[str, int]:\n        \"\"\"\n        List of system resources and corresponding usage given a compute device and\n        compute kernel\n        \"\"\"\n        tensor_bytes = tensor.element_size() * tensor.nelement() + tensor.shape[0] * 4\n        if compute_kernel in {\n            EmbeddingComputeKernel.QUANT_UVM.value,\n            EmbeddingComputeKernel.QUANT_UVM_CACHING.value,\n        }:\n            assert compute_device_type in {\"cuda\"}\n            return {ParameterStorage.DDR.value: tensor_bytes}\n        else:\n            assert compute_device_type in {\"cuda\", \"cpu\"}\n            storage_map = {\"cuda\": ParameterStorage.HBM, \"cpu\": ParameterStorage.DDR}\n            return {storage_map[compute_device_type].value: tensor_bytes}",
  "def __init__(self, features: List[KeyedJaggedTensor]) -> None:\n        self.features = features",
  "def __len__(self) -> int:\n        return len(self.features)",
  "def __setitem__(self, key: int, item: KeyedJaggedTensor) -> None:\n        self.features[key] = item",
  "def __getitem__(self, key: int) -> KeyedJaggedTensor:\n        return self.features[key]",
  "def __iter__(self) -> Iterator[KeyedJaggedTensor]:\n        return iter(self.features)",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for feature in self.features:\n            feature.record_stream(stream)",
  "def __fx_create_arg__(self, tracer: torch.fx.Tracer) -> fx.node.Argument:\n        return tracer.create_node(\n            \"call_function\",\n            KJTList,\n            args=(tracer.create_arg(self.features),),\n            kwargs={},\n        )",
  "def __init__(self, features: List[KJTList]) -> None:\n        self.features_list = features",
  "def __len__(self) -> int:\n        return len(self.features_list)",
  "def __setitem__(self, key: int, item: KJTList) -> None:\n        self.features_list[key] = item",
  "def __getitem__(self, key: int) -> KJTList:\n        return self.features_list[key]",
  "def __iter__(self) -> Iterator[KJTList]:\n        return iter(self.features_list)",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for feature in self.features_list:\n            feature.record_stream(stream)",
  "def __fx_create_arg__(self, tracer: torch.fx.Tracer) -> fx.node.Argument:\n        return tracer.create_node(\n            \"call_function\",\n            ListOfKJTList,\n            args=(tracer.create_arg(self.features_list),),\n            kwargs={},\n        )",
  "def feature_hash_sizes(self) -> List[int]:\n        feature_hash_sizes = []\n        for table in self.embedding_tables:\n            feature_hash_sizes.extend(table.num_features() * [table.num_embeddings])\n        return feature_hash_sizes",
  "def num_features(self) -> int:\n        num_features = 0\n        for table in self.embedding_tables:\n            num_features += table.num_features()\n        return num_features",
  "def dim_sum(self) -> int:\n        dim_sum = 0\n        for table in self.embedding_tables:\n            dim_sum += table.num_features() * table.local_cols\n        return dim_sum",
  "def feature_names(self) -> List[str]:\n        feature_names = []\n        for table in self.embedding_tables:\n            feature_names.extend(table.feature_names)\n        return feature_names",
  "def embedding_dims(self) -> List[int]:\n        embedding_dims = []\n        for table in self.embedding_tables:\n            embedding_dims.extend([table.local_cols] * table.num_features())\n        return embedding_dims",
  "def embedding_names(self) -> List[str]:\n        embedding_names = []\n        for table in self.embedding_tables:\n            embedding_names.extend(table.embedding_names)\n        return embedding_names",
  "def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        embedding_shard_metadata: List[Optional[ShardMetadata]] = []\n        for table in self.embedding_tables:\n            for _ in table.feature_names:\n                embedding_shard_metadata.append(table.local_metadata)\n        return embedding_shard_metadata",
  "def forward(\n        self,\n        sparse_features: F,\n    ) -> T:\n        pass",
  "def feature_names(self) -> List[str]:\n        raise NotImplementedError",
  "def feature_names_per_rank(self) -> List[List[str]]:\n        raise NotImplementedError",
  "def features_per_rank(self) -> List[int]:\n        raise NotImplementedError",
  "def shardings(self) -> Dict[str, FeatureShardingMixIn]:\n        raise NotImplementedError",
  "def __init__(\n        self, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None\n    ) -> None:\n        super().__init__(qcomm_codecs_registry)\n\n        self._input_dists: List[nn.Module] = []\n        self._lookups: List[nn.Module] = []\n        self._output_dists: List[nn.Module] = []",
  "def prefetch(\n        self, dist_input: KJTList, forward_stream: Optional[torch.cuda.Stream] = None\n    ) -> None:\n        \"\"\"\n        Prefetch input features for each lookup module.\n        \"\"\"\n\n        for feature, emb_lookup in zip(dist_input, self._lookups):\n            emb_lookup.prefetch(sparse_features=feature, forward_stream=forward_stream)",
  "def extra_repr(self) -> str:\n        \"\"\"\n        Pretty prints representation of the module's lookup modules, input_dists and output_dists\n        \"\"\"\n\n        def loop(key: str, modules: List[nn.Module]) -> List[str]:\n            child_lines = []\n            if len(modules) > 0:\n                child_lines.append(\"(\" + key + \"): \")\n            for module in modules:\n                mod_str = repr(module)\n                mod_str = _addindent(mod_str, 2)\n                child_lines.append(mod_str)\n            return child_lines\n\n        rep = []\n        rep.extend(loop(\"lookups\", self._lookups))\n        rep.extend(loop(\"_input_dists\", self._input_dists))\n        rep.extend(loop(\"_output_dists\", self._output_dists))\n\n        return \"\\n \".join(rep)",
  "def __init__(\n        self,\n        fused_params: Optional[Dict[str, Any]] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n\n        # TODO remove after decoupling\n        self._fused_params = fused_params",
  "def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.DATA_PARALLEL.value,\n            ShardingType.TABLE_WISE.value,\n            ShardingType.COLUMN_WISE.value,\n            ShardingType.TABLE_COLUMN_WISE.value,\n        ]\n        if compute_device_type in {\"cuda\"}:\n            types += [\n                ShardingType.ROW_WISE.value,\n                ShardingType.TABLE_ROW_WISE.value,\n            ]\n\n        return types",
  "def compute_kernels(\n        # TODO remove after decoupling\n        self,\n        sharding_type: str,\n        compute_device_type: str,\n    ) -> List[str]:\n        ret: List[str] = []\n        if sharding_type != ShardingType.DATA_PARALLEL.value:\n            ret += [\n                EmbeddingComputeKernel.FUSED.value,\n            ]\n            if compute_device_type in {\"cuda\"}:\n                ret += [\n                    EmbeddingComputeKernel.FUSED_UVM.value,\n                    EmbeddingComputeKernel.FUSED_UVM_CACHING.value,\n                ]\n        else:\n            # TODO re-enable model parallel and dense\n            ret += [\n                EmbeddingComputeKernel.DENSE.value,\n            ]\n        return ret",
  "def fused_params(self) -> Optional[Dict[str, Any]]:\n        return self._fused_params",
  "def storage_usage(\n        self, tensor: torch.Tensor, compute_device_type: str, compute_kernel: str\n    ) -> Dict[str, int]:\n        \"\"\"\n        List of system resources and corresponding usage given a compute device and\n        compute kernel\n        \"\"\"\n        tensor_bytes = tensor.element_size() * tensor.nelement()\n        if compute_kernel in {\n            EmbeddingComputeKernel.FUSED_UVM.value,\n            EmbeddingComputeKernel.FUSED_UVM_CACHING.value,\n        }:\n            assert compute_device_type in {\"cuda\"}\n            return {ParameterStorage.DDR.value: tensor_bytes}\n        else:\n            assert compute_device_type in {\"cuda\", \"cpu\"}\n            storage_map = {\"cuda\": ParameterStorage.HBM, \"cpu\": ParameterStorage.DDR}\n            return {\n                storage_map[compute_device_type].value: tensor.element_size()\n                * tensor.nelement()\n            }",
  "def forward(\n        self,\n        features: KeyedJaggedTensor,\n    ) -> KeyedJaggedTensor:\n        pass",
  "def __init__(\n        self,\n        fused_params: Optional[Dict[str, Any]] = None,\n        shardable_params: Optional[List[str]] = None,\n    ) -> None:\n        super().__init__()\n        self._fused_params = fused_params\n        if not shardable_params:\n            shardable_params = []\n        self._shardable_params: List[str] = shardable_params",
  "def sharding_types(self, compute_device_type: str) -> List[str]:\n        types = [\n            ShardingType.TABLE_WISE.value,\n            ShardingType.ROW_WISE.value,\n        ]\n\n        return types",
  "def shardable_parameters(self, module: M) -> Dict[str, nn.Parameter]:\n\n        shardable_params: Dict[str, nn.Parameter] = {}\n        for name, param in module.state_dict().items():\n            if name.endswith(\".weight\"):\n                table_name = name.split(\".\")[-2]\n                shardable_params[table_name] = param\n\n        if self._shardable_params:\n            assert all(\n                [\n                    table_name in self._shardable_params\n                    for table_name in shardable_params.keys()\n                ]\n            ) or all(\n                [\n                    table_name not in self._shardable_params\n                    for table_name in shardable_params.keys()\n                ]\n            ), f\"Cannot partially shard {type(module)}, please check sharder kwargs\"\n            shardable_params = {\n                table_name: param\n                for table_name, param in shardable_params.items()\n                if table_name in self._shardable_params\n            }\n\n        return shardable_params",
  "def compute_kernels(\n        self, sharding_type: str, compute_device_type: str\n    ) -> List[str]:\n        ret = [\n            EmbeddingComputeKernel.QUANT.value,\n        ]\n        if compute_device_type in {\"cuda\"}:\n            ret += [\n                EmbeddingComputeKernel.QUANT_UVM.value,\n                EmbeddingComputeKernel.QUANT_UVM_CACHING.value,\n            ]\n        return ret",
  "def fused_params(self) -> Optional[Dict[str, Any]]:\n        return self._fused_params",
  "def storage_usage(\n        self, tensor: torch.Tensor, compute_device_type: str, compute_kernel: str\n    ) -> Dict[str, int]:\n        \"\"\"\n        List of system resources and corresponding usage given a compute device and\n        compute kernel\n        \"\"\"\n        tensor_bytes = tensor.element_size() * tensor.nelement() + tensor.shape[0] * 4\n        if compute_kernel in {\n            EmbeddingComputeKernel.QUANT_UVM.value,\n            EmbeddingComputeKernel.QUANT_UVM_CACHING.value,\n        }:\n            assert compute_device_type in {\"cuda\"}\n            return {ParameterStorage.DDR.value: tensor_bytes}\n        else:\n            assert compute_device_type in {\"cuda\", \"cpu\"}\n            storage_map = {\"cuda\": ParameterStorage.HBM, \"cpu\": ParameterStorage.DDR}\n            return {storage_map[compute_device_type].value: tensor_bytes}",
  "def loop(key: str, modules: List[nn.Module]) -> List[str]:\n            child_lines = []\n            if len(modules) > 0:\n                child_lines.append(\"(\" + key + \"): \")\n            for module in modules:\n                mod_str = repr(module)\n                mod_str = _addindent(mod_str, 2)\n                child_lines.append(mod_str)\n            return child_lines",
  "def is_leader(pg: Optional[dist.ProcessGroup], leader_rank: int = 0) -> bool:\n    \"\"\"\n    Checks if the current processs is the leader.\n\n    Args:\n        pg (Optional[dist.ProcessGroup]): the process's rank within the pg is used to\n            determine if the process is the leader. pg being None implies that the\n            process is the only member in the group (e.g. a single process program).\n        leader_rank (int): the definition of leader (defaults to 0). The caller can\n            override it with a context-specific definition.\n    \"\"\"\n    if pg is None:\n        return leader_rank == 0\n    return pg.rank() == leader_rank",
  "def invoke_on_rank_and_broadcast_result(\n    pg: dist.ProcessGroup,\n    rank: int,\n    func: Callable[..., T],\n    *args: Any,\n    **kwargs: Any,\n) -> T:\n    \"\"\"\n    Invokes a function on the designated rank and broadcasts the result to all\n    members within the group.\n\n    Example::\n\n        id = invoke_on_rank_and_broadcast_result(pg, 0, allocate_id)\n    \"\"\"\n    if pg.rank() == rank:\n        res = func(*args, **kwargs)\n        object_list = [res]\n    else:\n        object_list = [None]\n    if pg.size() > 1:\n        dist.broadcast_object_list(object_list, rank, group=pg)\n    return cast(T, object_list[0])",
  "def run_on_leader(pg: dist.ProcessGroup, rank: int):\n    def callable(func: Callable[..., T]) -> T:\n        @wraps(func)\n        def wrapped(*args: Any, **kwargs: Any) -> T:\n            return invoke_on_rank_and_broadcast_result(pg, rank, func, *args, **kwargs)\n\n        return wrapped\n\n    return callable",
  "def callable(func: Callable[..., T]) -> T:\n        @wraps(func)\n        def wrapped(*args: Any, **kwargs: Any) -> T:\n            return invoke_on_rank_and_broadcast_result(pg, rank, func, *args, **kwargs)\n\n        return wrapped",
  "def wrapped(*args: Any, **kwargs: Any) -> T:\n            return invoke_on_rank_and_broadcast_result(pg, rank, func, *args, **kwargs)",
  "def get_default_sharders() -> List[ModuleSharder[nn.Module]]:\n    return [\n        cast(ModuleSharder[nn.Module], EmbeddingBagCollectionSharder()),\n        cast(ModuleSharder[nn.Module], FeatureProcessedEmbeddingBagCollectionSharder()),\n        cast(ModuleSharder[nn.Module], EmbeddingCollectionSharder()),\n        cast(ModuleSharder[nn.Module], FusedEmbeddingBagCollectionSharder()),\n        cast(ModuleSharder[nn.Module], QuantEmbeddingBagCollectionSharder()),\n        cast(ModuleSharder[nn.Module], QuantEmbeddingCollectionSharder()),\n        cast(ModuleSharder[nn.Module], ManagedCollisionEmbeddingBagCollectionSharder()),\n    ]",
  "def get_module_to_default_sharders() -> Dict[Type[nn.Module], ModuleSharder[nn.Module]]:\n    return {sharder.module_type: sharder for sharder in get_default_sharders()}",
  "def placement(\n    compute_device: str,\n    rank: int,\n    local_size: int,\n) -> str:\n    param_device = compute_device\n    if compute_device == \"cuda\":\n        param_device = torch.device(\"cuda\", rank % local_size)\n    return f\"rank:{rank}/{param_device}\"",
  "def calculate_shard_sizes_and_offsets(\n    tensor: torch.Tensor,\n    world_size: int,\n    local_world_size: int,\n    sharding_type: str,\n    col_wise_shard_dim: Optional[int] = None,\n) -> Tuple[List[List[int]], List[List[int]]]:\n    \"\"\"\n    Calculates sizes and offsets for tensor sharded according to provided sharding type.\n\n    Args:\n        tensor (torch.Tensor): tensor to be sharded.\n        world_size (int): total number of devices in topology.\n        local_world_size (int): total number of devices in host group topology.\n        sharding_type (str): provided ShardingType value.\n        col_wise_shard_dim (Optional[int]): dimension for column wise sharding split.\n\n    Returns:\n        Tuple[List[List[int]], List[List[int]]]: shard sizes, represented as a list of the dimensions of the sharded tensor on each device, and shard offsets, represented as a list of coordinates of placement on each device.\n\n    Raises:\n        ValueError: If `sharding_type` is not a valid ShardingType.\n    \"\"\"\n\n    (rows, columns) = tensor.shape\n\n    if sharding_type == ShardingType.DATA_PARALLEL.value:\n        return [[rows, columns]] * world_size, [[0, 0]] * world_size\n    elif sharding_type == ShardingType.TABLE_WISE.value:\n        return [[rows, columns]], [[0, 0]]\n    elif sharding_type == ShardingType.ROW_WISE.value:\n        return _calculate_rw_shard_sizes_and_offsets(rows, world_size, columns)\n    elif sharding_type == ShardingType.TABLE_ROW_WISE.value:\n        return _calculate_rw_shard_sizes_and_offsets(rows, local_world_size, columns)\n    elif (\n        sharding_type == ShardingType.COLUMN_WISE.value\n        or sharding_type == ShardingType.TABLE_COLUMN_WISE.value\n    ):\n        return _calculate_cw_shard_sizes_and_offsets(columns, rows, col_wise_shard_dim)\n\n    raise ValueError(\n        f\"Unrecognized or unsupported sharding type provided: {sharding_type}\"\n    )",
  "def _calculate_rw_shard_sizes_and_offsets(\n    hash_size: int, num_devices: int, columns: int\n) -> Tuple[List[List[int]], List[List[int]]]:\n    \"\"\"\n    Sets prefix of shard_sizes to be `math.ceil(hash_size/num_devices)`.\n\n    For example if hash_size = 10, num_devices = 4, we will allocate the rows as 3,3,3,1\n    (rather than 3,3,2,2).\n    This is due to implementation in RW sharding that sets block_size_lists to be ceil.\n    The balanced way is harder to support on GPU.\n    For more details see https://fb.quip.com/xbgbAchCTOL0\n\n    Also consider the example of hash_size = 5, num_devices = 4. The expected rows per\n    rank is [2,2,1,0].\n    \"\"\"\n\n    block_size: int = math.ceil(hash_size / num_devices)\n    last_rank: int = hash_size // block_size\n    last_block_size: int = hash_size - block_size * last_rank\n    shard_sizes: List[List[int]] = []\n\n    for rank in range(num_devices):\n        if rank < last_rank:\n            local_row: int = block_size\n        elif rank == last_rank:\n            local_row: int = last_block_size\n        else:\n            local_row: int = 0\n        shard_sizes.append([local_row, columns])\n    shard_offsets = [[0, 0]]\n\n    for i in range(num_devices - 1):\n        shard_offsets.append([shard_sizes[i][0] + shard_offsets[i][0], 0])\n\n    return shard_sizes, shard_offsets",
  "def _find_base_dim(lower_bound: int, dim: int) -> int:\n    for i in range(lower_bound, dim):\n        if dim % i == 0 and i % 4 == 0:\n            return i\n    return dim",
  "def _calculate_cw_shard_sizes_and_offsets(\n    columns: int,\n    rows: int,\n    col_wise_shard_dim: Optional[int] = None,\n) -> Tuple[List[List[int]], List[List[int]]]:\n    block_size: int = min(\n        _find_base_dim(col_wise_shard_dim, columns)\n        if col_wise_shard_dim\n        else _find_base_dim(MIN_CW_DIM, columns),\n        columns,\n    )\n\n    if columns % block_size != 0:\n        warnings.warn(\n            f\"Dim of {columns} cannot be evenly divided with column wise shard\"\n            \"dim {col_wise_shard_dim}, overriding block_size to embedding_dim={columns}\",\n            UserWarning,\n        )\n        block_size = columns\n\n    num_col_wise_shards, _residual = divmod(columns, block_size)\n\n    shard_sizes: List[List[int]] = [[rows, block_size]] * num_col_wise_shards\n    shard_offsets: List[List[int]] = [\n        [0, block_size * rank] for rank in range(num_col_wise_shards)\n    ]\n    return shard_sizes, shard_offsets",
  "def _get_parameter_size_offsets(\n    param: torch.nn.Parameter,\n    sharding_type: ShardingType,\n    local_size: int,\n    world_size: int,\n    col_wise_shard_dim: Optional[int] = None,\n) -> List[Tuple[List[int], List[int]]]:\n    (shard_sizes, shard_offsets,) = calculate_shard_sizes_and_offsets(\n        tensor=none_throws(param),\n        world_size=world_size,\n        local_world_size=local_size,\n        sharding_type=sharding_type.value,\n        col_wise_shard_dim=col_wise_shard_dim,\n    )\n    return list(zip(shard_sizes, shard_offsets))",
  "def _get_compute_kernel(\n    sharder: ModuleSharder[nn.Module],\n    param: nn.Parameter,\n    sharding_type: str,\n    device_type: str,\n) -> str:\n    # TODO add placement support for compute_kernel\n    compute_kernels = [EmbeddingComputeKernel.DENSE.value]\n    if sharding_type != ShardingType.DATA_PARALLEL.value:\n        compute_kernels += [\n            EmbeddingComputeKernel.FUSED.value,\n        ]\n    if device_type in {\"cuda\"}:\n        compute_kernels += [\n            EmbeddingComputeKernel.FUSED_UVM.value,\n            EmbeddingComputeKernel.FUSED_UVM_CACHING.value,\n        ]\n\n    if sharding_type == ShardingType.DATA_PARALLEL.value:\n        if EmbeddingComputeKernel.DENSE.value in compute_kernels:\n            return EmbeddingComputeKernel.DENSE.value\n        elif EmbeddingComputeKernel.QUANT.value in compute_kernels:\n            return EmbeddingComputeKernel.QUANT.value\n    else:\n        if (\n            hasattr(param, \"_in_backward_optimizers\")\n            and EmbeddingComputeKernel.FUSED.value in compute_kernels\n        ):\n            return EmbeddingComputeKernel.FUSED.value\n        elif EmbeddingComputeKernel.DENSE.value in compute_kernels:\n            return EmbeddingComputeKernel.DENSE.value\n        elif EmbeddingComputeKernel.QUANT.value in compute_kernels:\n            return EmbeddingComputeKernel.QUANT.value\n\n    raise ValueError(\n        f\"Could not find compute kernel for sharding_type={sharding_type} in {compute_kernels}\"\n    )",
  "def _get_parameter_sharding(\n    param: nn.Parameter,\n    sharding_type: str,\n    size_offset_ranks: List[Tuple[List[int], List[int], int]],\n    local_size: int,\n    device_type: str,\n    sharder: ModuleSharder[nn.Module],\n) -> ParameterSharding:\n    return ParameterSharding(\n        sharding_spec=None\n        if sharding_type == ShardingType.DATA_PARALLEL.value\n        else EnumerableShardingSpec(\n            [\n                ShardMetadata(\n                    shard_sizes=size,\n                    shard_offsets=offset,\n                    placement=placement(\n                        device_type,\n                        none_throws(rank),\n                        none_throws(local_size),\n                    ),\n                )\n                for (size, offset, rank) in (size_offset_ranks)\n            ]\n        ),\n        sharding_type=sharding_type,\n        compute_kernel=_get_compute_kernel(sharder, param, sharding_type, device_type),\n        ranks=[rank for (_, _, rank) in size_offset_ranks],\n    )",
  "def data_parallel() -> ParameterShardingGenerator:\n    \"\"\"\n    Returns a generator of ParameterShardingPlan for `ShardingType::DATA_PARALLEL` for construct_module_sharding_plan.\n\n    Example::\n\n        ebc = EmbeddingBagCollection(...)\n        plan = construct_module_sharding_plan(\n            ebc,\n            {\n                \"table_0\": data_parallel(),\n            },\n        )\n    \"\"\"\n\n    def _parameter_sharding_generator(\n        param: nn.Parameter,\n        local_size: int,\n        world_size: int,\n        device_type: str,\n        sharder: ModuleSharder[nn.Module],\n    ) -> ParameterSharding:\n        size_and_offsets = _get_parameter_size_offsets(\n            param,\n            ShardingType.DATA_PARALLEL,\n            local_size,\n            world_size,\n        )\n        size_offset_ranks = []\n\n        assert len(size_and_offsets) == world_size\n        for (size, offset), rank in zip(size_and_offsets, range(world_size)):\n            size_offset_ranks.append((size, offset, rank))\n\n        return _get_parameter_sharding(\n            param,\n            ShardingType.DATA_PARALLEL.value,\n            size_offset_ranks,\n            local_size,\n            device_type,\n            sharder,\n        )\n\n    return _parameter_sharding_generator",
  "def table_wise(\n    rank: int,\n) -> ParameterShardingGenerator:\n    \"\"\"\n    Returns a generator of ParameterShardingPlan for `ShardingType::TABLE_WISE` for construct_module_sharding_plan.\n\n    Args:\n    rank (int): rank to place table when doing table wise\n\n    Example::\n\n        ebc = EmbeddingBagCollection(...)\n        plan = construct_module_sharding_plan(\n            ebc,\n            {\n                \"table_0\": table_wise(rank=0),\n            },\n        )\n    \"\"\"\n\n    def _parameter_sharding_generator(\n        param: nn.Parameter,\n        local_size: int,\n        world_size: int,\n        device_type: str,\n        sharder: ModuleSharder[nn.Module],\n    ) -> ParameterSharding:\n        size_and_offsets = _get_parameter_size_offsets(\n            param,\n            ShardingType.TABLE_WISE,\n            local_size,\n            world_size,\n        )\n        assert len(size_and_offsets) == 1\n        (size, offset) = size_and_offsets[0]\n        size_offset_ranks = [(size, offset, rank)]\n\n        return _get_parameter_sharding(\n            param,\n            ShardingType.TABLE_WISE.value,\n            size_offset_ranks,\n            local_size,\n            device_type,\n            sharder,\n        )\n\n    return _parameter_sharding_generator",
  "def row_wise() -> ParameterShardingGenerator:\n    \"\"\"\n    Returns a generator of ParameterShardingPlan for `ShardingType::ROW_WISE` for construct_module_sharding_plan.\n\n    Example::\n\n        ebc = EmbeddingBagCollection(...)\n        plan = construct_module_sharding_plan(\n            ebc,\n            {\n                \"table_1\": row_wise(),\n            },\n        )\n    \"\"\"\n\n    def _parameter_sharding_generator(\n        param: nn.Parameter,\n        local_size: int,\n        world_size: int,\n        device_type: str,\n        sharder: ModuleSharder[nn.Module],\n    ) -> ParameterSharding:\n        size_and_offsets = _get_parameter_size_offsets(\n            param,\n            ShardingType.ROW_WISE,\n            local_size,\n            world_size,\n        )\n        assert len(size_and_offsets) <= world_size\n        size_offset_ranks = []\n        for (size, offset), rank in zip(size_and_offsets, range(world_size)):\n            size_offset_ranks.append((size, offset, rank))\n\n        return _get_parameter_sharding(\n            param,\n            ShardingType.ROW_WISE.value,\n            size_offset_ranks,\n            local_size,\n            device_type,\n            sharder,\n        )\n\n    return _parameter_sharding_generator",
  "def column_wise(\n    ranks: List[int],\n) -> ParameterShardingGenerator:\n    \"\"\"\n    Returns a generator of ParameterShardingPlan for `ShardingType::COLUMN_WISE` for construct_module_sharding_plan.\n    Table will the sharded column-wise evenly across specified ranks (and can reuse ranks).\n\n    Args:\n    ranks (List[int]): ranks to place columns\n\n    Example::\n\n        ebc = EmbeddingBagCollection(...)\n        plan = construct_module_sharding_plan(\n            ebc,\n            {\n                \"table_3\": column_wise(ranks=[0,1,2]),\n            },\n        )\n    \"\"\"\n\n    def _parameter_sharding_generator(\n        param: nn.Parameter,\n        local_size: int,\n        world_size: int,\n        device_type: str,\n        sharder: ModuleSharder[nn.Module],\n    ) -> ParameterSharding:\n        if param.shape[1] % len(ranks) != 0:\n            raise ValueError(\n                f\"column dim of {param.shape[1]} cannot be evenly divided across {ranks}\"\n            )\n        shard_dim = param.shape[1] // len(ranks)\n        size_and_offsets = _get_parameter_size_offsets(\n            param,\n            ShardingType.COLUMN_WISE,\n            local_size,\n            world_size,\n            col_wise_shard_dim=shard_dim,\n        )\n\n        size_offset_ranks = []\n        for (size, offset), rank in zip(size_and_offsets, ranks):\n            size_offset_ranks.append((size, offset, rank))\n\n        return _get_parameter_sharding(\n            param,\n            ShardingType.COLUMN_WISE.value,\n            size_offset_ranks,\n            local_size,\n            device_type,\n            sharder,\n        )\n\n    return _parameter_sharding_generator",
  "def table_row_wise(\n    host_index: int,\n) -> ParameterShardingGenerator:\n    \"\"\"\n    Returns a generator of ParameterShardingPlan for `ShardingType::TABLE_ROW_WISE` for construct_module_sharding_plan.\n\n    Args:\n    host_index (int): index of host (node) to do row wise\n\n    Example::\n\n        ebc = EmbeddingBagCollection(...)\n        plan = construct_module_sharding_plan(\n            ebc,\n            {\n                \"table_4\": table_row_wise(host_index=2),\n            },\n        )\n    \"\"\"\n\n    def _parameter_sharding_generator(\n        param: nn.Parameter,\n        local_size: int,\n        world_size: int,\n        device_type: str,\n        sharder: ModuleSharder[nn.Module],\n    ) -> ParameterSharding:\n        size_and_offsets = _get_parameter_size_offsets(\n            param,\n            ShardingType.TABLE_ROW_WISE,\n            local_size,\n            world_size,\n        )\n\n        size_offset_ranks = []\n        assert len(size_and_offsets) <= local_size\n        for (size, offset), rank in zip(size_and_offsets, range(local_size)):\n            rank_offset = host_index * local_size\n            size_offset_ranks.append((size, offset, rank_offset + rank))\n\n        return _get_parameter_sharding(\n            param,\n            ShardingType.TABLE_ROW_WISE.value,\n            size_offset_ranks,\n            local_size,\n            device_type,\n            sharder,\n        )\n\n    return _parameter_sharding_generator",
  "def apply_to_all(\n    module: nn.Module,\n    parameter_sharding_generator: ParameterShardingGenerator,\n    sharder: Optional[ModuleSharder[nn.Module]] = None,\n) -> Dict[str, ParameterShardingGenerator]:\n    \"\"\"\n    Convenience function to apply a sharding scheme generator for all modules in construct_module_sharding_plan.\n\n    Example::\n\n        ebc = EmbeddingBagCollection(...)\n        sharder = EmbeddingBagCollectionSharder()\n        plan = construct_parameter_sharding_plan(\n            ebc,\n            apply_to_all(ebc, row_wise(), sharder),\n        )\n    \"\"\"\n    if sharder is None:\n        sharder = get_module_to_default_sharders().get(type(module), None)\n    else:\n        assert isinstance(\n            module, sharder.module_type\n        ), f\"Incorrect sharder for module type {type(module)}\"\n\n    assert (\n        sharder is not None\n    ), f\"Could not find a valid sharder type for {type(module)}\"\n\n    shardable_parameters = sharder.shardable_parameters(module)\n    return {\n        param_name: parameter_sharding_generator for param_name in shardable_parameters\n    }",
  "def construct_module_sharding_plan(\n    module: nn.Module,\n    per_param_sharding: Dict[str, ParameterShardingGenerator],\n    sharder: Optional[ModuleSharder[nn.Module]] = None,\n    local_size: Optional[int] = None,\n    world_size: Optional[int] = None,\n    device_type: Optional[str] = None,\n) -> EmbeddingModuleShardingPlan:\n    \"\"\"\n    Helper function to create module sharding plans (EmbeddingModuleShardingPlan) for an module\n    Args:\n        module (nn.Module): module to create plan for.\n        per_param_sharding: Dict[str, Callable[[nn.Parameter, int, int, str], ParameterSharding]]: A mapping of parameter names to a generator function\n        that takes in [parameter, local_size, world_size, device_type] and returns a ParameterSharding. We recommend using one of the predefined generator functions\n        e.g. table_wise_sharding, row_wise_sharding, etc,\n        sharder: Optional[ModuleSharder[nn.Module]]: Sharder that we are creating a plan for. If set to none, we will try to derive it from the module. We recommend setting this to None.\n        local_size: Optional[int] = None: process group local size\n        world_size: Optional[int] = None: process_group world_size\n        device_type: str : Torch device type,\n\n    Example::\n\n        ebc = EmbeddingBagCollection(...)\n        plan = construct_module_sharding_plan(\n            ebc,\n            {\n                \"table_0\": data_parallel(),\n                \"table_1\": row_wise(),\n                \"table_2\": column_wise(),\n                \"table_3\": column_wise(ranks=[0,1,2]),\n                \"table_4\": table_row_wise(host_index=2),\n            },\n        )\n    \"\"\"\n    if device_type is None:\n        device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if sharder is None:\n        sharder = get_module_to_default_sharders().get(type(module), None)\n    assert (\n        sharder is not None\n    ), f\"Could not find a valid sharder type for {type(module)}\"\n\n    assert isinstance(\n        module, sharder.module_type\n    ), f\"Incorrect sharder for module type {type(module)}\"\n    shardable_parameters = sharder.shardable_parameters(module)\n    assert (\n        shardable_parameters.keys() == per_param_sharding.keys()\n    ), \"per_param_sharding_config doesn't match the shardable parameters of the module\"\n\n    local_size = local_size or get_local_size()\n    world_size = world_size or dist.get_world_size()\n\n    per_parameter_sharding = EmbeddingModuleShardingPlan()\n    for table_name, sharding_plan_generator in per_param_sharding.items():\n        param = shardable_parameters[table_name]\n        per_parameter_sharding[table_name] = sharding_plan_generator(\n            param, local_size, world_size, device_type, sharder\n        )\n\n    return per_parameter_sharding",
  "def _parameter_sharding_generator(\n        param: nn.Parameter,\n        local_size: int,\n        world_size: int,\n        device_type: str,\n        sharder: ModuleSharder[nn.Module],\n    ) -> ParameterSharding:\n        size_and_offsets = _get_parameter_size_offsets(\n            param,\n            ShardingType.DATA_PARALLEL,\n            local_size,\n            world_size,\n        )\n        size_offset_ranks = []\n\n        assert len(size_and_offsets) == world_size\n        for (size, offset), rank in zip(size_and_offsets, range(world_size)):\n            size_offset_ranks.append((size, offset, rank))\n\n        return _get_parameter_sharding(\n            param,\n            ShardingType.DATA_PARALLEL.value,\n            size_offset_ranks,\n            local_size,\n            device_type,\n            sharder,\n        )",
  "def _parameter_sharding_generator(\n        param: nn.Parameter,\n        local_size: int,\n        world_size: int,\n        device_type: str,\n        sharder: ModuleSharder[nn.Module],\n    ) -> ParameterSharding:\n        size_and_offsets = _get_parameter_size_offsets(\n            param,\n            ShardingType.TABLE_WISE,\n            local_size,\n            world_size,\n        )\n        assert len(size_and_offsets) == 1\n        (size, offset) = size_and_offsets[0]\n        size_offset_ranks = [(size, offset, rank)]\n\n        return _get_parameter_sharding(\n            param,\n            ShardingType.TABLE_WISE.value,\n            size_offset_ranks,\n            local_size,\n            device_type,\n            sharder,\n        )",
  "def _parameter_sharding_generator(\n        param: nn.Parameter,\n        local_size: int,\n        world_size: int,\n        device_type: str,\n        sharder: ModuleSharder[nn.Module],\n    ) -> ParameterSharding:\n        size_and_offsets = _get_parameter_size_offsets(\n            param,\n            ShardingType.ROW_WISE,\n            local_size,\n            world_size,\n        )\n        assert len(size_and_offsets) <= world_size\n        size_offset_ranks = []\n        for (size, offset), rank in zip(size_and_offsets, range(world_size)):\n            size_offset_ranks.append((size, offset, rank))\n\n        return _get_parameter_sharding(\n            param,\n            ShardingType.ROW_WISE.value,\n            size_offset_ranks,\n            local_size,\n            device_type,\n            sharder,\n        )",
  "def _parameter_sharding_generator(\n        param: nn.Parameter,\n        local_size: int,\n        world_size: int,\n        device_type: str,\n        sharder: ModuleSharder[nn.Module],\n    ) -> ParameterSharding:\n        if param.shape[1] % len(ranks) != 0:\n            raise ValueError(\n                f\"column dim of {param.shape[1]} cannot be evenly divided across {ranks}\"\n            )\n        shard_dim = param.shape[1] // len(ranks)\n        size_and_offsets = _get_parameter_size_offsets(\n            param,\n            ShardingType.COLUMN_WISE,\n            local_size,\n            world_size,\n            col_wise_shard_dim=shard_dim,\n        )\n\n        size_offset_ranks = []\n        for (size, offset), rank in zip(size_and_offsets, ranks):\n            size_offset_ranks.append((size, offset, rank))\n\n        return _get_parameter_sharding(\n            param,\n            ShardingType.COLUMN_WISE.value,\n            size_offset_ranks,\n            local_size,\n            device_type,\n            sharder,\n        )",
  "def _parameter_sharding_generator(\n        param: nn.Parameter,\n        local_size: int,\n        world_size: int,\n        device_type: str,\n        sharder: ModuleSharder[nn.Module],\n    ) -> ParameterSharding:\n        size_and_offsets = _get_parameter_size_offsets(\n            param,\n            ShardingType.TABLE_ROW_WISE,\n            local_size,\n            world_size,\n        )\n\n        size_offset_ranks = []\n        assert len(size_and_offsets) <= local_size\n        for (size, offset), rank in zip(size_and_offsets, range(local_size)):\n            rank_offset = host_index * local_size\n            size_offset_ranks.append((size, offset, rank_offset + rank))\n\n        return _get_parameter_sharding(\n            param,\n            ShardingType.TABLE_ROW_WISE.value,\n            size_offset_ranks,\n            local_size,\n            device_type,\n            sharder,\n        )",
  "class EmbeddingStats(Stats):\n    \"\"\"\n    Stats for a sharding planner execution.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._width: int = MIN_WIDTH\n        self._stats_table: List[str] = []\n\n    def log(\n        self,\n        sharding_plan: ShardingPlan,\n        topology: Topology,\n        batch_size: int,\n        storage_reservation: StorageReservation,\n        num_proposals: int,\n        num_plans: int,\n        run_time: float,\n        best_plan: List[ShardingOption],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        sharders: Optional[List[ModuleSharder[nn.Module]]] = None,\n        debug: bool = True,\n    ) -> None:\n        \"\"\"\n        Logs stats for a given sharding plan.\n\n        Provides a tabular view of stats for the given sharding plan with per device\n        storage usage (HBM and DDR), perf, input, output, and number/type of shards.\n\n        Args:\n            sharding_plan (ShardingPlan): sharding plan chosen by the planner.\n            topology (Topology): device topology.\n            batch_size (int): batch size.\n            storage_reservation (StorageReservation): reserves storage for unsharded\n                parts of the model\n            num_proposals (int): number of proposals evaluated.\n            num_plans (int): number of proposals successfully partitioned.\n            run_time (float): time taken to find plan (in seconds).\n            best_plan (List[ShardingOption]): plan with expected performance.\n            constraints (Optional[Dict[str, ParameterConstraints]]): dict of parameter\n                names to provided ParameterConstraints.\n            debug (bool): whether to enable debug mode.\n        \"\"\"\n\n        shard_by_fqn = {\n            module_name + \".\" + param_name: value\n            for module_name, param_dict in sharding_plan.plan.items()\n            # pyre-ignore - this is a EmbeddingShardingPlan below\n            for param_name, value in param_dict.items()\n        }\n        stats: Dict[int, Dict[str, Any]] = {\n            rank: {\"type\": {}, \"input_sizes\": 0.0, \"output_sizes\": 0.0}\n            for rank in range(topology.world_size)\n        }\n\n        used_sharding_types = set()\n        compute_kernels_to_count = defaultdict(int)\n\n        reserved_hbm_percent = (\n            storage_reservation._percentage\n            if isinstance(\n                storage_reservation,\n                (\n                    FixedPercentageStorageReservation,\n                    HeuristicalStorageReservation,\n                    InferenceStorageReservation,\n                ),\n            )\n            else 0.0\n        )\n        dense_storage = (\n            storage_reservation._dense_storage\n            if isinstance(\n                storage_reservation,\n                (HeuristicalStorageReservation, InferenceStorageReservation),\n            )\n            and storage_reservation._dense_storage is not None\n            else Storage(0, 0)\n        )\n        assert dense_storage\n        kjt_storage = (\n            storage_reservation._kjt_storage\n            if isinstance(\n                storage_reservation,\n                (HeuristicalStorageReservation, InferenceStorageReservation),\n            )\n            and storage_reservation._kjt_storage\n            else Storage(0, 0)\n        )\n        assert kjt_storage\n\n        for sharding_option in best_plan:\n            fqn = sharding_option.fqn\n\n            if shard_by_fqn.get(fqn) is None:\n                continue\n            shard: ParameterSharding = shard_by_fqn[fqn]\n\n            ranks, input_sizes, output_sizes = self._get_shard_stats(\n                shard=shard,\n                sharding_option=sharding_option,\n                world_size=topology.world_size,\n                local_world_size=topology.local_world_size,\n                constraints=constraints,\n            )\n            sharding_type_abbr = _get_sharding_type_abbr(shard.sharding_type)\n            used_sharding_types.add(sharding_type_abbr)\n            compute_kernels_to_count[sharding_option.compute_kernel] += 1\n\n            for i, rank in enumerate(ranks):\n                count = stats[rank][\"type\"].get(sharding_type_abbr, 0)\n                stats[rank][\"type\"][sharding_type_abbr] = count + 1\n                stats[rank][\"input_sizes\"] += input_sizes[i]\n                stats[rank][\"output_sizes\"] += output_sizes[i]\n\n        used_hbm = [0] * topology.world_size\n        used_ddr = [0] * topology.world_size\n        perf = [\n            Perf(fwd_compute=0, fwd_comms=0, bwd_compute=0, bwd_comms=0)\n            for _ in range(topology.world_size)\n        ]\n        for sharding_option in best_plan:\n            for shard in sharding_option.shards:\n                shard_storage = cast(Storage, shard.storage)\n                rank = cast(int, shard.rank)\n                used_hbm[rank] += shard_storage.hbm\n                used_ddr[rank] += shard_storage.ddr\n                perf[rank] += cast(Perf, shard.perf)\n\n        used_hbm = [hbm + dense_storage.hbm + kjt_storage.hbm for hbm in used_hbm]\n        used_ddr = [ddr + dense_storage.ddr + kjt_storage.ddr for ddr in used_ddr]\n\n        table: List[List[Union[str, int]]] = [\n            [\n                \"Rank\",\n                \"HBM (GB)\",\n                \"DDR (GB)\",\n                \"Perf (ms)\",\n                \"Input (MB)\",\n                \"Output (MB)\",\n                \"Shards\",\n            ],\n            [\n                \"------\",\n                \"----------\",\n                \"----------\",\n                \"-----------\",\n                \"------------\",\n                \"-------------\",\n                \"--------\",\n            ],\n        ]\n\n        for rank, device in enumerate(topology.devices):\n            used_hbm_gb = bytes_to_gb(used_hbm[rank])\n            used_hbm_ratio = (\n                used_hbm[rank] / ((1 - reserved_hbm_percent) * device.storage.hbm)\n                if topology.compute_device == \"cuda\"\n                else 0\n            )\n            used_ddr_gb = bytes_to_gb(used_ddr[rank])\n            used_ddr_ratio = (\n                used_ddr[rank] / device.storage.ddr if device.storage.ddr > 0 else 0\n            )\n            for sharding_type in used_sharding_types:\n                if sharding_type not in stats[rank][\"type\"]:\n                    stats[rank][\"type\"][sharding_type] = 0\n\n            rank_hbm = f\"{round(used_hbm_gb, 1)} ({used_hbm_ratio:.0%})\"\n            rank_ddr = f\"{round(used_ddr_gb, 1)} ({used_ddr_ratio:.0%})\"\n            rank_perf = _format_perf_breakdown(perf[rank])\n            rank_input = f\"{round(stats[rank]['input_sizes'], 2)}\"\n            rank_output = f\"{round(stats[rank]['output_sizes'], 2)}\"\n            rank_shards = \" \".join(\n                f\"{sharding_type}: {num_tables}\"\n                for sharding_type, num_tables in sorted(stats[rank][\"type\"].items())\n            )\n            table.append(\n                [\n                    rank,\n                    rank_hbm,\n                    rank_ddr,\n                    rank_perf,\n                    rank_input,\n                    rank_output,\n                    rank_shards,\n                ]\n            )\n        formatted_table = _format_table(table)\n        self._width = max(self._width, len(formatted_table[0]) + 8)\n\n        if debug:\n            param_table: List[List[Union[str, int]]] = [\n                [\n                    \"FQN\",\n                    \"Sharding\",\n                    \"Compute Kernel\",\n                    \"Perf (ms)\",\n                    \"Pooling Factor\",\n                    \"Num Poolings\",\n                    \"Output\",\n                    \"Weighing\",\n                    \"Sharder\",\n                    \"Features\",\n                    \"Emb Dim (CW Dim)\",\n                    \"Hash Size\",\n                    \"Ranks\",\n                ],\n                [\n                    \"-----\",\n                    \"----------\",\n                    \"----------------\",\n                    \"-----------\",\n                    \"----------------\",\n                    \"--------------\",\n                    \"--------\",\n                    \"----------\",\n                    \"---------\",\n                    \"----------\",\n                    \"------------------\",\n                    \"-----------\",\n                    \"-------\",\n                ],\n            ]\n            feat_batch_sizes = [\n                constraints[so.name].batch_sizes\n                if constraints and constraints.get(so.name)\n                else None\n                for so in best_plan\n            ]\n\n            sharder_map: Dict[str, ModuleSharder[nn.Module]] = {\n                get_sharder_name(sharder.module_type): sharder\n                # pyre-ignore - this is a ModuleSharder below\n                for sharder in sharders\n                if sharders\n            }\n\n            if include_batch_sizes := any(feat_batch_sizes):\n                param_table[0].append(\"Batch Sizes\")\n                param_table[1].append(\"-------------\")\n            for i, so in enumerate(best_plan):\n                ranks = sorted([cast(int, shard.rank) for shard in so.shards])\n                ranks = _collapse_consecutive_ranks(ranks)\n\n                so_perf = Perf(fwd_compute=0, fwd_comms=0, bwd_compute=0, bwd_comms=0)\n                for shard in so.shards:\n                    so_perf += cast(Perf, shard.perf)\n\n                shard_perfs = _format_perf_breakdown(so_perf)\n\n                pooling_factor = str(round(sum(so.input_lengths), 3))\n                num_poolings = (\n                    cast(List[float], constraints[so.name].num_poolings)\n                    if constraints\n                    and constraints.get(so.name)\n                    and constraints[so.name].num_poolings\n                    else [NUM_POOLINGS] * len(so.input_lengths)\n                )\n                num_poolings = str(round(sum(num_poolings), 3))\n                output = \"pooled\" if so.is_pooled else \"sequence\"\n                weighing = \"weighted\" if so.is_weighted else \"unweighted\"\n                sharder = sharder_map.get(get_sharder_name(type(so.module[1])), None)\n                sharder_name = type(sharder).__name__\n                num_features = len(so.input_lengths)\n                embedding_dim = (\n                    f\"{so.tensor.shape[1]} ({so.shards[0].size[1]})\"\n                    if so.sharding_type == ShardingType.COLUMN_WISE.value\n                    or so.sharding_type == ShardingType.TABLE_COLUMN_WISE.value\n                    else f\"{so.tensor.shape[1]}\"\n                )\n                hash_size = so.tensor.shape[0]\n                param_table.append(\n                    [\n                        so.fqn,\n                        _get_sharding_type_abbr(so.sharding_type),\n                        so.compute_kernel,\n                        shard_perfs,\n                        pooling_factor,\n                        num_poolings,\n                        output,\n                        weighing,\n                        sharder_name,\n                        num_features,\n                        embedding_dim,\n                        hash_size,\n                        \",\".join(ranks),\n                    ]\n                )\n                if include_batch_sizes:\n                    bs = feat_batch_sizes[i]\n                    param_table[-1].append(_reduce_int_list(bs) if bs else \"n/a\")\n            formatted_param_table = _format_table(param_table)\n            self._width = max(self._width, len(formatted_param_table[0]) + 6)\n\n        self._stats_table.clear()\n        self._stats_table.append(\"#\" * self._width)\n        header_text = \"--- Planner Statistics ---\"\n        self._stats_table.append(f\"#{header_text: ^{self._width-2}}#\")\n\n        iter_text = (\n            f\"--- Evaluated {num_proposals} proposal(s), \"\n            f\"found {num_plans} possible plan(s), \"\n            f\"ran for {run_time:.2f}s ---\"\n        )\n        self._stats_table.append(f\"#{iter_text: ^{self._width-2}}#\")\n\n        divider = \"-\" * (self._width - 4)\n        self._stats_table.append(f\"#{divider: ^{self._width-2}}#\")\n\n        for row in formatted_table:\n            self._stats_table.append(f\"# {row: <{self._width-3}}#\")\n\n        perf_breakdown = \"Perf: Total perf (Forward compute, Forward comms, Backward compute, Backward comms)\"\n        legend = \"Input: MB/iteration, Output: MB/iteration, Shards: number of tables\"\n        hbm_info = \"HBM: estimated peak memory usage for shards, dense tensors, and features (KJT)\"\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {perf_breakdown: <{self._width-3}}#\")\n        self._stats_table.append(f\"# {legend: <{self._width-3}}#\")\n        self._stats_table.append(f\"# {hbm_info: <{self._width-3}}#\")\n\n        if debug:\n            self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n            self._stats_table.append(f\"# {'Parameter Info:' : <{self._width-3}}#\")\n            for row in formatted_param_table:\n                self._stats_table.append(f\"# {row: <{self._width-3}}#\")\n\n        batch_size_text = f\"Batch Size: {batch_size}\"\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {batch_size_text : <{self._width-3}}#\")\n\n        self._log_compute_kernel_stats(compute_kernels_to_count)\n\n        if debug:\n            self._log_max_perf_and_max_hbm(perf, used_hbm)\n            self._log_storage_reservation_stats(\n                storage_reservation,\n                topology,\n                reserved_hbm_percent,\n                dense_storage,\n                kjt_storage,\n            )\n\n        self._stats_table.append(\"#\" * self._width)\n\n        for row in self._stats_table:\n            logger.info(row)\n\n    def _get_shard_stats(\n        self,\n        shard: ParameterSharding,\n        sharding_option: ShardingOption,\n        world_size: int,\n        local_world_size: int,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> Tuple[List[int], List[float], List[float]]:\n        \"\"\"\n        Gets ranks, input sizes, and output sizes per shard.\n        Input size is a function of pooling factor.\n        Output size is a function of embedding dimension * number of features.\n\n        Returns:\n            ranks: list of ranks.\n            input_sizes: input size per iter in MB across ranks for given shard.\n            output_sizes: output size per iter in MB across ranks for given shard.\n        \"\"\"\n        assert shard.ranks\n        ranks = shard.ranks\n\n        num_poolings = (\n            cast(List[float], constraints[sharding_option.name].num_poolings)\n            if constraints\n            and constraints.get(sharding_option.name)\n            and constraints[sharding_option.name].num_poolings\n            else [1.0] * sharding_option.num_inputs\n        )\n        batch_sizes = (\n            cast(List[int], constraints[sharding_option.name].batch_sizes)\n            if constraints\n            and constraints.get(sharding_option.name)\n            and constraints[sharding_option.name].batch_sizes\n            else [sharding_option.batch_size] * sharding_option.num_inputs\n        )\n        input_data_type_size = BIGINT_DTYPE\n        output_data_type_size = sharding_option.tensor.element_size()\n\n        input_sizes, output_sizes = _calculate_shard_io_sizes(\n            sharding_type=sharding_option.sharding_type,\n            batch_sizes=batch_sizes,\n            world_size=world_size,\n            local_world_size=local_world_size,\n            input_lengths=sharding_option.input_lengths,\n            emb_dim=sharding_option.tensor.shape[1],\n            shard_sizes=[shard.size for shard in sharding_option.shards],\n            input_data_type_size=input_data_type_size,\n            output_data_type_size=output_data_type_size,\n            num_poolings=num_poolings,\n            is_pooled=sharding_option.is_pooled,\n        )\n\n        input_sizes = [bytes_to_mb(input_size) for input_size in input_sizes]\n        output_sizes = [bytes_to_mb(output_size) for output_size in output_sizes]\n\n        return ranks, input_sizes, output_sizes\n\n    def _log_max_perf_and_max_hbm(self, perfs: List[Perf], used_hbm: List[int]) -> None:\n\n        max_total_perf_text = f\"Longest Critical Path (Maximum of Total Perf): {_generate_max_text([perf.total for perf in perfs])}\"\n        max_fwd_compute_perf_text = f\"Maximum of Forward Compute: {_generate_max_text([perf.fwd_compute for perf in perfs])}\"\n        max_fwd_comms_perf_text = f\"Maximum of Forward Comms: {_generate_max_text([perf.fwd_comms for perf in perfs])}\"\n        max_bwd_compute_perf_text = f\"Maximum of Backward Compute: {_generate_max_text([perf.bwd_compute for perf in perfs])}\"\n        max_bwd_comms_perf_text = f\"Maximum of Backward Comms: {_generate_max_text([perf.bwd_comms for perf in perfs])}\"\n\n        sum_of_maxima = (\n            max(perf.fwd_compute for perf in perfs)\n            + max(perf.fwd_comms for perf in perfs)\n            + max(perf.bwd_compute for perf in perfs)\n            + max(perf.bwd_comms for perf in perfs)\n        )\n        sum_of_maxima_text = f\"Sum of Maxima: {round(sum_of_maxima, 3)} ms\"\n\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {max_total_perf_text : <{self._width-3}}#\")\n        self._stats_table.append(f\"# {max_fwd_compute_perf_text : <{self._width-3}}#\")\n        self._stats_table.append(f\"# {max_fwd_comms_perf_text : <{self._width-3}}#\")\n        self._stats_table.append(f\"# {max_bwd_compute_perf_text : <{self._width-3}}#\")\n        self._stats_table.append(f\"# {max_bwd_comms_perf_text : <{self._width-3}}#\")\n        self._stats_table.append(f\"# {sum_of_maxima_text : <{self._width-3}}#\")\n\n        max_hbm = max(used_hbm)\n        max_hbm_indices = [i for i in range(len(used_hbm)) if used_hbm[i] == max_hbm]\n        rank_text = \"ranks\" if len(max_hbm_indices) > 1 else \"rank\"\n        max_hbm_indices = _collapse_consecutive_ranks(max_hbm_indices)\n        max_hbm_ranks = f\"{rank_text} {','.join(max_hbm_indices)}\"\n        peak_memory_pressure = f\"Peak Memory Pressure: {round(bytes_to_gb(max_hbm), 3)} GB on {max_hbm_ranks}\"\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {peak_memory_pressure : <{self._width-3}}#\")\n\n    def _log_storage_reservation_stats(\n        self,\n        storage_reservation: StorageReservation,\n        topology: Topology,\n        reserved_hbm_percent: float,\n        dense_storage: Storage,\n        kjt_storage: Storage,\n    ) -> None:\n        device_storage = topology.devices[0].storage\n        usable_hbm = round(\n            bytes_to_gb(int((1 - reserved_hbm_percent) * device_storage.hbm)), 3\n        )\n        usable_ddr = round(bytes_to_gb(int(device_storage.ddr)), 3)\n        usable_memory = f\"HBM: {usable_hbm} GB, DDR: {usable_ddr} GB\"\n        usable_hbm_percentage = (\n            f\"Percent of Total HBM: {(1 - reserved_hbm_percent):.0%}\"\n        )\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {'Usable Memory:' : <{self._width-3}}#\")\n        self._stats_table.append(f\"#    {usable_memory : <{self._width-6}}#\")\n        self._stats_table.append(f\"#    {usable_hbm_percentage : <{self._width-6}}#\")\n\n        if isinstance(storage_reservation, HeuristicalStorageReservation):\n            dense_hbm = round(bytes_to_gb(dense_storage.hbm), 3)\n            dense_ddr = round(bytes_to_gb(dense_storage.ddr), 3)\n            dense_storage_text = f\"HBM: {dense_hbm} GB, DDR: {dense_ddr} GB\"\n            self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n            self._stats_table.append(\n                f\"# {'Dense Storage (per rank): ' : <{self._width-3}}#\"\n            )\n            self._stats_table.append(f\"#    {dense_storage_text : <{self._width-6}}#\")\n\n            kjt_hbm = round(bytes_to_gb(kjt_storage.hbm), 3)\n            kjt_ddr = round(bytes_to_gb(kjt_storage.ddr), 3)\n            kjt_storage_text = f\"HBM: {kjt_hbm} GB, DDR: {kjt_ddr} GB\"\n            self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n            self._stats_table.append(\n                f\"# {'KJT Storage (per rank): ' : <{self._width-3}}#\"\n            )\n            self._stats_table.append(f\"#    {kjt_storage_text : <{self._width-6}}#\")\n\n    def _log_compute_kernel_stats(\n        self, compute_kernels_to_count: Dict[str, int]\n    ) -> None:\n        compute_kernels_count = [\n            f\"{compute_kernel}: {count}\"\n            for compute_kernel, count in sorted(compute_kernels_to_count.items())\n        ]\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {'Compute Kernels:' : <{self._width-3}}#\")\n        for compute_kernel_count in compute_kernels_count:\n            self._stats_table.append(f\"#    {compute_kernel_count : <{self._width-6}}#\")",
  "def _generate_max_text(perfs: List[float]) -> str:\n    max_perf = max(perfs)\n\n    max_perf_indices = [i for i in range(len(perfs)) if perfs[i] == max_perf]\n    rank_text = \"ranks\" if len(max_perf_indices) > 1 else \"rank\"\n    max_perf_indices = _collapse_consecutive_ranks(max_perf_indices)\n    max_perf_ranks = f\"{rank_text} {','.join(max_perf_indices)}\"\n\n    return f\"{round(max_perf, 3)} ms on {max_perf_ranks}\"",
  "def _get_sharding_type_abbr(sharding_type: str) -> str:\n    if sharding_type == ShardingType.DATA_PARALLEL.value:\n        return \"DP\"\n    elif sharding_type == ShardingType.TABLE_WISE.value:\n        return \"TW\"\n    elif sharding_type == ShardingType.COLUMN_WISE.value:\n        return \"CW\"\n    elif sharding_type == ShardingType.ROW_WISE.value:\n        return \"RW\"\n    elif sharding_type == ShardingType.TABLE_ROW_WISE.value:\n        return \"TWRW\"\n    elif sharding_type == ShardingType.TABLE_COLUMN_WISE.value:\n        return \"TWCW\"\n    else:\n        raise ValueError(\n            f\"Unrecognized or unsupported sharding type provided: {sharding_type}\"\n        )",
  "def _format_perf_breakdown(perf: Perf) -> str:\n    breakdown = [\n        perf.fwd_compute,\n        perf.fwd_comms,\n        perf.bwd_compute,\n        perf.bwd_comms,\n    ]\n    breakdown_string = \",\".join(\n        [str(round(num)) if num >= 1 else round_to_one_sigfig(num) for num in breakdown]\n    )\n\n    return f\"{str(round(perf.total, 3))} ({breakdown_string})\"",
  "def round_to_one_sigfig(x: float) -> str:\n    return f'{float(f\"{x:.1g}\"):g}'",
  "def _format_table(table: List[List[Union[str, int]]]) -> List[str]:\n    longest_cols = [\n        (max([len(str(row[i])) for row in table]) + 3) for i in range(len(table[0]))\n    ]\n    row_format = \"\".join(\n        [\"{:>\" + str(longest_col) + \"}\" for longest_col in longest_cols]\n    )\n    return [row_format.format(*row) for row in table]",
  "def _collapse_consecutive_ranks(ranks: List[int]) -> List[str]:\n    if len(ranks) > 1 and ranks == list(range(min(ranks), max(ranks) + 1)):\n        return [f\"{min(ranks)}-{max(ranks)}\"]\n    else:\n        return [str(rank) for rank in ranks]",
  "def _reduce_int_list(input_list: List[int]) -> str:\n    if len(input_list) == 0:\n        return \"\"\n    reduced = []\n    count = 1\n    prev_num = input_list[0]\n\n    for num in input_list[1:]:\n        if num == prev_num:\n            count += 1\n        else:\n            if count > 1:\n                reduced.append(f\"{prev_num} * {count}\")\n            else:\n                reduced.append(str(prev_num))\n            prev_num = num\n            count = 1\n\n    # Handle the last number\n    if count > 1:\n        reduced.append(f\"{prev_num}*{count}\")\n    else:\n        reduced.append(str(prev_num))\n\n    return \", \".join(reduced)",
  "def __init__(self) -> None:\n        self._width: int = MIN_WIDTH\n        self._stats_table: List[str] = []",
  "def log(\n        self,\n        sharding_plan: ShardingPlan,\n        topology: Topology,\n        batch_size: int,\n        storage_reservation: StorageReservation,\n        num_proposals: int,\n        num_plans: int,\n        run_time: float,\n        best_plan: List[ShardingOption],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        sharders: Optional[List[ModuleSharder[nn.Module]]] = None,\n        debug: bool = True,\n    ) -> None:\n        \"\"\"\n        Logs stats for a given sharding plan.\n\n        Provides a tabular view of stats for the given sharding plan with per device\n        storage usage (HBM and DDR), perf, input, output, and number/type of shards.\n\n        Args:\n            sharding_plan (ShardingPlan): sharding plan chosen by the planner.\n            topology (Topology): device topology.\n            batch_size (int): batch size.\n            storage_reservation (StorageReservation): reserves storage for unsharded\n                parts of the model\n            num_proposals (int): number of proposals evaluated.\n            num_plans (int): number of proposals successfully partitioned.\n            run_time (float): time taken to find plan (in seconds).\n            best_plan (List[ShardingOption]): plan with expected performance.\n            constraints (Optional[Dict[str, ParameterConstraints]]): dict of parameter\n                names to provided ParameterConstraints.\n            debug (bool): whether to enable debug mode.\n        \"\"\"\n\n        shard_by_fqn = {\n            module_name + \".\" + param_name: value\n            for module_name, param_dict in sharding_plan.plan.items()\n            # pyre-ignore - this is a EmbeddingShardingPlan below\n            for param_name, value in param_dict.items()\n        }\n        stats: Dict[int, Dict[str, Any]] = {\n            rank: {\"type\": {}, \"input_sizes\": 0.0, \"output_sizes\": 0.0}\n            for rank in range(topology.world_size)\n        }\n\n        used_sharding_types = set()\n        compute_kernels_to_count = defaultdict(int)\n\n        reserved_hbm_percent = (\n            storage_reservation._percentage\n            if isinstance(\n                storage_reservation,\n                (\n                    FixedPercentageStorageReservation,\n                    HeuristicalStorageReservation,\n                    InferenceStorageReservation,\n                ),\n            )\n            else 0.0\n        )\n        dense_storage = (\n            storage_reservation._dense_storage\n            if isinstance(\n                storage_reservation,\n                (HeuristicalStorageReservation, InferenceStorageReservation),\n            )\n            and storage_reservation._dense_storage is not None\n            else Storage(0, 0)\n        )\n        assert dense_storage\n        kjt_storage = (\n            storage_reservation._kjt_storage\n            if isinstance(\n                storage_reservation,\n                (HeuristicalStorageReservation, InferenceStorageReservation),\n            )\n            and storage_reservation._kjt_storage\n            else Storage(0, 0)\n        )\n        assert kjt_storage\n\n        for sharding_option in best_plan:\n            fqn = sharding_option.fqn\n\n            if shard_by_fqn.get(fqn) is None:\n                continue\n            shard: ParameterSharding = shard_by_fqn[fqn]\n\n            ranks, input_sizes, output_sizes = self._get_shard_stats(\n                shard=shard,\n                sharding_option=sharding_option,\n                world_size=topology.world_size,\n                local_world_size=topology.local_world_size,\n                constraints=constraints,\n            )\n            sharding_type_abbr = _get_sharding_type_abbr(shard.sharding_type)\n            used_sharding_types.add(sharding_type_abbr)\n            compute_kernels_to_count[sharding_option.compute_kernel] += 1\n\n            for i, rank in enumerate(ranks):\n                count = stats[rank][\"type\"].get(sharding_type_abbr, 0)\n                stats[rank][\"type\"][sharding_type_abbr] = count + 1\n                stats[rank][\"input_sizes\"] += input_sizes[i]\n                stats[rank][\"output_sizes\"] += output_sizes[i]\n\n        used_hbm = [0] * topology.world_size\n        used_ddr = [0] * topology.world_size\n        perf = [\n            Perf(fwd_compute=0, fwd_comms=0, bwd_compute=0, bwd_comms=0)\n            for _ in range(topology.world_size)\n        ]\n        for sharding_option in best_plan:\n            for shard in sharding_option.shards:\n                shard_storage = cast(Storage, shard.storage)\n                rank = cast(int, shard.rank)\n                used_hbm[rank] += shard_storage.hbm\n                used_ddr[rank] += shard_storage.ddr\n                perf[rank] += cast(Perf, shard.perf)\n\n        used_hbm = [hbm + dense_storage.hbm + kjt_storage.hbm for hbm in used_hbm]\n        used_ddr = [ddr + dense_storage.ddr + kjt_storage.ddr for ddr in used_ddr]\n\n        table: List[List[Union[str, int]]] = [\n            [\n                \"Rank\",\n                \"HBM (GB)\",\n                \"DDR (GB)\",\n                \"Perf (ms)\",\n                \"Input (MB)\",\n                \"Output (MB)\",\n                \"Shards\",\n            ],\n            [\n                \"------\",\n                \"----------\",\n                \"----------\",\n                \"-----------\",\n                \"------------\",\n                \"-------------\",\n                \"--------\",\n            ],\n        ]\n\n        for rank, device in enumerate(topology.devices):\n            used_hbm_gb = bytes_to_gb(used_hbm[rank])\n            used_hbm_ratio = (\n                used_hbm[rank] / ((1 - reserved_hbm_percent) * device.storage.hbm)\n                if topology.compute_device == \"cuda\"\n                else 0\n            )\n            used_ddr_gb = bytes_to_gb(used_ddr[rank])\n            used_ddr_ratio = (\n                used_ddr[rank] / device.storage.ddr if device.storage.ddr > 0 else 0\n            )\n            for sharding_type in used_sharding_types:\n                if sharding_type not in stats[rank][\"type\"]:\n                    stats[rank][\"type\"][sharding_type] = 0\n\n            rank_hbm = f\"{round(used_hbm_gb, 1)} ({used_hbm_ratio:.0%})\"\n            rank_ddr = f\"{round(used_ddr_gb, 1)} ({used_ddr_ratio:.0%})\"\n            rank_perf = _format_perf_breakdown(perf[rank])\n            rank_input = f\"{round(stats[rank]['input_sizes'], 2)}\"\n            rank_output = f\"{round(stats[rank]['output_sizes'], 2)}\"\n            rank_shards = \" \".join(\n                f\"{sharding_type}: {num_tables}\"\n                for sharding_type, num_tables in sorted(stats[rank][\"type\"].items())\n            )\n            table.append(\n                [\n                    rank,\n                    rank_hbm,\n                    rank_ddr,\n                    rank_perf,\n                    rank_input,\n                    rank_output,\n                    rank_shards,\n                ]\n            )\n        formatted_table = _format_table(table)\n        self._width = max(self._width, len(formatted_table[0]) + 8)\n\n        if debug:\n            param_table: List[List[Union[str, int]]] = [\n                [\n                    \"FQN\",\n                    \"Sharding\",\n                    \"Compute Kernel\",\n                    \"Perf (ms)\",\n                    \"Pooling Factor\",\n                    \"Num Poolings\",\n                    \"Output\",\n                    \"Weighing\",\n                    \"Sharder\",\n                    \"Features\",\n                    \"Emb Dim (CW Dim)\",\n                    \"Hash Size\",\n                    \"Ranks\",\n                ],\n                [\n                    \"-----\",\n                    \"----------\",\n                    \"----------------\",\n                    \"-----------\",\n                    \"----------------\",\n                    \"--------------\",\n                    \"--------\",\n                    \"----------\",\n                    \"---------\",\n                    \"----------\",\n                    \"------------------\",\n                    \"-----------\",\n                    \"-------\",\n                ],\n            ]\n            feat_batch_sizes = [\n                constraints[so.name].batch_sizes\n                if constraints and constraints.get(so.name)\n                else None\n                for so in best_plan\n            ]\n\n            sharder_map: Dict[str, ModuleSharder[nn.Module]] = {\n                get_sharder_name(sharder.module_type): sharder\n                # pyre-ignore - this is a ModuleSharder below\n                for sharder in sharders\n                if sharders\n            }\n\n            if include_batch_sizes := any(feat_batch_sizes):\n                param_table[0].append(\"Batch Sizes\")\n                param_table[1].append(\"-------------\")\n            for i, so in enumerate(best_plan):\n                ranks = sorted([cast(int, shard.rank) for shard in so.shards])\n                ranks = _collapse_consecutive_ranks(ranks)\n\n                so_perf = Perf(fwd_compute=0, fwd_comms=0, bwd_compute=0, bwd_comms=0)\n                for shard in so.shards:\n                    so_perf += cast(Perf, shard.perf)\n\n                shard_perfs = _format_perf_breakdown(so_perf)\n\n                pooling_factor = str(round(sum(so.input_lengths), 3))\n                num_poolings = (\n                    cast(List[float], constraints[so.name].num_poolings)\n                    if constraints\n                    and constraints.get(so.name)\n                    and constraints[so.name].num_poolings\n                    else [NUM_POOLINGS] * len(so.input_lengths)\n                )\n                num_poolings = str(round(sum(num_poolings), 3))\n                output = \"pooled\" if so.is_pooled else \"sequence\"\n                weighing = \"weighted\" if so.is_weighted else \"unweighted\"\n                sharder = sharder_map.get(get_sharder_name(type(so.module[1])), None)\n                sharder_name = type(sharder).__name__\n                num_features = len(so.input_lengths)\n                embedding_dim = (\n                    f\"{so.tensor.shape[1]} ({so.shards[0].size[1]})\"\n                    if so.sharding_type == ShardingType.COLUMN_WISE.value\n                    or so.sharding_type == ShardingType.TABLE_COLUMN_WISE.value\n                    else f\"{so.tensor.shape[1]}\"\n                )\n                hash_size = so.tensor.shape[0]\n                param_table.append(\n                    [\n                        so.fqn,\n                        _get_sharding_type_abbr(so.sharding_type),\n                        so.compute_kernel,\n                        shard_perfs,\n                        pooling_factor,\n                        num_poolings,\n                        output,\n                        weighing,\n                        sharder_name,\n                        num_features,\n                        embedding_dim,\n                        hash_size,\n                        \",\".join(ranks),\n                    ]\n                )\n                if include_batch_sizes:\n                    bs = feat_batch_sizes[i]\n                    param_table[-1].append(_reduce_int_list(bs) if bs else \"n/a\")\n            formatted_param_table = _format_table(param_table)\n            self._width = max(self._width, len(formatted_param_table[0]) + 6)\n\n        self._stats_table.clear()\n        self._stats_table.append(\"#\" * self._width)\n        header_text = \"--- Planner Statistics ---\"\n        self._stats_table.append(f\"#{header_text: ^{self._width-2}}#\")\n\n        iter_text = (\n            f\"--- Evaluated {num_proposals} proposal(s), \"\n            f\"found {num_plans} possible plan(s), \"\n            f\"ran for {run_time:.2f}s ---\"\n        )\n        self._stats_table.append(f\"#{iter_text: ^{self._width-2}}#\")\n\n        divider = \"-\" * (self._width - 4)\n        self._stats_table.append(f\"#{divider: ^{self._width-2}}#\")\n\n        for row in formatted_table:\n            self._stats_table.append(f\"# {row: <{self._width-3}}#\")\n\n        perf_breakdown = \"Perf: Total perf (Forward compute, Forward comms, Backward compute, Backward comms)\"\n        legend = \"Input: MB/iteration, Output: MB/iteration, Shards: number of tables\"\n        hbm_info = \"HBM: estimated peak memory usage for shards, dense tensors, and features (KJT)\"\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {perf_breakdown: <{self._width-3}}#\")\n        self._stats_table.append(f\"# {legend: <{self._width-3}}#\")\n        self._stats_table.append(f\"# {hbm_info: <{self._width-3}}#\")\n\n        if debug:\n            self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n            self._stats_table.append(f\"# {'Parameter Info:' : <{self._width-3}}#\")\n            for row in formatted_param_table:\n                self._stats_table.append(f\"# {row: <{self._width-3}}#\")\n\n        batch_size_text = f\"Batch Size: {batch_size}\"\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {batch_size_text : <{self._width-3}}#\")\n\n        self._log_compute_kernel_stats(compute_kernels_to_count)\n\n        if debug:\n            self._log_max_perf_and_max_hbm(perf, used_hbm)\n            self._log_storage_reservation_stats(\n                storage_reservation,\n                topology,\n                reserved_hbm_percent,\n                dense_storage,\n                kjt_storage,\n            )\n\n        self._stats_table.append(\"#\" * self._width)\n\n        for row in self._stats_table:\n            logger.info(row)",
  "def _get_shard_stats(\n        self,\n        shard: ParameterSharding,\n        sharding_option: ShardingOption,\n        world_size: int,\n        local_world_size: int,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> Tuple[List[int], List[float], List[float]]:\n        \"\"\"\n        Gets ranks, input sizes, and output sizes per shard.\n        Input size is a function of pooling factor.\n        Output size is a function of embedding dimension * number of features.\n\n        Returns:\n            ranks: list of ranks.\n            input_sizes: input size per iter in MB across ranks for given shard.\n            output_sizes: output size per iter in MB across ranks for given shard.\n        \"\"\"\n        assert shard.ranks\n        ranks = shard.ranks\n\n        num_poolings = (\n            cast(List[float], constraints[sharding_option.name].num_poolings)\n            if constraints\n            and constraints.get(sharding_option.name)\n            and constraints[sharding_option.name].num_poolings\n            else [1.0] * sharding_option.num_inputs\n        )\n        batch_sizes = (\n            cast(List[int], constraints[sharding_option.name].batch_sizes)\n            if constraints\n            and constraints.get(sharding_option.name)\n            and constraints[sharding_option.name].batch_sizes\n            else [sharding_option.batch_size] * sharding_option.num_inputs\n        )\n        input_data_type_size = BIGINT_DTYPE\n        output_data_type_size = sharding_option.tensor.element_size()\n\n        input_sizes, output_sizes = _calculate_shard_io_sizes(\n            sharding_type=sharding_option.sharding_type,\n            batch_sizes=batch_sizes,\n            world_size=world_size,\n            local_world_size=local_world_size,\n            input_lengths=sharding_option.input_lengths,\n            emb_dim=sharding_option.tensor.shape[1],\n            shard_sizes=[shard.size for shard in sharding_option.shards],\n            input_data_type_size=input_data_type_size,\n            output_data_type_size=output_data_type_size,\n            num_poolings=num_poolings,\n            is_pooled=sharding_option.is_pooled,\n        )\n\n        input_sizes = [bytes_to_mb(input_size) for input_size in input_sizes]\n        output_sizes = [bytes_to_mb(output_size) for output_size in output_sizes]\n\n        return ranks, input_sizes, output_sizes",
  "def _log_max_perf_and_max_hbm(self, perfs: List[Perf], used_hbm: List[int]) -> None:\n\n        max_total_perf_text = f\"Longest Critical Path (Maximum of Total Perf): {_generate_max_text([perf.total for perf in perfs])}\"\n        max_fwd_compute_perf_text = f\"Maximum of Forward Compute: {_generate_max_text([perf.fwd_compute for perf in perfs])}\"\n        max_fwd_comms_perf_text = f\"Maximum of Forward Comms: {_generate_max_text([perf.fwd_comms for perf in perfs])}\"\n        max_bwd_compute_perf_text = f\"Maximum of Backward Compute: {_generate_max_text([perf.bwd_compute for perf in perfs])}\"\n        max_bwd_comms_perf_text = f\"Maximum of Backward Comms: {_generate_max_text([perf.bwd_comms for perf in perfs])}\"\n\n        sum_of_maxima = (\n            max(perf.fwd_compute for perf in perfs)\n            + max(perf.fwd_comms for perf in perfs)\n            + max(perf.bwd_compute for perf in perfs)\n            + max(perf.bwd_comms for perf in perfs)\n        )\n        sum_of_maxima_text = f\"Sum of Maxima: {round(sum_of_maxima, 3)} ms\"\n\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {max_total_perf_text : <{self._width-3}}#\")\n        self._stats_table.append(f\"# {max_fwd_compute_perf_text : <{self._width-3}}#\")\n        self._stats_table.append(f\"# {max_fwd_comms_perf_text : <{self._width-3}}#\")\n        self._stats_table.append(f\"# {max_bwd_compute_perf_text : <{self._width-3}}#\")\n        self._stats_table.append(f\"# {max_bwd_comms_perf_text : <{self._width-3}}#\")\n        self._stats_table.append(f\"# {sum_of_maxima_text : <{self._width-3}}#\")\n\n        max_hbm = max(used_hbm)\n        max_hbm_indices = [i for i in range(len(used_hbm)) if used_hbm[i] == max_hbm]\n        rank_text = \"ranks\" if len(max_hbm_indices) > 1 else \"rank\"\n        max_hbm_indices = _collapse_consecutive_ranks(max_hbm_indices)\n        max_hbm_ranks = f\"{rank_text} {','.join(max_hbm_indices)}\"\n        peak_memory_pressure = f\"Peak Memory Pressure: {round(bytes_to_gb(max_hbm), 3)} GB on {max_hbm_ranks}\"\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {peak_memory_pressure : <{self._width-3}}#\")",
  "def _log_storage_reservation_stats(\n        self,\n        storage_reservation: StorageReservation,\n        topology: Topology,\n        reserved_hbm_percent: float,\n        dense_storage: Storage,\n        kjt_storage: Storage,\n    ) -> None:\n        device_storage = topology.devices[0].storage\n        usable_hbm = round(\n            bytes_to_gb(int((1 - reserved_hbm_percent) * device_storage.hbm)), 3\n        )\n        usable_ddr = round(bytes_to_gb(int(device_storage.ddr)), 3)\n        usable_memory = f\"HBM: {usable_hbm} GB, DDR: {usable_ddr} GB\"\n        usable_hbm_percentage = (\n            f\"Percent of Total HBM: {(1 - reserved_hbm_percent):.0%}\"\n        )\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {'Usable Memory:' : <{self._width-3}}#\")\n        self._stats_table.append(f\"#    {usable_memory : <{self._width-6}}#\")\n        self._stats_table.append(f\"#    {usable_hbm_percentage : <{self._width-6}}#\")\n\n        if isinstance(storage_reservation, HeuristicalStorageReservation):\n            dense_hbm = round(bytes_to_gb(dense_storage.hbm), 3)\n            dense_ddr = round(bytes_to_gb(dense_storage.ddr), 3)\n            dense_storage_text = f\"HBM: {dense_hbm} GB, DDR: {dense_ddr} GB\"\n            self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n            self._stats_table.append(\n                f\"# {'Dense Storage (per rank): ' : <{self._width-3}}#\"\n            )\n            self._stats_table.append(f\"#    {dense_storage_text : <{self._width-6}}#\")\n\n            kjt_hbm = round(bytes_to_gb(kjt_storage.hbm), 3)\n            kjt_ddr = round(bytes_to_gb(kjt_storage.ddr), 3)\n            kjt_storage_text = f\"HBM: {kjt_hbm} GB, DDR: {kjt_ddr} GB\"\n            self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n            self._stats_table.append(\n                f\"# {'KJT Storage (per rank): ' : <{self._width-3}}#\"\n            )\n            self._stats_table.append(f\"#    {kjt_storage_text : <{self._width-6}}#\")",
  "def _log_compute_kernel_stats(\n        self, compute_kernels_to_count: Dict[str, int]\n    ) -> None:\n        compute_kernels_count = [\n            f\"{compute_kernel}: {count}\"\n            for compute_kernel, count in sorted(compute_kernels_to_count.items())\n        ]\n        self._stats_table.append(f\"#{'' : ^{self._width-2}}#\")\n        self._stats_table.append(f\"# {'Compute Kernels:' : <{self._width-3}}#\")\n        for compute_kernel_count in compute_kernels_count:\n            self._stats_table.append(f\"#    {compute_kernel_count : <{self._width-6}}#\")",
  "class EmbeddingPerfEstimator(ShardEstimator):\n    \"\"\"\n    Embedding Wall Time Perf Estimator\n    \"\"\"\n\n    def __init__(\n        self,\n        topology: Topology,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        is_inference: bool = False,\n    ) -> None:\n        self._topology = topology\n        self._constraints = constraints\n        self._is_inference = is_inference\n\n    def estimate(\n        self,\n        sharding_options: List[ShardingOption],\n        sharder_map: Optional[Dict[str, ModuleSharder[nn.Module]]] = None,\n    ) -> None:\n        if not sharder_map:\n            assert not sharding_options, \"sharder_map not provided for sharding_options\"\n            return\n\n        for sharding_option in sharding_options:\n            sharder_key = sharder_name(type(sharding_option.module[1]))\n            sharder = sharder_map[sharder_key]\n\n            caching_ratio = (\n                self._constraints[  # pyre-ignore[16]\n                    sharding_option.name\n                ].cache_params.load_factor\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].cache_params\n                else None\n            )\n            # TODO: remove after deprecating fused_params in sharder\n            if caching_ratio is None:\n                caching_ratio = (\n                    sharder.fused_params.get(\"cache_load_factor\")  # pyre-ignore[16]\n                    if hasattr(sharder, \"fused_params\") and sharder.fused_params\n                    else None\n                )\n\n            num_poolings = (\n                cast(List[float], self._constraints[sharding_option.name].num_poolings)\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].num_poolings\n                else [1.0] * sharding_option.num_inputs\n            )\n            batch_sizes = (\n                cast(List[int], self._constraints[sharding_option.name].batch_sizes)\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].batch_sizes\n                else [sharding_option.batch_size] * sharding_option.num_inputs\n            )\n\n            assert (\n                len(sharding_option.input_lengths)\n                == len(num_poolings)\n                == len(batch_sizes)\n            ), \"Provided `pooling_factors`, `num_poolings`, and `batch_sizes` constraints must match.\"\n\n            module = sharding_option.module[1]\n\n            # TODO remove this hack once feature processor is disaggregated\n            has_feature_processor = False\n            if (\n                hasattr(module, \"_feature_processor\")\n                and hasattr(module._feature_processor, \"feature_processor_modules\")\n                and isinstance(\n                    module._feature_processor.feature_processor_modules,\n                    nn.ModuleDict,\n                )\n                and sharding_option.name\n                in module._feature_processor.feature_processor_modules.keys()\n            ):\n                has_feature_processor = True\n                logger.info(f\"Table {sharding_option.name} has feature processor.\")\n\n            if isinstance(module, EmbeddingBagCollectionInterface):\n                is_weighted = module.is_weighted()\n            elif (\n                self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].is_weighted\n            ):\n                is_weighted = self._constraints[sharding_option.name].is_weighted\n            else:\n                is_weighted = False\n\n            # TODO remove this once migrate away from PEA\n            is_weighted = is_weighted or has_feature_processor\n            sharding_option.is_weighted = is_weighted\n\n            table_data_type_size = sharding_option.tensor.element_size()\n            (\n                fwd_a2a_comm_data_type_size,\n                bwd_a2a_comm_data_type_size,\n                fwd_sr_comm_data_type_size,\n                bwd_sr_comm_data_type_size,\n            ) = _extract_comm_data_type_size(sharder, sharding_option)\n\n            shard_perfs = perf_func_emb_wall_time(\n                shard_sizes=[shard.size for shard in sharding_option.shards],\n                compute_kernel=sharding_option.compute_kernel,\n                compute_device=self._topology.compute_device,\n                sharding_type=sharding_option.sharding_type,\n                batch_sizes=batch_sizes,\n                world_size=self._topology.world_size,\n                local_world_size=self._topology.local_world_size,\n                input_lengths=sharding_option.input_lengths,\n                input_data_type_size=BIGINT_DTYPE,\n                table_data_type_size=table_data_type_size,\n                fwd_a2a_comm_data_type_size=fwd_a2a_comm_data_type_size,\n                bwd_a2a_comm_data_type_size=bwd_a2a_comm_data_type_size,\n                fwd_sr_comm_data_type_size=fwd_sr_comm_data_type_size,\n                bwd_sr_comm_data_type_size=bwd_sr_comm_data_type_size,\n                num_poolings=num_poolings,\n                hbm_mem_bw=self._topology.hbm_mem_bw,\n                ddr_mem_bw=self._topology.ddr_mem_bw,\n                intra_host_bw=self._topology.intra_host_bw,\n                inter_host_bw=self._topology.inter_host_bw,\n                is_pooled=sharding_option.is_pooled,\n                is_weighted=is_weighted,\n                is_inference=self._is_inference,\n                caching_ratio=caching_ratio,\n            )\n\n            for shard, perf in zip(sharding_option.shards, shard_perfs):\n                shard.perf = perf",
  "def perf_func_emb_wall_time(\n    shard_sizes: List[List[int]],\n    compute_kernel: str,\n    compute_device: str,\n    sharding_type: str,\n    batch_sizes: List[int],\n    world_size: int,\n    local_world_size: int,\n    input_lengths: List[float],\n    input_data_type_size: float,\n    table_data_type_size: float,\n    fwd_a2a_comm_data_type_size: float,\n    bwd_a2a_comm_data_type_size: float,\n    fwd_sr_comm_data_type_size: float,\n    bwd_sr_comm_data_type_size: float,\n    num_poolings: List[float],\n    hbm_mem_bw: float,\n    ddr_mem_bw: float,\n    intra_host_bw: float,\n    inter_host_bw: float,\n    is_pooled: bool,\n    is_weighted: bool = False,\n    caching_ratio: Optional[float] = None,\n    is_inference: bool = False,\n) -> List[Perf]:\n    \"\"\"\n    Attempts to model perfs as a function of relative wall times.\n\n    Args:\n        shard_sizes (List[List[int]]): the list of (local_rows, local_cols) of each\n            shard.\n        compute_kernel (str): compute kernel.\n        compute_device (str): compute device.\n        sharding_type (str): tw, rw, cw, twrw, dp.\n        batch_sizes (List[int]): batch size for each input feature.\n        world_size (int): the number of devices for all hosts.\n        local_world_size (int): the number of the device for each host.\n        input_lengths (List[float]): the list of the average number of lookups of each\n            input query feature.\n        input_data_type_size (float): the data type size of the distributed\n            data_parallel input.\n        table_data_type_size (float): the data type size of the table.\n        fwd_comm_data_type_size (float): the data type size of the distributed\n            data_parallel input during forward communication.\n        bwd_comm_data_type_size (float): the data type size of the distributed\n            data_parallel input during backward communication.\n        num_poolings (List[float]): number of poolings per sample, typically 1.0.\n        hbm_mem_bw (float): the bandwidth of the device HBM.\n        ddr_mem_bw (float): the bandwidth of the system DDR memory.\n        intra_host_bw (float): the bandwidth within a single host like multiple threads.\n        inter_host_bw (float): the bandwidth between two hosts like multiple machines.\n        is_pooled (bool): True if embedding output is pooled (ie. `EmbeddingBag`), False\n            if unpooled/sequential (ie. `Embedding`).\n        is_weighted (bool = False): if the module is an EBC and is weighted, typically\n            signifying an id score list feature.\n        is_inference (bool = False): if planning for inference.\n        caching_ratio (Optional[float] = None): cache ratio to determine the bandwidth\n            of device.\n\n    Returns:\n        List[float]: the list of perf for each shard.\n    \"\"\"\n\n    shard_perfs = []\n    device_bw = kernel_bw_lookup(\n        compute_device, compute_kernel, hbm_mem_bw, ddr_mem_bw, caching_ratio\n    )\n    if device_bw is None:\n        raise PlannerError(\n            f\"No kernel bandwidth exists for this combo of compute device: {compute_device}, compute kernel: {compute_kernel}\"\n        )\n\n    for hash_size, emb_dim in shard_sizes:\n        if (\n            sharding_type == ShardingType.TABLE_WISE.value\n            or sharding_type == ShardingType.COLUMN_WISE.value\n            or sharding_type == ShardingType.TABLE_COLUMN_WISE.value\n        ):\n            shard_perf = _get_tw_sharding_perf(\n                batch_sizes=batch_sizes,\n                world_size=world_size,\n                local_world_size=local_world_size,\n                input_lengths=input_lengths,\n                emb_dim=emb_dim,\n                input_data_type_size=input_data_type_size,\n                table_data_type_size=table_data_type_size,\n                fwd_a2a_comm_data_type_size=fwd_a2a_comm_data_type_size,\n                bwd_a2a_comm_data_type_size=bwd_a2a_comm_data_type_size,\n                num_poolings=num_poolings,\n                device_bw=device_bw,\n                inter_host_bw=inter_host_bw,\n                intra_host_bw=intra_host_bw,\n                is_pooled=is_pooled,\n                is_weighted=is_weighted,\n                is_inference=is_inference,\n            )\n        elif sharding_type == ShardingType.ROW_WISE.value:\n            shard_perf = _get_rw_sharding_perf(\n                batch_sizes=batch_sizes,\n                world_size=world_size,\n                local_world_size=local_world_size,\n                input_lengths=input_lengths,\n                emb_dim=emb_dim,\n                input_data_type_size=input_data_type_size,\n                table_data_type_size=table_data_type_size,\n                fwd_a2a_comm_data_type_size=fwd_a2a_comm_data_type_size,\n                bwd_a2a_comm_data_type_size=bwd_a2a_comm_data_type_size,\n                fwd_sr_comm_data_type_size=fwd_sr_comm_data_type_size,\n                bwd_sr_comm_data_type_size=bwd_sr_comm_data_type_size,\n                num_poolings=num_poolings,\n                device_bw=device_bw,\n                inter_host_bw=inter_host_bw,\n                intra_host_bw=intra_host_bw,\n                is_pooled=is_pooled,\n                is_weighted=is_weighted,\n            )\n        elif sharding_type == ShardingType.TABLE_ROW_WISE.value:\n            shard_perf = _get_twrw_sharding_perf(\n                batch_sizes=batch_sizes,\n                world_size=world_size,\n                local_world_size=local_world_size,\n                input_lengths=input_lengths,\n                emb_dim=emb_dim,\n                input_data_type_size=input_data_type_size,\n                table_data_type_size=table_data_type_size,\n                fwd_a2a_comm_data_type_size=fwd_a2a_comm_data_type_size,\n                bwd_a2a_comm_data_type_size=bwd_a2a_comm_data_type_size,\n                fwd_sr_comm_data_type_size=fwd_sr_comm_data_type_size,\n                bwd_sr_comm_data_type_size=bwd_sr_comm_data_type_size,\n                num_poolings=num_poolings,\n                device_bw=device_bw,\n                inter_host_bw=inter_host_bw,\n                intra_host_bw=intra_host_bw,\n                is_pooled=is_pooled,\n                is_weighted=is_weighted,\n            )\n        elif sharding_type == ShardingType.DATA_PARALLEL.value:\n            shard_perf = _get_dp_sharding_perf(\n                batch_sizes=batch_sizes,\n                world_size=world_size,\n                local_world_size=local_world_size,\n                input_lengths=input_lengths,\n                grad_num_elem=hash_size * emb_dim,\n                emb_dim=emb_dim,\n                input_data_type_size=input_data_type_size,\n                table_data_type_size=table_data_type_size,\n                num_poolings=num_poolings,\n                device_bw=device_bw,\n                inter_host_bw=inter_host_bw,\n                is_pooled=is_pooled,\n                is_weighted=is_weighted,\n            )\n        else:\n            raise ValueError(\n                f\"Unrecognized or unsupported sharding type provided: {sharding_type}\"\n            )\n        shard_perfs.append(shard_perf)\n\n    return shard_perfs",
  "def _get_tw_sharding_perf(\n    batch_sizes: List[int],\n    world_size: int,\n    local_world_size: int,\n    input_lengths: List[float],\n    emb_dim: int,\n    input_data_type_size: float,\n    table_data_type_size: float,\n    fwd_a2a_comm_data_type_size: float,\n    bwd_a2a_comm_data_type_size: float,\n    num_poolings: List[float],\n    device_bw: float,\n    inter_host_bw: float,\n    intra_host_bw: float,\n    is_pooled: bool,\n    is_weighted: bool = False,\n    is_inference: bool = False,\n) -> Perf:\n    batch_inputs = sum(\n        [x * y * z for x, y, z in zip(input_lengths, num_poolings, batch_sizes)]\n    )\n    batch_outputs = (\n        sum([x * y for x, y in zip(num_poolings, batch_sizes)])\n        if is_pooled\n        else batch_inputs\n    )\n\n    input_read_size = math.ceil(batch_inputs * world_size * input_data_type_size)\n    if is_weighted:\n        input_read_size *= 2\n\n    # minimum embedding dim is set to 32 due to kernel usage\n    embedding_lookup_size = (\n        batch_inputs * world_size * max(emb_dim, 32) * table_data_type_size\n    )\n\n    fwd_output_write_size = (\n        batch_outputs * world_size * emb_dim * fwd_a2a_comm_data_type_size\n    )\n    bwd_output_write_size = (\n        batch_outputs * world_size * emb_dim * bwd_a2a_comm_data_type_size\n    )\n\n    # embedding dim below 128 will reduce kernel efficency\n    block_usage_penalty = 1\n    if emb_dim < FULL_BLOCK_EMB_DIM:\n        if emb_dim >= 64:\n            block_usage_penalty = HALF_BLOCK_PENALTY\n        else:  # emb_dim >= 32\n            block_usage_penalty = QUARTER_BLOCK_PENALTY\n\n    comms_bw = inter_host_bw if world_size > local_world_size else intra_host_bw\n    fwd_comms = fwd_output_write_size / comms_bw\n\n    fwd_compute = (\n        (input_read_size + embedding_lookup_size + fwd_output_write_size)\n        * block_usage_penalty\n        / device_bw\n    )\n    if is_inference:\n        # only consider forward compute and comms for inference\n        return Perf(\n            fwd_compute=fwd_compute, fwd_comms=fwd_comms, bwd_compute=0, bwd_comms=0\n        )\n\n    bwd_comms = bwd_output_write_size / comms_bw\n\n    bwd_grad_indice_weights_kernel = (\n        fwd_compute * WEIGHTED_KERNEL_MULTIPLIER if is_weighted else 0\n    )\n\n    # includes fused optimizers\n    bwd_compute = fwd_compute * BWD_COMPUTE_MULTIPLIER\n\n    # in order of model parallel execution, starting with:\n    # BWD DP -> BWD MP ... FWD MP -> FWD DP\n    return Perf(\n        fwd_compute=fwd_compute,\n        fwd_comms=fwd_comms,\n        bwd_compute=bwd_compute + bwd_grad_indice_weights_kernel,\n        bwd_comms=bwd_comms,\n    )",
  "def _get_rw_sharding_perf(\n    batch_sizes: List[int],\n    world_size: int,\n    local_world_size: int,\n    input_lengths: List[float],\n    emb_dim: int,\n    input_data_type_size: float,\n    table_data_type_size: float,\n    fwd_a2a_comm_data_type_size: float,\n    bwd_a2a_comm_data_type_size: float,\n    fwd_sr_comm_data_type_size: float,\n    bwd_sr_comm_data_type_size: float,\n    num_poolings: List[float],\n    device_bw: float,\n    inter_host_bw: float,\n    intra_host_bw: float,\n    is_pooled: bool,\n    is_weighted: bool = False,\n) -> Perf:\n    batch_inputs = (\n        sum([x * y * z for x, y, z in zip(input_lengths, num_poolings, batch_sizes)])\n        / world_size\n    )\n    batch_outputs = (\n        sum([x * y for x, y in zip(num_poolings, batch_sizes)])\n        if is_pooled\n        else batch_inputs\n    )\n\n    input_read_size = math.ceil(batch_inputs * world_size * input_data_type_size)\n    if is_weighted:\n        input_read_size *= 2\n\n    embedding_lookup_size = batch_inputs * world_size * emb_dim * table_data_type_size\n\n    fwd_output_write_size = (\n        batch_outputs * world_size * emb_dim * fwd_sr_comm_data_type_size\n        if is_pooled\n        else batch_outputs * world_size * emb_dim * fwd_a2a_comm_data_type_size\n    )\n    bwd_output_write_size = (\n        batch_outputs * world_size * emb_dim * bwd_sr_comm_data_type_size\n        if is_pooled\n        else batch_outputs * world_size * emb_dim * bwd_a2a_comm_data_type_size\n    )\n\n    comms_bw = inter_host_bw if world_size > local_world_size else intra_host_bw\n    fwd_comms = fwd_output_write_size / comms_bw\n\n    fwd_compute = (\n        input_read_size + embedding_lookup_size + fwd_output_write_size\n    ) / device_bw\n\n    bwd_comms = bwd_output_write_size / comms_bw\n\n    bwd_batched_copy = bwd_output_write_size * BATCHED_COPY_PERF_FACTOR / device_bw\n\n    bwd_grad_indice_weights_kernel = (\n        fwd_compute * WEIGHTED_KERNEL_MULTIPLIER if is_weighted else 0\n    )\n\n    bwd_compute = fwd_compute * BWD_COMPUTE_MULTIPLIER\n\n    return Perf(\n        fwd_compute=fwd_compute,\n        fwd_comms=fwd_comms,\n        bwd_compute=bwd_compute + bwd_grad_indice_weights_kernel,\n        bwd_comms=bwd_comms + bwd_batched_copy,\n    )",
  "def _get_twrw_sharding_perf(\n    batch_sizes: List[int],\n    world_size: int,\n    local_world_size: int,\n    input_lengths: List[float],\n    emb_dim: int,\n    input_data_type_size: float,\n    table_data_type_size: float,\n    fwd_a2a_comm_data_type_size: float,\n    bwd_a2a_comm_data_type_size: float,\n    fwd_sr_comm_data_type_size: float,\n    bwd_sr_comm_data_type_size: float,\n    num_poolings: List[float],\n    device_bw: float,\n    inter_host_bw: float,\n    intra_host_bw: float,\n    is_pooled: bool,\n    is_weighted: bool = False,\n) -> Perf:\n    batch_inputs = (\n        sum([x * y * z for x, y, z in zip(input_lengths, num_poolings, batch_sizes)])\n        / local_world_size\n    )\n    batch_outputs = (\n        sum([x * y for x, y in zip(num_poolings, batch_sizes)])\n        if is_pooled\n        else batch_inputs\n    )\n\n    input_read_size = math.ceil(batch_inputs * world_size * input_data_type_size)\n    if is_weighted:\n        input_read_size *= 2\n\n    embedding_lookup_size = batch_inputs * world_size * emb_dim * table_data_type_size\n\n    fwd_output_write_size = (\n        batch_outputs * world_size * emb_dim * fwd_sr_comm_data_type_size\n    )\n    bwd_output_write_size = (\n        batch_outputs * world_size * emb_dim * bwd_sr_comm_data_type_size\n    )\n\n    # intra host comm\n    fwd_comms = fwd_output_write_size / intra_host_bw\n\n    # inter host comm\n    if world_size > local_world_size:\n        inter_host_fwd_fwd_output_write_size = (\n            batch_outputs * world_size * emb_dim * fwd_a2a_comm_data_type_size\n        )\n        fwd_comms += (\n            inter_host_fwd_fwd_output_write_size\n            * (local_world_size / world_size)\n            / inter_host_bw\n        )\n\n    fwd_compute = (\n        input_read_size + embedding_lookup_size + fwd_output_write_size\n    ) / device_bw\n\n    bwd_comms = bwd_output_write_size / intra_host_bw\n\n    bwd_grad_indice_weights_kernel = (\n        fwd_compute * WEIGHTED_KERNEL_MULTIPLIER if is_weighted else 0\n    )\n\n    bwd_batched_copy = bwd_output_write_size * BATCHED_COPY_PERF_FACTOR / device_bw\n\n    bwd_compute = fwd_compute * BWD_COMPUTE_MULTIPLIER\n\n    return Perf(\n        fwd_compute=fwd_compute,\n        fwd_comms=fwd_comms,\n        bwd_compute=bwd_compute + bwd_grad_indice_weights_kernel,\n        bwd_comms=bwd_comms + bwd_batched_copy,\n    )",
  "def _get_dp_sharding_perf(\n    batch_sizes: List[int],\n    world_size: int,\n    local_world_size: int,\n    input_lengths: List[float],\n    grad_num_elem: int,\n    emb_dim: int,\n    input_data_type_size: float,\n    table_data_type_size: float,\n    num_poolings: List[float],\n    device_bw: float,\n    inter_host_bw: float,\n    is_pooled: bool,\n    is_weighted: bool = False,\n) -> Perf:\n    batch_inputs = sum(\n        [x * y * z for x, y, z in zip(input_lengths, num_poolings, batch_sizes)]\n    )\n    batch_outputs = (\n        sum([x * y for x, y in zip(num_poolings, batch_sizes)])\n        if is_pooled\n        else batch_inputs\n    )\n\n    input_read_size = math.ceil(batch_inputs * input_data_type_size)\n    if is_weighted:\n        input_read_size *= 2\n\n    embedding_lookup_size = batch_inputs * emb_dim * table_data_type_size\n\n    output_write_size = batch_outputs * emb_dim * table_data_type_size\n    table_size = grad_num_elem * table_data_type_size\n\n    fwd_compute = (\n        input_read_size + embedding_lookup_size + output_write_size\n    ) / device_bw\n\n    num_nodes = min(world_size / local_world_size, 2)\n\n    # all-reduce data transfer: https://images.nvidia.com/events/sc15/pdfs/NCCL-Woolley.pdf\n    all_reduce = (\n        table_size\n        * (2 * num_nodes - 1)\n        / num_nodes\n        / (inter_host_bw * local_world_size)  # 1 NIC per GPU\n    )\n    # inter host communication constraint\n    if world_size > 2 * local_world_size:\n        all_reduce *= 2\n\n    # SGD + Fill + BUnary\n    optimizer_kernels = table_size * DP_ELEMENTWISE_KERNELS_PERF_FACTOR / device_bw\n\n    bwd_compute = fwd_compute * BWD_COMPUTE_MULTIPLIER\n\n    bwd_grad_indice_weights_kernel = (\n        fwd_compute * WEIGHTED_KERNEL_MULTIPLIER if is_weighted else 0\n    )\n\n    return Perf(\n        fwd_compute=fwd_compute,\n        fwd_comms=0,\n        bwd_compute=bwd_compute + bwd_grad_indice_weights_kernel,\n        bwd_comms=all_reduce + optimizer_kernels,\n    )",
  "def _extract_comm_data_type_size(\n    sharder: ModuleSharder[nn.Module], sharding_option: ShardingOption\n) -> Tuple[float, float, float, float]:\n    table_data_type_size = sharding_option.tensor.element_size()\n\n    fwd_a2a_comm_data_type_size = table_data_type_size\n    bwd_a2a_comm_data_type_size = table_data_type_size\n    fwd_sr_comm_data_type_size = table_data_type_size\n    bwd_sr_comm_data_type_size = table_data_type_size\n\n    if sharder.qcomm_codecs_registry is not None:\n        qcomm_codecs_registry = sharder.qcomm_codecs_registry\n        if (\n            sharding_option.is_pooled\n            and CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name in qcomm_codecs_registry\n        ):\n            codecs = sharder.qcomm_codecs_registry[\n                CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name\n            ]\n            fwd_a2a_comm_data_type_size = torch.tensor(\n                [], dtype=codecs.forward.quantized_dtype\n            ).element_size()\n            bwd_a2a_comm_data_type_size = torch.tensor(\n                [], dtype=codecs.backward.quantized_dtype\n            ).element_size()\n\n        if (\n            not sharding_option.is_pooled\n            and CommOp.SEQUENCE_EMBEDDINGS_ALL_TO_ALL.name in qcomm_codecs_registry\n        ):\n            codecs = qcomm_codecs_registry[CommOp.SEQUENCE_EMBEDDINGS_ALL_TO_ALL.name]\n            fwd_a2a_comm_data_type_size = torch.tensor(\n                [], dtype=codecs.forward.quantized_dtype\n            ).element_size()\n            bwd_a2a_comm_data_type_size = torch.tensor(\n                [], dtype=codecs.backward.quantized_dtype\n            ).element_size()\n\n        if (\n            sharding_option.is_pooled\n            and CommOp.POOLED_EMBEDDINGS_REDUCE_SCATTER.name in qcomm_codecs_registry\n        ):\n            codecs = qcomm_codecs_registry[CommOp.POOLED_EMBEDDINGS_REDUCE_SCATTER.name]\n            fwd_sr_comm_data_type_size = torch.tensor(\n                [], dtype=codecs.forward.quantized_dtype\n            ).element_size()\n            bwd_sr_comm_data_type_size = torch.tensor(\n                [], dtype=codecs.backward.quantized_dtype\n            ).element_size()\n\n    return (\n        fwd_a2a_comm_data_type_size,\n        bwd_a2a_comm_data_type_size,\n        fwd_sr_comm_data_type_size,\n        bwd_sr_comm_data_type_size,\n    )",
  "class EmbeddingStorageEstimator(ShardEstimator):\n    \"\"\"\n    Embedding Storage Usage Estimator\n    \"\"\"\n\n    def __init__(\n        self,\n        topology: Topology,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> None:\n        self._topology = topology\n        self._constraints = constraints\n\n    def estimate(\n        self,\n        sharding_options: List[ShardingOption],\n        sharder_map: Optional[Dict[str, ModuleSharder[nn.Module]]] = None,\n    ) -> None:\n        if not sharder_map:\n            assert not sharding_options, \"sharder_map not provided for sharding_options\"\n            return\n\n        for sharding_option in sharding_options:\n            sharder_key = sharder_name(type(sharding_option.module[1]))\n            sharder = sharder_map[sharder_key]\n\n            caching_ratio = (\n                self._constraints[  # pyre-ignore[16]\n                    sharding_option.name\n                ].cache_params.load_factor\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].cache_params\n                else None\n            )\n            # TODO: remove after deprecating fused_params in sharder\n            if caching_ratio is None:\n                caching_ratio = (\n                    sharder.fused_params.get(\"cache_load_factor\")  # pyre-ignore[16]\n                    if hasattr(sharder, \"fused_params\") and sharder.fused_params\n                    else None\n                )\n\n            num_poolings = (\n                cast(List[float], self._constraints[sharding_option.name].num_poolings)\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].num_poolings\n                else [1.0] * sharding_option.num_inputs\n            )\n            assert len(num_poolings) == sharding_option.num_inputs\n            batch_sizes = (\n                cast(List[int], self._constraints[sharding_option.name].batch_sizes)\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].batch_sizes\n                else [sharding_option.batch_size] * sharding_option.num_inputs\n            )\n\n            shard_storages = calculate_shard_storages(\n                sharder=sharder,\n                sharding_type=sharding_option.sharding_type,\n                tensor=sharding_option.tensor,\n                compute_device=self._topology.compute_device,\n                compute_kernel=sharding_option.compute_kernel,\n                shard_sizes=[shard.size for shard in sharding_option.shards],\n                batch_sizes=batch_sizes,\n                world_size=self._topology.world_size,\n                local_world_size=self._topology.local_world_size,\n                input_lengths=sharding_option.input_lengths,\n                num_poolings=num_poolings,\n                caching_ratio=caching_ratio if caching_ratio else UVM_CACHING_RATIO,\n                is_pooled=sharding_option.is_pooled,\n            )\n\n            for shard, storage in zip(sharding_option.shards, shard_storages):\n                shard.storage = storage",
  "def calculate_shard_storages(\n    sharder: ModuleSharder[nn.Module],\n    sharding_type: str,\n    tensor: torch.Tensor,\n    compute_device: str,\n    compute_kernel: str,\n    shard_sizes: List[List[int]],\n    batch_sizes: List[int],\n    world_size: int,\n    local_world_size: int,\n    input_lengths: List[float],\n    num_poolings: List[float],\n    caching_ratio: float,\n    is_pooled: bool,\n) -> List[Storage]:\n    \"\"\"\n    Calculates estimated storage sizes for each sharded tensor, comprised of input,\n    output, tensor, gradient, and optimizer sizes.\n\n    Args:\n        sharder (ModuleSharder[nn.Module]): sharder for module that supports sharding.\n        sharding_type (str): provided ShardingType value.\n        tensor (torch.Tensor): tensor to be sharded.\n        compute_device (str): compute device to be used.\n        compute_kernel (str): compute kernel to be used.\n        shard_sizes (List[List[int]]): list of dimensions of each sharded tensor.\n        batch_sizes (List[int]): batch size for each input feature.\n        world_size (int): total number of devices in topology.\n        local_world_size (int): total number of devices in host group topology.\n        input_lengths (List[float]): average input lengths synonymous with pooling\n            factors.\n        num_poolings (List[float]): average number of poolings per sample\n            (typically 1.0).\n        caching_ratio (float): ratio of HBM to DDR memory for UVM caching.\n        is_pooled (bool): True if embedding output is pooled (ie. `EmbeddingBag`), False\n            if unpooled/sequential (ie. `Embedding`).\n\n    Returns:\n        List[Storage]: storage object for each device in topology.\n    \"\"\"\n\n    input_data_type_size = BIGINT_DTYPE\n    output_data_type_size = tensor.element_size()\n\n    input_sizes, output_sizes = _calculate_shard_io_sizes(\n        sharding_type=sharding_type,\n        batch_sizes=batch_sizes,\n        world_size=world_size,\n        local_world_size=local_world_size,\n        input_lengths=input_lengths,\n        emb_dim=tensor.shape[1],\n        shard_sizes=shard_sizes,\n        input_data_type_size=input_data_type_size,\n        output_data_type_size=output_data_type_size,\n        num_poolings=num_poolings,\n        is_pooled=is_pooled,\n    )\n\n    tensor_storage = sharder.storage_usage(tensor, compute_device, compute_kernel)\n    hbm_storage: int = tensor_storage.get(\"hbm\", 0)\n    ddr_storage: int = tensor_storage.get(\"ddr\", 0)\n\n    if compute_kernel in {\n        EmbeddingComputeKernel.FUSED_UVM_CACHING.value,\n        EmbeddingComputeKernel.QUANT_UVM_CACHING.value,\n    }:\n        hbm_storage = round(ddr_storage * caching_ratio)\n\n    optimizer_class = getattr(tensor, \"_optimizer_class\", None)\n\n    hbm_specific_sizes: List[int] = _calculate_storage_specific_sizes(\n        storage=hbm_storage,\n        shape=tensor.shape,\n        shard_sizes=shard_sizes,\n        sharding_type=sharding_type,\n        optimizer_class=optimizer_class,\n    )\n    ddr_specific_sizes: List[int] = _calculate_storage_specific_sizes(\n        storage=ddr_storage,\n        shape=tensor.shape,\n        shard_sizes=shard_sizes,\n        sharding_type=sharding_type,\n        optimizer_class=optimizer_class,\n    )\n\n    hbm_sizes: List[int] = [\n        input_size + output_size + hbm_specific_size if compute_device == \"cuda\" else 0\n        for input_size, output_size, hbm_specific_size in zip(\n            input_sizes,\n            output_sizes,\n            hbm_specific_sizes,\n        )\n    ]\n    ddr_sizes: List[int] = [\n        input_size + output_size + ddr_specific_size\n        if compute_device == \"cpu\"\n        else ddr_specific_size\n        for input_size, output_size, ddr_specific_size in zip(\n            input_sizes,\n            output_sizes,\n            ddr_specific_sizes,\n        )\n    ]\n\n    return [\n        Storage(\n            hbm=hbm_size,\n            ddr=ddr_size,\n        )\n        for hbm_size, ddr_size in zip(hbm_sizes, ddr_sizes)\n    ]",
  "def _calculate_shard_io_sizes(\n    sharding_type: str,\n    batch_sizes: List[int],\n    world_size: int,\n    local_world_size: int,\n    input_lengths: List[float],\n    emb_dim: int,\n    shard_sizes: List[List[int]],\n    input_data_type_size: int,\n    output_data_type_size: int,\n    num_poolings: List[float],\n    is_pooled: bool,\n) -> Tuple[List[int], List[int]]:\n    if sharding_type == ShardingType.DATA_PARALLEL.value:\n        return _calculate_dp_shard_io_sizes(\n            batch_sizes=batch_sizes,\n            input_lengths=input_lengths,\n            emb_dim=emb_dim,\n            num_shards=len(shard_sizes),\n            input_data_type_size=input_data_type_size,\n            output_data_type_size=output_data_type_size,\n            num_poolings=num_poolings,\n            is_pooled=is_pooled,\n        )\n    elif sharding_type == ShardingType.TABLE_WISE.value:\n        return _calculate_tw_shard_io_sizes(\n            batch_sizes=batch_sizes,\n            world_size=world_size,\n            input_lengths=input_lengths,\n            emb_dim=emb_dim,\n            input_data_type_size=input_data_type_size,\n            output_data_type_size=output_data_type_size,\n            num_poolings=num_poolings,\n            is_pooled=is_pooled,\n        )\n    elif sharding_type in {\n        ShardingType.COLUMN_WISE.value,\n        ShardingType.TABLE_COLUMN_WISE.value,\n    }:\n        return _calculate_cw_shard_io_sizes(\n            batch_sizes=batch_sizes,\n            world_size=world_size,\n            input_lengths=input_lengths,\n            shard_sizes=shard_sizes,\n            input_data_type_size=input_data_type_size,\n            output_data_type_size=output_data_type_size,\n            num_poolings=num_poolings,\n            is_pooled=is_pooled,\n        )\n    elif sharding_type == ShardingType.ROW_WISE.value:\n        return _calculate_rw_shard_io_sizes(\n            batch_sizes=batch_sizes,\n            world_size=world_size,\n            input_lengths=input_lengths,\n            shard_sizes=shard_sizes,\n            input_data_type_size=input_data_type_size,\n            output_data_type_size=output_data_type_size,\n            num_poolings=num_poolings,\n            is_pooled=is_pooled,\n        )\n    elif sharding_type == ShardingType.TABLE_ROW_WISE.value:\n        return _calculate_twrw_shard_io_sizes(\n            batch_sizes=batch_sizes,\n            world_size=world_size,\n            local_world_size=local_world_size,\n            input_lengths=input_lengths,\n            shard_sizes=shard_sizes,\n            input_data_type_size=input_data_type_size,\n            output_data_type_size=output_data_type_size,\n            num_poolings=num_poolings,\n            is_pooled=is_pooled,\n        )\n    else:\n        raise ValueError(\n            f\"Unrecognized or unsupported sharding type provided: {sharding_type}\"\n        )",
  "def _calculate_dp_shard_io_sizes(\n    batch_sizes: List[int],\n    input_lengths: List[float],\n    emb_dim: int,\n    num_shards: int,\n    input_data_type_size: int,\n    output_data_type_size: int,\n    num_poolings: List[float],\n    is_pooled: bool,\n) -> Tuple[List[int], List[int]]:\n    batch_inputs = sum(\n        [x * y * z for x, y, z in zip(input_lengths, num_poolings, batch_sizes)]\n    )\n    batch_outputs = (\n        sum([x * y for x, y in zip(num_poolings, batch_sizes)])\n        if is_pooled\n        else batch_inputs\n    )\n\n    input_sizes = [math.ceil(batch_inputs * input_data_type_size)] * num_shards\n    output_sizes = [\n        math.ceil(batch_outputs * emb_dim * output_data_type_size)\n    ] * num_shards\n\n    return input_sizes, output_sizes",
  "def _calculate_tw_shard_io_sizes(\n    batch_sizes: List[int],\n    world_size: int,\n    input_lengths: List[float],\n    emb_dim: int,\n    input_data_type_size: int,\n    output_data_type_size: int,\n    num_poolings: List[float],\n    is_pooled: bool,\n) -> Tuple[List[int], List[int]]:\n    batch_inputs = sum(\n        [x * y * z for x, y, z in zip(input_lengths, num_poolings, batch_sizes)]\n    )\n    batch_outputs = (\n        sum([x * y for x, y in zip(num_poolings, batch_sizes)])\n        if is_pooled\n        else batch_inputs\n    )\n\n    input_sizes = [math.ceil(batch_inputs * world_size * input_data_type_size)]\n    output_sizes = [\n        math.ceil(batch_outputs * world_size * emb_dim * output_data_type_size)\n    ]\n\n    return input_sizes, output_sizes",
  "def _calculate_cw_shard_io_sizes(\n    batch_sizes: List[int],\n    world_size: int,\n    input_lengths: List[float],\n    shard_sizes: List[List[int]],\n    input_data_type_size: int,\n    output_data_type_size: int,\n    num_poolings: List[float],\n    is_pooled: bool,\n) -> Tuple[List[int], List[int]]:\n    batch_inputs = sum(\n        [x * y * z for x, y, z in zip(input_lengths, num_poolings, batch_sizes)]\n    )\n    batch_outputs = (\n        sum([x * y for x, y in zip(num_poolings, batch_sizes)])\n        if is_pooled\n        else batch_inputs\n    )\n\n    input_sizes = [math.ceil(batch_inputs * world_size * input_data_type_size)] * len(\n        shard_sizes\n    )\n    output_sizes = [\n        math.ceil(\n            batch_outputs * world_size * shard_sizes[i][1] * output_data_type_size\n        )\n        for i in range(len(shard_sizes))\n    ]\n\n    return input_sizes, output_sizes",
  "def _calculate_rw_shard_io_sizes(\n    batch_sizes: List[int],\n    world_size: int,\n    input_lengths: List[float],\n    shard_sizes: List[List[int]],\n    input_data_type_size: int,\n    output_data_type_size: int,\n    num_poolings: List[float],\n    is_pooled: bool,\n) -> Tuple[List[int], List[int]]:\n    batch_inputs = (\n        sum([x * y * z for x, y, z in zip(input_lengths, num_poolings, batch_sizes)])\n        / world_size\n    )\n    batch_outputs = (\n        sum([x * y for x, y in zip(num_poolings, batch_sizes)])\n        if is_pooled\n        else batch_inputs\n    )\n\n    input_sizes = [\n        math.ceil(batch_inputs * world_size * input_data_type_size)\n        if prod(shard) != 0\n        else 0\n        for shard in shard_sizes\n    ]\n    output_sizes = [\n        math.ceil(\n            batch_outputs * world_size * shard_sizes[i][1] * output_data_type_size\n        )\n        if prod(shard) != 0\n        else 0\n        for i, shard in enumerate(shard_sizes)\n    ]\n\n    return input_sizes, output_sizes",
  "def _calculate_twrw_shard_io_sizes(\n    batch_sizes: List[int],\n    world_size: int,\n    local_world_size: int,\n    input_lengths: List[float],\n    shard_sizes: List[List[int]],\n    input_data_type_size: int,\n    output_data_type_size: int,\n    num_poolings: List[float],\n    is_pooled: bool,\n) -> Tuple[List[int], List[int]]:\n    batch_inputs = (\n        sum([x * y * z for x, y, z in zip(input_lengths, num_poolings, batch_sizes)])\n        / local_world_size\n    )\n    batch_outputs = (\n        sum([x * y for x, y in zip(num_poolings, batch_sizes)])\n        if is_pooled\n        else batch_inputs\n    )\n\n    input_sizes = [\n        math.ceil(batch_inputs * world_size * input_data_type_size)\n        if prod(shard) != 0\n        else 0\n        for shard in shard_sizes\n    ]\n    output_sizes = [\n        math.ceil(\n            batch_outputs * world_size * shard_sizes[i][1] * output_data_type_size\n        )\n        if prod(shard) != 0\n        else 0\n        for i, shard in enumerate(shard_sizes)\n    ]\n\n    return input_sizes, output_sizes",
  "def _calculate_storage_specific_sizes(\n    storage: int,\n    shape: torch.Size,\n    shard_sizes: List[List[int]],\n    sharding_type: str,\n    optimizer_class: Optional[Type[torch.optim.Optimizer]] = None,\n) -> List[int]:\n    tensor_sizes: List[int] = [\n        math.ceil(storage * prod(size) / prod(shape))\n        if sharding_type != ShardingType.DATA_PARALLEL.value\n        else storage\n        for size in shard_sizes\n    ]\n    optimizer_multipler: float = _get_optimizer_multipler(optimizer_class, shape)\n\n    optimizer_sizes: List[int] = [\n        math.ceil(tensor_size * optimizer_multipler) for tensor_size in tensor_sizes\n    ]\n\n    return [\n        tensor_size + optimizer_size\n        for tensor_size, optimizer_size in zip(tensor_sizes, optimizer_sizes)\n    ]",
  "def _get_optimizer_multipler(\n    optimizer_class: Optional[Type[torch.optim.Optimizer]],\n    shape: torch.Size,\n) -> float:\n    if not optimizer_class:\n        return 0.0\n    if optimizer_class in [torch.optim.SGD, trec_optim.SGD]:\n        return 0\n    elif optimizer_class in [torch.optim.Adam, trec_optim.Adam]:\n        return 2\n    elif optimizer_class == trec_optim.RowWiseAdagrad:\n        return 1 / shape[-1]\n    else:\n        return 1",
  "def __init__(\n        self,\n        topology: Topology,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        is_inference: bool = False,\n    ) -> None:\n        self._topology = topology\n        self._constraints = constraints\n        self._is_inference = is_inference",
  "def estimate(\n        self,\n        sharding_options: List[ShardingOption],\n        sharder_map: Optional[Dict[str, ModuleSharder[nn.Module]]] = None,\n    ) -> None:\n        if not sharder_map:\n            assert not sharding_options, \"sharder_map not provided for sharding_options\"\n            return\n\n        for sharding_option in sharding_options:\n            sharder_key = sharder_name(type(sharding_option.module[1]))\n            sharder = sharder_map[sharder_key]\n\n            caching_ratio = (\n                self._constraints[  # pyre-ignore[16]\n                    sharding_option.name\n                ].cache_params.load_factor\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].cache_params\n                else None\n            )\n            # TODO: remove after deprecating fused_params in sharder\n            if caching_ratio is None:\n                caching_ratio = (\n                    sharder.fused_params.get(\"cache_load_factor\")  # pyre-ignore[16]\n                    if hasattr(sharder, \"fused_params\") and sharder.fused_params\n                    else None\n                )\n\n            num_poolings = (\n                cast(List[float], self._constraints[sharding_option.name].num_poolings)\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].num_poolings\n                else [1.0] * sharding_option.num_inputs\n            )\n            batch_sizes = (\n                cast(List[int], self._constraints[sharding_option.name].batch_sizes)\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].batch_sizes\n                else [sharding_option.batch_size] * sharding_option.num_inputs\n            )\n\n            assert (\n                len(sharding_option.input_lengths)\n                == len(num_poolings)\n                == len(batch_sizes)\n            ), \"Provided `pooling_factors`, `num_poolings`, and `batch_sizes` constraints must match.\"\n\n            module = sharding_option.module[1]\n\n            # TODO remove this hack once feature processor is disaggregated\n            has_feature_processor = False\n            if (\n                hasattr(module, \"_feature_processor\")\n                and hasattr(module._feature_processor, \"feature_processor_modules\")\n                and isinstance(\n                    module._feature_processor.feature_processor_modules,\n                    nn.ModuleDict,\n                )\n                and sharding_option.name\n                in module._feature_processor.feature_processor_modules.keys()\n            ):\n                has_feature_processor = True\n                logger.info(f\"Table {sharding_option.name} has feature processor.\")\n\n            if isinstance(module, EmbeddingBagCollectionInterface):\n                is_weighted = module.is_weighted()\n            elif (\n                self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].is_weighted\n            ):\n                is_weighted = self._constraints[sharding_option.name].is_weighted\n            else:\n                is_weighted = False\n\n            # TODO remove this once migrate away from PEA\n            is_weighted = is_weighted or has_feature_processor\n            sharding_option.is_weighted = is_weighted\n\n            table_data_type_size = sharding_option.tensor.element_size()\n            (\n                fwd_a2a_comm_data_type_size,\n                bwd_a2a_comm_data_type_size,\n                fwd_sr_comm_data_type_size,\n                bwd_sr_comm_data_type_size,\n            ) = _extract_comm_data_type_size(sharder, sharding_option)\n\n            shard_perfs = perf_func_emb_wall_time(\n                shard_sizes=[shard.size for shard in sharding_option.shards],\n                compute_kernel=sharding_option.compute_kernel,\n                compute_device=self._topology.compute_device,\n                sharding_type=sharding_option.sharding_type,\n                batch_sizes=batch_sizes,\n                world_size=self._topology.world_size,\n                local_world_size=self._topology.local_world_size,\n                input_lengths=sharding_option.input_lengths,\n                input_data_type_size=BIGINT_DTYPE,\n                table_data_type_size=table_data_type_size,\n                fwd_a2a_comm_data_type_size=fwd_a2a_comm_data_type_size,\n                bwd_a2a_comm_data_type_size=bwd_a2a_comm_data_type_size,\n                fwd_sr_comm_data_type_size=fwd_sr_comm_data_type_size,\n                bwd_sr_comm_data_type_size=bwd_sr_comm_data_type_size,\n                num_poolings=num_poolings,\n                hbm_mem_bw=self._topology.hbm_mem_bw,\n                ddr_mem_bw=self._topology.ddr_mem_bw,\n                intra_host_bw=self._topology.intra_host_bw,\n                inter_host_bw=self._topology.inter_host_bw,\n                is_pooled=sharding_option.is_pooled,\n                is_weighted=is_weighted,\n                is_inference=self._is_inference,\n                caching_ratio=caching_ratio,\n            )\n\n            for shard, perf in zip(sharding_option.shards, shard_perfs):\n                shard.perf = perf",
  "def __init__(\n        self,\n        topology: Topology,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> None:\n        self._topology = topology\n        self._constraints = constraints",
  "def estimate(\n        self,\n        sharding_options: List[ShardingOption],\n        sharder_map: Optional[Dict[str, ModuleSharder[nn.Module]]] = None,\n    ) -> None:\n        if not sharder_map:\n            assert not sharding_options, \"sharder_map not provided for sharding_options\"\n            return\n\n        for sharding_option in sharding_options:\n            sharder_key = sharder_name(type(sharding_option.module[1]))\n            sharder = sharder_map[sharder_key]\n\n            caching_ratio = (\n                self._constraints[  # pyre-ignore[16]\n                    sharding_option.name\n                ].cache_params.load_factor\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].cache_params\n                else None\n            )\n            # TODO: remove after deprecating fused_params in sharder\n            if caching_ratio is None:\n                caching_ratio = (\n                    sharder.fused_params.get(\"cache_load_factor\")  # pyre-ignore[16]\n                    if hasattr(sharder, \"fused_params\") and sharder.fused_params\n                    else None\n                )\n\n            num_poolings = (\n                cast(List[float], self._constraints[sharding_option.name].num_poolings)\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].num_poolings\n                else [1.0] * sharding_option.num_inputs\n            )\n            assert len(num_poolings) == sharding_option.num_inputs\n            batch_sizes = (\n                cast(List[int], self._constraints[sharding_option.name].batch_sizes)\n                if self._constraints\n                and self._constraints.get(sharding_option.name)\n                and self._constraints[sharding_option.name].batch_sizes\n                else [sharding_option.batch_size] * sharding_option.num_inputs\n            )\n\n            shard_storages = calculate_shard_storages(\n                sharder=sharder,\n                sharding_type=sharding_option.sharding_type,\n                tensor=sharding_option.tensor,\n                compute_device=self._topology.compute_device,\n                compute_kernel=sharding_option.compute_kernel,\n                shard_sizes=[shard.size for shard in sharding_option.shards],\n                batch_sizes=batch_sizes,\n                world_size=self._topology.world_size,\n                local_world_size=self._topology.local_world_size,\n                input_lengths=sharding_option.input_lengths,\n                num_poolings=num_poolings,\n                caching_ratio=caching_ratio if caching_ratio else UVM_CACHING_RATIO,\n                is_pooled=sharding_option.is_pooled,\n            )\n\n            for shard, storage in zip(sharding_option.shards, shard_storages):\n                shard.storage = storage",
  "def sharder_name(t: Type[Any]) -> str:\n    return t.__module__ + \".\" + t.__name__",
  "def bytes_to_gb(num_bytes: int) -> float:\n    return float(num_bytes / (1024 * 1024 * 1024))",
  "def bytes_to_mb(num_bytes: Union[float, int]) -> float:\n    return float(num_bytes / (1024 * 1024))",
  "def gb_to_bytes(gb: float) -> int:\n    return int(gb * 1024 * 1024 * 1024)",
  "def prod(iterable: Iterable[int]) -> int:\n    return reduce(operator.mul, iterable, 1)",
  "def placement(\n    compute_device: str,\n    rank: int,\n    local_size: int,\n) -> str:\n    \"\"\"\n    Returns placement, formatted as string\n    \"\"\"\n\n    param_device = compute_device\n    if compute_device == \"cuda\":\n        param_device = torch.device(\"cuda\", rank % local_size)\n    return f\"rank:{rank}/{param_device}\"",
  "def storage_repr_in_gb(storage: Optional[Storage]) -> str:\n    if storage is None:\n        return \"\"\n    return (\n        f\"Storage(hbm = {round(bytes_to_gb(storage.hbm), 3)} GB, \"\n        f\"ddr = {round(bytes_to_gb(storage.ddr), 3)} GB)\"\n    )",
  "def reset_shard_rank(proposal: List[ShardingOption]) -> None:\n    for sharding_option in proposal:\n        for shard in sharding_option.shards:\n            shard.rank = None",
  "def _to_sharding_plan(\n    sharding_options: List[ShardingOption],\n    topology: Topology,\n) -> ShardingPlan:\n\n    compute_device = topology.compute_device\n    local_size = topology.local_world_size\n\n    plan = {}\n    for sharding_option in sharding_options:\n        shards = sharding_option.shards\n        sharding_type = sharding_option.sharding_type\n\n        module_plan = plan.get(sharding_option.path, EmbeddingModuleShardingPlan())\n        module_plan[sharding_option.name] = ParameterSharding(\n            sharding_spec=None\n            if sharding_type == ShardingType.DATA_PARALLEL.value\n            else EnumerableShardingSpec(\n                [\n                    ShardMetadata(\n                        shard_sizes=shard.size,\n                        shard_offsets=shard.offset,\n                        placement=placement(\n                            compute_device, cast(int, shard.rank), local_size\n                        ),\n                    )\n                    for shard in shards\n                ]\n            ),\n            sharding_type=sharding_type,\n            compute_kernel=sharding_option.compute_kernel,\n            ranks=[cast(int, shard.rank) for shard in shards],\n            cache_params=sharding_option.cache_params,\n            enforce_hbm=sharding_option.enforce_hbm,\n            stochastic_rounding=sharding_option.stochastic_rounding,\n            bounds_check_mode=sharding_option.bounds_check_mode,\n        )\n        plan[sharding_option.path] = module_plan\n    return ShardingPlan(plan)",
  "class EmbeddingShardingPlanner(ShardingPlanner):\n    \"\"\"\n    Provides an optimized sharding plan for a given module with shardable parameters\n    according to the provided sharders, topology, and constraints.\n    \"\"\"\n\n    def __init__(\n        self,\n        topology: Optional[Topology] = None,\n        batch_size: Optional[int] = None,\n        enumerator: Optional[Enumerator] = None,\n        storage_reservation: Optional[StorageReservation] = None,\n        proposer: Optional[Union[Proposer, List[Proposer]]] = None,\n        partitioner: Optional[Partitioner] = None,\n        performance_model: Optional[PerfModel] = None,\n        stats: Optional[Union[Stats, List[Stats]]] = None,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        debug: bool = True,\n    ) -> None:\n        if topology is None:\n            topology = Topology(\n                local_world_size=get_local_size(),\n                world_size=dist.get_world_size(),\n                compute_device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n            )\n        self._topology: Topology = topology\n        self._batch_size: int = batch_size if batch_size else BATCH_SIZE\n        self._constraints = constraints\n        self._enumerator: Enumerator = (\n            enumerator\n            if enumerator\n            else EmbeddingEnumerator(\n                topology=topology,\n                batch_size=self._batch_size,\n                constraints=constraints,\n            )\n        )\n        self._storage_reservation: StorageReservation = (\n            storage_reservation\n            if storage_reservation\n            else HeuristicalStorageReservation(percentage=0.15)\n        )\n        self._partitioner: Partitioner = (\n            partitioner if partitioner else GreedyPerfPartitioner()\n        )\n        if proposer:\n            self._proposers: List[Proposer] = (\n                [proposer] if not isinstance(proposer, list) else proposer\n            )\n        else:\n            self._proposers = [\n                GridSearchProposer(),\n                GreedyProposer(),\n                GreedyProposer(use_depth=False),\n                UniformProposer(),\n            ]\n        self._perf_model: PerfModel = (\n            performance_model if performance_model else NoopPerfModel(topology=topology)\n        )\n\n        if stats:\n            self._stats: List[Stats] = [stats] if not isinstance(stats, list) else stats\n        else:\n            self._stats = [EmbeddingStats()]\n\n        self._debug = debug\n        self._num_proposals: int = 0\n        self._num_plans: int = 0\n        self._best_plan: Optional[List[ShardingOption]] = None\n\n    def collective_plan(\n        self,\n        module: nn.Module,\n        sharders: Optional[List[ModuleSharder[nn.Module]]] = None,\n        pg: Optional[dist.ProcessGroup] = dist.GroupMember.WORLD,\n    ) -> ShardingPlan:\n        \"\"\"\n        Call self.plan(...) on rank 0 and broadcast\n        \"\"\"\n        assert pg is not None, \"Process group is not initialized\"\n        if sharders is None:\n            sharders = get_default_sharders()\n        return invoke_on_rank_and_broadcast_result(\n            pg,\n            0,\n            self.plan,\n            module,\n            sharders,\n        )\n\n    def plan(\n        self,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n    ) -> ShardingPlan:\n\n        self._num_proposals = 0\n        self._num_plans = 0\n        start_time = perf_counter()\n        best_plan = None\n        lowest_storage = Storage(MAX_SIZE, MAX_SIZE)\n        last_planner_error: Optional[PlannerError] = None\n        best_perf_rating = MAX_SIZE\n\n        storage_constraint: Topology = self._storage_reservation.reserve(\n            topology=self._topology,\n            batch_size=self._batch_size,\n            module=module,\n            sharders=sharders,\n            constraints=self._constraints,\n        )\n\n        search_space = self._enumerator.enumerate(\n            module=module,\n            sharders=sharders,\n        )\n        if not search_space:\n            # No shardable parameters\n            return ShardingPlan({})\n\n        proposal_cache: Dict[\n            Tuple[int, ...],\n            Tuple[bool, Optional[List[ShardingOption]], Optional[float]],\n        ] = {}\n\n        for proposer in self._proposers:\n            proposer.load(search_space=search_space)\n\n        for proposer in self._proposers:\n            proposal = proposer.propose()\n\n            while proposal:\n                proposal_key = tuple(sorted(map(hash, proposal)))\n                if proposal_key in proposal_cache:\n                    partitionable, plan, perf_rating = proposal_cache[proposal_key]\n                    proposer.feedback(\n                        partitionable=partitionable,\n                        plan=plan,\n                        perf_rating=perf_rating,\n                    )\n                    proposal = proposer.propose()\n                    continue\n\n                self._num_proposals += 1\n                try:\n                    # plan is just proposal where shard.rank is populated\n                    plan = self._partitioner.partition(\n                        proposal=proposal,\n                        storage_constraint=storage_constraint,\n                    )\n                    self._num_plans += 1\n                    perf_rating = self._perf_model.rate(plan=plan)\n                    if perf_rating < best_perf_rating:\n                        best_perf_rating = perf_rating\n                        best_plan = copy.deepcopy(plan)\n                    proposal_cache[proposal_key] = (True, plan, perf_rating)\n                    proposer.feedback(\n                        partitionable=True, plan=plan, perf_rating=perf_rating\n                    )\n                except PlannerError as planner_error:\n                    last_planner_error = planner_error\n                    current_storage = cast(\n                        Storage,\n                        reduce(\n                            lambda x, y: x + y,\n                            [\n                                shard.storage\n                                for option in proposal\n                                for shard in option.shards\n                            ],\n                        ),\n                    )\n                    if current_storage < lowest_storage:\n                        lowest_storage = current_storage\n                    proposal_cache[proposal_key] = (False, None, None)\n                    proposer.feedback(partitionable=False)\n\n                # clear shard.rank for each sharding_option\n                reset_shard_rank(proposal)\n                proposal = proposer.propose()\n\n        if best_plan:\n            self._best_plan = best_plan\n            sharding_plan = _to_sharding_plan(best_plan, self._topology)\n\n            end_time = perf_counter()\n            for stats in self._stats:\n                stats.log(\n                    sharding_plan=sharding_plan,\n                    topology=self._topology,\n                    batch_size=self._batch_size,\n                    storage_reservation=self._storage_reservation,\n                    num_proposals=self._num_proposals,\n                    num_plans=self._num_plans,\n                    run_time=end_time - start_time,\n                    best_plan=best_plan,\n                    constraints=self._constraints,\n                    sharders=sharders,\n                    debug=self._debug,\n                )\n            return sharding_plan\n        else:\n            global_storage_capacity = reduce(\n                lambda x, y: x + y,\n                [device.storage for device in self._topology.devices],\n            )\n            global_storage_constraints = reduce(\n                lambda x, y: x + y,\n                [device.storage for device in storage_constraint.devices],\n            )\n            storage_reservation_solution = (\n                (\n                    f\"\\n\\t  Storage reservation percentage: {self._storage_reservation._percentage}, \"\n                    f\"\\n\\t  Per rank reservation for dense storage: {storage_repr_in_gb(self._storage_reservation._dense_storage)}, \"\n                    f\"\\n\\t  Per rank reservation for kjt storage: {storage_repr_in_gb(self._storage_reservation._kjt_storage)}, \"  # pyre-ignore[16]\n                )\n                if isinstance(self._storage_reservation, HeuristicalStorageReservation)\n                else f\"\\n\\t  Storage reservation percentage: {self._storage_reservation._percentage}, \"  # pyre-ignore[16]\n            )\n            no_plan_solution = (\n                f\"Planner evaluated {self._num_proposals} proposals.\"\n                \"\\nPossible solutions:\"\n                f\"\\n  1) Increase the number of devices ({self._topology.world_size})\"\n                f\"\\n  2) Reduce the model size (\"\n                f\"\\n\\t  Global storage: {round(bytes_to_gb(global_storage_capacity.hbm), 3)} GB, \"\n                f\"\\n\\t  Per rank hardware memory: {storage_repr_in_gb(self._topology.devices[0].storage)}, \"\n                f\"{storage_reservation_solution}\"\n                f\"\\n\\t  Global storage available for model parallel: {storage_repr_in_gb(global_storage_constraints)}, \"\n                f\"\\n\\t  Global storage requirement for model parallel: {storage_repr_in_gb(lowest_storage)})\"\n                f\"\\n  3) Reduce local batch size ({self._batch_size})\"\n                \"\\n  4) Remove planner constraints that might be reducing search space or available storage\\n\"\n            )\n            last_planner_error_info = f\"Last planner error: \\n\\t{last_planner_error}\\n\"\n            if not lowest_storage.fits_in(global_storage_constraints):\n                raise PlannerError(\n                    error_type=PlannerErrorType.INSUFFICIENT_STORAGE,\n                    message=\"Unable to find a plan for this model because of insufficient storage. \\n\"\n                    + no_plan_solution\n                    + last_planner_error_info,\n                )\n            else:\n                raise PlannerError(\n                    error_type=PlannerErrorType.STRICT_CONSTRAINTS,\n                    message=\"Unable to find a plan for this model because of the strict constraints. \\n\"\n                    + no_plan_solution\n                    + last_planner_error_info,\n                )",
  "def __init__(\n        self,\n        topology: Optional[Topology] = None,\n        batch_size: Optional[int] = None,\n        enumerator: Optional[Enumerator] = None,\n        storage_reservation: Optional[StorageReservation] = None,\n        proposer: Optional[Union[Proposer, List[Proposer]]] = None,\n        partitioner: Optional[Partitioner] = None,\n        performance_model: Optional[PerfModel] = None,\n        stats: Optional[Union[Stats, List[Stats]]] = None,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        debug: bool = True,\n    ) -> None:\n        if topology is None:\n            topology = Topology(\n                local_world_size=get_local_size(),\n                world_size=dist.get_world_size(),\n                compute_device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n            )\n        self._topology: Topology = topology\n        self._batch_size: int = batch_size if batch_size else BATCH_SIZE\n        self._constraints = constraints\n        self._enumerator: Enumerator = (\n            enumerator\n            if enumerator\n            else EmbeddingEnumerator(\n                topology=topology,\n                batch_size=self._batch_size,\n                constraints=constraints,\n            )\n        )\n        self._storage_reservation: StorageReservation = (\n            storage_reservation\n            if storage_reservation\n            else HeuristicalStorageReservation(percentage=0.15)\n        )\n        self._partitioner: Partitioner = (\n            partitioner if partitioner else GreedyPerfPartitioner()\n        )\n        if proposer:\n            self._proposers: List[Proposer] = (\n                [proposer] if not isinstance(proposer, list) else proposer\n            )\n        else:\n            self._proposers = [\n                GridSearchProposer(),\n                GreedyProposer(),\n                GreedyProposer(use_depth=False),\n                UniformProposer(),\n            ]\n        self._perf_model: PerfModel = (\n            performance_model if performance_model else NoopPerfModel(topology=topology)\n        )\n\n        if stats:\n            self._stats: List[Stats] = [stats] if not isinstance(stats, list) else stats\n        else:\n            self._stats = [EmbeddingStats()]\n\n        self._debug = debug\n        self._num_proposals: int = 0\n        self._num_plans: int = 0\n        self._best_plan: Optional[List[ShardingOption]] = None",
  "def collective_plan(\n        self,\n        module: nn.Module,\n        sharders: Optional[List[ModuleSharder[nn.Module]]] = None,\n        pg: Optional[dist.ProcessGroup] = dist.GroupMember.WORLD,\n    ) -> ShardingPlan:\n        \"\"\"\n        Call self.plan(...) on rank 0 and broadcast\n        \"\"\"\n        assert pg is not None, \"Process group is not initialized\"\n        if sharders is None:\n            sharders = get_default_sharders()\n        return invoke_on_rank_and_broadcast_result(\n            pg,\n            0,\n            self.plan,\n            module,\n            sharders,\n        )",
  "def plan(\n        self,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n    ) -> ShardingPlan:\n\n        self._num_proposals = 0\n        self._num_plans = 0\n        start_time = perf_counter()\n        best_plan = None\n        lowest_storage = Storage(MAX_SIZE, MAX_SIZE)\n        last_planner_error: Optional[PlannerError] = None\n        best_perf_rating = MAX_SIZE\n\n        storage_constraint: Topology = self._storage_reservation.reserve(\n            topology=self._topology,\n            batch_size=self._batch_size,\n            module=module,\n            sharders=sharders,\n            constraints=self._constraints,\n        )\n\n        search_space = self._enumerator.enumerate(\n            module=module,\n            sharders=sharders,\n        )\n        if not search_space:\n            # No shardable parameters\n            return ShardingPlan({})\n\n        proposal_cache: Dict[\n            Tuple[int, ...],\n            Tuple[bool, Optional[List[ShardingOption]], Optional[float]],\n        ] = {}\n\n        for proposer in self._proposers:\n            proposer.load(search_space=search_space)\n\n        for proposer in self._proposers:\n            proposal = proposer.propose()\n\n            while proposal:\n                proposal_key = tuple(sorted(map(hash, proposal)))\n                if proposal_key in proposal_cache:\n                    partitionable, plan, perf_rating = proposal_cache[proposal_key]\n                    proposer.feedback(\n                        partitionable=partitionable,\n                        plan=plan,\n                        perf_rating=perf_rating,\n                    )\n                    proposal = proposer.propose()\n                    continue\n\n                self._num_proposals += 1\n                try:\n                    # plan is just proposal where shard.rank is populated\n                    plan = self._partitioner.partition(\n                        proposal=proposal,\n                        storage_constraint=storage_constraint,\n                    )\n                    self._num_plans += 1\n                    perf_rating = self._perf_model.rate(plan=plan)\n                    if perf_rating < best_perf_rating:\n                        best_perf_rating = perf_rating\n                        best_plan = copy.deepcopy(plan)\n                    proposal_cache[proposal_key] = (True, plan, perf_rating)\n                    proposer.feedback(\n                        partitionable=True, plan=plan, perf_rating=perf_rating\n                    )\n                except PlannerError as planner_error:\n                    last_planner_error = planner_error\n                    current_storage = cast(\n                        Storage,\n                        reduce(\n                            lambda x, y: x + y,\n                            [\n                                shard.storage\n                                for option in proposal\n                                for shard in option.shards\n                            ],\n                        ),\n                    )\n                    if current_storage < lowest_storage:\n                        lowest_storage = current_storage\n                    proposal_cache[proposal_key] = (False, None, None)\n                    proposer.feedback(partitionable=False)\n\n                # clear shard.rank for each sharding_option\n                reset_shard_rank(proposal)\n                proposal = proposer.propose()\n\n        if best_plan:\n            self._best_plan = best_plan\n            sharding_plan = _to_sharding_plan(best_plan, self._topology)\n\n            end_time = perf_counter()\n            for stats in self._stats:\n                stats.log(\n                    sharding_plan=sharding_plan,\n                    topology=self._topology,\n                    batch_size=self._batch_size,\n                    storage_reservation=self._storage_reservation,\n                    num_proposals=self._num_proposals,\n                    num_plans=self._num_plans,\n                    run_time=end_time - start_time,\n                    best_plan=best_plan,\n                    constraints=self._constraints,\n                    sharders=sharders,\n                    debug=self._debug,\n                )\n            return sharding_plan\n        else:\n            global_storage_capacity = reduce(\n                lambda x, y: x + y,\n                [device.storage for device in self._topology.devices],\n            )\n            global_storage_constraints = reduce(\n                lambda x, y: x + y,\n                [device.storage for device in storage_constraint.devices],\n            )\n            storage_reservation_solution = (\n                (\n                    f\"\\n\\t  Storage reservation percentage: {self._storage_reservation._percentage}, \"\n                    f\"\\n\\t  Per rank reservation for dense storage: {storage_repr_in_gb(self._storage_reservation._dense_storage)}, \"\n                    f\"\\n\\t  Per rank reservation for kjt storage: {storage_repr_in_gb(self._storage_reservation._kjt_storage)}, \"  # pyre-ignore[16]\n                )\n                if isinstance(self._storage_reservation, HeuristicalStorageReservation)\n                else f\"\\n\\t  Storage reservation percentage: {self._storage_reservation._percentage}, \"  # pyre-ignore[16]\n            )\n            no_plan_solution = (\n                f\"Planner evaluated {self._num_proposals} proposals.\"\n                \"\\nPossible solutions:\"\n                f\"\\n  1) Increase the number of devices ({self._topology.world_size})\"\n                f\"\\n  2) Reduce the model size (\"\n                f\"\\n\\t  Global storage: {round(bytes_to_gb(global_storage_capacity.hbm), 3)} GB, \"\n                f\"\\n\\t  Per rank hardware memory: {storage_repr_in_gb(self._topology.devices[0].storage)}, \"\n                f\"{storage_reservation_solution}\"\n                f\"\\n\\t  Global storage available for model parallel: {storage_repr_in_gb(global_storage_constraints)}, \"\n                f\"\\n\\t  Global storage requirement for model parallel: {storage_repr_in_gb(lowest_storage)})\"\n                f\"\\n  3) Reduce local batch size ({self._batch_size})\"\n                \"\\n  4) Remove planner constraints that might be reducing search space or available storage\\n\"\n            )\n            last_planner_error_info = f\"Last planner error: \\n\\t{last_planner_error}\\n\"\n            if not lowest_storage.fits_in(global_storage_constraints):\n                raise PlannerError(\n                    error_type=PlannerErrorType.INSUFFICIENT_STORAGE,\n                    message=\"Unable to find a plan for this model because of insufficient storage. \\n\"\n                    + no_plan_solution\n                    + last_planner_error_info,\n                )\n            else:\n                raise PlannerError(\n                    error_type=PlannerErrorType.STRICT_CONSTRAINTS,\n                    message=\"Unable to find a plan for this model because of the strict constraints. \\n\"\n                    + no_plan_solution\n                    + last_planner_error_info,\n                )",
  "def kernel_bw_lookup(\n    compute_device: str,\n    compute_kernel: str,\n    hbm_mem_bw: float,\n    ddr_mem_bw: float,\n    caching_ratio: Optional[float] = None,\n) -> Optional[float]:\n    \"\"\"\n    Calculates the device bandwidth based on given compute device, compute kernel, and\n    caching ratio.\n\n    Args:\n        compute_kernel (str): compute kernel.\n        compute_device (str): compute device.\n        hbm_mem_bw (float): the bandwidth of the device HBM.\n        ddr_mem_bw (float): the bandwidth of the system DDR memory.\n        caching_ratio (Optional[float]): caching ratio used to determine device bandwidth\n            if UVM caching is enabled.\n\n    Returns:\n        Optional[float]: the device bandwidth.\n    \"\"\"\n    caching_ratio = caching_ratio if caching_ratio else UVM_CACHING_RATIO\n    lookup = {\n        # CPU\n        (\"cpu\", EmbeddingComputeKernel.DENSE.value): 0.5 * ddr_mem_bw,\n        (\"cpu\", EmbeddingComputeKernel.FUSED.value): 1 * ddr_mem_bw,\n        (\"cpu\", EmbeddingComputeKernel.QUANT.value): 1 * ddr_mem_bw,\n        # CUDA\n        (\"cuda\", EmbeddingComputeKernel.DENSE.value): 0.5 * hbm_mem_bw,\n        (\"cuda\", EmbeddingComputeKernel.FUSED.value): 1 * hbm_mem_bw,\n        (\"cuda\", EmbeddingComputeKernel.FUSED_UVM.value): ddr_mem_bw / 10,\n        (\"cuda\", EmbeddingComputeKernel.FUSED_UVM_CACHING.value): (\n            caching_ratio * hbm_mem_bw + (1 - caching_ratio) * ddr_mem_bw\n        )\n        / 10,\n        (\"cuda\", EmbeddingComputeKernel.QUANT.value): 1 * hbm_mem_bw,\n        (\"cuda\", EmbeddingComputeKernel.QUANT_UVM.value): ddr_mem_bw / 10,\n        (\"cuda\", EmbeddingComputeKernel.QUANT_UVM_CACHING.value): (\n            caching_ratio * hbm_mem_bw + (1 - caching_ratio) * ddr_mem_bw\n        )\n        / 10,\n    }\n    return lookup.get((compute_device, compute_kernel))",
  "class Perf:\n    \"\"\"\n    Representation of the breakdown of the perf estimate a single shard of an\n    embedding table.\n    \"\"\"\n\n    fwd_compute: float\n    fwd_comms: float\n    bwd_compute: float\n    bwd_comms: float\n\n    @property\n    def total(self) -> float:\n        return self.fwd_compute + self.bwd_compute + self.fwd_comms + self.bwd_comms\n\n    def __add__(self, other: \"Perf\") -> \"Perf\":\n        return Perf(\n            fwd_compute=self.fwd_compute + other.fwd_compute,\n            fwd_comms=self.fwd_comms + other.fwd_comms,\n            bwd_compute=self.bwd_compute + other.bwd_compute,\n            bwd_comms=self.bwd_comms + other.bwd_comms,\n        )\n\n    def __hash__(self) -> int:\n        return hash(\n            (\n                self.fwd_compute,\n                self.fwd_comms,\n                self.bwd_compute,\n                self.bwd_comms,\n            )\n        )",
  "class Storage:\n    \"\"\"\n    Representation of the storage capacities of a hardware used in training.\n    \"\"\"\n\n    hbm: int\n    ddr: int\n\n    def __add__(self, other: \"Storage\") -> \"Storage\":\n        return Storage(\n            hbm=self.hbm + other.hbm,\n            ddr=self.ddr + other.ddr,\n        )\n\n    def __sub__(self, other: \"Storage\") -> \"Storage\":\n        return Storage(\n            hbm=self.hbm - other.hbm,\n            ddr=self.ddr - other.ddr,\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.hbm, self.ddr))\n\n    def fits_in(self, other: \"Storage\") -> bool:\n        return self.hbm <= other.hbm and self.ddr <= other.ddr",
  "class DeviceHardware:\n    \"\"\"\n    Representation of a device in a process group. 'perf' is an estimation of network,\n    CPU, and storage usages.\n    \"\"\"\n\n    rank: int\n    storage: Storage\n    perf: Perf",
  "class Topology:\n    def __init__(\n        self,\n        world_size: int,\n        compute_device: str,\n        hbm_cap: Optional[int] = None,\n        ddr_cap: Optional[int] = None,\n        local_world_size: Optional[int] = None,\n        hbm_mem_bw: float = HBM_MEM_BW,\n        ddr_mem_bw: float = DDR_MEM_BW,\n        intra_host_bw: float = INTRA_NODE_BANDWIDTH,\n        inter_host_bw: float = CROSS_NODE_BANDWIDTH,\n    ) -> None:\n        \"\"\"\n        Representation of a network of devices in a cluster.\n        \"\"\"\n        # validate input\n        assert compute_device in [\n            \"cpu\",\n            \"cuda\",\n        ], f\"unsupported compute device {compute_device}\"\n\n        self._compute_device = compute_device\n        self._world_size = world_size\n\n        hbm_per_device = 0\n        if self._compute_device == \"cuda\":\n            hbm_per_device = hbm_cap if hbm_cap else HBM_CAP\n        ddr_cap = ddr_cap if ddr_cap else DDR_CAP\n\n        self._devices: List[DeviceHardware] = []\n        for rank in range(world_size):\n            self._devices.append(\n                DeviceHardware(\n                    rank=rank,\n                    storage=Storage(hbm=hbm_per_device, ddr=ddr_cap),\n                    perf=Perf(fwd_compute=0, fwd_comms=0, bwd_compute=0, bwd_comms=0),\n                )\n            )\n\n        self._local_world_size: int = (\n            local_world_size if local_world_size else world_size\n        )\n        self._hbm_mem_bw = hbm_mem_bw\n        self._ddr_mem_bw = ddr_mem_bw\n        self._intra_host_bw = intra_host_bw\n        self._inter_host_bw = inter_host_bw\n\n    @property\n    def compute_device(self) -> str:\n        return self._compute_device\n\n    @property\n    def devices(self) -> List[DeviceHardware]:\n        return self._devices\n\n    @property\n    def world_size(self) -> int:\n        return self._world_size\n\n    @property\n    def local_world_size(self) -> int:\n        return self._local_world_size\n\n    @property\n    def hbm_mem_bw(self) -> float:\n        return self._hbm_mem_bw\n\n    @property\n    def ddr_mem_bw(self) -> float:\n        return self._ddr_mem_bw\n\n    @property\n    def intra_host_bw(self) -> float:\n        return self._intra_host_bw\n\n    @property\n    def inter_host_bw(self) -> float:\n        return self._inter_host_bw\n\n    def __repr__(self) -> str:\n        topology_repr: str = f\"world_size={self._world_size} \\n\"\n        topology_repr += f\"compute_device={self._compute_device}\\n\"\n        topology_repr += \"devices=\\n\"\n        for idx, device in enumerate(self._devices):\n            topology_repr += f\"\\tdevice {idx} {device}\\n\"\n        topology_repr += f\"local_world_size={self._local_world_size} \\n\"\n        topology_repr += f\"intra_host_bw={self._intra_host_bw} \\n\"\n        topology_repr += f\"inter_host_bw={self._inter_host_bw} \\n\"\n        return topology_repr",
  "class Shard:\n    \"\"\"\n    Representation of a subset of an embedding table. 'size' and 'offset' fully\n    determine the tensors in the shard. 'storage' is an estimation of how much it takes\n    to store the shard with an estimation 'perf'.\n    \"\"\"\n\n    size: List[int]\n    offset: List[int]\n    storage: Optional[Storage] = None\n    perf: Optional[Perf] = None\n    rank: Optional[int] = None\n\n    def __hash__(self) -> int:\n        return hash(\n            (\n                tuple(self.size),\n                tuple(self.offset),\n                self.storage,\n                self.perf,\n                self.rank,\n            )\n        )",
  "class ShardingOption:\n    \"\"\"\n    One way of sharding an embedding table.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        tensor: torch.Tensor,\n        module: Tuple[str, nn.Module],\n        input_lengths: List[float],\n        batch_size: int,\n        sharding_type: str,\n        partition_by: str,\n        compute_kernel: str,\n        shards: List[Shard],\n        cache_params: Optional[CacheParams] = None,\n        enforce_hbm: Optional[bool] = None,\n        stochastic_rounding: Optional[bool] = None,\n        bounds_check_mode: Optional[BoundsCheckMode] = None,\n        dependency: Optional[str] = None,\n    ) -> None:\n        self.name = name\n        self._tensor = tensor\n        self._module = module\n        self.input_lengths = input_lengths\n        self.batch_size = batch_size\n        self.sharding_type = sharding_type\n        self.partition_by = partition_by\n        self.compute_kernel = compute_kernel\n        # relevant to planner output, must be populated if sharding option\n        # part of final solution\n        self.shards = shards\n        self.cache_params = cache_params\n        self.enforce_hbm = enforce_hbm\n        self.stochastic_rounding = stochastic_rounding\n        self.bounds_check_mode = bounds_check_mode\n        self.dependency = dependency\n        self._is_pooled: Optional[bool] = None\n        self.is_weighted: Optional[bool] = None\n\n    @property\n    def tensor(self) -> torch.Tensor:\n        return self._tensor\n\n    @property\n    def module(self) -> Tuple[str, nn.Module]:\n        return self._module\n\n    @property\n    def fqn(self) -> str:\n        return self.module[0] + \".\" + self.name\n\n    @property\n    def path(self) -> str:\n        return self.module[0]\n\n    @property\n    def num_shards(self) -> int:\n        return len(self.shards)\n\n    @property\n    def num_inputs(self) -> int:\n        return len(self.input_lengths)\n\n    @property\n    def total_storage(self) -> Storage:\n        storage: Storage = Storage(hbm=0, ddr=0)\n        for shard in self.shards:\n            storage += cast(Storage, shard.storage)\n        return storage\n\n    @property\n    def is_pooled(self) -> bool:\n        if self._is_pooled is not None:\n            return self._is_pooled\n\n        if isinstance(self.module[1], EmbeddingCollectionInterface):\n            self._is_pooled = False\n            return self.is_pooled\n        for module in self.module[1].modules():\n            if isinstance(module, EmbeddingCollectionInterface):\n                for name, _ in module.named_parameters():\n                    if self.name in name:\n                        self._is_pooled = False\n                        return self._is_pooled\n        self._is_pooled = True\n        return self._is_pooled\n\n    def __hash__(self) -> int:\n        return hash(\n            (\n                self.fqn,\n                self.sharding_type,\n                self.compute_kernel,\n                tuple(self.shards),\n            )\n        )\n\n    def __deepcopy__(\n        self, memo: Optional[Dict[int, \"ShardingOption\"]]\n    ) -> \"ShardingOption\":\n        cls = self.__class__\n        result = cls.__new__(cls)\n        for k, v in self.__dict__.items():\n            if k in [\"_tensor\", \"_module\"]:\n                setattr(result, k, v)\n            else:\n                setattr(result, k, deepcopy(v, memo))\n        return result",
  "class PartitionByType(Enum):\n    \"\"\"\n    Well-known partition types.\n    \"\"\"\n\n    # Partitioning based on device\n    DEVICE = \"device\"\n    # Partitioning based on host\n    HOST = \"host\"\n    # Uniform, (ie. fixed layout)\n    UNIFORM = \"uniform\"",
  "class ParameterConstraints:\n    \"\"\"\n    Stores user provided constraints around the sharding plan.\n\n    If provided, `pooling_factors`, `num_poolings`, and `batch_sizes` must match in\n    length, as per sample.\n    \"\"\"\n\n    sharding_types: Optional[List[str]] = None\n    compute_kernels: Optional[List[str]] = None\n    min_partition: Optional[int] = None  # CW sharding\n    pooling_factors: List[float] = field(\n        default_factory=lambda: [POOLING_FACTOR]\n    )  # average number of embedding lookups required per sample\n    num_poolings: Optional[List[float]] = None  # number of poolings per sample in batch\n    batch_sizes: Optional[List[int]] = None  # batch size per input feature\n    is_weighted: bool = False\n    cache_params: Optional[CacheParams] = None\n    enforce_hbm: Optional[bool] = None\n    stochastic_rounding: Optional[bool] = None\n    bounds_check_mode: Optional[BoundsCheckMode] = None",
  "class PlannerErrorType(Enum):\n    \"\"\"\n    Classify PlannerError based on the following cases.\n    \"\"\"\n\n    INSUFFICIENT_STORAGE = \"insufficient_storage\"\n    STRICT_CONSTRAINTS = \"strict_constraints\"\n    PARTITION = \"partition\"\n    OTHER = \"other\"",
  "class PlannerError(Exception):\n    def __init__(\n        self,\n        message: str,\n        error_type: PlannerErrorType = PlannerErrorType.OTHER,\n    ) -> None:\n        self.error_type = error_type\n        super().__init__(message)",
  "class StorageReservation(abc.ABC):\n    \"\"\"\n    Reserves storage space for non-sharded parts of the model.\n    \"\"\"\n\n    @abc.abstractmethod\n    def reserve(\n        self,\n        topology: Topology,\n        batch_size: int,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> Topology:\n        ...",
  "class PerfModel(abc.ABC):\n    @abc.abstractmethod\n    def rate(self, plan: List[ShardingOption]) -> float:\n        ...",
  "class ShardEstimator(abc.ABC):\n    \"\"\"\n    Estimates shard perf or storage, requires fully specified sharding options.\n    \"\"\"\n\n    @abc.abstractmethod\n    def __init__(\n        self,\n        topology: Topology,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> None:\n        ...\n\n    @abc.abstractmethod\n    def estimate(\n        self,\n        sharding_options: List[ShardingOption],\n        sharder_map: Optional[Dict[str, ModuleSharder[nn.Module]]] = None,\n    ) -> None:\n        # update sharding_options with per shard estimate in-place\n        ...",
  "class Enumerator(abc.ABC):\n    \"\"\"\n    Generates all relevant sharding options for given topology, constraints, nn.Module,\n    and sharders.\n    \"\"\"\n\n    @abc.abstractmethod\n    def __init__(\n        self,\n        topology: Topology,\n        batch_size: int = BATCH_SIZE,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        estimator: Optional[Union[ShardEstimator, List[ShardEstimator]]] = None,\n    ) -> None:\n        ...\n\n    @abc.abstractmethod\n    def enumerate(\n        self,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n    ) -> List[ShardingOption]:\n        \"\"\"\n        See class description.\n        \"\"\"\n        ...",
  "class Proposer(abc.ABC):\n    \"\"\"\n    Prosposes complete lists of sharding options which can be parititioned to generate a\n    plan.\n    \"\"\"\n\n    @abc.abstractmethod\n    def load(\n        self,\n        search_space: List[ShardingOption],\n    ) -> None:\n        ...\n\n    @abc.abstractmethod\n    def feedback(\n        self,\n        partitionable: bool,\n        plan: Optional[List[ShardingOption]] = None,\n        perf_rating: Optional[float] = None,\n    ) -> None:\n        ...\n\n    @abc.abstractmethod\n    def propose(self) -> Optional[List[ShardingOption]]:\n        ...",
  "class Partitioner(abc.ABC):\n    \"\"\"\n    Partitions shards.\n\n    Today we have multiple strategies ie. (Greedy, BLDM, Linear).\n    \"\"\"\n\n    @abc.abstractmethod\n    def partition(\n        self,\n        proposal: List[ShardingOption],\n        storage_constraint: Topology,\n    ) -> List[ShardingOption]:\n        # modifies sharding_options and topology in-place\n        ...",
  "class Stats(abc.ABC):\n    \"\"\"\n    Logs statistics related to the sharding plan.\n    \"\"\"\n\n    @abc.abstractmethod\n    def log(\n        self,\n        sharding_plan: ShardingPlan,\n        topology: Topology,\n        batch_size: int,\n        storage_reservation: StorageReservation,\n        num_proposals: int,\n        num_plans: int,\n        run_time: float,\n        best_plan: List[ShardingOption],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        sharders: Optional[List[ModuleSharder[nn.Module]]] = None,\n        debug: bool = False,\n    ) -> None:\n        \"\"\"\n        See class description\n        \"\"\"\n        ...",
  "def total(self) -> float:\n        return self.fwd_compute + self.bwd_compute + self.fwd_comms + self.bwd_comms",
  "def __add__(self, other: \"Perf\") -> \"Perf\":\n        return Perf(\n            fwd_compute=self.fwd_compute + other.fwd_compute,\n            fwd_comms=self.fwd_comms + other.fwd_comms,\n            bwd_compute=self.bwd_compute + other.bwd_compute,\n            bwd_comms=self.bwd_comms + other.bwd_comms,\n        )",
  "def __hash__(self) -> int:\n        return hash(\n            (\n                self.fwd_compute,\n                self.fwd_comms,\n                self.bwd_compute,\n                self.bwd_comms,\n            )\n        )",
  "def __add__(self, other: \"Storage\") -> \"Storage\":\n        return Storage(\n            hbm=self.hbm + other.hbm,\n            ddr=self.ddr + other.ddr,\n        )",
  "def __sub__(self, other: \"Storage\") -> \"Storage\":\n        return Storage(\n            hbm=self.hbm - other.hbm,\n            ddr=self.ddr - other.ddr,\n        )",
  "def __hash__(self) -> int:\n        return hash((self.hbm, self.ddr))",
  "def fits_in(self, other: \"Storage\") -> bool:\n        return self.hbm <= other.hbm and self.ddr <= other.ddr",
  "def __init__(\n        self,\n        world_size: int,\n        compute_device: str,\n        hbm_cap: Optional[int] = None,\n        ddr_cap: Optional[int] = None,\n        local_world_size: Optional[int] = None,\n        hbm_mem_bw: float = HBM_MEM_BW,\n        ddr_mem_bw: float = DDR_MEM_BW,\n        intra_host_bw: float = INTRA_NODE_BANDWIDTH,\n        inter_host_bw: float = CROSS_NODE_BANDWIDTH,\n    ) -> None:\n        \"\"\"\n        Representation of a network of devices in a cluster.\n        \"\"\"\n        # validate input\n        assert compute_device in [\n            \"cpu\",\n            \"cuda\",\n        ], f\"unsupported compute device {compute_device}\"\n\n        self._compute_device = compute_device\n        self._world_size = world_size\n\n        hbm_per_device = 0\n        if self._compute_device == \"cuda\":\n            hbm_per_device = hbm_cap if hbm_cap else HBM_CAP\n        ddr_cap = ddr_cap if ddr_cap else DDR_CAP\n\n        self._devices: List[DeviceHardware] = []\n        for rank in range(world_size):\n            self._devices.append(\n                DeviceHardware(\n                    rank=rank,\n                    storage=Storage(hbm=hbm_per_device, ddr=ddr_cap),\n                    perf=Perf(fwd_compute=0, fwd_comms=0, bwd_compute=0, bwd_comms=0),\n                )\n            )\n\n        self._local_world_size: int = (\n            local_world_size if local_world_size else world_size\n        )\n        self._hbm_mem_bw = hbm_mem_bw\n        self._ddr_mem_bw = ddr_mem_bw\n        self._intra_host_bw = intra_host_bw\n        self._inter_host_bw = inter_host_bw",
  "def compute_device(self) -> str:\n        return self._compute_device",
  "def devices(self) -> List[DeviceHardware]:\n        return self._devices",
  "def world_size(self) -> int:\n        return self._world_size",
  "def local_world_size(self) -> int:\n        return self._local_world_size",
  "def hbm_mem_bw(self) -> float:\n        return self._hbm_mem_bw",
  "def ddr_mem_bw(self) -> float:\n        return self._ddr_mem_bw",
  "def intra_host_bw(self) -> float:\n        return self._intra_host_bw",
  "def inter_host_bw(self) -> float:\n        return self._inter_host_bw",
  "def __repr__(self) -> str:\n        topology_repr: str = f\"world_size={self._world_size} \\n\"\n        topology_repr += f\"compute_device={self._compute_device}\\n\"\n        topology_repr += \"devices=\\n\"\n        for idx, device in enumerate(self._devices):\n            topology_repr += f\"\\tdevice {idx} {device}\\n\"\n        topology_repr += f\"local_world_size={self._local_world_size} \\n\"\n        topology_repr += f\"intra_host_bw={self._intra_host_bw} \\n\"\n        topology_repr += f\"inter_host_bw={self._inter_host_bw} \\n\"\n        return topology_repr",
  "def __hash__(self) -> int:\n        return hash(\n            (\n                tuple(self.size),\n                tuple(self.offset),\n                self.storage,\n                self.perf,\n                self.rank,\n            )\n        )",
  "def __init__(\n        self,\n        name: str,\n        tensor: torch.Tensor,\n        module: Tuple[str, nn.Module],\n        input_lengths: List[float],\n        batch_size: int,\n        sharding_type: str,\n        partition_by: str,\n        compute_kernel: str,\n        shards: List[Shard],\n        cache_params: Optional[CacheParams] = None,\n        enforce_hbm: Optional[bool] = None,\n        stochastic_rounding: Optional[bool] = None,\n        bounds_check_mode: Optional[BoundsCheckMode] = None,\n        dependency: Optional[str] = None,\n    ) -> None:\n        self.name = name\n        self._tensor = tensor\n        self._module = module\n        self.input_lengths = input_lengths\n        self.batch_size = batch_size\n        self.sharding_type = sharding_type\n        self.partition_by = partition_by\n        self.compute_kernel = compute_kernel\n        # relevant to planner output, must be populated if sharding option\n        # part of final solution\n        self.shards = shards\n        self.cache_params = cache_params\n        self.enforce_hbm = enforce_hbm\n        self.stochastic_rounding = stochastic_rounding\n        self.bounds_check_mode = bounds_check_mode\n        self.dependency = dependency\n        self._is_pooled: Optional[bool] = None\n        self.is_weighted: Optional[bool] = None",
  "def tensor(self) -> torch.Tensor:\n        return self._tensor",
  "def module(self) -> Tuple[str, nn.Module]:\n        return self._module",
  "def fqn(self) -> str:\n        return self.module[0] + \".\" + self.name",
  "def path(self) -> str:\n        return self.module[0]",
  "def num_shards(self) -> int:\n        return len(self.shards)",
  "def num_inputs(self) -> int:\n        return len(self.input_lengths)",
  "def total_storage(self) -> Storage:\n        storage: Storage = Storage(hbm=0, ddr=0)\n        for shard in self.shards:\n            storage += cast(Storage, shard.storage)\n        return storage",
  "def is_pooled(self) -> bool:\n        if self._is_pooled is not None:\n            return self._is_pooled\n\n        if isinstance(self.module[1], EmbeddingCollectionInterface):\n            self._is_pooled = False\n            return self.is_pooled\n        for module in self.module[1].modules():\n            if isinstance(module, EmbeddingCollectionInterface):\n                for name, _ in module.named_parameters():\n                    if self.name in name:\n                        self._is_pooled = False\n                        return self._is_pooled\n        self._is_pooled = True\n        return self._is_pooled",
  "def __hash__(self) -> int:\n        return hash(\n            (\n                self.fqn,\n                self.sharding_type,\n                self.compute_kernel,\n                tuple(self.shards),\n            )\n        )",
  "def __deepcopy__(\n        self, memo: Optional[Dict[int, \"ShardingOption\"]]\n    ) -> \"ShardingOption\":\n        cls = self.__class__\n        result = cls.__new__(cls)\n        for k, v in self.__dict__.items():\n            if k in [\"_tensor\", \"_module\"]:\n                setattr(result, k, v)\n            else:\n                setattr(result, k, deepcopy(v, memo))\n        return result",
  "def __init__(\n        self,\n        message: str,\n        error_type: PlannerErrorType = PlannerErrorType.OTHER,\n    ) -> None:\n        self.error_type = error_type\n        super().__init__(message)",
  "def reserve(\n        self,\n        topology: Topology,\n        batch_size: int,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> Topology:\n        ...",
  "def rate(self, plan: List[ShardingOption]) -> float:\n        ...",
  "def __init__(\n        self,\n        topology: Topology,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> None:\n        ...",
  "def estimate(\n        self,\n        sharding_options: List[ShardingOption],\n        sharder_map: Optional[Dict[str, ModuleSharder[nn.Module]]] = None,\n    ) -> None:\n        # update sharding_options with per shard estimate in-place\n        ...",
  "def __init__(\n        self,\n        topology: Topology,\n        batch_size: int = BATCH_SIZE,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        estimator: Optional[Union[ShardEstimator, List[ShardEstimator]]] = None,\n    ) -> None:\n        ...",
  "def enumerate(\n        self,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n    ) -> List[ShardingOption]:\n        \"\"\"\n        See class description.\n        \"\"\"\n        ...",
  "def load(\n        self,\n        search_space: List[ShardingOption],\n    ) -> None:\n        ...",
  "def feedback(\n        self,\n        partitionable: bool,\n        plan: Optional[List[ShardingOption]] = None,\n        perf_rating: Optional[float] = None,\n    ) -> None:\n        ...",
  "def propose(self) -> Optional[List[ShardingOption]]:\n        ...",
  "def partition(\n        self,\n        proposal: List[ShardingOption],\n        storage_constraint: Topology,\n    ) -> List[ShardingOption]:\n        # modifies sharding_options and topology in-place\n        ...",
  "def log(\n        self,\n        sharding_plan: ShardingPlan,\n        topology: Topology,\n        batch_size: int,\n        storage_reservation: StorageReservation,\n        num_proposals: int,\n        num_plans: int,\n        run_time: float,\n        best_plan: List[ShardingOption],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        sharders: Optional[List[ModuleSharder[nn.Module]]] = None,\n        debug: bool = False,\n    ) -> None:\n        \"\"\"\n        See class description\n        \"\"\"\n        ...",
  "def _get_module_size(module: nn.Module, multiplier: float) -> int:\n    parameters_size = sum(\n        [\n            multiplier * parameter.element_size() * parameter.nelement()\n            for parameter in module.parameters()\n        ]\n    )\n\n    buffers_size = sum(\n        [buffer.element_size() * buffer.nelement() for buffer in module.buffers()]\n    )\n\n    return round(parameters_size + buffers_size)",
  "def _get_dense_tensor_size(\n    module: nn.Module,\n    shardable_modules: Set[nn.Module],\n    multiplier: float = 6.0,\n) -> int:\n    dense_tensor_size = _get_module_size(module, multiplier) - sum(\n        [\n            _get_module_size(shardable_module, multiplier)\n            for shardable_module in shardable_modules\n        ]\n    )\n    return dense_tensor_size",
  "def _reserve_dense_storage(\n    topology: Topology,\n    module: nn.Module,\n    shardable_modules: Set[nn.Module],\n    multiplier: float,\n    dense_tensor_estimate: Optional[int] = None,\n) -> Storage:\n\n    dense_tensor_size = _get_dense_tensor_size(module, shardable_modules, multiplier)\n    if dense_tensor_estimate:\n        logger.info(\n            f\"We override default dense tensor estimate ({dense_tensor_size} bytes) \"\n            f\"with user-provided dense tensor estimate ({dense_tensor_estimate} bytes).\"\n        )\n        dense_tensor_size = dense_tensor_estimate\n\n    dense_tensor_storage = Storage(\n        hbm=dense_tensor_size if topology.compute_device == \"cuda\" else 0,\n        ddr=dense_tensor_size if topology.compute_device == \"cpu\" else 0,\n    )\n\n    for device in topology.devices:\n        device.storage -= dense_tensor_storage\n\n    return dense_tensor_storage",
  "def _reserve_kjt_storage(\n    topology: Topology,\n    batch_size: int,\n    batch_inputs: List[float],\n    input_data_type_size: int,\n    multiplier: int,\n) -> Storage:\n    kjt_size = math.ceil(sum(batch_inputs) * float(input_data_type_size)) * multiplier\n\n    kjt_storage = Storage(\n        hbm=kjt_size if topology.compute_device == \"cuda\" else 0,\n        ddr=kjt_size if topology.compute_device == \"cpu\" else 0,\n    )\n\n    for device in topology.devices:\n        device.storage -= kjt_storage\n\n    return kjt_storage",
  "def _reserve_storage_percentage(topology: Topology, percent: float) -> None:\n    for device in topology.devices:\n        device.storage.hbm = int((1 - percent) * device.storage.hbm)",
  "def _get_batch_inputs_and_shardable_parameters(\n    module: nn.Module,\n    sharders: List[ModuleSharder[nn.Module]],\n    batch_size: int,\n    constraints: Optional[Dict[str, ParameterConstraints]] = None,\n) -> Tuple[List[float], Set[nn.Module]]:\n    sharder_map: Dict[str, ModuleSharder[nn.Module]] = {\n        sharder_name(sharder.module_type): sharder for sharder in sharders\n    }\n    input_lengths: List[float] = []\n    batch_sizes: List[int] = []\n    shardable_modules: Set[nn.Module] = set()\n\n    def populate_shardable_modules(\n        module: nn.Module,\n    ) -> None:\n        sharder_key = sharder_name(type(module))\n        sharder = sharder_map.get(sharder_key)\n\n        if not sharder:\n            for _child_name, child in module.named_children():\n                populate_shardable_modules(child)\n        else:\n            names = sharder.shardable_parameters(module).keys()\n            shardable_modules.add(module)\n\n            for name in names:\n                pooling_factors = (\n                    constraints[name].pooling_factors\n                    if constraints and constraints.get(name)\n                    else [POOLING_FACTOR]\n                )\n                input_lengths.extend(pooling_factors)\n                batch_sizes.extend(\n                    constraints[name].batch_sizes  # pyre-ignore[6]\n                    if constraints\n                    and constraints.get(name)\n                    and constraints[name].batch_sizes\n                    else [batch_size] * len(pooling_factors)\n                )\n\n    populate_shardable_modules(module)\n\n    batch_inputs: List[float] = [\n        input_length * batch_size\n        for input_length, batch_size in zip(input_lengths, batch_sizes)\n    ]\n\n    return batch_inputs, shardable_modules",
  "class FixedPercentageStorageReservation(StorageReservation):\n    def __init__(self, percentage: float) -> None:\n        assert percentage >= 0 and percentage <= 1\n        self._percentage: float = percentage\n\n    def reserve(\n        self,\n        topology: Topology,\n        batch_size: int,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> Topology:\n        reserved_topology = copy.deepcopy(topology)\n        _reserve_storage_percentage(reserved_topology, self._percentage)\n        return reserved_topology",
  "class HeuristicalStorageReservation(StorageReservation):\n    \"\"\"\n    Reserves storage for model to be sharded with heuristical calculation. The storage\n    reservation is comprised of dense tensor storage, KJT storage, and an extra\n    percentage of total storage.\n\n    Args:\n        percentage (float): extra storage percent to reserve that acts as a margin of\n            error beyond heuristic calculation of storage.\n        parameter_multiplier (float): heuristic multiplier for total parameter storage.\n        dense_tensor_estimate (Optional[int]): storage estimate for dense tensors, uses\n            default heuristic estimate if not provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        percentage: float,\n        # heuristic: 6 * dense parameter size\n        # parameter + optimizer (~2x parameter) + ddp (~3x parameter)\n        parameter_multiplier: float = 6.0,\n        dense_tensor_estimate: Optional[int] = None,\n    ) -> None:\n        assert percentage >= 0 and percentage <= 1\n        self._percentage: float = percentage\n        self._parameter_multiplier = parameter_multiplier\n        self._dense_tensor_estimate = dense_tensor_estimate\n\n        self._dense_storage: Optional[Storage] = None\n        self._kjt_storage: Optional[Storage] = None\n\n    def reserve(\n        self,\n        topology: Topology,\n        batch_size: int,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> Topology:\n        reserved_topology = copy.deepcopy(topology)\n\n        batch_inputs, shardable_modules = _get_batch_inputs_and_shardable_parameters(\n            module, sharders, batch_size, constraints\n        )\n\n        _reserve_storage_percentage(reserved_topology, self._percentage)\n\n        self._dense_storage = _reserve_dense_storage(\n            topology=reserved_topology,\n            module=module,\n            shardable_modules=shardable_modules,\n            multiplier=self._parameter_multiplier,\n            dense_tensor_estimate=self._dense_tensor_estimate,\n        )\n\n        self._kjt_storage = _reserve_kjt_storage(\n            topology=reserved_topology,\n            batch_size=batch_size,\n            batch_inputs=batch_inputs,\n            input_data_type_size=BIGINT_DTYPE,\n            # 2 pipelined batches each with 10 internal copies\n            multiplier=20,\n        )\n\n        if reserved_topology.devices[0].storage.hbm < 0:\n            negative_storage_solution = (\n                f\"The reserved topology ({storage_repr_in_gb(reserved_topology.devices[0].storage)}) \"\n                \"has negative available hbm storage, \"\n                \"after taking into account of the reserved hbm percentage, \"\n                \"the storage for dense modules, and the kjt storages. Hence \"\n                \"it is not possible to find a valid sharding plan. \"\n                \"\\nPossible solutions:\"\n                \"\\n  1) If FSDP is used, consider switching to FixedPercentageStorageReservation, since \"\n                f\"HeuristicalStorageReservation would not be able to calculate the \"\n                f\"dense storage ({storage_repr_in_gb(self._dense_storage)}) correctly. \"\n                f\"\\n  2) Reduce local batch size ({batch_size}), which can help \"\n                f\"reduce the per rank kjt storage ({storage_repr_in_gb(self._kjt_storage)}). \"\n                f\"\\n  3) Decrease the reserved hbm percentage ({self._percentage}). \"\n                \"\\n  4) Use hardware with a higher hbm cap (current hardware has \"\n                f\"{storage_repr_in_gb(topology.devices[0].storage)} per rank). \"\n            )\n            raise PlannerError(\n                error_type=PlannerErrorType.INSUFFICIENT_STORAGE,\n                message=negative_storage_solution,\n            )\n\n        return reserved_topology",
  "class InferenceStorageReservation(StorageReservation):\n    \"\"\"\n    Reserves storage for model to be sharded for inference. The storage reservation\n    is comprised of dense tensor storage, KJT storage, and an extra percentage of total\n    storage. Note that when estimating for storage, dense modules are assumed to be on\n    GPUs and replicated across ranks. If this is not the case, please override the\n    estimates with dense_tensor_estimate.\n\n    Args:\n        percentage (float): extra storage percentage to reserve that acts as a margin of\n            error beyond storage calculation.\n        dense_tensor_estimate (Optional[int]): storage estimate for dense tensors, use\n            default heuristic estimate if not provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        percentage: float,\n        dense_tensor_estimate: Optional[int] = None,\n    ) -> None:\n        assert percentage >= 0 and percentage <= 1\n        self._percentage: float = percentage\n        self._dense_tensor_estimate = dense_tensor_estimate\n\n        self._dense_storage: Optional[Storage] = None\n        self._kjt_storage: Optional[Storage] = None\n\n    def reserve(\n        self,\n        topology: Topology,\n        batch_size: int,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> Topology:\n        reserved_topology = copy.deepcopy(topology)\n\n        batch_inputs, shardable_modules = _get_batch_inputs_and_shardable_parameters(\n            module, sharders, batch_size, constraints\n        )\n\n        _reserve_storage_percentage(reserved_topology, self._percentage)\n\n        self._dense_storage = _reserve_dense_storage(\n            topology=reserved_topology,\n            module=module,\n            shardable_modules=shardable_modules,\n            multiplier=1,\n            dense_tensor_estimate=self._dense_tensor_estimate,\n        )\n\n        self._kjt_storage = _reserve_kjt_storage(\n            topology=reserved_topology,\n            batch_size=batch_size,\n            batch_inputs=batch_inputs,\n            input_data_type_size=BIGINT_DTYPE,\n            multiplier=1,\n        )\n\n        return reserved_topology",
  "def populate_shardable_modules(\n        module: nn.Module,\n    ) -> None:\n        sharder_key = sharder_name(type(module))\n        sharder = sharder_map.get(sharder_key)\n\n        if not sharder:\n            for _child_name, child in module.named_children():\n                populate_shardable_modules(child)\n        else:\n            names = sharder.shardable_parameters(module).keys()\n            shardable_modules.add(module)\n\n            for name in names:\n                pooling_factors = (\n                    constraints[name].pooling_factors\n                    if constraints and constraints.get(name)\n                    else [POOLING_FACTOR]\n                )\n                input_lengths.extend(pooling_factors)\n                batch_sizes.extend(\n                    constraints[name].batch_sizes  # pyre-ignore[6]\n                    if constraints\n                    and constraints.get(name)\n                    and constraints[name].batch_sizes\n                    else [batch_size] * len(pooling_factors)\n                )",
  "def __init__(self, percentage: float) -> None:\n        assert percentage >= 0 and percentage <= 1\n        self._percentage: float = percentage",
  "def reserve(\n        self,\n        topology: Topology,\n        batch_size: int,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> Topology:\n        reserved_topology = copy.deepcopy(topology)\n        _reserve_storage_percentage(reserved_topology, self._percentage)\n        return reserved_topology",
  "def __init__(\n        self,\n        percentage: float,\n        # heuristic: 6 * dense parameter size\n        # parameter + optimizer (~2x parameter) + ddp (~3x parameter)\n        parameter_multiplier: float = 6.0,\n        dense_tensor_estimate: Optional[int] = None,\n    ) -> None:\n        assert percentage >= 0 and percentage <= 1\n        self._percentage: float = percentage\n        self._parameter_multiplier = parameter_multiplier\n        self._dense_tensor_estimate = dense_tensor_estimate\n\n        self._dense_storage: Optional[Storage] = None\n        self._kjt_storage: Optional[Storage] = None",
  "def reserve(\n        self,\n        topology: Topology,\n        batch_size: int,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> Topology:\n        reserved_topology = copy.deepcopy(topology)\n\n        batch_inputs, shardable_modules = _get_batch_inputs_and_shardable_parameters(\n            module, sharders, batch_size, constraints\n        )\n\n        _reserve_storage_percentage(reserved_topology, self._percentage)\n\n        self._dense_storage = _reserve_dense_storage(\n            topology=reserved_topology,\n            module=module,\n            shardable_modules=shardable_modules,\n            multiplier=self._parameter_multiplier,\n            dense_tensor_estimate=self._dense_tensor_estimate,\n        )\n\n        self._kjt_storage = _reserve_kjt_storage(\n            topology=reserved_topology,\n            batch_size=batch_size,\n            batch_inputs=batch_inputs,\n            input_data_type_size=BIGINT_DTYPE,\n            # 2 pipelined batches each with 10 internal copies\n            multiplier=20,\n        )\n\n        if reserved_topology.devices[0].storage.hbm < 0:\n            negative_storage_solution = (\n                f\"The reserved topology ({storage_repr_in_gb(reserved_topology.devices[0].storage)}) \"\n                \"has negative available hbm storage, \"\n                \"after taking into account of the reserved hbm percentage, \"\n                \"the storage for dense modules, and the kjt storages. Hence \"\n                \"it is not possible to find a valid sharding plan. \"\n                \"\\nPossible solutions:\"\n                \"\\n  1) If FSDP is used, consider switching to FixedPercentageStorageReservation, since \"\n                f\"HeuristicalStorageReservation would not be able to calculate the \"\n                f\"dense storage ({storage_repr_in_gb(self._dense_storage)}) correctly. \"\n                f\"\\n  2) Reduce local batch size ({batch_size}), which can help \"\n                f\"reduce the per rank kjt storage ({storage_repr_in_gb(self._kjt_storage)}). \"\n                f\"\\n  3) Decrease the reserved hbm percentage ({self._percentage}). \"\n                \"\\n  4) Use hardware with a higher hbm cap (current hardware has \"\n                f\"{storage_repr_in_gb(topology.devices[0].storage)} per rank). \"\n            )\n            raise PlannerError(\n                error_type=PlannerErrorType.INSUFFICIENT_STORAGE,\n                message=negative_storage_solution,\n            )\n\n        return reserved_topology",
  "def __init__(\n        self,\n        percentage: float,\n        dense_tensor_estimate: Optional[int] = None,\n    ) -> None:\n        assert percentage >= 0 and percentage <= 1\n        self._percentage: float = percentage\n        self._dense_tensor_estimate = dense_tensor_estimate\n\n        self._dense_storage: Optional[Storage] = None\n        self._kjt_storage: Optional[Storage] = None",
  "def reserve(\n        self,\n        topology: Topology,\n        batch_size: int,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n    ) -> Topology:\n        reserved_topology = copy.deepcopy(topology)\n\n        batch_inputs, shardable_modules = _get_batch_inputs_and_shardable_parameters(\n            module, sharders, batch_size, constraints\n        )\n\n        _reserve_storage_percentage(reserved_topology, self._percentage)\n\n        self._dense_storage = _reserve_dense_storage(\n            topology=reserved_topology,\n            module=module,\n            shardable_modules=shardable_modules,\n            multiplier=1,\n            dense_tensor_estimate=self._dense_tensor_estimate,\n        )\n\n        self._kjt_storage = _reserve_kjt_storage(\n            topology=reserved_topology,\n            batch_size=batch_size,\n            batch_inputs=batch_inputs,\n            input_data_type_size=BIGINT_DTYPE,\n            multiplier=1,\n        )\n\n        return reserved_topology",
  "class NoopPerfModel(PerfModel):\n    def __init__(self, topology: Topology) -> None:\n        self._topology = topology\n\n    def rate(self, plan: List[ShardingOption]) -> float:\n        perfs = [0] * self._topology.world_size\n        for sharding_option in plan:\n            for shard in sharding_option.shards:\n                # pyre-ignore [6]: Expected `typing_extensions.SupportsIndex`\n                perfs[shard.rank] += cast(Perf, shard.perf).total\n\n        return max(perfs)",
  "def __init__(self, topology: Topology) -> None:\n        self._topology = topology",
  "def rate(self, plan: List[ShardingOption]) -> float:\n        perfs = [0] * self._topology.world_size\n        for sharding_option in plan:\n            for shard in sharding_option.shards:\n                # pyre-ignore [6]: Expected `typing_extensions.SupportsIndex`\n                perfs[shard.rank] += cast(Perf, shard.perf).total\n\n        return max(perfs)",
  "class EmbeddingEnumerator(Enumerator):\n    \"\"\"\n    Generates embedding sharding options for given `nn.Module`, considering user provided\n    constraints.\n\n    Args:\n        topology (Topology): device topology.\n        batch_size (int): batch size.\n        constraints (Optional[Dict[str, ParameterConstraints]]): dict of parameter names\n            to provided ParameterConstraints.\n    \"\"\"\n\n    def __init__(\n        self,\n        topology: Topology,\n        batch_size: int,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        estimator: Optional[Union[ShardEstimator, List[ShardEstimator]]] = None,\n    ) -> None:\n        self._compute_device: str = topology.compute_device\n        self._world_size: int = topology.world_size\n        self._local_world_size: int = topology.local_world_size\n        self._batch_size: int = batch_size\n        self._constraints = constraints\n        self._sharder_map: Dict[str, ModuleSharder[nn.Module]] = {}\n\n        if estimator:\n            self._estimators: List[ShardEstimator] = (\n                [estimator] if not isinstance(estimator, list) else estimator\n            )\n        else:\n            self._estimators: List[ShardEstimator] = [\n                EmbeddingPerfEstimator(topology=topology, constraints=constraints),\n                EmbeddingStorageEstimator(topology=topology, constraints=constraints),\n            ]\n\n    def enumerate(\n        self,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n    ) -> List[ShardingOption]:\n        \"\"\"\n        Generates relevant sharding options given module and sharders.\n\n        Args:\n            module (nn.Module): module to be sharded.\n            sharders (List[ModuleSharder[nn.Module]]): provided sharders for module.\n\n        Returns:\n            List[ShardingOption]: valid sharding options with values populated.\n        \"\"\"\n\n        self._sharder_map = {\n            sharder_name(sharder.module_type): sharder for sharder in sharders\n        }\n        sharding_options: List[ShardingOption] = []\n\n        named_modules_queue = [(\"\", module)]\n        while named_modules_queue:\n            child_path, child_module = named_modules_queue.pop()\n            sharder_key = sharder_name(type(child_module))\n            sharder = self._sharder_map.get(sharder_key, None)\n            if not sharder:\n                for n, m in child_module.named_children():\n                    if child_path != \"\":\n                        named_modules_queue.append((child_path + \".\" + n, m))\n                    else:\n                        named_modules_queue.append((n, m))\n                continue\n\n            for name, param in sharder.shardable_parameters(child_module).items():\n                (\n                    input_lengths,\n                    col_wise_shard_dim,\n                    cache_params,\n                    enforce_hbm,\n                    stochastic_rounding,\n                    bounds_check_mode,\n                ) = _extract_constraints_for_param(self._constraints, name)\n\n                for sharding_type in self._filter_sharding_types(\n                    name, sharder.sharding_types(self._compute_device)\n                ):\n                    for compute_kernel in self._filter_compute_kernels(\n                        name,\n                        sharder.compute_kernels(sharding_type, self._compute_device),\n                    ):\n                        (\n                            shard_sizes,\n                            shard_offsets,\n                        ) = calculate_shard_sizes_and_offsets(\n                            tensor=param,\n                            world_size=self._world_size,\n                            local_world_size=self._local_world_size,\n                            sharding_type=sharding_type,\n                            col_wise_shard_dim=col_wise_shard_dim,\n                        )\n                        dependency = None\n                        if isinstance(child_module, EmbeddingTower):\n                            dependency = child_path\n                        elif isinstance(child_module, EmbeddingTowerCollection):\n                            tower_index = _get_tower_index(name, child_module)\n                            dependency = child_path + \".tower_\" + str(tower_index)\n                        sharding_options.append(\n                            ShardingOption(\n                                name=name,\n                                tensor=param,\n                                module=(child_path, child_module),\n                                input_lengths=input_lengths,\n                                batch_size=self._batch_size,\n                                compute_kernel=compute_kernel,\n                                sharding_type=sharding_type,\n                                partition_by=get_partition_by_type(sharding_type),\n                                shards=[\n                                    Shard(size=size, offset=offset)\n                                    for size, offset in zip(shard_sizes, shard_offsets)\n                                ],\n                                cache_params=cache_params,\n                                enforce_hbm=enforce_hbm,\n                                stochastic_rounding=stochastic_rounding,\n                                bounds_check_mode=bounds_check_mode,\n                                dependency=dependency,\n                            )\n                        )\n                if not sharding_options:\n                    raise RuntimeError(\n                        \"No available sharding type and compute kernel combination \"\n                        f\"after applying user provided constraints for {name}\"\n                    )\n\n        self.populate_estimates(sharding_options)\n\n        return sharding_options\n\n    def populate_estimates(self, sharding_options: List[ShardingOption]) -> None:\n        for estimator in self._estimators:\n            estimator.estimate(sharding_options, self._sharder_map)\n\n    def _filter_sharding_types(self, name: str, sharding_types: List[str]) -> List[str]:\n        if not self._constraints or not self._constraints.get(name):\n            return sharding_types\n        constraints: ParameterConstraints = self._constraints[name]\n        if not constraints.sharding_types:\n            return sharding_types\n        constrained_sharding_types: List[str] = constraints.sharding_types\n\n        sharding_types = list(set(constrained_sharding_types) & set(sharding_types))\n\n        if not sharding_types:\n            logger.warn(\n                f\"No available sharding types after applying user provided constraints for {name}\"\n            )\n        return sharding_types\n\n    def _filter_compute_kernels(\n        self,\n        name: str,\n        compute_kernels: List[str],\n    ) -> List[str]:\n\n        if not self._constraints or not self._constraints.get(name):\n            filtered_compute_kernels = compute_kernels\n        else:\n            constraints: ParameterConstraints = self._constraints[name]\n            if not constraints.compute_kernels:\n                filtered_compute_kernels = compute_kernels\n            else:\n                constrained_compute_kernels: List[str] = constraints.compute_kernels\n                filtered_compute_kernels = list(\n                    set(constrained_compute_kernels) & set(compute_kernels)\n                )\n\n        if EmbeddingComputeKernel.DENSE.value in filtered_compute_kernels:\n            if (\n                EmbeddingComputeKernel.FUSED.value in filtered_compute_kernels\n            ):  # always false for data_parallel\n                filtered_compute_kernels.remove(EmbeddingComputeKernel.DENSE.value)\n\n        if not filtered_compute_kernels:\n            logger.warn(\n                f\"No available compute kernels after applying user provided constraints for {name}\"\n            )\n        return filtered_compute_kernels",
  "def _extract_constraints_for_param(\n    constraints: Optional[Dict[str, ParameterConstraints]], name: str\n) -> Tuple[\n    List[float],\n    Optional[int],\n    Optional[CacheParams],\n    Optional[bool],\n    Optional[bool],\n    Optional[BoundsCheckMode],\n]:\n    input_lengths = [POOLING_FACTOR]\n    col_wise_shard_dim = None\n    cache_params = None\n    enforce_hbm = None\n    stochastic_rounding = None\n    bounds_check_mode = None\n\n    if constraints and constraints.get(name):\n        input_lengths = constraints[name].pooling_factors\n        col_wise_shard_dim = constraints[name].min_partition\n        cache_params = constraints[name].cache_params\n        enforce_hbm = constraints[name].enforce_hbm\n        stochastic_rounding = constraints[name].stochastic_rounding\n        bounds_check_mode = constraints[name].bounds_check_mode\n\n    return (\n        input_lengths,\n        col_wise_shard_dim,\n        cache_params,\n        enforce_hbm,\n        stochastic_rounding,\n        bounds_check_mode,\n    )",
  "def get_partition_by_type(sharding_type: str) -> str:\n    \"\"\"\n    Gets corresponding partition by type for provided sharding type.\n\n    Args:\n        sharding_type (str): sharding type string.\n\n    Returns:\n        str: the corresponding `PartitionByType` value.\n    \"\"\"\n\n    device_sharding_types = {\n        ShardingType.TABLE_WISE.value,\n        ShardingType.COLUMN_WISE.value,\n    }\n    host_sharding_types = {\n        ShardingType.TABLE_ROW_WISE.value,\n        ShardingType.TABLE_COLUMN_WISE.value,\n    }\n    uniform_sharding_types = {\n        ShardingType.ROW_WISE.value,\n        ShardingType.DATA_PARALLEL.value,\n    }\n\n    if sharding_type in device_sharding_types:\n        return PartitionByType.DEVICE.value\n    elif sharding_type in host_sharding_types:\n        return PartitionByType.HOST.value\n    elif sharding_type in uniform_sharding_types:\n        return PartitionByType.UNIFORM.value\n\n    raise ValueError(\n        f\"Unrecognized or unsupported sharding type provided: {sharding_type}\"\n    )",
  "def _get_tower_index(name: str, child_module: EmbeddingTowerCollection) -> int:\n    for i, tower in enumerate(child_module.towers):\n        for n, m in tower.named_modules():\n            if isinstance(m, nn.Embedding) or isinstance(m, nn.EmbeddingBag):\n                table_name = n.split(\".\")[-1]\n                if name == table_name:\n                    return i\n    raise RuntimeError(\n        f\"couldn't get the tower index for table {name}, tower collection: {child_module}\"\n    )",
  "def __init__(\n        self,\n        topology: Topology,\n        batch_size: int,\n        constraints: Optional[Dict[str, ParameterConstraints]] = None,\n        estimator: Optional[Union[ShardEstimator, List[ShardEstimator]]] = None,\n    ) -> None:\n        self._compute_device: str = topology.compute_device\n        self._world_size: int = topology.world_size\n        self._local_world_size: int = topology.local_world_size\n        self._batch_size: int = batch_size\n        self._constraints = constraints\n        self._sharder_map: Dict[str, ModuleSharder[nn.Module]] = {}\n\n        if estimator:\n            self._estimators: List[ShardEstimator] = (\n                [estimator] if not isinstance(estimator, list) else estimator\n            )\n        else:\n            self._estimators: List[ShardEstimator] = [\n                EmbeddingPerfEstimator(topology=topology, constraints=constraints),\n                EmbeddingStorageEstimator(topology=topology, constraints=constraints),\n            ]",
  "def enumerate(\n        self,\n        module: nn.Module,\n        sharders: List[ModuleSharder[nn.Module]],\n    ) -> List[ShardingOption]:\n        \"\"\"\n        Generates relevant sharding options given module and sharders.\n\n        Args:\n            module (nn.Module): module to be sharded.\n            sharders (List[ModuleSharder[nn.Module]]): provided sharders for module.\n\n        Returns:\n            List[ShardingOption]: valid sharding options with values populated.\n        \"\"\"\n\n        self._sharder_map = {\n            sharder_name(sharder.module_type): sharder for sharder in sharders\n        }\n        sharding_options: List[ShardingOption] = []\n\n        named_modules_queue = [(\"\", module)]\n        while named_modules_queue:\n            child_path, child_module = named_modules_queue.pop()\n            sharder_key = sharder_name(type(child_module))\n            sharder = self._sharder_map.get(sharder_key, None)\n            if not sharder:\n                for n, m in child_module.named_children():\n                    if child_path != \"\":\n                        named_modules_queue.append((child_path + \".\" + n, m))\n                    else:\n                        named_modules_queue.append((n, m))\n                continue\n\n            for name, param in sharder.shardable_parameters(child_module).items():\n                (\n                    input_lengths,\n                    col_wise_shard_dim,\n                    cache_params,\n                    enforce_hbm,\n                    stochastic_rounding,\n                    bounds_check_mode,\n                ) = _extract_constraints_for_param(self._constraints, name)\n\n                for sharding_type in self._filter_sharding_types(\n                    name, sharder.sharding_types(self._compute_device)\n                ):\n                    for compute_kernel in self._filter_compute_kernels(\n                        name,\n                        sharder.compute_kernels(sharding_type, self._compute_device),\n                    ):\n                        (\n                            shard_sizes,\n                            shard_offsets,\n                        ) = calculate_shard_sizes_and_offsets(\n                            tensor=param,\n                            world_size=self._world_size,\n                            local_world_size=self._local_world_size,\n                            sharding_type=sharding_type,\n                            col_wise_shard_dim=col_wise_shard_dim,\n                        )\n                        dependency = None\n                        if isinstance(child_module, EmbeddingTower):\n                            dependency = child_path\n                        elif isinstance(child_module, EmbeddingTowerCollection):\n                            tower_index = _get_tower_index(name, child_module)\n                            dependency = child_path + \".tower_\" + str(tower_index)\n                        sharding_options.append(\n                            ShardingOption(\n                                name=name,\n                                tensor=param,\n                                module=(child_path, child_module),\n                                input_lengths=input_lengths,\n                                batch_size=self._batch_size,\n                                compute_kernel=compute_kernel,\n                                sharding_type=sharding_type,\n                                partition_by=get_partition_by_type(sharding_type),\n                                shards=[\n                                    Shard(size=size, offset=offset)\n                                    for size, offset in zip(shard_sizes, shard_offsets)\n                                ],\n                                cache_params=cache_params,\n                                enforce_hbm=enforce_hbm,\n                                stochastic_rounding=stochastic_rounding,\n                                bounds_check_mode=bounds_check_mode,\n                                dependency=dependency,\n                            )\n                        )\n                if not sharding_options:\n                    raise RuntimeError(\n                        \"No available sharding type and compute kernel combination \"\n                        f\"after applying user provided constraints for {name}\"\n                    )\n\n        self.populate_estimates(sharding_options)\n\n        return sharding_options",
  "def populate_estimates(self, sharding_options: List[ShardingOption]) -> None:\n        for estimator in self._estimators:\n            estimator.estimate(sharding_options, self._sharder_map)",
  "def _filter_sharding_types(self, name: str, sharding_types: List[str]) -> List[str]:\n        if not self._constraints or not self._constraints.get(name):\n            return sharding_types\n        constraints: ParameterConstraints = self._constraints[name]\n        if not constraints.sharding_types:\n            return sharding_types\n        constrained_sharding_types: List[str] = constraints.sharding_types\n\n        sharding_types = list(set(constrained_sharding_types) & set(sharding_types))\n\n        if not sharding_types:\n            logger.warn(\n                f\"No available sharding types after applying user provided constraints for {name}\"\n            )\n        return sharding_types",
  "def _filter_compute_kernels(\n        self,\n        name: str,\n        compute_kernels: List[str],\n    ) -> List[str]:\n\n        if not self._constraints or not self._constraints.get(name):\n            filtered_compute_kernels = compute_kernels\n        else:\n            constraints: ParameterConstraints = self._constraints[name]\n            if not constraints.compute_kernels:\n                filtered_compute_kernels = compute_kernels\n            else:\n                constrained_compute_kernels: List[str] = constraints.compute_kernels\n                filtered_compute_kernels = list(\n                    set(constrained_compute_kernels) & set(compute_kernels)\n                )\n\n        if EmbeddingComputeKernel.DENSE.value in filtered_compute_kernels:\n            if (\n                EmbeddingComputeKernel.FUSED.value in filtered_compute_kernels\n            ):  # always false for data_parallel\n                filtered_compute_kernels.remove(EmbeddingComputeKernel.DENSE.value)\n\n        if not filtered_compute_kernels:\n            logger.warn(\n                f\"No available compute kernels after applying user provided constraints for {name}\"\n            )\n        return filtered_compute_kernels",
  "class GreedyProposer(Proposer):\n    \"\"\"\n    Proposes sharding plans in greedy fashion.\n\n    Sorts sharding options for each shardable parameter by perf.\n    On each iteration, finds parameter with largest current storage usage and tries its\n    next sharding option.\n\n    Args:\n        use_depth (bool): When enabled, sharding_options of a fqn are sorted based on\n            `max(shard.perf.total)`, otherwise sharding_options are sorted by\n            `sum(shard.perf.total)`.\n        threshold (Optional[int]): Threshold for early stopping. When specified, the\n            proposer stops proposing when the proposals have consecutive worse perf_rating\n            than best_perf_rating.\n    \"\"\"\n\n    def __init__(self, use_depth: bool = True, threshold: Optional[int] = None) -> None:\n        self._use_depth: bool = use_depth\n        self._threshold: Optional[int] = threshold if threshold else None\n        self._sharding_options_by_fqn: Dict[str, List[ShardingOption]] = {}\n        self._current_proposal: Dict[str, int] = {}\n        self._best_perf_rating: float = float(\"inf\")\n        self._num_inferior_perf: int = 0\n\n    def load(self, search_space: List[ShardingOption]) -> None:\n        self._reset()\n        for sharding_option in search_space:\n            fqn = sharding_option.fqn\n            if fqn not in self._sharding_options_by_fqn:\n                self._sharding_options_by_fqn[fqn] = []\n            self._sharding_options_by_fqn[fqn].append(sharding_option)\n\n        for sharding_options in self._sharding_options_by_fqn.values():\n            sharding_options.sort(\n                key=lambda x: _sharding_option_score(x, self._use_depth)\n            )\n\n        self._current_proposal = {\n            fqn: 0 for fqn in self._sharding_options_by_fqn.keys()\n        }\n\n    def _reset(self) -> None:\n        self._sharding_options_by_fqn = {}\n        self._current_proposal = {}\n\n    def propose(self) -> Optional[List[ShardingOption]]:\n        if self._current_proposal:\n            return [\n                self._sharding_options_by_fqn[fqn][index]\n                for fqn, index in self._current_proposal.items()\n            ]\n        else:\n            return None\n\n    def feedback(\n        self,\n        partitionable: bool,\n        plan: Optional[List[ShardingOption]] = None,\n        perf_rating: Optional[float] = None,\n    ) -> None:\n        # When threshold is passed, observe the perf_rating trend. If the perf_rating\n        # of the newly proposed plans have worse perf_rating, stop proposing.\n        if self._threshold and perf_rating:\n            self._num_inferior_perf += 1\n            if perf_rating < self._best_perf_rating:\n                self._best_perf_rating = perf_rating\n                self._num_inferior_perf = 0\n            # pyre-fixme [58]: `>` is not supported for operand types `int` and `Optional[int]`.\n            if self._num_inferior_perf > self._threshold:\n                self._current_proposal = {}\n                return\n        # static strategy, ignore feedback and just provide next proposal\n        largest_fqn: Optional[str] = None\n        largest_storage: Tuple[float, float, float, float] = (0, 0, 0, 0)\n        for fqn, sharding_options in self._sharding_options_by_fqn.items():\n            index = self._current_proposal[fqn]\n            if index + 1 < len(sharding_options):\n                sharding_option = sharding_options[index]\n                current_storage = (\n                    # pyre-fixme [16]: `Optional` has no attribute `hbm`\n                    max([shard.storage.hbm for shard in sharding_option.shards]),\n                    sum([shard.storage.hbm for shard in sharding_option.shards]),\n                    # pyre-fixme [16]: `Optional` has no attribute `ddr`\n                    max([shard.storage.ddr for shard in sharding_option.shards]),\n                    sum([shard.storage.ddr for shard in sharding_option.shards]),\n                )\n                if current_storage > largest_storage:\n                    largest_fqn = fqn\n                    largest_storage = current_storage\n\n        if largest_fqn is not None:\n            self._current_proposal[largest_fqn] += 1\n        else:\n            self._current_proposal = {}",
  "class UniformProposer(Proposer):\n    \"\"\"\n    Proposes uniform sharding plans, plans that have the same sharding type for all\n    sharding options.\n    \"\"\"\n\n    def __init__(self, use_depth: bool = True) -> None:\n        self._use_depth: bool = use_depth\n        self._grouped_sharding_options: List[List[ShardingOption]] = []\n        self._proposal_index: int = 0\n\n    def load(self, search_space: List[ShardingOption]) -> None:\n        self._reset()\n        all_fqns = set()\n        sharding_options_by_type_and_fqn: Dict[\n            str, Dict[str, List[ShardingOption]]\n        ] = {}\n\n        for sharding_option in search_space:\n            sharding_type = sharding_option.sharding_type\n            fqn = sharding_option.fqn\n            all_fqns.add(fqn)\n\n            if sharding_type not in sharding_options_by_type_and_fqn:\n                sharding_options_by_type_and_fqn[sharding_type] = {}\n            if fqn not in sharding_options_by_type_and_fqn[sharding_type]:\n                sharding_options_by_type_and_fqn[sharding_type][fqn] = []\n\n            sharding_options_by_type_and_fqn[sharding_type][fqn].append(sharding_option)\n\n        for sharding_options_by_fqn in sharding_options_by_type_and_fqn.values():\n            for sharding_options in sharding_options_by_fqn.values():\n                sharding_options.sort(\n                    key=lambda x: _sharding_option_score(x, self._use_depth)\n                )\n\n        for sharding_options_by_fqn in sharding_options_by_type_and_fqn.values():\n            if sharding_options_by_fqn.keys() == all_fqns:\n                self._grouped_sharding_options.append(\n                    [\n                        sorted_sharding_options[0]\n                        for sorted_sharding_options in sharding_options_by_fqn.values()\n                    ]\n                )\n\n    def _reset(self) -> None:\n        self._grouped_sharding_options = []\n        self._proposal_index = 0\n\n    def propose(self) -> Optional[List[ShardingOption]]:\n        if self._proposal_index < len(self._grouped_sharding_options):\n            return self._grouped_sharding_options[self._proposal_index]\n        else:\n            return None\n\n    def feedback(\n        self,\n        partitionable: bool,\n        plan: Optional[List[ShardingOption]] = None,\n        perf_rating: Optional[float] = None,\n    ) -> None:\n        # static strategy, ignore feedback and just provide next proposal\n        self._proposal_index += 1",
  "class GridSearchProposer(Proposer):\n    def __init__(self, max_proposals: int = MAX_PROPOSALS) -> None:\n        self._max_proposals: int = max_proposals\n        self._sharding_options_by_fqn: Dict[str, List[ShardingOption]] = {}\n        self._proposal_index: int = 0\n        self._proposals: List[List[int]] = []\n\n    def load(self, search_space: List[ShardingOption]) -> None:\n        self._reset()\n        for sharding_option in search_space:\n            fqn = sharding_option.fqn\n            if fqn not in self._sharding_options_by_fqn:\n                self._sharding_options_by_fqn[fqn] = []\n            self._sharding_options_by_fqn[fqn].append(sharding_option)\n\n        for sharding_options in self._sharding_options_by_fqn.values():\n            sharding_options.sort(key=lambda x: _sharding_option_score(x))\n\n        total_proposals = prod(\n            [\n                len(sharding_options)\n                for sharding_options in self._sharding_options_by_fqn.values()\n            ]\n        )\n        if total_proposals > self._max_proposals:\n            total_proposals = (\n                \"{:.2e}\".format(Decimal(total_proposals))\n                if total_proposals > 1e6\n                else total_proposals\n            )\n            logger.info(\n                \"Skipping grid search proposer as there are too many proposals.\\n\"\n                f\"Total proposals to search: {total_proposals}\\n\"\n                f\"Max proposals allowed: {self._max_proposals}\\n\"\n            )\n            return\n        sharding_options_by_fqn_indices = [\n            range(len(sharding_options))\n            for sharding_options in self._sharding_options_by_fqn.values()\n        ]\n        # pyre-fixme[8]: Attribute has type `List[List[int]]`; used as\n        #  `List[Tuple[int]]`.\n        self._proposals = list(itertools.product(*sharding_options_by_fqn_indices))\n\n    def _reset(self) -> None:\n        self._sharding_options_by_fqn = {}\n        self._proposal_index = 0\n        self._proposals = []\n\n    def propose(self) -> Optional[List[ShardingOption]]:\n        if self._proposals and self._proposal_index < len(self._proposals):\n            proposal_indices = self._proposals[self._proposal_index]\n            return [\n                sharding_options[index]\n                for index, sharding_options in zip(\n                    proposal_indices, self._sharding_options_by_fqn.values()\n                )\n            ]\n        else:\n            return None\n\n    def feedback(\n        self,\n        partitionable: bool,\n        plan: Optional[List[ShardingOption]] = None,\n        perf_rating: Optional[float] = None,\n    ) -> None:\n        # static strategy, ignore feedback and just provide next proposal\n        self._proposal_index += 1",
  "def _sharding_option_score(\n    sharding_option: ShardingOption, use_depth: bool = True\n) -> float:\n    return (\n        max([cast(Perf, shard.perf).total for shard in sharding_option.shards])\n        if use_depth\n        else sum([cast(Perf, shard.perf).total for shard in sharding_option.shards])\n    )",
  "def proposers_to_proposals_list(\n    proposers_list: List[Proposer], search_space: List[ShardingOption]\n) -> List[List[ShardingOption]]:\n    \"\"\"\n    only works for static_feedback proposers (the path of proposals to check is independent of the performance of the proposals)\n    \"\"\"\n\n    proposals_list = []\n\n    proposal_cache: Set[Tuple[int, ...]] = set()\n\n    for proposer in proposers_list:\n        proposer.load(search_space=search_space)\n\n    for proposer in proposers_list:\n        proposal = proposer.propose()\n\n        while proposal:\n            proposal_key = tuple(sorted(map(hash, proposal)))\n            proposer.feedback(partitionable=True)\n            if proposal_key in proposal_cache:\n                proposal = proposer.propose()\n                continue\n\n            proposals_list.append(proposal)\n            proposal_cache.add(proposal_key)\n            proposal = proposer.propose()\n\n    return proposals_list",
  "def __init__(self, use_depth: bool = True, threshold: Optional[int] = None) -> None:\n        self._use_depth: bool = use_depth\n        self._threshold: Optional[int] = threshold if threshold else None\n        self._sharding_options_by_fqn: Dict[str, List[ShardingOption]] = {}\n        self._current_proposal: Dict[str, int] = {}\n        self._best_perf_rating: float = float(\"inf\")\n        self._num_inferior_perf: int = 0",
  "def load(self, search_space: List[ShardingOption]) -> None:\n        self._reset()\n        for sharding_option in search_space:\n            fqn = sharding_option.fqn\n            if fqn not in self._sharding_options_by_fqn:\n                self._sharding_options_by_fqn[fqn] = []\n            self._sharding_options_by_fqn[fqn].append(sharding_option)\n\n        for sharding_options in self._sharding_options_by_fqn.values():\n            sharding_options.sort(\n                key=lambda x: _sharding_option_score(x, self._use_depth)\n            )\n\n        self._current_proposal = {\n            fqn: 0 for fqn in self._sharding_options_by_fqn.keys()\n        }",
  "def _reset(self) -> None:\n        self._sharding_options_by_fqn = {}\n        self._current_proposal = {}",
  "def propose(self) -> Optional[List[ShardingOption]]:\n        if self._current_proposal:\n            return [\n                self._sharding_options_by_fqn[fqn][index]\n                for fqn, index in self._current_proposal.items()\n            ]\n        else:\n            return None",
  "def feedback(\n        self,\n        partitionable: bool,\n        plan: Optional[List[ShardingOption]] = None,\n        perf_rating: Optional[float] = None,\n    ) -> None:\n        # When threshold is passed, observe the perf_rating trend. If the perf_rating\n        # of the newly proposed plans have worse perf_rating, stop proposing.\n        if self._threshold and perf_rating:\n            self._num_inferior_perf += 1\n            if perf_rating < self._best_perf_rating:\n                self._best_perf_rating = perf_rating\n                self._num_inferior_perf = 0\n            # pyre-fixme [58]: `>` is not supported for operand types `int` and `Optional[int]`.\n            if self._num_inferior_perf > self._threshold:\n                self._current_proposal = {}\n                return\n        # static strategy, ignore feedback and just provide next proposal\n        largest_fqn: Optional[str] = None\n        largest_storage: Tuple[float, float, float, float] = (0, 0, 0, 0)\n        for fqn, sharding_options in self._sharding_options_by_fqn.items():\n            index = self._current_proposal[fqn]\n            if index + 1 < len(sharding_options):\n                sharding_option = sharding_options[index]\n                current_storage = (\n                    # pyre-fixme [16]: `Optional` has no attribute `hbm`\n                    max([shard.storage.hbm for shard in sharding_option.shards]),\n                    sum([shard.storage.hbm for shard in sharding_option.shards]),\n                    # pyre-fixme [16]: `Optional` has no attribute `ddr`\n                    max([shard.storage.ddr for shard in sharding_option.shards]),\n                    sum([shard.storage.ddr for shard in sharding_option.shards]),\n                )\n                if current_storage > largest_storage:\n                    largest_fqn = fqn\n                    largest_storage = current_storage\n\n        if largest_fqn is not None:\n            self._current_proposal[largest_fqn] += 1\n        else:\n            self._current_proposal = {}",
  "def __init__(self, use_depth: bool = True) -> None:\n        self._use_depth: bool = use_depth\n        self._grouped_sharding_options: List[List[ShardingOption]] = []\n        self._proposal_index: int = 0",
  "def load(self, search_space: List[ShardingOption]) -> None:\n        self._reset()\n        all_fqns = set()\n        sharding_options_by_type_and_fqn: Dict[\n            str, Dict[str, List[ShardingOption]]\n        ] = {}\n\n        for sharding_option in search_space:\n            sharding_type = sharding_option.sharding_type\n            fqn = sharding_option.fqn\n            all_fqns.add(fqn)\n\n            if sharding_type not in sharding_options_by_type_and_fqn:\n                sharding_options_by_type_and_fqn[sharding_type] = {}\n            if fqn not in sharding_options_by_type_and_fqn[sharding_type]:\n                sharding_options_by_type_and_fqn[sharding_type][fqn] = []\n\n            sharding_options_by_type_and_fqn[sharding_type][fqn].append(sharding_option)\n\n        for sharding_options_by_fqn in sharding_options_by_type_and_fqn.values():\n            for sharding_options in sharding_options_by_fqn.values():\n                sharding_options.sort(\n                    key=lambda x: _sharding_option_score(x, self._use_depth)\n                )\n\n        for sharding_options_by_fqn in sharding_options_by_type_and_fqn.values():\n            if sharding_options_by_fqn.keys() == all_fqns:\n                self._grouped_sharding_options.append(\n                    [\n                        sorted_sharding_options[0]\n                        for sorted_sharding_options in sharding_options_by_fqn.values()\n                    ]\n                )",
  "def _reset(self) -> None:\n        self._grouped_sharding_options = []\n        self._proposal_index = 0",
  "def propose(self) -> Optional[List[ShardingOption]]:\n        if self._proposal_index < len(self._grouped_sharding_options):\n            return self._grouped_sharding_options[self._proposal_index]\n        else:\n            return None",
  "def feedback(\n        self,\n        partitionable: bool,\n        plan: Optional[List[ShardingOption]] = None,\n        perf_rating: Optional[float] = None,\n    ) -> None:\n        # static strategy, ignore feedback and just provide next proposal\n        self._proposal_index += 1",
  "def __init__(self, max_proposals: int = MAX_PROPOSALS) -> None:\n        self._max_proposals: int = max_proposals\n        self._sharding_options_by_fqn: Dict[str, List[ShardingOption]] = {}\n        self._proposal_index: int = 0\n        self._proposals: List[List[int]] = []",
  "def load(self, search_space: List[ShardingOption]) -> None:\n        self._reset()\n        for sharding_option in search_space:\n            fqn = sharding_option.fqn\n            if fqn not in self._sharding_options_by_fqn:\n                self._sharding_options_by_fqn[fqn] = []\n            self._sharding_options_by_fqn[fqn].append(sharding_option)\n\n        for sharding_options in self._sharding_options_by_fqn.values():\n            sharding_options.sort(key=lambda x: _sharding_option_score(x))\n\n        total_proposals = prod(\n            [\n                len(sharding_options)\n                for sharding_options in self._sharding_options_by_fqn.values()\n            ]\n        )\n        if total_proposals > self._max_proposals:\n            total_proposals = (\n                \"{:.2e}\".format(Decimal(total_proposals))\n                if total_proposals > 1e6\n                else total_proposals\n            )\n            logger.info(\n                \"Skipping grid search proposer as there are too many proposals.\\n\"\n                f\"Total proposals to search: {total_proposals}\\n\"\n                f\"Max proposals allowed: {self._max_proposals}\\n\"\n            )\n            return\n        sharding_options_by_fqn_indices = [\n            range(len(sharding_options))\n            for sharding_options in self._sharding_options_by_fqn.values()\n        ]\n        # pyre-fixme[8]: Attribute has type `List[List[int]]`; used as\n        #  `List[Tuple[int]]`.\n        self._proposals = list(itertools.product(*sharding_options_by_fqn_indices))",
  "def _reset(self) -> None:\n        self._sharding_options_by_fqn = {}\n        self._proposal_index = 0\n        self._proposals = []",
  "def propose(self) -> Optional[List[ShardingOption]]:\n        if self._proposals and self._proposal_index < len(self._proposals):\n            proposal_indices = self._proposals[self._proposal_index]\n            return [\n                sharding_options[index]\n                for index, sharding_options in zip(\n                    proposal_indices, self._sharding_options_by_fqn.values()\n                )\n            ]\n        else:\n            return None",
  "def feedback(\n        self,\n        partitionable: bool,\n        plan: Optional[List[ShardingOption]] = None,\n        perf_rating: Optional[float] = None,\n    ) -> None:\n        # static strategy, ignore feedback and just provide next proposal\n        self._proposal_index += 1",
  "def _sort_devices_by_perf(\n    devices: List[List[DeviceHardware]],\n) -> List[List[DeviceHardware]]:\n    def _get_perf_sum(device_list: List[DeviceHardware]) -> float:\n        perf = 0\n        for device in device_list:\n            perf += device.perf.total\n        return perf\n\n    return sorted(devices, key=_get_perf_sum)",
  "def _get_uniform_sharding_options(\n    sharding_options: List[ShardingOption],\n) -> List[ShardingOption]:\n    uniform_sharding_options: List[ShardingOption] = []\n    for sharding_option in sharding_options:\n        if sharding_option.partition_by == PartitionByType.UNIFORM.value:\n            uniform_sharding_options.append(sharding_option)\n    return uniform_sharding_options",
  "class ShardingOptionGroup:\n    sharding_options: List[ShardingOption]\n    storage_sum: Storage",
  "def _group_and_sort_non_uniform_sharding_options(\n    sharding_options: List[ShardingOption],\n) -> List[ShardingOptionGroup]:\n    sharding_option_groups_by_dependency = {}\n    for sharding_option in sharding_options:\n        if sharding_option.partition_by == PartitionByType.UNIFORM.value:\n            continue\n\n        group_key = sharding_option.dependency or sharding_option.fqn\n        if group_key not in sharding_option_groups_by_dependency:\n            sharding_option_groups_by_dependency[group_key] = ShardingOptionGroup(\n                [sharding_option], sharding_option.total_storage\n            )\n        else:\n            sharding_option_groups_by_dependency[group_key].sharding_options.append(\n                sharding_option\n            )\n            sharding_option_groups_by_dependency[\n                group_key\n            ].storage_sum += sharding_option.total_storage\n    sharding_option_groups = list(sharding_option_groups_by_dependency.values())\n\n    sharding_option_groups.sort(key=lambda group: group.storage_sum, reverse=True)\n    return sharding_option_groups",
  "class GreedyPerfPartitioner(Partitioner):\n    \"\"\"\n    Greedy Partitioner\n    \"\"\"\n\n    def partition(\n        self,\n        proposal: List[ShardingOption],\n        storage_constraint: Topology,\n    ) -> List[ShardingOption]:\n        \"\"\"\n        Places sharding options on topology based on each sharding option's\n        `partition_by` attribute.\n        The topology, storage, and perfs are updated at the end of the placement.\n\n        Args:\n            proposal (List[ShardingOption]): list of populated sharding options.\n            storage_constraint (Topology): device topology.\n\n        Returns:\n            List[ShardingOption]: list of sharding options for selected plan.\n\n        Example::\n\n            sharding_options = [\n                    ShardingOption(partition_by=\"uniform\",\n                            shards=[\n                                Shards(storage=1, perf=1),\n                                Shards(storage=1, perf=1),\n                            ]),\n                    ShardingOption(partition_by=\"uniform\",\n                            shards=[\n                                Shards(storage=2, perf=2),\n                                Shards(storage=2, perf=2),\n                            ]),\n                    ShardingOption(partition_by=\"device\",\n                            shards=[\n                                Shards(storage=3, perf=3),\n                                Shards(storage=3, perf=3),\n                            ])\n                    ShardingOption(partition_by=\"device\",\n                            shards=[\n                                Shards(storage=4, perf=4),\n                                Shards(storage=4, perf=4),\n                            ]),\n                ]\n            topology = Topology(world_size=2)\n\n            # First [sharding_options[0] and sharding_options[1]] will be placed on the\n            # topology with the uniform strategy, resulting in\n\n            topology.devices[0].perf.total = (1,2)\n            topology.devices[1].perf.total = (1,2)\n\n            # Finally sharding_options[2] and sharding_options[3]] will be placed on the\n            # topology with the device strategy (see docstring of `partition_by_device` for\n            # more details).\n\n            topology.devices[0].perf.total = (1,2) + (3,4)\n            topology.devices[1].perf.total = (1,2) + (3,4)\n\n            # The topology updates are done after the end of all the placements (the other\n            # in the example is just for clarity).\n        \"\"\"\n\n        _topology: Topology = copy.deepcopy(storage_constraint)\n        # shallow copy to keep an almost sorted list around\n        # we try to not modify the order of devices in the topology\n        # since _get_host_level_devices relies on the order\n        sorted_devices = _topology.devices.copy()\n        _host_level_devices = GreedyPerfPartitioner._get_host_level_devices(_topology)\n\n        # first partition the uniform sharding options (RW & DP)\n        uniform_sharding_options = _get_uniform_sharding_options(proposal)\n        GreedyPerfPartitioner._uniform_partition(\n            uniform_sharding_options, _topology.devices\n        )\n\n        # group the rest sharding options by colocation type (co-host, co-device, none)\n        # and sort the groups by storage in reverse order\n        sharding_option_groups = _group_and_sort_non_uniform_sharding_options(proposal)\n\n        for sharding_option_group in sharding_option_groups:\n            if (\n                sharding_option_group.sharding_options[0].partition_by\n                == PartitionByType.HOST.value\n            ):\n                GreedyPerfPartitioner._cohost_partition(\n                    sharding_option_group, _host_level_devices\n                )\n            elif (\n                sharding_option_group.sharding_options[0].partition_by\n                == PartitionByType.DEVICE.value\n            ):\n                assert (\n                    len(sharding_option_group.sharding_options) == 1\n                ), f\"Unexpected length for sharding options: {len(sharding_option_group.sharding_options)}\"\n                GreedyPerfPartitioner._device_partition(\n                    sharding_option_group.sharding_options[0],\n                    sorted_devices,\n                    _topology.local_world_size,\n                )\n            else:\n                raise RuntimeError(\n                    f\"Unexpected sharding option group {sharding_option_group}\"\n                )\n        # pyre-ignore [16]: `GreedyPerfPartitioner` has no attribute `_topology`.\n        self._topology: Topology = _topology\n        return proposal\n\n    @staticmethod\n    def _device_partition(\n        sharding_option: ShardingOption,\n        devices: List[DeviceHardware],\n        local_world_size: int = 1,\n    ) -> None:\n        for shard in sharding_option.shards:\n            devices.sort(\n                # We use the \"local_rank\" as the secondary key for sorting. This\n                # is to even out the pressure on different hosts. For example, in UVM\n                # case, we will allocate UVM table with the global rank order, and host0\n                # will use a lot more CPU memory than the others. With local rank as the\n                # secondary key, we could even out CPU memory pressure on different host\n                key=lambda device: (device.perf.total, device.rank % local_world_size),\n            )\n            success = False\n            for device in devices:\n                if cast(Storage, shard.storage).fits_in(device.storage):\n                    shard.rank = device.rank\n                    device.storage -= cast(Storage, shard.storage)\n                    device.perf += cast(Perf, shard.perf)\n                    success = True\n                    break\n            if not success:\n                raise PlannerError(\n                    error_type=PlannerErrorType.PARTITION,\n                    message=(\n                        f\"Device partition failed. Couldn't find a rank for shard {shard} of table {sharding_option.name}, \"\n                        f\"largest device storage: {max(devices, key=lambda device: device.storage).storage}\"\n                    ),\n                )\n\n    @staticmethod\n    def _cohost_partition(\n        sharding_option_group: ShardingOptionGroup,\n        _host_level_devices: List[List[DeviceHardware]],\n    ) -> None:\n        sorted_host_level_devices = _sort_devices_by_perf(_host_level_devices)\n        for devices in sorted_host_level_devices:\n            host_devices = copy.deepcopy(devices)\n            host_storage = Storage(hbm=0, ddr=0)\n            for device in host_devices:\n                host_storage += device.storage\n            if not sharding_option_group.storage_sum.fits_in(host_storage):\n                continue\n\n            success = True\n            for sharding_option in sharding_option_group.sharding_options:\n                try:\n                    if (\n                        sharding_option.sharding_type\n                        == ShardingType.TABLE_ROW_WISE.value\n                    ):\n                        GreedyPerfPartitioner._uniform_partition(\n                            [sharding_option], host_devices\n                        )\n                    elif (\n                        sharding_option.sharding_type\n                        == ShardingType.TABLE_COLUMN_WISE.value\n                    ):\n                        GreedyPerfPartitioner._device_partition(\n                            sharding_option, host_devices, len(host_devices)\n                        )\n                    else:\n                        raise RuntimeError(\n                            f\"unexpected cohost sharding type: {sharding_option.sharding_type}\"\n                        )\n                except PlannerError:\n                    success = False\n                    break\n            if success:\n                # successfully found a host and partitioned on that host\n                # need to update the devices\n                # resorting host_devices before copying data back\n                host_devices.sort(key=lambda device: device.rank)\n                for device, device_copy in zip(devices, host_devices):\n                    device.storage = device_copy.storage\n                    device.perf = device_copy.perf\n                return\n        raise PlannerError(\n            error_type=PlannerErrorType.PARTITION,\n            message=f\"can't find a host for sharding option group {sharding_option_group}\",\n        )\n\n    @staticmethod\n    def _get_host_level_devices(_topology: Topology) -> List[List[DeviceHardware]]:\n        num_hosts: int = _topology.world_size // _topology.local_world_size\n        host_level_devices: List[List[DeviceHardware]] = []\n        for i in range(num_hosts):\n            devices_in_host = _topology.devices[\n                i * _topology.local_world_size : (i + 1) * _topology.local_world_size\n            ]\n            host_level_devices.append(devices_in_host)\n        return host_level_devices\n\n    @staticmethod\n    def _uniform_partition(\n        sharding_options: List[ShardingOption], devices: List[DeviceHardware]\n    ) -> None:\n        for sharding_option in sharding_options:\n            if sharding_option.num_shards != len(devices):\n                raise RuntimeError(\n                    f\"For a uniform partition, the number of shards ({sharding_option.num_shards}) must equal the number of devices ({len(devices)})\"\n                )\n            for i in range(len(devices)):\n                storage_needed = cast(Storage, sharding_option.shards[i].storage)\n                if not storage_needed.fits_in(devices[i].storage):\n                    raise PlannerError(\n                        error_type=PlannerErrorType.PARTITION,\n                        message=f\"Shard of size {storage_needed} bytes does not fit on any rank. Device memory cap: {devices[i].storage}.\",\n                    )\n                else:\n                    sharding_option.shards[i].rank = devices[i].rank\n                    devices[i].storage -= storage_needed\n                    devices[i].perf += cast(Perf, sharding_option.shards[i].perf)",
  "class MemoryBalancedPartitioner(Partitioner):\n    \"\"\"\n    Memory balanced Partitioner.\n    \"\"\"\n\n    def __init__(self, max_search_count: int = 10, tolerance: float = 0.02) -> None:\n        self._max_search_count: int = max_search_count\n        self._tolerance: float = tolerance\n\n    def partition(\n        self,\n        proposal: List[ShardingOption],\n        storage_constraint: Topology,\n    ) -> List[ShardingOption]:\n        \"\"\"\n        Repeatedly calls the GreedyPerfPartitioner to find a plan with perf\n        within the tolerance of the original plan that uses the least amount\n        of memory.\n        \"\"\"\n        _perf_model: PerfModel = NoopPerfModel(storage_constraint)\n        _partitioner = GreedyPerfPartitioner()\n        # copying storage_constraint, since we modify it in place\n        _topology: Topology = copy.deepcopy(storage_constraint)\n\n        # set up default plan to fall back on\n        default_plan = _partitioner.partition(proposal, _topology)\n        default_plan = copy.deepcopy(default_plan)\n        original_plan_perf = _perf_model.rate(default_plan)\n\n        max_hbm_per_device: int = _topology.devices[0].storage.hbm\n        logger.info(\n            f\"Default plan uses {round(bytes_to_gb(max_hbm_per_device), 3)} GB per device.\"\n        )\n\n        hbm_requirement: int = 0\n        for sharding_option in proposal:\n            for shard in sharding_option.shards:\n                if shard.storage is not None:\n                    hbm_requirement += shard.storage.hbm\n        min_hbm_per_device: int = int(hbm_requirement / _topology.world_size)\n        logger.info(\n            \"Searching in the range (min_hbm_per_device, max_hbm_per_device): \"\n            f\"({round(bytes_to_gb(min_hbm_per_device), 3)}, \"\n            f\"{round(bytes_to_gb(max_hbm_per_device), 3)})\"\n        )\n\n        # binary search with (min, max] setting\n        search_count = 0\n        while (\n            search_count < self._max_search_count\n            and min_hbm_per_device + 10 * 1024**2 < max_hbm_per_device  # 10MB\n        ):\n            search_count += 1\n            reset_shard_rank(proposal)\n            mid_hbm_per_device: int = (max_hbm_per_device + min_hbm_per_device) // 2\n            set_hbm_per_device(_topology, mid_hbm_per_device)\n            try:\n                new_plan = _partitioner.partition(proposal, _topology)\n                new_plan_perf = _perf_model.rate(new_plan)\n                perf_diff = (\n                    (new_plan_perf - original_plan_perf) / original_plan_perf\n                    if original_plan_perf\n                    else 100\n                )\n                if new_plan_perf > original_plan_perf * (1 + self._tolerance):\n                    # the new plan is worse than the original one\n                    logger.info(\n                        f\"Found a plan with {round(bytes_to_gb(mid_hbm_per_device), 3)} \"\n                        f\"GB per device for embedding tables, \"\n                        f\"but its perf is {round(perf_diff * 100, 3)}% worse than the original plan, \"\n                        f\"which exceeds the {self._tolerance * 100}% tolerance.\"\n                    )\n                    min_hbm_per_device = mid_hbm_per_device\n                else:\n                    # the new plan is better than original one\n                    if perf_diff > 0:\n                        perf_diff_str = (\n                            f\"{round((perf_diff) * 100, 3)}% worse than the original plan, \"\n                            f\"which is within the {self._tolerance * 100}% tolerance.\"\n                        )\n                    else:\n                        perf_diff_str = f\"{round((perf_diff) * 100, 3)}% better than the original plan.\"\n                    logger.info(\n                        f\"Found a more memory-balanced plan with {round(bytes_to_gb(mid_hbm_per_device), 3)} \"\n                        f\"GB per device for embedding tables. The new plan is {perf_diff_str}\"\n                    )\n                    default_plan = copy.deepcopy(new_plan)\n                    max_hbm_per_device = mid_hbm_per_device\n            except PlannerError:\n                logger.info(\n                    f\"Couldn't find a plan with {round(bytes_to_gb(max_hbm_per_device), 3)} \"\n                    f\"GB per device for embedding tables.\"\n                )\n                min_hbm_per_device = mid_hbm_per_device\n\n        return default_plan",
  "def set_hbm_per_device(storage_constraint: Topology, hbm_per_device: int) -> None:\n    for device in storage_constraint.devices:\n        device.storage.hbm = hbm_per_device",
  "def _get_perf_sum(device_list: List[DeviceHardware]) -> float:\n        perf = 0\n        for device in device_list:\n            perf += device.perf.total\n        return perf",
  "def partition(\n        self,\n        proposal: List[ShardingOption],\n        storage_constraint: Topology,\n    ) -> List[ShardingOption]:\n        \"\"\"\n        Places sharding options on topology based on each sharding option's\n        `partition_by` attribute.\n        The topology, storage, and perfs are updated at the end of the placement.\n\n        Args:\n            proposal (List[ShardingOption]): list of populated sharding options.\n            storage_constraint (Topology): device topology.\n\n        Returns:\n            List[ShardingOption]: list of sharding options for selected plan.\n\n        Example::\n\n            sharding_options = [\n                    ShardingOption(partition_by=\"uniform\",\n                            shards=[\n                                Shards(storage=1, perf=1),\n                                Shards(storage=1, perf=1),\n                            ]),\n                    ShardingOption(partition_by=\"uniform\",\n                            shards=[\n                                Shards(storage=2, perf=2),\n                                Shards(storage=2, perf=2),\n                            ]),\n                    ShardingOption(partition_by=\"device\",\n                            shards=[\n                                Shards(storage=3, perf=3),\n                                Shards(storage=3, perf=3),\n                            ])\n                    ShardingOption(partition_by=\"device\",\n                            shards=[\n                                Shards(storage=4, perf=4),\n                                Shards(storage=4, perf=4),\n                            ]),\n                ]\n            topology = Topology(world_size=2)\n\n            # First [sharding_options[0] and sharding_options[1]] will be placed on the\n            # topology with the uniform strategy, resulting in\n\n            topology.devices[0].perf.total = (1,2)\n            topology.devices[1].perf.total = (1,2)\n\n            # Finally sharding_options[2] and sharding_options[3]] will be placed on the\n            # topology with the device strategy (see docstring of `partition_by_device` for\n            # more details).\n\n            topology.devices[0].perf.total = (1,2) + (3,4)\n            topology.devices[1].perf.total = (1,2) + (3,4)\n\n            # The topology updates are done after the end of all the placements (the other\n            # in the example is just for clarity).\n        \"\"\"\n\n        _topology: Topology = copy.deepcopy(storage_constraint)\n        # shallow copy to keep an almost sorted list around\n        # we try to not modify the order of devices in the topology\n        # since _get_host_level_devices relies on the order\n        sorted_devices = _topology.devices.copy()\n        _host_level_devices = GreedyPerfPartitioner._get_host_level_devices(_topology)\n\n        # first partition the uniform sharding options (RW & DP)\n        uniform_sharding_options = _get_uniform_sharding_options(proposal)\n        GreedyPerfPartitioner._uniform_partition(\n            uniform_sharding_options, _topology.devices\n        )\n\n        # group the rest sharding options by colocation type (co-host, co-device, none)\n        # and sort the groups by storage in reverse order\n        sharding_option_groups = _group_and_sort_non_uniform_sharding_options(proposal)\n\n        for sharding_option_group in sharding_option_groups:\n            if (\n                sharding_option_group.sharding_options[0].partition_by\n                == PartitionByType.HOST.value\n            ):\n                GreedyPerfPartitioner._cohost_partition(\n                    sharding_option_group, _host_level_devices\n                )\n            elif (\n                sharding_option_group.sharding_options[0].partition_by\n                == PartitionByType.DEVICE.value\n            ):\n                assert (\n                    len(sharding_option_group.sharding_options) == 1\n                ), f\"Unexpected length for sharding options: {len(sharding_option_group.sharding_options)}\"\n                GreedyPerfPartitioner._device_partition(\n                    sharding_option_group.sharding_options[0],\n                    sorted_devices,\n                    _topology.local_world_size,\n                )\n            else:\n                raise RuntimeError(\n                    f\"Unexpected sharding option group {sharding_option_group}\"\n                )\n        # pyre-ignore [16]: `GreedyPerfPartitioner` has no attribute `_topology`.\n        self._topology: Topology = _topology\n        return proposal",
  "def _device_partition(\n        sharding_option: ShardingOption,\n        devices: List[DeviceHardware],\n        local_world_size: int = 1,\n    ) -> None:\n        for shard in sharding_option.shards:\n            devices.sort(\n                # We use the \"local_rank\" as the secondary key for sorting. This\n                # is to even out the pressure on different hosts. For example, in UVM\n                # case, we will allocate UVM table with the global rank order, and host0\n                # will use a lot more CPU memory than the others. With local rank as the\n                # secondary key, we could even out CPU memory pressure on different host\n                key=lambda device: (device.perf.total, device.rank % local_world_size),\n            )\n            success = False\n            for device in devices:\n                if cast(Storage, shard.storage).fits_in(device.storage):\n                    shard.rank = device.rank\n                    device.storage -= cast(Storage, shard.storage)\n                    device.perf += cast(Perf, shard.perf)\n                    success = True\n                    break\n            if not success:\n                raise PlannerError(\n                    error_type=PlannerErrorType.PARTITION,\n                    message=(\n                        f\"Device partition failed. Couldn't find a rank for shard {shard} of table {sharding_option.name}, \"\n                        f\"largest device storage: {max(devices, key=lambda device: device.storage).storage}\"\n                    ),\n                )",
  "def _cohost_partition(\n        sharding_option_group: ShardingOptionGroup,\n        _host_level_devices: List[List[DeviceHardware]],\n    ) -> None:\n        sorted_host_level_devices = _sort_devices_by_perf(_host_level_devices)\n        for devices in sorted_host_level_devices:\n            host_devices = copy.deepcopy(devices)\n            host_storage = Storage(hbm=0, ddr=0)\n            for device in host_devices:\n                host_storage += device.storage\n            if not sharding_option_group.storage_sum.fits_in(host_storage):\n                continue\n\n            success = True\n            for sharding_option in sharding_option_group.sharding_options:\n                try:\n                    if (\n                        sharding_option.sharding_type\n                        == ShardingType.TABLE_ROW_WISE.value\n                    ):\n                        GreedyPerfPartitioner._uniform_partition(\n                            [sharding_option], host_devices\n                        )\n                    elif (\n                        sharding_option.sharding_type\n                        == ShardingType.TABLE_COLUMN_WISE.value\n                    ):\n                        GreedyPerfPartitioner._device_partition(\n                            sharding_option, host_devices, len(host_devices)\n                        )\n                    else:\n                        raise RuntimeError(\n                            f\"unexpected cohost sharding type: {sharding_option.sharding_type}\"\n                        )\n                except PlannerError:\n                    success = False\n                    break\n            if success:\n                # successfully found a host and partitioned on that host\n                # need to update the devices\n                # resorting host_devices before copying data back\n                host_devices.sort(key=lambda device: device.rank)\n                for device, device_copy in zip(devices, host_devices):\n                    device.storage = device_copy.storage\n                    device.perf = device_copy.perf\n                return\n        raise PlannerError(\n            error_type=PlannerErrorType.PARTITION,\n            message=f\"can't find a host for sharding option group {sharding_option_group}\",\n        )",
  "def _get_host_level_devices(_topology: Topology) -> List[List[DeviceHardware]]:\n        num_hosts: int = _topology.world_size // _topology.local_world_size\n        host_level_devices: List[List[DeviceHardware]] = []\n        for i in range(num_hosts):\n            devices_in_host = _topology.devices[\n                i * _topology.local_world_size : (i + 1) * _topology.local_world_size\n            ]\n            host_level_devices.append(devices_in_host)\n        return host_level_devices",
  "def _uniform_partition(\n        sharding_options: List[ShardingOption], devices: List[DeviceHardware]\n    ) -> None:\n        for sharding_option in sharding_options:\n            if sharding_option.num_shards != len(devices):\n                raise RuntimeError(\n                    f\"For a uniform partition, the number of shards ({sharding_option.num_shards}) must equal the number of devices ({len(devices)})\"\n                )\n            for i in range(len(devices)):\n                storage_needed = cast(Storage, sharding_option.shards[i].storage)\n                if not storage_needed.fits_in(devices[i].storage):\n                    raise PlannerError(\n                        error_type=PlannerErrorType.PARTITION,\n                        message=f\"Shard of size {storage_needed} bytes does not fit on any rank. Device memory cap: {devices[i].storage}.\",\n                    )\n                else:\n                    sharding_option.shards[i].rank = devices[i].rank\n                    devices[i].storage -= storage_needed\n                    devices[i].perf += cast(Perf, sharding_option.shards[i].perf)",
  "def __init__(self, max_search_count: int = 10, tolerance: float = 0.02) -> None:\n        self._max_search_count: int = max_search_count\n        self._tolerance: float = tolerance",
  "def partition(\n        self,\n        proposal: List[ShardingOption],\n        storage_constraint: Topology,\n    ) -> List[ShardingOption]:\n        \"\"\"\n        Repeatedly calls the GreedyPerfPartitioner to find a plan with perf\n        within the tolerance of the original plan that uses the least amount\n        of memory.\n        \"\"\"\n        _perf_model: PerfModel = NoopPerfModel(storage_constraint)\n        _partitioner = GreedyPerfPartitioner()\n        # copying storage_constraint, since we modify it in place\n        _topology: Topology = copy.deepcopy(storage_constraint)\n\n        # set up default plan to fall back on\n        default_plan = _partitioner.partition(proposal, _topology)\n        default_plan = copy.deepcopy(default_plan)\n        original_plan_perf = _perf_model.rate(default_plan)\n\n        max_hbm_per_device: int = _topology.devices[0].storage.hbm\n        logger.info(\n            f\"Default plan uses {round(bytes_to_gb(max_hbm_per_device), 3)} GB per device.\"\n        )\n\n        hbm_requirement: int = 0\n        for sharding_option in proposal:\n            for shard in sharding_option.shards:\n                if shard.storage is not None:\n                    hbm_requirement += shard.storage.hbm\n        min_hbm_per_device: int = int(hbm_requirement / _topology.world_size)\n        logger.info(\n            \"Searching in the range (min_hbm_per_device, max_hbm_per_device): \"\n            f\"({round(bytes_to_gb(min_hbm_per_device), 3)}, \"\n            f\"{round(bytes_to_gb(max_hbm_per_device), 3)})\"\n        )\n\n        # binary search with (min, max] setting\n        search_count = 0\n        while (\n            search_count < self._max_search_count\n            and min_hbm_per_device + 10 * 1024**2 < max_hbm_per_device  # 10MB\n        ):\n            search_count += 1\n            reset_shard_rank(proposal)\n            mid_hbm_per_device: int = (max_hbm_per_device + min_hbm_per_device) // 2\n            set_hbm_per_device(_topology, mid_hbm_per_device)\n            try:\n                new_plan = _partitioner.partition(proposal, _topology)\n                new_plan_perf = _perf_model.rate(new_plan)\n                perf_diff = (\n                    (new_plan_perf - original_plan_perf) / original_plan_perf\n                    if original_plan_perf\n                    else 100\n                )\n                if new_plan_perf > original_plan_perf * (1 + self._tolerance):\n                    # the new plan is worse than the original one\n                    logger.info(\n                        f\"Found a plan with {round(bytes_to_gb(mid_hbm_per_device), 3)} \"\n                        f\"GB per device for embedding tables, \"\n                        f\"but its perf is {round(perf_diff * 100, 3)}% worse than the original plan, \"\n                        f\"which exceeds the {self._tolerance * 100}% tolerance.\"\n                    )\n                    min_hbm_per_device = mid_hbm_per_device\n                else:\n                    # the new plan is better than original one\n                    if perf_diff > 0:\n                        perf_diff_str = (\n                            f\"{round((perf_diff) * 100, 3)}% worse than the original plan, \"\n                            f\"which is within the {self._tolerance * 100}% tolerance.\"\n                        )\n                    else:\n                        perf_diff_str = f\"{round((perf_diff) * 100, 3)}% better than the original plan.\"\n                    logger.info(\n                        f\"Found a more memory-balanced plan with {round(bytes_to_gb(mid_hbm_per_device), 3)} \"\n                        f\"GB per device for embedding tables. The new plan is {perf_diff_str}\"\n                    )\n                    default_plan = copy.deepcopy(new_plan)\n                    max_hbm_per_device = mid_hbm_per_device\n            except PlannerError:\n                logger.info(\n                    f\"Couldn't find a plan with {round(bytes_to_gb(max_hbm_per_device), 3)} \"\n                    f\"GB per device for embedding tables.\"\n                )\n                min_hbm_per_device = mid_hbm_per_device\n\n        return default_plan",
  "class TableBatchedEmbeddingSlice(nn.Parameter):\n    \"\"\"\n    Parameter to represent a slice of a table batched embedding. The slice will be\n    a view of the TBE of shape (num_embeddings, embedding_dim) and contain consistent .grad\n\n    unlike nn.Parameter, requires_grad is not present and follows requires_grad of TBE.data\n\n    Args:\n        data (torch.Tensor): original Data (of a TBE) to make a slice of\n        start_offset (int):\n        end_offset (int):\n        num_embeddings (int):\n        embedding_dim (int):\n    \"\"\"\n\n    __slots__ = [\n        \"_original_tensor\",\n        \"_start_offset\",\n        \"_end_offset\",\n        \"_num_embeddings\",\n        \"_embedding_dim\",\n    ]\n\n    def __init__(\n        self,\n        data: torch.Tensor,\n        start_offset: int,\n        end_offset: int,\n        num_embeddings: int,\n        embedding_dim: int,\n    ) -> None:\n        super().__init__()\n        self._original_tensor: torch.Tensor = data\n        self._start_offset: int = start_offset\n        self._end_offset: int = end_offset\n        self._num_embeddings: int = num_embeddings\n        self._embedding_dim: int = embedding_dim\n        self._init_grad: Optional[torch.Tensor] = None\n        if self._original_tensor.requires_grad:\n            self.retain_grad()\n\n    def __new__(\n        cls,\n        data: torch.Tensor,\n        start_offset: int,\n        end_offset: int,\n        num_embeddings: int,\n        embedding_dim: int,\n    ) -> \"TableBatchedEmbeddingSlice\":\n        _slice = data[start_offset:end_offset].view(num_embeddings, embedding_dim)\n        return _slice.as_subclass(cls)\n\n    def __deepcopy__(\n        self, memo: Dict[int, \"TableBatchedEmbeddingSlice\"]\n    ) -> \"TableBatchedEmbeddingSlice\":\n        if id(self) in memo:\n            return memo[id(self)]\n        else:\n            result = TableBatchedEmbeddingSlice(\n                self._original_tensor.clone(memory_format=torch.preserve_format),\n                self._start_offset,\n                self._end_offset,\n                self._num_embeddings,\n                self._embedding_dim,\n            )\n            memo[id(self)] = result\n            return result\n\n    @property\n    def grad(self) -> Optional[torch.Tensor]:\n        if self._original_tensor.grad is None:\n            return self._init_grad\n        return self._original_tensor.grad[self._start_offset : self._end_offset].view(\n            self._num_embeddings, self._embedding_dim\n        )\n\n    @grad.setter\n    def grad(self, set_grad: torch.Tensor) -> None:\n        self._init_grad = set_grad\n        if set_grad is None:\n            self._original_tensor.grad = None\n        elif self._original_tensor.grad is not None:\n            self._original_tensor.grad[self._start_offset : self._end_offset].copy_(\n                set_grad.view(-1)\n            )\n\n    @property\n    def grad_fn(self) -> None:\n        # set as leaf node\n        return None",
  "def __init__(\n        self,\n        data: torch.Tensor,\n        start_offset: int,\n        end_offset: int,\n        num_embeddings: int,\n        embedding_dim: int,\n    ) -> None:\n        super().__init__()\n        self._original_tensor: torch.Tensor = data\n        self._start_offset: int = start_offset\n        self._end_offset: int = end_offset\n        self._num_embeddings: int = num_embeddings\n        self._embedding_dim: int = embedding_dim\n        self._init_grad: Optional[torch.Tensor] = None\n        if self._original_tensor.requires_grad:\n            self.retain_grad()",
  "def __new__(\n        cls,\n        data: torch.Tensor,\n        start_offset: int,\n        end_offset: int,\n        num_embeddings: int,\n        embedding_dim: int,\n    ) -> \"TableBatchedEmbeddingSlice\":\n        _slice = data[start_offset:end_offset].view(num_embeddings, embedding_dim)\n        return _slice.as_subclass(cls)",
  "def __deepcopy__(\n        self, memo: Dict[int, \"TableBatchedEmbeddingSlice\"]\n    ) -> \"TableBatchedEmbeddingSlice\":\n        if id(self) in memo:\n            return memo[id(self)]\n        else:\n            result = TableBatchedEmbeddingSlice(\n                self._original_tensor.clone(memory_format=torch.preserve_format),\n                self._start_offset,\n                self._end_offset,\n                self._num_embeddings,\n                self._embedding_dim,\n            )\n            memo[id(self)] = result\n            return result",
  "def grad(self) -> Optional[torch.Tensor]:\n        if self._original_tensor.grad is None:\n            return self._init_grad\n        return self._original_tensor.grad[self._start_offset : self._end_offset].view(\n            self._num_embeddings, self._embedding_dim\n        )",
  "def grad(self, set_grad: torch.Tensor) -> None:\n        self._init_grad = set_grad\n        if set_grad is None:\n            self._original_tensor.grad = None\n        elif self._original_tensor.grad is not None:\n            self._original_tensor.grad[self._start_offset : self._end_offset].copy_(\n                set_grad.view(-1)\n            )",
  "def grad_fn(self) -> None:\n        # set as leaf node\n        return None",
  "class TwCwPooledEmbeddingSharding(CwPooledEmbeddingSharding):\n    \"\"\"\n    Shards embedding bags table-wise column-wise, i.e.. a given embedding table is\n    partitioned along its columns and the table slices are placed on all ranks\n    within a host group.\n    \"\"\"\n\n    def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        permute_embeddings: bool = False,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(\n            sharding_infos,\n            env,\n            device,\n            permute_embeddings=permute_embeddings,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )",
  "def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        permute_embeddings: bool = False,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(\n            sharding_infos,\n            env,\n            device,\n            permute_embeddings=permute_embeddings,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )",
  "class SequenceShardingContext(EmbeddingShardingContext):\n    \"\"\"\n    Stores KJTAllToAll context and reuses it in SequenceEmbeddingsAllToAll.\n    SequenceEmbeddingsAllToAll has the same comm pattern as KJTAllToAll.\n\n    Attributes:\n        features_before_input_dist (Optional[KeyedJaggedTensor]): stores the original\n            KJT before input dist.\n        input_splits(List[int]): stores the input splits of KJT AlltoAll.\n        output_splits (List[int]): stores the output splits of KJT AlltoAll.\n        unbucketize_permute_tensor (Optional[torch.Tensor]): stores the permute order of\n            KJT bucketize (for row-wise sharding only).\n        lengths_after_input_dist (Optional[torch.Tensor]): stores the KJT length after\n            input dist.\n    \"\"\"\n\n    features_before_input_dist: Optional[KeyedJaggedTensor] = None\n    input_splits: List[int] = field(default_factory=list)\n    output_splits: List[int] = field(default_factory=list)\n    sparse_features_recat: Optional[torch.Tensor] = None\n    unbucketize_permute_tensor: Optional[torch.Tensor] = None\n    lengths_after_input_dist: Optional[torch.Tensor] = None\n\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        if self.features_before_input_dist is not None:\n            self.features_before_input_dist.record_stream(stream)\n        if self.sparse_features_recat is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            self.sparse_features_recat.record_stream(stream)\n        if self.unbucketize_permute_tensor is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            self.unbucketize_permute_tensor.record_stream(stream)\n        if self.lengths_after_input_dist is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            self.lengths_after_input_dist.record_stream(stream)",
  "class InferSequenceShardingContext(Multistreamable):\n    \"\"\"\n    Stores inference context and reuses it in sequence embedding output_dist or result return.\n\n    Attributes:\n        features KJTList: stores the shards of KJT after input dist.\n        features_before_input_dist KJT: stores the original input KJT (before input dist).\n        unbucketize_permute_tensor Optional[torch.Tensor]: stores unbucketize tensor, only for RowWise sharding.\n    \"\"\"\n\n    features: KJTList\n    features_before_input_dist: Optional[KeyedJaggedTensor] = None\n    unbucketize_permute_tensor: Optional[torch.Tensor] = None\n\n    def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for feature in self.features:\n            feature.record_stream(stream)\n        if self.features_before_input_dist is not None:\n            self.features_before_input_dist.record_stream(stream)\n        if self.unbucketize_permute_tensor is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            self.unbucketize_permute_tensor.record_stream(stream)",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        if self.features_before_input_dist is not None:\n            self.features_before_input_dist.record_stream(stream)\n        if self.sparse_features_recat is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            self.sparse_features_recat.record_stream(stream)\n        if self.unbucketize_permute_tensor is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            self.unbucketize_permute_tensor.record_stream(stream)\n        if self.lengths_after_input_dist is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            self.lengths_after_input_dist.record_stream(stream)",
  "def record_stream(self, stream: torch.cuda.streams.Stream) -> None:\n        for feature in self.features:\n            feature.record_stream(stream)\n        if self.features_before_input_dist is not None:\n            self.features_before_input_dist.record_stream(stream)\n        if self.unbucketize_permute_tensor is not None:\n            # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.\n            self.unbucketize_permute_tensor.record_stream(stream)",
  "class RwSequenceEmbeddingDist(\n    BaseEmbeddingDist[SequenceShardingContext, torch.Tensor, torch.Tensor]\n):\n    \"\"\"\n    Redistributes sequence embedding tensor in RW fashion with an AlltoAll operation.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n        num_features (int): total number of features.\n        device (Optional[torch.device]): device on which buffers will be allocated.\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        num_features: int,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n        self._dist = SequenceEmbeddingsAllToAll(\n            pg,\n            [num_features] * pg.size(),\n            device,\n            codecs=qcomm_codecs_registry.get(\n                CommOp.SEQUENCE_EMBEDDINGS_ALL_TO_ALL.name, None\n            )\n            if qcomm_codecs_registry\n            else None,\n        )\n\n    def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[SequenceShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        Performs AlltoAll operation on sequence embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n            sharding_ctx (SequenceShardingContext): shared context from KJTAllToAll\n                operation.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of sequence embeddings.\n        \"\"\"\n        assert sharding_ctx is not None\n        return self._dist(\n            local_embs,\n            lengths=sharding_ctx.lengths_after_input_dist,\n            input_splits=sharding_ctx.input_splits,\n            output_splits=sharding_ctx.output_splits,\n            batch_size_per_rank=sharding_ctx.batch_size_per_rank,\n            sparse_features_recat=sharding_ctx.sparse_features_recat,\n            unbucketize_permute_tensor=sharding_ctx.unbucketize_permute_tensor,\n        )",
  "class RwSequenceEmbeddingSharding(\n    BaseRwEmbeddingSharding[\n        SequenceShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n    ]\n):\n    \"\"\"\n    Shards sequence (unpooled) row-wise, i.e.. a given embedding table is evenly\n    distributed by rows and table slices are placed on all ranks.\n    \"\"\"\n\n    def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        num_features = self._get_num_features()\n        feature_hash_sizes = self._get_feature_hash_sizes()\n        return RwSparseFeaturesDist(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            pg=self._pg,\n            num_features=num_features,\n            feature_hash_sizes=feature_hash_sizes,\n            device=device if device is not None else self._device,\n            is_sequence=True,\n            has_feature_processor=self._has_feature_processor,\n            need_pos=False,\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[SequenceShardingContext, torch.Tensor, torch.Tensor]:\n        return RwSequenceEmbeddingDist(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._pg,\n            self._get_num_features(),\n            device if device is not None else self._device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "class InferRwSequenceEmbeddingDist(\n    BaseEmbeddingDist[\n        InferSequenceShardingContext, List[torch.Tensor], List[torch.Tensor]\n    ]\n):\n    def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: SeqEmbeddingsAllToOne = SeqEmbeddingsAllToOne(device, world_size)\n\n    def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[InferSequenceShardingContext] = None,\n    ) -> List[torch.Tensor]:\n        return self._dist.forward(local_embs)",
  "class InferRwSequenceEmbeddingSharding(\n    BaseRwEmbeddingSharding[\n        InferSequenceShardingContext, KJTList, List[torch.Tensor], List[torch.Tensor]\n    ]\n):\n    \"\"\"\n    Shards sequence (unpooled) row-wise, i.e.. a given embedding table is evenly\n    distributed by rows and table slices are placed on all ranks for inference.\n    \"\"\"\n\n    def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        num_features = self._get_num_features()\n        feature_hash_sizes = self._get_feature_hash_sizes()\n        return InferRwSparseFeaturesDist(\n            world_size=self._world_size,\n            num_features=num_features,\n            feature_hash_sizes=feature_hash_sizes,\n            device=device if device is not None else self._device,\n            is_sequence=True,\n            has_feature_processor=self._has_feature_processor,\n            need_pos=False,\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device if device is not None else self._device,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[\n        InferSequenceShardingContext, List[torch.Tensor], List[torch.Tensor]\n    ]:\n        return InferRwSequenceEmbeddingDist(\n            device if device is not None else self._device,\n            self._world_size,\n        )",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        num_features: int,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n        self._dist = SequenceEmbeddingsAllToAll(\n            pg,\n            [num_features] * pg.size(),\n            device,\n            codecs=qcomm_codecs_registry.get(\n                CommOp.SEQUENCE_EMBEDDINGS_ALL_TO_ALL.name, None\n            )\n            if qcomm_codecs_registry\n            else None,\n        )",
  "def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[SequenceShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        Performs AlltoAll operation on sequence embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n            sharding_ctx (SequenceShardingContext): shared context from KJTAllToAll\n                operation.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of sequence embeddings.\n        \"\"\"\n        assert sharding_ctx is not None\n        return self._dist(\n            local_embs,\n            lengths=sharding_ctx.lengths_after_input_dist,\n            input_splits=sharding_ctx.input_splits,\n            output_splits=sharding_ctx.output_splits,\n            batch_size_per_rank=sharding_ctx.batch_size_per_rank,\n            sparse_features_recat=sharding_ctx.sparse_features_recat,\n            unbucketize_permute_tensor=sharding_ctx.unbucketize_permute_tensor,\n        )",
  "def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        num_features = self._get_num_features()\n        feature_hash_sizes = self._get_feature_hash_sizes()\n        return RwSparseFeaturesDist(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            pg=self._pg,\n            num_features=num_features,\n            feature_hash_sizes=feature_hash_sizes,\n            device=device if device is not None else self._device,\n            is_sequence=True,\n            has_feature_processor=self._has_feature_processor,\n            need_pos=False,\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[SequenceShardingContext, torch.Tensor, torch.Tensor]:\n        return RwSequenceEmbeddingDist(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._pg,\n            self._get_num_features(),\n            device if device is not None else self._device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: SeqEmbeddingsAllToOne = SeqEmbeddingsAllToOne(device, world_size)",
  "def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[InferSequenceShardingContext] = None,\n    ) -> List[torch.Tensor]:\n        return self._dist.forward(local_embs)",
  "def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        num_features = self._get_num_features()\n        feature_hash_sizes = self._get_feature_hash_sizes()\n        return InferRwSparseFeaturesDist(\n            world_size=self._world_size,\n            num_features=num_features,\n            feature_hash_sizes=feature_hash_sizes,\n            device=device if device is not None else self._device,\n            is_sequence=True,\n            has_feature_processor=self._has_feature_processor,\n            need_pos=False,\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device if device is not None else self._device,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[\n        InferSequenceShardingContext, List[torch.Tensor], List[torch.Tensor]\n    ]:\n        return InferRwSequenceEmbeddingDist(\n            device if device is not None else self._device,\n            self._world_size,\n        )",
  "class BaseTwEmbeddingSharding(EmbeddingSharding[C, F, T, W]):\n    \"\"\"\n    Base class for table wise sharding.\n    \"\"\"\n\n    def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._env = env\n        self._device = device\n        self._pg: Optional[dist.ProcessGroup] = self._env.process_group\n        self._world_size: int = self._env.world_size\n        self._rank: int = self._env.rank\n        sharded_tables_per_rank = self._shard(sharding_infos)\n\n        self._sharded_tables_per_rank: List[\n            List[ShardedEmbeddingTable]\n        ] = sharded_tables_per_rank\n\n        self._grouped_embedding_configs_per_rank: List[\n            List[GroupedEmbeddingConfig]\n        ] = []\n        self._grouped_embedding_configs_per_rank = group_tables(sharded_tables_per_rank)\n        self._grouped_embedding_configs: List[\n            GroupedEmbeddingConfig\n        ] = self._grouped_embedding_configs_per_rank[self._rank]\n\n    def _shard(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n    ) -> List[List[ShardedEmbeddingTable]]:\n        world_size = self._world_size\n        tables_per_rank: List[List[ShardedEmbeddingTable]] = [\n            [] for i in range(world_size)\n        ]\n        for info in sharding_infos:\n            # pyre-fixme [16]\n            shards = info.param_sharding.sharding_spec.shards\n            # construct the global sharded_tensor_metadata\n            global_metadata = ShardedTensorMetadata(\n                shards_metadata=shards,\n                size=torch.Size(\n                    [\n                        info.embedding_config.num_embeddings,\n                        info.embedding_config.embedding_dim,\n                    ]\n                ),\n            )\n\n            # pyre-fixme [16]\n            tables_per_rank[info.param_sharding.ranks[0]].append(\n                ShardedEmbeddingTable(\n                    num_embeddings=info.embedding_config.num_embeddings,\n                    embedding_dim=info.embedding_config.embedding_dim,\n                    name=info.embedding_config.name,\n                    embedding_names=info.embedding_config.embedding_names,\n                    data_type=info.embedding_config.data_type,\n                    feature_names=info.embedding_config.feature_names,\n                    pooling=info.embedding_config.pooling,\n                    is_weighted=info.embedding_config.is_weighted,\n                    has_feature_processor=info.embedding_config.has_feature_processor,\n                    local_rows=info.embedding_config.num_embeddings,\n                    local_cols=info.embedding_config.embedding_dim,\n                    compute_kernel=EmbeddingComputeKernel(\n                        info.param_sharding.compute_kernel\n                    ),\n                    local_metadata=shards[0],\n                    global_metadata=global_metadata,\n                    weight_init_max=info.embedding_config.weight_init_max,\n                    weight_init_min=info.embedding_config.weight_init_min,\n                    fused_params=info.fused_params,\n                )\n            )\n        return tables_per_rank\n\n    def _dim_sum_per_rank(self) -> List[int]:\n        dim_sum_per_rank = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            dim_sum = 0\n            for grouped_config in grouped_embedding_configs:\n                dim_sum += grouped_config.dim_sum()\n            dim_sum_per_rank.append(dim_sum)\n        return dim_sum_per_rank\n\n    def _emb_dim_per_rank_per_feature(self) -> List[List[int]]:\n        emb_dim_per_rank_per_feature = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            emb_dim_per_feature = []\n            for grouped_config in grouped_embedding_configs:\n                emb_dim_per_feature += grouped_config.embedding_dims()\n            emb_dim_per_rank_per_feature.append(emb_dim_per_feature)\n        return emb_dim_per_rank_per_feature\n\n    def embedding_dims(self) -> List[int]:\n        embedding_dims = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            for grouped_config in grouped_embedding_configs:\n                embedding_dims.extend(grouped_config.embedding_dims())\n        return embedding_dims\n\n    def embedding_names(self) -> List[str]:\n        embedding_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            for grouped_config in grouped_embedding_configs:\n                embedding_names.extend(grouped_config.embedding_names())\n        return embedding_names\n\n    def embedding_names_per_rank(self) -> List[List[str]]:\n        embedding_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            embedding_names_per_rank = []\n            for grouped_config in grouped_embedding_configs:\n                embedding_names_per_rank.extend(grouped_config.embedding_names())\n            embedding_names.append(embedding_names_per_rank)\n        return embedding_names\n\n    def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        embedding_shard_metadata = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            for grouped_config in grouped_embedding_configs:\n                embedding_shard_metadata.extend(\n                    grouped_config.embedding_shard_metadata()\n                )\n        return embedding_shard_metadata\n\n    def feature_names(self) -> List[str]:\n        feature_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            for grouped_config in grouped_embedding_configs:\n                feature_names.extend(grouped_config.feature_names())\n        return feature_names\n\n    def embedding_tables(self) -> List[ShardedEmbeddingTable]:\n        embedding_tables = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            for grouped_config in grouped_embedding_configs:\n                embedding_tables.extend(grouped_config.embedding_tables)\n        return embedding_tables\n\n    def feature_names_per_rank(self) -> List[List[str]]:\n        feature_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            feature_names_per_rank = []\n            for grouped_config in grouped_embedding_configs:\n                feature_names_per_rank.extend(grouped_config.feature_names())\n            feature_names.append(feature_names_per_rank)\n        return feature_names\n\n    def features_per_rank(self) -> List[int]:\n        features_per_rank = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            num_features = 0\n            for grouped_config in grouped_embedding_configs:\n                num_features += grouped_config.num_features()\n            features_per_rank.append(num_features)\n        return features_per_rank",
  "class TwSparseFeaturesDist(BaseSparseFeaturesDist[KeyedJaggedTensor]):\n    \"\"\"\n    Redistributes sparse features with an AlltoAll collective operation for table wise\n    sharding.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n        features_per_rank (List[int]): number of features to send to each rank.\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        features_per_rank: List[int],\n    ) -> None:\n        super().__init__()\n        self._dist = KJTAllToAll(\n            pg=pg,\n            splits=features_per_rank,\n        )\n\n    def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> Awaitable[Awaitable[KeyedJaggedTensor]]:\n        \"\"\"\n        Performs AlltoAll operation on sparse features.\n\n        Args:\n            sparse_features (KeyedJaggedTensor): sparse features to redistribute.\n\n        Returns:\n            Awaitable[Awaitable[KeyedJaggedTensor]]: awaitable of awaitable of KeyedJaggedTensor.\n        \"\"\"\n\n        return self._dist(sparse_features)",
  "class TwPooledEmbeddingDist(\n    BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]\n):\n    \"\"\"\n    Redistributes pooled embedding tensor with an AlltoAll collective operation for\n    table wise sharding.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n        dim_sum_per_rank (List[int]): number of features (sum of dimensions) of the\n            embedding in each rank.\n        emb_dim_per_rank_per_feature (List[List[int]]): embedding dimension per rank per\n            feature, used for variable batch per feature.\n        device (Optional[torch.device]): device on which buffers will be allocated.\n        callbacks (Optional[List[Callable[[torch.Tensor], torch.Tensor]]]):\n        qcomm_codecs_registry (Optional[Dict[str, QuantizedCommCodecs]]):\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        dim_sum_per_rank: List[int],\n        emb_dim_per_rank_per_feature: List[List[int]],\n        device: Optional[torch.device] = None,\n        callbacks: Optional[List[Callable[[torch.Tensor], torch.Tensor]]] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._dim_sum_per_rank = dim_sum_per_rank\n        self._device = device\n        self._callbacks = callbacks\n        self._codecs: Optional[QuantizedCommCodecs] = (\n            qcomm_codecs_registry.get(CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name, None)\n            if qcomm_codecs_registry\n            else None\n        )\n        self._emb_dim_per_rank_per_feature = emb_dim_per_rank_per_feature\n        self._dist: Optional[\n            Union[PooledEmbeddingsAllToAll, VariableBatchPooledEmbeddingsAllToAll]\n        ] = None\n\n    def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[EmbeddingShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        Performs AlltoAll operation on pooled embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n            sharding_ctx (Optional[EmbeddingShardingContext]): shared context from\n                KJTAllToAll operation.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of pooled embeddings.\n        \"\"\"\n        if self._dist is None:\n            self._create_output_dist_module(sharding_ctx)\n\n        if sharding_ctx is None:\n            return cast(PooledEmbeddingsAllToAll, self._dist)(local_embs)\n        elif sharding_ctx.variable_batch_per_feature:\n            return cast(VariableBatchPooledEmbeddingsAllToAll, self._dist)(\n                local_embs,\n                batch_size_per_rank_per_feature=sharding_ctx.batch_size_per_rank_per_feature,\n                batch_size_per_feature_pre_a2a=sharding_ctx.batch_size_per_feature_pre_a2a,\n            )\n        else:\n            return cast(PooledEmbeddingsAllToAll, self._dist)(\n                local_embs,\n                batch_size_per_rank=sharding_ctx.batch_size_per_rank,\n            )\n\n    def _create_output_dist_module(\n        self, sharding_ctx: Optional[EmbeddingShardingContext] = None\n    ) -> None:\n        if sharding_ctx is not None and sharding_ctx.variable_batch_per_feature:\n            self._dist = VariableBatchPooledEmbeddingsAllToAll(\n                pg=self._pg,\n                emb_dim_per_rank_per_feature=self._emb_dim_per_rank_per_feature,\n                device=self._device,\n                callbacks=self._callbacks,\n                codecs=self._codecs,\n            )\n        else:\n            self._dist = PooledEmbeddingsAllToAll(\n                pg=self._pg,\n                dim_sum_per_rank=self._dim_sum_per_rank,\n                device=self._device,\n                callbacks=self._callbacks,\n                codecs=self._codecs,\n            )",
  "class TwPooledEmbeddingSharding(\n    BaseTwEmbeddingSharding[\n        EmbeddingShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n    ]\n):\n    \"\"\"\n    Shards embedding bags table-wise, i.e.. a given embedding table is entirely placed\n    on a selected rank.\n    \"\"\"\n\n    def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        assert self._pg is not None\n        return TwSparseFeaturesDist(\n            self._pg,\n            self.features_per_rank(),\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedPooledEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n            feature_processor=feature_processor,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]:\n        assert self._pg is not None\n        return TwPooledEmbeddingDist(\n            pg=self._pg,\n            dim_sum_per_rank=self._dim_sum_per_rank(),\n            emb_dim_per_rank_per_feature=self._emb_dim_per_rank_per_feature(),\n            device=device if device is not None else self._device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "class InferTwSparseFeaturesDist(BaseSparseFeaturesDist[KJTList]):\n    \"\"\"\n    Redistributes sparse features to all devices for inference.\n\n    Args:\n        features_per_rank (List[int]): number of features to send to each rank.\n        world_size (int): number of devices in the topology.\n        fused_params (Dict[str, Any]): fused parameters of the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        features_per_rank: List[int],\n        world_size: int,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._dist = KJTOneToAll(\n            splits=features_per_rank,\n            world_size=world_size,\n            device=device,\n        )\n\n    def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> KJTList:\n        \"\"\"\n        Performs OnetoAll operation on sparse features.\n\n        Args:\n            sparse_features (KeyedJaggedTensor): sparse features to redistribute.\n\n        Returns:\n            Awaitable[Awaitable[KeyedJaggedTensor]]: awaitable of awaitable of KeyedJaggedTensor.\n        \"\"\"\n        return self._dist.forward(sparse_features)",
  "class InferTwPooledEmbeddingDist(\n    BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]\n):\n    \"\"\"\n    Merges pooled embedding tensor from each device for inference.\n\n    Args:\n        device (Optional[torch.device]): device on which buffer will be allocated.\n        world_size (int): number of devices in the topology.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: EmbeddingsAllToOne = EmbeddingsAllToOne(device, world_size, 1)\n\n    def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[NullShardingContext] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Performs AlltoOne operation on pooled embedding tensors.\n\n        Args:\n            local_embs (List[torch.Tensor]): pooled embedding tensors with\n                `len(local_embs) == world_size`.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of merged pooled embedding tensor.\n        \"\"\"\n\n        return self._dist(local_embs)",
  "class InferTwEmbeddingSharding(\n    BaseTwEmbeddingSharding[\n        NullShardingContext, KJTList, List[torch.Tensor], torch.Tensor\n    ]\n):\n    \"\"\"\n    Shards embedding bags table-wise for inference\n    \"\"\"\n\n    def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        return InferTwSparseFeaturesDist(\n            features_per_rank=self.features_per_rank(),\n            world_size=self._world_size,\n            device=device,\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedPooledEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]:\n        device = device if device is not None else self._device\n        assert device is not None\n        return InferTwPooledEmbeddingDist(\n            device,\n            self._world_size,\n        )",
  "def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._env = env\n        self._device = device\n        self._pg: Optional[dist.ProcessGroup] = self._env.process_group\n        self._world_size: int = self._env.world_size\n        self._rank: int = self._env.rank\n        sharded_tables_per_rank = self._shard(sharding_infos)\n\n        self._sharded_tables_per_rank: List[\n            List[ShardedEmbeddingTable]\n        ] = sharded_tables_per_rank\n\n        self._grouped_embedding_configs_per_rank: List[\n            List[GroupedEmbeddingConfig]\n        ] = []\n        self._grouped_embedding_configs_per_rank = group_tables(sharded_tables_per_rank)\n        self._grouped_embedding_configs: List[\n            GroupedEmbeddingConfig\n        ] = self._grouped_embedding_configs_per_rank[self._rank]",
  "def _shard(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n    ) -> List[List[ShardedEmbeddingTable]]:\n        world_size = self._world_size\n        tables_per_rank: List[List[ShardedEmbeddingTable]] = [\n            [] for i in range(world_size)\n        ]\n        for info in sharding_infos:\n            # pyre-fixme [16]\n            shards = info.param_sharding.sharding_spec.shards\n            # construct the global sharded_tensor_metadata\n            global_metadata = ShardedTensorMetadata(\n                shards_metadata=shards,\n                size=torch.Size(\n                    [\n                        info.embedding_config.num_embeddings,\n                        info.embedding_config.embedding_dim,\n                    ]\n                ),\n            )\n\n            # pyre-fixme [16]\n            tables_per_rank[info.param_sharding.ranks[0]].append(\n                ShardedEmbeddingTable(\n                    num_embeddings=info.embedding_config.num_embeddings,\n                    embedding_dim=info.embedding_config.embedding_dim,\n                    name=info.embedding_config.name,\n                    embedding_names=info.embedding_config.embedding_names,\n                    data_type=info.embedding_config.data_type,\n                    feature_names=info.embedding_config.feature_names,\n                    pooling=info.embedding_config.pooling,\n                    is_weighted=info.embedding_config.is_weighted,\n                    has_feature_processor=info.embedding_config.has_feature_processor,\n                    local_rows=info.embedding_config.num_embeddings,\n                    local_cols=info.embedding_config.embedding_dim,\n                    compute_kernel=EmbeddingComputeKernel(\n                        info.param_sharding.compute_kernel\n                    ),\n                    local_metadata=shards[0],\n                    global_metadata=global_metadata,\n                    weight_init_max=info.embedding_config.weight_init_max,\n                    weight_init_min=info.embedding_config.weight_init_min,\n                    fused_params=info.fused_params,\n                )\n            )\n        return tables_per_rank",
  "def _dim_sum_per_rank(self) -> List[int]:\n        dim_sum_per_rank = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            dim_sum = 0\n            for grouped_config in grouped_embedding_configs:\n                dim_sum += grouped_config.dim_sum()\n            dim_sum_per_rank.append(dim_sum)\n        return dim_sum_per_rank",
  "def _emb_dim_per_rank_per_feature(self) -> List[List[int]]:\n        emb_dim_per_rank_per_feature = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            emb_dim_per_feature = []\n            for grouped_config in grouped_embedding_configs:\n                emb_dim_per_feature += grouped_config.embedding_dims()\n            emb_dim_per_rank_per_feature.append(emb_dim_per_feature)\n        return emb_dim_per_rank_per_feature",
  "def embedding_dims(self) -> List[int]:\n        embedding_dims = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            for grouped_config in grouped_embedding_configs:\n                embedding_dims.extend(grouped_config.embedding_dims())\n        return embedding_dims",
  "def embedding_names(self) -> List[str]:\n        embedding_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            for grouped_config in grouped_embedding_configs:\n                embedding_names.extend(grouped_config.embedding_names())\n        return embedding_names",
  "def embedding_names_per_rank(self) -> List[List[str]]:\n        embedding_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            embedding_names_per_rank = []\n            for grouped_config in grouped_embedding_configs:\n                embedding_names_per_rank.extend(grouped_config.embedding_names())\n            embedding_names.append(embedding_names_per_rank)\n        return embedding_names",
  "def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        embedding_shard_metadata = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            for grouped_config in grouped_embedding_configs:\n                embedding_shard_metadata.extend(\n                    grouped_config.embedding_shard_metadata()\n                )\n        return embedding_shard_metadata",
  "def feature_names(self) -> List[str]:\n        feature_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            for grouped_config in grouped_embedding_configs:\n                feature_names.extend(grouped_config.feature_names())\n        return feature_names",
  "def embedding_tables(self) -> List[ShardedEmbeddingTable]:\n        embedding_tables = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            for grouped_config in grouped_embedding_configs:\n                embedding_tables.extend(grouped_config.embedding_tables)\n        return embedding_tables",
  "def feature_names_per_rank(self) -> List[List[str]]:\n        feature_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            feature_names_per_rank = []\n            for grouped_config in grouped_embedding_configs:\n                feature_names_per_rank.extend(grouped_config.feature_names())\n            feature_names.append(feature_names_per_rank)\n        return feature_names",
  "def features_per_rank(self) -> List[int]:\n        features_per_rank = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            num_features = 0\n            for grouped_config in grouped_embedding_configs:\n                num_features += grouped_config.num_features()\n            features_per_rank.append(num_features)\n        return features_per_rank",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        features_per_rank: List[int],\n    ) -> None:\n        super().__init__()\n        self._dist = KJTAllToAll(\n            pg=pg,\n            splits=features_per_rank,\n        )",
  "def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> Awaitable[Awaitable[KeyedJaggedTensor]]:\n        \"\"\"\n        Performs AlltoAll operation on sparse features.\n\n        Args:\n            sparse_features (KeyedJaggedTensor): sparse features to redistribute.\n\n        Returns:\n            Awaitable[Awaitable[KeyedJaggedTensor]]: awaitable of awaitable of KeyedJaggedTensor.\n        \"\"\"\n\n        return self._dist(sparse_features)",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        dim_sum_per_rank: List[int],\n        emb_dim_per_rank_per_feature: List[List[int]],\n        device: Optional[torch.device] = None,\n        callbacks: Optional[List[Callable[[torch.Tensor], torch.Tensor]]] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n        self._pg = pg\n        self._dim_sum_per_rank = dim_sum_per_rank\n        self._device = device\n        self._callbacks = callbacks\n        self._codecs: Optional[QuantizedCommCodecs] = (\n            qcomm_codecs_registry.get(CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name, None)\n            if qcomm_codecs_registry\n            else None\n        )\n        self._emb_dim_per_rank_per_feature = emb_dim_per_rank_per_feature\n        self._dist: Optional[\n            Union[PooledEmbeddingsAllToAll, VariableBatchPooledEmbeddingsAllToAll]\n        ] = None",
  "def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[EmbeddingShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        Performs AlltoAll operation on pooled embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n            sharding_ctx (Optional[EmbeddingShardingContext]): shared context from\n                KJTAllToAll operation.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of pooled embeddings.\n        \"\"\"\n        if self._dist is None:\n            self._create_output_dist_module(sharding_ctx)\n\n        if sharding_ctx is None:\n            return cast(PooledEmbeddingsAllToAll, self._dist)(local_embs)\n        elif sharding_ctx.variable_batch_per_feature:\n            return cast(VariableBatchPooledEmbeddingsAllToAll, self._dist)(\n                local_embs,\n                batch_size_per_rank_per_feature=sharding_ctx.batch_size_per_rank_per_feature,\n                batch_size_per_feature_pre_a2a=sharding_ctx.batch_size_per_feature_pre_a2a,\n            )\n        else:\n            return cast(PooledEmbeddingsAllToAll, self._dist)(\n                local_embs,\n                batch_size_per_rank=sharding_ctx.batch_size_per_rank,\n            )",
  "def _create_output_dist_module(\n        self, sharding_ctx: Optional[EmbeddingShardingContext] = None\n    ) -> None:\n        if sharding_ctx is not None and sharding_ctx.variable_batch_per_feature:\n            self._dist = VariableBatchPooledEmbeddingsAllToAll(\n                pg=self._pg,\n                emb_dim_per_rank_per_feature=self._emb_dim_per_rank_per_feature,\n                device=self._device,\n                callbacks=self._callbacks,\n                codecs=self._codecs,\n            )\n        else:\n            self._dist = PooledEmbeddingsAllToAll(\n                pg=self._pg,\n                dim_sum_per_rank=self._dim_sum_per_rank,\n                device=self._device,\n                callbacks=self._callbacks,\n                codecs=self._codecs,\n            )",
  "def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        assert self._pg is not None\n        return TwSparseFeaturesDist(\n            self._pg,\n            self.features_per_rank(),\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedPooledEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n            feature_processor=feature_processor,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]:\n        assert self._pg is not None\n        return TwPooledEmbeddingDist(\n            pg=self._pg,\n            dim_sum_per_rank=self._dim_sum_per_rank(),\n            emb_dim_per_rank_per_feature=self._emb_dim_per_rank_per_feature(),\n            device=device if device is not None else self._device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def __init__(\n        self,\n        features_per_rank: List[int],\n        world_size: int,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._dist = KJTOneToAll(\n            splits=features_per_rank,\n            world_size=world_size,\n            device=device,\n        )",
  "def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> KJTList:\n        \"\"\"\n        Performs OnetoAll operation on sparse features.\n\n        Args:\n            sparse_features (KeyedJaggedTensor): sparse features to redistribute.\n\n        Returns:\n            Awaitable[Awaitable[KeyedJaggedTensor]]: awaitable of awaitable of KeyedJaggedTensor.\n        \"\"\"\n        return self._dist.forward(sparse_features)",
  "def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: EmbeddingsAllToOne = EmbeddingsAllToOne(device, world_size, 1)",
  "def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[NullShardingContext] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Performs AlltoOne operation on pooled embedding tensors.\n\n        Args:\n            local_embs (List[torch.Tensor]): pooled embedding tensors with\n                `len(local_embs) == world_size`.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of merged pooled embedding tensor.\n        \"\"\"\n\n        return self._dist(local_embs)",
  "def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        return InferTwSparseFeaturesDist(\n            features_per_rank=self.features_per_rank(),\n            world_size=self._world_size,\n            device=device,\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedPooledEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]:\n        device = device if device is not None else self._device\n        assert device is not None\n        return InferTwPooledEmbeddingDist(\n            device,\n            self._world_size,\n        )",
  "class BaseTwRwEmbeddingSharding(EmbeddingSharding[C, F, T, W]):\n    \"\"\"\n    Base class for table wise row wise sharding.\n    \"\"\"\n\n    def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        need_pos: bool = False,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._env = env\n        self._pg: Optional[dist.ProcessGroup] = self._env.process_group\n        self._world_size: int = self._env.world_size\n        self._rank: int = self._env.rank\n        self._device = device\n        self._need_pos = need_pos\n        intra_pg, cross_pg = intra_and_cross_node_pg(\n            device, backend=dist.get_backend(self._pg)\n        )\n        self._intra_pg: Optional[dist.ProcessGroup] = intra_pg\n        self._cross_pg: Optional[dist.ProcessGroup] = cross_pg\n        self._local_size: int = (\n            intra_pg.size() if intra_pg else get_local_size(self._world_size)\n        )\n\n        sharded_tables_per_rank = self._shard(sharding_infos)\n        self._grouped_embedding_configs_per_rank: List[\n            List[GroupedEmbeddingConfig]\n        ] = []\n        self._grouped_embedding_configs_per_node: List[\n            List[GroupedEmbeddingConfig]\n        ] = []\n        self._grouped_embedding_configs_per_rank = group_tables(sharded_tables_per_rank)\n        self._grouped_embedding_configs_per_node = [\n            self._grouped_embedding_configs_per_rank[rank]\n            for rank in range(self._world_size)\n            if rank % self._local_size == 0\n        ]\n        self._has_feature_processor: bool = False\n        for group_config in self._grouped_embedding_configs_per_rank[\n            self._rank // self._local_size\n        ]:\n            if group_config.has_feature_processor:\n                self._has_feature_processor = True\n\n    def _shard(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n    ) -> List[List[ShardedEmbeddingTable]]:\n        world_size = self._world_size\n        local_size = self._local_size\n        tables_per_rank: List[List[ShardedEmbeddingTable]] = [\n            [] for i in range(world_size)\n        ]\n        for info in sharding_infos:\n            # pyre-ignore [16]\n            table_node = info.param_sharding.ranks[0] // local_size\n            # pyre-fixme [16]\n            shards = info.param_sharding.sharding_spec.shards\n\n            # construct the global sharded_tensor_metadata\n            global_metadata = ShardedTensorMetadata(\n                shards_metadata=shards,\n                size=torch.Size(\n                    [\n                        info.embedding_config.num_embeddings,\n                        info.embedding_config.embedding_dim,\n                    ]\n                ),\n            )\n\n            for rank in range(\n                table_node * local_size,\n                (table_node + 1) * local_size,\n            ):\n                rank_idx = rank - (table_node * local_size)\n                tables_per_rank[rank].append(\n                    ShardedEmbeddingTable(\n                        num_embeddings=info.embedding_config.num_embeddings,\n                        embedding_dim=info.embedding_config.embedding_dim,\n                        name=info.embedding_config.name,\n                        embedding_names=info.embedding_config.embedding_names,\n                        data_type=info.embedding_config.data_type,\n                        feature_names=info.embedding_config.feature_names,\n                        pooling=info.embedding_config.pooling,\n                        is_weighted=info.embedding_config.is_weighted,\n                        has_feature_processor=info.embedding_config.has_feature_processor,\n                        local_rows=shards[rank_idx].shard_sizes[0],\n                        local_cols=info.embedding_config.embedding_dim,\n                        compute_kernel=EmbeddingComputeKernel(\n                            info.param_sharding.compute_kernel\n                        ),\n                        local_metadata=shards[rank_idx],\n                        global_metadata=global_metadata,\n                        weight_init_max=info.embedding_config.weight_init_max,\n                        weight_init_min=info.embedding_config.weight_init_min,\n                        fused_params=info.fused_params,\n                    )\n                )\n\n        return tables_per_rank\n\n    def embedding_dims(self) -> List[int]:\n        embedding_dims = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_node:\n            for grouped_config in grouped_embedding_configs:\n                embedding_dims.extend(grouped_config.embedding_dims())\n        return embedding_dims\n\n    def embedding_names(self) -> List[str]:\n        embedding_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_node:\n            for grouped_config in grouped_embedding_configs:\n                embedding_names.extend(grouped_config.embedding_names())\n        return embedding_names\n\n    def embedding_names_per_rank(self) -> List[List[str]]:\n        raise NotImplementedError\n\n    def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        embedding_shard_metadata = []\n        for grouped_config in self._grouped_embedding_configs_per_node:\n            for config in grouped_config:\n                embedding_shard_metadata.extend(config.embedding_shard_metadata())\n        return embedding_shard_metadata\n\n    def feature_names(self) -> List[str]:\n        feature_names = []\n        for grouped_config in self._grouped_embedding_configs_per_node:\n            for config in grouped_config:\n                feature_names.extend(config.feature_names())\n        return feature_names\n\n    def _get_feature_hash_sizes(self) -> List[int]:\n        feature_hash_sizes: List[int] = []\n        for grouped_config in self._grouped_embedding_configs_per_node:\n            for config in grouped_config:\n                feature_hash_sizes.extend(config.feature_hash_sizes())\n        return feature_hash_sizes\n\n    def _dim_sum_per_node(self) -> List[int]:\n        dim_sum_per_rank = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_node:\n            dim_sum = 0\n            for grouped_config in grouped_embedding_configs:\n                dim_sum += grouped_config.dim_sum()\n            dim_sum_per_rank.append(dim_sum)\n        return dim_sum_per_rank\n\n    def _features_per_rank(\n        self, group: List[List[GroupedEmbeddingConfig]]\n    ) -> List[int]:\n        features_per_rank = []\n        for grouped_embedding_configs in group:\n            num_features = 0\n            for grouped_config in grouped_embedding_configs:\n                num_features += grouped_config.num_features()\n            features_per_rank.append(num_features)\n        return features_per_rank",
  "class TwRwSparseFeaturesDist(BaseSparseFeaturesDist[KeyedJaggedTensor]):\n    \"\"\"\n    Bucketizes sparse features in TWRW fashion and then redistributes with an AlltoAll\n    collective operation.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n        intra_pg (dist.ProcessGroup): ProcessGroup within single host group for AlltoAll\n            communication.\n        id_list_features_per_rank (List[int]): number of id list features to send to\n            each rank.\n        id_score_list_features_per_rank (List[int]): number of id score list features to\n            send to each rank.\n        id_list_feature_hash_sizes (List[int]): hash sizes of id list features.\n        id_score_list_feature_hash_sizes (List[int]): hash sizes of id score list\n            features.\n        device (Optional[torch.device]): device on which buffers will be allocated.\n        has_feature_processor (bool): existence of a feature processor (ie. position\n            weighted features).\n\n    Example::\n\n        3 features\n        2 hosts with 2 devices each\n\n        Bucketize each feature into 2 buckets\n        Staggered shuffle with feature splits [2, 1]\n        AlltoAll operation\n\n        NOTE: result of staggered shuffle and AlltoAll operation look the same after\n        reordering in AlltoAll\n\n        Result:\n            host 0 device 0:\n                feature 0 bucket 0\n                feature 1 bucket 0\n\n            host 0 device 1:\n                feature 0 bucket 1\n                feature 1 bucket 1\n\n            host 1 device 0:\n                feature 2 bucket 0\n\n            host 1 device 1:\n                feature 2 bucket 1\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        local_size: int,\n        features_per_rank: List[int],\n        feature_hash_sizes: List[int],\n        device: Optional[torch.device] = None,\n        has_feature_processor: bool = False,\n        need_pos: bool = False,\n    ) -> None:\n        super().__init__()\n        assert pg.size() % local_size == 0, \"currently group granularity must be node\"\n\n        self._world_size: int = pg.size()\n        self._local_size: int = local_size\n        self._num_cross_nodes: int = self._world_size // self._local_size\n        feature_block_sizes = [\n            math.ceil(hash_size / self._local_size) for hash_size in feature_hash_sizes\n        ]\n\n        self._sf_staggered_shuffle: List[int] = self._staggered_shuffle(\n            features_per_rank\n        )\n        self.register_buffer(\n            \"_feature_block_sizes_tensor\",\n            torch.tensor(\n                feature_block_sizes,\n                device=device,\n                dtype=torch.int32,\n            ),\n        )\n        self.register_buffer(\n            \"_sf_staggered_shuffle_tensor\",\n            torch.tensor(\n                self._sf_staggered_shuffle,\n                device=device,\n                dtype=torch.int32,\n            ),\n        )\n        self._dist = KJTAllToAll(\n            pg=pg,\n            splits=features_per_rank,\n            stagger=self._num_cross_nodes,\n        )\n        self._has_feature_processor = has_feature_processor\n        self._need_pos = need_pos\n\n    def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> Awaitable[Awaitable[KeyedJaggedTensor]]:\n        \"\"\"\n        Bucketizes sparse feature values into local world size number of buckets,\n        performs staggered shuffle on the sparse features, and then performs AlltoAll\n        operation.\n\n        Args:\n            sparse_features (KeyedJaggedTensor): sparse features to bucketize and\n                redistribute.\n\n        Returns:\n            Awaitable[KeyedJaggedTensor]: awaitable of KeyedJaggedTensor.\n        \"\"\"\n\n        if sparse_features.variable_stride_per_key():\n            raise ValueError(\n                \"Variable batch per feature is not supported with table-wise-row-wise sharding\"\n            )\n\n        bucketized_features = bucketize_kjt_before_all2all(\n            sparse_features,\n            num_buckets=self._local_size,\n            block_sizes=self._feature_block_sizes_tensor,\n            output_permute=False,\n            bucketize_pos=self._has_feature_processor\n            if sparse_features.weights_or_none() is None\n            else self._need_pos,\n        )[0].permute(\n            self._sf_staggered_shuffle,\n            self._sf_staggered_shuffle_tensor,\n        )\n\n        return self._dist(bucketized_features)\n\n    def _staggered_shuffle(self, features_per_rank: List[int]) -> List[int]:\n        \"\"\"\n        Reorders sparse data such that data is in contiguous blocks and correctly\n        ordered for global TWRW layout.\n        \"\"\"\n\n        nodes = self._world_size // self._local_size\n        features_per_node = [\n            features_per_rank[node * self._local_size] for node in range(nodes)\n        ]\n        node_offsets = [0] + list(itertools.accumulate(features_per_node))\n        num_features = node_offsets[-1]\n\n        return [\n            bucket * num_features + feature\n            for node in range(nodes)\n            for bucket in range(self._local_size)\n            for feature in range(node_offsets[node], node_offsets[node + 1])\n        ]",
  "class TwRwPooledEmbeddingDist(\n    BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]\n):\n    \"\"\"\n    Redistributes pooled embedding tensor in TWRW fashion by performing a reduce-scatter\n    operation row wise on the host level and then an AlltoAll operation table wise on\n    the global level.\n\n    Args:\n        cross_pg (dist.ProcessGroup): global level ProcessGroup for AlltoAll\n            communication.\n        intra_pg (dist.ProcessGroup): host level ProcessGroup for reduce-scatter\n            communication.\n        dim_sum_per_node (List[int]): number of features (sum of dimensions) of the\n            embedding for each host.\n        device (Optional[torch.device]): device on which buffers will be allocated.\n    \"\"\"\n\n    def __init__(\n        self,\n        rank: int,\n        cross_pg: dist.ProcessGroup,\n        intra_pg: dist.ProcessGroup,\n        dim_sum_per_node: List[int],\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n        self._rank = rank\n        self._intra_pg: dist.ProcessGroup = intra_pg\n        self._cross_pg: dist.ProcessGroup = cross_pg\n\n        self._intra_dist = PooledEmbeddingsReduceScatter(\n            intra_pg,\n            codecs=qcomm_codecs_registry.get(\n                CommOp.POOLED_EMBEDDINGS_REDUCE_SCATTER.name, None\n            )\n            if qcomm_codecs_registry\n            else None,\n        )\n        self._cross_dist = PooledEmbeddingsAllToAll(\n            cross_pg,\n            dim_sum_per_node,\n            device,\n            codecs=qcomm_codecs_registry.get(\n                CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name, None\n            )\n            if qcomm_codecs_registry\n            else None,\n        )\n\n    def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[EmbeddingShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        Performs reduce-scatter pooled operation on pooled embeddings tensor followed by\n        AlltoAll pooled operation.\n\n        Args:\n            local_embs (torch.Tensor): pooled embeddings tensor to distribute.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of pooled embeddings tensor.\n        \"\"\"\n        if sharding_ctx is not None and len(set(sharding_ctx.batch_size_per_rank)) > 1:\n            # preprocess batch_size_per_rank\n            (\n                batch_size_per_rank_by_cross_group,\n                batch_size_sum_by_cross_group,\n            ) = self._preprocess_batch_size_per_rank(\n                self._intra_pg.size(),\n                self._cross_pg.size(),\n                sharding_ctx.batch_size_per_rank,\n            )\n            # Perform ReduceScatterV within one host\n            lengths = batch_size_sum_by_cross_group\n            local_rank = self._rank % self._intra_pg.size()\n            batch_size_per_rank = batch_size_per_rank_by_cross_group[local_rank]\n            rs_result = self._intra_dist(local_embs, input_splits=lengths).wait()\n            return self._cross_dist(rs_result, batch_size_per_rank=batch_size_per_rank)\n        else:\n            return self._cross_dist(self._intra_dist(local_embs).wait())\n\n    def _preprocess_batch_size_per_rank(\n        self, local_size: int, nodes: int, batch_size_per_rank: List[int]\n    ) -> Tuple[List[List[int]], List[int]]:\n        \"\"\"\n        Reorders `batch_size_per_rank` so it's aligned with reordered features after\n        AlltoAll.\n        \"\"\"\n        batch_size_per_rank_by_cross_group: List[List[int]] = []\n        batch_size_sum_by_cross_group: List[int] = []\n        for local_rank in range(local_size):\n            batch_size_per_rank_: List[int] = []\n            batch_size_sum = 0\n            for node in range(nodes):\n                batch_size_per_rank_.append(\n                    batch_size_per_rank[local_rank + node * local_size]\n                )\n                batch_size_sum += batch_size_per_rank[local_rank + node * local_size]\n            batch_size_per_rank_by_cross_group.append(batch_size_per_rank_)\n            batch_size_sum_by_cross_group.append(batch_size_sum)\n\n        return batch_size_per_rank_by_cross_group, batch_size_sum_by_cross_group",
  "class TwRwPooledEmbeddingSharding(\n    BaseTwRwEmbeddingSharding[\n        EmbeddingShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n    ]\n):\n    \"\"\"\n    Shards embedding bags table-wise then row-wise.\n    \"\"\"\n\n    def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        features_per_rank = self._features_per_rank(\n            self._grouped_embedding_configs_per_rank\n        )\n        feature_hash_sizes = self._get_feature_hash_sizes()\n        assert self._pg is not None\n        assert self._intra_pg is not None\n        return TwRwSparseFeaturesDist(\n            pg=self._pg,\n            local_size=self._intra_pg.size(),\n            features_per_rank=features_per_rank,\n            feature_hash_sizes=feature_hash_sizes,\n            device=device if device is not None else self._device,\n            has_feature_processor=self._has_feature_processor,\n            need_pos=self._need_pos,\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedPooledEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs_per_rank[self._rank],\n            pg=self._pg,\n            device=device if device is not None else self._device,\n            feature_processor=feature_processor,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]:\n        return TwRwPooledEmbeddingDist(\n            rank=self._rank,\n            cross_pg=cast(dist.ProcessGroup, self._cross_pg),\n            intra_pg=cast(dist.ProcessGroup, self._intra_pg),\n            dim_sum_per_node=self._dim_sum_per_node(),\n            device=device if device is not None else self._device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        need_pos: bool = False,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(qcomm_codecs_registry=qcomm_codecs_registry)\n        self._env = env\n        self._pg: Optional[dist.ProcessGroup] = self._env.process_group\n        self._world_size: int = self._env.world_size\n        self._rank: int = self._env.rank\n        self._device = device\n        self._need_pos = need_pos\n        intra_pg, cross_pg = intra_and_cross_node_pg(\n            device, backend=dist.get_backend(self._pg)\n        )\n        self._intra_pg: Optional[dist.ProcessGroup] = intra_pg\n        self._cross_pg: Optional[dist.ProcessGroup] = cross_pg\n        self._local_size: int = (\n            intra_pg.size() if intra_pg else get_local_size(self._world_size)\n        )\n\n        sharded_tables_per_rank = self._shard(sharding_infos)\n        self._grouped_embedding_configs_per_rank: List[\n            List[GroupedEmbeddingConfig]\n        ] = []\n        self._grouped_embedding_configs_per_node: List[\n            List[GroupedEmbeddingConfig]\n        ] = []\n        self._grouped_embedding_configs_per_rank = group_tables(sharded_tables_per_rank)\n        self._grouped_embedding_configs_per_node = [\n            self._grouped_embedding_configs_per_rank[rank]\n            for rank in range(self._world_size)\n            if rank % self._local_size == 0\n        ]\n        self._has_feature_processor: bool = False\n        for group_config in self._grouped_embedding_configs_per_rank[\n            self._rank // self._local_size\n        ]:\n            if group_config.has_feature_processor:\n                self._has_feature_processor = True",
  "def _shard(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n    ) -> List[List[ShardedEmbeddingTable]]:\n        world_size = self._world_size\n        local_size = self._local_size\n        tables_per_rank: List[List[ShardedEmbeddingTable]] = [\n            [] for i in range(world_size)\n        ]\n        for info in sharding_infos:\n            # pyre-ignore [16]\n            table_node = info.param_sharding.ranks[0] // local_size\n            # pyre-fixme [16]\n            shards = info.param_sharding.sharding_spec.shards\n\n            # construct the global sharded_tensor_metadata\n            global_metadata = ShardedTensorMetadata(\n                shards_metadata=shards,\n                size=torch.Size(\n                    [\n                        info.embedding_config.num_embeddings,\n                        info.embedding_config.embedding_dim,\n                    ]\n                ),\n            )\n\n            for rank in range(\n                table_node * local_size,\n                (table_node + 1) * local_size,\n            ):\n                rank_idx = rank - (table_node * local_size)\n                tables_per_rank[rank].append(\n                    ShardedEmbeddingTable(\n                        num_embeddings=info.embedding_config.num_embeddings,\n                        embedding_dim=info.embedding_config.embedding_dim,\n                        name=info.embedding_config.name,\n                        embedding_names=info.embedding_config.embedding_names,\n                        data_type=info.embedding_config.data_type,\n                        feature_names=info.embedding_config.feature_names,\n                        pooling=info.embedding_config.pooling,\n                        is_weighted=info.embedding_config.is_weighted,\n                        has_feature_processor=info.embedding_config.has_feature_processor,\n                        local_rows=shards[rank_idx].shard_sizes[0],\n                        local_cols=info.embedding_config.embedding_dim,\n                        compute_kernel=EmbeddingComputeKernel(\n                            info.param_sharding.compute_kernel\n                        ),\n                        local_metadata=shards[rank_idx],\n                        global_metadata=global_metadata,\n                        weight_init_max=info.embedding_config.weight_init_max,\n                        weight_init_min=info.embedding_config.weight_init_min,\n                        fused_params=info.fused_params,\n                    )\n                )\n\n        return tables_per_rank",
  "def embedding_dims(self) -> List[int]:\n        embedding_dims = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_node:\n            for grouped_config in grouped_embedding_configs:\n                embedding_dims.extend(grouped_config.embedding_dims())\n        return embedding_dims",
  "def embedding_names(self) -> List[str]:\n        embedding_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_node:\n            for grouped_config in grouped_embedding_configs:\n                embedding_names.extend(grouped_config.embedding_names())\n        return embedding_names",
  "def embedding_names_per_rank(self) -> List[List[str]]:\n        raise NotImplementedError",
  "def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        embedding_shard_metadata = []\n        for grouped_config in self._grouped_embedding_configs_per_node:\n            for config in grouped_config:\n                embedding_shard_metadata.extend(config.embedding_shard_metadata())\n        return embedding_shard_metadata",
  "def feature_names(self) -> List[str]:\n        feature_names = []\n        for grouped_config in self._grouped_embedding_configs_per_node:\n            for config in grouped_config:\n                feature_names.extend(config.feature_names())\n        return feature_names",
  "def _get_feature_hash_sizes(self) -> List[int]:\n        feature_hash_sizes: List[int] = []\n        for grouped_config in self._grouped_embedding_configs_per_node:\n            for config in grouped_config:\n                feature_hash_sizes.extend(config.feature_hash_sizes())\n        return feature_hash_sizes",
  "def _dim_sum_per_node(self) -> List[int]:\n        dim_sum_per_rank = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_node:\n            dim_sum = 0\n            for grouped_config in grouped_embedding_configs:\n                dim_sum += grouped_config.dim_sum()\n            dim_sum_per_rank.append(dim_sum)\n        return dim_sum_per_rank",
  "def _features_per_rank(\n        self, group: List[List[GroupedEmbeddingConfig]]\n    ) -> List[int]:\n        features_per_rank = []\n        for grouped_embedding_configs in group:\n            num_features = 0\n            for grouped_config in grouped_embedding_configs:\n                num_features += grouped_config.num_features()\n            features_per_rank.append(num_features)\n        return features_per_rank",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        local_size: int,\n        features_per_rank: List[int],\n        feature_hash_sizes: List[int],\n        device: Optional[torch.device] = None,\n        has_feature_processor: bool = False,\n        need_pos: bool = False,\n    ) -> None:\n        super().__init__()\n        assert pg.size() % local_size == 0, \"currently group granularity must be node\"\n\n        self._world_size: int = pg.size()\n        self._local_size: int = local_size\n        self._num_cross_nodes: int = self._world_size // self._local_size\n        feature_block_sizes = [\n            math.ceil(hash_size / self._local_size) for hash_size in feature_hash_sizes\n        ]\n\n        self._sf_staggered_shuffle: List[int] = self._staggered_shuffle(\n            features_per_rank\n        )\n        self.register_buffer(\n            \"_feature_block_sizes_tensor\",\n            torch.tensor(\n                feature_block_sizes,\n                device=device,\n                dtype=torch.int32,\n            ),\n        )\n        self.register_buffer(\n            \"_sf_staggered_shuffle_tensor\",\n            torch.tensor(\n                self._sf_staggered_shuffle,\n                device=device,\n                dtype=torch.int32,\n            ),\n        )\n        self._dist = KJTAllToAll(\n            pg=pg,\n            splits=features_per_rank,\n            stagger=self._num_cross_nodes,\n        )\n        self._has_feature_processor = has_feature_processor\n        self._need_pos = need_pos",
  "def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> Awaitable[Awaitable[KeyedJaggedTensor]]:\n        \"\"\"\n        Bucketizes sparse feature values into local world size number of buckets,\n        performs staggered shuffle on the sparse features, and then performs AlltoAll\n        operation.\n\n        Args:\n            sparse_features (KeyedJaggedTensor): sparse features to bucketize and\n                redistribute.\n\n        Returns:\n            Awaitable[KeyedJaggedTensor]: awaitable of KeyedJaggedTensor.\n        \"\"\"\n\n        if sparse_features.variable_stride_per_key():\n            raise ValueError(\n                \"Variable batch per feature is not supported with table-wise-row-wise sharding\"\n            )\n\n        bucketized_features = bucketize_kjt_before_all2all(\n            sparse_features,\n            num_buckets=self._local_size,\n            block_sizes=self._feature_block_sizes_tensor,\n            output_permute=False,\n            bucketize_pos=self._has_feature_processor\n            if sparse_features.weights_or_none() is None\n            else self._need_pos,\n        )[0].permute(\n            self._sf_staggered_shuffle,\n            self._sf_staggered_shuffle_tensor,\n        )\n\n        return self._dist(bucketized_features)",
  "def _staggered_shuffle(self, features_per_rank: List[int]) -> List[int]:\n        \"\"\"\n        Reorders sparse data such that data is in contiguous blocks and correctly\n        ordered for global TWRW layout.\n        \"\"\"\n\n        nodes = self._world_size // self._local_size\n        features_per_node = [\n            features_per_rank[node * self._local_size] for node in range(nodes)\n        ]\n        node_offsets = [0] + list(itertools.accumulate(features_per_node))\n        num_features = node_offsets[-1]\n\n        return [\n            bucket * num_features + feature\n            for node in range(nodes)\n            for bucket in range(self._local_size)\n            for feature in range(node_offsets[node], node_offsets[node + 1])\n        ]",
  "def __init__(\n        self,\n        rank: int,\n        cross_pg: dist.ProcessGroup,\n        intra_pg: dist.ProcessGroup,\n        dim_sum_per_node: List[int],\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n        self._rank = rank\n        self._intra_pg: dist.ProcessGroup = intra_pg\n        self._cross_pg: dist.ProcessGroup = cross_pg\n\n        self._intra_dist = PooledEmbeddingsReduceScatter(\n            intra_pg,\n            codecs=qcomm_codecs_registry.get(\n                CommOp.POOLED_EMBEDDINGS_REDUCE_SCATTER.name, None\n            )\n            if qcomm_codecs_registry\n            else None,\n        )\n        self._cross_dist = PooledEmbeddingsAllToAll(\n            cross_pg,\n            dim_sum_per_node,\n            device,\n            codecs=qcomm_codecs_registry.get(\n                CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name, None\n            )\n            if qcomm_codecs_registry\n            else None,\n        )",
  "def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[EmbeddingShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        Performs reduce-scatter pooled operation on pooled embeddings tensor followed by\n        AlltoAll pooled operation.\n\n        Args:\n            local_embs (torch.Tensor): pooled embeddings tensor to distribute.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of pooled embeddings tensor.\n        \"\"\"\n        if sharding_ctx is not None and len(set(sharding_ctx.batch_size_per_rank)) > 1:\n            # preprocess batch_size_per_rank\n            (\n                batch_size_per_rank_by_cross_group,\n                batch_size_sum_by_cross_group,\n            ) = self._preprocess_batch_size_per_rank(\n                self._intra_pg.size(),\n                self._cross_pg.size(),\n                sharding_ctx.batch_size_per_rank,\n            )\n            # Perform ReduceScatterV within one host\n            lengths = batch_size_sum_by_cross_group\n            local_rank = self._rank % self._intra_pg.size()\n            batch_size_per_rank = batch_size_per_rank_by_cross_group[local_rank]\n            rs_result = self._intra_dist(local_embs, input_splits=lengths).wait()\n            return self._cross_dist(rs_result, batch_size_per_rank=batch_size_per_rank)\n        else:\n            return self._cross_dist(self._intra_dist(local_embs).wait())",
  "def _preprocess_batch_size_per_rank(\n        self, local_size: int, nodes: int, batch_size_per_rank: List[int]\n    ) -> Tuple[List[List[int]], List[int]]:\n        \"\"\"\n        Reorders `batch_size_per_rank` so it's aligned with reordered features after\n        AlltoAll.\n        \"\"\"\n        batch_size_per_rank_by_cross_group: List[List[int]] = []\n        batch_size_sum_by_cross_group: List[int] = []\n        for local_rank in range(local_size):\n            batch_size_per_rank_: List[int] = []\n            batch_size_sum = 0\n            for node in range(nodes):\n                batch_size_per_rank_.append(\n                    batch_size_per_rank[local_rank + node * local_size]\n                )\n                batch_size_sum += batch_size_per_rank[local_rank + node * local_size]\n            batch_size_per_rank_by_cross_group.append(batch_size_per_rank_)\n            batch_size_sum_by_cross_group.append(batch_size_sum)\n\n        return batch_size_per_rank_by_cross_group, batch_size_sum_by_cross_group",
  "def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        features_per_rank = self._features_per_rank(\n            self._grouped_embedding_configs_per_rank\n        )\n        feature_hash_sizes = self._get_feature_hash_sizes()\n        assert self._pg is not None\n        assert self._intra_pg is not None\n        return TwRwSparseFeaturesDist(\n            pg=self._pg,\n            local_size=self._intra_pg.size(),\n            features_per_rank=features_per_rank,\n            feature_hash_sizes=feature_hash_sizes,\n            device=device if device is not None else self._device,\n            has_feature_processor=self._has_feature_processor,\n            need_pos=self._need_pos,\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedPooledEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs_per_rank[self._rank],\n            pg=self._pg,\n            device=device if device is not None else self._device,\n            feature_processor=feature_processor,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]:\n        return TwRwPooledEmbeddingDist(\n            rank=self._rank,\n            cross_pg=cast(dist.ProcessGroup, self._cross_pg),\n            intra_pg=cast(dist.ProcessGroup, self._intra_pg),\n            dim_sum_per_node=self._dim_sum_per_node(),\n            device=device if device is not None else self._device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "class BaseCwEmbeddingSharding(BaseTwEmbeddingSharding[C, F, T, W]):\n    \"\"\"\n    Base class for column-wise sharding.\n    \"\"\"\n\n    def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        permute_embeddings: bool = False,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(\n            sharding_infos,\n            env,\n            device,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n        self._permute_embeddings = permute_embeddings\n        if self._permute_embeddings:\n            self._init_combined_embeddings()\n\n    def _init_combined_embeddings(self) -> None:\n        \"\"\"\n        Grabs the embedding names and dims from TwEmbeddingSharder.\n\n        NOTE:\n            This could have duplications if there are multiple shards from the same\n            table on a rank. Later on we process these to combine shards together.\n        \"\"\"\n\n        embedding_names: List[str] = super().embedding_names()\n        embedding_dims: List[int] = super().embedding_dims()\n\n        embedding_shard_metadata: List[\n            Optional[ShardMetadata]\n        ] = super().embedding_shard_metadata()\n\n        embedding_name_to_index_offset_tuples: Dict[str, List[Tuple[int, int]]] = {}\n        for i, (name, metadata) in enumerate(\n            zip(embedding_names, embedding_shard_metadata)\n        ):\n            if name not in embedding_name_to_index_offset_tuples:\n                embedding_name_to_index_offset_tuples[name] = []\n            embedding_name_to_index_offset_tuples[name].append(\n                (i, metadata.shard_offsets[1] if metadata is not None else 0)\n            )\n\n        embedding_name_to_index: Dict[str, List[int]] = {}\n        for name, index_offset_tuples in embedding_name_to_index_offset_tuples.items():\n            embedding_name_to_index[name] = [\n                idx_off_tuple[0]\n                for idx_off_tuple in sorted(\n                    index_offset_tuples,\n                    key=lambda idx_off_tuple: idx_off_tuple[1],\n                )\n            ]\n\n        combined_embedding_names: List[str] = []\n        seen_embedding_names: Set[str] = set()\n\n        for name in embedding_names:\n            if name not in seen_embedding_names:\n                combined_embedding_names.append(name)\n                seen_embedding_names.add(name)\n\n        combined_embedding_dims: List[int] = []\n\n        embedding_order: List[int] = []\n        for name in combined_embedding_names:\n            combined_embedding_dims.append(\n                sum([embedding_dims[idx] for idx in embedding_name_to_index[name]])\n            )\n            embedding_order.extend(embedding_name_to_index[name])\n\n        self._embedding_names: List[str] = embedding_names\n        self._embedding_dims: List[int] = embedding_dims\n        self._embedding_order: List[int] = embedding_order\n\n        self._combined_embedding_names: List[str] = combined_embedding_names\n        self._combined_embedding_dims: List[int] = combined_embedding_dims\n\n    def _shard(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n    ) -> List[List[ShardedEmbeddingTable]]:\n        world_size: int = self._env.world_size\n        tables_per_rank: List[List[ShardedEmbeddingTable]] = [\n            [] for i in range(world_size)\n        ]\n        for info in sharding_infos:\n            # pyre-fixme [16]\n            shards: List[ShardMetadata] = info.param_sharding.sharding_spec.shards\n\n            # construct the global sharded_tensor_metadata\n            global_metadata = ShardedTensorMetadata(\n                shards_metadata=shards,\n                size=torch.Size(\n                    [\n                        info.embedding_config.num_embeddings,\n                        info.embedding_config.embedding_dim,\n                    ]\n                ),\n            )\n\n            # pyre-fixme [6]\n            for i, rank in enumerate(info.param_sharding.ranks):\n                tables_per_rank[rank].append(\n                    ShardedEmbeddingTable(\n                        num_embeddings=info.embedding_config.num_embeddings,\n                        embedding_dim=info.embedding_config.embedding_dim,\n                        name=info.embedding_config.name,\n                        embedding_names=info.embedding_config.embedding_names,\n                        data_type=info.embedding_config.data_type,\n                        feature_names=info.embedding_config.feature_names,\n                        pooling=info.embedding_config.pooling,\n                        is_weighted=info.embedding_config.is_weighted,\n                        has_feature_processor=info.embedding_config.has_feature_processor,\n                        local_rows=info.embedding_config.num_embeddings,\n                        local_cols=shards[i].shard_sizes[1],\n                        compute_kernel=EmbeddingComputeKernel(\n                            info.param_sharding.compute_kernel\n                        ),\n                        local_metadata=shards[i],\n                        global_metadata=global_metadata,\n                        fused_params=info.fused_params,\n                        weight_init_max=info.embedding_config.weight_init_max,\n                        weight_init_min=info.embedding_config.weight_init_min,\n                    )\n                )\n\n        return tables_per_rank\n\n    def embedding_dims(self) -> List[int]:\n        return (\n            self._combined_embedding_dims\n            if self._permute_embeddings\n            else super().embedding_dims()\n        )\n\n    def embedding_names(self) -> List[str]:\n        return (\n            self._combined_embedding_names\n            if self._permute_embeddings\n            else super().embedding_names()\n        )",
  "class CwPooledEmbeddingSharding(\n    BaseCwEmbeddingSharding[\n        EmbeddingShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n    ]\n):\n    \"\"\"\n    Shards embedding bags column-wise, i.e.. a given embedding table is partitioned\n    along its columns and placed on specified ranks.\n    \"\"\"\n\n    def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        assert self._pg is not None\n        return TwSparseFeaturesDist(\n            self._pg,\n            self.features_per_rank(),\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedPooledEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n            feature_processor=feature_processor,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]:\n        device = device if device is not None else self._device\n        embedding_permute_op: Optional[PermutePooledEmbeddingsSplit] = None\n        callbacks: Optional[List[Callable[[torch.Tensor], torch.Tensor]]] = None\n        if self._permute_embeddings and self._embedding_order != list(\n            range(len(self._embedding_order))\n        ):\n            assert len(self._embedding_order) == len(self._embedding_dims)\n            embedding_permute_op = PermutePooledEmbeddingsSplit(\n                self._embedding_dims, self._embedding_order, device=device\n            )\n            callbacks = [embedding_permute_op]\n        assert self._pg is not None\n        return TwPooledEmbeddingDist(\n            pg=self._pg,\n            dim_sum_per_rank=self._dim_sum_per_rank(),\n            emb_dim_per_rank_per_feature=self._emb_dim_per_rank_per_feature(),\n            device=device,\n            callbacks=callbacks,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "class InferCwPooledEmbeddingSharding(\n    BaseCwEmbeddingSharding[\n        NullShardingContext, KJTList, List[torch.Tensor], torch.Tensor\n    ]\n):\n    def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        return InferTwSparseFeaturesDist(\n            self.features_per_rank(),\n            self._world_size,\n            device if device is not None else self._device,\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedPooledEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device if device is not None else self._device,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]:\n        device = device if device is not None else self._device\n        assert device is not None\n\n        dist_out = InferCwPooledEmbeddingDist(\n            device,\n            self._world_size,\n        )\n\n        if self._permute_embeddings and self._embedding_order != list(\n            range(len(self._embedding_order))\n        ):\n            return InferCwPooledEmbeddingDistWithPermute(\n                device, self._world_size, self._embedding_dims, self._embedding_order\n            )\n\n        return dist_out",
  "class InferCwPooledEmbeddingDist(\n    BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]\n):\n    def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: EmbeddingsAllToOne = EmbeddingsAllToOne(\n            device=device, world_size=world_size, cat_dim=1\n        )\n\n    def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[NullShardingContext] = None,\n    ) -> torch.Tensor:\n        return self._dist.forward(\n            local_embs,\n        )",
  "def _fx_wrap_permute(\n    permute_module: PermutePooledEmbeddingsSplit, input: torch.Tensor\n) -> torch.Tensor:\n    return permute_module.forward(input)",
  "class InferCwPooledEmbeddingDistWithPermute(\n    BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]\n):\n    def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n        embedding_dims: List[int],\n        permute: List[int],\n    ) -> None:\n        super().__init__()\n        self._dist: EmbeddingsAllToOne = EmbeddingsAllToOne(\n            device=device, world_size=world_size, cat_dim=1\n        )\n        self._permute: PermutePooledEmbeddingsSplit = PermutePooledEmbeddingsSplit(\n            embs_dims=embedding_dims,\n            permute=permute,\n            device=device,\n        )\n\n    def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[NullShardingContext] = None,\n    ) -> torch.Tensor:\n        return self._permute.forward(self._dist.forward(local_embs))",
  "def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        permute_embeddings: bool = False,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(\n            sharding_infos,\n            env,\n            device,\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n        self._permute_embeddings = permute_embeddings\n        if self._permute_embeddings:\n            self._init_combined_embeddings()",
  "def _init_combined_embeddings(self) -> None:\n        \"\"\"\n        Grabs the embedding names and dims from TwEmbeddingSharder.\n\n        NOTE:\n            This could have duplications if there are multiple shards from the same\n            table on a rank. Later on we process these to combine shards together.\n        \"\"\"\n\n        embedding_names: List[str] = super().embedding_names()\n        embedding_dims: List[int] = super().embedding_dims()\n\n        embedding_shard_metadata: List[\n            Optional[ShardMetadata]\n        ] = super().embedding_shard_metadata()\n\n        embedding_name_to_index_offset_tuples: Dict[str, List[Tuple[int, int]]] = {}\n        for i, (name, metadata) in enumerate(\n            zip(embedding_names, embedding_shard_metadata)\n        ):\n            if name not in embedding_name_to_index_offset_tuples:\n                embedding_name_to_index_offset_tuples[name] = []\n            embedding_name_to_index_offset_tuples[name].append(\n                (i, metadata.shard_offsets[1] if metadata is not None else 0)\n            )\n\n        embedding_name_to_index: Dict[str, List[int]] = {}\n        for name, index_offset_tuples in embedding_name_to_index_offset_tuples.items():\n            embedding_name_to_index[name] = [\n                idx_off_tuple[0]\n                for idx_off_tuple in sorted(\n                    index_offset_tuples,\n                    key=lambda idx_off_tuple: idx_off_tuple[1],\n                )\n            ]\n\n        combined_embedding_names: List[str] = []\n        seen_embedding_names: Set[str] = set()\n\n        for name in embedding_names:\n            if name not in seen_embedding_names:\n                combined_embedding_names.append(name)\n                seen_embedding_names.add(name)\n\n        combined_embedding_dims: List[int] = []\n\n        embedding_order: List[int] = []\n        for name in combined_embedding_names:\n            combined_embedding_dims.append(\n                sum([embedding_dims[idx] for idx in embedding_name_to_index[name]])\n            )\n            embedding_order.extend(embedding_name_to_index[name])\n\n        self._embedding_names: List[str] = embedding_names\n        self._embedding_dims: List[int] = embedding_dims\n        self._embedding_order: List[int] = embedding_order\n\n        self._combined_embedding_names: List[str] = combined_embedding_names\n        self._combined_embedding_dims: List[int] = combined_embedding_dims",
  "def _shard(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n    ) -> List[List[ShardedEmbeddingTable]]:\n        world_size: int = self._env.world_size\n        tables_per_rank: List[List[ShardedEmbeddingTable]] = [\n            [] for i in range(world_size)\n        ]\n        for info in sharding_infos:\n            # pyre-fixme [16]\n            shards: List[ShardMetadata] = info.param_sharding.sharding_spec.shards\n\n            # construct the global sharded_tensor_metadata\n            global_metadata = ShardedTensorMetadata(\n                shards_metadata=shards,\n                size=torch.Size(\n                    [\n                        info.embedding_config.num_embeddings,\n                        info.embedding_config.embedding_dim,\n                    ]\n                ),\n            )\n\n            # pyre-fixme [6]\n            for i, rank in enumerate(info.param_sharding.ranks):\n                tables_per_rank[rank].append(\n                    ShardedEmbeddingTable(\n                        num_embeddings=info.embedding_config.num_embeddings,\n                        embedding_dim=info.embedding_config.embedding_dim,\n                        name=info.embedding_config.name,\n                        embedding_names=info.embedding_config.embedding_names,\n                        data_type=info.embedding_config.data_type,\n                        feature_names=info.embedding_config.feature_names,\n                        pooling=info.embedding_config.pooling,\n                        is_weighted=info.embedding_config.is_weighted,\n                        has_feature_processor=info.embedding_config.has_feature_processor,\n                        local_rows=info.embedding_config.num_embeddings,\n                        local_cols=shards[i].shard_sizes[1],\n                        compute_kernel=EmbeddingComputeKernel(\n                            info.param_sharding.compute_kernel\n                        ),\n                        local_metadata=shards[i],\n                        global_metadata=global_metadata,\n                        fused_params=info.fused_params,\n                        weight_init_max=info.embedding_config.weight_init_max,\n                        weight_init_min=info.embedding_config.weight_init_min,\n                    )\n                )\n\n        return tables_per_rank",
  "def embedding_dims(self) -> List[int]:\n        return (\n            self._combined_embedding_dims\n            if self._permute_embeddings\n            else super().embedding_dims()\n        )",
  "def embedding_names(self) -> List[str]:\n        return (\n            self._combined_embedding_names\n            if self._permute_embeddings\n            else super().embedding_names()\n        )",
  "def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        assert self._pg is not None\n        return TwSparseFeaturesDist(\n            self._pg,\n            self.features_per_rank(),\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedPooledEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n            feature_processor=feature_processor,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]:\n        device = device if device is not None else self._device\n        embedding_permute_op: Optional[PermutePooledEmbeddingsSplit] = None\n        callbacks: Optional[List[Callable[[torch.Tensor], torch.Tensor]]] = None\n        if self._permute_embeddings and self._embedding_order != list(\n            range(len(self._embedding_order))\n        ):\n            assert len(self._embedding_order) == len(self._embedding_dims)\n            embedding_permute_op = PermutePooledEmbeddingsSplit(\n                self._embedding_dims, self._embedding_order, device=device\n            )\n            callbacks = [embedding_permute_op]\n        assert self._pg is not None\n        return TwPooledEmbeddingDist(\n            pg=self._pg,\n            dim_sum_per_rank=self._dim_sum_per_rank(),\n            emb_dim_per_rank_per_feature=self._emb_dim_per_rank_per_feature(),\n            device=device,\n            callbacks=callbacks,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        return InferTwSparseFeaturesDist(\n            self.features_per_rank(),\n            self._world_size,\n            device if device is not None else self._device,\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedPooledEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device if device is not None else self._device,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]:\n        device = device if device is not None else self._device\n        assert device is not None\n\n        dist_out = InferCwPooledEmbeddingDist(\n            device,\n            self._world_size,\n        )\n\n        if self._permute_embeddings and self._embedding_order != list(\n            range(len(self._embedding_order))\n        ):\n            return InferCwPooledEmbeddingDistWithPermute(\n                device, self._world_size, self._embedding_dims, self._embedding_order\n            )\n\n        return dist_out",
  "def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: EmbeddingsAllToOne = EmbeddingsAllToOne(\n            device=device, world_size=world_size, cat_dim=1\n        )",
  "def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[NullShardingContext] = None,\n    ) -> torch.Tensor:\n        return self._dist.forward(\n            local_embs,\n        )",
  "def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n        embedding_dims: List[int],\n        permute: List[int],\n    ) -> None:\n        super().__init__()\n        self._dist: EmbeddingsAllToOne = EmbeddingsAllToOne(\n            device=device, world_size=world_size, cat_dim=1\n        )\n        self._permute: PermutePooledEmbeddingsSplit = PermutePooledEmbeddingsSplit(\n            embs_dims=embedding_dims,\n            permute=permute,\n            device=device,\n        )",
  "def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[NullShardingContext] = None,\n    ) -> torch.Tensor:\n        return self._permute.forward(self._dist.forward(local_embs))",
  "class DpSequenceEmbeddingDist(\n    BaseEmbeddingDist[SequenceShardingContext, torch.Tensor, torch.Tensor]\n):\n    \"\"\"\n    Distributes sequence embeddings to be data-parallel.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[SequenceShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        No-op as sequence embeddings are already distributed in data-parallel fashion.\n\n        Args:\n            local_embs (torch.Tensor): output sequence embeddings.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of pooled embeddings tensor.\n        \"\"\"\n\n        return NoWait(local_embs)",
  "class DpSequenceEmbeddingSharding(\n    BaseDpEmbeddingSharding[\n        SequenceShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n    ]\n):\n    \"\"\"\n    Shards sequence (unpooled) embedding data-parallel, with no table sharding i.e.. a\n    given embedding table is replicated across all ranks.\n    \"\"\"\n\n    def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        return DpSparseFeaturesDist()\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        assert feature_processor is None\n        return GroupedEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._env.process_group,\n            device=device if device is not None else self._device,\n        )\n\n    def create_output_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseEmbeddingDist[SequenceShardingContext, torch.Tensor, torch.Tensor]:\n        return DpSequenceEmbeddingDist()",
  "def __init__(self) -> None:\n        super().__init__()",
  "def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[SequenceShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        No-op as sequence embeddings are already distributed in data-parallel fashion.\n\n        Args:\n            local_embs (torch.Tensor): output sequence embeddings.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of pooled embeddings tensor.\n        \"\"\"\n\n        return NoWait(local_embs)",
  "def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        return DpSparseFeaturesDist()",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        assert feature_processor is None\n        return GroupedEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._env.process_group,\n            device=device if device is not None else self._device,\n        )",
  "def create_output_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseEmbeddingDist[SequenceShardingContext, torch.Tensor, torch.Tensor]:\n        return DpSequenceEmbeddingDist()",
  "class BaseRwEmbeddingSharding(EmbeddingSharding[C, F, T, W]):\n    \"\"\"\n    Base class for row-wise sharding.\n    \"\"\"\n\n    def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        need_pos: bool = False,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n\n        self._env = env\n        self._pg: Optional[dist.ProcessGroup] = self._env.process_group\n        self._world_size: int = self._env.world_size\n        self._rank: int = self._env.rank\n        if device is None:\n            device = torch.device(\"cpu\")\n        self._device: torch.device = device\n        sharded_tables_per_rank = self._shard(sharding_infos)\n        self._need_pos = need_pos\n        self._grouped_embedding_configs_per_rank: List[\n            List[GroupedEmbeddingConfig]\n        ] = []\n        self._grouped_embedding_configs_per_rank = group_tables(sharded_tables_per_rank)\n        self._grouped_embedding_configs: List[\n            GroupedEmbeddingConfig\n        ] = self._grouped_embedding_configs_per_rank[self._rank]\n\n        self._has_feature_processor: bool = False\n        for group_config in self._grouped_embedding_configs:\n            if group_config.has_feature_processor:\n                self._has_feature_processor = True\n\n    def _shard(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n    ) -> List[List[ShardedEmbeddingTable]]:\n        tables_per_rank: List[List[ShardedEmbeddingTable]] = [\n            [] for i in range(self._world_size)\n        ]\n        for info in sharding_infos:\n            # pyre-fixme [16]\n            shards = info.param_sharding.sharding_spec.shards\n\n            # construct the global sharded_tensor_metadata\n            global_metadata = ShardedTensorMetadata(\n                shards_metadata=shards,\n                size=torch.Size(\n                    [\n                        info.embedding_config.num_embeddings,\n                        info.embedding_config.embedding_dim,\n                    ]\n                ),\n            )\n\n            for rank in range(self._world_size):\n                tables_per_rank[rank].append(\n                    ShardedEmbeddingTable(\n                        num_embeddings=info.embedding_config.num_embeddings,\n                        embedding_dim=info.embedding_config.embedding_dim,\n                        name=info.embedding_config.name,\n                        embedding_names=info.embedding_config.embedding_names,\n                        data_type=info.embedding_config.data_type,\n                        feature_names=info.embedding_config.feature_names,\n                        pooling=info.embedding_config.pooling,\n                        is_weighted=info.embedding_config.is_weighted,\n                        has_feature_processor=info.embedding_config.has_feature_processor,\n                        local_rows=shards[rank].shard_sizes[0],\n                        local_cols=info.embedding_config.embedding_dim,\n                        compute_kernel=EmbeddingComputeKernel(\n                            info.param_sharding.compute_kernel\n                        ),\n                        local_metadata=shards[rank],\n                        global_metadata=global_metadata,\n                        weight_init_max=info.embedding_config.weight_init_max,\n                        weight_init_min=info.embedding_config.weight_init_min,\n                        fused_params=info.fused_params,\n                    )\n                )\n        return tables_per_rank\n\n    def embedding_dims(self) -> List[int]:\n        embedding_dims = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_dims.extend(grouped_config.embedding_dims())\n        return embedding_dims\n\n    def embedding_names(self) -> List[str]:\n        embedding_names = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_names.extend(grouped_config.embedding_names())\n        return embedding_names\n\n    def embedding_names_per_rank(self) -> List[List[str]]:\n        embedding_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            embedding_names_per_rank = []\n            for grouped_config in grouped_embedding_configs:\n                embedding_names_per_rank.extend(grouped_config.embedding_names())\n            embedding_names.append(embedding_names_per_rank)\n        return embedding_names\n\n    def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        embedding_shard_metadata = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_shard_metadata.extend(grouped_config.embedding_shard_metadata())\n        return embedding_shard_metadata\n\n    def feature_names(self) -> List[str]:\n        feature_names = []\n        for grouped_config in self._grouped_embedding_configs:\n            feature_names.extend(grouped_config.feature_names())\n        return feature_names\n\n    def embedding_tables(self) -> List[ShardedEmbeddingTable]:\n        embedding_tables = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_tables.extend(grouped_config.embedding_tables)\n        return embedding_tables\n\n    def _get_num_features(self) -> int:\n        return sum(\n            group_config.num_features()\n            for group_config in self._grouped_embedding_configs\n        )\n\n    def _get_feature_hash_sizes(self) -> List[int]:\n        feature_hash_sizes: List[int] = []\n        for group_config in self._grouped_embedding_configs:\n            feature_hash_sizes.extend(group_config.feature_hash_sizes())\n        return feature_hash_sizes",
  "class RwSparseFeaturesDist(BaseSparseFeaturesDist[KeyedJaggedTensor]):\n    \"\"\"\n    Bucketizes sparse features in RW fashion and then redistributes with an AlltoAll\n    collective operation.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n        intra_pg (dist.ProcessGroup): ProcessGroup within single host group for AlltoAll\n            communication.\n        num_features (int): total number of features.\n        feature_hash_sizes (List[int]): hash sizes of features.\n        device (Optional[torch.device]): device on which buffers will be allocated.\n        is_sequence (bool): if this is for a sequence embedding.\n        has_feature_processor (bool): existence of feature processor (ie. position\n            weighted features).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        num_features: int,\n        feature_hash_sizes: List[int],\n        device: Optional[torch.device] = None,\n        is_sequence: bool = False,\n        has_feature_processor: bool = False,\n        need_pos: bool = False,\n    ) -> None:\n        super().__init__()\n        self._world_size: int = pg.size()\n        self._num_features = num_features\n        feature_block_sizes = [\n            (hash_size + self._world_size - 1) // self._world_size\n            for hash_size in feature_hash_sizes\n        ]\n        self.register_buffer(\n            \"_feature_block_sizes_tensor\",\n            torch.tensor(\n                feature_block_sizes,\n                device=device,\n                dtype=torch.int64,\n            ),\n        )\n        self._dist = KJTAllToAll(\n            pg=pg,\n            splits=[self._num_features] * self._world_size,\n        )\n        self._is_sequence = is_sequence\n        self._has_feature_processor = has_feature_processor\n        self._need_pos = need_pos\n        self.unbucketize_permute_tensor: Optional[torch.Tensor] = None\n\n    def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> Awaitable[Awaitable[KeyedJaggedTensor]]:\n        \"\"\"\n        Bucketizes sparse feature values into world size number of buckets and then\n        performs AlltoAll operation.\n\n        Args:\n            sparse_features (KeyedJaggedTensor): sparse features to bucketize and\n                redistribute.\n\n        Returns:\n            Awaitable[Awaitable[KeyedJaggedTensor]]: awaitable of awaitable of KeyedJaggedTensor.\n        \"\"\"\n\n        if sparse_features.variable_stride_per_key():\n            raise ValueError(\n                \"Variable batch per feature is not supported with row-wise sharding\"\n            )\n\n        (\n            bucketized_features,\n            self.unbucketize_permute_tensor,\n        ) = bucketize_kjt_before_all2all(\n            sparse_features,\n            num_buckets=self._world_size,\n            block_sizes=self._feature_block_sizes_tensor,\n            output_permute=self._is_sequence,\n            bucketize_pos=self._has_feature_processor\n            if sparse_features.weights_or_none() is None\n            else self._need_pos,\n        )\n\n        return self._dist(bucketized_features)",
  "class RwPooledEmbeddingDist(\n    BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]\n):\n    \"\"\"\n    Redistributes pooled embedding tensor in RW fashion by performing a reduce-scatter\n    operation.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for reduce-scatter communication.\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n\n        self._dist = PooledEmbeddingsReduceScatter(\n            pg,\n            codecs=qcomm_codecs_registry.get(\n                CommOp.POOLED_EMBEDDINGS_REDUCE_SCATTER.name, None\n            )\n            if qcomm_codecs_registry\n            else None,\n        )\n\n    def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[EmbeddingShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        Performs reduce-scatter pooled operation on pooled embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): pooled embeddings tensor to distribute.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of pooled embeddings tensor.\n        \"\"\"\n\n        if sharding_ctx is None:\n            return self._dist(local_embs)\n        else:\n            return self._dist(local_embs, input_splits=sharding_ctx.batch_size_per_rank)",
  "class InferRwPooledEmbeddingDist(\n    BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]\n):\n    \"\"\"\n    Redistributes sequence embedding tensor in RW fashion with an AlltoOne operation.\n\n    Args:\n        device (torch.device): device on which the tensors will be communicated to.\n        world_size (int): number of devices in the topology.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: EmbeddingsAllToOneReduce = EmbeddingsAllToOneReduce(\n            device=device,\n            world_size=world_size,\n            cat_dim=1,\n        )\n\n    def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[NullShardingContext] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Performs AlltoOne operation on sequence embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of sequence embeddings.\n        \"\"\"\n\n        return self._dist.forward(\n            local_embs,\n        )",
  "class RwPooledEmbeddingSharding(\n    BaseRwEmbeddingSharding[\n        EmbeddingShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n    ]\n):\n    \"\"\"\n    Shards embedding bags row-wise, i.e.. a given embedding table is evenly distributed\n    by rows and table slices are placed on all ranks.\n    \"\"\"\n\n    def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        num_features = self._get_num_features()\n        feature_hash_sizes = self._get_feature_hash_sizes()\n        return RwSparseFeaturesDist(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            pg=self._pg,\n            num_features=num_features,\n            feature_hash_sizes=feature_hash_sizes,\n            device=device if device is not None else self._device,\n            is_sequence=False,\n            has_feature_processor=self._has_feature_processor,\n            need_pos=self._need_pos,\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedPooledEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n            feature_processor=feature_processor,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]:\n        return RwPooledEmbeddingDist(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._pg,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def get_block_sizes_runtime_device(\n    block_sizes: List[int],\n    runtime_device: torch.device,\n    tensor_cache: Dict[str, torch.Tensor],\n) -> torch.Tensor:\n    cache_key: str = \"__block_sizes\"\n    if cache_key not in tensor_cache:\n        tensor_cache[cache_key] = torch.tensor(\n            block_sizes,\n            device=runtime_device,\n            dtype=torch.int32,\n        )\n\n    return tensor_cache[cache_key]",
  "class InferRwSparseFeaturesDist(BaseSparseFeaturesDist[KJTList]):\n    def __init__(\n        self,\n        world_size: int,\n        num_features: int,\n        feature_hash_sizes: List[int],\n        device: Optional[torch.device] = None,\n        is_sequence: bool = False,\n        has_feature_processor: bool = False,\n        need_pos: bool = False,\n    ) -> None:\n        super().__init__()\n        self._world_size: int = world_size\n        self._num_features = num_features\n        self.feature_block_sizes: List[int] = [\n            (hash_size + self._world_size - 1) // self._world_size\n            for hash_size in feature_hash_sizes\n        ]\n        self.tensor_cache: Dict[str, torch.Tensor] = {}\n        self._dist = KJTOneToAll(\n            splits=self._world_size * [self._num_features],\n            world_size=world_size,\n            device=device,\n        )\n        self._is_sequence = is_sequence\n        self._has_feature_processor = has_feature_processor\n        self._need_pos = need_pos\n        self.unbucketize_permute_tensor: Optional[torch.Tensor] = None\n\n    def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> KJTList:\n        block_sizes = get_block_sizes_runtime_device(\n            self.feature_block_sizes,\n            sparse_features.device(),\n            self.tensor_cache,\n        )\n        (\n            bucketized_features,\n            self.unbucketize_permute_tensor,\n        ) = bucketize_kjt_before_all2all(\n            sparse_features,\n            num_buckets=self._world_size,\n            block_sizes=block_sizes,\n            output_permute=self._is_sequence,\n            bucketize_pos=self._has_feature_processor\n            if sparse_features.weights_or_none() is None\n            else self._need_pos,\n        )\n        return self._dist.forward(bucketized_features)",
  "class InferRwPooledEmbeddingSharding(\n    BaseRwEmbeddingSharding[\n        NullShardingContext, KJTList, List[torch.Tensor], torch.Tensor\n    ]\n):\n    def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        num_features = self._get_num_features()\n        feature_hash_sizes = self._get_feature_hash_sizes()\n        return InferRwSparseFeaturesDist(\n            world_size=self._world_size,\n            num_features=num_features,\n            feature_hash_sizes=feature_hash_sizes,\n            device=device if device is not None else self._device,\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedPooledEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device if device is not None else self._device,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]:\n        assert device is not None\n        return InferRwPooledEmbeddingDist(\n            device=device,\n            world_size=self._world_size,\n        )",
  "def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n        need_pos: bool = False,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__(\n            qcomm_codecs_registry=qcomm_codecs_registry,\n        )\n\n        self._env = env\n        self._pg: Optional[dist.ProcessGroup] = self._env.process_group\n        self._world_size: int = self._env.world_size\n        self._rank: int = self._env.rank\n        if device is None:\n            device = torch.device(\"cpu\")\n        self._device: torch.device = device\n        sharded_tables_per_rank = self._shard(sharding_infos)\n        self._need_pos = need_pos\n        self._grouped_embedding_configs_per_rank: List[\n            List[GroupedEmbeddingConfig]\n        ] = []\n        self._grouped_embedding_configs_per_rank = group_tables(sharded_tables_per_rank)\n        self._grouped_embedding_configs: List[\n            GroupedEmbeddingConfig\n        ] = self._grouped_embedding_configs_per_rank[self._rank]\n\n        self._has_feature_processor: bool = False\n        for group_config in self._grouped_embedding_configs:\n            if group_config.has_feature_processor:\n                self._has_feature_processor = True",
  "def _shard(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n    ) -> List[List[ShardedEmbeddingTable]]:\n        tables_per_rank: List[List[ShardedEmbeddingTable]] = [\n            [] for i in range(self._world_size)\n        ]\n        for info in sharding_infos:\n            # pyre-fixme [16]\n            shards = info.param_sharding.sharding_spec.shards\n\n            # construct the global sharded_tensor_metadata\n            global_metadata = ShardedTensorMetadata(\n                shards_metadata=shards,\n                size=torch.Size(\n                    [\n                        info.embedding_config.num_embeddings,\n                        info.embedding_config.embedding_dim,\n                    ]\n                ),\n            )\n\n            for rank in range(self._world_size):\n                tables_per_rank[rank].append(\n                    ShardedEmbeddingTable(\n                        num_embeddings=info.embedding_config.num_embeddings,\n                        embedding_dim=info.embedding_config.embedding_dim,\n                        name=info.embedding_config.name,\n                        embedding_names=info.embedding_config.embedding_names,\n                        data_type=info.embedding_config.data_type,\n                        feature_names=info.embedding_config.feature_names,\n                        pooling=info.embedding_config.pooling,\n                        is_weighted=info.embedding_config.is_weighted,\n                        has_feature_processor=info.embedding_config.has_feature_processor,\n                        local_rows=shards[rank].shard_sizes[0],\n                        local_cols=info.embedding_config.embedding_dim,\n                        compute_kernel=EmbeddingComputeKernel(\n                            info.param_sharding.compute_kernel\n                        ),\n                        local_metadata=shards[rank],\n                        global_metadata=global_metadata,\n                        weight_init_max=info.embedding_config.weight_init_max,\n                        weight_init_min=info.embedding_config.weight_init_min,\n                        fused_params=info.fused_params,\n                    )\n                )\n        return tables_per_rank",
  "def embedding_dims(self) -> List[int]:\n        embedding_dims = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_dims.extend(grouped_config.embedding_dims())\n        return embedding_dims",
  "def embedding_names(self) -> List[str]:\n        embedding_names = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_names.extend(grouped_config.embedding_names())\n        return embedding_names",
  "def embedding_names_per_rank(self) -> List[List[str]]:\n        embedding_names = []\n        for grouped_embedding_configs in self._grouped_embedding_configs_per_rank:\n            embedding_names_per_rank = []\n            for grouped_config in grouped_embedding_configs:\n                embedding_names_per_rank.extend(grouped_config.embedding_names())\n            embedding_names.append(embedding_names_per_rank)\n        return embedding_names",
  "def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        embedding_shard_metadata = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_shard_metadata.extend(grouped_config.embedding_shard_metadata())\n        return embedding_shard_metadata",
  "def feature_names(self) -> List[str]:\n        feature_names = []\n        for grouped_config in self._grouped_embedding_configs:\n            feature_names.extend(grouped_config.feature_names())\n        return feature_names",
  "def embedding_tables(self) -> List[ShardedEmbeddingTable]:\n        embedding_tables = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_tables.extend(grouped_config.embedding_tables)\n        return embedding_tables",
  "def _get_num_features(self) -> int:\n        return sum(\n            group_config.num_features()\n            for group_config in self._grouped_embedding_configs\n        )",
  "def _get_feature_hash_sizes(self) -> List[int]:\n        feature_hash_sizes: List[int] = []\n        for group_config in self._grouped_embedding_configs:\n            feature_hash_sizes.extend(group_config.feature_hash_sizes())\n        return feature_hash_sizes",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        num_features: int,\n        feature_hash_sizes: List[int],\n        device: Optional[torch.device] = None,\n        is_sequence: bool = False,\n        has_feature_processor: bool = False,\n        need_pos: bool = False,\n    ) -> None:\n        super().__init__()\n        self._world_size: int = pg.size()\n        self._num_features = num_features\n        feature_block_sizes = [\n            (hash_size + self._world_size - 1) // self._world_size\n            for hash_size in feature_hash_sizes\n        ]\n        self.register_buffer(\n            \"_feature_block_sizes_tensor\",\n            torch.tensor(\n                feature_block_sizes,\n                device=device,\n                dtype=torch.int64,\n            ),\n        )\n        self._dist = KJTAllToAll(\n            pg=pg,\n            splits=[self._num_features] * self._world_size,\n        )\n        self._is_sequence = is_sequence\n        self._has_feature_processor = has_feature_processor\n        self._need_pos = need_pos\n        self.unbucketize_permute_tensor: Optional[torch.Tensor] = None",
  "def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> Awaitable[Awaitable[KeyedJaggedTensor]]:\n        \"\"\"\n        Bucketizes sparse feature values into world size number of buckets and then\n        performs AlltoAll operation.\n\n        Args:\n            sparse_features (KeyedJaggedTensor): sparse features to bucketize and\n                redistribute.\n\n        Returns:\n            Awaitable[Awaitable[KeyedJaggedTensor]]: awaitable of awaitable of KeyedJaggedTensor.\n        \"\"\"\n\n        if sparse_features.variable_stride_per_key():\n            raise ValueError(\n                \"Variable batch per feature is not supported with row-wise sharding\"\n            )\n\n        (\n            bucketized_features,\n            self.unbucketize_permute_tensor,\n        ) = bucketize_kjt_before_all2all(\n            sparse_features,\n            num_buckets=self._world_size,\n            block_sizes=self._feature_block_sizes_tensor,\n            output_permute=self._is_sequence,\n            bucketize_pos=self._has_feature_processor\n            if sparse_features.weights_or_none() is None\n            else self._need_pos,\n        )\n\n        return self._dist(bucketized_features)",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n\n        self._dist = PooledEmbeddingsReduceScatter(\n            pg,\n            codecs=qcomm_codecs_registry.get(\n                CommOp.POOLED_EMBEDDINGS_REDUCE_SCATTER.name, None\n            )\n            if qcomm_codecs_registry\n            else None,\n        )",
  "def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[EmbeddingShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        Performs reduce-scatter pooled operation on pooled embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): pooled embeddings tensor to distribute.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of pooled embeddings tensor.\n        \"\"\"\n\n        if sharding_ctx is None:\n            return self._dist(local_embs)\n        else:\n            return self._dist(local_embs, input_splits=sharding_ctx.batch_size_per_rank)",
  "def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: EmbeddingsAllToOneReduce = EmbeddingsAllToOneReduce(\n            device=device,\n            world_size=world_size,\n            cat_dim=1,\n        )",
  "def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[NullShardingContext] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Performs AlltoOne operation on sequence embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of sequence embeddings.\n        \"\"\"\n\n        return self._dist.forward(\n            local_embs,\n        )",
  "def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        num_features = self._get_num_features()\n        feature_hash_sizes = self._get_feature_hash_sizes()\n        return RwSparseFeaturesDist(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            pg=self._pg,\n            num_features=num_features,\n            feature_hash_sizes=feature_hash_sizes,\n            device=device if device is not None else self._device,\n            is_sequence=False,\n            has_feature_processor=self._has_feature_processor,\n            need_pos=self._need_pos,\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedPooledEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n            feature_processor=feature_processor,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]:\n        return RwPooledEmbeddingDist(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._pg,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def __init__(\n        self,\n        world_size: int,\n        num_features: int,\n        feature_hash_sizes: List[int],\n        device: Optional[torch.device] = None,\n        is_sequence: bool = False,\n        has_feature_processor: bool = False,\n        need_pos: bool = False,\n    ) -> None:\n        super().__init__()\n        self._world_size: int = world_size\n        self._num_features = num_features\n        self.feature_block_sizes: List[int] = [\n            (hash_size + self._world_size - 1) // self._world_size\n            for hash_size in feature_hash_sizes\n        ]\n        self.tensor_cache: Dict[str, torch.Tensor] = {}\n        self._dist = KJTOneToAll(\n            splits=self._world_size * [self._num_features],\n            world_size=world_size,\n            device=device,\n        )\n        self._is_sequence = is_sequence\n        self._has_feature_processor = has_feature_processor\n        self._need_pos = need_pos\n        self.unbucketize_permute_tensor: Optional[torch.Tensor] = None",
  "def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> KJTList:\n        block_sizes = get_block_sizes_runtime_device(\n            self.feature_block_sizes,\n            sparse_features.device(),\n            self.tensor_cache,\n        )\n        (\n            bucketized_features,\n            self.unbucketize_permute_tensor,\n        ) = bucketize_kjt_before_all2all(\n            sparse_features,\n            num_buckets=self._world_size,\n            block_sizes=block_sizes,\n            output_permute=self._is_sequence,\n            bucketize_pos=self._has_feature_processor\n            if sparse_features.weights_or_none() is None\n            else self._need_pos,\n        )\n        return self._dist.forward(bucketized_features)",
  "def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        num_features = self._get_num_features()\n        feature_hash_sizes = self._get_feature_hash_sizes()\n        return InferRwSparseFeaturesDist(\n            world_size=self._world_size,\n            num_features=num_features,\n            feature_hash_sizes=feature_hash_sizes,\n            device=device if device is not None else self._device,\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedPooledEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device if device is not None else self._device,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]:\n        assert device is not None\n        return InferRwPooledEmbeddingDist(\n            device=device,\n            world_size=self._world_size,\n        )",
  "class TwSequenceEmbeddingDist(\n    BaseEmbeddingDist[SequenceShardingContext, torch.Tensor, torch.Tensor]\n):\n    \"\"\"\n    Redistributes sequence embedding tensor in TW fashion with an AlltoAll operation.\n\n    Args:\n        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.\n        features_per_rank (List[int]): number of features (sum of dimensions) of the\n            embedding for each rank.\n        device (Optional[torch.device]): device on which buffers will be allocated.\n    \"\"\"\n\n    def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        features_per_rank: List[int],\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n        self._dist = SequenceEmbeddingsAllToAll(\n            pg,\n            features_per_rank,\n            device,\n            codecs=qcomm_codecs_registry.get(\n                CommOp.SEQUENCE_EMBEDDINGS_ALL_TO_ALL.name, None\n            )\n            if qcomm_codecs_registry\n            else None,\n        )\n\n    def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[SequenceShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        Performs AlltoAll operation on sequence embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n            sharding_ctx (SequenceShardingContext): shared context from KJTAllToAll\n                operation.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of sequence embeddings.\n        \"\"\"\n\n        assert sharding_ctx is not None\n        return self._dist(\n            local_embs,\n            lengths=sharding_ctx.lengths_after_input_dist,\n            input_splits=sharding_ctx.input_splits,\n            output_splits=sharding_ctx.output_splits,\n            batch_size_per_rank=sharding_ctx.batch_size_per_rank,\n            sparse_features_recat=sharding_ctx.sparse_features_recat,\n            unbucketize_permute_tensor=None,\n        )",
  "class TwSequenceEmbeddingSharding(\n    BaseTwEmbeddingSharding[\n        SequenceShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n    ]\n):\n    \"\"\"\n    Shards sequence (unpooled) embedding table-wise, i.e.. a given embedding table is\n    placed entirely on a selected rank.\n    \"\"\"\n\n    def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        return TwSparseFeaturesDist(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._pg,\n            self.features_per_rank(),\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        assert feature_processor is None\n        return GroupedEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[SequenceShardingContext, torch.Tensor, torch.Tensor]:\n        assert self._pg is not None\n        return TwSequenceEmbeddingDist(\n            self._pg,\n            self.features_per_rank(),\n            device if device is not None else self._device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "class InferTwSequenceEmbeddingDist(\n    BaseEmbeddingDist[\n        InferSequenceShardingContext, List[torch.Tensor], List[torch.Tensor]\n    ]\n):\n    \"\"\"\n    Redistributes sequence embedding tensor in hierarchical fashion with an AlltoOne\n    operation.\n\n    Args:\n        device (torch.device): device on which the tensors will be communicated to.\n        world_size (int): number of devices in the topology.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: SeqEmbeddingsAllToOne = SeqEmbeddingsAllToOne(device, world_size)\n\n    def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[InferSequenceShardingContext] = None,\n    ) -> List[torch.Tensor]:\n        \"\"\"\n        Performs AlltoOne operation on sequence embeddings tensor.\n\n        Args:\n            local_embs (List[orch.Tensor]): tensor of values to distribute.\n            sharding_ctx (InferSequenceShardingContext): shared context from KJTAllToOne\n                operation.\n\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of sequence embeddings.\n        \"\"\"\n        return self._dist(local_embs)",
  "class InferTwSequenceEmbeddingSharding(\n    BaseTwEmbeddingSharding[\n        InferSequenceShardingContext,\n        KJTList,\n        List[torch.Tensor],\n        List[torch.Tensor],\n    ]\n):\n    \"\"\"\n    Shards sequence (unpooled) embedding table-wise, i.e.. a given embedding table is\n    placed entirely on a selected rank, for inference.\n    \"\"\"\n\n    def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        return InferTwSparseFeaturesDist(\n            features_per_rank=self.features_per_rank(),\n            world_size=self._world_size,\n            device=device,\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[\n        InferSequenceShardingContext, List[torch.Tensor], List[torch.Tensor]\n    ]:\n        device = device if device is not None else self._device\n        return InferTwSequenceEmbeddingDist(\n            # pyre-fixme [6]\n            device,\n            self._world_size,\n        )",
  "def __init__(\n        self,\n        pg: dist.ProcessGroup,\n        features_per_rank: List[int],\n        device: Optional[torch.device] = None,\n        qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None,\n    ) -> None:\n        super().__init__()\n        self._dist = SequenceEmbeddingsAllToAll(\n            pg,\n            features_per_rank,\n            device,\n            codecs=qcomm_codecs_registry.get(\n                CommOp.SEQUENCE_EMBEDDINGS_ALL_TO_ALL.name, None\n            )\n            if qcomm_codecs_registry\n            else None,\n        )",
  "def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[SequenceShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        Performs AlltoAll operation on sequence embeddings tensor.\n\n        Args:\n            local_embs (torch.Tensor): tensor of values to distribute.\n            sharding_ctx (SequenceShardingContext): shared context from KJTAllToAll\n                operation.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of sequence embeddings.\n        \"\"\"\n\n        assert sharding_ctx is not None\n        return self._dist(\n            local_embs,\n            lengths=sharding_ctx.lengths_after_input_dist,\n            input_splits=sharding_ctx.input_splits,\n            output_splits=sharding_ctx.output_splits,\n            batch_size_per_rank=sharding_ctx.batch_size_per_rank,\n            sparse_features_recat=sharding_ctx.sparse_features_recat,\n            unbucketize_permute_tensor=None,\n        )",
  "def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        return TwSparseFeaturesDist(\n            # pyre-fixme[6]: For 1st param expected `ProcessGroup` but got\n            #  `Optional[ProcessGroup]`.\n            self._pg,\n            self.features_per_rank(),\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        assert feature_processor is None\n        return GroupedEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[SequenceShardingContext, torch.Tensor, torch.Tensor]:\n        assert self._pg is not None\n        return TwSequenceEmbeddingDist(\n            self._pg,\n            self.features_per_rank(),\n            device if device is not None else self._device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: SeqEmbeddingsAllToOne = SeqEmbeddingsAllToOne(device, world_size)",
  "def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[InferSequenceShardingContext] = None,\n    ) -> List[torch.Tensor]:\n        \"\"\"\n        Performs AlltoOne operation on sequence embeddings tensor.\n\n        Args:\n            local_embs (List[orch.Tensor]): tensor of values to distribute.\n            sharding_ctx (InferSequenceShardingContext): shared context from KJTAllToOne\n                operation.\n\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of sequence embeddings.\n        \"\"\"\n        return self._dist(local_embs)",
  "def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        return InferTwSparseFeaturesDist(\n            features_per_rank=self.features_per_rank(),\n            world_size=self._world_size,\n            device=device,\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[\n        InferSequenceShardingContext, List[torch.Tensor], List[torch.Tensor]\n    ]:\n        device = device if device is not None else self._device\n        return InferTwSequenceEmbeddingDist(\n            # pyre-fixme [6]\n            device,\n            self._world_size,\n        )",
  "class CwSequenceEmbeddingSharding(\n    BaseCwEmbeddingSharding[\n        SequenceShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n    ]\n):\n    \"\"\"\n    Shards sequence (unpooled) embeddings column-wise, i.e.. a given embedding is\n    partitioned along its columns and placed on specified ranks.\n    \"\"\"\n\n    def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        assert self._pg is not None\n        return TwSparseFeaturesDist(\n            self._pg,\n            self.features_per_rank(),\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        assert feature_processor is None\n        return GroupedEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[SequenceShardingContext, torch.Tensor, torch.Tensor]:\n        assert self._pg is not None\n        return TwSequenceEmbeddingDist(\n            self._pg,\n            self.features_per_rank(),\n            device if device is not None else self._device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "class InferCwSequenceEmbeddingSharding(\n    BaseCwEmbeddingSharding[\n        InferSequenceShardingContext, KJTList, List[torch.Tensor], List[torch.Tensor]\n    ]\n):\n    def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        return InferTwSparseFeaturesDist(\n            features_per_rank=self.features_per_rank(),\n            world_size=self._world_size,\n            device=device if device is not None else self._device,\n        )\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device if device is not None else self._device,\n        )\n\n    def create_output_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseEmbeddingDist[\n        InferSequenceShardingContext, List[torch.Tensor], List[torch.Tensor]\n    ]:\n        device = device if device is not None else self._device\n        assert device is not None\n\n        dist_out = InferCwSequenceEmbeddingDist(\n            device,\n            self._world_size,\n        )\n        return dist_out",
  "class InferCwSequenceEmbeddingDist(\n    BaseEmbeddingDist[\n        InferSequenceShardingContext, List[torch.Tensor], List[torch.Tensor]\n    ]\n):\n    def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: SeqEmbeddingsAllToOne = SeqEmbeddingsAllToOne(\n            device=device, world_size=world_size\n        )\n\n    def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[InferSequenceShardingContext] = None,\n    ) -> List[torch.Tensor]:\n        return self._dist.forward(local_embs)",
  "def create_input_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        assert self._pg is not None\n        return TwSparseFeaturesDist(\n            self._pg,\n            self.features_per_rank(),\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        assert feature_processor is None\n        return GroupedEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._pg,\n            device=device if device is not None else self._device,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[SequenceShardingContext, torch.Tensor, torch.Tensor]:\n        assert self._pg is not None\n        return TwSequenceEmbeddingDist(\n            self._pg,\n            self.features_per_rank(),\n            device if device is not None else self._device,\n            qcomm_codecs_registry=self.qcomm_codecs_registry,\n        )",
  "def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KJTList]:\n        return InferTwSparseFeaturesDist(\n            features_per_rank=self.features_per_rank(),\n            world_size=self._world_size,\n            device=device if device is not None else self._device,\n        )",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:\n        return InferGroupedEmbeddingsLookup(\n            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,\n            world_size=self._world_size,\n            fused_params=fused_params,\n            device=device if device is not None else self._device,\n        )",
  "def create_output_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseEmbeddingDist[\n        InferSequenceShardingContext, List[torch.Tensor], List[torch.Tensor]\n    ]:\n        device = device if device is not None else self._device\n        assert device is not None\n\n        dist_out = InferCwSequenceEmbeddingDist(\n            device,\n            self._world_size,\n        )\n        return dist_out",
  "def __init__(\n        self,\n        device: torch.device,\n        world_size: int,\n    ) -> None:\n        super().__init__()\n        self._dist: SeqEmbeddingsAllToOne = SeqEmbeddingsAllToOne(\n            device=device, world_size=world_size\n        )",
  "def forward(\n        self,\n        local_embs: List[torch.Tensor],\n        sharding_ctx: Optional[InferSequenceShardingContext] = None,\n    ) -> List[torch.Tensor]:\n        return self._dist.forward(local_embs)",
  "class BaseDpEmbeddingSharding(EmbeddingSharding[C, F, T, W]):\n    \"\"\"\n    Base class for data-parallel sharding.\n    \"\"\"\n\n    def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._env = env\n        self._device = device\n        self._rank: int = self._env.rank\n        self._world_size: int = self._env.world_size\n        sharded_tables_per_rank = self._shard(sharding_infos)\n        self._grouped_embedding_configs_per_rank: List[\n            List[GroupedEmbeddingConfig]\n        ] = []\n        self._grouped_embedding_configs_per_rank = group_tables(sharded_tables_per_rank)\n        self._grouped_embedding_configs: List[\n            GroupedEmbeddingConfig\n        ] = self._grouped_embedding_configs_per_rank[env.rank]\n\n    def _shard(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n    ) -> List[List[ShardedEmbeddingTable]]:\n        world_size = self._world_size\n        tables_per_rank: List[List[ShardedEmbeddingTable]] = [\n            [] for i in range(world_size)\n        ]\n        for info in sharding_infos:\n            for rank in range(world_size):\n                tables_per_rank[rank].append(\n                    ShardedEmbeddingTable(\n                        num_embeddings=info.embedding_config.num_embeddings,\n                        embedding_dim=info.embedding_config.embedding_dim,\n                        name=info.embedding_config.name,\n                        embedding_names=info.embedding_config.embedding_names,\n                        data_type=info.embedding_config.data_type,\n                        feature_names=info.embedding_config.feature_names,\n                        pooling=info.embedding_config.pooling,\n                        is_weighted=info.embedding_config.is_weighted,\n                        has_feature_processor=info.embedding_config.has_feature_processor,\n                        local_rows=info.param.size(0),\n                        local_cols=info.param.size(1),\n                        compute_kernel=EmbeddingComputeKernel(\n                            info.param_sharding.compute_kernel\n                        ),\n                        local_metadata=None,\n                        global_metadata=None,\n                        weight_init_max=info.embedding_config.weight_init_max,\n                        weight_init_min=info.embedding_config.weight_init_min,\n                        fused_params=info.fused_params,\n                    )\n                )\n        return tables_per_rank\n\n    def embedding_dims(self) -> List[int]:\n        embedding_dims = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_dims.extend(grouped_config.embedding_dims())\n        return embedding_dims\n\n    def embedding_names(self) -> List[str]:\n        embedding_names = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_names.extend(grouped_config.embedding_names())\n        return embedding_names\n\n    def embedding_names_per_rank(self) -> List[List[str]]:\n        raise NotImplementedError\n\n    def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        embedding_shard_metadata = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_shard_metadata.extend(grouped_config.embedding_shard_metadata())\n        return embedding_shard_metadata\n\n    def feature_names(self) -> List[str]:\n        feature_names = []\n        for grouped_config in self._grouped_embedding_configs:\n            feature_names.extend(grouped_config.feature_names())\n        return feature_names\n\n    def embedding_tables(self) -> List[ShardedEmbeddingTable]:\n        embedding_tables = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_tables.extend(grouped_config.embedding_tables)\n        return embedding_tables",
  "class DpSparseFeaturesDist(BaseSparseFeaturesDist[KeyedJaggedTensor]):\n    \"\"\"\n    Distributes sparse features (input) to be data-parallel.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> Awaitable[Awaitable[KeyedJaggedTensor]]:\n        \"\"\"\n        No-op as sparse features are already distributed in data-parallel fashion.\n\n        Args:\n            sparse_features (SparseFeatures): input sparse features.\n\n        Returns:\n            Awaitable[Awaitable[SparseFeatures]]: awaitable of awaitable of SparseFeatures.\n        \"\"\"\n\n        if sparse_features.variable_stride_per_key():\n            raise ValueError(\n                \"Dense TBE kernel does not support variable batch per feature\"\n            )\n        return NoWait(cast(Awaitable[KeyedJaggedTensor], NoWait(sparse_features)))",
  "class DpPooledEmbeddingDist(\n    BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]\n):\n    \"\"\"\n    Distributes pooled embeddings to be data-parallel.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[EmbeddingShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        No-op as pooled embeddings are already distributed in data-parallel fashion.\n\n        Args:\n            local_embs (torch.Tensor): output sequence embeddings.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of pooled embeddings tensor.\n        \"\"\"\n\n        return NoWait(local_embs)",
  "class DpPooledEmbeddingSharding(\n    BaseDpEmbeddingSharding[\n        EmbeddingShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor\n    ]\n):\n    \"\"\"\n    Shards embedding bags data-parallel, with no table sharding i.e.. a given embedding\n    table is replicated across all ranks.\n    \"\"\"\n\n    def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        return DpSparseFeaturesDist()\n\n    def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedPooledEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._env.process_group,\n            device=device if device is not None else self._device,\n            feature_processor=feature_processor,\n            # For data parallel we need to turn always gradient scaling in for weights\n            # because get_gradient_scaling from comm_ops only affects model_parallel tables, not DP\n            scale_weight_gradients=False,\n        )\n\n    def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]:\n        return DpPooledEmbeddingDist()",
  "def __init__(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n        env: ShardingEnv,\n        device: Optional[torch.device] = None,\n    ) -> None:\n        super().__init__()\n        self._env = env\n        self._device = device\n        self._rank: int = self._env.rank\n        self._world_size: int = self._env.world_size\n        sharded_tables_per_rank = self._shard(sharding_infos)\n        self._grouped_embedding_configs_per_rank: List[\n            List[GroupedEmbeddingConfig]\n        ] = []\n        self._grouped_embedding_configs_per_rank = group_tables(sharded_tables_per_rank)\n        self._grouped_embedding_configs: List[\n            GroupedEmbeddingConfig\n        ] = self._grouped_embedding_configs_per_rank[env.rank]",
  "def _shard(\n        self,\n        sharding_infos: List[EmbeddingShardingInfo],\n    ) -> List[List[ShardedEmbeddingTable]]:\n        world_size = self._world_size\n        tables_per_rank: List[List[ShardedEmbeddingTable]] = [\n            [] for i in range(world_size)\n        ]\n        for info in sharding_infos:\n            for rank in range(world_size):\n                tables_per_rank[rank].append(\n                    ShardedEmbeddingTable(\n                        num_embeddings=info.embedding_config.num_embeddings,\n                        embedding_dim=info.embedding_config.embedding_dim,\n                        name=info.embedding_config.name,\n                        embedding_names=info.embedding_config.embedding_names,\n                        data_type=info.embedding_config.data_type,\n                        feature_names=info.embedding_config.feature_names,\n                        pooling=info.embedding_config.pooling,\n                        is_weighted=info.embedding_config.is_weighted,\n                        has_feature_processor=info.embedding_config.has_feature_processor,\n                        local_rows=info.param.size(0),\n                        local_cols=info.param.size(1),\n                        compute_kernel=EmbeddingComputeKernel(\n                            info.param_sharding.compute_kernel\n                        ),\n                        local_metadata=None,\n                        global_metadata=None,\n                        weight_init_max=info.embedding_config.weight_init_max,\n                        weight_init_min=info.embedding_config.weight_init_min,\n                        fused_params=info.fused_params,\n                    )\n                )\n        return tables_per_rank",
  "def embedding_dims(self) -> List[int]:\n        embedding_dims = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_dims.extend(grouped_config.embedding_dims())\n        return embedding_dims",
  "def embedding_names(self) -> List[str]:\n        embedding_names = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_names.extend(grouped_config.embedding_names())\n        return embedding_names",
  "def embedding_names_per_rank(self) -> List[List[str]]:\n        raise NotImplementedError",
  "def embedding_shard_metadata(self) -> List[Optional[ShardMetadata]]:\n        embedding_shard_metadata = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_shard_metadata.extend(grouped_config.embedding_shard_metadata())\n        return embedding_shard_metadata",
  "def feature_names(self) -> List[str]:\n        feature_names = []\n        for grouped_config in self._grouped_embedding_configs:\n            feature_names.extend(grouped_config.feature_names())\n        return feature_names",
  "def embedding_tables(self) -> List[ShardedEmbeddingTable]:\n        embedding_tables = []\n        for grouped_config in self._grouped_embedding_configs:\n            embedding_tables.extend(grouped_config.embedding_tables)\n        return embedding_tables",
  "def __init__(self) -> None:\n        super().__init__()",
  "def forward(\n        self,\n        sparse_features: KeyedJaggedTensor,\n    ) -> Awaitable[Awaitable[KeyedJaggedTensor]]:\n        \"\"\"\n        No-op as sparse features are already distributed in data-parallel fashion.\n\n        Args:\n            sparse_features (SparseFeatures): input sparse features.\n\n        Returns:\n            Awaitable[Awaitable[SparseFeatures]]: awaitable of awaitable of SparseFeatures.\n        \"\"\"\n\n        if sparse_features.variable_stride_per_key():\n            raise ValueError(\n                \"Dense TBE kernel does not support variable batch per feature\"\n            )\n        return NoWait(cast(Awaitable[KeyedJaggedTensor], NoWait(sparse_features)))",
  "def __init__(self) -> None:\n        super().__init__()",
  "def forward(\n        self,\n        local_embs: torch.Tensor,\n        sharding_ctx: Optional[EmbeddingShardingContext] = None,\n    ) -> Awaitable[torch.Tensor]:\n        \"\"\"\n        No-op as pooled embeddings are already distributed in data-parallel fashion.\n\n        Args:\n            local_embs (torch.Tensor): output sequence embeddings.\n\n        Returns:\n            Awaitable[torch.Tensor]: awaitable of pooled embeddings tensor.\n        \"\"\"\n\n        return NoWait(local_embs)",
  "def create_input_dist(\n        self, device: Optional[torch.device] = None\n    ) -> BaseSparseFeaturesDist[KeyedJaggedTensor]:\n        return DpSparseFeaturesDist()",
  "def create_lookup(\n        self,\n        device: Optional[torch.device] = None,\n        fused_params: Optional[Dict[str, Any]] = None,\n        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,\n    ) -> BaseEmbeddingLookup:\n        return GroupedPooledEmbeddingsLookup(\n            grouped_configs=self._grouped_embedding_configs,\n            pg=self._env.process_group,\n            device=device if device is not None else self._device,\n            feature_processor=feature_processor,\n            # For data parallel we need to turn always gradient scaling in for weights\n            # because get_gradient_scaling from comm_ops only affects model_parallel tables, not DP\n            scale_weight_gradients=False,\n        )",
  "def create_output_dist(\n        self,\n        device: Optional[torch.device] = None,\n    ) -> BaseEmbeddingDist[EmbeddingShardingContext, torch.Tensor, torch.Tensor]:\n        return DpPooledEmbeddingDist()",
  "def print_error_message(\n    python_path: str, node: ast.AST, name: str, message: str, severity: str = \"warning\"\n) -> None:\n    \"\"\"\n    This function will print linter error in a format that is compatible with\n    our internal tools.\n\n    Args:\n        python_path: Path to the file with the error\n        node: AST node describing snippet of code\n        name: Name of the linter error\n        message: Error message to show to user\n\n    Optional Args:\n        severity: How severe should be considered the error. Default level: 'error'\n\n    Returns:\n        None\n    \"\"\"\n    lint_item = {\n        \"path\": python_path,\n        \"line\": node.lineno,\n        \"char\": node.col_offset + 1,\n        \"severity\": severity,\n        \"name\": name,\n        \"description\": message,\n    }\n    print(json.dumps(lint_item))",
  "def get_function_args(node: ast.FunctionDef) -> Tuple[List[Any], List[Any]]:\n    \"\"\"\n    This functon will process function definition and will extract all\n    arguments used by a given function and return all optional and non-optional\n    args used by the function.\n\n    Args:\n        node: Function node containing function that needs to be analyzed\n\n    Returns:\n        (non_optional_args, optional_args): named function args\n    \"\"\"\n    assert (\n        type(node) == ast.FunctionDef\n    ), \"Incorrect node type. Expected ast.FunctionDef, got {}\".format(type(node))\n    total_args = len(node.args.args)\n    default_args = len(node.args.defaults)\n\n    optional_args = []\n    non_optional_args = []\n    # Handle positional args\n    for i in range(total_args):\n        if i + default_args < total_args:\n            non_optional_args.append(node.args.args[i].arg)\n        else:\n            optional_args.append(node.args.args[i].arg)\n\n    # Handle named args\n    for arg in node.args.kwonlyargs:\n        optional_args.append(arg.arg)\n\n    return non_optional_args, optional_args",
  "def check_class_definition(python_path: str, node: ast.ClassDef) -> None:\n    \"\"\"\n    This function will run set of sanity checks against class definitions\n    and their docstrings.\n\n    Args:\n        python_path: Path to the file that is getting checked\n        node: AST node with the ClassDef that needs to be checked\n\n    Returns:\n        None\n    \"\"\"\n    assert (\n        type(node) == ast.ClassDef\n    ), \"Received invalid node type. Expected ClassDef, got: {}\".format(type(node))\n\n    is_TorchRec_module = False\n    is_test_file = \"tests\" in python_path\n    for base in node.bases:\n        # For now only names and attributes are supported\n        if type(base) != ast.Name and type(base) != ast.Attribute:  # pragma: nocover\n            continue\n\n        # We assume that TorchRec module has one of the following inheritance patterns:\n        # 1. `class SomeTorchRecModule(LazyModuleExtensionMixin, torch.nn.Module)`\n        # 2. `class SomeTorchRecModule(torch.nn.Module)`\n        # pyre-ignore[16]: `_ast.expr` has no attribute `id`.\n        if hasattr(base, \"id\") and base.id == \"LazyModuleExtensionMixin\":\n            is_TorchRec_module = True\n            break\n        # pyre-ignore[16]: `_ast.expr` has no attribute `id`.\n        elif hasattr(base, \"attr\") and base.attr == \"Module\":\n            is_TorchRec_module = True\n            break\n\n    if not is_TorchRec_module or is_test_file:\n        return\n\n    docstring: Optional[str] = ast.get_docstring(node)\n    if docstring is None:\n        print_error_message(\n            python_path,\n            node,\n            \"No docstring found in a TorchRec module\",\n            \"TorchRec modules are required to have a docstring describing how \"\n            \"to use them. Given Module don't have a docstring, please fix this.\",\n        )\n        return\n\n    # Check presence of the example:\n    if \"Example:\" not in docstring:\n        print_error_message(\n            python_path,\n            node,\n            \"No runnable example in a TorchRec module\",\n            \"TorchRec modules are required to have runnable examples in \"\n            '\"Example:\" section. Please fix the docstring',\n        )\n\n    # Check correctness of the Args for a class definition:\n    required_keywords = [\"Args:\"]\n    missing_keywords = []\n    for keyword in required_keywords:\n        if keyword not in docstring:\n            missing_keywords.append(keyword)\n\n    if len(missing_keywords) > 0:\n        print_error_message(\n            python_path,\n            node,\n            \"Missing required keywords from TorchRec module\",\n            \"TorchRec modules are required to description of their args and \"\n            'results in \"Args:\". '\n            \"Missing keywords: {}.\".format(missing_keywords),\n        )\n\n    # Check actual args from the functions\n    # pyre-ignore[33]: Explicit annotation for `functions` cannot contain `Any`.\n    functions: Dict[str, Tuple[List[Any], List[Any]]] = {}\n    function_sub_nodes = {}\n    for sub_node in node.body:\n        if type(sub_node) == ast.FunctionDef:\n            assert isinstance(sub_node, ast.FunctionDef)\n            functions[sub_node.name] = get_function_args(sub_node)\n            function_sub_nodes[sub_node.name] = sub_node\n\n    def check_function(function_name: str) -> None:\n        if function_name not in functions:\n            return\n\n        if function_name == \"__init__\":\n            # NOTE: -1 to not count the `self` argument.\n            num_args = sum([len(args) for args in functions[function_name]]) - 1\n            if num_args > MAX_NUM_ARGS_IN_MODULE_CTOR:\n                print_error_message(\n                    python_path,\n                    node,\n                    \"TorchRec module has too many constructor arguments\",\n                    \"TorchRec module can have at most {} constructor arguments, but this module has {}.\".format(\n                        MAX_NUM_ARGS_IN_MODULE_CTOR,\n                        num_args,\n                    ),\n                )\n        if function_name in functions:\n            missing_required_args = []\n            missing_optional_args = []\n            for arg in functions[function_name][0]:\n                # Ignore checks for required self and net args\n                if arg == \"self\" or arg == \"net\":\n                    continue\n                assert docstring is not None\n                if arg not in docstring:\n                    missing_required_args.append(arg)\n            for arg in functions[function_name][1]:\n                assert docstring is not None\n                if arg not in docstring:\n                    missing_optional_args.append(arg)\n            if len(missing_required_args) > 0 or len(missing_optional_args) > 0:\n                print_error_message(\n                    python_path,\n                    node,\n                    \"Missing docstring descriptions for {} function arguments.\".format(\n                        function_name\n                    ),\n                    (\n                        \"Missing descriptions for {} function arguments. \"\n                        \"Missing required args: {}, missing optional args: {}\"\n                    ).format(\n                        function_name,\n                        missing_required_args,\n                        missing_optional_args,\n                    ),\n                )\n\n    # pyre-ignore[53]\n    def check_function_docstring(function_name: str) -> None:\n        if function_name not in functions:\n            return\n\n        function_docstring: Optional[str] = None\n        function_docstring = ast.get_docstring(function_sub_nodes[function_name])\n        if function_docstring is None:\n            print_error_message(\n                python_path,\n                node,\n                \"Missing docstring for {} function\".format(function_name),\n                \"Missing docstring for {} function\".format(function_name),\n            )\n            return\n\n        missing_required_args = []\n        missing_optional_args = []\n        for arg in functions[function_name][0]:\n            # Ignore checks for required self and net args\n            if arg == \"self\" or arg == \"net\":\n                continue\n            assert function_docstring is not None\n            if arg not in function_docstring:\n                missing_required_args.append(arg)\n        for arg in functions[function_name][1]:\n            assert function_docstring is not None\n            if arg not in function_docstring:\n                missing_optional_args.append(arg)\n        if len(missing_required_args) > 0 or len(missing_optional_args) > 0:\n            print_error_message(\n                python_path,\n                node,\n                \"Missing docstring descriptions for {} function arguments.\".format(\n                    function_name\n                ),\n                (\n                    \"Missing descriptions for {} function arguments. \"\n                    \"Missing required args: {}, missing optional args: {}\"\n                ).format(\n                    function_name,\n                    missing_required_args,\n                    missing_optional_args,\n                ),\n            )\n        assert function_docstring is not None\n        if \"Returns:\" not in function_docstring:\n            print_error_message(\n                python_path,\n                node,\n                \"Missing docstring descriptions for {} function arguments.\".format(\n                    function_name\n                ),\n                (\n                    \"Missing descriptions for {} function arguments. \"\n                    \"Missing Returns section\"\n                ).format(\n                    function_name,\n                ),\n            )\n\n    check_function(\"__init__\")\n    check_function_docstring(\"forward\")",
  "def read_file(path: str) -> str:  # pragma: nocover\n    \"\"\"\n    This function simply reads contents of the file. It's moved out to a function\n    purely to simplify testing process.\n\n    Args:\n        path: File to read.\n\n    Returns:\n        content(str): Content of given file.\n    \"\"\"\n    return open(path).read()",
  "def linter_one_file(python_path: str) -> None:\n    \"\"\"\n    This function will check all Modules defined in the given file for a valid\n    documentation based on the AST.\n\n    Input args:\n        python_path: Path to the file that need to be verified with the linter.\n\n    Returns:\n        None\n    \"\"\"\n    python_path = python_path.strip()\n    try:\n        for node in ast.parse(read_file(python_path)).body:\n            if type(node) == ast.ClassDef:\n                assert isinstance(node, ast.ClassDef)\n                check_class_definition(python_path, node)\n    except SyntaxError as e:  # pragma: nocover\n        # possible failing due to file parsing error\n        lint_item = {\n            \"path\": python_path,\n            \"line\": e.lineno,\n            \"char\": e.offset,\n            \"severity\": \"warning\",\n            \"name\": \"syntax-error\",\n            \"description\": (\n                f\"There is a linter parser error with message: {e.msg}. \"\n                \"Please report the diff to torchrec oncall\"\n            ),\n            \"bypassChangedLineFiltering\": True,\n        }\n        print(json.dumps(lint_item))",
  "def _make_argparse() -> ArgumentParser:  # pragma: nocover\n    parser = ArgumentParser(\n        description=\"TorchRec docstring linter\", fromfile_prefix_chars=\"@\"\n    )\n    parser.add_argument(\"source_files\", nargs=\"+\", help=\"Path to python source files\")\n\n    return parser",
  "def _parse_args() -> Namespace:  # pragma: nocover\n    ap = _make_argparse()\n    return ap.parse_args()",
  "def check_function(function_name: str) -> None:\n        if function_name not in functions:\n            return\n\n        if function_name == \"__init__\":\n            # NOTE: -1 to not count the `self` argument.\n            num_args = sum([len(args) for args in functions[function_name]]) - 1\n            if num_args > MAX_NUM_ARGS_IN_MODULE_CTOR:\n                print_error_message(\n                    python_path,\n                    node,\n                    \"TorchRec module has too many constructor arguments\",\n                    \"TorchRec module can have at most {} constructor arguments, but this module has {}.\".format(\n                        MAX_NUM_ARGS_IN_MODULE_CTOR,\n                        num_args,\n                    ),\n                )\n        if function_name in functions:\n            missing_required_args = []\n            missing_optional_args = []\n            for arg in functions[function_name][0]:\n                # Ignore checks for required self and net args\n                if arg == \"self\" or arg == \"net\":\n                    continue\n                assert docstring is not None\n                if arg not in docstring:\n                    missing_required_args.append(arg)\n            for arg in functions[function_name][1]:\n                assert docstring is not None\n                if arg not in docstring:\n                    missing_optional_args.append(arg)\n            if len(missing_required_args) > 0 or len(missing_optional_args) > 0:\n                print_error_message(\n                    python_path,\n                    node,\n                    \"Missing docstring descriptions for {} function arguments.\".format(\n                        function_name\n                    ),\n                    (\n                        \"Missing descriptions for {} function arguments. \"\n                        \"Missing required args: {}, missing optional args: {}\"\n                    ).format(\n                        function_name,\n                        missing_required_args,\n                        missing_optional_args,\n                    ),\n                )",
  "def check_function_docstring(function_name: str) -> None:\n        if function_name not in functions:\n            return\n\n        function_docstring: Optional[str] = None\n        function_docstring = ast.get_docstring(function_sub_nodes[function_name])\n        if function_docstring is None:\n            print_error_message(\n                python_path,\n                node,\n                \"Missing docstring for {} function\".format(function_name),\n                \"Missing docstring for {} function\".format(function_name),\n            )\n            return\n\n        missing_required_args = []\n        missing_optional_args = []\n        for arg in functions[function_name][0]:\n            # Ignore checks for required self and net args\n            if arg == \"self\" or arg == \"net\":\n                continue\n            assert function_docstring is not None\n            if arg not in function_docstring:\n                missing_required_args.append(arg)\n        for arg in functions[function_name][1]:\n            assert function_docstring is not None\n            if arg not in function_docstring:\n                missing_optional_args.append(arg)\n        if len(missing_required_args) > 0 or len(missing_optional_args) > 0:\n            print_error_message(\n                python_path,\n                node,\n                \"Missing docstring descriptions for {} function arguments.\".format(\n                    function_name\n                ),\n                (\n                    \"Missing descriptions for {} function arguments. \"\n                    \"Missing required args: {}, missing optional args: {}\"\n                ).format(\n                    function_name,\n                    missing_required_args,\n                    missing_optional_args,\n                ),\n            )\n        assert function_docstring is not None\n        if \"Returns:\" not in function_docstring:\n            print_error_message(\n                python_path,\n                node,\n                \"Missing docstring descriptions for {} function arguments.\".format(\n                    function_name\n                ),\n                (\n                    \"Missing descriptions for {} function arguments. \"\n                    \"Missing Returns section\"\n                ).format(\n                    function_name,\n                ),\n            )",
  "class LintSeverity(str, Enum):\n    ERROR = \"error\"\n    WARNING = \"warning\"\n    ADVICE = \"advice\"\n    DISABLED = \"disabled\"",
  "class LintMessage(NamedTuple):\n    path: Optional[str]\n    line: Optional[int]\n    char: Optional[int]\n    code: str\n    severity: LintSeverity\n    name: str\n    original: Optional[str]\n    replacement: Optional[str]\n    description: Optional[str]",
  "def as_posix(name: str) -> str:\n    return name.replace(\"\\\\\", \"/\") if IS_WINDOWS else name",
  "def _run_command(\n    args: List[str],\n    *,\n    stdin: BinaryIO,\n    timeout: int,\n) -> \"subprocess.CompletedProcess[bytes]\":\n    logging.debug(\"$ %s\", \" \".join(args))\n    start_time = time.monotonic()\n    try:\n        return subprocess.run(\n            args,\n            stdin=stdin,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            shell=IS_WINDOWS,  # So batch scripts are found.\n            timeout=timeout,\n            check=True,\n        )\n    finally:\n        end_time = time.monotonic()\n        logging.debug(\"took %dms\", (end_time - start_time) * 1000)",
  "def run_command(\n    args: List[str],\n    *,\n    stdin: BinaryIO,\n    retries: int,\n    timeout: int,\n) -> \"subprocess.CompletedProcess[bytes]\":\n    remaining_retries = retries\n    while True:\n        try:\n            return _run_command(args, stdin=stdin, timeout=timeout)\n        except subprocess.TimeoutExpired as err:\n            if remaining_retries == 0:\n                raise err\n            remaining_retries -= 1\n            logging.warning(\n                \"(%s/%s) Retrying because command failed with: %r\",\n                retries - remaining_retries,\n                retries,\n                err,\n            )\n            time.sleep(1)",
  "def check_file(\n    filename: str,\n    retries: int,\n    timeout: int,\n) -> List[LintMessage]:\n    try:\n        with open(filename, \"rb\") as f:\n            original = f.read()\n        with open(filename, \"rb\") as f:\n            proc = run_command(\n                [sys.executable, \"-mblack\", \"--stdin-filename\", filename, \"-\"],\n                stdin=f,\n                retries=retries,\n                timeout=timeout,\n            )\n    except subprocess.TimeoutExpired:\n        return [\n            LintMessage(\n                path=filename,\n                line=None,\n                char=None,\n                code=\"BLACK\",\n                severity=LintSeverity.ERROR,\n                name=\"timeout\",\n                original=None,\n                replacement=None,\n                description=(\n                    \"black timed out while trying to process a file. \"\n                    \"Please report an issue in pytorch/torchrec.\"\n                ),\n            )\n        ]\n    except (OSError, subprocess.CalledProcessError) as err:\n        return [\n            LintMessage(\n                path=filename,\n                line=None,\n                char=None,\n                code=\"BLACK\",\n                severity=LintSeverity.ADVICE,\n                name=\"command-failed\",\n                original=None,\n                replacement=None,\n                description=(\n                    f\"Failed due to {err.__class__.__name__}:\\n{err}\"\n                    if not isinstance(err, subprocess.CalledProcessError)\n                    else (\n                        \"COMMAND (exit code {returncode})\\n\"\n                        \"{command}\\n\\n\"\n                        \"STDERR\\n{stderr}\\n\\n\"\n                        \"STDOUT\\n{stdout}\"\n                    ).format(\n                        returncode=err.returncode,\n                        command=\" \".join(as_posix(x) for x in err.cmd),\n                        stderr=err.stderr.decode(\"utf-8\").strip() or \"(empty)\",\n                        stdout=err.stdout.decode(\"utf-8\").strip() or \"(empty)\",\n                    )\n                ),\n            )\n        ]\n\n    replacement = proc.stdout\n    if original == replacement:\n        return []\n\n    return [\n        LintMessage(\n            path=filename,\n            line=None,\n            char=None,\n            code=\"BLACK\",\n            severity=LintSeverity.WARNING,\n            name=\"format\",\n            original=original.decode(\"utf-8\"),\n            replacement=replacement.decode(\"utf-8\"),\n            description=\"Run `lintrunner -a` to apply this patch.\",\n        )\n    ]",
  "def main() -> None:\n    parser = argparse.ArgumentParser(\n        description=\"Format files with black.\",\n        fromfile_prefix_chars=\"@\",\n    )\n    parser.add_argument(\n        \"--retries\",\n        default=3,\n        type=int,\n        help=\"times to retry timed out black\",\n    )\n    parser.add_argument(\n        \"--timeout\",\n        default=90,\n        type=int,\n        help=\"seconds to wait for black\",\n    )\n    parser.add_argument(\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"verbose logging\",\n    )\n    parser.add_argument(\n        \"filenames\",\n        nargs=\"+\",\n        help=\"paths to lint\",\n    )\n    args = parser.parse_args()\n\n    logging.basicConfig(\n        format=\"<%(threadName)s:%(levelname)s> %(message)s\",\n        level=logging.NOTSET\n        if args.verbose\n        else logging.DEBUG\n        if len(args.filenames) < 1000\n        else logging.INFO,\n        stream=sys.stderr,\n    )\n\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=os.cpu_count(),\n        thread_name_prefix=\"Thread\",\n    ) as executor:\n        futures = {\n            executor.submit(check_file, filename, args.retries, args.timeout): filename\n            for filename in args.filenames\n        }\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                for lint_message in future.result():\n                    print(json.dumps(lint_message._asdict()), flush=True)\n            except Exception:\n                logging.critical('Failed at \"%s\".', futures[future])\n                raise",
  "def run_command(args: List[str]) -> \"subprocess.CompletedProcess[bytes]\":\n    logging.debug(\"$ %s\", \" \".join(args))\n    start_time = time.monotonic()\n    try:\n        return subprocess.run(args, check=True)\n    finally:\n        end_time = time.monotonic()\n        logging.debug(\"took %dms\", (end_time - start_time) * 1000)",
  "def check_file(\n    filename: str,\n) -> List[LintMessage]:\n    try:\n        top_of_file_cat = usort_config.Category(\"top_of_file\")\n        known = usort_config.known_factory()\n        # cinder magic imports must be on top (after future imports)\n        known[\"__strict__\"] = top_of_file_cat\n        known[\"__static__\"] = top_of_file_cat\n\n        config = usort_config.Config(\n            categories=(\n                (\n                    usort_config.CAT_FUTURE,\n                    top_of_file_cat,\n                    usort_config.CAT_STANDARD_LIBRARY,\n                    usort_config.CAT_THIRD_PARTY,\n                    usort_config.CAT_FIRST_PARTY,\n                )\n            ),\n            known=known,\n        )\n\n        with open(filename, mode=\"rb\") as f:\n            original = f.read()\n            result = usort(original, config)\n            if result.error:\n                raise result.error\n\n    except subprocess.TimeoutExpired:\n        return [\n            LintMessage(\n                path=filename,\n                line=None,\n                char=None,\n                code=\"USORT\",\n                severity=LintSeverity.ERROR,\n                name=\"timeout\",\n                original=None,\n                replacement=None,\n                description=(\n                    \"usort timed out while trying to process a file. \"\n                    \"Please report an issue in pytorch/torchrec.\"\n                ),\n            )\n        ]\n    except (OSError, subprocess.CalledProcessError) as err:\n        return [\n            LintMessage(\n                path=filename,\n                line=None,\n                char=None,\n                code=\"USORT\",\n                severity=LintSeverity.ADVICE,\n                name=\"command-failed\",\n                original=None,\n                replacement=None,\n                description=(\n                    f\"Failed due to {err.__class__.__name__}:\\n{err}\"\n                    if not isinstance(err, subprocess.CalledProcessError)\n                    else (\n                        \"COMMAND (exit code {returncode})\\n\"\n                        \"{command}\\n\\n\"\n                        \"STDERR\\n{stderr}\\n\\n\"\n                        \"STDOUT\\n{stdout}\"\n                    ).format(\n                        returncode=err.returncode,\n                        command=\" \".join(as_posix(x) for x in err.cmd),\n                        stderr=err.stderr.decode(\"utf-8\").strip() or \"(empty)\",\n                        stdout=err.stdout.decode(\"utf-8\").strip() or \"(empty)\",\n                    )\n                ),\n            )\n        ]\n\n    replacement = result.output\n    if original == replacement:\n        return []\n\n    return [\n        LintMessage(\n            path=filename,\n            line=None,\n            char=None,\n            code=\"USORT\",\n            severity=LintSeverity.WARNING,\n            name=\"format\",\n            original=original.decode(\"utf-8\"),\n            replacement=replacement.decode(\"utf-8\"),\n            description=\"Run `lintrunner -a` to apply this patch.\",\n        )\n    ]",
  "def main() -> None:\n    parser = argparse.ArgumentParser(\n        description=\"Format files with usort.\",\n        fromfile_prefix_chars=\"@\",\n    )\n    parser.add_argument(\n        \"filenames\",\n        nargs=\"+\",\n        help=\"paths to lint\",\n    )\n    args = parser.parse_args()\n\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=os.cpu_count(),\n        thread_name_prefix=\"Thread\",\n    ) as executor:\n        futures = {\n            executor.submit(check_file, filename): filename\n            for filename in args.filenames\n        }\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                for lint_message in future.result():\n                    print(json.dumps(lint_message._asdict()), flush=True)\n            except Exception:\n                raise RuntimeError(f\"Failed at {futures[future]}\")",
  "def _get_sharded_modules_recursive(\n    module: nn.Module,\n    path: str,\n    plan: ShardingPlan,\n) -> Dict[str, nn.Module]:\n    \"\"\"\n    Get all sharded modules of module from `plan`.\n    \"\"\"\n    params_plan = plan.get_plan_for_module(path)\n    if params_plan:\n        return {path: (module, params_plan)}\n\n    res = {}\n    for name, child in module.named_children():\n        new_path = f\"{path}.{name}\" if path else name\n        res.update(_get_sharded_modules_recursive(child, new_path, plan))\n    return res",
  "class IDTransformerCollection:\n    def __init__(\n        self,\n        tables: Union[List[EmbeddingConfig], List[EmbeddingBagConfig]],\n        eviction_config=None,\n        transform_config=None,\n        ps_collection: PSCollection = None,\n    ):\n        \"\"\"\n        IDTransformerCollection could transform the input of a `Embedding(Bag)Collection`.\n        It contains the `IDTransformer` of tables in the\n        `Embedding(Bag)Collection`.\n\n        Args:\n            tables: list of `Embedding(Bag)Config` or `EmbeddingBagConfig` one passed to\n                `Embedding(Bag)Collection`.\n            eviction_config: config of the eviction strategy for IDTransformers.\n            transform_config: config of the transform strategy for IDTransformers.\n            ps_collection: `PSCollection` of the collection, if `None`, won't do eviction or fetch.\n                By default, IDTransformerCollection will evict half the ids when full.\n        \"\"\"\n        self._configs = tables\n        self._ps_collection = ps_collection\n\n        self._transformers = []\n        self._table_names = []\n        feature_names = set()\n        for config in tables:\n            if config.name in self._table_names:\n                raise ValueError(f\"Duplicate table name {config.name}\")\n            if not config.feature_names:\n                config.feature_names = [config.name]\n            self._table_names.append(config.name)\n            for feature_name in config.feature_names:\n                if feature_name in feature_names:\n                    raise ValueError(f\"Shared feature not allowed yet.\")\n            # only rank 0 will have the id transformer\n            # and other ranks will gather their to rank 0.\n            if dist.get_rank() == 0:\n                transformer = IDTransformer(\n                    num_embedding=config.num_embeddings,\n                    eviction_config=eviction_config,\n                    transform_config=transform_config,\n                )\n            else:\n                transformer = None\n            self._transformers.append(transformer)\n        self._feature_names: List[List[str]] = [\n            config.feature_names for config in tables\n        ]\n        self._ever_evicted = False\n        self._time = 0\n\n        if dist.get_world_size() > 1:\n            self._pg = dist.new_group(backend=\"gloo\")\n        self._stream = torch.cuda.Stream()\n\n    def _transform(\n        self, transformer, global_ids: List[torch.Tensor], cache_ids: List[torch.Tensor]\n    ):\n        with torch.cuda.stream(self._stream):\n            total_numel = sum([tensor.numel() for tensor in global_ids])\n            if total_numel > 1e6:\n                all_tensor = torch.cat(global_ids).to(\"cuda:0\")\n                unique_all_tensor, index = torch.unique(all_tensor, return_inverse=True)\n                unique_all_tensor = unique_all_tensor.to(\"cpu\")\n                all_cache = torch.empty_like(unique_all_tensor)\n                success, ids_to_fetch = transformer.transform(\n                    TensorList([unique_all_tensor]),\n                    TensorList([all_cache]),\n                    self._time,\n                )\n                del all_tensor\n                all_tensor = torch.take(all_cache.to(\"cuda:0\"), index)\n                offset = 0\n                for tensor in cache_ids:\n                    numel = tensor.numel()\n                    tensor.copy_(all_tensor[offset : offset + numel])\n                    offset += numel\n                assert (\n                    total_numel == offset\n                ), f\"total_numel not equal offset, {total_numel} vs {offset}\"\n            else:\n                # broadcast result\n                success, ids_to_fetch = transformer.transform(\n                    TensorList(global_ids),\n                    TensorList(cache_ids),\n                    self._time,\n                )\n            return success, ids_to_fetch\n\n    def transform(\n        self, global_features: KeyedJaggedTensor\n    ) -> Tuple[KeyedJaggedTensor, List[torch.classes.tde.FetchHandle]]:\n        \"\"\"\n        Transform global kjts into local kjts.\n\n        Return:\n            KeyedJaggedTensor: the transformed kjt.\n            List[torch.classes.tde.FetchHandle]: list of fetch handles to wait.\n        \"\"\"\n        global_values = global_features.values()\n        cache_values = torch.empty_like(global_values)\n\n        global_feature_indices = {\n            feature_name: i for i, feature_name in enumerate(global_features.keys())\n        }\n        offset_per_key = global_features.offset_per_key()\n\n        fetch_handles = []\n        for i, transformer in enumerate(self._transformers):\n            feature_names = self._feature_names[i]\n            feature_indices = [\n                global_feature_indices[feature_name] for feature_name in feature_names\n            ]\n            global_ids = [\n                global_values[offset_per_key[idx] : offset_per_key[idx + 1]]\n                for idx in feature_indices\n            ]\n            cache_ids = [\n                cache_values[offset_per_key[idx] : offset_per_key[idx + 1]]\n                for idx in feature_indices\n            ]\n\n            if dist.get_world_size() > 1:\n                concat_global_ids, concat_numel_list = gather_global_ids(\n                    global_ids, self._pg\n                )\n                if dist.get_rank() == 0:\n                    global_ids = global_ids + concat_global_ids[1:]\n                    cache_ids = cache_ids + [\n                        torch.empty_like(tensor) for tensor in concat_global_ids[1:]\n                    ]\n\n                    success, ids_to_fetch = self._transform(\n                        transformer, global_ids, cache_ids\n                    )\n                else:\n                    success, ids_to_fetch = True, None\n                success, ids_to_fetch = broadcast_transform_result(\n                    success, ids_to_fetch, self._pg\n                )\n\n                if self._ps_collection is not None:\n                    table_name = self._table_names[i]\n                    ps = self._ps_collection[table_name]\n                    if ids_to_fetch.numel() > 0:\n                        handle = ps.fetch(\n                            ids_to_fetch,\n                            self._time,\n                            self._ever_evicted,\n                            self._configs[i].get_weight_init_min(),\n                            self._configs[i].get_weight_init_max(),\n                        )\n                        fetch_handles.append(handle)\n                    if not success:\n                        # TODO(zilinzhu): make this configurable\n                        # broadcast ids_to_evict\n                        if dist.get_rank() == 0:\n                            ids_to_evict = transformer.evict(\n                                transformer._num_embedding // 2\n                            )\n                        else:\n                            ids_to_evict = None\n                        ids_to_evict = broadcast_ids_to_evict(ids_to_evict, self._pg)\n\n                        ps.evict(ids_to_evict)\n                        self._ever_evicted = True\n\n                        # retry after eviction.\n                        # broadcast result\n                        if dist.get_rank() == 0:\n                            success, ids_to_fetch = transformer.transform(\n                                TensorList(global_ids),\n                                TensorList(cache_ids),\n                                self._time,\n                            )\n                        else:\n                            success, ids_to_fetch = True, None\n                        success, ids_to_fetch = broadcast_transform_result(\n                            success, ids_to_fetch, self._pg\n                        )\n\n                        if not success:\n                            raise RuntimeError(\n                                \"Failed to transform global ids after eviction. \"\n                                f\"Maybe the num_embedding of table {table_name} is too small?\"\n                            )\n                        if ids_to_fetch.numel() > 0:\n                            fetch_handles.append(\n                                ps.fetch(\n                                    ids_to_fetch,\n                                    self._time,\n                                    self._ever_evicted,\n                                    self._configs[i].get_weight_init_min(),\n                                    self._configs[i].get_weight_init_max(),\n                                )\n                            )\n\n                scatter_cache_ids(cache_ids, concat_numel_list, self._pg)\n            else:\n                success, ids_to_fetch = self._transform(\n                    transformer, global_ids, cache_ids\n                )\n                if self._ps_collection is not None:\n                    table_name = self._table_names[i]\n                    ps = self._ps_collection[table_name]\n                    if ids_to_fetch.numel() > 0:\n                        handle = ps.fetch(\n                            ids_to_fetch,\n                            self._time,\n                            self._ever_evicted,\n                            self._configs[i].get_weight_init_min(),\n                            self._configs[i].get_weight_init_max(),\n                        )\n                        fetch_handles.append(handle)\n                    if not success:\n                        # TODO(zilinzhu): make this configurable\n                        ids_to_evict = transformer.evict(\n                            transformer._num_embedding // 2\n                        )\n                        ps.evict(ids_to_evict)\n                        self._ever_evicted = True\n\n                        # retry after eviction.\n                        success, ids_to_fetch = transformer.transform(\n                            TensorList(global_ids),\n                            TensorList(cache_ids),\n                            self._time,\n                        )\n                        if not success:\n                            raise RuntimeError(\n                                \"Failed to transform global ids after eviction. \"\n                                f\"Maybe the num_embedding of table {table_name} is too small?\"\n                            )\n                        if ids_to_fetch is not None:\n                            fetch_handles.append(\n                                ps.fetch(\n                                    ids_to_fetch,\n                                    self._time,\n                                    self._ever_evicted,\n                                    self._configs[i].get_weight_init_min(),\n                                    self._configs[i].get_weight_init_max(),\n                                )\n                            )\n\n        cache_values = KeyedJaggedTensor(\n            keys=global_features.keys(),\n            values=cache_values,\n            lengths=global_features.lengths(),\n            weights=global_features.weights_or_none(),\n        )\n        self._time += 1\n        return cache_values, fetch_handles\n\n    def save(self):\n        if self._ps_collection is None:\n            return\n        for i, transformer in enumerate(self._transformers):\n            table_name = self._table_names[i]\n            if dist.get_world_size() > 1:\n                if dist.get_rank() == 0:\n                    ids = transformer.save()\n                    numel = torch.tensor(ids.numel())\n                    dist.broadcast(numel, src=0, group=self._pg)\n                    dist.broadcast(ids, src=0, group=self._pg)\n                else:\n                    numel = torch.tensor(0)\n                    dist.broadcast(numel, src=0, group=self._pg)\n                    ids = torch.empty((numel // 2, 2), dtype=torch.int64)\n                    dist.broadcast(ids, src=0, group=self._pg)\n            else:\n                ids = transformer.save()\n\n            self._ps_collection[table_name].evict(ids)",
  "def __init__(\n        self,\n        tables: Union[List[EmbeddingConfig], List[EmbeddingBagConfig]],\n        eviction_config=None,\n        transform_config=None,\n        ps_collection: PSCollection = None,\n    ):\n        \"\"\"\n        IDTransformerCollection could transform the input of a `Embedding(Bag)Collection`.\n        It contains the `IDTransformer` of tables in the\n        `Embedding(Bag)Collection`.\n\n        Args:\n            tables: list of `Embedding(Bag)Config` or `EmbeddingBagConfig` one passed to\n                `Embedding(Bag)Collection`.\n            eviction_config: config of the eviction strategy for IDTransformers.\n            transform_config: config of the transform strategy for IDTransformers.\n            ps_collection: `PSCollection` of the collection, if `None`, won't do eviction or fetch.\n                By default, IDTransformerCollection will evict half the ids when full.\n        \"\"\"\n        self._configs = tables\n        self._ps_collection = ps_collection\n\n        self._transformers = []\n        self._table_names = []\n        feature_names = set()\n        for config in tables:\n            if config.name in self._table_names:\n                raise ValueError(f\"Duplicate table name {config.name}\")\n            if not config.feature_names:\n                config.feature_names = [config.name]\n            self._table_names.append(config.name)\n            for feature_name in config.feature_names:\n                if feature_name in feature_names:\n                    raise ValueError(f\"Shared feature not allowed yet.\")\n            # only rank 0 will have the id transformer\n            # and other ranks will gather their to rank 0.\n            if dist.get_rank() == 0:\n                transformer = IDTransformer(\n                    num_embedding=config.num_embeddings,\n                    eviction_config=eviction_config,\n                    transform_config=transform_config,\n                )\n            else:\n                transformer = None\n            self._transformers.append(transformer)\n        self._feature_names: List[List[str]] = [\n            config.feature_names for config in tables\n        ]\n        self._ever_evicted = False\n        self._time = 0\n\n        if dist.get_world_size() > 1:\n            self._pg = dist.new_group(backend=\"gloo\")\n        self._stream = torch.cuda.Stream()",
  "def _transform(\n        self, transformer, global_ids: List[torch.Tensor], cache_ids: List[torch.Tensor]\n    ):\n        with torch.cuda.stream(self._stream):\n            total_numel = sum([tensor.numel() for tensor in global_ids])\n            if total_numel > 1e6:\n                all_tensor = torch.cat(global_ids).to(\"cuda:0\")\n                unique_all_tensor, index = torch.unique(all_tensor, return_inverse=True)\n                unique_all_tensor = unique_all_tensor.to(\"cpu\")\n                all_cache = torch.empty_like(unique_all_tensor)\n                success, ids_to_fetch = transformer.transform(\n                    TensorList([unique_all_tensor]),\n                    TensorList([all_cache]),\n                    self._time,\n                )\n                del all_tensor\n                all_tensor = torch.take(all_cache.to(\"cuda:0\"), index)\n                offset = 0\n                for tensor in cache_ids:\n                    numel = tensor.numel()\n                    tensor.copy_(all_tensor[offset : offset + numel])\n                    offset += numel\n                assert (\n                    total_numel == offset\n                ), f\"total_numel not equal offset, {total_numel} vs {offset}\"\n            else:\n                # broadcast result\n                success, ids_to_fetch = transformer.transform(\n                    TensorList(global_ids),\n                    TensorList(cache_ids),\n                    self._time,\n                )\n            return success, ids_to_fetch",
  "def transform(\n        self, global_features: KeyedJaggedTensor\n    ) -> Tuple[KeyedJaggedTensor, List[torch.classes.tde.FetchHandle]]:\n        \"\"\"\n        Transform global kjts into local kjts.\n\n        Return:\n            KeyedJaggedTensor: the transformed kjt.\n            List[torch.classes.tde.FetchHandle]: list of fetch handles to wait.\n        \"\"\"\n        global_values = global_features.values()\n        cache_values = torch.empty_like(global_values)\n\n        global_feature_indices = {\n            feature_name: i for i, feature_name in enumerate(global_features.keys())\n        }\n        offset_per_key = global_features.offset_per_key()\n\n        fetch_handles = []\n        for i, transformer in enumerate(self._transformers):\n            feature_names = self._feature_names[i]\n            feature_indices = [\n                global_feature_indices[feature_name] for feature_name in feature_names\n            ]\n            global_ids = [\n                global_values[offset_per_key[idx] : offset_per_key[idx + 1]]\n                for idx in feature_indices\n            ]\n            cache_ids = [\n                cache_values[offset_per_key[idx] : offset_per_key[idx + 1]]\n                for idx in feature_indices\n            ]\n\n            if dist.get_world_size() > 1:\n                concat_global_ids, concat_numel_list = gather_global_ids(\n                    global_ids, self._pg\n                )\n                if dist.get_rank() == 0:\n                    global_ids = global_ids + concat_global_ids[1:]\n                    cache_ids = cache_ids + [\n                        torch.empty_like(tensor) for tensor in concat_global_ids[1:]\n                    ]\n\n                    success, ids_to_fetch = self._transform(\n                        transformer, global_ids, cache_ids\n                    )\n                else:\n                    success, ids_to_fetch = True, None\n                success, ids_to_fetch = broadcast_transform_result(\n                    success, ids_to_fetch, self._pg\n                )\n\n                if self._ps_collection is not None:\n                    table_name = self._table_names[i]\n                    ps = self._ps_collection[table_name]\n                    if ids_to_fetch.numel() > 0:\n                        handle = ps.fetch(\n                            ids_to_fetch,\n                            self._time,\n                            self._ever_evicted,\n                            self._configs[i].get_weight_init_min(),\n                            self._configs[i].get_weight_init_max(),\n                        )\n                        fetch_handles.append(handle)\n                    if not success:\n                        # TODO(zilinzhu): make this configurable\n                        # broadcast ids_to_evict\n                        if dist.get_rank() == 0:\n                            ids_to_evict = transformer.evict(\n                                transformer._num_embedding // 2\n                            )\n                        else:\n                            ids_to_evict = None\n                        ids_to_evict = broadcast_ids_to_evict(ids_to_evict, self._pg)\n\n                        ps.evict(ids_to_evict)\n                        self._ever_evicted = True\n\n                        # retry after eviction.\n                        # broadcast result\n                        if dist.get_rank() == 0:\n                            success, ids_to_fetch = transformer.transform(\n                                TensorList(global_ids),\n                                TensorList(cache_ids),\n                                self._time,\n                            )\n                        else:\n                            success, ids_to_fetch = True, None\n                        success, ids_to_fetch = broadcast_transform_result(\n                            success, ids_to_fetch, self._pg\n                        )\n\n                        if not success:\n                            raise RuntimeError(\n                                \"Failed to transform global ids after eviction. \"\n                                f\"Maybe the num_embedding of table {table_name} is too small?\"\n                            )\n                        if ids_to_fetch.numel() > 0:\n                            fetch_handles.append(\n                                ps.fetch(\n                                    ids_to_fetch,\n                                    self._time,\n                                    self._ever_evicted,\n                                    self._configs[i].get_weight_init_min(),\n                                    self._configs[i].get_weight_init_max(),\n                                )\n                            )\n\n                scatter_cache_ids(cache_ids, concat_numel_list, self._pg)\n            else:\n                success, ids_to_fetch = self._transform(\n                    transformer, global_ids, cache_ids\n                )\n                if self._ps_collection is not None:\n                    table_name = self._table_names[i]\n                    ps = self._ps_collection[table_name]\n                    if ids_to_fetch.numel() > 0:\n                        handle = ps.fetch(\n                            ids_to_fetch,\n                            self._time,\n                            self._ever_evicted,\n                            self._configs[i].get_weight_init_min(),\n                            self._configs[i].get_weight_init_max(),\n                        )\n                        fetch_handles.append(handle)\n                    if not success:\n                        # TODO(zilinzhu): make this configurable\n                        ids_to_evict = transformer.evict(\n                            transformer._num_embedding // 2\n                        )\n                        ps.evict(ids_to_evict)\n                        self._ever_evicted = True\n\n                        # retry after eviction.\n                        success, ids_to_fetch = transformer.transform(\n                            TensorList(global_ids),\n                            TensorList(cache_ids),\n                            self._time,\n                        )\n                        if not success:\n                            raise RuntimeError(\n                                \"Failed to transform global ids after eviction. \"\n                                f\"Maybe the num_embedding of table {table_name} is too small?\"\n                            )\n                        if ids_to_fetch is not None:\n                            fetch_handles.append(\n                                ps.fetch(\n                                    ids_to_fetch,\n                                    self._time,\n                                    self._ever_evicted,\n                                    self._configs[i].get_weight_init_min(),\n                                    self._configs[i].get_weight_init_max(),\n                                )\n                            )\n\n        cache_values = KeyedJaggedTensor(\n            keys=global_features.keys(),\n            values=cache_values,\n            lengths=global_features.lengths(),\n            weights=global_features.weights_or_none(),\n        )\n        self._time += 1\n        return cache_values, fetch_handles",
  "def save(self):\n        if self._ps_collection is None:\n            return\n        for i, transformer in enumerate(self._transformers):\n            table_name = self._table_names[i]\n            if dist.get_world_size() > 1:\n                if dist.get_rank() == 0:\n                    ids = transformer.save()\n                    numel = torch.tensor(ids.numel())\n                    dist.broadcast(numel, src=0, group=self._pg)\n                    dist.broadcast(ids, src=0, group=self._pg)\n                else:\n                    numel = torch.tensor(0)\n                    dist.broadcast(numel, src=0, group=self._pg)\n                    ids = torch.empty((numel // 2, 2), dtype=torch.int64)\n                    dist.broadcast(ids, src=0, group=self._pg)\n            else:\n                ids = transformer.save()\n\n            self._ps_collection[table_name].evict(ids)",
  "class TensorList:\n    def __init__(self, tensors: List[torch.Tensor]):\n        self.tensor_list = torch.classes.tde.TensorList()\n        for tensor in tensors:\n            # tensor.data will allow inplace ops during autograd.\n            # https://discuss.pytorch.org/t/disable-in-place-correctness-version-check-any-other-workaround/90738/2\n            self.tensor_list.append(tensor.data)\n\n    def __len__(self):\n        return len(self.tensor_list)\n\n    def __getitem__(self, i):\n        return self.tensor_list[i]",
  "def __init__(self, tensors: List[torch.Tensor]):\n        self.tensor_list = torch.classes.tde.TensorList()\n        for tensor in tensors:\n            # tensor.data will allow inplace ops during autograd.\n            # https://discuss.pytorch.org/t/disable-in-place-correctness-version-check-any-other-workaround/90738/2\n            self.tensor_list.append(tensor.data)",
  "def __len__(self):\n        return len(self.tensor_list)",
  "def __getitem__(self, i):\n        return self.tensor_list[i]",
  "class IDTransformer:\n    def __init__(self, num_embedding, eviction_config=None, transform_config=None):\n        self._num_embedding = num_embedding\n        if not eviction_config:\n            eviction_config = {\"type\": \"mixed_lru_lfu\"}\n        if not transform_config:\n            transform_config = {\"type\": \"naive\"}\n        config = json.dumps(\n            {\n                \"lxu_strategy\": eviction_config,\n                \"id_transformer\": transform_config,\n            }\n        )\n        self._transformer = torch.classes.tde.IDTransformer(num_embedding, config)\n\n    def transform(self, global_ids: TensorList, cache_ids: TensorList, time: int):\n        \"\"\"\n        Transform `global_ids` and store the results in `cache_ids`.\n        \"\"\"\n        result = self._transformer.transform(\n            global_ids.tensor_list, cache_ids.tensor_list, time\n        )\n        return result.success, result.ids_to_fetch\n\n    def evict(self, num_to_evict):\n        \"\"\"\n        Evict `num_to_evict` ids from the transformer.\n        \"\"\"\n        return self._transformer.evict(num_to_evict)\n\n    def save(self):\n        \"\"\"\n        Get ids to save.\n        \"\"\"\n        return self._transformer.save()",
  "def __init__(self, num_embedding, eviction_config=None, transform_config=None):\n        self._num_embedding = num_embedding\n        if not eviction_config:\n            eviction_config = {\"type\": \"mixed_lru_lfu\"}\n        if not transform_config:\n            transform_config = {\"type\": \"naive\"}\n        config = json.dumps(\n            {\n                \"lxu_strategy\": eviction_config,\n                \"id_transformer\": transform_config,\n            }\n        )\n        self._transformer = torch.classes.tde.IDTransformer(num_embedding, config)",
  "def transform(self, global_ids: TensorList, cache_ids: TensorList, time: int):\n        \"\"\"\n        Transform `global_ids` and store the results in `cache_ids`.\n        \"\"\"\n        result = self._transformer.transform(\n            global_ids.tensor_list, cache_ids.tensor_list, time\n        )\n        return result.success, result.ids_to_fetch",
  "def evict(self, num_to_evict):\n        \"\"\"\n        Evict `num_to_evict` ids from the transformer.\n        \"\"\"\n        return self._transformer.evict(num_to_evict)",
  "def save(self):\n        \"\"\"\n        Get ids to save.\n        \"\"\"\n        return self._transformer.save()",
  "class PS:\n    def __init__(\n        self,\n        table_name: str,\n        tensors: Union[List[torch.Tensor], List[ShardedTensor]],\n        url: str,\n        chunk_size: str,\n    ):\n        \"\"\"\n        PS table of an embedding table.\n\n        Args:\n            table_name: name of the table.\n            tensors: tensors of the table, the first one is the parameter tensor, others are\n                tenors of optimizers, e.g. for Adam, it will be [weight, m, v].\n            url: url of the PS.\n        \"\"\"\n        shards = torch.classes.tde.LocalShardList()\n        num_optimizer_stats = len(tensors)\n        if isinstance(tensors[0], ShardedTensor):\n            # Here we assume the shard metadata of optimizer state and weight are the same.\n            for i, shard in enumerate(tensors[0].local_shards()):\n                local_tensors = [tensor.local_shards()[i].tensor for tensor in tensors]\n                shards.append(\n                    shard.metadata.shard_offsets[0],\n                    shard.metadata.shard_offsets[1],\n                    shard.metadata.shard_sizes[0],\n                    shard.metadata.shard_sizes[1],\n                    TensorList(local_tensors).tensor_list,\n                )\n                # This assumes all shard have the same column size.\n                col_size = shard.tensor.shape[1]\n        elif isinstance(tensors[0], torch.Tensor):\n            shards.append(\n                0,\n                0,\n                tensors[0].shape[0],\n                tensors[0].shape[1],\n                TensorList(tensors).tensor_list,\n            )\n            col_size = tensors[0].shape[1]\n        self._ps = torch.classes.tde.PS(\n            table_name, shards, col_size, num_optimizer_stats, url, chunk_size\n        )\n\n    def evict(self, ids_to_evict: torch.Tensor):\n        \"\"\"\n        Evict the `ids_to_evict` to PS.\n        \"\"\"\n        self._ps.evict(ids_to_evict)\n\n    def fetch(\n        self,\n        ids_to_fetch: torch.Tensor,\n        time: int,\n        reinit: bool = False,\n        weight_init_max: float = 0,\n        weight_init_min: float = 0,\n    ):\n        \"\"\"\n        Fetch `ids_to_fetch` from tensor. If `reinit` is set to `True`, will\n        reinitialize the embedding if the global id is not in PS.\n        \"\"\"\n        return self._ps.fetch(\n            ids_to_fetch, time, reinit, weight_init_max, weight_init_min\n        )",
  "class PSCollection:\n    \"\"\"\n    PS tables correspond to an EmbeddingCollection or EmbeddingBagCollection.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        plan: Dict[str, Tuple[ParameterSharding, Union[torch.Tensor, ShardedTensor]]],\n        url: Union[str, Callable[[str], str]],\n        ps_config=None,\n    ):\n        \"\"\"\n        Args:\n            path: module path.\n            plan: dict keyed by table name of ParameterSharding and tensor of the table.\n            url: configuration for PS, e.g. redis://127.0.0.1:6379/?prefix=model.\n            ps_config: config of the PS, currently only support setting chunk size.\n        \"\"\"\n        self._path = path\n        self._ps_collection = {}\n        chunk_size = DEFAULT_PS_CHUNK_SIZE\n        if ps_config is not None and \"chunk_size\" in ps_config:\n            chunk_size = ps_config[\"chunk_size\"]\n        for table_name, (param_plan, tensor) in plan.items():\n            if isinstance(url, str):\n                table_config = url\n            else:\n                table_config = url(table_name)\n            self._ps_collection[table_name] = PS(\n                f\"{path}.{table_name}\", tensor, table_config, chunk_size\n            )\n\n    def table_names(self):\n        return self._ps_collection.keys()\n\n    def __getitem__(self, table_name):\n        return self._ps_collection[table_name]\n\n    @staticmethod\n    def fromModule(path, sharded_module, params_plan, url, ps_config=None):\n        \"\"\"\n        Create PSCollection for `sharded_module`, whose module path is `path`\n\n        Args:\n            path: module path of the sharded module.\n            sharded_module: the sharded module.\n            params_plan: the sharding plan of `sharded_module`.\n            url: configuration for PS, e.g. redis://127.0.0.1:6379/?prefix=model.\n            ps_config: config of the PS, currently only support setting chunk size.\n\n        Return:\n            PSCollection of the sharded module.\n        \"\"\"\n\n        state_dict = sharded_module.state_dict()\n        optimizer_state_dict = sharded_module.fused_optimizer.state_dict()[\"state\"]\n        tensor_infos = {}\n        for key, tensor in state_dict.items():\n            # Here we use the fact that state_dict will be shape of\n            # `embeddings.xxx.weight` or `embeddingbags.xxx.weight`\n            if len(key.split(\".\")) <= 1 or key.split(\".\")[1] not in params_plan:\n                continue\n            table_name = key.split(\".\")[1]\n            param_plan = params_plan.pop(table_name)\n            tensors = [tensor]\n            # This is really hardcoded right now...\n            optimizer_state = optimizer_state_dict[key]\n            if f\"{table_name}.momentum1\" in optimizer_state:\n                tensors.append(optimizer_state[f\"{table_name}.momentum1\"])\n            if f\"{table_name}.momentum2\" in optimizer_state:\n                tensors.append(optimizer_state[f\"{table_name}.momentum2\"])\n            tensor_infos[table_name] = (param_plan, tensors)\n\n        assert (\n            len(params_plan) == 0\n        ), f\"There are sharded param not found, leaving: {params_plan}.\"\n\n        if isinstance(url, str):\n            collection_schema = url\n        else:\n            collection_schema = lambda table_name: url(path, table_name)\n\n        return PSCollection(path, tensor_infos, collection_schema, ps_config)",
  "def __init__(\n        self,\n        table_name: str,\n        tensors: Union[List[torch.Tensor], List[ShardedTensor]],\n        url: str,\n        chunk_size: str,\n    ):\n        \"\"\"\n        PS table of an embedding table.\n\n        Args:\n            table_name: name of the table.\n            tensors: tensors of the table, the first one is the parameter tensor, others are\n                tenors of optimizers, e.g. for Adam, it will be [weight, m, v].\n            url: url of the PS.\n        \"\"\"\n        shards = torch.classes.tde.LocalShardList()\n        num_optimizer_stats = len(tensors)\n        if isinstance(tensors[0], ShardedTensor):\n            # Here we assume the shard metadata of optimizer state and weight are the same.\n            for i, shard in enumerate(tensors[0].local_shards()):\n                local_tensors = [tensor.local_shards()[i].tensor for tensor in tensors]\n                shards.append(\n                    shard.metadata.shard_offsets[0],\n                    shard.metadata.shard_offsets[1],\n                    shard.metadata.shard_sizes[0],\n                    shard.metadata.shard_sizes[1],\n                    TensorList(local_tensors).tensor_list,\n                )\n                # This assumes all shard have the same column size.\n                col_size = shard.tensor.shape[1]\n        elif isinstance(tensors[0], torch.Tensor):\n            shards.append(\n                0,\n                0,\n                tensors[0].shape[0],\n                tensors[0].shape[1],\n                TensorList(tensors).tensor_list,\n            )\n            col_size = tensors[0].shape[1]\n        self._ps = torch.classes.tde.PS(\n            table_name, shards, col_size, num_optimizer_stats, url, chunk_size\n        )",
  "def evict(self, ids_to_evict: torch.Tensor):\n        \"\"\"\n        Evict the `ids_to_evict` to PS.\n        \"\"\"\n        self._ps.evict(ids_to_evict)",
  "def fetch(\n        self,\n        ids_to_fetch: torch.Tensor,\n        time: int,\n        reinit: bool = False,\n        weight_init_max: float = 0,\n        weight_init_min: float = 0,\n    ):\n        \"\"\"\n        Fetch `ids_to_fetch` from tensor. If `reinit` is set to `True`, will\n        reinitialize the embedding if the global id is not in PS.\n        \"\"\"\n        return self._ps.fetch(\n            ids_to_fetch, time, reinit, weight_init_max, weight_init_min\n        )",
  "def __init__(\n        self,\n        path: str,\n        plan: Dict[str, Tuple[ParameterSharding, Union[torch.Tensor, ShardedTensor]]],\n        url: Union[str, Callable[[str], str]],\n        ps_config=None,\n    ):\n        \"\"\"\n        Args:\n            path: module path.\n            plan: dict keyed by table name of ParameterSharding and tensor of the table.\n            url: configuration for PS, e.g. redis://127.0.0.1:6379/?prefix=model.\n            ps_config: config of the PS, currently only support setting chunk size.\n        \"\"\"\n        self._path = path\n        self._ps_collection = {}\n        chunk_size = DEFAULT_PS_CHUNK_SIZE\n        if ps_config is not None and \"chunk_size\" in ps_config:\n            chunk_size = ps_config[\"chunk_size\"]\n        for table_name, (param_plan, tensor) in plan.items():\n            if isinstance(url, str):\n                table_config = url\n            else:\n                table_config = url(table_name)\n            self._ps_collection[table_name] = PS(\n                f\"{path}.{table_name}\", tensor, table_config, chunk_size\n            )",
  "def table_names(self):\n        return self._ps_collection.keys()",
  "def __getitem__(self, table_name):\n        return self._ps_collection[table_name]",
  "def fromModule(path, sharded_module, params_plan, url, ps_config=None):\n        \"\"\"\n        Create PSCollection for `sharded_module`, whose module path is `path`\n\n        Args:\n            path: module path of the sharded module.\n            sharded_module: the sharded module.\n            params_plan: the sharding plan of `sharded_module`.\n            url: configuration for PS, e.g. redis://127.0.0.1:6379/?prefix=model.\n            ps_config: config of the PS, currently only support setting chunk size.\n\n        Return:\n            PSCollection of the sharded module.\n        \"\"\"\n\n        state_dict = sharded_module.state_dict()\n        optimizer_state_dict = sharded_module.fused_optimizer.state_dict()[\"state\"]\n        tensor_infos = {}\n        for key, tensor in state_dict.items():\n            # Here we use the fact that state_dict will be shape of\n            # `embeddings.xxx.weight` or `embeddingbags.xxx.weight`\n            if len(key.split(\".\")) <= 1 or key.split(\".\")[1] not in params_plan:\n                continue\n            table_name = key.split(\".\")[1]\n            param_plan = params_plan.pop(table_name)\n            tensors = [tensor]\n            # This is really hardcoded right now...\n            optimizer_state = optimizer_state_dict[key]\n            if f\"{table_name}.momentum1\" in optimizer_state:\n                tensors.append(optimizer_state[f\"{table_name}.momentum1\"])\n            if f\"{table_name}.momentum2\" in optimizer_state:\n                tensors.append(optimizer_state[f\"{table_name}.momentum2\"])\n            tensor_infos[table_name] = (param_plan, tensors)\n\n        assert (\n            len(params_plan) == 0\n        ), f\"There are sharded param not found, leaving: {params_plan}.\"\n\n        if isinstance(url, str):\n            collection_schema = url\n        else:\n            collection_schema = lambda table_name: url(path, table_name)\n\n        return PSCollection(path, tensor_infos, collection_schema, ps_config)",
  "def transform_loop(dataloader, transform_fn, out_queue, done_event):\n    # This setting is thread local, and prevents the copy in pin_memory from\n    # consuming all CPU cores.\n    torch.set_num_threads(1)\n\n    for data in dataloader:\n        if done_event.is_set():\n            break\n        transformed_data = transform_fn(data)\n\n        while not done_event.is_set():\n            try:\n                out_queue.put(transformed_data, timeout=MP_STATUS_CHECK_INTERVAL)\n                break\n            except queue.Full:\n                continue\n        # save memory\n        del transformed_data\n\n    if not done_event.is_set():\n        done_event.set()",
  "class DataLoaderIter:\n    def __init__(self, dataloader, transform_fn, num_prefetch=0):\n        self._data_queue = queue.Queue(maxsize=num_prefetch)\n        self._done_event = threading.Event()\n        self._transform_thread = threading.Thread(\n            target=transform_loop,\n            args=(dataloader, transform_fn, self._data_queue, self._done_event),\n        )\n        self._transform_thread.start()\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self._get_data()\n\n    def __del__(self):\n        self._done_event.set()\n\n    def _get_data(self):\n        if self._done_event.is_set():\n            raise StopIteration\n        if not self._transform_thread.is_alive():\n            raise RuntimeError(\"Transform thread exited unexpectedly\")\n        data, handles = self._data_queue.get()\n        for handle in handles:\n            handle.wait()\n        return data",
  "class DataLoader:\n    def __init__(\n        self,\n        id_transformer_group: IDTransformerGroup,\n        dataloader,\n        *,\n        data_info: Dict[int, str] = None,\n        paths: List[str] = None,\n        num_prefetch=0,\n    ):\n        self._id_transformer_group = id_transformer_group\n\n        if data_info is not None:\n            for _, path in data_info.items():\n                if path not in self._id_transformer_group:\n                    raise ValueError(\n                        f\"invalid path `{path}` data_info. No id transformer for this path.\"\n                    )\n        else:\n            self._paths = paths\n\n        self._data_info = data_info\n\n        self._data_queue = queue.Queue(maxsize=num_prefetch)\n        self._done_event = threading.Event()\n\n        self._dataloader = dataloader\n        self._num_prefetch = num_prefetch\n\n    def _transform_fn(self, data):\n        \"\"\"\n        transform data with `data_info`\n        \"\"\"\n        if self._data_info is None:\n            data_info = {}\n            path_idx = 0\n            for i in range(len(data)):\n                if isinstance(data[i], KeyedJaggedTensor):\n                    if path_idx >= len(self._paths):\n                        raise ValueError(\n                            \"Has more KJT in a data sample than the number of modules, \"\n                            \"could not infer data_info, please set data_info manually\"\n                        )\n                    data_info[i] = self._paths[path_idx]\n                    path_idx += 1\n        else:\n            data_info = self._data_info\n        global_kjts = {path: data[idx] for idx, path in data_info.items()}\n        cache_kjts, fetch_handles = self._id_transformer_group.transform(global_kjts)\n        data = list(data)\n        for idx, path in data_info.items():\n            data[idx] = cache_kjts[path]\n        return tuple(data), fetch_handles\n\n    def __iter__(self):\n        return DataLoaderIter(\n            self._dataloader, self._transform_fn, num_prefetch=self._num_prefetch\n        )\n\n    def __len__(self):\n        return len(self._dataloader)",
  "def wrap(\n    url: str,\n    dataloader,\n    module: DistributedModelParallel,\n    configs_dict: Dict[str, Union[List[EmbeddingBagConfig], List[EmbeddingConfig]]],\n    *,\n    data_info: Dict[int, str] = None,\n    eviction_config=None,\n    transform_config=None,\n    ps_config=None,\n    parallel=True,\n    num_prefetch=0,\n):\n    \"\"\"\n    DataLoader to transform data from global id to cache id.\n\n    Args:\n        url: configuration for PS, e.g. redis://127.0.0.1:6379/?prefix=model.\n        dataloader: dataloader to transform.\n        module: DMP module that need dynamic embedding.\n        configs_dict: a dictionary that maps the module path of the sharded module to its embedding\n            configs or embeddingbag configs. The plan of `module` should contain the module path\n            in `configs_dict`.\n        data_info: dict keyed by int index of module path. For example, if the dataloader produces\n            `label, kjt1, kjt2` each iteration and `kjt1` and `kjt2` are inputs to modules of path\n            `emb1` and `emb2` respectively, then `data_info` should be `{ 1: \"emb1\", 2: \"emb2\" }`.\n        eviction_config: configuration for eviction policy. Default is `{\"type\": \"mixed_lru_lfu\"}`\n        transform_config: configuration for the transformer. Default is `{\"type\": \"naive\"}`\n        transform_config: configuration for the ps. Default is `{\"chunk_size\": 8 * 1024 * 1024}\n        parallel: Whether the IDTransformerCollections will run paralell. When set to True,\n            IDTransformerGroup will start a thread for each IDTransformerCollection.\n        num_prefetch: number of samples to prefetch.\n\n    Return:\n        DataLoader: the dataloader to transform data.\n        DistributedModelParallel: model with id_transformer_group attached.\n\n    Example:\n        class Model(nn.Module):\n            def __init__(self, config1, config2):\n                super().__init__()\n                self.emb1 = EmbeddingCollection(tables=config1, device=torch.device(\"meta\"))\n                self.emb2 = EmbeddingCollection(tables=config2, device=torch.device(\"meta\"))\n                ...\n\n            def forward(self, kjt1, kjt2):\n                ...\n\n        m = Model(config1, config2)\n        m = DistributedModelParallel(m)\n        dataloader, m = tde.wrap(\"redis://127.0.0.1:6379/\", dataloader, m, { \"emb1\": config1, \"emb2\": config2 })\n\n        for label, kjt1, kjt2 in dataloader:\n            output = m(kjt1, kjt2)\n            ...\n    \"\"\"\n    id_transformer_group = IDTransformerGroup(\n        url,\n        module,\n        configs_dict,\n        eviction_config=eviction_config,\n        transform_config=transform_config,\n        ps_config=ps_config,\n        parallel=parallel,\n    )\n    paths = list(configs_dict.keys())\n    # Attach the id transformer group to module for saving.\n    module._id_transformer_group = id_transformer_group\n\n    return (\n        DataLoader(\n            id_transformer_group=id_transformer_group,\n            dataloader=dataloader,\n            data_info=data_info,\n            paths=paths,\n            num_prefetch=num_prefetch,\n        ),\n        module,\n    )",
  "def save(module: DistributedModelParallel):\n    \"\"\"\n    Save the dynamic embedding part of the model.\n    \"\"\"\n    if not hasattr(module, \"_id_transformer_group\"):\n        raise ValueError(\n            \"No _id_transformer_group property for module, is this a module with dynamic embeding?\"\n        )\n    if not isinstance(module._id_transformer_group, IDTransformerGroup):\n        raise ValueError(\n            \"module._id_transformer_group property is not IDTransformerGroup, is this a module with dynamic embeding?\"\n        )\n\n    module._id_transformer_group.save()",
  "def __init__(self, dataloader, transform_fn, num_prefetch=0):\n        self._data_queue = queue.Queue(maxsize=num_prefetch)\n        self._done_event = threading.Event()\n        self._transform_thread = threading.Thread(\n            target=transform_loop,\n            args=(dataloader, transform_fn, self._data_queue, self._done_event),\n        )\n        self._transform_thread.start()",
  "def __iter__(self):\n        return self",
  "def __next__(self):\n        return self._get_data()",
  "def __del__(self):\n        self._done_event.set()",
  "def _get_data(self):\n        if self._done_event.is_set():\n            raise StopIteration\n        if not self._transform_thread.is_alive():\n            raise RuntimeError(\"Transform thread exited unexpectedly\")\n        data, handles = self._data_queue.get()\n        for handle in handles:\n            handle.wait()\n        return data",
  "def __init__(\n        self,\n        id_transformer_group: IDTransformerGroup,\n        dataloader,\n        *,\n        data_info: Dict[int, str] = None,\n        paths: List[str] = None,\n        num_prefetch=0,\n    ):\n        self._id_transformer_group = id_transformer_group\n\n        if data_info is not None:\n            for _, path in data_info.items():\n                if path not in self._id_transformer_group:\n                    raise ValueError(\n                        f\"invalid path `{path}` data_info. No id transformer for this path.\"\n                    )\n        else:\n            self._paths = paths\n\n        self._data_info = data_info\n\n        self._data_queue = queue.Queue(maxsize=num_prefetch)\n        self._done_event = threading.Event()\n\n        self._dataloader = dataloader\n        self._num_prefetch = num_prefetch",
  "def _transform_fn(self, data):\n        \"\"\"\n        transform data with `data_info`\n        \"\"\"\n        if self._data_info is None:\n            data_info = {}\n            path_idx = 0\n            for i in range(len(data)):\n                if isinstance(data[i], KeyedJaggedTensor):\n                    if path_idx >= len(self._paths):\n                        raise ValueError(\n                            \"Has more KJT in a data sample than the number of modules, \"\n                            \"could not infer data_info, please set data_info manually\"\n                        )\n                    data_info[i] = self._paths[path_idx]\n                    path_idx += 1\n        else:\n            data_info = self._data_info\n        global_kjts = {path: data[idx] for idx, path in data_info.items()}\n        cache_kjts, fetch_handles = self._id_transformer_group.transform(global_kjts)\n        data = list(data)\n        for idx, path in data_info.items():\n            data[idx] = cache_kjts[path]\n        return tuple(data), fetch_handles",
  "def __iter__(self):\n        return DataLoaderIter(\n            self._dataloader, self._transform_fn, num_prefetch=self._num_prefetch\n        )",
  "def __len__(self):\n        return len(self._dataloader)",
  "def _create_transformer_thread(transformer: IDTransformerCollection):\n    \"\"\"\n    Create a thread for transformer.\n    \"\"\"\n\n    def loop(transformer, input_queue, output_queue):\n        while True:\n            global_kjt = input_queue.get()\n            if global_kjt is None:\n                break\n            cache_kjt = transformer.transform(global_kjt)\n            output_queue.put(cache_kjt)\n\n    input_queue = queue.Queue()\n    output_queue = queue.Queue()\n    thread = threading.Thread(\n        target=loop, args=(transformer, input_queue, output_queue)\n    )\n    thread.start()\n    return thread, input_queue, output_queue",
  "class IDTransformerGroup:\n    def __init__(\n        self,\n        url,\n        module: DistributedModelParallel,\n        configs_dict: Dict[str, Union[List[EmbeddingBagConfig], List[EmbeddingConfig]]],\n        *,\n        eviction_config=None,\n        transform_config=None,\n        ps_config=None,\n        parallel=True,\n    ):\n        \"\"\"\n        IDTransformerGroup stores the IDTransformer for all sharded modules in a DMP module.\n\n        Args:\n            url: configuration for PS, e.g. redis://127.0.0.1:6379/?prefix=model.\n            module: DMP module that need dynamic embedding.\n            configs_dict: a dictionary that maps the module path of the sharded module to its embedding\n                configs or embeddingbag configs. The plan of `module` should contain the module path\n                in `configs_dict`.\n            eviction_config: configuration for eviction policy. Default is `{\"type\": \"mixed_lru_lfu\"}`\n            transform_config: configuration for the transformer. Default is `{\"type\": \"naive\"}`\n            parallel: Whether the IDTransformerCollections will run parallel. When set to True,\n                IDTransformerGroup will start a thread for each IDTransformerCollection.\n\n        Example:\n            class Model(nn.Module):\n                def __init__(self, config1, config2):\n                    super().__init__()\n                    self.emb1 = EmbeddingCollection(tables=config1, device=torch.device(\"meta\"))\n                    self.emb2 = EmbeddingCollection(tables=config2, device=torch.device(\"meta\"))\n                    ...\n\n                def forward(self, kjt1, kjt2):\n                    ...\n\n            m = Model(config1, config2)\n            m = DistributedModelParallel(m)\n            transformers = IDTransformerGroup(\n                \"redis://127.0.0.1:6379/?prefix=model\",\n                m,\n                { \"emb1\": config1, \"emb2\": config2 })\n\n            for label, kjt1, kjt2 in dataset:\n                kjts = transformers.transform({ \"emb1\": kjt1, \"emb2\": kjt2 })\n                kjt1, kjt2 = kjts[\"emb1\"], kjts[\"emb2\"]\n                output = m(kjt1, kjt2)\n                ...\n        \"\"\"\n        self._parallel = parallel\n\n        # get all sharded_modules from plan\n        plan = module.plan\n        sharded_modules = _get_sharded_modules_recursive(module.module, \"\", plan)\n\n        self._id_transformer_collections: Dict[str, IDTransformerCollection] = {}\n        for path, configs in configs_dict.items():\n            if path not in sharded_modules:\n                raise ValueError(\n                    f\"`{path}` in configs dooes not match any sharded modules. \"\n                    f\"Paths for current sharded modules are: {list(sharded_modules.keys())}.\"\n                )\n            sharded_module, params_plan = sharded_modules[path]\n            ps_collection = PSCollection.fromModule(\n                path, sharded_module, params_plan, url, ps_config\n            )\n            id_transformer_collection = IDTransformerCollection(\n                configs, eviction_config, transform_config, ps_collection\n            )\n            self._id_transformer_collections[path] = id_transformer_collection\n\n        if self._parallel:\n            self._threads = {}\n            self._input_queues = {}\n            self._output_queues = {}\n            for path, transformer in self._id_transformer_collections.items():\n                thread, input_queue, output_queue = _create_transformer_thread(\n                    transformer\n                )\n                self._threads[path] = thread\n                self._input_queues[path] = input_queue\n                self._output_queues[path] = output_queue\n\n    def transform(self, kjt_dict: Dict[str, KeyedJaggedTensor]):\n        \"\"\"\n        Transform global `KeyedJaggedTensor`s to local ones.\n\n        Args:\n            kjt_dict: dict keyed by module path of global kjts.\n        Return:\n            Dict[str, KeyedJaggedTensor]\n            List[torch.classes.tde.FetchHandle]: list of fetch handles to wait.\n        \"\"\"\n        result = {}\n        fetch_handles = []\n        if self._parallel:\n            for path, kjt in kjt_dict.items():\n                if path not in self._id_transformer_collections:\n                    raise ValueError(\n                        f\"kjt_dict contain invalid path {path}. \"\n                        f\"should be one of {self._id_transformer_collections.keys()}\"\n                    )\n                self._input_queues[path].put(kjt)\n\n            for path in kjt_dict:\n                kjt, handles = self._output_queues[path].get()\n                result[path] = kjt\n                fetch_handles.extend(handles)\n        else:\n            for path, kjt in kjt_dict.items():\n                if path not in self._id_transformer_collections:\n                    raise ValueError(\n                        f\"kjt_dict contain invalid path {path}. \"\n                        f\"should be one of {self._id_transformer_collections.keys()}\"\n                    )\n                kjt, handles = self._id_transformer_collections[path].transform(kjt)\n                result[path] = kjt\n                fetch_handles.extend(handles)\n        return result, fetch_handles\n\n    def save(self):\n        for _, id_transformer_collection in self._id_transformer_collections.items():\n            id_transformer_collection.save()\n\n    def __contains__(self, path):\n        \"\"\"\n        Check if there is transformer for the path.\n        \"\"\"\n        return path in self._id_transformer_collections\n\n    def __del__(self):\n        \"\"\"\n        Stop the parallel threads\n        \"\"\"\n        if self._parallel:\n            # stop the threads\n            for _, input_queue in self._input_queues.items():\n                input_queue.put(None)",
  "def loop(transformer, input_queue, output_queue):\n        while True:\n            global_kjt = input_queue.get()\n            if global_kjt is None:\n                break\n            cache_kjt = transformer.transform(global_kjt)\n            output_queue.put(cache_kjt)",
  "def __init__(\n        self,\n        url,\n        module: DistributedModelParallel,\n        configs_dict: Dict[str, Union[List[EmbeddingBagConfig], List[EmbeddingConfig]]],\n        *,\n        eviction_config=None,\n        transform_config=None,\n        ps_config=None,\n        parallel=True,\n    ):\n        \"\"\"\n        IDTransformerGroup stores the IDTransformer for all sharded modules in a DMP module.\n\n        Args:\n            url: configuration for PS, e.g. redis://127.0.0.1:6379/?prefix=model.\n            module: DMP module that need dynamic embedding.\n            configs_dict: a dictionary that maps the module path of the sharded module to its embedding\n                configs or embeddingbag configs. The plan of `module` should contain the module path\n                in `configs_dict`.\n            eviction_config: configuration for eviction policy. Default is `{\"type\": \"mixed_lru_lfu\"}`\n            transform_config: configuration for the transformer. Default is `{\"type\": \"naive\"}`\n            parallel: Whether the IDTransformerCollections will run parallel. When set to True,\n                IDTransformerGroup will start a thread for each IDTransformerCollection.\n\n        Example:\n            class Model(nn.Module):\n                def __init__(self, config1, config2):\n                    super().__init__()\n                    self.emb1 = EmbeddingCollection(tables=config1, device=torch.device(\"meta\"))\n                    self.emb2 = EmbeddingCollection(tables=config2, device=torch.device(\"meta\"))\n                    ...\n\n                def forward(self, kjt1, kjt2):\n                    ...\n\n            m = Model(config1, config2)\n            m = DistributedModelParallel(m)\n            transformers = IDTransformerGroup(\n                \"redis://127.0.0.1:6379/?prefix=model\",\n                m,\n                { \"emb1\": config1, \"emb2\": config2 })\n\n            for label, kjt1, kjt2 in dataset:\n                kjts = transformers.transform({ \"emb1\": kjt1, \"emb2\": kjt2 })\n                kjt1, kjt2 = kjts[\"emb1\"], kjts[\"emb2\"]\n                output = m(kjt1, kjt2)\n                ...\n        \"\"\"\n        self._parallel = parallel\n\n        # get all sharded_modules from plan\n        plan = module.plan\n        sharded_modules = _get_sharded_modules_recursive(module.module, \"\", plan)\n\n        self._id_transformer_collections: Dict[str, IDTransformerCollection] = {}\n        for path, configs in configs_dict.items():\n            if path not in sharded_modules:\n                raise ValueError(\n                    f\"`{path}` in configs dooes not match any sharded modules. \"\n                    f\"Paths for current sharded modules are: {list(sharded_modules.keys())}.\"\n                )\n            sharded_module, params_plan = sharded_modules[path]\n            ps_collection = PSCollection.fromModule(\n                path, sharded_module, params_plan, url, ps_config\n            )\n            id_transformer_collection = IDTransformerCollection(\n                configs, eviction_config, transform_config, ps_collection\n            )\n            self._id_transformer_collections[path] = id_transformer_collection\n\n        if self._parallel:\n            self._threads = {}\n            self._input_queues = {}\n            self._output_queues = {}\n            for path, transformer in self._id_transformer_collections.items():\n                thread, input_queue, output_queue = _create_transformer_thread(\n                    transformer\n                )\n                self._threads[path] = thread\n                self._input_queues[path] = input_queue\n                self._output_queues[path] = output_queue",
  "def transform(self, kjt_dict: Dict[str, KeyedJaggedTensor]):\n        \"\"\"\n        Transform global `KeyedJaggedTensor`s to local ones.\n\n        Args:\n            kjt_dict: dict keyed by module path of global kjts.\n        Return:\n            Dict[str, KeyedJaggedTensor]\n            List[torch.classes.tde.FetchHandle]: list of fetch handles to wait.\n        \"\"\"\n        result = {}\n        fetch_handles = []\n        if self._parallel:\n            for path, kjt in kjt_dict.items():\n                if path not in self._id_transformer_collections:\n                    raise ValueError(\n                        f\"kjt_dict contain invalid path {path}. \"\n                        f\"should be one of {self._id_transformer_collections.keys()}\"\n                    )\n                self._input_queues[path].put(kjt)\n\n            for path in kjt_dict:\n                kjt, handles = self._output_queues[path].get()\n                result[path] = kjt\n                fetch_handles.extend(handles)\n        else:\n            for path, kjt in kjt_dict.items():\n                if path not in self._id_transformer_collections:\n                    raise ValueError(\n                        f\"kjt_dict contain invalid path {path}. \"\n                        f\"should be one of {self._id_transformer_collections.keys()}\"\n                    )\n                kjt, handles = self._id_transformer_collections[path].transform(kjt)\n                result[path] = kjt\n                fetch_handles.extend(handles)\n        return result, fetch_handles",
  "def save(self):\n        for _, id_transformer_collection in self._id_transformer_collections.items():\n            id_transformer_collection.save()",
  "def __contains__(self, path):\n        \"\"\"\n        Check if there is transformer for the path.\n        \"\"\"\n        return path in self._id_transformer_collections",
  "def __del__(self):\n        \"\"\"\n        Stop the parallel threads\n        \"\"\"\n        if self._parallel:\n            # stop the threads\n            for _, input_queue in self._input_queues.items():\n                input_queue.put(None)",
  "def gather_global_ids(global_ids: List[torch.Tensor], group):\n    world_size = dist.get_world_size()\n    rank = dist.get_rank()\n\n    concat_global_ids = torch.cat(global_ids)\n\n    concat_numel = torch.tensor(concat_global_ids.numel(), dtype=torch.int64)\n    concat_numel_list = [torch.tensor(0, dtype=torch.int64) for _ in range(world_size)]\n    dist.all_gather(concat_numel_list, concat_numel, group=group, async_op=False)\n\n    max_numel = max(concat_numel_list)\n    concat_global_ids.resize_(max_numel)\n\n    if rank == 0:\n        concat_global_ids_list = [\n            torch.empty_like(concat_global_ids) for _ in range(world_size)\n        ]\n        dist.gather(concat_global_ids, concat_global_ids_list, 0, group, async_op=False)\n        return [\n            concat_global_ids_list[i][: concat_numel_list[i]] for i in range(world_size)\n        ], concat_numel_list\n    else:\n        dist.gather(concat_global_ids, None, 0, group, async_op=False)\n        return None, concat_numel_list",
  "def scatter_cache_ids(\n    cache_ids_list: Optional[List[torch.Tensor]], concat_numel_list: List[int], group\n):\n    world_size = dist.get_world_size()\n    rank = dist.get_rank()\n\n    max_numel = max(concat_numel_list)\n\n    concat_cache_ids = torch.empty(max_numel, dtype=torch.int64)\n    if rank == 0:\n        concat_cache_ids_list = [concat_cache_ids] + [\n            cache_ids.resize_(max_numel)\n            for cache_ids in cache_ids_list[-world_size + 1 :]\n        ]\n        assert len(concat_cache_ids_list) == world_size\n        dist.scatter(concat_cache_ids, concat_cache_ids_list, group=group)\n    else:\n        dist.scatter(concat_cache_ids, None, group=group)\n        offset = 0\n        for cache_ids in cache_ids_list:\n            cache_ids[:] = concat_cache_ids[offset : offset + cache_ids.numel()]\n            offset += cache_ids.numel()",
  "def broadcast_transform_result(\n    success: bool, ids_to_fetch: Optional[torch.Tensor], group\n):\n    if dist.get_rank() == 0:\n        success_and_numel = torch.tensor(\n            [1 if success else 0, ids_to_fetch.numel()], dtype=torch.int64\n        )\n        dist.broadcast(success_and_numel, src=0, group=group)\n    else:\n        success_and_numel = torch.tensor([0, 0], dtype=torch.int64)\n        dist.broadcast(success_and_numel, src=0, group=group)\n        success, numel = success_and_numel.tolist()\n        success = success != 0\n        ids_to_fetch = torch.empty((numel // 2, 2), dtype=torch.int64)\n\n    if ids_to_fetch.numel() > 0:\n        dist.broadcast(ids_to_fetch, src=0, group=group)\n    return success, ids_to_fetch",
  "def broadcast_ids_to_evict(ids, group):\n    if dist.get_rank() == 0:\n        numel = torch.tensor(ids.numel(), dtype=torch.int64)\n        dist.broadcast(numel, src=0, group=group)\n    else:\n        numel = torch.tensor(0, dtype=torch.int64)\n        dist.broadcast(numel, src=0, group=group)\n        numel = numel.item()\n        ids = torch.empty((numel // 2, 2), dtype=torch.int64)\n\n    if numel > 0:\n        dist.broadcast(ids, src=0, group=group)\n    return ids"
]