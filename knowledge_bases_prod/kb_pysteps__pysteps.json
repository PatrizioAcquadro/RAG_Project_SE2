[
  "def get_version():\n    \"\"\"\n    Returns project version as string from 'git describe' command.\n    \"\"\"\n\n    from subprocess import check_output\n    _version = check_output([\"git\", \"describe\", \"--tags\", \"--always\"])\n\n    if _version:\n        return _version.decode(\"utf-8\")\n    else:\n        return 'X.Y'",
  "def set_root():\n    fn = os.path.abspath(os.path.join(\"..\", \"..\", \"pysteps\", \"pystepsrc\"))\n    with open(fn, \"r\") as f:\n        rcparams = json.loads(jsmin(f.read()))\n\n    for key, value in rcparams[\"data_sources\"].items():\n        original_path = value[\"root_path\"]\n\n        new_path = os.path.join(\"..\", \"..\", \"pysteps-data\", value[\"root_path\"])\n        new_path = os.path.abspath(new_path)\n\n        value[\"root_path\"] = new_path\n\n    fn = os.path.abspath(os.path.join(\"..\", \"..\", \"pystepsrc.rtd\"))\n    with open(fn, \"w\") as f:\n        json.dump(rcparams, f, indent=4)",
  "class MissingOptionalDependency(Exception):\n    \"\"\"Raised when an optional dependency is needed but not found.\"\"\"\n    pass",
  "class DataModelError(Exception):\n    \"\"\"Raised when a file is not compilant with the Data Information Model.\"\"\"\n    pass",
  "class UnsupportedSomercProjection(Exception):\n    \"\"\"\n    Raised when the Swiss Oblique Mercator (somerc) projection is passed\n    to cartopy.\n    Necessary since cartopy doesn't support the Swiss projection.\n    TODO: remove once the somerc projection is supported in cartopy.\n    \"\"\"\n    pass",
  "def check_input_frames(minimum_input_frames=2,\n                       maximum_input_frames=np.inf,\n                       just_ndim=False):\n    \"\"\"\n    Check that the input_images used as inputs in the optical-flow\n    methods has the correct shape (t, x, y ).\n    \"\"\"\n\n    def _check_input_frames(motion_method_func):\n        @wraps(motion_method_func)\n        def new_function(*args, **kwargs):\n            \"\"\"\n            Return new function with the checks prepended to the\n            target motion_method_func function.\n            \"\"\"\n\n            input_images = args[0]\n            if input_images.ndim != 3:\n                raise ValueError(\n                    \"input_images dimension mismatch.\\n\"\n                    f\"input_images.shape: {str(input_images.shape)}\\n\"\n                    \"(t, x, y ) dimensions expected\"\n                )\n\n            if not just_ndim:\n                num_of_frames = input_images.shape[0]\n\n                if minimum_input_frames < num_of_frames > maximum_input_frames:\n                    raise ValueError(\n                        f\"input_images frames {num_of_frames} mismatch.\\n\"\n                        f\"Minimum frames: {minimum_input_frames}\\n\"\n                        f\"Maximum frames: {maximum_input_frames}\\n\"\n                    )\n\n            return motion_method_func(*args, **kwargs)\n\n        return new_function\n\n    return _check_input_frames",
  "def _check_input_frames(motion_method_func):\n        @wraps(motion_method_func)\n        def new_function(*args, **kwargs):\n            \"\"\"\n            Return new function with the checks prepended to the\n            target motion_method_func function.\n            \"\"\"\n\n            input_images = args[0]\n            if input_images.ndim != 3:\n                raise ValueError(\n                    \"input_images dimension mismatch.\\n\"\n                    f\"input_images.shape: {str(input_images.shape)}\\n\"\n                    \"(t, x, y ) dimensions expected\"\n                )\n\n            if not just_ndim:\n                num_of_frames = input_images.shape[0]\n\n                if minimum_input_frames < num_of_frames > maximum_input_frames:\n                    raise ValueError(\n                        f\"input_images frames {num_of_frames} mismatch.\\n\"\n                        f\"Minimum frames: {minimum_input_frames}\\n\"\n                        f\"Maximum frames: {maximum_input_frames}\\n\"\n                    )\n\n            return motion_method_func(*args, **kwargs)\n\n        return new_function",
  "def new_function(*args, **kwargs):\n            \"\"\"\n            Return new function with the checks prepended to the\n            target motion_method_func function.\n            \"\"\"\n\n            input_images = args[0]\n            if input_images.ndim != 3:\n                raise ValueError(\n                    \"input_images dimension mismatch.\\n\"\n                    f\"input_images.shape: {str(input_images.shape)}\\n\"\n                    \"(t, x, y ) dimensions expected\"\n                )\n\n            if not just_ndim:\n                num_of_frames = input_images.shape[0]\n\n                if minimum_input_frames < num_of_frames > maximum_input_frames:\n                    raise ValueError(\n                        f\"input_images frames {num_of_frames} mismatch.\\n\"\n                        f\"Minimum frames: {minimum_input_frames}\\n\"\n                        f\"Maximum frames: {maximum_input_frames}\\n\"\n                    )\n\n            return motion_method_func(*args, **kwargs)",
  "def _get_config_file_schema():\n    \"\"\"\n    Return the path to the parameters file json schema.\n    \"\"\"\n    module_file = _decode_filesystem_path(__file__)\n    return os.path.join(os.path.dirname(module_file), 'pystepsrc_schema.json')",
  "def _fconfig_candidates_generator():\n    \"\"\"\n    Configuration files candidates generator.\n\n    See :py:func:~config_fname for more details.\n    \"\"\"\n\n    yield os.path.join(os.getcwd(), 'pystepsrc')\n\n    try:\n        pystepsrc = os.environ['PYSTEPSRC']\n    except KeyError:\n        pass\n    else:\n        yield pystepsrc\n        yield os.path.join(pystepsrc, 'pystepsrc')\n\n    if os.name == \"nt\":\n        # Windows environment\n        env_variable = 'USERPROFILE'\n        subdir = 'pysteps'\n    else:\n        # UNIX like\n        env_variable = 'HOME'\n        subdir = '.pysteps'\n\n    try:\n        pystepsrc = os.environ[env_variable]\n    except KeyError:\n        pass\n    else:\n        yield os.path.join(pystepsrc, subdir, 'pystepsrc')\n\n    module_file = _decode_filesystem_path(__file__)\n    yield os.path.join(os.path.dirname(module_file), 'pystepsrc')\n    yield None",
  "def config_fname():\n    \"\"\"\n    Get the location of the config file.\n\n    Looks for pystepsrc file in the following order:\n    - $PWD/pystepsrc : Looks for the file in the current directory\n    - $PYSTEPSRC : If the system variable $PYSTEPSRC is defined and it points\n    to a file, it is used..\n    - $PYSTEPSRC/pystepsrc : If $PYSTEPSRC points to a directory, it looks for\n    the pystepsrc file inside that directory.\n    - $HOME/.pysteps/pystepsrc (unix and Mac OS X) :\n    If the system variable $HOME is defined, it looks\n    for the configuration file in this path.\n    - $USERPROFILE/pysteps/pystepsrc (windows only): It looks for the\n    configuration file in the pysteps directory located user's home directory.\n    - Lastly, it looks inside the library in pysteps/pystepsrc for a\n    system-defined copy.\n    \"\"\"\n\n    file_name = None\n    for file_name in _fconfig_candidates_generator():\n\n        if file_name is not None:\n            if os.path.exists(file_name):\n                st_mode = os.stat(file_name).st_mode\n                if stat.S_ISREG(st_mode) or stat.S_ISFIFO(st_mode):\n                    return file_name\n\n            # Return first candidate that is a file,\n            # or last candidate if none is valid\n            # (in that case, a warning is raised at startup by `rc_params`).\n\n    return file_name",
  "def _decode_filesystem_path(path):\n    if not isinstance(path, str):\n        return path.decode(sys.getfilesystemencoding())\n    else:\n        return path",
  "class DotDictify(AttrDict):\n    \"\"\"\n    Class used to recursively access dict via attributes as well\n    as index access.\n    This is introduced to maintain backward compatibility with older pysteps\n    configuration parameters implementations.\n\n    Code adapted from:\n    https://stackoverflow.com/questions/3031219/recursively-access-dict-via-attributes-as-well-as-index-access\n\n    Credits: `Curt Hagenlocher`_\n\n    .. _`Curt Hagenlocher`: https://stackoverflow.com/users/533/curt-hagenlocher\n    \"\"\"\n\n    def __setitem__(self, key, value):\n        if isinstance(value, dict) and not isinstance(value, DotDictify):\n            value = DotDictify(value)\n        super().__setitem__(key, value)\n\n    def __getitem__(self, key):\n        value = super().__getitem__(key)\n        if isinstance(value, dict) and not isinstance(value, DotDictify):\n            value = DotDictify(value)\n            super().__setitem__(key, value)\n        return value\n\n    __setattr__, __getattr__ = __setitem__, __getitem__",
  "def __setitem__(self, key, value):\n        if isinstance(value, dict) and not isinstance(value, DotDictify):\n            value = DotDictify(value)\n        super().__setitem__(key, value)",
  "def __getitem__(self, key):\n        value = super().__getitem__(key)\n        if isinstance(value, dict) and not isinstance(value, DotDictify):\n            value = DotDictify(value)\n            super().__setitem__(key, value)\n        return value",
  "def DARTS(input_images, **kwargs):\n    \"\"\"Compute the advection field from a sequence of input images by using the\n    DARTS method. :cite:`RCW2011`\n\n    Parameters\n    ----------\n    input_images : array-like\n      Array of shape (T,m,n) containing a sequence of T two-dimensional input\n      images of shape (m,n).\n\n    Other Parameters\n    ----------------\n    N_x : int\n        Number of DFT coefficients to use for the input images, x-axis (default=50).\n    N_y : int\n        Number of DFT coefficients to use for the input images, y-axis (default=50).\n    N_t : int\n        Number of DFT coefficients to use for the input images, time axis (default=4).\n        N_t must be strictly smaller than T.\n    M_x : int\n        Number of DFT coefficients to compute for the output advection field,\n        x-axis  (default=2).\n    M_y : int\n        Number of DFT coefficients to compute for the output advection field,\n        y-axis (default=2).\n    fft_method : str\n        A string defining the FFT method to use, see utils.fft.get_method.\n        Defaults to 'numpy'.\n    output_type : {\"spatial\", \"spectral\"}\n        The type of the output: \"spatial\"=apply the inverse FFT to obtain the\n        spatial representation of the advection field, \"spectral\"=return the\n        (truncated) DFT representation.\n    n_threads : int\n        Number of threads to use for the FFT computation. Applicable if\n        fft_method is 'pyfftw'.\n    verbose : bool\n        If True, print information messages.\n    lsq_method : {1, 2}\n        The method to use for solving the linear equations in the least squares\n        sense: 1=numpy.linalg.lstsq, 2=explicit computation of the Moore-Penrose\n        pseudoinverse and SVD.\n    verbose : bool\n        if set to True, it prints information about the program\n\n    Returns\n    -------\n    out : ndarray\n        Three-dimensional array (2,m,n) containing the dense x- and y-components\n        of the motion field in units of pixels / timestep as given by the input\n        array R.\n\n    \"\"\"\n\n    N_x = kwargs.get(\"N_x\", 50)\n    N_y = kwargs.get(\"N_y\", 50)\n    N_t = kwargs.get(\"N_t\", 4)\n    M_x = kwargs.get(\"M_x\", 2)\n    M_y = kwargs.get(\"M_y\", 2)\n    fft_method = kwargs.get(\"fft_method\", \"numpy\")\n    output_type = kwargs.get(\"output_type\", \"spatial\")\n    lsq_method = kwargs.get(\"lsq_method\", 2)\n    verbose = kwargs.get(\"verbose\", True)\n\n    if N_t >= input_images.shape[0]:\n        raise ValueError(\"N_t = %d >= %d = T, but N_t < T required\" % (N_t, input_images.shape[0]))\n\n    if output_type not in [\"spatial\", \"spectral\"]:\n        raise ValueError(\"invalid output_type=%s, must be 'spatial' or 'spectral'\" % output_type)\n\n    if verbose:\n        print(\"Computing the motion field with the DARTS method.\")\n        t0 = time.time()\n\n    input_images = np.moveaxis(input_images, (0, 1, 2), (2, 0, 1))\n\n    fft = utils.get_method(fft_method, shape=input_images.shape[:2], fftn_shape=input_images.shape,\n                           **kwargs)\n\n    T_x = input_images.shape[1]\n    T_y = input_images.shape[0]\n    T_t = input_images.shape[2]\n\n    if verbose:\n        print(\"-----\")\n        print(\"DARTS\")\n        print(\"-----\")\n\n        print(\"  Computing the FFT of the reflectivity fields...\"),\n        sys.stdout.flush()\n        starttime = time.time()\n\n    input_images = fft.fftn(input_images)\n\n    if verbose:\n        print(\"Done in %.2f seconds.\" % (time.time() - starttime))\n\n        print(\"  Constructing the y-vector...\"),\n        sys.stdout.flush()\n        starttime = time.time()\n\n    m = (2 * N_x + 1) * (2 * N_y + 1) * (2 * N_t + 1)\n    n = (2 * M_x + 1) * (2 * M_y + 1)\n\n    y = np.zeros(m, dtype=complex)\n\n    k_t, k_y, k_x = np.unravel_index(np.arange(m), (2 * N_t + 1, 2 * N_y + 1, 2 * N_x + 1))\n\n    for i in range(m):\n        k_x_ = k_x[i] - N_x\n        k_y_ = k_y[i] - N_y\n        k_t_ = k_t[i] - N_t\n\n        y[i] = k_t_ * input_images[k_y_, k_x_, k_t_]\n\n    if verbose:\n        print(\"Done in %.2f seconds.\" % (time.time() - starttime))\n\n    A = np.zeros((m, n), dtype=complex)\n    B = np.zeros((m, n), dtype=complex)\n\n    if verbose:\n        print(\"  Constructing the H-matrix...\"),\n        sys.stdout.flush()\n        starttime = time.time()\n\n    c1 = -1.0 * T_t / (T_x * T_y)\n\n    kp_y, kp_x = np.unravel_index(np.arange(n), (2 * M_y + 1, 2 * M_x + 1))\n\n    for i in range(m):\n        k_x_ = k_x[i] - N_x\n        k_y_ = k_y[i] - N_y\n        k_t_ = k_t[i] - N_t\n\n        kp_x_ = kp_x[:] - M_x\n        kp_y_ = kp_y[:] - M_y\n\n        i_ = k_y_ - kp_y_\n        j_ = k_x_ - kp_x_\n\n        R_ = input_images[i_, j_, k_t_]\n\n        c2 = c1 / T_y * i_\n        A[i, :] = c2 * R_\n\n        c2 = c1 / T_x * j_\n        B[i, :] = c2 * R_\n\n    if verbose:\n        print(\"Done in %.2f seconds.\" % (time.time() - starttime))\n\n        print(\"  Solving the linear systems...\"),\n        sys.stdout.flush()\n        starttime = time.time()\n\n    if lsq_method == 1:\n        x = lstsq(np.hstack([A, B]), y, rcond=0.01)[0]\n    else:\n        x = _leastsq(A, B, y)\n\n    if verbose:\n        print(\"Done in %.2f seconds.\" % (time.time() - starttime))\n\n    h, w = 2 * M_y + 1, 2 * M_x + 1\n\n    U = np.zeros((h, w), dtype=complex)\n    V = np.zeros((h, w), dtype=complex)\n\n    i, j = np.unravel_index(np.arange(h * w), (h, w))\n\n    V[i, j] = x[0:h * w]\n    U[i, j] = x[h * w:2 * h * w]\n\n    k_x, k_y = np.meshgrid(np.arange(-M_x, M_x + 1), np.arange(-M_y, M_y + 1))\n\n    if output_type == \"spatial\":\n        U = np.real(fft.ifft2(_fill(U, input_images.shape[0], input_images.shape[1], k_x, k_y)))\n        V = np.real(fft.ifft2(_fill(V, input_images.shape[0], input_images.shape[1], k_x, k_y)))\n\n    if verbose:\n        print(\"--- %s seconds ---\" % (time.time() - t0))\n\n    return np.stack([U, V])",
  "def _leastsq(A, B, y):\n    M = np.hstack([A, B])\n    M_ct = M.conjugate().T\n    MM = np.dot(M_ct, M)\n\n    U, s, V = svd(MM, full_matrices=False)\n\n    mask = s > 0.01 * s[0]\n    s = 1.0 / s[mask]\n\n    MM_inv = np.dot(np.dot(V[:len(s), :].conjugate().T, np.diag(s)),\n                    U[:, :len(s)].conjugate().T)\n\n    return np.dot(MM_inv, np.dot(M_ct, y))",
  "def _fill(X, h, w, k_x, k_y):\n    X_f = np.zeros((h, w), dtype=complex)\n    X_f[k_y, k_x] = X\n\n    return X_f",
  "def proesmans(input_images, lam=50.0, num_iter=100,\n              num_levels=6, filter_std=0.0, verbose=True, ):\n    \"\"\"Implementation of the anisotropic diffusion method of Proesmans et al.\n    (1994).\n\n    Parameters\n    ----------\n    input_images : array_like\n        Array of shape (2, m, n) containing the first and second input image.\n    lam : float\n        Multiplier of the smoothness term. Smaller values give a smoother motion\n        field.\n    num_iter : float\n        The number of iterations to use.\n    num_levels : int\n        The number of image pyramid levels to use.\n    filter_std : float\n        Standard deviation of an optional Gaussian filter that is applied before\n        computing the optical flow.\n    verbose : bool, optional\n        Verbosity enabled if True (default).\n\n    Returns\n    -------\n    out : ndarray\n        The advection field having shape (2, m, n), where out[0, :, :] contains\n        the x-components of the motion vectors and out[1, :, :] contains the\n        y-components. The velocities are in units of pixels / timestep, where\n        timestep is the time difference between the two input images.\n\n    References\n    ----------\n    :cite:`PGPO1994`\n\n    \"\"\"\n    del verbose  # Not used\n\n    im1 = input_images[-2, :, :].copy()\n    im2 = input_images[-1, :, :].copy()\n\n    im = np.stack([im1, im2])\n    im_min = np.min(im)\n    im_max = np.max(im)\n    if im_max - im_min > 1e-8:\n        im = (im - im_min) / (im_max - im_min) * 255.0\n\n    if filter_std > 0.0:\n        im[0, :, :] = gaussian_filter(im[0, :, :], filter_std)\n        im[1, :, :] = gaussian_filter(im[1, :, :], filter_std)\n\n    return _compute_advection_field(im, lam, num_iter, num_levels)",
  "def round_int(scalar):\n    \"\"\"\n    Round number to nearest integer. Returns and integer value.\n    \"\"\"\n    return int(numpy.round(scalar))",
  "def ceil_int(scalar):\n    \"\"\"\n    Round number to nearest integer. Returns and integer value.\n    \"\"\"\n    return int(numpy.ceil(scalar))",
  "def get_padding(dimension_size, sectors):\n    \"\"\"\n    Get the padding at each side of the one dimensions of the image\n    so the new image dimensions are divided evenly in the\n    number of *sectors* specified.\n\n    Parameters\n    ----------\n\n    dimension_size : int\n        Actual dimension size.\n\n    sectors : int\n        number of sectors over which the the image will be divided.\n\n    Returns\n    -------\n\n    pad_before , pad_after: int, int\n        Padding at each side of the image for the corresponding dimension.\n    \"\"\"\n    reminder = dimension_size % sectors\n\n    if reminder != 0:\n        pad = sectors - reminder\n        pad_before = pad // 2\n        if pad % 2 == 0:\n            pad_after = pad_before\n        else:\n            pad_after = pad_before + 1\n\n        return pad_before, pad_after\n\n    return 0, 0",
  "def morph(image, displacement, gradient=False):\n    \"\"\"\n    Morph image by applying a displacement field (Warping).\n\n    The new image is created by selecting for each position the values of the\n    input image at the positions given by the x and y displacements.\n    The routine works in a backward sense.\n    The displacement vectors have to refer to their destination.\n\n    For more information in Morphing functions see Section 3 in\n    `Beezley and Mandel (2008)`_.\n\n    Beezley, J. D., & Mandel, J. (2008).\n    Morphing ensemble Kalman filters. Tellus A, 60(1), 131-140.\n\n    .. _`Beezley and Mandel (2008)`: http://dx.doi.org/10.1111/\\\n    j.1600-0870.2007.00275.x\n\n\n    The displacement field in x and y directions and the image must have the\n    same dimensions.\n\n    The morphing is executed in parallel over x axis.\n\n    The value of displaced pixels that fall outside the limits takes the\n    value of the nearest edge. Those pixels are indicated by values greater\n    than 1 in the output mask.\n\n    Parameters\n    ----------\n\n    image : ndarray (ndim = 2)\n        Image to morph\n\n    displacement : ndarray (ndim = 3)\n        Displacement field to be applied (Warping). The first dimension\n        corresponds to the coordinate to displace.\n\n        The dimensions are: displacement [ i/x (0) or j/y (1) ,\n        i index of pixel, j index of pixel ]\n\n\n    gradient : bool, optional\n        If True, the gradient of the morphing function is returned.\n\n\n    Returns\n    -------\n\n    image : ndarray (float64 ,ndim = 2)\n        Morphed image.\n\n    mask : ndarray (int8 ,ndim = 2)\n        Invalid values mask. Points outside the boundaries are masked.\n        Values greater than 1, indicate masked values.\n\n    gradient_values : ndarray (float64 ,ndim = 3), optional\n        If gradient keyword is True, the gradient of the function is also\n        returned.\n\n    \"\"\"\n\n    if not isinstance(image, MaskedArray):\n        _mask = numpy.zeros_like(image, dtype='int8')\n    else:\n        _mask = numpy.asarray(numpy.ma.getmaskarray(image),\n                              dtype='int8',\n                              order='C')\n\n    _image = numpy.asarray(image, dtype='float64', order='C')\n    _displacement = numpy.asarray(displacement, dtype='float64', order='C')\n\n    return _warp(_image, _mask, _displacement, gradient=gradient)",
  "def vet_cost_function_gradient(*args, **kwargs):\n    \"\"\"Compute the vet cost function gradient.\n    See :py:func:`vet_cost_function` for more information.\n    \"\"\"\n    kwargs[\"gradient\"] = True\n    return vet_cost_function(*args, **kwargs)",
  "def vet_cost_function(sector_displacement_1d,\n                      input_images,\n                      blocks_shape,\n                      mask,\n                      smooth_gain,\n                      debug=False,\n                      gradient=False):\n    \"\"\"\n    .. _`scipy minimization`: \\\n    https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html\n    \n    Variational Echo Tracking Cost Function.\n\n    This function is designed to be used with the `scipy minimization`_.\n    The function first argument is the variable to be used in the\n    minimization procedure.\n\n    The sector displacement must be a flat array compatible with the\n    dimensions of the input image and sectors shape (see parameters section\n    below for more details).\n\n\n\n    .. _ndarray:\\\n    https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html\n\n\n    Parameters\n    ----------\n\n    sector_displacement_1d : ndarray_\n        Array of displacements to apply to each sector. The dimensions are:\n        sector_displacement_2d\n        [ x (0) or y (1) displacement, i index of sector, j index of sector ].\n        The shape of the sector displacements must be compatible with the\n        input image and the block shape.\n        The shape should be (2, mx, my) where mx and my are the numbers of\n        sectors in the x and the y dimension.\n\n    input_images : ndarray_\n        Input images, sequence of 2D arrays, or 3D arrays.\n        The first dimension represents the images time dimension.\n\n        The template_image (first element in first dimensions) denotes the\n        reference image used to obtain the displacement (2D array).\n        The second is the target image.\n\n        The expected dimensions are (2,nx,ny).\n        Be aware the the 2D images dimensions correspond to (lon,lat) or (x,y).\n\n    blocks_shape : ndarray_ (ndim=2)\n        Number of sectors in each dimension (x and y).\n        blocks_shape.shape = (mx,my)\n\n    mask : ndarray_ (ndim=2)\n        Data mask. If is True, the data is marked as not valid and is not\n        used in the computations.\n\n    smooth_gain : float\n        Smoothness constrain gain\n\n    debug : bool, optional\n        If True, print debugging information.\n\n    gradient : bool, optional\n        If True, the gradient of the morphing function is returned.\n\n    Returns\n    -------\n\n    penalty or  gradient values.\n\n    penalty : float\n        Value of the cost function\n\n    gradient_values : ndarray (float64 ,ndim = 3), optional\n        If gradient keyword is True, the gradient of the function is also\n        returned.\n\n    \"\"\"\n\n    sector_displacement_2d = \\\n        sector_displacement_1d.reshape(*((2,) + tuple(blocks_shape)))\n\n    if input_images.shape[0] == 3:\n        three_times = True\n        previous_image = input_images[0]\n        center_image = input_images[1]\n        next_image = input_images[2]\n\n    else:\n        previous_image = None\n        center_image = input_images[0]\n        next_image = input_images[1]\n        three_times = False\n\n    if gradient:\n        gradient_values = _cost_function(sector_displacement_2d,\n                                         center_image,\n                                         next_image,\n                                         mask,\n                                         smooth_gain,\n                                         gradient=True)\n        if three_times:\n            gradient_values += _cost_function(sector_displacement_2d,\n                                              previous_image,\n                                              center_image,\n                                              mask,\n                                              smooth_gain,\n                                              gradient=True)\n\n        return gradient_values.ravel()\n\n    else:\n        residuals, smoothness_penalty = _cost_function(sector_displacement_2d,\n                                                       center_image,\n                                                       next_image,\n                                                       mask,\n                                                       smooth_gain,\n                                                       gradient=False)\n\n        if three_times:\n            _residuals, _smoothness = _cost_function(sector_displacement_2d,\n                                                     previous_image,\n                                                     center_image,\n                                                     mask,\n                                                     smooth_gain,\n                                                     gradient=False)\n\n            residuals += _residuals\n            smoothness_penalty += _smoothness\n\n        if debug:\n            print(\"\\nresiduals\", residuals)\n            print(\"smoothness_penalty\", smoothness_penalty)\n\n        return residuals + smoothness_penalty",
  "def vet(input_images,\n        sectors=((32, 16, 4, 2), (32, 16, 4, 2)),\n        smooth_gain=1e6,\n        first_guess=None,\n        intermediate_steps=False,\n        verbose=True,\n        indexing='yx',\n        padding=0,\n        options=None):\n    \"\"\"\n    Variational Echo Tracking Algorithm presented in\n    `Laroche and Zawadzki (1995)`_  and used in the McGill Algorithm for\n    Prediction by Lagrangian Extrapolation (MAPLE) described in\n    `Germann and Zawadzki (2002)`_.\n\n    .. _`Laroche and Zawadzki (1995)`:\\\n        http://dx.doi.org/10.1175/1520-0426(1995)012<0721:ROHWFS>2.0.CO;2\n\n    .. _`Germann and Zawadzki (2002)`:\\\n        http://dx.doi.org/10.1175/1520-0493(2002)130<2859:SDOTPO>2.0.CO;2\n\n    This algorithm computes the displacement field between two images\n    ( the input_image with respect to the template image).\n    The displacement is sought by minimizing the sum of the residuals of the\n    squared differences of the images pixels and the contribution of a\n    smoothness constraint.\n    In the case that a MaskedArray is used as input, the residuals term in\n    the cost function is only computed over areas with non-masked values.\n    Otherwise, it is computed over the entire domain.\n\n    To find the minimum, a scaling guess procedure is applied,\n    from larger to smaller scales.\n    This reduces the chances that the minimization procedure\n    converges to a local minimum.\n    The first scaling guess is defined by the scaling sectors keyword.\n\n    The smoothness of the returned displacement field is controlled by the\n    smoothness constraint gain (**smooth_gain** keyword).\n\n    If a first guess is not given, zero displacements are used as the first\n    guess.\n\n    The cost function is minimized using the `scipy minimization`_ function,\n    with the 'CG' method by default.\n    This method proved to give the best results under many different conditions\n    and is the most similar one to the original VET implementation in\n    `Laroche and Zawadzki (1995)`_.\n\n\n    The method CG uses a nonlinear conjugate gradient algorithm by Polak and\n    Ribiere, a variant of the Fletcher-Reeves method described in\n    Nocedal and Wright (2006), pp. 120-122.\n\n    .. _`scipy minimization`: \\\n    https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html\n\n    .. _MaskedArray: https://docs.scipy.org/doc/numpy/reference/\\\n        maskedarray.baseclass.html#numpy.ma.MaskedArray\n\n    .. _ndarray:\\\n    https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html\n\n    Parameters\n    ----------\n\n    input_images : ndarray_ or MaskedArray\n        Input images, sequence of 2D arrays, or 3D arrays.\n        The first dimension represents the images time dimension.\n\n        The template_image (first element in first dimensions) denotes the\n        reference image used to obtain the displacement (2D array).\n        The second is the target image.\n\n        The expected dimensions are (2,ni,nj).\n\n    sectors : list or array, optional\n        Number of sectors on each dimension used in the scaling procedure.\n        If dimension is 1, the same sectors will be used both image dimensions\n        (x and y). If **sectors** is a 1D array, the same number of sectors\n        is used in both dimensions.\n\n    smooth_gain : float, optional\n        Smooth gain factor\n\n    first_guess : ndarray_, optional\n        The shape of the first guess should have the same shape as the initial\n        sectors shapes used in the scaling procedure.\n        If first_guess is not present zeros are used as first guess.\n\n        E.g.:\n            If the first sector shape in the scaling procedure is (ni,nj), then\n            the first_guess should have (2, ni, nj ) shape.\n\n    intermediate_steps : bool, optional\n        If True, also return a list with the first guesses obtained during the\n        scaling procedure. False, by default.\n\n    verbose : bool, optional\n        Verbosity enabled if True (default).\n\n    indexing : str, optional\n        Input indexing order.'ij' and 'xy' indicates that the\n        dimensions of the input are (time, longitude, latitude), while\n        'yx' indicates (time, latitude, longitude).\n        The displacement field dimensions are ordered accordingly in a way that\n        the first dimension indicates the displacement along x (0) or y (1).\n        That is, UV displacements are always returned.\n\n    padding : int\n        Padding width in grid points. A border is added to the input array\n        to reduce the effects of the minimization at the border.\n\n    options : dict, optional\n        A dictionary of solver options.\n        See `scipy minimization`_ function for more details.\n\n    Returns\n    -------\n\n    displacement_field : ndarray_\n        Displacement Field (2D array representing the transformation) that\n        warps the template image into the input image.\n        The dimensions are (2,ni,nj), where the first\n        dimension indicates the displacement along x (0) or y (1) in units of\n        pixels / timestep as given by the input_images array.\n\n    intermediate_steps : list of ndarray_\n        List with the first guesses obtained during the scaling procedure.\n\n    References\n    ----------\n\n    Laroche, S., and I. Zawadzki, 1995:\n    Retrievals of horizontal winds from single-Doppler clear-air data by\n    methods of cross-correlation and variational analysis.\n    J. Atmos. Oceanic Technol., 12, 721\u2013738.\n    doi: http://dx.doi.org/10.1175/1520-0426(1995)012<0721:ROHWFS>2.0.CO;2\n\n    Germann, U. and I. Zawadzki, 2002:\n    Scale-Dependence of the Predictability of Precipitation from Continental\n    Radar Images.  Part I: Description of the Methodology.\n    Mon. Wea. Rev., 130, 2859\u20132873,\n    doi: 10.1175/1520-0493(2002)130<2859:SDOTPO>2.0.CO;2.\n\n    Nocedal, J, and S J Wright. 2006. Numerical Optimization. Springer New York.\n\n    \"\"\"\n\n    if verbose:\n        def debug_print(*args, **kwargs):\n            print(*args, **kwargs)\n    else:\n        def debug_print(*args, **kwargs):\n            del args\n            del kwargs\n\n    if options is None:\n        options = dict()\n    else:\n        options = dict(options)\n\n    options.setdefault('eps', 0.1)\n    options.setdefault('gtol', 0.1)\n    options.setdefault('maxiter', 100)\n    options.setdefault('disp', False)\n\n    # Set to None to suppress pylint warning.\n    pad_i = None\n    pad_j = None\n    sectors_in_i = None\n    sectors_in_j = None\n\n    debug_print(\"Running VET algorithm\")\n\n    valid_indexing = ['yx', 'xy', 'ij']\n\n    if indexing not in valid_indexing:\n        raise ValueError(\"Invalid indexing values: {0}\\n\".format(indexing)\n                         + \"Supported values: {0}\".format(str(valid_indexing)))\n\n    # Get mask\n    if isinstance(input_images, MaskedArray):\n        mask = numpy.ma.getmaskarray(input_images)\n    else:\n        # Mask invalid data\n        if padding > 0:\n            padding_tuple = ((0, 0), (padding, padding), (padding, padding))\n\n            input_images = numpy.pad(input_images,\n                                     padding_tuple,\n                                     'constant',\n                                     constant_values=numpy.nan)\n\n        input_images = numpy.ma.masked_invalid(input_images)\n        mask = numpy.ma.getmaskarray(input_images)\n\n    input_images.data[mask] = 0  # Remove any Nan from the raw data\n\n    # Create a 2D mask with the right data type for _vet\n    mask = numpy.asarray(numpy.any(mask, axis=0), dtype='int8', order='C')\n\n    input_images = numpy.asarray(input_images.data, dtype='float64', order='C')\n\n    # Check that the sectors divide the domain\n    sectors = numpy.asarray(sectors, dtype=\"int\", order='C')\n\n    if sectors.ndim == 1:\n\n        new_sectors = (numpy.zeros((2,) + sectors.shape, dtype='int', order='C')\n                       + sectors.reshape((1, sectors.shape[0]))\n                       )\n        sectors = new_sectors\n    elif sectors.ndim > 2 or sectors.ndim < 1:\n        raise ValueError(\"Incorrect sectors dimensions.\\n\"\n                         + \"Only 1D or 2D arrays are supported to define\"\n                         + \"the number of sectors used in\"\n                         + \"the scaling procedure\")\n\n    # Sort sectors in descending order\n    sectors[0, :].sort()\n    sectors[1, :].sort()\n\n    # Prepare first guest\n    first_guess_shape = (2, int(sectors[0, 0]), int(sectors[1, 0]))\n\n    if first_guess is None:\n        first_guess = numpy.zeros(first_guess_shape, order='C')\n    else:\n        if first_guess.shape != first_guess_shape:\n            raise ValueError(\n                \"The shape of the initial guess do not match the number of \"\n                + \"sectors of the first scaling guess\\n\"\n                + \"first_guess.shape={}\\n\".format(str(first_guess.shape))\n                + \"Expected shape={}\".format(str(first_guess_shape)))\n        else:\n            first_guess = numpy.asarray(first_guess, order='C', dtype='float64')\n\n    scaling_guesses = list()\n\n    previous_sectors_in_i = sectors[0, 0]\n    previous_sectors_in_j = sectors[1, 0]\n\n    for n, (sectors_in_i, sectors_in_j) in enumerate(zip(sectors[0, :],\n                                                         sectors[1, :])):\n\n        # Minimize for each sector size\n        pad_i = get_padding(input_images.shape[1], sectors_in_i)\n        pad_j = get_padding(input_images.shape[2], sectors_in_j)\n\n        if (pad_i != (0, 0)) or (pad_j != (0, 0)):\n\n            _input_images = numpy.pad(input_images, ((0, 0), pad_i, pad_j),\n                                      'edge')\n\n            _mask = numpy.pad(mask, (pad_i, pad_j),\n                              'constant',\n                              constant_values=1)\n            _mask = numpy.ascontiguousarray(_mask)\n            if first_guess is None:\n                first_guess = numpy.pad(first_guess,\n                                        ((0, 0), pad_i, pad_j),\n                                        'edge')\n                first_guess = numpy.ascontiguousarray(first_guess)\n\n        else:\n            _input_images = input_images\n            _mask = mask\n\n        sector_shape = (_input_images.shape[1] // sectors_in_i,\n                        _input_images.shape[2] // sectors_in_j)\n\n        debug_print(\"original image shape: \" + str(input_images.shape))\n        debug_print(\"padded image shape: \" + str(_input_images.shape))\n        debug_print(\"padded template_image image shape: \"\n                    + str(_input_images.shape))\n\n        debug_print(\"\\nNumber of sectors: {0:d},{1:d}\".format(sectors_in_i,\n                                                              sectors_in_j))\n\n        debug_print(\"Sector Shape:\", sector_shape)\n\n        if n > 0:\n            first_guess = zoom(first_guess,\n                               (1,\n                                sectors_in_i / previous_sectors_in_i,\n                                sectors_in_j / previous_sectors_in_j),\n                               order=1, mode='nearest')\n\n        debug_print(\"Minimizing\")\n\n        result = minimize(vet_cost_function,\n                          first_guess.flatten(),\n                          jac=vet_cost_function_gradient,\n                          args=(_input_images,\n                                (sectors_in_i, sectors_in_j),\n                                _mask,\n                                smooth_gain),\n                          method='CG',\n                          options=options)\n\n        first_guess = result.x.reshape(*first_guess.shape)\n\n        if verbose:\n            vet_cost_function(result.x,\n                              _input_images,\n                              (sectors_in_i, sectors_in_j),\n                              _mask,\n                              smooth_gain,\n                              debug=True)\n        if indexing == 'yx':\n            scaling_guesses.append(first_guess[::-1, ...])\n        else:\n            scaling_guesses.append(first_guess)\n\n        previous_sectors_in_i = sectors_in_i\n        previous_sectors_in_j = sectors_in_j\n\n    first_guess = zoom(first_guess,\n                       (1,\n                        _input_images.shape[1] / sectors_in_i,\n                        _input_images.shape[2] / sectors_in_j),\n                       order=1, mode='nearest')\n\n    first_guess = numpy.ascontiguousarray(first_guess)\n    # Remove the extra padding if any\n    ni = _input_images.shape[1]\n    nj = _input_images.shape[2]\n\n    first_guess = first_guess[:, pad_i[0]:ni - pad_i[1], pad_j[0]:nj - pad_j[1]]\n\n    if indexing == 'yx':\n        first_guess = first_guess[::-1, ...]\n\n    if padding > 0:\n        first_guess = first_guess[:, padding:-padding, padding:-padding]\n\n    if intermediate_steps:\n        return first_guess, scaling_guesses\n\n    return first_guess",
  "def debug_print(*args, **kwargs):\n            print(*args, **kwargs)",
  "def debug_print(*args, **kwargs):\n            del args\n            del kwargs",
  "def dense_lucaskanade(input_images,\n                      lk_kwargs=None,\n                      fd_method=\"ShiTomasi\",\n                      fd_kwargs=None,\n                      interp_method=\"rbfinterp2d\",\n                      interp_kwargs=None,\n                      dense=True,\n                      nr_std_outlier=3,\n                      k_outlier=30,\n                      size_opening=3,\n                      decl_scale=10,\n                      verbose=False):\n    \"\"\"Run the Lucas-Kanade optical flow routine and interpolate the motion\n    vectors.\n\n    .. _OpenCV: https://opencv.org/\n\n    .. _`Lucas-Kanade`:\\\n        https://docs.opencv.org/3.4/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323\n\n    .. _MaskedArray:\\\n        https://docs.scipy.org/doc/numpy/reference/maskedarray.baseclass.html#numpy.ma.MaskedArray\n\n    Interface to the OpenCV_ implementation of the local `Lucas-Kanade`_ optical\n    flow method applied in combination to a feature detection routine.\n\n    The sparse motion vectors are finally interpolated to return the whole\n    motion field.\n\n    Parameters\n    ----------\n\n    input_images : array_like or MaskedArray_\n        Array of shape (T, m, n) containing a sequence of *T* two-dimensional\n        input images of shape (m, n). The indexing order in **input_images** is\n        assumed to be (time, latitude, longitude).\n\n        *T* = 2 is the minimum required number of images.\n        With *T* > 2, all the resulting sparse vectors are pooled together for\n        the final interpolation on a regular grid.\n\n        In case of array_like, invalid values (Nans or infs) are masked,\n        otherwise the mask of the MaskedArray_ is used. Such mask defines a\n        region where features are not detected for the tracking algorithm.\n\n    lk_kwargs : dict, optional\n        Optional dictionary containing keyword arguments for the `Lucas-Kanade`_\n        features tracking algorithm. See the documentation of\n        :py:func:`pysteps.motion.lucaskanade.track_features`.\n\n    fd_method : {\"ShiTomasi\"}, optional\n      Name of the feature detection routine. See feature detection methods in\n      :py:mod:`pysteps.utils.images`.\n\n    fd_kwargs : dict, optional\n        Optional dictionary containing keyword arguments for the features\n        detection algorithm.\n        See the documentation of :py:mod:`pysteps.utils.images`.\n\n    interp_method : {\"rbfinterp2d\"}, optional\n      Name of the interpolation method to use. See interpolation methods in\n      :py:mod:`pysteps.utils.interpolate`.\n\n    interp_kwargs : dict, optional\n        Optional dictionary containing keyword arguments for the interpolation\n        algorithm. See the documentation of :py:mod:`pysteps.utils.interpolate`.\n\n    dense : bool, optional\n        If True, return the three-dimensional array (2, m, n) containing\n        the dense x- and y-components of the motion field.\n\n        If False, return the sparse motion vectors as 2-D **xy** and **uv**\n        arrays, where **xy** defines the vector positions, **uv** defines the\n        x and y direction components of the vectors.\n\n    nr_std_outlier : int, optional\n        Maximum acceptable deviation from the mean in terms of number of\n        standard deviations. Any sparse vector with a deviation larger than\n        this threshold is flagged as outlier and excluded from the\n        interpolation.\n        See the documentation of\n        :py:func:`pysteps.utils.cleansing.detect_outliers`.\n\n    k_outlier : int or None, optional\n        The number of nearest neighbours used to localize the outlier detection.\n        If set to None, it employs all the data points (global detection).\n        See the documentation of\n        :py:func:`pysteps.utils.cleansing.detect_outliers`.\n\n    size_opening : int, optional\n        The size of the structuring element kernel in pixels. This is used to\n        perform a binary morphological opening on the input fields in order to\n        filter isolated echoes due to clutter. If set to zero, the filtering\n        is not perfomed.\n        See the documentation of\n        :py:func:`pysteps.utils.images.morph_opening`.\n\n    decl_scale : int, optional\n        The scale declustering parameter in pixels used to reduce the number of\n        redundant sparse vectors before the interpolation.\n        Sparse vectors within this declustering scale are averaged together.\n        If set to less than 2 pixels, the declustering is not perfomed.\n        See the documentation of\n        :py:func:`pysteps.utils.cleansing.decluster`.\n\n    verbose : bool, optional\n        If set to True, print some information about the program.\n\n    Returns\n    -------\n\n    out : array_like or tuple\n        If **dense=True** (the default), return the advection field having shape\n        (2, m, n), where out[0, :, :] contains the x-components of the motion\n        vectors and out[1, :, :] contains the y-components.\n        The velocities are in units of pixels / timestep, where timestep is the\n        time difference between the two input images.\n        Return a zero motion field of shape (2, m, n) when no motion is\n        detected.\n\n        If **dense=False**, it returns a tuple containing the 2-dimensional\n        arrays **xy** and **uv**, where x, y define the vector locations,\n        u, v define the x and y direction components of the vectors.\n        Return two empty arrays when no motion is detected.\n\n    See also\n    --------\n\n    pysteps.motion.lucaskanade.track_features\n\n    References\n    ----------\n\n    Bouguet,  J.-Y.:  Pyramidal  implementation  of  the  affine  Lucas Kanade\n    feature tracker description of the algorithm, Intel Corp., 5, 4,\n    https://doi.org/10.1109/HPDC.2004.1323531, 2001\n\n    Lucas, B. D. and Kanade, T.: An iterative image registration technique with\n    an application to stereo vision, in: Proceedings of the 1981 DARPA Imaging\n    Understanding Workshop, pp. 121\u2013130, 1981.\n    \"\"\"\n\n    input_images = input_images.copy()\n\n    if verbose:\n        print(\"Computing the motion field with the Lucas-Kanade method.\")\n        t0 = time.time()\n\n    nr_fields = input_images.shape[0]\n    domain_size = (input_images.shape[1], input_images.shape[2])\n\n    feature_detection_method = utils.get_method(fd_method)\n    interpolation_method = utils.get_method(interp_method)\n\n    if fd_kwargs is None:\n        fd_kwargs = dict()\n\n    if lk_kwargs is None:\n        lk_kwargs = dict()\n\n    if interp_kwargs is None:\n        interp_kwargs = dict()\n\n    xy = np.empty(shape=(0, 2))\n    uv = np.empty(shape=(0, 2))\n    for n in range(nr_fields - 1):\n\n        # extract consecutive images\n        prvs_img = input_images[n, :, :].copy()\n        next_img = input_images[n + 1, :, :].copy()\n\n        if ~isinstance(prvs_img, MaskedArray):\n            prvs_img = np.ma.masked_invalid(prvs_img)\n        np.ma.set_fill_value(prvs_img, prvs_img.min())\n\n        if ~isinstance(next_img, MaskedArray):\n            next_img = np.ma.masked_invalid(next_img)\n        np.ma.set_fill_value(next_img, next_img.min())\n\n        # remove small noise with a morphological operator (opening)\n        if size_opening > 0:\n            prvs_img = morph_opening(prvs_img, prvs_img.min(), size_opening)\n            next_img = morph_opening(next_img, next_img.min(), size_opening)\n\n        # features detection\n        points = feature_detection_method(prvs_img, **fd_kwargs)\n\n        # skip loop if no features to track\n        if points.shape[0] == 0:\n            continue\n\n        # get sparse u, v vectors with Lucas-Kanade tracking\n        xy_, uv_ = track_features(prvs_img, next_img, points, **lk_kwargs)\n\n        # skip loop if no vectors\n        if xy_.shape[0] == 0:\n            continue\n\n        # stack vectors\n        xy = np.append(xy, xy_, axis=0)\n        uv = np.append(uv, uv_, axis=0)\n\n    # return zero motion field is no sparse vectors are found\n    if xy.shape[0] == 0:\n        if dense:\n            return np.zeros((2, domain_size[0], domain_size[1]))\n        else:\n            return xy, uv\n\n    # detect and remove outliers\n    outliers = detect_outliers(uv, nr_std_outlier, xy, k_outlier, verbose)\n    xy = xy[~outliers, :]\n    uv = uv[~outliers, :]\n\n    if verbose:\n        print(\"--- LK found %i sparse vectors ---\" % xy.shape[0])\n\n    # return sparse vectors if required\n    if not dense:\n        return xy, uv\n\n    # decluster sparse motion vectors\n    if decl_scale > 1:\n        xy, uv = decluster(xy, uv, decl_scale, 1, verbose)\n\n    # return zero motion field if no sparse vectors are left for interpolation\n    if xy.shape[0] == 0:\n        return np.zeros((2, domain_size[0], domain_size[1]))\n\n    # interpolation\n    xgrid = np.arange(domain_size[1])\n    ygrid = np.arange(domain_size[0])\n    UV = interpolation_method(xy, uv, xgrid, ygrid, **interp_kwargs)\n\n    if verbose:\n        print(\"--- total time: %.2f seconds ---\" % (time.time() - t0))\n\n    return UV",
  "def track_features(\n        prvs_image,\n        next_image,\n        points,\n        winsize=(50, 50),\n        nr_levels=3,\n        criteria=(3, 10, 0),\n        flags=0,\n        min_eig_thr=1e-4,\n        verbose=False,\n):\n    \"\"\"\n    Interface to the OpenCV `Lucas-Kanade`_ features tracking algorithm\n    (cv.calcOpticalFlowPyrLK).\n\n    .. _`Lucas-Kanade`:\\\n       https://docs.opencv.org/3.4/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323\n\n    .. _calcOpticalFlowPyrLK:\\\n       https://docs.opencv.org/3.4/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323\n\n\n    .. _MaskedArray:\\\n        https://docs.scipy.org/doc/numpy/reference/maskedarray.baseclass.html#numpy.ma.MaskedArray\n\n    Parameters\n    ----------\n\n    prvs_image : array_like or MaskedArray_\n        Array of shape (m, n) containing the first image.\n        Invalid values (Nans or infs) are filled using the min value.\n\n    next_image : array_like or MaskedArray_\n        Array of shape (m, n) containing the successive image.\n        Invalid values (Nans or infs) are filled using the min value.\n\n    points : array_like\n        Array of shape (p, 2) indicating the pixel coordinates of the\n        tracking points (corners).\n\n    winsize : tuple of int, optional\n        The **winSize** parameter in calcOpticalFlowPyrLK_.\n        It represents the size of the search window that it is used at each\n        pyramid level.\n\n    nr_levels : int, optional\n        The **maxLevel** parameter in calcOpticalFlowPyrLK_.\n        It represents the 0-based maximal pyramid level number.\n\n    criteria : tuple of int, optional\n        The **TermCriteria** parameter in calcOpticalFlowPyrLK_ ,\n        which specifies the termination criteria of the iterative search\n        algorithm.\n\n    flags : int, optional\n        Operation flags, see documentation calcOpticalFlowPyrLK_.\n\n    min_eig_thr : float, optional\n        The **minEigThreshold** parameter in calcOpticalFlowPyrLK_.\n\n    verbose : bool, optional\n        Print the number of vectors that have been found.\n\n    Returns\n    -------\n\n    xy : array_like\n        Array of shape (d, 2) with the x- and y-coordinates of *d* <= *p*\n        detected sparse motion vectors.\n\n    uv : array_like\n        Array of shape (d, 2) with the u- and v-components of *d* <= *p*\n        detected sparse motion vectors.\n\n    Notes\n    -----\n\n    The tracking points can be obtained with the\n    :py:func:`pysteps.utils.images.ShiTomasi_detection` routine.\n\n    See also\n    --------\n\n    pysteps.motion.lucaskanade.dense_lucaskanade\n\n    References\n    ----------\n\n    Bouguet,  J.-Y.:  Pyramidal  implementation  of  the  affine  Lucas Kanade\n    feature tracker description of the algorithm, Intel Corp., 5, 4,\n    https://doi.org/10.1109/HPDC.2004.1323531, 2001\n\n    Lucas, B. D. and Kanade, T.: An iterative image registration technique with\n    an application to stereo vision, in: Proceedings of the 1981 DARPA Imaging\n    Understanding Workshop, pp. 121\u2013130, 1981.\n    \"\"\"\n\n    if not CV2_IMPORTED:\n        raise MissingOptionalDependency(\n            \"opencv package is required for the calcOpticalFlowPyrLK() \"\n            \"routine but it is not installed\"\n        )\n\n    prvs_img = np.copy(prvs_image)\n    next_img = np.copy(next_image)\n    p0 = np.copy(points)\n\n    if ~isinstance(prvs_img, MaskedArray):\n        prvs_img = np.ma.masked_invalid(prvs_img)\n    np.ma.set_fill_value(prvs_img, prvs_img.min())\n\n    if ~isinstance(next_img, MaskedArray):\n        next_img = np.ma.masked_invalid(next_img)\n    np.ma.set_fill_value(next_img, next_img.min())\n\n    # scale between 0 and 255\n    prvs_img = ((prvs_img.filled() - prvs_img.min()) /\n                (prvs_img.max() - prvs_img.min()) * 255)\n\n    next_img = ((next_img.filled() - next_img.min()) /\n                (next_img.max() - next_img.min()) * 255)\n\n    # convert to 8-bit\n    prvs_img = np.ndarray.astype(prvs_img, \"uint8\")\n    next_img = np.ndarray.astype(next_img, \"uint8\")\n\n    # Lucas-Kanade\n    # TODO: use the error returned by the OpenCV routine\n    params = dict(\n        winSize=winsize,\n        maxLevel=nr_levels,\n        criteria=criteria,\n        flags=flags,\n        minEigThreshold=min_eig_thr,\n    )\n    p1, st, __ = cv2.calcOpticalFlowPyrLK(prvs_img, next_img,\n                                          p0, None, **params)\n\n    # keep only features that have been found\n    st = st.squeeze() == 1\n    if np.any(st):\n        p1 = p1[st, :]\n        p0 = p0[st, :]\n\n        # extract vectors\n        xy = p0\n        uv = p1 - p0\n\n    else:\n        xy = uv = np.empty(shape=(0, 2))\n\n    if verbose:\n        print(\"--- %i sparse vectors found ---\" % xy.shape[0])\n\n    return xy, uv",
  "def constant(R, **kwargs):\n    \"\"\"Compute a constant advection field by finding a translation vector that\n    maximizes the correlation between two successive images.\n\n    Parameters\n    ----------\n    R : array_like\n      Array of shape (T,m,n) containing a sequence of T two-dimensional input\n      images of shape (m,n). If T > 2, two last elements along axis 0 are used.\n    \"\"\"\n    X, Y = np.meshgrid(np.arange(R.shape[2]), np.arange(R.shape[1]))\n\n    def f(v):\n        XYW = [Y + v[1], X + v[0]]\n        R_w = ip.map_coordinates(R[-2, :, :], XYW, mode=\"constant\", cval=np.nan,\n                                 order=0, prefilter=False)\n\n        mask = np.isfinite(R_w)\n\n        return -np.corrcoef(R[-1, :, :][mask], R_w[mask])[0, 1]\n\n    options = {\"initial_simplex\" : (np.array([(0, 1), (1, 0), (1, 1)]))}\n    result = op.minimize(f, (1, 1), method=\"Nelder-Mead\", options=options)\n\n    return -result.x",
  "def f(v):\n        XYW = [Y + v[1], X + v[0]]\n        R_w = ip.map_coordinates(R[-2, :, :], XYW, mode=\"constant\", cval=np.nan,\n                                 order=0, prefilter=False)\n\n        mask = np.isfinite(R_w)\n\n        return -np.corrcoef(R[-1, :, :][mask], R_w[mask])[0, 1]",
  "def get_method(name):\n    \"\"\"Return a callable function for the optical flow method corresponding to\n    the given name. The available options are:\\n\\\n\n    +--------------------------------------------------------------------------+\n    | Python-based implementations                                             |\n    +-------------------+------------------------------------------------------+\n    |     Name          |              Description                             |\n    +===================+======================================================+\n    |  None             | returns a zero motion field                          |\n    +-------------------+------------------------------------------------------+\n    |  constant         | constant advection field estimated by maximizing the |\n    |                   | correlation between two images                       |\n    +-------------------+------------------------------------------------------+\n    |  darts            | implementation of the DARTS method of Ruzanski et    |\n    |                   | al. (2011)                                           |\n    +-------------------+------------------------------------------------------+\n    |  lucaskanade      | OpenCV implementation of the Lucas-Kanade method     |\n    |                   | with interpolated motion vectors for areas with no   |\n    |                   | precipitation                                        |\n    +-------------------+------------------------------------------------------+\n    |  proesmans        | the anisotropic diffusion method of Proesmans et     |\n    |                   | al. (1994)                                           |\n    +-------------------+------------------------------------------------------+\n    |  vet              | implementation of the VET method of                  |\n    |                   | Laroche and Zawadzki (1995) and                      |\n    |                   | Germann and Zawadzki (2002)                          |\n    +-------------------+------------------------------------------------------+\n\n    +--------------------------------------------------------------------------+\n    | Methods implemented in C (these require separate compilation and linkage)|\n    +-------------------+------------------------------------------------------+\n    |     Name          |              Description                             |\n    +===================+======================================================+\n    |  brox             | implementation of the variational method of          |\n    |                   | Brox et al. (2004) from IPOL                         |\n    |                   | (http://www.ipol.im/pub/art/2013/21)                 |\n    +-------------------+------------------------------------------------------+\n    |  clg              | implementation of the Combined Local-Global (CLG)    |\n    |                   | method of Bruhn et al., 2005 from IPOL               |\n    |                   | (http://www.ipol.im/pub/art/2015/44)                 |\n    +-------------------+------------------------------------------------------+\n\n    \"\"\"\n\n    if isinstance(name, str):\n        name = name.lower()\n\n    if name in [\"brox\", \"clg\"]:\n        raise NotImplementedError(\"Method %s not implemented\" % name)\n    else:\n        try:\n            motion_method = _methods[name]\n            return motion_method\n        except KeyError:\n            raise ValueError(\"Unknown method {}\\n\".format(name)\n                             + \"The available methods are:\"\n                             + str(list(_methods.keys()))) from None",
  "def print_ar_params(PHI):\n    \"\"\"Print the parameters of an AR(p) model.\n\n    Parameters\n    ----------\n    PHI : array_like\n      Array of shape (n, p) containing the AR(p) parameters for n cascade\n      levels.\n    \"\"\"\n    print(\"****************************************\")\n    print(\"* AR(p) parameters for cascade levels: *\")\n    print(\"****************************************\")\n\n    n = PHI.shape[1]\n\n    hline_str = \"---------\"\n    for k in range(n):\n        hline_str += \"---------------\"\n\n    print(hline_str)\n    title_str = \"| Level |\"\n    for k in range(n - 1):\n        title_str += \"    Phi-%d     |\" % (k + 1)\n    title_str += \"    Phi-0     |\"\n    print(title_str)\n    print(hline_str)\n\n    fmt_str = \"| %-5d |\"\n    for k in range(n):\n        fmt_str += \" %-12.6f |\"\n\n    for k in range(PHI.shape[0]):\n        print(fmt_str % ((k + 1,) + tuple(PHI[k, :])))\n        print(hline_str)",
  "def print_corrcoefs(GAMMA):\n    \"\"\"Print the parameters of an AR(p) model.\n\n    Parameters\n    ----------\n    GAMMA : array_like\n      Array of shape (m, n) containing n correlation coefficients for m cascade\n      levels.\n    \"\"\"\n    print(\"************************************************\")\n    print(\"* Correlation coefficients for cascade levels: *\")\n    print(\"************************************************\")\n\n    m = GAMMA.shape[0]\n    n = GAMMA.shape[1]\n\n    hline_str = \"---------\"\n    for k in range(n):\n        hline_str += \"----------------\"\n\n    print(hline_str)\n    title_str = \"| Level |\"\n    for k in range(n):\n        title_str += \"     Lag-%d     |\" % (k + 1)\n    print(title_str)\n    print(hline_str)\n\n    fmt_str = \"| %-5d |\"\n    for k in range(n):\n        fmt_str += \" %-13.6f |\"\n\n    for k in range(m):\n        print(fmt_str % ((k + 1,) + tuple(GAMMA[k, :])))\n        print(hline_str)",
  "def stack_cascades(R_d, n_levels, donorm=True):\n    \"\"\"Stack the given cascades into a larger array.\n\n    Parameters\n    ----------\n    R_d : list\n      List of cascades obtained by calling a method implemented in\n      pysteps.cascade.decomposition.\n    n_levels : int\n      Number of cascade levels.\n    donorm : bool\n      If True, normalize the cascade levels before stacking.\n\n    Returns\n    -------\n    out : tuple\n      A three-element tuple containing a four-dimensional array of stacked\n      cascade levels and lists of mean values and standard deviations for each\n      cascade level (taken from the last cascade).\n    \"\"\"\n    R_c = []\n    mu = np.empty(n_levels)\n    sigma = np.empty(n_levels)\n\n    n_inputs = len(R_d)\n\n    for i in range(n_levels):\n        R_ = []\n        mu_ = 0\n        sigma_ = 1\n        for j in range(n_inputs):\n            if donorm:\n                mu_ = R_d[j][\"means\"][i]\n                sigma_ = R_d[j][\"stds\"][i]\n            R__ = (R_d[j][\"cascade_levels\"][i, :, :] - mu_) / sigma_\n            R_.append(R__)\n        mu[i] = R_d[n_inputs - 1][\"means\"][i]\n        sigma[i] = R_d[n_inputs - 1][\"stds\"][i]\n        R_c.append(np.stack(R_))\n\n    return np.stack(R_c), mu, sigma",
  "def recompose_cascade(R, mu, sigma):\n    \"\"\"Recompose a cascade by inverting the normalization and summing the\n    cascade levels.\n\n    Parameters\n    ----------\n    R : array_like\n\n    \"\"\"\n    R_rc = [(R[i, :, :] * sigma[i]) + mu[i] for i in range(len(mu))]\n    R_rc = np.sum(np.stack(R_rc), axis=0)\n\n    return R_rc",
  "def forecast(\n    R,\n    metadata,\n    V,\n    n_timesteps,\n    n_ens_members=24,\n    n_cascade_levels=6,\n    win_size=256,\n    overlap=0.1,\n    war_thr=0.1,\n    extrap_method=\"semilagrangian\",\n    decomp_method=\"fft\",\n    bandpass_filter_method=\"gaussian\",\n    noise_method=\"ssft\",\n    ar_order=2,\n    vel_pert_method=None,\n    probmatching_method=\"cdf\",\n    mask_method=\"incremental\",\n    callback=None,\n    fft_method=\"numpy\",\n    return_output=True,\n    seed=None,\n    num_workers=1,\n    extrap_kwargs=None,\n    filter_kwargs=None,\n    noise_kwargs=None,\n    vel_pert_kwargs=None,\n    mask_kwargs=None,\n    measure_time=False,\n):\n    \"\"\"\n    Generate a nowcast ensemble by using the Short-space ensemble prediction\n    system (SSEPS) method.\n    This is an experimental version of STEPS which allows for localization\n    by means of a window function.\n\n    Parameters\n    ----------\n    R : array-like\n        Array of shape (ar_order+1,m,n) containing the input precipitation fields\n        ordered by timestamp from oldest to newest. The time steps between the inputs\n        are assumed to be regular, and the inputs are required to have finite values.\n    metadata : dict\n        Metadata dictionary containing the accutime, xpixelsize, threshold and\n        zerovalue attributes as described in the documentation of \n        :py:mod:`pysteps.io.importers`.\n    V : array-like\n        Array of shape (2,m,n) containing the x- and y-components of the advection\n        field. The velocities are assumed to represent one time step between the\n        inputs. All values are required to be finite.\n    win_size : int or two-element sequence of ints\n        Size-length of the localization window.\n    overlap : float [0,1[\n        A float between 0 and 1 prescribing the level of overlap between\n        successive windows. If set to 0, no overlap is used.\n    war_thr : float\n        Threshold for the minimum fraction of rain in a given window.\n    n_timesteps : int\n        Number of time steps to forecast.\n    n_ens_members : int\n        The number of ensemble members to generate.\n    n_cascade_levels : int\n        The number of cascade levels to use.\n\n    extrap_method : {'semilagrangian'}\n        Name of the extrapolation method to use. See the documentation of\n        pysteps.extrapolation.interface.\n    decomp_method : {'fft'}\n        Name of the cascade decomposition method to use. See the documentation\n        of pysteps.cascade.interface.\n    bandpass_filter_method : {'gaussian', 'uniform'}\n        Name of the bandpass filter method to use with the cascade\n        decomposition.\n    noise_method : {'parametric','nonparametric','ssft','nested',None}\n        Name of the noise generator to use for perturbating the precipitation\n        field. See the documentation of pysteps.noise.interface. If set to None,\n        no noise is generated.\n    ar_order: int\n        The order of the autoregressive model to use. Must be >= 1.\n    vel_pert_method: {'bps',None}\n        Name of the noise generator to use for perturbing the advection field.\n        See the documentation of pysteps.noise.interface. If set to None,\n        the advection field is not perturbed.\n    mask_method : {'incremental', None}\n        The method to use for masking no precipitation areas in the forecast\n        field. The masked pixels are set to the minimum value of the\n        observations. 'incremental' = iteratively buffer the mask with a\n        certain rate (currently it is 1 km/min), None=no masking.\n    probmatching_method : {'cdf', None}\n        Method for matching the statistics of the forecast field with those of\n        the most recently observed one. 'cdf'=map the forecast CDF to the\n        observed one, None=no matching applied. Using 'mean' requires\n        that mask_method is not None.\n    callback : function\n        Optional function that is called after computation of each time step of\n        the nowcast. The function takes one argument: a three-dimensional array\n        of shape (n_ens_members,h,w), where h and w are the height and width\n        of the input field R, respectively. This can be used, for instance,\n        writing the outputs into files.\n    return_output : bool\n        Set to False to disable returning the outputs as numpy arrays. This can\n        save memory if the intermediate results are written to output files\n        using the callback function.\n    seed : int\n        Optional seed number for the random generators.\n    num_workers : int\n        The number of workers to use for parallel computation. Applicable if\n        dask is enabled or pyFFTW is used for computing the FFT.\n        When num_workers>1, it is advisable to disable OpenMP by setting the\n        environment variable OMP_NUM_THREADS to 1.\n        This avoids slowdown caused by too many simultaneous threads.\n    fft_method : str\n        A string defining the FFT method to use (see utils.fft.get_method).\n        Defaults to 'numpy' for compatibility reasons. If pyFFTW is installed,\n        the recommended method is 'pyfftw'.\n    extrap_kwargs : dict\n        Optional dictionary containing keyword arguments for the extrapolation\n        method. See the documentation of pysteps.extrapolation.\n    filter_kwargs : dict\n        Optional dictionary containing keyword arguments for the filter method.\n        See the documentation of pysteps.cascade.bandpass_filters.py.\n    noise_kwargs : dict\n        Optional dictionary containing keyword arguments for the initializer of\n        the noise generator. See the documentation of\n        pysteps.noise.fftgenerators.\n    vel_pert_kwargs : dict\n        Optional dictionary containing keyword arguments \"p_pert_par\" and\n        \"p_pert_perp\" for the initializer of the velocity perturbator.\n        See the documentation of pysteps.noise.motion.\n    mask_kwargs : dict\n        Optional dictionary containing mask keyword arguments 'mask_f' and\n        'mask_rim', the factor defining the the mask increment and the rim size,\n        respectively.\n        The mask increment is defined as mask_f*timestep/kmperpixel.\n    measure_time : bool\n        If set to True, measure, print and return the computation time.\n\n    Returns\n    -------\n    out : ndarray\n        If return_output is True, a four-dimensional array of shape\n        (n_ens_members,n_timesteps,m,n) containing a time series of forecast\n        precipitation fields for each ensemble member. Otherwise, a None value\n        is returned. The time series starts from t0+timestep, where timestep is\n        taken from the input precipitation fields R.\n\n    See also\n    --------\n    pysteps.extrapolation.interface, pysteps.cascade.interface,\n    pysteps.noise.interface, pysteps.noise.utils.compute_noise_stddev_adjs\n\n    Notes\n    -----\n    Please be aware that this represents a (very) experimental implementation.\n\n    References\n    ----------\n    :cite:`Seed2003`, :cite:`BPS2006`, :cite:`SPN2013`, :cite:`NBSG2017`\n\n    \"\"\"\n    _check_inputs(R, V, ar_order)\n\n    if extrap_kwargs is None:\n        extrap_kwargs = dict()\n\n    if filter_kwargs is None:\n        filter_kwargs = dict()\n\n    if noise_kwargs is None:\n        noise_kwargs = dict()\n\n    if vel_pert_kwargs is None:\n        vel_pert_kwargs = dict()\n\n    if mask_kwargs is None:\n        mask_kwargs = dict()\n\n    if np.any(~np.isfinite(R)):\n        raise ValueError(\"R contains non-finite values\")\n\n    if np.any(~np.isfinite(V)):\n        raise ValueError(\"V contains non-finite values\")\n\n    if mask_method not in [\"incremental\", None]:\n        raise ValueError(\n            \"unknown mask method %s: must be 'incremental' or None\" % mask_method\n        )\n\n    if np.isscalar(win_size):\n        win_size = (np.int(win_size), np.int(win_size))\n    else:\n        win_size = tuple([np.int(win_size[i]) for i in range(2)])\n\n    timestep = metadata[\"accutime\"]\n    kmperpixel = metadata[\"xpixelsize\"] / 1000\n\n    print(\"Computing SSEPS nowcast:\")\n    print(\"------------------------\")\n    print(\"\")\n\n    print(\"Inputs:\")\n    print(\"-------\")\n    print(\"input dimensions: %dx%d\" % (R.shape[1], R.shape[2]))\n    print(\"km/pixel:         %g\" % kmperpixel)\n    print(\"time step:        %d minutes\" % timestep)\n    print(\"\")\n\n    print(\"Methods:\")\n    print(\"--------\")\n    print(\"extrapolation:          %s\" % extrap_method)\n    print(\"bandpass filter:        %s\" % bandpass_filter_method)\n    print(\"decomposition:          %s\" % decomp_method)\n    print(\"noise generator:        %s\" % noise_method)\n    print(\"velocity perturbator:   %s\" % vel_pert_method)\n    print(\"precip. mask method:    %s\" % mask_method)\n    print(\"probability matching:   %s\" % probmatching_method)\n    print(\"FFT method:             %s\" % fft_method)\n    print(\"\")\n\n    print(\"Parameters:\")\n    print(\"-----------\")\n    print(\"localization window:      %dx%d\" % (win_size[0], win_size[1]))\n    print(\"overlap:                  %.1f\" % overlap)\n    print(\"war thr:                  %.2f\" % war_thr)\n    print(\"number of time steps:     %d\" % n_timesteps)\n    print(\"ensemble size:            %d\" % n_ens_members)\n    print(\"number of cascade levels: %d\" % n_cascade_levels)\n    print(\"order of the AR(p) model: %d\" % ar_order)\n    print(\"dask imported:            %s\" % (\"yes\" if dask_imported else \"no\"))\n    print(\"num workers:              %d\" % num_workers)\n\n    if vel_pert_method is \"bps\":\n        vp_par = vel_pert_kwargs.get(\n            \"p_pert_par\", noise.motion.get_default_params_bps_par()\n        )\n        vp_perp = vel_pert_kwargs.get(\n            \"p_pert_perp\", noise.motion.get_default_params_bps_perp()\n        )\n        print(\n            \"velocity perturbations, parallel:      %g,%g,%g\"\n            % (vp_par[0], vp_par[1], vp_par[2])\n        )\n        print(\n            \"velocity perturbations, perpendicular: %g,%g,%g\"\n            % (vp_perp[0], vp_perp[1], vp_perp[2])\n        )\n\n    R_thr = metadata[\"threshold\"]\n    R_min = metadata[\"zerovalue\"]\n\n    num_ensemble_workers = n_ens_members if num_workers > n_ens_members else num_workers\n\n    if measure_time:\n        starttime_init = time.time()\n\n    # get methods\n    extrapolator_method = extrapolation.get_method(extrap_method)\n\n    x_values, y_values = np.meshgrid(np.arange(R.shape[2]), np.arange(R.shape[1]))\n\n    xy_coords = np.stack([x_values, y_values])\n\n    decomp_method = cascade.get_method(decomp_method)\n    filter_method = cascade.get_method(bandpass_filter_method)\n    if noise_method is not None:\n        init_noise, generate_noise = noise.get_method(noise_method)\n\n    # advect the previous precipitation fields to the same position with the\n    # most recent one (i.e. transform them into the Lagrangian coordinates)\n    R = R[-(ar_order + 1) :, :, :].copy()\n    extrap_kwargs = extrap_kwargs.copy()\n    extrap_kwargs[\"xy_coords\"] = xy_coords\n    res = []\n    f = lambda R, i: extrapolator_method(\n        R[i, :, :], V, ar_order - i, \"min\", **extrap_kwargs\n    )[-1]\n    for i in range(ar_order):\n        if not dask_imported:\n            R[i, :, :] = f(R, i)\n        else:\n            res.append(dask.delayed(f)(R, i))\n\n    if dask_imported:\n        num_workers_ = len(res) if num_workers > len(res) else num_workers\n        R = np.stack(list(dask.compute(*res, num_workers=num_workers_)) + [R[-1, :, :]])\n\n    if mask_method == \"incremental\":\n        # get mask parameters\n        mask_rim = mask_kwargs.get(\"mask_rim\", 10)\n        mask_f = mask_kwargs.get(\"mask_f\", 1.0)\n        # initialize the structuring element\n        struct = scipy.ndimage.generate_binary_structure(2, 1)\n        # iterate it to expand it nxn\n        n = mask_f * timestep / kmperpixel\n        struct = scipy.ndimage.iterate_structure(struct, int((n - 1) / 2.0))\n\n    noise_kwargs.update(\n        {\n            \"win_size\": win_size,\n            \"overlap\": overlap,\n            \"war_thr\": war_thr,\n            \"rm_rdisc\": True,\n            \"donorm\": True,\n        }\n    )\n\n    print(\"Estimating nowcast parameters.\")\n\n    def estimator(R, parsglob=None, idxm=None, idxn=None):\n\n        pars = {}\n\n        # initialize the perturbation generator for the precipitation field\n        if noise_method is not None and parsglob is None:\n            P = init_noise(R, fft_method=fft_method, **noise_kwargs)\n        else:\n            P = None\n        pars[\"P\"] = P\n\n        # initialize the band-pass filter\n        if parsglob is None:\n            filter = filter_method(R.shape[1:], n_cascade_levels, **filter_kwargs)\n            pars[\"filter\"] = filter\n        else:\n            pars[\"filter\"] = None\n\n        # compute the cascade decompositions of the input precipitation fields\n        if parsglob is None:\n            R_d = []\n            for i in range(ar_order + 1):\n                R_d_ = decomp_method(R[i, :, :], filter, fft_method=fft_method)\n                R_d.append(R_d_)\n            R_d_ = None\n\n        # normalize the cascades and rearrange them into a four-dimensional array\n        # of shape (n_cascade_levels,ar_order+1,m,n) for the autoregressive model\n        if parsglob is None:\n            R_c, mu, sigma = nowcast_utils.stack_cascades(R_d, n_cascade_levels)\n            R_d = None\n        else:\n            R_c = parsglob[\"R_c\"][0][\n                :, :, idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)\n            ].copy()\n            mu = np.mean(R_c, axis=(2, 3))\n            sigma = np.std(R_c, axis=(2, 3))\n\n            R_c = (R_c - mu[:, :, None, None]) / sigma[:, :, None, None]\n\n            mu = mu[:, -1]\n            sigma = sigma[:, -1]\n\n        pars[\"mu\"] = mu\n        pars[\"sigma\"] = sigma\n\n        # compute lag-l temporal autocorrelation coefficients for each cascade level\n        GAMMA = np.empty((n_cascade_levels, ar_order))\n        for i in range(n_cascade_levels):\n            R_c_ = np.stack([R_c[i, j, :, :] for j in range(ar_order + 1)])\n            GAMMA[i, :] = correlation.temporal_autocorrelation(R_c_)\n        R_c_ = None\n\n        if ar_order == 2:\n            # adjust the local lag-2 correlation coefficient to ensure that the AR(p)\n            # process is stationary\n            for i in range(n_cascade_levels):\n                GAMMA[i, 1] = autoregression.adjust_lag2_corrcoef2(\n                    GAMMA[i, 0], GAMMA[i, 1]\n                )\n\n        # estimate the parameters of the AR(p) model from the autocorrelation\n        # coefficients\n        PHI = np.empty((n_cascade_levels, ar_order + 1))\n        for i in range(n_cascade_levels):\n            PHI[i, :] = autoregression.estimate_ar_params_yw(GAMMA[i, :])\n        pars[\"PHI\"] = PHI\n\n        # stack the cascades into a five-dimensional array containing all ensemble\n        # members\n        R_c = [R_c.copy() for i in range(n_ens_members)]\n        pars[\"R_c\"] = R_c\n\n        if mask_method is not None and parsglob is None:\n            MASK_prec = R[-1, :, :] >= R_thr\n            if mask_method == \"incremental\":\n                # initialize precip mask for each member\n                MASK_prec = _compute_incremental_mask(MASK_prec, struct, mask_rim)\n                MASK_prec = [MASK_prec.copy() for j in range(n_ens_members)]\n        else:\n            MASK_prec = None\n        pars[\"MASK_prec\"] = MASK_prec\n\n        return pars\n\n    # prepare windows\n    M, N = R.shape[1:]\n    n_windows_M = np.ceil(1.0 * M / win_size[0]).astype(int)\n    n_windows_N = np.ceil(1.0 * N / win_size[1]).astype(int)\n    idxm = np.zeros((2, 1), dtype=int)\n    idxn = np.zeros((2, 1), dtype=int)\n\n    sys.stdout.flush()\n    if measure_time:\n        starttime = time.time()\n\n    # compute global parameters to be used as defaults\n    parsglob = estimator(R)\n\n    # loop windows\n    if n_windows_M > 1 or n_windows_N > 1:\n        war = np.empty((n_windows_M, n_windows_N))\n        PHI = np.empty((n_windows_M, n_windows_N, n_cascade_levels, ar_order + 1))\n        mu = np.empty((n_windows_M, n_windows_N, n_cascade_levels))\n        sigma = np.empty((n_windows_M, n_windows_N, n_cascade_levels))\n        ff = []\n        rc = []\n        pp = []\n        mm = []\n        for m in range(n_windows_M):\n            ff_ = []\n            pp_ = []\n            rc_ = []\n            mm_ = []\n            for n in range(n_windows_N):\n\n                # compute indices of local window\n                idxm[0] = int(np.max((m * win_size[0] - overlap * win_size[0], 0)))\n                idxm[1] = int(\n                    np.min((idxm[0] + win_size[0] + overlap * win_size[0], M))\n                )\n                idxn[0] = int(np.max((n * win_size[1] - overlap * win_size[1], 0)))\n                idxn[1] = int(\n                    np.min((idxn[0] + win_size[1] + overlap * win_size[1], N))\n                )\n\n                mask = np.zeros((M, N), dtype=bool)\n                mask[idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)] = True\n\n                R_ = R[:, idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)]\n\n                war[m, n] = np.sum(R_[-1, :, :] >= R_thr) / R_[-1, :, :].size\n                if war[m, n] > war_thr:\n\n                    # estimate local parameters\n                    pars = estimator(R, parsglob, idxm, idxn)\n                    ff_.append(pars[\"filter\"])\n                    pp_.append(pars[\"P\"])\n                    rc_.append(pars[\"R_c\"])\n                    mm_.append(pars[\"MASK_prec\"])\n                    mu[m, n, :] = pars[\"mu\"]\n                    sigma[m, n, :] = pars[\"sigma\"]\n                    PHI[m, n, :, :] = pars[\"PHI\"]\n\n                else:\n                    # dry window\n                    ff_.append(None)\n                    pp_.append(None)\n                    rc_.append(None)\n                    mm_.append(None)\n\n            ff.append(ff_)\n            pp.append(pp_)\n            rc.append(rc_)\n            mm.append(mm_)\n\n        # remove unnecessary variables\n        ff_ = None\n        pp_ = None\n        rc_ = None\n        mm_ = None\n        pars = None\n\n    if measure_time:\n        print(\"%.2f seconds.\" % (time.time() - starttime))\n    else:\n        print(\"done.\")\n\n    # initialize the random generators\n    if noise_method is not None:\n        randgen_prec = []\n        randgen_motion = []\n        np.random.seed(seed)\n        for j in range(n_ens_members):\n            rs = np.random.RandomState(seed)\n            randgen_prec.append(rs)\n            seed = rs.randint(0, high=1e9)\n            rs = np.random.RandomState(seed)\n            randgen_motion.append(rs)\n            seed = rs.randint(0, high=1e9)\n\n    if vel_pert_method is not None:\n        init_vel_noise, generate_vel_noise = noise.get_method(vel_pert_method)\n\n        # initialize the perturbation generators for the motion field\n        vps = []\n        for j in range(n_ens_members):\n            kwargs = {\n                \"randstate\": randgen_motion[j],\n                \"p_par\": vp_par,\n                \"p_perp\": vp_perp,\n            }\n            vp_ = init_vel_noise(V, 1.0 / kmperpixel, timestep, **kwargs)\n            vps.append(vp_)\n\n    D = [None for j in range(n_ens_members)]\n    R_f = [[] for j in range(n_ens_members)]\n\n    if measure_time:\n        init_time = time.time() - starttime_init\n\n    R = R[-1, :, :]\n\n    print(\"Starting nowcast computation.\")\n\n    if measure_time:\n        starttime_mainloop = time.time()\n\n    # iterate each time step\n    for t in range(n_timesteps):\n        print(\"Computing nowcast for time step %d... \" % (t + 1), end=\"\")\n        sys.stdout.flush()\n        if measure_time:\n            starttime = time.time()\n\n        # iterate each ensemble member\n        def worker(j):\n\n            # first the global step\n\n            if noise_method is not None:\n                # generate noise field\n                EPS = generate_noise(\n                    parsglob[\"P\"], randstate=randgen_prec[j], fft_method=fft_method\n                )\n                # decompose the noise field into a cascade\n                EPS_d = decomp_method(EPS, parsglob[\"filter\"], fft_method=fft_method)\n            else:\n                EPS_d = None\n\n            # iterate the AR(p) model for each cascade level\n            R_c = parsglob[\"R_c\"][j].copy()\n            if R_c.shape[1] >= ar_order:\n                R_c = R_c[:, -ar_order:, :, :].copy()\n            for i in range(n_cascade_levels):\n                # normalize the noise cascade\n                if EPS_d is not None:\n                    EPS_ = (\n                        EPS_d[\"cascade_levels\"][i, :, :] - EPS_d[\"means\"][i]\n                    ) / EPS_d[\"stds\"][i]\n                else:\n                    EPS_ = None\n                # apply AR(p) process to cascade level\n                R_c[i, :, :, :] = autoregression.iterate_ar_model(\n                    R_c[i, :, :, :], parsglob[\"PHI\"][i, :], EPS=EPS_\n                )\n                EPS_ = None\n            parsglob[\"R_c\"][j] = R_c.copy()\n            EPS = None\n\n            # compute the recomposed precipitation field(s) from the cascades\n            # obtained from the AR(p) model(s)\n            R_c_ = _recompose_cascade(R_c, parsglob[\"mu\"], parsglob[\"sigma\"])\n            R_c = None\n\n            # then the local steps\n            if n_windows_M > 1 or n_windows_N > 1:\n                idxm = np.zeros((2, 1), dtype=int)\n                idxn = np.zeros((2, 1), dtype=int)\n                R_l = np.zeros((M, N), dtype=float)\n                M_s = np.zeros((M, N), dtype=float)\n                for m in range(n_windows_M):\n                    for n in range(n_windows_N):\n\n                        # compute indices of local window\n                        idxm[0] = int(\n                            np.max((m * win_size[0] - overlap * win_size[0], 0))\n                        )\n                        idxm[1] = int(\n                            np.min((idxm[0] + win_size[0] + overlap * win_size[0], M))\n                        )\n                        idxn[0] = int(\n                            np.max((n * win_size[1] - overlap * win_size[1], 0))\n                        )\n                        idxn[1] = int(\n                            np.min((idxn[0] + win_size[1] + overlap * win_size[1], N))\n                        )\n\n                        # build localization mask\n                        mask = _get_mask((M, N), idxm, idxn)\n                        mask_l = mask[\n                            idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)\n                        ]\n                        M_s += mask\n\n                        # skip if dry\n                        if war[m, n] > war_thr:\n\n                            R_c = rc[m][n][j].copy()\n                            if R_c.shape[1] >= ar_order:\n                                R_c = R_c[:, -ar_order:, :, :]\n                            if noise_method is not None:\n                                # extract noise field\n                                EPS_d_l = EPS_d[\"cascade_levels\"][\n                                    :,\n                                    idxm.item(0) : idxm.item(1),\n                                    idxn.item(0) : idxn.item(1),\n                                ].copy()\n                                mu_ = np.mean(EPS_d_l, axis=(1, 2))\n                                sigma_ = np.std(EPS_d_l, axis=(1, 2))\n                            else:\n                                EPS_d_l = None\n\n                            # iterate the AR(p) model for each cascade level\n                            for i in range(n_cascade_levels):\n                                # normalize the noise cascade\n                                if EPS_d_l is not None:\n                                    EPS_ = (\n                                        EPS_d_l[i, :, :] - mu_[i, None, None]\n                                    ) / sigma_[i, None, None]\n                                else:\n                                    EPS_ = None\n                                # apply AR(p) process to cascade level\n                                R_c[i, :, :, :] = autoregression.iterate_ar_model(\n                                    R_c[i, :, :, :], PHI[m, n, i, :], EPS=EPS_\n                                )\n                                EPS_ = None\n                            rc[m][n][j] = R_c.copy()\n                            EPS_d_l = mu_ = sigma_ = None\n\n                            # compute the recomposed precipitation field(s) from the cascades\n                            # obtained from the AR(p) model(s)\n                            mu_ = mu[m, n, :]\n                            sigma_ = sigma[m, n, :]\n                            R_c = [\n                                ((R_c[i, -1, :, :] * sigma_[i]) + mu_[i])\n                                * parsglob[\"sigma\"][i]\n                                + parsglob[\"mu\"][i]\n                                for i in range(len(mu_))\n                            ]\n                            R_l_ = np.sum(np.stack(R_c), axis=0)\n                            R_c = mu_ = sigma_ = None\n                            # R_l_ = _recompose_cascade(R_c[:, :, :], mu[m, n, :], sigma[m, n, :])\n                        else:\n                            R_l_ = R_c_[\n                                idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)\n                            ].copy()\n\n                        if probmatching_method == \"cdf\":\n                            # adjust the CDF of the forecast to match the most recently\n                            # observed precipitation field\n                            R_ = R[\n                                idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)\n                            ].copy()\n                            R_l_ = probmatching.nonparam_match_empirical_cdf(R_l_, R_)\n                            R_ = None\n\n                        R_l[\n                            idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)\n                        ] += (R_l_ * mask_l)\n                        R_l_ = None\n\n                ind = M_s > 0\n                R_l[ind] *= 1 / M_s[ind]\n                R_l[~ind] = R_min\n\n                R_c_ = R_l.copy()\n                R_l = None\n\n            if probmatching_method == \"cdf\":\n                # adjust the CDF of the forecast to match the most recently\n                # observed precipitation field\n                R_c_[R_c_ < R_thr] = R_min\n                R_c_ = probmatching.nonparam_match_empirical_cdf(R_c_, R)\n\n            if mask_method is not None:\n                # apply the precipitation mask to prevent generation of new\n                # precipitation into areas where it was not originally\n                # observed\n                if mask_method == \"incremental\":\n                    MASK_prec = parsglob[\"MASK_prec\"][j].copy()\n                    R_c_ = R_c_.min() + (R_c_ - R_c_.min()) * MASK_prec\n                    MASK_prec = None\n\n            if mask_method == \"incremental\":\n                parsglob[\"MASK_prec\"][j] = _compute_incremental_mask(\n                    R_c_ >= R_thr, struct, mask_rim\n                )\n\n            # compute the perturbed motion field\n            if vel_pert_method is not None:\n                V_ = V + generate_vel_noise(vps[j], (t + 1) * timestep)\n            else:\n                V_ = V\n\n            # advect the recomposed precipitation field to obtain the forecast\n            # for time step t\n            extrap_kwargs.update({\"D_prev\": D[j], \"return_displacement\": True})\n            R_f_, D_ = extrapolator_method(R_c_, V_, 1, **extrap_kwargs)\n            D[j] = D_\n            R_f_ = R_f_[0]\n\n            R_f_[R_f_ < R_thr] = R_min\n\n            return R_f_\n\n        res = []\n        for j in range(n_ens_members):\n            if not dask_imported or n_ens_members == 1:\n                res.append(worker(j))\n            else:\n                res.append(dask.delayed(worker)(j))\n\n        R_f_ = (\n            dask.compute(*res, num_workers=num_ensemble_workers)\n            if dask_imported and n_ens_members > 1\n            else res\n        )\n        res = None\n\n        if measure_time:\n            print(\"%.2f seconds.\" % (time.time() - starttime))\n        else:\n            print(\"done.\")\n\n        if callback is not None:\n            callback(np.stack(R_f_))\n            R_f_ = None\n\n        if return_output:\n            for j in range(n_ens_members):\n                R_f[j].append(R_f_[j])\n\n    if measure_time:\n        mainloop_time = time.time() - starttime_mainloop\n\n    if return_output:\n        outarr = np.stack([np.stack(R_f[j]) for j in range(n_ens_members)])\n        if measure_time:\n            return outarr, init_time, mainloop_time\n        else:\n            return outarr\n    else:\n        return None",
  "def _check_inputs(R, V, ar_order):\n    if len(R.shape) != 3:\n        raise ValueError(\"R must be a three-dimensional array\")\n    if R.shape[0] < ar_order + 1:\n        raise ValueError(\"R.shape[0] < ar_order+1\")\n    if len(V.shape) != 3:\n        raise ValueError(\"V must be a three-dimensional array\")\n    if R.shape[1:3] != V.shape[1:3]:\n        raise ValueError(\n            \"dimension mismatch between R and V: shape(R)=%s, shape(V)=%s\"\n            % (str(R.shape), str(V.shape))\n        )",
  "def _compute_incremental_mask(Rbin, kr, r):\n    # buffer the observation mask Rbin using the kernel kr\n    # add a grayscale rim r (for smooth rain/no-rain transition)\n\n    # buffer observation mask\n    Rbin = np.ndarray.astype(Rbin.copy(), \"uint8\")\n    Rd = scipy.ndimage.morphology.binary_dilation(Rbin, kr)\n\n    # add grayscale rim\n    kr1 = scipy.ndimage.generate_binary_structure(2, 1)\n    mask = Rd.astype(int)\n    for n in range(r):\n        Rd = scipy.ndimage.morphology.binary_dilation(Rd, kr1)\n        mask += Rd\n    # normalize between 0 and 1\n    return mask / mask.max()",
  "def _recompose_cascade(R, mu, sigma):\n    R_rc = [(R[i, -1, :, :] * sigma[i]) + mu[i] for i in range(len(mu))]\n    R_rc = np.sum(np.stack(R_rc), axis=0)\n\n    return R_rc",
  "def _build_2D_tapering_function(win_size, win_type=\"flat-hanning\"):\n    \"\"\"Produces two-dimensional tapering function for rectangular fields.\n\n    Parameters\n    ----------\n    win_size : tuple of int\n        Size of the tapering window as two-element tuple of integers.\n    win_type : str\n        Name of the tapering window type (hanning, flat-hanning)\n\n    Returns\n    -------\n    w2d : array-like\n        A two-dimensional numpy array containing the 2D tapering function.\n    \"\"\"\n\n    if len(win_size) != 2:\n        raise ValueError(\"win_size is not a two-element tuple\")\n\n    if win_type == \"hanning\":\n        w1dr = np.hanning(win_size[0])\n        w1dc = np.hanning(win_size[1])\n\n    elif win_type == \"flat-hanning\":\n\n        T = win_size[0] / 4.0\n        W = win_size[0] / 2.0\n        B = np.linspace(-W, W, 2 * W)\n        R = np.abs(B) - T\n        R[R < 0] = 0.0\n        A = 0.5 * (1.0 + np.cos(np.pi * R / T))\n        A[np.abs(B) > (2 * T)] = 0.0\n        w1dr = A\n\n        T = win_size[1] / 4.0\n        W = win_size[1] / 2.0\n        B = np.linspace(-W, W, 2 * W)\n        R = np.abs(B) - T\n        R[R < 0] = 0.0\n        A = 0.5 * (1.0 + np.cos(np.pi * R / T))\n        A[np.abs(B) > (2 * T)] = 0.0\n        w1dc = A\n\n    elif win_type == \"rectangular\":\n\n        w1dr = np.ones(win_size[0])\n        w1dc = np.ones(win_size[1])\n\n    else:\n        raise ValueError(\"unknown win_type %s\" % win_type)\n\n    # Expand to 2-D\n    # w2d = np.sqrt(np.outer(w1dr,w1dc))\n    w2d = np.outer(w1dr, w1dc)\n\n    # Set nans to zero\n    if np.any(np.isnan(w2d)):\n        w2d[np.isnan(w2d)] = np.min(w2d[w2d > 0])\n\n    w2d[w2d < 1e-3] = 1e-3\n\n    return w2d",
  "def _get_mask(Size, idxi, idxj, win_type=\"flat-hanning\"):\n    \"\"\"Compute a mask of zeros with a window at a given position.\n    \"\"\"\n\n    idxi = np.array(idxi).astype(int)\n    idxj = np.array(idxj).astype(int)\n\n    win_size = (idxi[1] - idxi[0], idxj[1] - idxj[0])\n    wind = _build_2D_tapering_function(win_size, win_type)\n\n    mask = np.zeros(Size)\n    mask[idxi.item(0) : idxi.item(1), idxj.item(0) : idxj.item(1)] = wind\n\n    return mask",
  "def estimator(R, parsglob=None, idxm=None, idxn=None):\n\n        pars = {}\n\n        # initialize the perturbation generator for the precipitation field\n        if noise_method is not None and parsglob is None:\n            P = init_noise(R, fft_method=fft_method, **noise_kwargs)\n        else:\n            P = None\n        pars[\"P\"] = P\n\n        # initialize the band-pass filter\n        if parsglob is None:\n            filter = filter_method(R.shape[1:], n_cascade_levels, **filter_kwargs)\n            pars[\"filter\"] = filter\n        else:\n            pars[\"filter\"] = None\n\n        # compute the cascade decompositions of the input precipitation fields\n        if parsglob is None:\n            R_d = []\n            for i in range(ar_order + 1):\n                R_d_ = decomp_method(R[i, :, :], filter, fft_method=fft_method)\n                R_d.append(R_d_)\n            R_d_ = None\n\n        # normalize the cascades and rearrange them into a four-dimensional array\n        # of shape (n_cascade_levels,ar_order+1,m,n) for the autoregressive model\n        if parsglob is None:\n            R_c, mu, sigma = nowcast_utils.stack_cascades(R_d, n_cascade_levels)\n            R_d = None\n        else:\n            R_c = parsglob[\"R_c\"][0][\n                :, :, idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)\n            ].copy()\n            mu = np.mean(R_c, axis=(2, 3))\n            sigma = np.std(R_c, axis=(2, 3))\n\n            R_c = (R_c - mu[:, :, None, None]) / sigma[:, :, None, None]\n\n            mu = mu[:, -1]\n            sigma = sigma[:, -1]\n\n        pars[\"mu\"] = mu\n        pars[\"sigma\"] = sigma\n\n        # compute lag-l temporal autocorrelation coefficients for each cascade level\n        GAMMA = np.empty((n_cascade_levels, ar_order))\n        for i in range(n_cascade_levels):\n            R_c_ = np.stack([R_c[i, j, :, :] for j in range(ar_order + 1)])\n            GAMMA[i, :] = correlation.temporal_autocorrelation(R_c_)\n        R_c_ = None\n\n        if ar_order == 2:\n            # adjust the local lag-2 correlation coefficient to ensure that the AR(p)\n            # process is stationary\n            for i in range(n_cascade_levels):\n                GAMMA[i, 1] = autoregression.adjust_lag2_corrcoef2(\n                    GAMMA[i, 0], GAMMA[i, 1]\n                )\n\n        # estimate the parameters of the AR(p) model from the autocorrelation\n        # coefficients\n        PHI = np.empty((n_cascade_levels, ar_order + 1))\n        for i in range(n_cascade_levels):\n            PHI[i, :] = autoregression.estimate_ar_params_yw(GAMMA[i, :])\n        pars[\"PHI\"] = PHI\n\n        # stack the cascades into a five-dimensional array containing all ensemble\n        # members\n        R_c = [R_c.copy() for i in range(n_ens_members)]\n        pars[\"R_c\"] = R_c\n\n        if mask_method is not None and parsglob is None:\n            MASK_prec = R[-1, :, :] >= R_thr\n            if mask_method == \"incremental\":\n                # initialize precip mask for each member\n                MASK_prec = _compute_incremental_mask(MASK_prec, struct, mask_rim)\n                MASK_prec = [MASK_prec.copy() for j in range(n_ens_members)]\n        else:\n            MASK_prec = None\n        pars[\"MASK_prec\"] = MASK_prec\n\n        return pars",
  "def worker(j):\n\n            # first the global step\n\n            if noise_method is not None:\n                # generate noise field\n                EPS = generate_noise(\n                    parsglob[\"P\"], randstate=randgen_prec[j], fft_method=fft_method\n                )\n                # decompose the noise field into a cascade\n                EPS_d = decomp_method(EPS, parsglob[\"filter\"], fft_method=fft_method)\n            else:\n                EPS_d = None\n\n            # iterate the AR(p) model for each cascade level\n            R_c = parsglob[\"R_c\"][j].copy()\n            if R_c.shape[1] >= ar_order:\n                R_c = R_c[:, -ar_order:, :, :].copy()\n            for i in range(n_cascade_levels):\n                # normalize the noise cascade\n                if EPS_d is not None:\n                    EPS_ = (\n                        EPS_d[\"cascade_levels\"][i, :, :] - EPS_d[\"means\"][i]\n                    ) / EPS_d[\"stds\"][i]\n                else:\n                    EPS_ = None\n                # apply AR(p) process to cascade level\n                R_c[i, :, :, :] = autoregression.iterate_ar_model(\n                    R_c[i, :, :, :], parsglob[\"PHI\"][i, :], EPS=EPS_\n                )\n                EPS_ = None\n            parsglob[\"R_c\"][j] = R_c.copy()\n            EPS = None\n\n            # compute the recomposed precipitation field(s) from the cascades\n            # obtained from the AR(p) model(s)\n            R_c_ = _recompose_cascade(R_c, parsglob[\"mu\"], parsglob[\"sigma\"])\n            R_c = None\n\n            # then the local steps\n            if n_windows_M > 1 or n_windows_N > 1:\n                idxm = np.zeros((2, 1), dtype=int)\n                idxn = np.zeros((2, 1), dtype=int)\n                R_l = np.zeros((M, N), dtype=float)\n                M_s = np.zeros((M, N), dtype=float)\n                for m in range(n_windows_M):\n                    for n in range(n_windows_N):\n\n                        # compute indices of local window\n                        idxm[0] = int(\n                            np.max((m * win_size[0] - overlap * win_size[0], 0))\n                        )\n                        idxm[1] = int(\n                            np.min((idxm[0] + win_size[0] + overlap * win_size[0], M))\n                        )\n                        idxn[0] = int(\n                            np.max((n * win_size[1] - overlap * win_size[1], 0))\n                        )\n                        idxn[1] = int(\n                            np.min((idxn[0] + win_size[1] + overlap * win_size[1], N))\n                        )\n\n                        # build localization mask\n                        mask = _get_mask((M, N), idxm, idxn)\n                        mask_l = mask[\n                            idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)\n                        ]\n                        M_s += mask\n\n                        # skip if dry\n                        if war[m, n] > war_thr:\n\n                            R_c = rc[m][n][j].copy()\n                            if R_c.shape[1] >= ar_order:\n                                R_c = R_c[:, -ar_order:, :, :]\n                            if noise_method is not None:\n                                # extract noise field\n                                EPS_d_l = EPS_d[\"cascade_levels\"][\n                                    :,\n                                    idxm.item(0) : idxm.item(1),\n                                    idxn.item(0) : idxn.item(1),\n                                ].copy()\n                                mu_ = np.mean(EPS_d_l, axis=(1, 2))\n                                sigma_ = np.std(EPS_d_l, axis=(1, 2))\n                            else:\n                                EPS_d_l = None\n\n                            # iterate the AR(p) model for each cascade level\n                            for i in range(n_cascade_levels):\n                                # normalize the noise cascade\n                                if EPS_d_l is not None:\n                                    EPS_ = (\n                                        EPS_d_l[i, :, :] - mu_[i, None, None]\n                                    ) / sigma_[i, None, None]\n                                else:\n                                    EPS_ = None\n                                # apply AR(p) process to cascade level\n                                R_c[i, :, :, :] = autoregression.iterate_ar_model(\n                                    R_c[i, :, :, :], PHI[m, n, i, :], EPS=EPS_\n                                )\n                                EPS_ = None\n                            rc[m][n][j] = R_c.copy()\n                            EPS_d_l = mu_ = sigma_ = None\n\n                            # compute the recomposed precipitation field(s) from the cascades\n                            # obtained from the AR(p) model(s)\n                            mu_ = mu[m, n, :]\n                            sigma_ = sigma[m, n, :]\n                            R_c = [\n                                ((R_c[i, -1, :, :] * sigma_[i]) + mu_[i])\n                                * parsglob[\"sigma\"][i]\n                                + parsglob[\"mu\"][i]\n                                for i in range(len(mu_))\n                            ]\n                            R_l_ = np.sum(np.stack(R_c), axis=0)\n                            R_c = mu_ = sigma_ = None\n                            # R_l_ = _recompose_cascade(R_c[:, :, :], mu[m, n, :], sigma[m, n, :])\n                        else:\n                            R_l_ = R_c_[\n                                idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)\n                            ].copy()\n\n                        if probmatching_method == \"cdf\":\n                            # adjust the CDF of the forecast to match the most recently\n                            # observed precipitation field\n                            R_ = R[\n                                idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)\n                            ].copy()\n                            R_l_ = probmatching.nonparam_match_empirical_cdf(R_l_, R_)\n                            R_ = None\n\n                        R_l[\n                            idxm.item(0) : idxm.item(1), idxn.item(0) : idxn.item(1)\n                        ] += (R_l_ * mask_l)\n                        R_l_ = None\n\n                ind = M_s > 0\n                R_l[ind] *= 1 / M_s[ind]\n                R_l[~ind] = R_min\n\n                R_c_ = R_l.copy()\n                R_l = None\n\n            if probmatching_method == \"cdf\":\n                # adjust the CDF of the forecast to match the most recently\n                # observed precipitation field\n                R_c_[R_c_ < R_thr] = R_min\n                R_c_ = probmatching.nonparam_match_empirical_cdf(R_c_, R)\n\n            if mask_method is not None:\n                # apply the precipitation mask to prevent generation of new\n                # precipitation into areas where it was not originally\n                # observed\n                if mask_method == \"incremental\":\n                    MASK_prec = parsglob[\"MASK_prec\"][j].copy()\n                    R_c_ = R_c_.min() + (R_c_ - R_c_.min()) * MASK_prec\n                    MASK_prec = None\n\n            if mask_method == \"incremental\":\n                parsglob[\"MASK_prec\"][j] = _compute_incremental_mask(\n                    R_c_ >= R_thr, struct, mask_rim\n                )\n\n            # compute the perturbed motion field\n            if vel_pert_method is not None:\n                V_ = V + generate_vel_noise(vps[j], (t + 1) * timestep)\n            else:\n                V_ = V\n\n            # advect the recomposed precipitation field to obtain the forecast\n            # for time step t\n            extrap_kwargs.update({\"D_prev\": D[j], \"return_displacement\": True})\n            R_f_, D_ = extrapolator_method(R_c_, V_, 1, **extrap_kwargs)\n            D[j] = D_\n            R_f_ = R_f_[0]\n\n            R_f_[R_f_ < R_thr] = R_min\n\n            return R_f_",
  "def forecast(R, V, n_timesteps, n_ens_members=24, n_cascade_levels=6,\n             R_thr=None, kmperpixel=None, timestep=None,\n             extrap_method=\"semilagrangian\", decomp_method=\"fft\",\n             bandpass_filter_method=\"gaussian\", noise_method=\"nonparametric\",\n             noise_stddev_adj=None, ar_order=2, vel_pert_method=\"bps\",\n             conditional=False, probmatching_method=\"cdf\",\n             mask_method=\"incremental\", callback=None, return_output=True,\n             seed=None, num_workers=1, fft_method=\"numpy\", extrap_kwargs=None,\n             filter_kwargs=None, noise_kwargs=None, vel_pert_kwargs=None,\n             mask_kwargs=None, measure_time=False):\n    \"\"\"Generate a nowcast ensemble by using the Short-Term Ensemble Prediction\n    System (STEPS) method.\n\n    Parameters\n    ----------\n    R : array-like\n      Array of shape (ar_order+1,m,n) containing the input precipitation fields\n      ordered by timestamp from oldest to newest. The time steps between the\n      inputs are assumed to be regular, and the inputs are required to have\n      finite values.\n    V : array-like\n      Array of shape (2,m,n) containing the x- and y-components of the advection\n      field. The velocities are assumed to represent one time step between the\n      inputs. All values are required to be finite.\n    n_timesteps : int\n      Number of time steps to forecast.\n    n_ens_members : int, optional\n      The number of ensemble members to generate.\n    n_cascade_levels : int, optional\n      The number of cascade levels to use.\n    R_thr : float, optional\n      Specifies the threshold value for minimum observable precipitation\n      intensity. Required if mask_method is not None or conditional is True.\n    kmperpixel : float, optional\n      Spatial resolution of the input data (kilometers/pixel). Required if\n      vel_pert_method is not None or mask_method is 'incremental'.\n    timestep : float, optional\n      Time step of the motion vectors (minutes). Required if vel_pert_method is\n      not None or mask_method is 'incremental'.\n    extrap_method : str, optional\n      Name of the extrapolation method to use. See the documentation of\n      pysteps.extrapolation.interface.\n    decomp_method : {'fft'}, optional\n      Name of the cascade decomposition method to use. See the documentation\n      of pysteps.cascade.interface.\n    bandpass_filter_method : {'gaussian', 'uniform'}, optional\n      Name of the bandpass filter method to use with the cascade decomposition.\n      See the documentation of pysteps.cascade.interface.\n    noise_method : {'parametric','nonparametric','ssft','nested',None}, optional\n      Name of the noise generator to use for perturbating the precipitation\n      field. See the documentation of pysteps.noise.interface. If set to None,\n      no noise is generated.\n    noise_stddev_adj : {'auto','fixed',None}, optional\n      Optional adjustment for the standard deviations of the noise fields added\n      to each cascade level. This is done to compensate incorrect std. dev.\n      estimates of casace levels due to presence of no-rain areas. 'auto'=use\n      the method implemented in pysteps.noise.utils.compute_noise_stddev_adjs.\n      'fixed'= use the formula given in :cite:`BPS2006` (eq. 6), None=disable\n      noise std. dev adjustment.\n    ar_order : int, optional\n      The order of the autoregressive model to use. Must be >= 1.\n    vel_pert_method : {'bps',None}, optional\n      Name of the noise generator to use for perturbing the advection field. See\n      the documentation of pysteps.noise.interface. If set to None, the advection\n      field is not perturbed.\n    conditional : bool, optional\n      If set to True, compute the statistics of the precipitation field\n      conditionally by excluding pixels where the values are below the threshold\n      R_thr.\n    mask_method : {'obs','sprog','incremental',None}, optional\n      The method to use for masking no precipitation areas in the forecast field.\n      The masked pixels are set to the minimum value of the observations.\n      'obs' = apply R_thr to the most recently observed precipitation intensity\n      field, 'sprog' = use the smoothed forecast field from S-PROG, where the\n      AR(p) model has been applied, 'incremental' = iteratively buffer the mask\n      with a certain rate (currently it is 1 km/min), None=no masking.\n    probmatching_method : {'cdf','mean',None}, optional\n      Method for matching the statistics of the forecast field with those of\n      the most recently observed one. 'cdf'=map the forecast CDF to the observed\n      one, 'mean'=adjust only the conditional mean value of the forecast field\n      in precipitation areas, None=no matching applied. Using 'mean' requires\n      that mask_method is not None.\n    callback : function, optional\n      Optional function that is called after computation of each time step of\n      the nowcast. The function takes one argument: a three-dimensional array\n      of shape (n_ens_members,h,w), where h and w are the height and width\n      of the input field R, respectively. This can be used, for instance,\n      writing the outputs into files.\n    return_output : bool, optional\n      Set to False to disable returning the outputs as numpy arrays. This can\n      save memory if the intermediate results are written to output files using\n      the callback function.\n    seed : int, optional\n      Optional seed number for the random generators.\n    num_workers : int, optional\n      The number of workers to use for parallel computation. Applicable if dask\n      is enabled or pyFFTW is used for computing the FFT. When num_workers>1, it\n      is advisable to disable OpenMP by setting the environment variable\n      OMP_NUM_THREADS to 1. This avoids slowdown caused by too many simultaneous\n      threads.\n    fft_method : str, optional\n      A string defining the FFT method to use (see utils.fft.get_method).\n      Defaults to 'numpy' for compatibility reasons. If pyFFTW is installed,\n      the recommended method is 'pyfftw'.\n    extrap_kwargs : dict, optional\n      Optional dictionary containing keyword arguments for the extrapolation\n      method. See the documentation of pysteps.extrapolation.\n    filter_kwargs : dict, optional\n      Optional dictionary containing keyword arguments for the filter method.\n      See the documentation of pysteps.cascade.bandpass_filters.py.\n    noise_kwargs : dict, optional\n      Optional dictionary containing keyword arguments for the initializer of\n      the noise generator. See the documentation of pysteps.noise.fftgenerators.\n    vel_pert_kwargs : dict, optional\n      Optional dictionary containing keyword arguments 'p_par' and 'p_perp' for\n      the initializer of the velocity perturbator. The choice of the optimal\n      parameters depends on the domain and the used optical flow method.\n\n      Default parameters from :cite:`BPS2006`:\n      p_par  = [10.88, 0.23, -7.68]\n      p_perp = [5.76, 0.31, -2.72]\n\n      Parameters fitted to the data (optical flow/domain):\n\n      darts/fmi:\n      p_par  = [13.71259667, 0.15658963, -16.24368207]\n      p_perp = [8.26550355, 0.17820458, -9.54107834]\n\n      darts/mch:\n      p_par  = [24.27562298, 0.11297186, -27.30087471]\n      p_perp = [-7.80797846e+01, -3.38641048e-02, 7.56715304e+01]\n\n      darts/fmi+mch:\n      p_par  = [16.55447057, 0.14160448, -19.24613059]\n      p_perp = [14.75343395, 0.11785398, -16.26151612]\n\n      lucaskanade/fmi:\n      p_par  = [2.20837526, 0.33887032, -2.48995355]\n      p_perp = [2.21722634, 0.32359621, -2.57402761]\n\n      lucaskanade/mch:\n      p_par  = [2.56338484, 0.3330941, -2.99714349]\n      p_perp = [1.31204508, 0.3578426, -1.02499891]\n\n      lucaskanade/fmi+mch:\n      p_par  = [2.31970635, 0.33734287, -2.64972861]\n      p_perp = [1.90769947, 0.33446594, -2.06603662]\n\n      vet/fmi:\n      p_par  = [0.25337388, 0.67542291, 11.04895538]\n      p_perp = [0.02432118, 0.99613295, 7.40146505]\n\n      vet/mch:\n      p_par  = [0.5075159, 0.53895212, 7.90331791]\n      p_perp = [0.68025501, 0.41761289, 4.73793581]\n\n      vet/fmi+mch:\n      p_par  = [0.29495222, 0.62429207, 8.6804131 ]\n      p_perp = [0.23127377, 0.59010281, 5.98180004]\n\n      fmi=Finland, mch=Switzerland, fmi+mch=both pooled into the same data set\n\n      The above parameters have been fitten by using run_vel_pert_analysis.py\n      and fit_vel_pert_params.py located in the scripts directory.\n\n      See pysteps.noise.motion for additional documentation.\n    mask_kwargs : dict\n      Optional dictionary containing mask keyword arguments 'mask_f' and\n      'mask_rim', the factor defining the the mask increment and the rim size,\n      respectively.\n      The mask increment is defined as mask_f*timestep/kmperpixel.\n    measure_time : bool\n      If set to True, measure, print and return the computation time.\n\n    Returns\n    -------\n    out : ndarray\n      If return_output is True, a four-dimensional array of shape\n      (n_ens_members,n_timesteps,m,n) containing a time series of forecast\n      precipitation fields for each ensemble member. Otherwise, a None value\n      is returned. The time series starts from t0+timestep, where timestep is\n      taken from the input precipitation fields R. If measure_time is True, the\n      return value is a three-element tuple containing the nowcast array, the\n      initialization time of the nowcast generator and the time used in the\n      main loop (seconds).\n\n    See also\n    --------\n    pysteps.extrapolation.interface, pysteps.cascade.interface,\n    pysteps.noise.interface, pysteps.noise.utils.compute_noise_stddev_adjs\n\n    References\n    ----------\n    :cite:`Seed2003`, :cite:`BPS2006`, :cite:`SPN2013`\n\n    \"\"\"\n    _check_inputs(R, V, ar_order)\n\n    if extrap_kwargs is None:\n        extrap_kwargs = dict()\n\n    if filter_kwargs is None:\n        filter_kwargs = dict()\n\n    if noise_kwargs is None:\n        noise_kwargs = dict()\n\n    if vel_pert_kwargs is None:\n        vel_pert_kwargs = dict()\n\n    if mask_kwargs is None:\n        mask_kwargs = dict()\n\n    if np.any(~np.isfinite(R)):\n        raise ValueError(\"R contains non-finite values\")\n\n    if np.any(~np.isfinite(V)):\n        raise ValueError(\"V contains non-finite values\")\n\n    if mask_method not in [\"obs\", \"sprog\", \"incremental\", None]:\n        raise ValueError(\"unknown mask method %s: must be 'obs', 'sprog' or 'incremental' or None\" % mask_method)\n\n    if conditional and R_thr is None:\n        raise ValueError(\"conditional=True but R_thr is not set\")\n\n    if mask_method is not None and R_thr is None:\n        raise ValueError(\"mask_method!=None but R_thr=None\")\n\n    if noise_stddev_adj not in ['auto', 'fixed', None]:\n        raise ValueError(\"unknown noise_std_dev_adj method %s: must be 'auto', 'fixed', or None\" % noise_stddev_adj)\n\n    if kmperpixel is None:\n        if vel_pert_method is not None:\n            raise ValueError(\"vel_pert_method is set but kmperpixel=None\")\n        if mask_method == \"incremental\":\n            raise ValueError(\"mask_method='incremental' but kmperpixel=None\")\n\n    if timestep is None:\n        if vel_pert_method is not None:\n            raise ValueError(\"vel_pert_method is set but timestep=None\")\n        if mask_method == \"incremental\":\n            raise ValueError(\"mask_method='incremental' but timestep=None\")\n\n    print(\"Computing STEPS nowcast:\")\n    print(\"------------------------\")\n    print(\"\")\n\n    print(\"Inputs:\")\n    print(\"-------\")\n    print(\"input dimensions: %dx%d\" % (R.shape[1], R.shape[2]))\n    if kmperpixel is not None:\n        print(\"km/pixel:         %g\" % kmperpixel)\n    if timestep is not None:\n        print(\"time step:        %d minutes\" % timestep)\n    print(\"\")\n\n    print(\"Methods:\")\n    print(\"--------\")\n    print(\"extrapolation:          %s\" % extrap_method)\n    print(\"bandpass filter:        %s\" % bandpass_filter_method)\n    print(\"decomposition:          %s\" % decomp_method)\n    print(\"noise generator:        %s\" % noise_method)\n    print(\"noise adjustment:       %s\" % (\"yes\" if noise_stddev_adj else \"no\"))\n    print(\"velocity perturbator:   %s\" % vel_pert_method)\n    print(\"conditional statistics: %s\" % (\"yes\" if conditional else \"no\"))\n    print(\"precip. mask method:    %s\" % mask_method)\n    print(\"probability matching:   %s\" % probmatching_method)\n    print(\"FFT method:             %s\" % fft_method)\n    print(\"\")\n\n    print(\"Parameters:\")\n    print(\"-----------\")\n    print(\"number of time steps:     %d\" % n_timesteps)\n    print(\"ensemble size:            %d\" % n_ens_members)\n    print(\"parallel threads:         %d\" % num_workers)\n    print(\"number of cascade levels: %d\" % n_cascade_levels)\n    print(\"order of the AR(p) model: %d\" % ar_order)\n    if vel_pert_method == \"bps\":\n        vp_par = vel_pert_kwargs.get(\"p_par\", noise.motion.get_default_params_bps_par())\n        vp_perp = vel_pert_kwargs.get(\"p_perp\", noise.motion.get_default_params_bps_perp())\n        print(\"velocity perturbations, parallel:      %g,%g,%g\" % \\\n              (vp_par[0], vp_par[1], vp_par[2]))\n        print(\"velocity perturbations, perpendicular: %g,%g,%g\" % \\\n              (vp_perp[0], vp_perp[1], vp_perp[2]))\n\n    if conditional or mask_method is not None:\n        print(\"precip. intensity threshold: %g\" % R_thr)\n\n    num_ensemble_workers = n_ens_members if num_workers > n_ens_members \\\n        else num_workers\n\n    if measure_time:\n        starttime_init = time.time()\n\n    fft = utils.get_method(fft_method, shape=R.shape[1:], n_threads=num_workers)\n\n    M, N = R.shape[1:]\n\n    # initialize the band-pass filter\n    filter_method = cascade.get_method(bandpass_filter_method)\n    filter = filter_method((M, N), n_cascade_levels, **filter_kwargs)\n\n    decomp_method = cascade.get_method(decomp_method)\n\n    extrapolator_method = extrapolation.get_method(extrap_method)\n\n    x_values, y_values = np.meshgrid(np.arange(R.shape[2]),\n                                     np.arange(R.shape[1]))\n\n    xy_coords = np.stack([x_values, y_values])\n\n    R = R[-(ar_order + 1):, :, :].copy()\n\n    if conditional:\n        MASK_thr = np.logical_and.reduce([R[i, :, :] >= R_thr for i in range(R.shape[0])])\n    else:\n        MASK_thr = None\n\n    # advect the previous precipitation fields to the same position with the\n    # most recent one (i.e. transform them into the Lagrangian coordinates)\n    extrap_kwargs = extrap_kwargs.copy()\n    extrap_kwargs['xy_coords'] = xy_coords\n    res = list()\n\n    def f(R, i):\n        return extrapolator_method(R[i, :, :], V, ar_order - i,\n                                   \"min\",\n                                   **extrap_kwargs)[-1]\n\n    for i in range(ar_order):\n        if not DASK_IMPORTED:\n            R[i, :, :] = f(R, i)\n        else:\n            res.append(dask.delayed(f)(R, i))\n\n    if DASK_IMPORTED:\n        num_workers_ = len(res) if num_workers > len(res) else num_workers\n        R = np.stack(list(dask.compute(*res, num_workers=num_workers_)) + [R[-1, :, :]])\n\n    if noise_method is not None:\n        # get methods for perturbations\n        init_noise, generate_noise = noise.get_method(noise_method)\n\n        # initialize the perturbation generator for the precipitation field\n        pp = init_noise(R, fft_method=fft, **noise_kwargs)\n\n        if noise_stddev_adj == \"auto\":\n            print(\"Computing noise adjustment coefficients... \", end=\"\")\n            sys.stdout.flush()\n            if measure_time:\n                starttime = time.time()\n\n            R_min = np.min(R)\n            noise_std_coeffs = noise.utils.compute_noise_stddev_adjs(R[-1, :, :],\n                                                                     R_thr, R_min, filter, decomp_method, pp,\n                                                                     generate_noise, 20,\n                                                                     conditional=True, num_workers=num_workers)\n\n            if measure_time:\n                print(\"%.2f seconds.\" % (time.time() - starttime))\n            else:\n                print(\"done.\")\n        elif noise_stddev_adj == \"fixed\":\n            f = lambda k: 1.0 / (0.75 + 0.09 * k)\n            noise_std_coeffs = [f(k) for k in range(1, n_cascade_levels + 1)]\n        else:\n            noise_std_coeffs = np.ones(n_cascade_levels)\n\n        if noise_stddev_adj is not None:\n            print(\"noise std. dev. coeffs:   %s\" % str(noise_std_coeffs))\n\n    # compute the cascade decompositions of the input precipitation fields\n    R_d = []\n    for i in range(ar_order + 1):\n        R_ = decomp_method(R[i, :, :], filter, MASK=MASK_thr, fft_method=fft)\n        R_d.append(R_)\n\n    # normalize the cascades and rearrange them into a four-dimensional array\n    # of shape (n_cascade_levels,ar_order+1,m,n) for the autoregressive model\n    R_c, mu, sigma = nowcast_utils.stack_cascades(R_d, n_cascade_levels)\n    R_d = None\n\n    # compute lag-l temporal autocorrelation coefficients for each cascade level\n    GAMMA = np.empty((n_cascade_levels, ar_order))\n    for i in range(n_cascade_levels):\n        R_c_ = np.stack([R_c[i, j, :, :] for j in range(ar_order + 1)])\n        GAMMA[i, :] = correlation.temporal_autocorrelation(R_c_, MASK=MASK_thr)\n    R_c_ = None\n\n    nowcast_utils.print_corrcoefs(GAMMA)\n\n    if ar_order == 2:\n        # adjust the lag-2 correlation coefficient to ensure that the AR(p)\n        # process is stationary\n        for i in range(n_cascade_levels):\n            GAMMA[i, 1] = autoregression.adjust_lag2_corrcoef2(GAMMA[i, 0], GAMMA[i, 1])\n\n    # estimate the parameters of the AR(p) model from the autocorrelation\n    # coefficients\n    PHI = np.empty((n_cascade_levels, ar_order + 1))\n    for i in range(n_cascade_levels):\n        PHI[i, :] = autoregression.estimate_ar_params_yw(GAMMA[i, :])\n\n    nowcast_utils.print_ar_params(PHI)\n\n    # discard all except the p-1 last cascades because they are not needed for\n    # the AR(p) model\n    R_c = R_c[:, -ar_order:, :, :]\n\n    # stack the cascades into a five-dimensional array containing all ensemble\n    # members\n    R_c = np.stack([R_c.copy() for i in range(n_ens_members)])\n\n    # initialize the random generators\n    if noise_method is not None:\n        randgen_prec = []\n        randgen_motion = []\n        np.random.seed(seed)\n        for j in range(n_ens_members):\n            rs = np.random.RandomState(seed)\n            randgen_prec.append(rs)\n            seed = rs.randint(0, high=1e9)\n            rs = np.random.RandomState(seed)\n            randgen_motion.append(rs)\n            seed = rs.randint(0, high=1e9)\n\n    if vel_pert_method is not None:\n        init_vel_noise, generate_vel_noise = noise.get_method(vel_pert_method)\n\n        # initialize the perturbation generators for the motion field\n        vps = []\n        for j in range(n_ens_members):\n            kwargs = {\"randstate\": randgen_motion[j],\n                      \"p_par\": vp_par,\n                      \"p_perp\": vp_perp}\n            vp_ = init_vel_noise(V, 1. / kmperpixel, timestep, **kwargs)\n            vps.append(vp_)\n\n    D = [None for j in range(n_ens_members)]\n    R_f = [[] for j in range(n_ens_members)]\n\n    if probmatching_method == \"mean\":\n        mu_0 = np.mean(R[-1, :, :][R[-1, :, :] >= R_thr])\n\n    if mask_method is not None:\n        MASK_prec = R[-1, :, :] >= R_thr\n\n        if mask_method == \"obs\":\n            pass\n        elif mask_method == \"sprog\":\n            # compute the wet area ratio and the precipitation mask\n            war = 1.0 * np.sum(MASK_prec) / (R.shape[1] * R.shape[2])\n            R_m = R_c[0, :, :, :].copy()\n        elif mask_method == \"incremental\":\n            # get mask parameters\n            mask_rim = mask_kwargs.get(\"mask_rim\", 10)\n            mask_f = mask_kwargs.get(\"mask_f\", 1.)\n            # initialize the structuring element\n            struct = scipy.ndimage.generate_binary_structure(2, 1)\n            # iterate it to expand it nxn\n            n = mask_f * timestep / kmperpixel\n            struct = scipy.ndimage.iterate_structure(struct, int((n - 1) / 2.))\n            # initialize precip mask for each member\n            MASK_prec = _compute_incremental_mask(MASK_prec, struct, mask_rim)\n            MASK_prec = [MASK_prec.copy() for j in range(n_ens_members)]\n\n    if noise_method is None:\n        R_m = R_c[0, :, :, :].copy()\n\n    fft_objs = []\n    for i in range(n_ens_members):\n        fft_objs.append(utils.get_method(fft_method, shape=R.shape[1:]))\n\n    if measure_time:\n        init_time = time.time() - starttime_init\n\n    R = R[-1, :, :]\n\n    print(\"Starting nowcast computation.\")\n\n    if measure_time:\n        starttime_mainloop = time.time()\n\n    # iterate each time step\n    for t in range(n_timesteps):\n        print(\"Computing nowcast for time step %d... \" % (t + 1), end=\"\")\n        sys.stdout.flush()\n        if measure_time:\n            starttime = time.time()\n\n        if noise_method is None or mask_method == \"sprog\":\n            for i in range(n_cascade_levels):\n                # use a separate AR(p) model for the non-perturbed forecast,\n                # from which the mask is obtained\n                R_m[i, :, :, :] = \\\n                    autoregression.iterate_ar_model(R_m[i, :, :, :], PHI[i, :])\n\n            R_m_ = nowcast_utils.recompose_cascade(R_m[:, -1, :, :], mu, sigma)\n\n            if mask_method == \"sprog\":\n                MASK_prec = _compute_sprog_mask(R_m_, war)\n\n        # iterate each ensemble member\n        def worker(j):\n            if noise_method is not None:\n                # generate noise field\n                EPS = generate_noise(pp, randstate=randgen_prec[j],\n                                     fft_method=fft_objs[j])\n                # decompose the noise field into a cascade\n                EPS = decomp_method(EPS, filter, fft_method=fft_objs[j])\n            else:\n                EPS = None\n\n            # iterate the AR(p) model for each cascade level\n            for i in range(n_cascade_levels):\n                # normalize the noise cascade\n                if EPS is not None:\n                    EPS_ = (EPS[\"cascade_levels\"][i, :, :] - EPS[\"means\"][i]) / EPS[\"stds\"][i]\n                    EPS_ *= noise_std_coeffs[i]\n                else:\n                    EPS_ = None\n                # apply AR(p) process to cascade level\n                if EPS is not None or vel_pert_method is not None:\n                    R_c[j, i, :, :, :] = \\\n                        autoregression.iterate_ar_model(R_c[j, i, :, :, :],\n                                                        PHI[i, :], EPS=EPS_)\n                else:\n                    # use the deterministic AR(p) model computed above if\n                    # perturbations are disabled\n                    R_c[j, i, :, :, :] = R_m[i, :, :, :]\n\n            EPS = None\n            EPS_ = None\n\n            # compute the recomposed precipitation field(s) from the cascades\n            # obtained from the AR(p) model(s)\n            R_c_ = nowcast_utils.recompose_cascade(R_c[j, :, -1, :, :], mu, sigma)\n\n            if mask_method is not None:\n                # apply the precipitation mask to prevent generation of new\n                # precipitation into areas where it was not originally\n                # observed\n                R_cmin = R_c_.min()\n                if mask_method == \"incremental\":\n                    R_c_ = R_cmin + (R_c_ - R_cmin) * MASK_prec[j]\n                    MASK_prec_ = R_c_ > R_cmin\n                else:\n                    MASK_prec_ = MASK_prec\n\n                # Set to min value outside of mask\n                R_c_[~MASK_prec_] = R_cmin\n\n            if probmatching_method == \"cdf\":\n                # adjust the CDF of the forecast to match the most recently\n                # observed precipitation field\n                R_c_ = probmatching.nonparam_match_empirical_cdf(R_c_, R)\n            elif probmatching_method == \"mean\":\n                MASK = R_c_ >= R_thr\n                mu_fct = np.mean(R_c_[MASK])\n                R_c_[MASK] = R_c_[MASK] - mu_fct + mu_0\n\n            if mask_method == \"incremental\":\n                MASK_prec[j] = _compute_incremental_mask(R_c_ >= R_thr, struct, mask_rim)\n\n            # compute the perturbed motion field\n            if vel_pert_method is not None:\n                V_ = V + generate_vel_noise(vps[j], (t + 1) * timestep)\n            else:\n                V_ = V\n\n            # advect the recomposed precipitation field to obtain the forecast\n            # for time step t\n            extrap_kwargs.update({\"D_prev\": D[j], \"return_displacement\": True})\n            R_f_, D_ = extrapolator_method(R_c_, V_, 1, **extrap_kwargs)\n            D[j] = D_\n            R_f_ = R_f_[0]\n\n            return R_f_\n\n        res = []\n        for j in range(n_ens_members):\n            if not DASK_IMPORTED or n_ens_members == 1:\n                res.append(worker(j))\n            else:\n                res.append(dask.delayed(worker)(j))\n\n        R_f_ = dask.compute(*res, num_workers=num_ensemble_workers) \\\n            if DASK_IMPORTED and n_ens_members > 1 else res\n        res = None\n\n        if measure_time:\n            print(\"%.2f seconds.\" % (time.time() - starttime))\n        else:\n            print(\"done.\")\n\n        if callback is not None:\n            callback(np.stack(R_f_))\n            R_f_ = None\n\n        if return_output:\n            for j in range(n_ens_members):\n                R_f[j].append(R_f_[j])\n\n    if measure_time:\n        mainloop_time = time.time() - starttime_mainloop\n\n    if return_output:\n        outarr = np.stack([np.stack(R_f[j]) for j in range(n_ens_members)])\n        if measure_time:\n            return outarr, init_time, mainloop_time\n        else:\n            return outarr\n    else:\n        return None",
  "def _check_inputs(R, V, ar_order):\n    if len(R.shape) != 3:\n        raise ValueError(\"R must be a three-dimensional array\")\n    if R.shape[0] < ar_order + 1:\n        raise ValueError(\"R.shape[0] < ar_order+1\")\n    if len(V.shape) != 3:\n        raise ValueError(\"V must be a three-dimensional array\")\n    if R.shape[1:3] != V.shape[1:3]:\n        raise ValueError(\"dimension mismatch between R and V: shape(R)=%s, shape(V)=%s\" % \\\n                         (str(R.shape), str(V.shape)))",
  "def _compute_incremental_mask(Rbin, kr, r):\n    # buffer the observation mask Rbin using the kernel kr\n    # add a grayscale rim r (for smooth rain/no-rain transition)\n\n    # buffer observation mask\n    Rbin = np.ndarray.astype(Rbin.copy(), \"uint8\")\n    Rd = scipy.ndimage.morphology.binary_dilation(Rbin, kr)\n\n    # add grayscale rim\n    kr1 = scipy.ndimage.generate_binary_structure(2, 1)\n    mask = Rd.astype(float)\n    for n in range(r):\n        Rd = scipy.ndimage.morphology.binary_dilation(Rd, kr1)\n        mask += Rd\n    # normalize between 0 and 1\n    return mask / mask.max()",
  "def _compute_sprog_mask(R, war):\n    # obtain the CDF from the non-perturbed forecast that is\n    # scale-filtered by the AR(p) model\n    R_s = R.flatten()\n\n    # compute the threshold value R_pct_thr corresponding to the\n    # same fraction of precipitation pixels (forecast values above\n    # R_thr) as in the most recently observed precipitation field\n    R_s.sort(kind=\"quicksort\")\n    x = 1.0 * np.arange(1, len(R_s) + 1)[::-1] / len(R_s)\n    i = np.argmin(abs(x - war))\n    # handle ties\n    if R_s[i] == R_s[i + 1]:\n        i = np.where(R_s == R_s[i])[0][-1] + 1\n    R_pct_thr = R_s[i]\n\n    # determine a mask using the above threshold value to preserve the\n    # wet-area ratio\n    return R >= R_pct_thr",
  "def f(R, i):\n        return extrapolator_method(R[i, :, :], V, ar_order - i,\n                                   \"min\",\n                                   **extrap_kwargs)[-1]",
  "def worker(j):\n            if noise_method is not None:\n                # generate noise field\n                EPS = generate_noise(pp, randstate=randgen_prec[j],\n                                     fft_method=fft_objs[j])\n                # decompose the noise field into a cascade\n                EPS = decomp_method(EPS, filter, fft_method=fft_objs[j])\n            else:\n                EPS = None\n\n            # iterate the AR(p) model for each cascade level\n            for i in range(n_cascade_levels):\n                # normalize the noise cascade\n                if EPS is not None:\n                    EPS_ = (EPS[\"cascade_levels\"][i, :, :] - EPS[\"means\"][i]) / EPS[\"stds\"][i]\n                    EPS_ *= noise_std_coeffs[i]\n                else:\n                    EPS_ = None\n                # apply AR(p) process to cascade level\n                if EPS is not None or vel_pert_method is not None:\n                    R_c[j, i, :, :, :] = \\\n                        autoregression.iterate_ar_model(R_c[j, i, :, :, :],\n                                                        PHI[i, :], EPS=EPS_)\n                else:\n                    # use the deterministic AR(p) model computed above if\n                    # perturbations are disabled\n                    R_c[j, i, :, :, :] = R_m[i, :, :, :]\n\n            EPS = None\n            EPS_ = None\n\n            # compute the recomposed precipitation field(s) from the cascades\n            # obtained from the AR(p) model(s)\n            R_c_ = nowcast_utils.recompose_cascade(R_c[j, :, -1, :, :], mu, sigma)\n\n            if mask_method is not None:\n                # apply the precipitation mask to prevent generation of new\n                # precipitation into areas where it was not originally\n                # observed\n                R_cmin = R_c_.min()\n                if mask_method == \"incremental\":\n                    R_c_ = R_cmin + (R_c_ - R_cmin) * MASK_prec[j]\n                    MASK_prec_ = R_c_ > R_cmin\n                else:\n                    MASK_prec_ = MASK_prec\n\n                # Set to min value outside of mask\n                R_c_[~MASK_prec_] = R_cmin\n\n            if probmatching_method == \"cdf\":\n                # adjust the CDF of the forecast to match the most recently\n                # observed precipitation field\n                R_c_ = probmatching.nonparam_match_empirical_cdf(R_c_, R)\n            elif probmatching_method == \"mean\":\n                MASK = R_c_ >= R_thr\n                mu_fct = np.mean(R_c_[MASK])\n                R_c_[MASK] = R_c_[MASK] - mu_fct + mu_0\n\n            if mask_method == \"incremental\":\n                MASK_prec[j] = _compute_incremental_mask(R_c_ >= R_thr, struct, mask_rim)\n\n            # compute the perturbed motion field\n            if vel_pert_method is not None:\n                V_ = V + generate_vel_noise(vps[j], (t + 1) * timestep)\n            else:\n                V_ = V\n\n            # advect the recomposed precipitation field to obtain the forecast\n            # for time step t\n            extrap_kwargs.update({\"D_prev\": D[j], \"return_displacement\": True})\n            R_f_, D_ = extrapolator_method(R_c_, V_, 1, **extrap_kwargs)\n            D[j] = D_\n            R_f_ = R_f_[0]\n\n            return R_f_",
  "def forecast(precip, velocity, num_timesteps,\n             extrap_method=\"semilagrangian\", extrap_kwargs=None,\n             measure_time=False):\n    \"\"\"Generate a nowcast by applying a simple advection-based extrapolation to\n    the given precipitation field.\n\n    .. _ndarray: http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html\n\n    Parameters\n    ----------\n    precip : array-like\n      Two-dimensional array of shape (m,n) containing the input precipitation\n      field.\n    velocity : array-like\n      Array of shape (2,m,n) containing the x- and y-components of the\n      advection field. The velocities are assumed to represent one time step\n      between the inputs.\n    num_timesteps : int\n      Number of time steps to forecast.\n    extrap_method : str, optional\n      Name of the extrapolation method to use. See the documentation of\n      pysteps.extrapolation.interface.\n    extrap_kwargs : dict, , optional\n      Optional dictionary that is expanded into keyword arguments for the\n      extrapolation method.\n    measure_time : bool, optional\n      If True, measure, print, and return the computation time.\n\n    Returns\n    -------\n    out : ndarray_\n      Three-dimensional array of shape (num_timesteps, m, n) containing a time\n      series of nowcast precipitation fields. The time series starts from\n      t0 + timestep, where timestep is taken from the advection field velocity.\n      If *measure_time* is True, the return value is a two-element tuple\n      containing this array and the computation time (seconds).\n\n    See also\n    --------\n\n    pysteps.extrapolation.interface\n\n\n    \"\"\"\n    _check_inputs(precip, velocity)\n\n    if extrap_kwargs is None:\n        extrap_kwargs = dict()\n\n    if measure_time:\n        print(\"Computing extrapolation nowcast from a \"\n              f\"{precip.shape[0]:d}x{precip.shape[1]:d} input grid... \",\n              end=\"\")\n\n    if measure_time:\n        start_time = time.time()\n\n    extrapolation_method = extrapolation.get_method(extrap_method)\n\n    precip_forecast = extrapolation_method(precip, velocity, num_timesteps,\n                                           **extrap_kwargs)\n\n    if measure_time:\n        computation_time = time.time() - start_time\n        print(f\"{computation_time:.2f} seconds.\")\n\n    if measure_time:\n        return precip_forecast, computation_time\n    else:\n        return precip_forecast",
  "def _check_inputs(precip, velocity):\n    if len(precip.shape) != 2:\n        raise ValueError(\"The input precipitation must be a \"\n                         \"two-dimensional array\")\n    if len(velocity.shape) != 3:\n        raise ValueError(\"Input velocity must be a three-dimensional array\")\n    if precip.shape != velocity.shape[1:3]:\n        raise ValueError(\"Dimension mismatch between \"\n                         \"input precipitation and velocity: \" +\n                         \"shape(precip)=%s, shape(velocity)=%s\" %\n                         (str(precip.shape), str(velocity.shape)))",
  "def forecast(R, V, n_timesteps, n_cascade_levels=6, R_thr=None,\n             extrap_method=\"semilagrangian\", decomp_method=\"fft\",\n             bandpass_filter_method=\"gaussian\", ar_order=2, conditional=False,\n             probmatching_method=\"mean\", num_workers=1, fft_method=\"numpy\",\n             extrap_kwargs=None, filter_kwargs=None, measure_time=False):\n    \"\"\"Generate a nowcast by using the Spectral Prognosis (S-PROG) method.\n\n    Parameters\n    ----------\n    R : array-like\n      Array of shape (ar_order+1,m,n) containing the input precipitation fields\n      ordered by timestamp from oldest to newest. The time steps between\n      the inputs are assumed to be regular, and the inputs are required to have\n      finite values.\n    V : array-like\n      Array of shape (2,m,n) containing the x- and y-components of the\n      advection field.\n      The velocities are assumed to represent one time step between the\n      inputs. All values are required to be finite.\n    n_timesteps : int\n      Number of time steps to forecast.\n    n_cascade_levels : int, optional\n      The number of cascade levels to use.\n    R_thr : float\n      The threshold value for minimum observable precipitation intensity.\n    extrap_method : str, optional\n      Name of the extrapolation method to use. See the documentation of\n      pysteps.extrapolation.interface.\n    decomp_method : {'fft'}, optional\n      Name of the cascade decomposition method to use. See the documentation\n      of pysteps.cascade.interface.\n    bandpass_filter_method : {'gaussian', 'uniform'}, optional\n      Name of the bandpass filter method to use with the cascade decomposition.\n      See the documentation of pysteps.cascade.interface.\n    ar_order : int, optional\n      The order of the autoregressive model to use. Must be >= 1.\n    conditional : bool, optional\n      If set to True, compute the statistics of the precipitation field\n      conditionally by excluding pixels where the values are\n      below the threshold R_thr.\n    probmatching_method : {'cdf','mean',None}, optional\n      Method for matching the conditional statistics of the forecast field\n      (areas with precipitation intensity above the threshold R_thr) with those\n      of the most recently observed one. 'cdf'=map the forecast CDF to the\n      observed one, 'mean'=adjust only the mean value,\n      None=no matching applied.\n    num_workers : int, optional\n      The number of workers to use for parallel computation. Applicable if dask\n      is enabled or pyFFTW is used for computing the FFT.\n      When num_workers>1, it is advisable to disable OpenMP by setting\n      the environment variable OMP_NUM_THREADS to 1.\n      This avoids slowdown caused by too many simultaneous threads.\n    fft_method : str, optional\n      A string defining the FFT method to use (see utils.fft.get_method).\n      Defaults to 'numpy' for compatibility reasons. If pyFFTW is installed,\n      the recommended method is 'pyfftw'.\n    extrap_kwargs : dict, optional\n      Optional dictionary containing keyword arguments for the extrapolation\n      method. See the documentation of pysteps.extrapolation.\n    filter_kwargs : dict, optional\n      Optional dictionary containing keyword arguments for the filter method.\n      See the documentation of pysteps.cascade.bandpass_filters.py.\n    measure_time : bool\n      If set to True, measure, print and return the computation time.\n\n    Returns\n    -------\n    out : ndarray\n      A three-dimensional array of shape (n_timesteps,m,n) containing a time\n      series of forecast precipitation fields. The time series starts from\n      t0+timestep, where timestep is taken from the input precipitation fields\n      R. If measure_time is True, the return value is a three-element tuple\n      containing the nowcast array, the initialization time of the nowcast\n      generator and the time used in the main loop (seconds).\n\n    See also\n    --------\n    pysteps.extrapolation.interface, pysteps.cascade.interface\n\n    References\n    ----------\n    :cite:`Seed2003`\n\n    \"\"\"\n    _check_inputs(R, V, ar_order)\n\n    if extrap_kwargs is None:\n        extrap_kwargs = dict()\n\n    if filter_kwargs is None:\n        filter_kwargs = dict()\n\n    if np.any(~np.isfinite(R)):\n        raise ValueError(\"R contains non-finite values\")\n\n    if np.any(~np.isfinite(V)):\n        raise ValueError(\"V contains non-finite values\")\n\n    print(\"Computing S-PROG nowcast:\")\n    print(\"-------------------------\")\n    print(\"\")\n\n    print(\"Inputs:\")\n    print(\"-------\")\n    print(\"input dimensions: %dx%d\" % (R.shape[1], R.shape[2]))\n    print(\"\")\n\n    print(\"Methods:\")\n    print(\"--------\")\n    print(\"extrapolation:          %s\" % extrap_method)\n    print(\"bandpass filter:        %s\" % bandpass_filter_method)\n    print(\"decomposition:          %s\" % decomp_method)\n    print(\"conditional statistics: %s\" % (\"yes\" if conditional else \"no\"))\n    print(\"probability matching:   %s\" % probmatching_method)\n    print(\"FFT method:             %s\" % fft_method)\n    print(\"\")\n\n    print(\"Parameters:\")\n    print(\"-----------\")\n    print(\"number of time steps:     %d\" % n_timesteps)\n    print(\"parallel threads:         %d\" % num_workers)\n    print(\"number of cascade levels: %d\" % n_cascade_levels)\n    print(\"order of the AR(p) model: %d\" % ar_order)\n    print(\"precip. intensity threshold: %g\" % R_thr)\n\n    if measure_time:\n        starttime_init = time.time()\n\n    fft = utils.get_method(fft_method, shape=R.shape[1:],\n                           n_threads=num_workers)\n\n    M, N = R.shape[1:]\n\n    # initialize the band-pass filter\n    filter_method = cascade.get_method(bandpass_filter_method)\n    filter = filter_method((M, N), n_cascade_levels, **filter_kwargs)\n\n    decomp_method = cascade.get_method(decomp_method)\n\n    extrapolator_method = extrapolation.get_method(extrap_method)\n\n    R = R[-(ar_order + 1):, :, :].copy()\n    R_min = R.min()\n\n    if conditional:\n        MASK_thr = np.logical_and.reduce([R[i, :, :] >= R_thr for i in range(R.shape[0])])\n    else:\n        MASK_thr = None\n\n    # initialize the extrapolator\n    x_values, y_values = np.meshgrid(np.arange(R.shape[2]),\n                                     np.arange(R.shape[1]))\n\n    xy_coords = np.stack([x_values, y_values])\n\n    extrap_kwargs = extrap_kwargs.copy()\n    extrap_kwargs['xy_coords'] = xy_coords\n\n    # advect the previous precipitation fields to the same position with the\n    # most recent one (i.e. transform them into the Lagrangian coordinates)\n    res = list()\n\n    def f(R, i):\n        return extrapolator_method(R[i, :, :], V, ar_order - i,\n                                   \"min\",\n                                   **extrap_kwargs)[-1]\n\n    for i in range(ar_order):\n        if not DASK_IMPORTED:\n            R[i, :, :] = f(R, i)\n        else:\n            res.append(dask.delayed(f)(R, i))\n\n    if DASK_IMPORTED:\n        num_workers_ = len(res) if num_workers > len(res) else num_workers\n        R = np.stack(list(dask.compute(*res, num_workers=num_workers_)) + [R[-1, :, :]])\n\n    # compute the cascade decompositions of the input precipitation fields\n    R_d = []\n    for i in range(ar_order + 1):\n        R_ = decomp_method(R[i, :, :], filter, MASK=MASK_thr, fft_method=fft)\n        R_d.append(R_)\n\n    # normalize the cascades and rearrange them into a four-dimensional array\n    # of shape (n_cascade_levels,ar_order+1,m,n) for the autoregressive model\n    R_c, mu, sigma = nowcast_utils.stack_cascades(R_d, n_cascade_levels)\n\n    # compute lag-l temporal autocorrelation coefficients\n    # for each cascade level\n    GAMMA = np.empty((n_cascade_levels, ar_order))\n    for i in range(n_cascade_levels):\n        R_c_ = np.stack([R_c[i, j, :, :] for j in range(ar_order + 1)])\n        GAMMA[i, :] = correlation.temporal_autocorrelation(R_c_, MASK=MASK_thr)\n\n    nowcast_utils.print_corrcoefs(GAMMA)\n\n    if ar_order == 2:\n        # adjust the lag-2 correlation coefficient to ensure that the AR(p)\n        # process is stationary\n        for i in range(n_cascade_levels):\n            GAMMA[i, 1] = autoregression.adjust_lag2_corrcoef2(GAMMA[i, 0],\n                                                               GAMMA[i, 1])\n\n    # estimate the parameters of the AR(p) model from the autocorrelation\n    # coefficients\n    PHI = np.empty((n_cascade_levels, ar_order + 1))\n    for i in range(n_cascade_levels):\n        PHI[i, :] = autoregression.estimate_ar_params_yw(GAMMA[i, :])\n\n    nowcast_utils.print_ar_params(PHI)\n\n    # discard all except the p-1 last cascades because they are not needed for\n    # the AR(p) model\n    R_c = R_c[:, -ar_order:, :, :]\n\n    D = None\n\n    if probmatching_method == \"mean\":\n        mu_0 = np.mean(R[-1, :, :][R[-1, :, :] >= R_thr])\n\n    # compute precipitation mask and wet area ratio\n    MASK_p = R[-1, :, :] >= R_thr\n    war = 1.0 * np.sum(MASK_p) / (R.shape[1] * R.shape[2])\n\n    if measure_time:\n        init_time = time.time() - starttime_init\n\n    R = R[-1, :, :]\n\n    print(\"Starting nowcast computation.\")\n\n    if measure_time:\n        starttime_mainloop = time.time()\n\n    R_f = []\n\n    # iterate each time step\n    for t in range(n_timesteps):\n        print(\"Computing nowcast for time step %d... \" % (t + 1), end=\"\")\n        sys.stdout.flush()\n        if measure_time:\n            starttime = time.time()\n\n        for i in range(n_cascade_levels):\n            # use a separate AR(p) model for the non-perturbed forecast,\n            # from which the mask is obtained\n            R_c[i, :, :, :] = \\\n                autoregression.iterate_ar_model(R_c[i, :, :, :], PHI[i, :])\n\n        R_c_ = nowcast_utils.recompose_cascade(R_c[:, -1, :, :], mu, sigma)\n\n        MASK = _compute_sprog_mask(R_c_, war)\n        R_c_[~MASK] = R_min\n\n        if probmatching_method == \"cdf\":\n            # adjust the CDF of the forecast to match the most recently\n            # observed precipitation field\n            R_c_ = probmatching.nonparam_match_empirical_cdf(R_c_, R)\n        elif probmatching_method == \"mean\":\n            mu_fct = np.mean(R_c_[MASK])\n            R_c_[MASK] = R_c_[MASK] - mu_fct + mu_0\n\n        # advect the recomposed precipitation field to obtain the forecast for\n        # time step t\n        extrap_kwargs.update({\"D_prev\": D, \"return_displacement\": True})\n        R_f_, D_ = extrapolator_method(R_c_, V, 1, **extrap_kwargs)\n        D = D_\n        R_f_ = R_f_[0]\n        R_f.append(R_f_)\n\n        if measure_time:\n            print(\"%.2f seconds.\" % (time.time() - starttime))\n        else:\n            print(\"done.\")\n\n    if measure_time:\n        mainloop_time = time.time() - starttime_mainloop\n\n    R_f = np.stack(R_f)\n\n    if measure_time:\n        return R_f, init_time, mainloop_time\n    else:\n        return R_f",
  "def _check_inputs(R, V, ar_order):\n    if len(R.shape) != 3:\n        raise ValueError(\"R must be a three-dimensional array\")\n    if R.shape[0] < ar_order + 1:\n        raise ValueError(\"R.shape[0] < ar_order+1\")\n    if len(V.shape) != 3:\n        raise ValueError(\"V must be a three-dimensional array\")\n    if R.shape[1:3] != V.shape[1:3]:\n        raise ValueError(\"dimension mismatch between R and V: shape(R)=%s, shape(V)=%s\" % \\\n                         (str(R.shape), str(V.shape)))",
  "def _compute_sprog_mask(R, war):\n    # obtain the CDF from the non-perturbed forecast that is\n    # scale-filtered by the AR(p) model\n    R_s = R.flatten()\n\n    # compute the threshold value R_pct_thr corresponding to the\n    # same fraction of precipitation pixels (forecast values above\n    # R_thr) as in the most recently observed precipitation field\n    R_s.sort(kind=\"quicksort\")\n    x = 1.0 * np.arange(1, len(R_s) + 1)[::-1] / len(R_s)\n    i = np.argmin(abs(x - war))\n    # handle ties\n    if R_s[i] == R_s[i + 1]:\n        i = np.where(R_s == R_s[i])[0][-1] + 1\n    R_pct_thr = R_s[i]\n\n    # determine a mask using the above threshold value to preserve the\n    # wet-area ratio\n    return R >= R_pct_thr",
  "def f(R, i):\n        return extrapolator_method(R[i, :, :], V, ar_order - i,\n                                   \"min\",\n                                   **extrap_kwargs)[-1]",
  "def get_method(name):\n    \"\"\"Return a callable function for computing nowcasts.\n\n    Description:\n    Return a callable function for computing deterministic or ensemble\n    precipitation nowcasts.\n\n    Implemented methods:\n\n    +-----------------+-------------------------------------------------------+\n    |     Name        |              Description                              |\n    +=================+=======================================================+\n    |  eulerian       | this approach keeps the last observation frozen       |\n    |                 | (Eulerian persistence)                                |\n    +-----------------+-------------------------------------------------------+\n    |  lagrangian or  | this approach extrapolates the last observation       |\n    |  extrapolation  | using the motion field (Lagrangian persistence)       |\n    +-----------------+-------------------------------------------------------+\n    |  sprog          | the S-PROG method described in :cite:`Seed2003`       |\n    +-----------------+-------------------------------------------------------+\n    |  steps          | the STEPS stochastic nowcasting method described in   |\n    |                 | :cite:`Seed2003`, :cite:`BPS2006` and :cite:`SPN2013` |\n    |                 |                                                       |\n    +-----------------+-------------------------------------------------------+\n    |  sseps          | short-space ensemble prediction system (SSEPS).       |\n    |                 | Essentially, this is a localization of STEPS.         |\n    +-----------------+-------------------------------------------------------+\n\n    steps and sseps produce stochastic nowcasts, and the other methods are\n    deterministic.\n    \"\"\"\n    if isinstance(name, str):\n        name = name.lower()\n    else:\n        raise TypeError(\"Only strings supported for the method's names.\\n\" +\n                        \"Available names:\" +\n                        str(list(_nowcast_methods.keys()))) from None\n\n    try:\n        return _nowcast_methods[name]\n    except KeyError:\n        raise ValueError(\"Unknown nowcasting method {}\\n\".format(name) +\n                         \"The available methods are:\" +\n                         str(list(_nowcast_methods.keys()))) from None",
  "def extrapolate(precip, velocity, num_timesteps, outval=np.nan, xy_coords=None,\n                allow_nonfinite_values=False, **kwargs):\n    \"\"\"Apply semi-Lagrangian backward extrapolation to a two-dimensional\n    precipitation field.\n\n    Parameters\n    ----------\n    precip : array-like\n        Array of shape (m,n) containing the input precipitation field. All\n        values are required to be finite by default.\n    velocity : array-like\n        Array of shape (2,m,n) containing the x- and y-components of the m*n\n        advection field. All values are required to be finite by default.\n    num_timesteps : int\n        Number of time steps to extrapolate.\n    outval : float, optional\n        Optional argument for specifying the value for pixels advected from\n        outside the domain. If outval is set to 'min', the value is taken as\n        the minimum value of R.\n        Default : np.nan\n    xy_coords : ndarray, optional\n        Array with the coordinates of the grid dimension (2, m, n ).\n\n        * xy_coords[0] : x coordinates\n        * xy_coords[1] : y coordinates\n\n        By default, the *xy_coords* are computed for each extrapolation.\n    allow_nonfinite_values : bool, optional\n        If True, allow non-finite values in the precipitation and advection\n        fields. This option is useful if the input fields contain a radar mask\n        (i.e. pixels with no observations are set to nan).\n\n    Other Parameters\n    ----------------\n\n    D_prev : array-like\n        Optional initial displacement vector field of shape (2,m,n) for the\n        extrapolation.\n        Default : None\n    n_iter : int\n        Number of inner iterations in the semi-Lagrangian scheme. If n_iter > 0,\n        the integration is done using the midpoint rule. Otherwise, the advection\n        vectors are taken from the starting point of each interval.\n        Default : 1\n    return_displacement : bool\n        If True, return the total advection velocity (displacement) between the\n        initial input field and the advected one integrated along\n        the trajectory. Default : False\n\n    Returns\n    -------\n    out : array or tuple\n        If return_displacement=False, return a time series extrapolated fields\n        of shape (num_timesteps,m,n). Otherwise, return a tuple containing the\n        extrapolated fields and the total displacement along the advection\n        trajectory.\n\n    References\n    ----------\n    :cite:`GZ2002` Germann et al (2002)\n\n    \"\"\"\n    if len(precip.shape) != 2:\n        raise ValueError(\"precip must be a two-dimensional array\")\n\n    if len(velocity.shape) != 3:\n        raise ValueError(\"velocity must be a three-dimensional array\")\n\n    if not allow_nonfinite_values:\n        if np.any(~np.isfinite(precip)):\n            raise ValueError(\"precip contains non-finite values\")\n    \n        if np.any(~np.isfinite(velocity)):\n            raise ValueError(\"velocity contains non-finite values\")\n\n    # defaults\n    verbose = kwargs.get(\"verbose\", False)\n    D_prev = kwargs.get(\"D_prev\", None)\n    n_iter = kwargs.get(\"n_iter\", 1)\n    return_displacement = kwargs.get(\"return_displacement\", False)\n\n    if verbose:\n        print(\"Computing the advection with the semi-lagrangian scheme.\")\n        t0 = time.time()\n\n    if outval == \"min\":\n        outval = np.nanmin(precip)\n\n    if xy_coords is None:\n        x_values, y_values = np.meshgrid(np.arange(precip.shape[1]),\n                                         np.arange(precip.shape[0]))\n\n        xy_coords = np.stack([x_values, y_values])\n\n    def interpolate_motion(D, V_inc):\n        XYW = xy_coords + D\n        XYW = [XYW[1, :, :], XYW[0, :, :]]\n\n        VWX = ip.map_coordinates(velocity[0, :, :], XYW, mode=\"nearest\",\n                                 order=0, prefilter=False)\n        VWY = ip.map_coordinates(velocity[1, :, :], XYW, mode=\"nearest\",\n                                 order=0, prefilter=False)\n\n        V_inc[0, :, :] = VWX\n        V_inc[1, :, :] = VWY\n\n        if n_iter > 1:\n            V_inc /= n_iter\n\n    R_e = []\n    if D_prev is None:\n        D = np.zeros((2, velocity.shape[1], velocity.shape[2]))\n        V_inc = velocity.copy()\n    else:\n        D = D_prev.copy()\n        V_inc = np.empty(velocity.shape)\n        interpolate_motion(D, V_inc)\n\n    for t in range(num_timesteps):\n        if n_iter > 0:\n            for k in range(n_iter):\n                interpolate_motion(D - V_inc / 2.0, V_inc)\n                D -= V_inc\n                interpolate_motion(D, V_inc)\n        else:\n            if t > 0 or D_prev is not None:\n                interpolate_motion(D, V_inc)\n\n            D -= V_inc\n\n        XYW = xy_coords + D\n        XYW = [XYW[1, :, :], XYW[0, :, :]]\n\n        IW = ip.map_coordinates(precip, XYW, mode=\"constant\", cval=outval,\n                                order=0, prefilter=False)\n        R_e.append(np.reshape(IW, precip.shape))\n\n    if verbose:\n        print(\"--- %s seconds ---\" % (time.time() - t0))\n\n    if not return_displacement:\n        return np.stack(R_e)\n    else:\n        return np.stack(R_e), D",
  "def interpolate_motion(D, V_inc):\n        XYW = xy_coords + D\n        XYW = [XYW[1, :, :], XYW[0, :, :]]\n\n        VWX = ip.map_coordinates(velocity[0, :, :], XYW, mode=\"nearest\",\n                                 order=0, prefilter=False)\n        VWY = ip.map_coordinates(velocity[1, :, :], XYW, mode=\"nearest\",\n                                 order=0, prefilter=False)\n\n        V_inc[0, :, :] = VWX\n        V_inc[1, :, :] = VWY\n\n        if n_iter > 1:\n            V_inc /= n_iter",
  "def eulerian_persistence(precip, velocity, num_timesteps, outval=np.nan,\n                         **kwargs):\n    \"\"\"A dummy extrapolation method to apply Eulerian persistence to a\n    two-dimensional precipitation field. The method returns the a sequence\n    of the same initial field with no extrapolation applied (i.e. Eulerian\n    persistence).\n\n    Parameters\n    ----------\n    precip : array-like\n        Array of shape (m,n) containing the input precipitation field. All\n        values are required to be finite.\n    velocity : array-like\n        Not used by the method. \n    num_timesteps : int\n        Number of time steps.\n    outval : float, optional\n        Not used by the method. \n\n    Other Parameters\n    ----------------\n\n    return_displacement : bool\n        If True, return the total advection velocity (displacement) between the\n        initial input field and the advected one integrated along\n        the trajectory. Default : False\n\n    Returns\n    -------\n    out : array or tuple\n        If return_displacement=False, return a sequence of the same initial field\n        of shape (num_timesteps,m,n). Otherwise, return a tuple containing the\n        replicated fields and a (2,m,n) array of zeros.\n\n    References\n    ----------\n    :cite:`GZ2002` Germann et al (2002)\n\n    \"\"\"\n    del velocity, outval  # Unused by _eulerian_persistence\n    return_displacement = kwargs.get(\"return_displacement\", False)\n\n    extrapolated_precip = np.repeat(precip[np.newaxis, :, :, ],\n                                    num_timesteps,\n                                    axis=0)\n\n    if not return_displacement:\n        return extrapolated_precip\n    else:\n        return extrapolated_precip, np.zeros((2,) + extrapolated_precip.shape)",
  "def _do_nothing(precip, velocity, num_timesteps, outval=np.nan,\n                **kwargs):\n    \"\"\"Return None.\"\"\"\n    del precip, velocity, num_timesteps, outval, kwargs  # Unused\n    return None",
  "def _return_none(**kwargs):\n    del kwargs  # Not used\n    return None",
  "def get_method(name):\n    \"\"\"Return two-element tuple for the extrapolation method corresponding to\n    the given name. The elements of the tuple are callable functions for the\n    initializer of the extrapolator and the extrapolation method, respectively.\n    The available options are:\\n\n\n    +-----------------+--------------------------------------------------------+\n    |     Name        |              Description                               |\n    +=================+========================================================+\n    |  None           | returns None                                           |\n    +-----------------+--------------------------------------------------------+\n    |  eulerian       | this methods does not apply any advection to the input |\n    |                 | precipitation field (Eulerian persistence)             |\n    +-----------------+--------------------------------------------------------+\n    | semilagrangian  | implementation of the semi-Lagrangian method of        |\n    |                 | Germann et al. (2002) :cite:`GZ2002`                   |\n    +-----------------+--------------------------------------------------------+\n\n    \"\"\"\n    if isinstance(name, str):\n        name = name.lower()\n\n    try:\n        return _extrapolation_methods[name]\n\n    except KeyError:\n        raise ValueError(\"Unknown method {}\\n\".format(name)\n                         + \"The available methods are:\"\n                         + str(list(_extrapolation_methods.keys()))) from None",
  "def compute_empirical_cdf(bin_edges, hist):\n    \"\"\"Compute an empirical cumulative distribution function from the given\n    histogram.\n\n    Parameters\n    ----------\n    bin_edges : array_like\n        Coordinates of left edges of the histogram bins.\n    hist : array_like\n        Histogram counts for each bin.\n\n    Returns\n    -------\n    out : ndarray\n        CDF values corresponding to the bin edges.\n\n    \"\"\"\n    cdf = []\n    xs = 0.0\n\n    for x, h in zip(zip(bin_edges[:-1], bin_edges[1:]), hist):\n        cdf.append(xs)\n        xs += (x[1] - x[0]) * h\n\n    cdf.append(xs)\n    cdf = np.array(cdf) / xs\n\n    return cdf",
  "def nonparam_match_empirical_cdf(R, R_trg):\n    \"\"\"Matches the empirical CDF of the initial array with the empirical CDF\n    of a target array. Initial ranks are conserved, but empirical distribution\n    matches the target one. Zero-pixels (i.e. pixels having the minimum value)\n    in the initial array are conserved.\n\n    Parameters\n    ----------\n    R : array_like\n        The initial array whose CDF is to be matched with the target.\n    R_trg : array_like\n        The target array.\n\n    Returns\n    -------\n    out : array_like\n        The matched array.\n\n    \"\"\"\n\n    if np.any(~np.isfinite(R)):\n        raise ValueError(\"initial array contains non-finite values\")\n    if np.any(~np.isfinite(R_trg)):\n        raise ValueError(\"target array contains non-finite values\")\n    if R.size != R_trg.size:\n        raise ValueError(\"dimension mismatch between R and R_trg: R.shape=%s, R_trg.shape=%s\" % \\\n                         (str(R.shape), str(R_trg.shape)))\n\n    # zeros in initial array\n    zvalue = R.min()\n    idxzeros = R == zvalue\n\n    # zeros in the target array\n    zvalue_trg = R_trg.min()\n\n    # adjust the fraction of rain in target distribution if the number of\n    # nonzeros is greater than in the initial array\n    if np.sum(R_trg > zvalue_trg) > np.sum(R > zvalue):\n        war = np.sum(R > zvalue)/R.size\n        p = np.percentile(R_trg, 100*(1 - war))\n        R_trg = R_trg.copy()\n        R_trg[R_trg < p] = zvalue_trg\n\n    # flatten the arrays\n    arrayshape = R.shape\n    R_trg = R_trg.flatten()\n    R = R.flatten()\n\n    # rank target values\n    order = R_trg.argsort()\n    ranked = R_trg[order]\n\n    # rank initial values order\n    orderin = R.argsort()\n    ranks = np.empty(len(R), int)\n    ranks[orderin] = np.arange(len(R))\n\n    # get ranked values from target and rearrange with the initial order\n    R = ranked[ranks]\n\n    # reshape to the original array dimensions\n    R = R.reshape(arrayshape)\n\n    # readd original zeros\n    R[idxzeros] = zvalue_trg\n\n    return R",
  "def pmm_init(bin_edges_1, cdf_1, bin_edges_2, cdf_2):\n    \"\"\"Initialize a probability matching method (PMM) object from binned\n    cumulative distribution functions (CDF).\n\n    Parameters\n    ----------\n    bin_edges_1 : array_like\n        Coordinates of the left bin edges of the source cdf.\n    cdf_1 : array_like\n        Values of the source CDF at the bin edges.\n    bin_edges_2 : array_like\n        Coordinates of the left bin edges of the target cdf.\n    cdf_2 : array_like\n        Values of the target CDF at the bin edges.\n\n    \"\"\"\n    pmm = {}\n\n    pmm[\"bin_edges_1\"] = bin_edges_1.copy()\n    pmm[\"cdf_1\"] = cdf_1.copy()\n    pmm[\"bin_edges_2\"] = bin_edges_2.copy()\n    pmm[\"cdf_2\"] = cdf_2.copy()\n    pmm[\"cdf_interpolator\"] = sip.interp1d(bin_edges_1, cdf_1, kind=\"linear\")\n\n    return pmm",
  "def pmm_compute(pmm, x):\n    \"\"\"For a given PMM object and x-coordinate, compute the probability matched\n    value (i.e. the x-coordinate for which the target CDF has the same value as\n    the source CDF).\n\n    Parameters\n    ----------\n    pmm : dict\n        A PMM object returned by pmm_init.\n    x : float\n        The coordinate for which to compute the probability matched value.\n\n    \"\"\"\n    mask = np.logical_and(x >= pmm[\"bin_edges_1\"][0],\n                          x <= pmm[\"bin_edges_1\"][-1])\n    p = pmm[\"cdf_interpolator\"](x[mask])\n\n    result = np.ones(len(mask)) * np.nan\n    result[mask] = _invfunc(p, pmm[\"bin_edges_2\"], pmm[\"cdf_2\"])\n\n    return result",
  "def shift_scale(R, f, rain_fraction_trg, second_moment_trg, **kwargs):\n    \"\"\"Find shift and scale that is needed to return the required second_moment\n    and rain area. The optimization is performed with the Nelder-Mead algorithm\n    available in scipy.\n    It assumes a forward transformation ln_rain = ln(rain)-ln(min_rain) if\n    rain > min_rain, else 0.\n\n    Parameters\n    ----------\n    R : array_like\n        The initial array to be shift and scaled.\n    f : function\n        The inverse transformation that is applied after the shift and scale.\n    rain_fraction_trg : float\n        The required rain fraction to be matched by shifting.\n    second_moment_trg : float\n        The required second moment to be matched by scaling.\n        The second_moment is defined as second_moment = var + mean^2.\n\n    Other Parameters\n    ----------------\n    scale : float\n        Optional initial value of the scale parameter for the Nelder-Mead\n        optimisation.\n        Typically, this would be the scale parameter estimated the previous\n        time step.\n        Default : 1.\n    max_iterations : int\n        Maximum allowed number of iterations and function evaluations.\n        More details: https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html\n        Deafult: 100.\n    tol : float\n        Tolerance for termination.\n        More details: https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html\n        Default: 0.05*second_moment_trg, i.e. terminate the search if the error\n        is less than 5% since the second moment is a bit unstable.\n\n    Returns\n    -------\n    shift : float\n        The shift value that produces the required rain fraction.\n    scale : float\n        The scale value that produces the required second_moment.\n    R : array_like\n        The shifted, scaled and back-transformed array.\n    \"\"\"\n\n    shape = R.shape\n    R = R.flatten()\n\n    # defaults\n    scale = kwargs.get(\"scale\", 1.)\n    max_iterations = kwargs.get(\"max_iterations\", 100)\n    tol = kwargs.get(\"tol\", 0.05*second_moment_trg)\n\n    # calculate the shift parameter based on the required rain fraction\n    shift = np.percentile(R, 100*(1 - rain_fraction_trg))\n    idx_wet = R > shift\n\n    # define objective function\n    def _get_error(scale):\n        R_ = np.zeros_like(R)\n        R_[idx_wet] = f((R[idx_wet] - shift)*scale)\n        R_[~idx_wet] = 0\n        second_moment = np.nanstd(R_)**2 + np.nanmean(R_)**2\n        return np.abs(second_moment - second_moment_trg)\n\n    # Nelder-Mead optimisation\n    nm_scale = sop.minimize(_get_error, scale, method=\"Nelder-Mead\", tol=tol,\n                            options={\"disp\": False, \"maxiter\": max_iterations})\n    scale = nm_scale[\"x\"][0]\n\n    R[idx_wet] = f((R[idx_wet] - shift)*scale)\n    R[~idx_wet] = 0\n\n    return shift, scale, R.reshape(shape)",
  "def _invfunc(y, fx, fy):\n    if len(y) == 0:\n        return np.array([])\n\n    b = np.digitize(y, fy)\n    mask = np.logical_and(b > 0, b < len(fy))\n    c = (y[mask] - fy[b[mask]-1]) / (fy[b[mask]] - fy[b[mask]-1])\n\n    result = np.ones(len(y)) * np.nan\n    result[mask] = c * fx[b[mask]] + (1.0-c) * fx[b[mask]-1]\n\n    return result",
  "def _get_error(scale):\n        R_ = np.zeros_like(R)\n        R_[idx_wet] = f((R[idx_wet] - shift)*scale)\n        R_[~idx_wet] = 0\n        second_moment = np.nanstd(R_)**2 + np.nanmean(R_)**2\n        return np.abs(second_moment - second_moment_trg)",
  "def mean(X, ignore_nan=False, X_thr=None):\n    \"\"\"Compute the mean value from a forecast ensemble field.\n\n    Parameters\n    ----------\n\n    X : array_like\n        Array of shape (k, m, n) containing a k-member ensemble of forecast\n        fields of shape (m, n).\n\n    ignore_nan : bool\n        If True, ignore nan values.\n\n    X_thr : float\n        Optional threshold for computing the ensemble mean.\n        Values below **X_thr** are ignored.\n\n    Returns\n    -------\n\n    out : ndarray\n        Array of shape (m, n) containing the ensemble mean.\n    \"\"\"\n\n    X = np.asanyarray(X)\n    X_ndim = X.ndim\n\n    if X_ndim > 3 or X_ndim <= 1:\n        raise Exception(\n            \"Number of dimensions of X should be 2 or 3.\"\n            + \"It was: {}\".format(X_ndim)\n        )\n    elif X.ndim == 2:\n        X = X[None, ...]\n\n    if ignore_nan or X_thr is not None:\n        if X_thr is not None:\n            X = X.copy()\n            X[X < X_thr] = np.nan\n\n        return np.nanmean(X, axis=0)\n    else:\n        return np.mean(X, axis=0)",
  "def excprob(X, X_thr, ignore_nan=False):\n    \"\"\"For a given forecast ensemble field, compute exceedance probabilities\n    for the given intensity thresholds.\n\n    Parameters\n    ----------\n\n    X : array_like\n        Array of shape (k, m, n, ...) containing an k-member ensemble of\n        forecasts with shape (m, n, ...).\n\n    X_thr : float or a sequence of floats\n        Intensity threshold(s) for which the exceedance probabilities are\n        computed.\n\n    ignore_nan : bool\n        If True, ignore nan values.\n\n    Returns\n    -------\n\n    out : ndarray\n        Array of shape (len(X_thr), m, n) containing the exceedance\n        probabilities for the given intensity thresholds.\n        If len(X_thr)=1, the first dimension is dropped.\n    \"\"\"\n    #  Checks\n    X = np.asanyarray(X)\n    X_ndim = X.ndim\n\n    if X_ndim < 3:\n        raise Exception(\n            \"Number of dimensions of X should be 3 or more.\"\n            + \" It was: {}\".format(X_ndim)\n        )\n\n    P = []\n\n    if np.isscalar(X_thr):\n        X_thr = [X_thr]\n        scalar_thr = True\n    else:\n        scalar_thr = False\n\n    for x in X_thr:\n        X_ = X.copy()\n\n        X_[X >= x] = 1.0\n        X_[X < x] = 0.0\n\n        if ignore_nan:\n            P.append(np.nanmean(X_, axis=0))\n        else:\n            P.append(np.mean(X_, axis=0))\n\n    if not scalar_thr:\n        return np.stack(P)\n    else:\n        return P[0]",
  "def banddepth(X, thr=None, norm=False):\n    \"\"\"Compute the modified band depth (Lopez-Pintado and Romo, 2009) for a\n    k-member ensemble data set.\n\n    Implementation of the exact fast algorithm for computing the modified band\n    detpth as described in Sun et al (2012).\n\n    Parameters\n    ----------\n\n    X : array_like\n        Array of shape (k, m, ...) representing an ensemble of *k* members\n        (i.e., samples) with shape (m, ...).\n\n    thr : float\n        Optional threshold for excluding pixels that have no samples equal or\n        above the **thr** value.\n\n    Returns\n    -------\n\n    out : array_like\n        Array of shape *k* containing the (normalized) band depth values for\n        each ensemble member.\n\n    References\n    ----------\n\n    Lopez-Pintado, Sara, and Juan Romo. 2009. \"On the Concept of Depth for\n    Functional Data.\" Journal of the American Statistical Association\n    104 (486): 718\u201334. https://doi.org/10.1198/jasa.2009.0108.\n\n    Sun, Ying, Marc G. Genton, and Douglas W. Nychka. 2012. \"Exact Fast\n    Computation of Band Depth for Large Functional Datasets: How Quickly Can\n    One Million Curves Be Ranked?\" Stat 1 (1): 68\u201374.\n    https://doi.org/10.1002/sta4.8.\n    \"\"\"\n\n    # mask invalid pixels\n    if thr is None:\n        thr = np.nanmin(X)\n    mask = np.logical_and(\n        np.all(np.isfinite(X), axis=0), np.any(X >= thr, axis=0)\n    )\n\n    n = X.shape[0]\n    p = np.sum(mask)\n    depth = np.zeros(n)\n\n    # assign ranks\n    b = np.random.random((n, p))\n    order = np.lexsort((b, X[:, mask]), axis=0)  # random rank for ties\n    rank = order.argsort(axis=0) + 1\n\n    # compute band depth\n    nabove = n - rank\n    nbelow = rank - 1\n    match = nabove * nbelow\n    nchoose2 = comb(n, 2)\n    proportion = np.sum(match, axis=1) / p\n    depth = (proportion + n - 1) / nchoose2\n\n    # normalize depth between 0 and 1\n    if norm:\n        depth = (depth - depth.min()) / (depth.max() - depth.min())\n\n    return depth",
  "def compute_noise_stddev_adjs(R, R_thr_1, R_thr_2, F, decomp_method,\n                              noise_filter, noise_generator, num_iter,\n                              conditional=True, num_workers=1, seed=None):\n    \"\"\"Apply a scale-dependent adjustment factor to the noise fields used in STEPS.\n\n    Simulates the effect of applying a precipitation mask to a Gaussian noise\n    field obtained by the nonparametric filter method. The idea is to decompose\n    the masked noise field into a cascade and compare the standard deviations\n    of each level into those of the observed precipitation intensity field.\n    This gives correction factors for the standard deviations :cite:`BPS2006`.\n    The calculations are done for n realizations of the noise field, and the\n    correction factors are calculated from the average values of the standard\n    deviations.\n\n    Parameters\n    ----------\n    R : array_like\n        The input precipitation field, assumed to be in logarithmic units\n        (dBR or reflectivity).\n    R_thr_1 : float\n        Intensity threshold for precipitation/no precipitation.\n    R_thr_2 : float\n        Intensity values below R_thr_1 are set to this value.\n    F : dict\n        A bandpass filter dictionary returned by a method defined in\n        pysteps.cascade.bandpass_filters. This defines the filter to use and\n        the number of cascade levels.\n    decomp_method : function\n        A function defined in pysteps.cascade.decomposition. Specifies the\n        method to use for decomposing the observed precipitation field and\n        noise field into different spatial scales.\n    num_iter : int\n        The number of noise fields to generate.\n    conditional : bool\n        If set to True, compute the statistics conditionally by excluding areas\n        of no precipitation.\n    num_workers : int\n        The number of workers to use for parallel computation. Applicable if\n        dask is installed.\n    seed : int\n        Optional seed number for the random generators.\n\n    Returns\n    -------\n    out : list\n        A list containing the standard deviation adjustment factor for each\n        cascade level.\n\n    \"\"\"\n\n    MASK = R >= R_thr_1\n\n    R = R.copy()\n    R[~np.isfinite(R)] = R_thr_2\n    R[~MASK] = R_thr_2\n    if not conditional:\n        mu, sigma = np.mean(R),np.std(R)\n    else:\n        mu, sigma = np.mean(R[MASK]),np.std(R[MASK])\n    R -= mu\n\n    MASK_ = MASK if conditional else None\n    decomp_R = decomp_method(R, F, MASK=MASK_)\n\n    if dask_imported and num_workers > 1:\n        res = []\n    else:\n        N_stds = []\n\n    randstates = []\n    seed = None\n    for k in range(num_iter):\n        randstates.append(np.random.RandomState(seed=seed))\n        seed = np.random.randint(0, high=1e9)\n\n    for k in range(num_iter):\n        def worker():\n            # generate Gaussian white noise field, filter it using the chosen\n            # method, multiply it with the standard deviation of the observed\n            # field and apply the precipitation mask\n            N = noise_generator(noise_filter, randstate=randstates[k], seed=seed)\n            N = N / np.std(N) * sigma + mu\n            N[~MASK] = R_thr_2\n\n            # subtract the mean and decompose the masked noise field into a\n            # cascade\n            N -= mu\n            decomp_N = decomp_method(N, F, MASK=MASK_)\n\n            return decomp_N[\"stds\"]\n\n        if dask_imported and num_workers > 1:\n            res.append(dask.delayed(worker)())\n        else:\n            N_stds.append(worker())\n\n    if dask_imported and num_workers > 1:\n        N_stds = dask.compute(*res, num_workers=num_workers)\n\n    # for each cascade level, compare the standard deviations between the\n    # observed field and the masked noise field, which gives the correction\n    # factors\n    return decomp_R[\"stds\"] / np.mean(np.vstack(N_stds), axis=0)",
  "def worker():\n            # generate Gaussian white noise field, filter it using the chosen\n            # method, multiply it with the standard deviation of the observed\n            # field and apply the precipitation mask\n            N = noise_generator(noise_filter, randstate=randstates[k], seed=seed)\n            N = N / np.std(N) * sigma + mu\n            N[~MASK] = R_thr_2\n\n            # subtract the mean and decompose the masked noise field into a\n            # cascade\n            N -= mu\n            decomp_N = decomp_method(N, F, MASK=MASK_)\n\n            return decomp_N[\"stds\"]",
  "def get_default_params_bps_par():\n    \"\"\"Return a tuple containing the default velocity perturbation parameters\n    given in :cite:`BPS2006` for the parallel component.\"\"\"\n    return (10.88, 0.23, -7.68)",
  "def get_default_params_bps_perp():\n    \"\"\"Return a tuple containing the default velocity perturbation parameters\n    given in :cite:`BPS2006` for the perpendicular component.\"\"\"\n    return (5.76, 0.31, -2.72)",
  "def initialize_bps(V, pixelsperkm, timestep, p_par=None, p_perp=None,\n                   randstate=None, seed=None):\n    \"\"\"Initialize the motion field perturbator described in :cite:`BPS2006`.\n    For simplicity, the bias adjustment procedure described there has not been\n    implemented. The perturbator generates a field whose magnitude increases\n    with respect to lead time.\n\n    Parameters\n    ----------\n    V : array_like\n      Array of shape (2,m,n) containing the x- and y-components of the m*n\n      motion field to perturb.\n    p_par : tuple\n      Tuple containing the parameters a,b and c for the standard deviation of\n      the perturbations in the direction parallel to the motion vectors. The\n      standard deviations are modeled by the function f_par(t) = a*t**b+c,\n      where t is lead time. The default values are taken from :cite:`BPS2006`.\n    p_perp : tuple\n      Tuple containing the parameters a,b and c for the standard deviation of\n      the perturbations in the direction perpendicular to the motion vectors.\n      The standard deviations are modeled by the function f_par(t) = a*t**b+c,\n      where t is lead time. The default values are taken from :cite:`BPS2006`.\n    pixelsperkm : float\n      Spatial resolution of the motion field (pixels/kilometer).\n    timestep : float\n      Time step for the motion vectors (minutes).\n    randstate : mtrand.RandomState\n      Optional random generator to use. If set to None, use numpy.random.\n    seed : int\n      Optional seed number for the random generator.\n\n    Returns\n    -------\n    out : dict\n      A dictionary containing the perturbator that can be supplied to\n      generate_motion_perturbations_bps.\n\n    See also\n    --------\n    pysteps.noise.motion.generate_bps\n\n    \"\"\"\n    if len(V.shape) != 3:\n        raise ValueError(\"V is not a three-dimensional array\")\n    if V.shape[0] != 2:\n        raise ValueError(\"the first dimension of V is not 2\")\n\n    if p_par is None:\n        p_par = get_default_params_bps_par()\n    if p_perp is None:\n        p_perp = get_default_params_bps_perp()\n\n    if len(p_par) != 3:\n        raise ValueError(\"the length of p_par is not 3\")\n    if len(p_perp) != 3:\n        raise ValueError(\"the length of p_perp is not 3\")\n\n    perturbator = {}\n    if randstate is None:\n        randstate = np.random\n\n    if seed is not None:\n        randstate.seed(seed)\n\n    eps_par = randstate.laplace(scale=1.0/np.sqrt(2))\n    eps_perp = randstate.laplace(scale=1.0/np.sqrt(2))\n\n    # scale factor for converting the unit of the advection velocities\n    # into km/h\n    vsf = 60.0 / (timestep * pixelsperkm)\n\n    N = linalg.norm(V, axis=0)\n    V_n = V / np.stack([N, N])\n\n    perturbator[\"randstate\"] = randstate\n    perturbator[\"vsf\"] = vsf\n    perturbator[\"p_par\"] = p_par\n    perturbator[\"p_perp\"] = p_perp\n    perturbator[\"eps_par\"] = eps_par\n    perturbator[\"eps_perp\"] = eps_perp\n    perturbator[\"V_par\"] = V_n\n    perturbator[\"V_perp\"] = np.stack([-V_n[1, :, :], V_n[0, :, :]])\n\n    return perturbator",
  "def generate_bps(perturbator, t):\n    \"\"\"Generate a motion perturbation field by using the method described in\n    :cite:`BPS2006`.\n\n    Parameters\n    ----------\n    perturbator : dict\n      A dictionary returned by initialize_motion_perturbations_bps.\n    t : float\n      Lead time for the perturbation field (minutes).\n\n    Returns\n    -------\n    out : ndarray\n      Array of shape (2,m,n) containing the x- and y-components of the motion\n      vector perturbations, where m and n are determined from the perturbator.\n\n    See also\n    --------\n    pysteps.noise.motion.initialize_bps\n\n    \"\"\"\n    vsf = perturbator[\"vsf\"]\n    p_par = perturbator[\"p_par\"]\n    p_perp = perturbator[\"p_perp\"]\n    eps_par = perturbator[\"eps_par\"]\n    eps_perp = perturbator[\"eps_perp\"]\n    V_par = perturbator[\"V_par\"]\n    V_perp = perturbator[\"V_perp\"]\n\n    g_par = p_par[0] * pow(t, p_par[1]) + p_par[2]\n    g_perp = p_perp[0] * pow(t, p_perp[1]) + p_perp[2]\n\n    return (g_par*eps_par*V_par + g_perp*eps_perp*V_perp) / vsf",
  "def initialize_param_2d_fft_filter(X, **kwargs):\n    \"\"\"Takes one ore more 2d input fields, fits two spectral slopes, beta1 and beta2,\n    to produce one parametric, global and isotropic fourier filter.\n\n    Parameters\n    ----------\n    X : array-like\n        Two- or three-dimensional array containing one or more input fields.\n        All values are required to be finite. If more than one field are passed,\n        the average fourier filter is returned. It assumes that fields are stacked\n        by the first axis: [nr_fields, y, x].\n\n    Other Parameters\n    ----------------\n    win_type : {'hanning', 'flat-hanning' or None}\n        Optional tapering function to be applied to X, generated with\n        :py:func:`pysteps.noise.fftgenerators.build_2D_tapering_function`\n        (default None).\n    model : {'power-law'}\n        The name of the parametric model to be used to fit the power spectrum of\n        X (default 'power-law').\n    weighted : bool\n        Whether or not to apply 1/sqrt(power) as weight in the numpy.polyfit()\n        function (default False).\n    rm_rdisc : bool\n        Whether or not to remove the rain/no-rain disconituity (default False).\n        It assumes no-rain pixels are assigned with lowest value.\n    fft_method : str or tuple\n        A string or a (function,kwargs) tuple defining the FFT method to use\n        (see \"FFT methods\" in :py:func:`pysteps.utils.interface.get_method`).\n        Defaults to \"numpy\".\n\n    Returns\n    -------\n    out : dict\n        A a dictionary containing the keys field, input_shape, model and pars.\n        The first is a two-dimensional array of shape (m, int(n/2)+1) that\n        defines the filter. The second one is the shape of the input field for\n        the filter. The last two are the model and fitted parameters,\n        respectively.\n\n        This dictionary can be passed to\n        :py:func:`pysteps.noise.fftgenerators.generate_noise_2d_fft_filter` to\n        generate noise fields.\n    \"\"\"\n\n    if len(X.shape) < 2 or len(X.shape) > 3:\n        raise ValueError(\"the input is not two- or three-dimensional array\")\n    if np.any(~np.isfinite(X)):\n        raise ValueError(\"X contains non-finite values\")\n\n    # defaults\n    win_type = kwargs.get(\"win_type\", None)\n    model = kwargs.get(\"model\", \"power-law\")\n    weighted = kwargs.get(\"weighted\", False)\n    rm_rdisc = kwargs.get(\"rm_rdisc\", False)\n    fft = kwargs.get(\"fft_method\", \"numpy\")\n    if type(fft) == str:\n        fft_shape = X.shape if len(X.shape) == 2 else X.shape[1:]\n        fft = utils.get_method(fft, shape=fft_shape)\n\n    X = X.copy()\n\n    # remove rain/no-rain discontinuity\n    if rm_rdisc:\n        X[X > X.min()] -= X[X > X.min()].min() - X.min()\n\n    # dims\n    if len(X.shape) == 2:\n        X = X[None, :, :]\n    nr_fields = X.shape[0]\n    M, N = X.shape[1:]\n\n    if win_type is not None:\n        tapering = build_2D_tapering_function((M, N), win_type)\n\n        # make sure non-rainy pixels are set to zero\n        X -= X.min(axis=(1, 2))[:, None, None]\n    else:\n        tapering = np.ones((M, N))\n\n    if model.lower() == \"power-law\":\n\n        # compute average 2D PSD\n        F = np.zeros((M, N), dtype=complex)\n        for i in range(nr_fields):\n            F += fft.fftshift(fft.fft2(X[i, :, :] * tapering))\n        F /= nr_fields\n        F = abs(F) ** 2 / F.size\n\n        # compute radially averaged 1D PSD\n        psd = utils.spectral.rapsd(F)\n        L = max(M, N)\n\n        # wavenumbers\n        if L % 2 == 0:\n            wn = np.arange(0, int(L / 2) + 1)\n        else:\n            wn = np.arange(0, int(L / 2))\n\n        # compute single spectral slope beta as first guess\n        if weighted:\n            p0 = np.polyfit(np.log(wn[1:]), np.log(psd[1:]), 1, w=np.sqrt(psd[1:]))\n        else:\n            p0 = np.polyfit(np.log(wn[1:]), np.log(psd[1:]), 1)\n        beta = p0[0]\n\n        # create the piecewise function with two spectral slopes beta1 and beta2\n        # and scaling break x0\n        def piecewise_linear(x, x0, y0, beta1, beta2):\n            return np.piecewise(\n                x,\n                [x < x0, x >= x0],\n                [\n                    lambda x: beta1 * x + y0 - beta1 * x0,\n                    lambda x: beta2 * x + y0 - beta2 * x0,\n                ],\n            )\n\n        # fit the two betas and the scaling break\n        p0 = [2.0, 0, beta, beta]  # first guess\n        bounds = (\n            [2.0, 0, -4, -4],\n            [5.0, 20, -1.0, -1.0],\n        )  # TODO: provide better bounds\n        if weighted:\n            p, e = optimize.curve_fit(\n                piecewise_linear,\n                np.log(wn[1:]),\n                np.log(psd[1:]),\n                p0=p0,\n                bounds=bounds,\n                sigma=1 / np.sqrt(psd[1:]),\n            )\n        else:\n            p, e = optimize.curve_fit(\n                piecewise_linear, np.log(wn[1:]), np.log(psd[1:]), p0=p0, bounds=bounds\n            )\n\n        # compute 2d filter\n        YC, XC = utils.arrays.compute_centred_coord_array(M, N)\n        R = np.sqrt(XC * XC + YC * YC)\n        R = fft.fftshift(R)\n        pf = p.copy()\n        pf[2:] = pf[2:] / 2\n        F = np.exp(piecewise_linear(np.log(R), *pf))\n        F[~np.isfinite(F)] = 1\n\n        f = piecewise_linear\n\n    else:\n        raise ValueError(\"unknown parametric model %s\" % model)\n\n    return {\n        \"field\": F,\n        \"input_shape\": X.shape[1:],\n        \"use_full_fft\": True,\n        \"model\": f,\n        \"pars\": p,\n    }",
  "def initialize_nonparam_2d_fft_filter(X, **kwargs):\n    \"\"\"Takes one ore more 2d input fields and produces one non-paramtric, global\n    and anasotropic fourier filter.\n\n    Parameters\n    ----------\n    X : array-like\n        Two- or three-dimensional array containing one or more input fields.\n        All values are required to be finite. If more than one field are passed,\n        the average fourier filter is returned. It assumes that fields are stacked\n        by the first axis: [nr_fields, y, x].\n\n    Other Parameters\n    ----------------\n    win_type : {'hanning', 'flat-hanning'}\n        Optional tapering function to be applied to X, generated with\n        :py:func:`pysteps.noise.fftgenerators.build_2D_tapering_function`\n        (default 'flat-hanning').\n    donorm : bool\n       Option to normalize the real and imaginary parts.\n       Default : False\n    rm_rdisc : bool\n        Whether or not to remove the rain/no-rain disconituity (default True).\n        It assumes no-rain pixels are assigned with lowest value.\n    fft_method : str or tuple\n        A string or a (function,kwargs) tuple defining the FFT method to use\n        (see \"FFT methods\" in :py:func:`pysteps.utils.interface.get_method`).\n        Defaults to \"numpy\".\n\n    Returns\n    -------\n    out : dict\n      A dictionary containing the keys field and input_shape. The first is a\n      two-dimensional array of shape (m, int(n/2)+1) that defines the filter.\n      The second one is the shape of the input field for the filter.\n\n      It can be passed to\n      :py:func:`pysteps.noise.fftgenerators.generate_noise_2d_fft_filter`.\n    \"\"\"\n    if len(X.shape) < 2 or len(X.shape) > 3:\n        raise ValueError(\"the input is not two- or three-dimensional array\")\n    if np.any(~np.isfinite(X)):\n        raise ValueError(\"X contains non-finite values\")\n\n    # defaults\n    win_type = kwargs.get(\"win_type\", \"flat-hanning\")\n    donorm = kwargs.get(\"donorm\", False)\n    rm_rdisc = kwargs.get(\"rm_rdisc\", True)\n    use_full_fft = kwargs.get(\"use_full_fft\", False)\n    fft = kwargs.get(\"fft_method\", \"numpy\")\n    if type(fft) == str:\n        fft_shape = X.shape if len(X.shape) == 2 else X.shape[1:]\n        fft = utils.get_method(fft, shape=fft_shape)\n\n    X = X.copy()\n\n    # remove rain/no-rain discontinuity\n    if rm_rdisc:\n        X[X > X.min()] -= X[X > X.min()].min() - X.min()\n\n    # dims\n    if len(X.shape) == 2:\n        X = X[None, :, :]\n    nr_fields = X.shape[0]\n    field_shape = X.shape[1:]\n    if use_full_fft:\n        fft_shape = (X.shape[1], X.shape[2])\n    else:\n        fft_shape = (X.shape[1], int(X.shape[2] / 2) + 1)\n\n    # make sure non-rainy pixels are set to zero\n    X -= X.min(axis=(1, 2))[:, None, None]\n\n    if win_type is not None:\n        tapering = build_2D_tapering_function(field_shape, win_type)\n    else:\n        tapering = np.ones(field_shape)\n\n    F = np.zeros(fft_shape, dtype=complex)\n    for i in range(nr_fields):\n        if use_full_fft:\n            F += fft.fft2(X[i, :, :] * tapering)\n        else:\n            F += fft.rfft2(X[i, :, :] * tapering)\n    F /= nr_fields\n\n    # normalize the real and imaginary parts\n    if donorm:\n        if np.std(F.imag) > 0:\n            F.imag = (F.imag - np.mean(F.imag)) / np.std(F.imag)\n        if np.std(F.real) > 0:\n            F.real = (F.real - np.mean(F.real)) / np.std(F.real)\n\n    return {\"field\": np.abs(F), \"input_shape\": X.shape[1:], \"use_full_fft\": use_full_fft}",
  "def generate_noise_2d_fft_filter(F, randstate=None, seed=None, fft_method=None):\n    \"\"\"Produces a field of correlated noise using global Fourier filtering.\n\n    Parameters\n    ----------\n    F : dict\n        A filter object returned by\n        :py:func:`pysteps.noise.fftgenerators.initialize_param_2d_fft_filter` or\n        :py:func:`pysteps.noise.fftgenerators.initialize_nonparam_2d_fft_filter`.\n        All values in the filter array are required to be finite.\n    randstate : mtrand.RandomState\n        Optional random generator to use. If set to None, use numpy.random.\n    seed : int\n        Value to set a seed for the generator. None will not set the seed.\n    fft_method : str or tuple\n        A string or a (function,kwargs) tuple defining the FFT method to use\n        (see \"FFT methods\" in :py:func:`pysteps.utils.interface.get_method`).\n        Defaults to \"numpy\".\n\n    Returns\n    -------\n    N : array-like\n        A two-dimensional numpy array of stationary correlated noise.\n    \"\"\"\n    input_shape = F[\"input_shape\"]\n    use_full_fft = F[\"use_full_fft\"]\n    F = F[\"field\"]\n\n    if len(F.shape) != 2:\n        raise ValueError(\"field is not two-dimensional array\")\n    if np.any(~np.isfinite(F)):\n        raise ValueError(\"field contains non-finite values\")\n\n    if randstate is None:\n        randstate = np.random\n\n    # set the seed\n    if seed is not None:\n        randstate.seed(seed)\n\n    if fft_method is None:\n        fft = utils.get_method(\"numpy\", shape=input_shape)\n    else:\n        if type(fft_method) == str:\n            fft = utils.get_method(fft_method, shape=input_shape)\n        else:\n            fft = fft_method\n\n    # produce fields of white noise\n    N = randstate.randn(input_shape[0], input_shape[1])\n\n    # apply the global Fourier filter to impose a correlation structure\n    if use_full_fft:\n        fN = fft.fft2(N)\n    else:\n        fN = fft.rfft2(N)\n    fN *= F\n    if use_full_fft:\n        N = np.array(fft.ifft2(fN).real)\n    else:\n        N = np.array(fft.irfft2(fN))\n    N = (N - N.mean()) / N.std()\n\n    return N",
  "def initialize_nonparam_2d_ssft_filter(X, **kwargs):\n    \"\"\"Function to compute the local Fourier filters using the Short-Space Fourier\n    filtering approach.\n\n    Parameters\n    ----------\n    X : array-like\n        Two- or three-dimensional array containing one or more input fields.\n        All values are required to be finite. If more than one field are passed,\n        the average fourier filter is returned. It assumes that fields are stacked\n        by the first axis: [nr_fields, y, x].\n\n    Other Parameters\n    ----------------\n    win_size : int or two-element tuple of ints\n        Size-length of the window to compute the SSFT (default (128, 128)).\n    win_type : {'hanning', 'flat-hanning'}\n        Optional tapering function to be applied to X, generated with\n        :py:func:`pysteps.noise.fftgenerators.build_2D_tapering_function`\n        (default 'flat-hanning').\n    overlap : float [0,1[\n        The proportion of overlap to be applied between successive windows\n        (default 0.3).\n    war_thr : float [0,1]\n        Threshold for the minimum fraction of rain needed for computing the FFT\n        (default 0.1).\n    rm_rdisc : bool\n        Whether or not to remove the rain/no-rain disconituity. It assumes no-rain\n        pixels are assigned with lowest value.\n    fft_method : str or tuple\n        A string or a (function,kwargs) tuple defining the FFT method to use\n        (see \"FFT methods\" in :py:func:`pysteps.utils.interface.get_method`).\n        Defaults to \"numpy\".\n\n    Returns\n    -------\n    field : array-like\n        Four-dimensional array containing the 2d fourier filters distributed over\n        a 2d spatial grid.\n        It can be passed to\n        :py:func:`pysteps.noise.fftgenerators.generate_noise_2d_ssft_filter`.\n\n    References\n    ----------\n    :cite:`NBSG2017`\n\n    \"\"\"\n\n    if len(X.shape) < 2 or len(X.shape) > 3:\n        raise ValueError(\"the input is not two- or three-dimensional array\")\n    if np.any(np.isnan(X)):\n        raise ValueError(\"X must not contain NaNs\")\n\n    # defaults\n    win_size = kwargs.get(\"win_size\", (128, 128))\n    if type(win_size) == int:\n        win_size = (win_size, win_size)\n    win_type = kwargs.get(\"win_type\", \"flat-hanning\")\n    overlap = kwargs.get(\"overlap\", 0.3)\n    war_thr = kwargs.get(\"war_thr\", 0.1)\n    rm_rdisc = kwargs.get(\"rm_disc\", True)\n    fft = kwargs.get(\"fft_method\", \"numpy\")\n    if type(fft) == str:\n        fft_shape = X.shape if len(X.shape) == 2 else X.shape[1:]\n        fft = utils.get_method(fft, shape=fft_shape)\n\n    X = X.copy()\n\n    # remove rain/no-rain discontinuity\n    if rm_rdisc:\n        X[X > X.min()] -= X[X > X.min()].min() - X.min()\n\n    # dims\n    if len(X.shape) == 2:\n        X = X[None, :, :]\n    nr_fields = X.shape[0]\n    dim = X.shape[1:]\n    dim_x = dim[1]\n    dim_y = dim[0]\n\n    # make sure non-rainy pixels are set to zero\n    X -= X.min(axis=(1, 2))[:, None, None]\n\n    # SSFT algorithm\n\n    # prepare indices\n    idxi = np.zeros((2, 1), dtype=int)\n    idxj = np.zeros((2, 1), dtype=int)\n\n    # number of windows\n    num_windows_y = np.ceil(float(dim_y) / win_size[0]).astype(int)\n    num_windows_x = np.ceil(float(dim_x) / win_size[1]).astype(int)\n\n    # domain fourier filter\n    F0 = initialize_nonparam_2d_fft_filter(\n        X, win_type=win_type, donorm=True, use_full_fft=True, fft_method=fft\n    )[\"field\"]\n    # and allocate it to the final grid\n    F = np.zeros((num_windows_y, num_windows_x, F0.shape[0], F0.shape[1]))\n    F += F0[np.newaxis, np.newaxis, :, :]\n\n    # loop rows\n    for i in range(F.shape[0]):\n        # loop columns\n        for j in range(F.shape[1]):\n\n            # compute indices of local window\n            idxi[0] = int(np.max((i * win_size[0] - overlap * win_size[0], 0)))\n            idxi[1] = int(\n                np.min((idxi[0] + win_size[0] + overlap * win_size[0], dim_y))\n            )\n            idxj[0] = int(np.max((j * win_size[1] - overlap * win_size[1], 0)))\n            idxj[1] = int(\n                np.min((idxj[0] + win_size[1] + overlap * win_size[1], dim_x))\n            )\n\n            # build localization mask\n            # TODO: the 0.01 rain threshold must be improved\n            mask = _get_mask(dim, idxi, idxj, win_type)\n            war = float(np.sum((X * mask[None, :, :]) > 0.01)) / (\n                (idxi[1] - idxi[0]) * (idxj[1] - idxj[0]) * nr_fields\n            )\n\n            if war > war_thr:\n                # the new filter\n                F[i, j, :, :] = initialize_nonparam_2d_fft_filter(\n                    X * mask[None, :, :],\n                    win_type=None,\n                    donorm=True,\n                    use_full_fft=True,\n                    fft_method=fft,\n                )[\"field\"]\n\n    return {\"field\": F, \"input_shape\": X.shape[1:], \"use_full_fft\": True}",
  "def initialize_nonparam_2d_nested_filter(X, gridres=1.0, **kwargs):\n    \"\"\"Function to compute the local Fourier filters using a nested approach.\n\n    Parameters\n    ----------\n    X : array-like\n        Two- or three-dimensional array containing one or more input fields.\n        All values are required to be finite.\n        If more than one field are passed, the average fourier filter is returned.\n        It assumes that fields are stacked by the first axis: [nr_fields, y, x].\n    gridres : float\n        Grid resolution in km.\n\n    Other Parameters\n    ----------------\n    max_level : int\n        Localization parameter. 0: global noise, >0: increasing degree of\n        localization (default 3).\n    win_type : {'hanning', 'flat-hanning'}\n        Optional tapering function to be applied to X, generated with\n        :py:func:`pysteps.noise.fftgenerators.build_2D_tapering_function`\n        (default 'flat-hanning').\n    war_thr : float [0;1]\n        Threshold for the minimum fraction of rain needed for computing the FFT\n        (default 0.1).\n    rm_rdisc : bool\n        Whether or not to remove the rain/no-rain disconituity. It assumes no-rain\n        pixels are assigned with lowest value.\n    fft_method : str or tuple\n        A string or a (function,kwargs) tuple defining the FFT method to use\n        (see \"FFT methods\" in :py:func:`pysteps.utils.interface.get_method`).\n        Defaults to \"numpy\".\n\n    Returns\n    -------\n    field : array-like\n        Four-dimensional array containing the 2d fourier filters distributed over\n        a 2d spatial grid.\n        It can be passed to\n        :py:func:`pysteps.noise.fftgenerators.generate_noise_2d_ssft_filter`.\n    \"\"\"\n\n    if len(X.shape) < 2 or len(X.shape) > 3:\n        raise ValueError(\"the input is not two- or three-dimensional array\")\n    if np.any(np.isnan(X)):\n        raise ValueError(\"X must not contain NaNs\")\n\n    # defaults\n    max_level = kwargs.get(\"max_level\", 3)\n    win_type = kwargs.get(\"win_type\", \"flat-hanning\")\n    war_thr = kwargs.get(\"war_thr\", 0.1)\n    rm_rdisc = kwargs.get(\"rm_disc\", True)\n    fft = kwargs.get(\"fft_method\", \"numpy\")\n    if type(fft) == str:\n        fft_shape = X.shape if len(X.shape) == 2 else X.shape[1:]\n        fft = utils.get_method(fft, shape=fft_shape)\n\n    X = X.copy()\n\n    # remove rain/no-rain discontinuity\n    if rm_rdisc:\n        X[X > X.min()] -= X[X > X.min()].min() - X.min()\n\n    # dims\n    if len(X.shape) == 2:\n        X = X[None, :, :]\n    nr_fields = X.shape[0]\n    dim = X.shape[1:]\n    dim_x = dim[1]\n    dim_y = dim[0]\n\n    # make sure non-rainy pixels are set to zero\n    X -= X.min(axis=(1, 2))[:, None, None]\n\n    # Nested algorithm\n\n    # prepare indices\n    Idxi = np.array([[0, dim_y]])\n    Idxj = np.array([[0, dim_x]])\n    Idxipsd = np.array([[0, 2 ** max_level]])\n    Idxjpsd = np.array([[0, 2 ** max_level]])\n\n    # generate the FFT sample frequencies\n    freqx = fft.fftfreq(dim_x, gridres)\n    freqy = fft.fftfreq(dim_y, gridres)\n    fx, fy = np.meshgrid(freqx, freqy)\n    freq_grid = np.sqrt(fx ** 2 + fy ** 2)\n\n    # domain fourier filter\n    F0 = initialize_nonparam_2d_fft_filter(\n        X, win_type=win_type, donorm=True, use_full_fft=True, fft_method=fft\n    )[\"field\"]\n    # and allocate it to the final grid\n    F = np.zeros((2 ** max_level, 2 ** max_level, F0.shape[0], F0.shape[1]))\n    F += F0[np.newaxis, np.newaxis, :, :]\n\n    # now loop levels and build composite spectra\n    level = 0\n    while level < max_level:\n\n        for m in range(len(Idxi)):\n\n            # the indices of rainfall field\n            Idxinext, Idxjnext = _split_field(Idxi[m, :], Idxj[m, :], 2)\n            # the indices of the field of fourier filters\n            Idxipsdnext, Idxjpsdnext = _split_field(Idxipsd[m, :], Idxjpsd[m, :], 2)\n\n            for n in range(len(Idxinext)):\n\n                mask = _get_mask(dim, Idxinext[n, :], Idxjnext[n, :], win_type)\n                war = np.sum((X * mask[None, :, :]) > 0.01) / float(\n                    (Idxinext[n, 1] - Idxinext[n, 0])\n                    * (Idxjnext[n, 1] - Idxjnext[n, 0])\n                    * nr_fields\n                )\n\n                if war > war_thr:\n                    # the new filter\n                    newfilter = initialize_nonparam_2d_fft_filter(\n                        X * mask[None, :, :],\n                        win_type=None,\n                        donorm=True,\n                        use_full_fft=True,\n                        fft_method=fft,\n                    )[\"field\"]\n\n                    # compute logistic function to define weights as function of frequency\n                    # k controls the shape of the weighting function\n                    # TODO: optimize parameters\n                    k = 0.05\n                    x0 = (\n                        Idxinext[n, 1] - Idxinext[n, 0]\n                    ) / 2.0  # TODO: consider y dimension, too\n                    merge_weights = 1 / (1 + np.exp(-k * (1 / freq_grid - x0)))\n                    newfilter *= 1 - merge_weights\n\n                    # perform the weighted average of previous and new fourier filters\n                    F[\n                        Idxipsdnext[n, 0] : Idxipsdnext[n, 1],\n                        Idxjpsdnext[n, 0] : Idxjpsdnext[n, 1],\n                        :,\n                        :,\n                    ] *= merge_weights[np.newaxis, np.newaxis, :, :]\n                    F[\n                        Idxipsdnext[n, 0] : Idxipsdnext[n, 1],\n                        Idxjpsdnext[n, 0] : Idxjpsdnext[n, 1],\n                        :,\n                        :,\n                    ] += newfilter[np.newaxis, np.newaxis, :, :]\n\n        # update indices\n        level += 1\n        Idxi, Idxj = _split_field((0, dim[0]), (0, dim[1]), 2 ** level)\n        Idxipsd, Idxjpsd = _split_field(\n            (0, 2 ** max_level), (0, 2 ** max_level), 2 ** level\n        )\n\n    return {\"field\": F, \"input_shape\": X.shape[1:], \"use_full_fft\": True}",
  "def generate_noise_2d_ssft_filter(F, randstate=None, seed=None, **kwargs):\n    \"\"\"Function to compute the locally correlated noise using a nested approach.\n\n    Parameters\n    ----------\n    F : array-like\n        A filter object returned by\n        :py:func:`pysteps.noise.fftgenerators.initialize_nonparam_2d_ssft_filter` or\n        :py:func:`pysteps.noise.fftgenerators.initialize_nonparam_2d_nested_filter`.\n        The filter is a four-dimensional array containing the 2d fourier filters\n        distributed over a 2d spatial grid.\n    randstate : mtrand.RandomState\n        Optional random generator to use. If set to None, use numpy.random.\n    seed : int\n        Value to set a seed for the generator. None will not set the seed.\n\n    Other Parameters\n    ----------------\n    overlap : float\n        Percentage overlap [0-1] between successive windows (default 0.2).\n    win_type : {'hanning', 'flat-hanning'}\n        Optional tapering function to be applied to X, generated with\n        :py:func:`pysteps.noise.fftgenerators.build_2D_tapering_function`\n        (default 'flat-hanning').\n    fft_method : str or tuple\n        A string or a (function,kwargs) tuple defining the FFT method to use\n        (see \"FFT methods\" in :py:func:`pysteps.utils.interface.get_method`).\n        Defaults to \"numpy\".\n\n    Returns\n    -------\n    N : array-like\n        A two-dimensional numpy array of non-stationary correlated noise.\n\n    \"\"\"\n    input_shape = F[\"input_shape\"]\n    use_full_fft = F[\"use_full_fft\"]\n    F = F[\"field\"]\n\n    if len(F.shape) != 4:\n        raise ValueError(\"the input is not four-dimensional array\")\n    if np.any(~np.isfinite(F)):\n        raise ValueError(\"field contains non-finite values\")\n\n    # defaults\n    overlap = kwargs.get(\"overlap\", 0.2)\n    win_type = kwargs.get(\"win_type\", \"flat-hanning\")\n    fft = kwargs.get(\"fft_method\", \"numpy\")\n    if type(fft) == str:\n        fft = utils.get_method(fft, shape=input_shape)\n\n    if randstate is None:\n        randstate = np.random\n\n    # set the seed\n    if seed is not None:\n        randstate.seed(seed)\n\n    dim_y = F.shape[2]\n    dim_x = F.shape[3]\n    dim = (dim_y, dim_x)\n\n    # produce fields of white noise\n    N = randstate.randn(dim_y, dim_x)\n    fN = fft.fft2(N)\n\n    # initialize variables\n    cN = np.zeros(dim)\n    sM = np.zeros(dim)\n\n    idxi = np.zeros((2, 1), dtype=int)\n    idxj = np.zeros((2, 1), dtype=int)\n\n    # get the window size\n    win_size = (float(dim_y) / F.shape[0], float(dim_x) / F.shape[1])\n\n    # loop the windows and build composite image of correlated noise\n\n    # loop rows\n    for i in range(F.shape[0]):\n        # loop columns\n        for j in range(F.shape[1]):\n\n            # apply fourier filtering with local filter\n            lF = F[i, j, :, :]\n            flN = fN * lF\n            flN = np.array(fft.ifft2(flN).real)\n\n            # compute indices of local window\n            idxi[0] = int(np.max((i * win_size[0] - overlap * win_size[0], 0)))\n            idxi[1] = int(np.min(\n                (idxi[0] + win_size[0] + overlap * win_size[0], dim_y)\n            ))\n            idxj[0] = int(np.max((j * win_size[1] - overlap * win_size[1], 0)))\n            idxj[1] = int(np.min(\n                (idxj[0] + win_size[1] + overlap * win_size[1], dim_x)\n            ))\n\n            # build mask and add local noise field to the composite image\n            M = _get_mask(dim, idxi, idxj, win_type)\n            cN += flN * M\n            sM += M\n\n    # normalize the field\n    cN[sM > 0] /= sM[sM > 0]\n    cN = (cN - cN.mean()) / cN.std()\n\n    return cN",
  "def build_2D_tapering_function(win_size, win_type=\"flat-hanning\"):\n    \"\"\"Produces two-dimensional tapering function for rectangular fields.\n\n    Parameters\n    ----------\n    win_size : tuple of int\n        Size of the tapering window as two-element tuple of integers.\n    win_type : {'hanning', 'flat-hanning'}\n        Name of the tapering window type\n\n    Returns\n    -------\n    w2d : array-like\n        A two-dimensional numpy array containing the 2D tapering function.\n    \"\"\"\n\n    if len(win_size) != 2:\n        raise ValueError(\"win_size is not a two-element tuple\")\n\n    if win_type == \"hanning\":\n        w1dr = np.hanning(win_size[0])\n        w1dc = np.hanning(win_size[1])\n\n    elif win_type == \"flat-hanning\":\n\n        T = win_size[0] / 4.0\n        W = win_size[0] / 2.0\n        B = np.linspace(-W, W, 2 * W)\n        R = np.abs(B) - T\n        R[R < 0] = 0.0\n        A = 0.5 * (1.0 + np.cos(np.pi * R / T))\n        A[np.abs(B) > (2 * T)] = 0.0\n        w1dr = A\n\n        T = win_size[1] / 4.0\n        W = win_size[1] / 2.0\n        B = np.linspace(-W, W, 2 * W)\n        R = np.abs(B) - T\n        R[R < 0] = 0.0\n        A = 0.5 * (1.0 + np.cos(np.pi * R / T))\n        A[np.abs(B) > (2 * T)] = 0.0\n        w1dc = A\n\n    else:\n        raise ValueError(\"unknown win_type %s\" % win_type)\n\n    # Expand to 2-D\n    # w2d = np.sqrt(np.outer(w1dr,w1dc))\n    w2d = np.outer(w1dr, w1dc)\n\n    # Set nans to zero\n    if np.sum(np.isnan(w2d)) > 0:\n        w2d[np.isnan(w2d)] = np.min(w2d[w2d > 0])\n\n    return w2d",
  "def _split_field(idxi, idxj, Segments):\n    \"\"\" Split domain field into a number of equally sapced segments.\n    \"\"\"\n\n    sizei = idxi[1] - idxi[0]\n    sizej = idxj[1] - idxj[0]\n\n    winsizei = int(sizei / Segments)\n    winsizej = int(sizej / Segments)\n\n    Idxi = np.zeros((Segments ** 2, 2))\n    Idxj = np.zeros((Segments ** 2, 2))\n\n    count = -1\n    for i in range(Segments):\n        for j in range(Segments):\n            count += 1\n            Idxi[count, 0] = idxi[0] + i * winsizei\n            Idxi[count, 1] = np.min((Idxi[count, 0] + winsizei, idxi[1]))\n            Idxj[count, 0] = idxj[0] + j * winsizej\n            Idxj[count, 1] = np.min((Idxj[count, 0] + winsizej, idxj[1]))\n\n    Idxi = np.array(Idxi).astype(int)\n    Idxj = np.array(Idxj).astype(int)\n\n    return Idxi, Idxj",
  "def _get_mask(Size, idxi, idxj, win_type):\n    \"\"\"Compute a mask of zeros with a window at a given position.\n    \"\"\"\n\n    idxi = np.array(idxi).astype(int)\n    idxj = np.array(idxj).astype(int)\n\n    win_size = (idxi[1] - idxi[0], idxj[1] - idxj[0])\n    wind = build_2D_tapering_function(win_size, win_type)\n\n    mask = np.zeros(Size)\n    mask[idxi.item(0) : idxi.item(1), idxj.item(0) : idxj.item(1)] = wind\n\n    return mask",
  "def piecewise_linear(x, x0, y0, beta1, beta2):\n            return np.piecewise(\n                x,\n                [x < x0, x >= x0],\n                [\n                    lambda x: beta1 * x + y0 - beta1 * x0,\n                    lambda x: beta2 * x + y0 - beta2 * x0,\n                ],\n            )",
  "def get_method(name):\n    \"\"\"\n    Return two callable functions to initialize and generate 2d perturbations\n    of precipitation or velocity fields.\\n\n\n    Methods for precipitation fields:\n\n    +-------------------+------------------------------------------------------+\n    |     Name          |              Description                             |\n    +===================+======================================================+\n    |  parametric       | this global generator uses parametric Fourier        |\n    |                   | filtering (power-law model)                          |\n    +-------------------+------------------------------------------------------+\n    |  nonparametric    | this global generator uses nonparametric Fourier     |\n    |                   | filtering                                            |\n    +-------------------+------------------------------------------------------+\n    |  ssft             | this local generator uses the short-space Fourier    |\n    |                   | filtering                                            |\n    +-------------------+------------------------------------------------------+\n    |  nested           | this local generator uses a nested Fourier filtering |\n    +-------------------+------------------------------------------------------+\n\n    Methods for velocity fields:\n\n    +-------------------+------------------------------------------------------+\n    |     Name          |              Description                             |\n    +===================+======================================================+\n    |  bps              | The method described in :cite:`BPS2006`, where       |\n    |                   | time-dependent velocity perturbations are sampled    |\n    |                   | from the exponential distribution                    |\n    +-------------------+------------------------------------------------------+\n\n    \"\"\"\n    if isinstance(name, str):\n        name = name.lower()\n    else:\n        raise TypeError(\"Only strings supported for the method's names.\\n\"\n                        + \"Available names:\"\n                        + str(list(_noise_methods.keys()))) from None\n\n    try:\n        return _noise_methods[name]\n    except KeyError:\n        raise ValueError(\"Unknown method {}\\n\".format(name)\n                         + \"The available methods are:\"\n                         + str(list(_noise_methods.keys()))) from None",
  "def adjust_lag2_corrcoef1(gamma_1, gamma_2):\n    \"\"\"A simple adjustment of lag-2 temporal autocorrelation coefficient to\n    ensure that the resulting AR(2) process is stationary when the parameters\n    are estimated from the Yule-Walker equations.\n\n    Parameters\n    ----------\n    gamma_1 : float\n      Lag-1 temporal autocorrelation coeffient.\n    gamma_2 : float\n      Lag-2 temporal autocorrelation coeffient.\n\n    Returns\n    -------\n    out : float\n      The adjusted lag-2 correlation coefficient.\n\n    \"\"\"\n    gamma_2 = max(gamma_2, 2*gamma_1*gamma_1-1+1e-10)\n    gamma_2 = min(gamma_2, 1-1e-10)\n\n    return gamma_2",
  "def adjust_lag2_corrcoef2(gamma_1, gamma_2):\n    \"\"\"A more advanced adjustment of lag-2 temporal autocorrelation coefficient\n    to ensure that the resulting AR(2) process is stationary when\n    the parameters are estimated from the Yule-Walker equations.\n\n    Parameters\n    ----------\n    gamma_1 : float\n      Lag-1 temporal autocorrelation coeffient.\n    gamma_2 : float\n      Lag-2 temporal autocorrelation coeffient.\n\n    Returns\n    -------\n    out : float\n      The adjusted lag-2 correlation coefficient.\n\n    \"\"\"\n    gamma_2 = max(gamma_2, 2*gamma_1*gamma_2-1)\n    gamma_2 = max(gamma_2, (3*gamma_1**2-2+2*(1-gamma_1**2)**1.5) / gamma_1**2)\n\n    return gamma_2",
  "def ar_acf(gamma, n=None):\n    \"\"\"Compute theoretical autocorrelation function (ACF) from the AR(p) model\n    with lag-l, l=1,2,...,p temporal autocorrelation coefficients.\n\n    Parameters\n    ----------\n    gamma : array-like\n      Array of length p containing the lag-l, l=1,2,...p, temporal\n      autocorrelation coefficients.\n      The correlation coefficients are assumed to be in ascending\n      order with respect to time lag.\n    n : int\n      Desired length of ACF array. Must be greater than len(gamma).\n\n    Returns\n    -------\n    out : array-like\n      Array containing the ACF values.\n\n    \"\"\"\n    ar_order = len(gamma)\n    if n == ar_order or n is None:\n        return gamma\n    elif n < ar_order:\n        raise ValueError(\"n=%i, but must be larger than the order of the AR process %i\" % (n,ar_order))\n\n    phi = estimate_ar_params_yw(gamma)[:-1]\n\n    acf = gamma.copy()\n    for t in range(0, n - ar_order):\n        # Retrieve gammas (in reverse order)\n        gammas = acf[t:t + ar_order][::-1]\n        # Compute next gamma\n        gamma_ = np.sum(gammas*phi)\n        acf.append(gamma_)\n\n    return acf",
  "def estimate_ar_params_yw(gamma):\n    \"\"\"Estimate the parameters of an AR(p) model from the Yule-Walker equations\n    using the given set of autocorrelation coefficients.\n\n    Parameters\n    ----------\n    gamma : array_like\n      Array of length p containing the lag-l, l=1,2,...p, temporal\n      autocorrelation coefficients.\n      The correlation coefficients are assumed to be in ascending\n      order with respect to time lag.\n\n    Returns\n    -------\n    out : ndarray\n      An array of shape (n,p+1) containing the AR(p) parameters for for the\n      lag-p terms for each cascade level, and also the standard deviation of\n      the innovation term.\n\n    \"\"\"\n    p = len(gamma)\n\n    phi = np.empty(p+1)\n\n    g = np.hstack([[1.0], gamma])\n    G = []\n    for j in range(p):\n        G.append(np.roll(g[:-1], j))\n    G = np.array(G)\n    phi_ = np.linalg.solve(G, g[1:].flatten())\n\n    # Check that the absolute values of the roots of the characteristic\n    # polynomial are less than one.\n    # Otherwise the AR(p) model is not stationary.\n    r = np.array([np.abs(r_) for r_ in np.roots([1.0 if i == 0 else -phi_[i] \\\n                  for i in range(p)])])\n    if any(r >= 1):\n        raise RuntimeError(\n            \"Error in estimate_ar_params_y: \"\n            \"nonstationary AR(p) process\")\n\n    c = 1.0\n    for j in range(p):\n        c -= gamma[j] * phi_[j]\n    phi_pert = np.sqrt(c)\n\n    # If the expression inside the square root is negative, phi_pert cannot\n    # be computed and it is set to zero instead.\n    if not np.isfinite(phi_pert):\n        phi_pert = 0.0\n\n    phi[:p] = phi_\n    phi[-1] = phi_pert\n\n    return phi",
  "def iterate_ar_model(X, phi, EPS=None):\n    \"\"\"Apply an AR(p) model to a time series of two-dimensional fields.\n\n    Parameters\n    ----------\n    X : array_like\n      Three-dimensional array of shape (p,h,w) containing a time series of p\n      two-dimensional fields of shape (h,w). The fields are assumed to be in\n      ascending order by time, and the time intervals are assumed\n      to be regular.\n    phi : array_like\n      Array of length p+1 specifying the parameters of the AR(p) model. The\n      parameters are in ascending order by increasing time lag, and the last\n      element is the parameter corresponding to the innovation term EPS.\n    EPS : array_like\n      Optional perturbation field for the AR(p) process. If EPS is None, the\n      innovation term is not added.\n\n    \"\"\"\n    if X.shape[0] != len(phi)-1:\n        raise ValueError(\"dimension mismatch between X and phi: X.shape[0]=%d, len(phi)=%d\" % (X.shape[0], len(phi)))\n\n    if EPS is not None and EPS.shape != (X.shape[1], X.shape[2]):\n        raise ValueError(\"dimension mismatch between X and EPS: X.shape=%s, EPS.shape=%s\" % (str(X.shape), str(EPS.shape)))\n\n    X_new = 0.0\n\n    p = len(phi) - 1\n\n    for i in range(p):\n        X_new += phi[i] * X[-(i+1), :, :]\n\n    if EPS is not None:\n        X_new += phi[-1] * EPS\n\n    return np.stack(list(X[1:, :, :]) + [X_new])",
  "def temporal_autocorrelation(X, MASK=None):\n    \"\"\"Compute lag-l autocorrelation coefficients gamma_l, l=1,2,...,n-1, for a\n    time series of n two-dimensional input fields.\n\n    Parameters\n    ----------\n    X : array_like\n      Two-dimensional array of shape (n, w, h) containing a time series of n\n      two-dimensional fields of shape (w, h). The input fields are assumed to\n      be in increasing order with respect to time, and the time step is assumed\n      to be regular (i.e. no missing data). X is required to have\n      finite values.\n    MASK : array_like\n      Optional mask to use for computing the correlation coefficients. Pixels\n      with MASK==False are excluded from the computations.\n\n    Returns\n    -------\n    out : ndarray\n      Array of length n-1 containing the temporal autocorrelation coefficients\n      for time lags l=1,2,...,n-1.\n\n    \"\"\"\n    if len(X.shape) != 3:\n        raise ValueError(\"the input X is not three-dimensional array\")\n    if MASK is not None and MASK.shape != X.shape[1:3]:\n        raise ValueError(\"dimension mismatch between X and MASK: X.shape=%s, MASK.shape=%s\" % \\\n                         (str(X.shape), str(MASK.shape)))\n    if np.any(~np.isfinite(X)):\n        raise ValueError(\"X contains non-finite values\")\n\n    gamma = np.empty(X.shape[0]-1)\n\n    if MASK is None:\n        MASK = np.ones((X.shape[1], X.shape[2]), dtype=bool)\n\n    gamma = []\n    for k in range(X.shape[0] - 1):\n        gamma.append(np.corrcoef(X[-1, :, :][MASK],\n                                 X[-(k+2), :, :][MASK])[0, 1])\n\n    return gamma",
  "def det_cont_fct(pred, obs, scores=\"\", axis=None, conditioning=None, thr=0.0):\n    \"\"\"Calculate simple and skill scores for deterministic continuous forecasts.\n\n    Parameters\n    ----------\n\n    pred : array_like\n        Array of predictions. NaNs are ignored.\n\n    obs : array_like\n        Array of verifying observations. NaNs are ignored.\n\n    scores : {string, list of strings}, optional\n        The name(s) of the scores. The default, scores=\"\", will compute all\n        available scores.\n        The available score names are:\n\n        .. tabularcolumns:: |p{2cm}|L|\n\n        +------------+--------------------------------------------------------+\n        | Name       | Description                                            |\n        +============+========================================================+\n        |  beta1     | linear regression slope (type 1 conditional bias)      |\n        +------------+--------------------------------------------------------+\n        |  beta2     | linear regression slope (type 2 conditional bias)      |\n        +------------+--------------------------------------------------------+\n        |  corr_p    | pearson's correleation coefficien (linear correlation) |\n        +------------+--------------------------------------------------------+\n        |  corr_s*   | spearman's correlation coefficient (rank correlation)  |\n        +------------+--------------------------------------------------------+\n        |  DRMSE     | debiased root mean squared error                       |\n        +------------+--------------------------------------------------------+\n        |  MAE       | mean absolute error                                    |\n        +------------+--------------------------------------------------------+\n        |  ME        | mean error or bias                                     |\n        +------------+--------------------------------------------------------+\n        |  MSE       | mean squared error                                     |\n        +------------+--------------------------------------------------------+\n        |  NMSE      | normalized mean squared error                          |\n        +------------+--------------------------------------------------------+\n        |  RMSE      | root mean squared error                                |\n        +------------+--------------------------------------------------------+\n        |  RV        | reduction of variance                                  |\n        |            | (Brier Score, Nash-Sutcliffe Efficiency)               |\n        +------------+--------------------------------------------------------+\n        |  scatter*  | half the distance between the 16% and 84% percentiles  |\n        |            | of the weighted cumulative error distribution,         |\n        |            | where error = dB(pred/obs),                            |\n        |            | as in Germann et al. (2006)                            |\n        +------------+--------------------------------------------------------+\n\n    axis : {int, tuple of int, None}, optional\n        Axis or axes along which a score is integrated. The default, axis=None,\n        will integrate all of the elements of the input arrays.\\n\n        If axis is -1 (or any negative integer),\n        the integration is not performed\n        and scores are computed on all of the elements in the input arrays.\\n\n        If axis is a tuple of ints, the integration is performed on all of the\n        axes specified in the tuple.\n\n    conditioning : {None, \"single\", \"double\"}, optional\n        The type of conditioning used for the verification.\n        The default, conditioning=None, includes all pairs. With\n        conditioning=\"single\", only pairs with either pred or obs > thr are\n        included. With conditioning=\"double\", only pairs with both pred and\n        obs > thr are included.\n\n    thr : float\n        Optional threshold value for conditioning. Defaults to 0.\n\n    Returns\n    -------\n\n    result : dict\n        Dictionary containing the verification results.\n\n    Notes\n    -----\n\n    Multiplicative scores can be computed by passing log-tranformed values.\n    Note that \"scatter\" is the only score that will be computed in dB units of\n    the multiplicative error, i.e.: 10*log10(pred/obs).\n\n    beta1 measures the degree of conditional bias of the observations given the\n    forecasts (type 1).\n\n    beta2 measures the degree of conditional bias of the forecasts given the\n    observations (type 2).\n\n    The normalized MSE is computed as\n    NMSE = E[(pred - obs)^2]/E[(pred + obs)^2].\n\n    The debiased RMSE is computed as DRMSE = sqrt(RMSE - ME^2).\n\n    The reduction of variance score is computed as RV = 1 - MSE/Var(obs).\n\n    Score names denoted by * can only be computed offline, meaning that the\n    these cannot be computed using _init, _accum and _compute methods of this\n    module.\n\n\n    References\n    ----------\n\n    Germann, U. , Galli, G. , Boscacci, M. and Bolliger, M. (2006), Radar\n    precipitation measurement in a mountainous region. Q.J.R. Meteorol. Soc.,\n    132: 1669-1692. doi:10.1256/qj.05.190\n\n    Potts, J. (2012), Chapter 2 - Basic concepts. Forecast verification: a\n    practitioner\u2019s guide in atmospheric sciences, I. T. Jolliffe, and D. B.\n    Stephenson, Eds., Wiley-Blackwell, 11\u201329.\n\n    See also\n    --------\n\n    pysteps.verification.detcatscores.det_cat_fct\n    \"\"\"\n\n    # catch case of single score passed as string\n    def get_iterable(x):\n        if isinstance(x, collections.Iterable) and not isinstance(x, str):\n            return x\n        else:\n            return (x,)\n\n    scores = get_iterable(scores)\n\n    # split between online and offline scores\n    loffline = [\"scatter\", \"corr_s\"]\n    onscores = [\n        score\n        for score in scores\n        if str(score).lower() not in loffline or score == \"\"\n    ]\n    offscores = [\n        score\n        for score in scores\n        if str(score).lower() in loffline or score == \"\"\n    ]\n\n    # unique lists\n    onscores = _uniquelist(onscores)\n    offscores = _uniquelist(offscores)\n\n    # online scores\n    onresult = {}\n    if onscores:\n\n        err = det_cont_fct_init(axis=axis, conditioning=conditioning, thr=thr)\n        det_cont_fct_accum(err, pred, obs)\n        onresult = det_cont_fct_compute(err, onscores)\n\n    # offline scores\n    offresult = {}\n    if offscores:\n\n        pred = np.asarray(pred.copy())\n        obs = np.asarray(obs.copy())\n\n        if pred.shape != obs.shape:\n            raise ValueError(\n                \"the shape of pred does not match the shape of obs %s!=%s\"\n                % (pred.shape, obs.shape)\n            )\n\n        # conditioning\n        if conditioning is not None:\n            if conditioning == \"single\":\n                idx = np.logical_or(obs > thr, pred > thr)\n            elif conditioning == \"double\":\n                idx = np.logical_and(obs > thr, pred > thr)\n            else:\n                raise ValueError(\"unkown conditioning %s\" % conditioning)\n            obs[~idx] = np.nan\n            pred[~idx] = np.nan\n\n        for score in offscores:\n            # catch None passed as score\n            if score is None:\n                continue\n\n            score_ = score.lower()\n\n            # spearman corr (rank correlation)\n            if score_ in [\"corr_s\", \"spearmanr\", \"\"]:\n                corr_s = _spearmanr(pred, obs, axis=axis)\n                offresult[\"corr_s\"] = corr_s\n\n            # scatter\n            if score_ in [\"scatter\", \"\"]:\n                scatter = _scatter(pred, obs, axis=axis)\n                offresult[\"scatter\"] = scatter\n\n    # pull all results together\n    result = onresult\n    result.update(offresult)\n\n    return result",
  "def det_cont_fct_init(axis=None, conditioning=None, thr=0.0):\n    \"\"\"Initialize a verification error object.\n\n    Parameters\n    ----------\n\n    axis : {int, tuple of int, None}, optional\n        Axis or axes along which a score is integrated. The default, axis=None,\n        will integrate all of the elements of the input arrays.\\n\n        If axis is -1 (or any negative integer),\n        the integration is not performed\n        and scores are computed on all of the elements in the input arrays.\\n\n        If axis is a tuple of ints, the integration is performed on all of the\n        axes specified in the tuple.\n\n    conditioning : {None, \"single\", \"double\"}, optional\n        The type of conditioning used for the verification.\n        The default, conditioning=None, includes all pairs. With\n        conditioning=\"single\", only pairs with either pred or obs > thr are\n        included. With conditioning=\"double\", only pairs with both pred and\n        obs > thr are included.\n\n    thr : float\n        Optional threshold value for conditioning. Defaults to 0.\n\n    Returns\n    -------\n\n    out : dict\n        The verification error object.\n\n    \"\"\"\n\n    err = {}\n\n    # catch case of axis passed as integer\n    def get_iterable(x):\n        if x is None or (\n            isinstance(x, collections.Iterable) and not isinstance(x, int)\n        ):\n            return x\n        else:\n            return (x,)\n\n    err[\"axis\"] = get_iterable(axis)\n    err[\"conditioning\"] = conditioning\n    err[\"thr\"] = thr\n    err[\"cov\"] = None\n    err[\"vobs\"] = None\n    err[\"vpred\"] = None\n    err[\"mobs\"] = None\n    err[\"mpred\"] = None\n    err[\"me\"] = None\n    err[\"mse\"] = None\n    err[\"mss\"] = None  # mean square sum, i.e. E[(pred + obs)^2]\n    err[\"mae\"] = None\n    err[\"n\"] = None\n\n    return err",
  "def det_cont_fct_accum(err, pred, obs):\n    \"\"\"Accumulate the forecast error in the verification error object.\n\n    Parameters\n    ----------\n\n    err : dict\n        A verification error object initialized with\n        :py:func:`pysteps.verification.detcontscores.det_cont_fct_init`.\n\n    pred : array_like\n        Array of predictions. NaNs are ignored.\n\n    obs : array_like\n        Array of verifying observations. NaNs are ignored.\n\n    References\n    ----------\n\n    Chan, Tony field.; Golub, Gene H.; LeVeque, Randall J. (1979), \"Updating\n    Formulae and a Pairwise Algorithm for Computing Sample Variances.\",\n    Technical Report STAN-CS-79-773, Department of Computer Science,\n    Stanford University.\n\n    Schubert, Erich; Gertz, Michael (2018-07-09). \"Numerically stable parallel\n    computation of (co-)variance\". ACM: 10. doi:10.1145/3221269.3223036.\n    \"\"\"\n\n    pred = np.asarray(pred.copy())\n    obs = np.asarray(obs.copy())\n    axis = tuple(range(pred.ndim)) if err[\"axis\"] is None else err[\"axis\"]\n\n    # checks\n    if pred.shape != obs.shape:\n        raise ValueError(\n            \"the shape of pred does not match the shape of obs %s!=%s\"\n            % (pred.shape, obs.shape)\n        )\n\n    if pred.ndim <= np.max(axis):\n        raise ValueError(\n            \"axis %d is out of bounds for array of dimension %d\"\n            % (np.max(axis), len(pred.shape))\n        )\n\n    idims = [dim not in axis for dim in range(pred.ndim)]\n    nshape = tuple(np.array(pred.shape)[np.array(idims)])\n    if err[\"cov\"] is None:\n        # initialize the error arrays in the verification object\n        err[\"cov\"] = np.zeros(nshape)\n        err[\"vobs\"] = np.zeros(nshape)\n        err[\"vpred\"] = np.zeros(nshape)\n        err[\"mobs\"] = np.zeros(nshape)\n        err[\"mpred\"] = np.zeros(nshape)\n        err[\"me\"] = np.zeros(nshape)\n        err[\"mse\"] = np.zeros(nshape)\n        err[\"mss\"] = np.zeros(nshape)\n        err[\"mae\"] = np.zeros(nshape)\n        err[\"n\"] = np.zeros(nshape)\n\n    else:\n        # check dimensions\n        if err[\"cov\"].shape != nshape:\n            raise ValueError(\n                \"the shape of the input arrays does not match \"\n                + \"the shape of the \"\n                + \"verification object %s!=%s\" % (nshape, err[\"cov\"].shape)\n            )\n\n    # conditioning\n    if err[\"conditioning\"] is not None:\n        if err[\"conditioning\"] == \"single\":\n            idx = np.logical_or(obs > err[\"thr\"], pred > err[\"thr\"])\n        elif err[\"conditioning\"] == \"double\":\n            idx = np.logical_and(obs > err[\"thr\"], pred > err[\"thr\"])\n        else:\n            raise ValueError(\"unkown conditioning %s\" % err[\"conditioning\"])\n        obs[~idx] = np.nan\n        pred[~idx] = np.nan\n\n    # add dummy axis in case integration is not required\n    if np.max(axis) < 0:\n        pred = pred[None, :]\n        obs = obs[None, :]\n        axis = (0,)\n    axis = tuple([a for a in axis if a >= 0])\n\n    # compute residuals\n    res = pred - obs\n    sum = pred + obs\n    n = np.sum(np.isfinite(res), axis=axis)\n\n    # new means\n    mobs = np.nanmean(obs, axis=axis)\n    mpred = np.nanmean(pred, axis=axis)\n    me = np.nanmean(res, axis=axis)\n    mse = np.nanmean(res ** 2, axis=axis)\n    mss = np.nanmean(sum ** 2, axis=axis)\n    mae = np.nanmean(np.abs(res), axis=axis)\n\n    # expand axes for broadcasting\n    for ax in sorted(axis):\n        mobs = np.expand_dims(mobs, ax)\n        mpred = np.expand_dims(mpred, ax)\n\n    # new cov matrix\n    cov = np.nanmean((obs - mobs) * (pred - mpred), axis=axis)\n    vobs = np.nanmean(np.abs(obs - mobs) ** 2, axis=axis)\n    vpred = np.nanmean(np.abs(pred - mpred) ** 2, axis=axis)\n\n    mobs = mobs.squeeze()\n    mpred = mpred.squeeze()\n\n    # update variances\n    _parallel_var(err[\"mobs\"], err[\"n\"], err[\"vobs\"], mobs, n, vobs)\n    _parallel_var(err[\"mpred\"], err[\"n\"], err[\"vpred\"], mpred, n, vpred)\n\n    # update covariance\n    _parallel_cov(\n        err[\"cov\"], err[\"mobs\"], err[\"mpred\"], err[\"n\"], cov, mobs, mpred, n\n    )\n\n    # update means\n    _parallel_mean(err[\"mobs\"], err[\"n\"], mobs, n)\n    _parallel_mean(err[\"mpred\"], err[\"n\"], mpred, n)\n    _parallel_mean(err[\"me\"], err[\"n\"], me, n)\n    _parallel_mean(err[\"mse\"], err[\"n\"], mse, n)\n    _parallel_mean(err[\"mss\"], err[\"n\"], mss, n)\n    _parallel_mean(err[\"mae\"], err[\"n\"], mae, n)\n\n    # update number of samples\n    err[\"n\"] += n",
  "def det_cont_fct_merge(err_1, err_2):\n    \"\"\"Merge two verification error objects.\n\n    Parameters\n    ----------\n\n    err_1 : dict\n      A verification error object initialized with\n      :py:func:`pysteps.verification.detcontscores.det_cont_fct_init`\n      and populated with\n      :py:func:`pysteps.verification.detcontscores.det_cont_fct_accum`.\n\n    err_2 : dict\n      Another verification error object initialized with\n      :py:func:`pysteps.verification.detcontscores.det_cont_fct_init`\n      and populated with\n      :py:func:`pysteps.verification.detcontscores.det_cont_fct_accum`.\n\n    Returns\n    -------\n\n    out : dict\n      The merged verification error object.\n    \"\"\"\n\n    # checks\n    if err_1[\"axis\"] != err_2[\"axis\"]:\n        raise ValueError(\n            \"cannot merge: the axis are not same %s!=%s\"\n            % (err_1[\"axis\"], err_2[\"axis\"])\n        )\n    if err_1[\"conditioning\"] != err_2[\"conditioning\"]:\n        raise ValueError(\n            \"cannot merge: the conditioning is not same %s!=%s\"\n            % (err_1[\"conditioning\"], err_2[\"conditioning\"])\n        )\n    if err_1[\"thr\"] != err_2[\"thr\"]:\n        raise ValueError(\n            \"cannot merge: the threshold is not same %s!=%s\"\n            % (err_1[\"thr\"], err_2[\"thr\"])\n        )\n    if err_1[\"cov\"] is None or err_2[\"cov\"] is None:\n        raise ValueError(\"cannot merge: no data found\")\n\n    # merge the two verification error objects\n    err = err_1.copy()\n\n    # update variances\n    _parallel_var(\n        err[\"mobs\"],\n        err[\"n\"],\n        err[\"vobs\"],\n        err_2[\"mobs\"],\n        err_2[\"n\"],\n        err_2[\"vobs\"],\n    )\n    _parallel_var(\n        err[\"mpred\"],\n        err[\"n\"],\n        err[\"vpred\"],\n        err_2[\"mpred\"],\n        err_2[\"n\"],\n        err_2[\"vpred\"],\n    )\n\n    # update covariance\n    _parallel_cov(\n        err[\"cov\"],\n        err[\"mobs\"],\n        err[\"mpred\"],\n        err[\"n\"],\n        err_2[\"cov\"],\n        err_2[\"mobs\"],\n        err_2[\"mpred\"],\n        err_2[\"n\"],\n    )\n\n    # update means\n    _parallel_mean(err[\"mobs\"], err[\"n\"], err_2[\"mobs\"], err_2[\"n\"])\n    _parallel_mean(err[\"mpred\"], err[\"n\"], err_2[\"mpred\"], err_2[\"n\"])\n    _parallel_mean(err[\"me\"], err[\"n\"], err_2[\"me\"], err_2[\"n\"])\n    _parallel_mean(err[\"mse\"], err[\"n\"], err_2[\"mse\"], err_2[\"n\"])\n    _parallel_mean(err[\"mss\"], err[\"n\"], err_2[\"mss\"], err_2[\"n\"])\n    _parallel_mean(err[\"mae\"], err[\"n\"], err_2[\"mae\"], err_2[\"n\"])\n\n    # update number of samples\n    err[\"n\"] += err_2[\"n\"]\n\n    return err",
  "def det_cont_fct_compute(err, scores=\"\"):\n    \"\"\"Compute simple and skill scores for deterministic continuous forecasts\n    from a verification error object.\n\n    Parameters\n    ----------\n\n    err : dict\n        A verification error object initialized with\n        :py:func:`pysteps.verification.detcontscores.det_cont_fct_init` and\n        populated with\n        :py:func:`pysteps.verification.detcontscores.det_cont_fct_accum`.\n\n    scores : {string, list of strings}, optional\n        The name(s) of the scores. The default, scores=\"\", will compute all\n        available scores.\n        The available score names are:\n\n        .. tabularcolumns:: |p{2cm}|L|\n\n        +------------+--------------------------------------------------------+\n        | Name       | Description                                            |\n        +============+========================================================+\n        |  beta1      | linear regression slope (type 1 conditional bias)     |\n        +------------+--------------------------------------------------------+\n        |  beta2      | linear regression slope (type 2 conditional bias)     |\n        +------------+--------------------------------------------------------+\n        |  corr_p    | pearson's correleation coefficien (linear correlation) |\n        +------------+--------------------------------------------------------+\n        |  DRMSE     | debiased root mean squared error, i.e.                 |\n        |            | :math:`DRMSE = \\\\sqrt{RMSE - ME^2}`                    |\n        +------------+--------------------------------------------------------+\n        |  MAE       | mean absolute error                                    |\n        +------------+--------------------------------------------------------+\n        |  ME        | mean error or bias                                     |\n        +------------+--------------------------------------------------------+\n        |  MSE       | mean squared error                                     |\n        +------------+--------------------------------------------------------+\n        |  NMSE      | normalized mean squared error                          |\n        +------------+--------------------------------------------------------+\n        |  RMSE      | root mean squared error                                |\n        +------------+--------------------------------------------------------+\n        |  RV        | reduction of variance                                  |\n        |            | (Brier Score, Nash-Sutcliffe Efficiency), i.e.         |\n        |            | :math:`RV = 1 - \\\\frac{MSE}{s^2_o}`                    |\n        +------------+--------------------------------------------------------+\n\n    Returns\n    -------\n\n    result : dict\n        Dictionary containing the verification results.\n    \"\"\"\n\n    # catch case of single score passed as string\n    def get_iterable(x):\n        if isinstance(x, collections.Iterable) and not isinstance(x, str):\n            return x\n        else:\n            return (x,)\n\n    scores = get_iterable(scores)\n\n    result = {}\n    for score in scores:\n        # catch None passed as score\n        if score is None:\n            continue\n\n        score_ = score.lower()\n\n        # bias (mean error, systematic error)\n        if score_ in [\"bias\", \"me\", \"\"]:\n            bias = err[\"me\"]\n            result[\"ME\"] = bias\n\n        # mean absolute error\n        if score_ in [\"mae\", \"\"]:\n            MAE = err[\"mae\"]\n            result[\"MAE\"] = MAE\n\n        # mean squared error\n        if score_ in [\"mse\", \"\"]:\n            MSE = err[\"mse\"]\n            result[\"MSE\"] = MSE\n\n        # normalized mean squared error\n        if score_ in [\"nmse\", \"\"]:\n            NMSE = err[\"mse\"] / err[\"mss\"]\n            result[\"NMSE\"] = NMSE\n\n        # root mean squared error\n        if score_ in [\"rmse\", \"\"]:\n            RMSE = np.sqrt(err[\"mse\"])\n            result[\"RMSE\"] = RMSE\n\n        # linear correlation coeff (pearson corr)\n        if score_ in [\"corr_p\", \"pearsonr\", \"\"]:\n            corr_p = err[\"cov\"] / np.sqrt(err[\"vobs\"]) / np.sqrt(err[\"vpred\"])\n            result[\"corr_p\"] = corr_p\n\n        # beta1 (linear regression slope)\n        if score_ in [\"beta\", \"beta1\", \"\"]:\n            beta1 = err[\"cov\"] / err[\"vpred\"]\n            result[\"beta1\"] = beta1\n\n        # beta2 (linear regression slope)\n        if score_ in [\"beta2\", \"\"]:\n            beta2 = err[\"cov\"] / err[\"vobs\"]\n            result[\"beta2\"] = beta2\n\n        # debiased RMSE\n        if score_ in [\"drmse\", \"\"]:\n            RMSE_d = np.sqrt(err[\"mse\"] - err[\"me\"] ** 2)\n            result[\"DRMSE\"] = RMSE_d\n\n        # reduction of variance\n        # (Brier Score, Nash-Sutcliffe efficiency coefficient,\n        # MSE skill score)\n        if score_ in [\"rv\", \"brier_score\", \"nse\", \"\"]:\n            RV = 1.0 - err[\"mse\"] / err[\"vobs\"]\n            result[\"RV\"] = RV\n\n    return result",
  "def _parallel_mean(avg_a, count_a, avg_b, count_b):\n    \"\"\"Update avg_a with avg_b.\n    \"\"\"\n    idx = count_b > 0\n    avg_a[idx] = (count_a[idx] * avg_a[idx] + count_b[idx] * avg_b[idx]) / (\n        count_a[idx] + count_b[idx]\n    )",
  "def _parallel_var(avg_a, count_a, var_a, avg_b, count_b, var_b):\n    \"\"\"Update var_a with var_b.\n    source: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n    \"\"\"\n    idx = count_b > 0\n    delta = avg_b - avg_a\n    m_a = var_a * count_a\n    m_b = var_b * count_b\n    var_a[idx] = (\n        m_a[idx]\n        + m_b[idx]\n        + delta[idx] ** 2\n        * count_a[idx]\n        * count_b[idx]\n        / (count_a[idx] + count_b[idx])\n    )\n    var_a[idx] = var_a[idx] / (count_a[idx] + count_b[idx])",
  "def _parallel_cov(\n    cov_a, avg_xa, avg_ya, count_a, cov_b, avg_xb, avg_yb, count_b\n):\n    \"\"\"Update cov_a with cov_b.\n    \"\"\"\n    idx = count_b > 0\n    deltax = avg_xb - avg_xa\n    deltay = avg_yb - avg_ya\n    c_a = cov_a * count_a\n    c_b = cov_b * count_b\n    cov_a[idx] = (\n        c_a[idx]\n        + c_b[idx]\n        + deltax[idx]\n        * deltay[idx]\n        * count_a[idx]\n        * count_b[idx]\n        / (count_a[idx] + count_b[idx])\n    )\n    cov_a[idx] = cov_a[idx] / (count_a[idx] + count_b[idx])",
  "def _uniquelist(mylist):\n    used = set()\n    return [x for x in mylist if x not in used and (used.add(x) or True)]",
  "def _scatter(pred, obs, axis=None):\n\n    pred = pred.copy()\n    obs = obs.copy()\n\n    # catch case of axis passed as integer\n    def get_iterable(x):\n        if x is None or (\n            isinstance(x, collections.Iterable) and not isinstance(x, int)\n        ):\n            return x\n        else:\n            return (x,)\n\n    axis = get_iterable(axis)\n\n    # reshape arrays as 2d matrices\n    # rows : samples; columns : variables\n    axis = tuple(range(pred.ndim)) if axis is None else axis\n    axis = tuple(np.sort(axis))\n    for ax in axis:\n        pred = np.rollaxis(pred, ax, 0)\n        obs = np.rollaxis(obs, ax, 0)\n    shp_rows = pred.shape[: len(axis)]\n    shp_cols = pred.shape[len(axis):]\n    pred = np.reshape(pred, (np.prod(shp_rows), -1))\n    obs = np.reshape(obs, (np.prod(shp_rows), -1))\n\n    # compute multiplicative erros in dB\n    q = 10 * np.log10(pred / obs)\n\n    # nans are given zero weight and are set equal to (min value - 1)\n    idkeep = np.isfinite(q)\n    q[~idkeep] = q[idkeep].min() - 1\n    obs[~idkeep] = 0\n\n    # compute scatter along rows\n    xs = np.sort(q, axis=0)\n    xs = np.vstack((xs[0, :], xs))\n    ixs = np.argsort(q, axis=0)\n    ws = np.take_along_axis(obs, ixs, axis=0)\n    ws = np.vstack((ws[0, :] * 0.0, ws))\n    wsc = np.cumsum(ws, axis=0) / np.sum(ws, axis=0)\n    xint = np.zeros((2, xs.shape[1]))\n    for i in range(xint.shape[1]):\n        xint[:, i] = np.interp([0.16, 0.84], wsc[:, i], xs[:, i])\n    scatter = (xint[1, :] - xint[0, :]) / 2.0\n\n    # reshape back\n    scatter = scatter.reshape(shp_cols)\n\n    return float(scatter) if scatter.size == 1 else scatter",
  "def _spearmanr(pred, obs, axis=None):\n\n    pred = pred.copy()\n    obs = obs.copy()\n\n    # catch case of axis passed as integer\n    def get_iterable(x):\n        if x is None or (\n            isinstance(x, collections.Iterable) and not isinstance(x, int)\n        ):\n            return x\n        else:\n            return (x,)\n\n    axis = get_iterable(axis)\n\n    # reshape arrays as 2d matrices\n    # rows : samples; columns : variables\n    axis = tuple(range(pred.ndim)) if axis is None else axis\n    axis = tuple(np.sort(axis))\n    for ax in axis:\n        pred = np.rollaxis(pred, ax, 0)\n        obs = np.rollaxis(obs, ax, 0)\n    shp_rows = pred.shape[: len(axis)]\n    shp_cols = pred.shape[len(axis):]\n    pred = np.reshape(pred, (np.prod(shp_rows), -1))\n    obs = np.reshape(obs, (np.prod(shp_rows), -1))\n\n    # apply only with more than 2 valid samples\n    # although this does not seem to solve the error\n    # \"ValueError: The input must have at least 3 entries!\" ...\n    corr_s = np.zeros(pred.shape[1]) * np.nan\n    nsamp = np.sum(np.logical_and(np.isfinite(pred), np.isfinite(obs)), axis=0)\n    idx = nsamp > 2\n    if np.any(idx):\n        corr_s_ = spearmanr(\n            pred[:, idx], obs[:, idx], axis=0, nan_policy=\"omit\"\n        )[0]\n\n        if corr_s_.size > 1:\n            corr_s[idx] = np.diag(corr_s_, idx.sum())\n        else:\n            corr_s = corr_s_\n\n    return float(corr_s) if corr_s.size == 1 else corr_s.reshape(shp_cols)",
  "def get_iterable(x):\n        if isinstance(x, collections.Iterable) and not isinstance(x, str):\n            return x\n        else:\n            return (x,)",
  "def get_iterable(x):\n        if x is None or (\n            isinstance(x, collections.Iterable) and not isinstance(x, int)\n        ):\n            return x\n        else:\n            return (x,)",
  "def get_iterable(x):\n        if isinstance(x, collections.Iterable) and not isinstance(x, str):\n            return x\n        else:\n            return (x,)",
  "def get_iterable(x):\n        if x is None or (\n            isinstance(x, collections.Iterable) and not isinstance(x, int)\n        ):\n            return x\n        else:\n            return (x,)",
  "def get_iterable(x):\n        if x is None or (\n            isinstance(x, collections.Iterable) and not isinstance(x, int)\n        ):\n            return x\n        else:\n            return (x,)",
  "def CRPS(X_f, X_o):\n    \"\"\"Compute the continuous ranked probability score (CRPS).\n\n    Parameters\n    ----------\n    X_f : array_like\n      Array of shape (k,m,n,...) containing the values from an ensemble\n      forecast of k members with shape (m,n,...).\n    X_o : array_like\n      Array of shape (m,n,...) containing the observed values corresponding\n      to the forecast.\n\n    Returns\n    -------\n    out : float\n      The computed CRPS.\n\n    References\n    ----------\n    :cite:`Her2000`\n\n    \"\"\"\n\n    X_f = X_f.copy()\n    X_o = X_o.copy()\n    crps = CRPS_init()\n    CRPS_accum(crps, X_f, X_o)\n    return CRPS_compute(crps)",
  "def CRPS_init():\n    \"\"\"Initialize a CRPS object.\n\n    Returns\n    -------\n    out : dict\n      The CRPS object.\n    \"\"\"\n    return {\"CRPS_sum\": 0.0, \"n\": 0.0}",
  "def CRPS_accum(CRPS, X_f, X_o):\n    \"\"\"Compute the average continuous ranked probability score (CRPS) for a set\n    of forecast ensembles and the corresponding observations and accumulate the\n    result to the given CRPS object.\n\n    Parameters\n    ----------\n    CRPS : dict\n      The CRPS object.\n    X_f : array_like\n      Array of shape (k,m,n,...) containing the values from an ensemble\n      forecast of k members with shape (m,n,...).\n    X_o : array_like\n      Array of shape (m,n,...) containing the observed values corresponding\n      to the forecast.\n\n    References\n    ----------\n    :cite:`Her2000`\n\n    \"\"\"\n    X_f = np.vstack([X_f[i, :].flatten() for i in range(X_f.shape[0])]).T\n    X_o = X_o.flatten()\n\n    mask = np.logical_and(np.all(np.isfinite(X_f), axis=1), np.isfinite(X_o))\n\n    X_f = X_f[mask, :].copy()\n    X_f.sort(axis=1)\n    X_o = X_o[mask]\n\n    n = X_f.shape[0]\n    m = X_f.shape[1]\n\n    alpha = np.zeros((n, m+1))\n    beta = np.zeros((n, m+1))\n\n    for i in range(1, m):\n        mask = X_o > X_f[:, i]\n        alpha[mask, i] = X_f[mask, i] - X_f[mask, i-1]\n        beta[mask, i] = 0.0\n\n        mask = np.logical_and(X_f[:, i] > X_o, X_o > X_f[:, i-1])\n        alpha[mask, i] = X_o[mask] - X_f[mask, i-1]\n        beta[mask, i] = X_f[mask, i] - X_o[mask]\n\n        mask = X_o < X_f[:, i-1]\n        alpha[mask, i] = 0.0\n        beta[mask, i] = X_f[mask, i] - X_f[mask, i-1]\n\n    mask = X_o < X_f[:, 0]\n    alpha[mask, 0] = 0.0\n    beta[mask, 0] = X_f[mask, 0] - X_o[mask]\n\n    mask = X_f[:, -1] < X_o\n    alpha[mask, -1] = X_o[mask] - X_f[mask, -1]\n    beta[mask, -1] = 0.0\n\n    p = 1.0*np.arange(m+1) / m\n    res = np.sum(alpha*p**2.0 + beta*(1.0-p)**2.0, axis=1)\n\n    CRPS[\"CRPS_sum\"] += np.sum(res)\n    CRPS[\"n\"] += len(res)",
  "def CRPS_compute(CRPS):\n    \"\"\"Compute the averaged values from the given CRPS object.\n\n    Parameters\n    ----------\n    CRPS : dict\n      A CRPS object created with CRPS_init.\n\n    Returns\n    -------\n    out : float\n      The computed CRPS.\n    \"\"\"\n    return 1.0*CRPS[\"CRPS_sum\"] / CRPS[\"n\"]",
  "def reldiag(P_f, X_o, X_min, n_bins=10, min_count=10):\n    \"\"\"Compute the x- and y- coordinates of the points in the reliability diagram.\n\n    Parameters\n    ----------\n    P_f : array-like\n      Forecast probabilities for exceeding the intensity threshold specified\n      in the reliability diagram object.\n    X_o : array-like\n      Observed values.\n    X_min : float\n      Precipitation intensity threshold for yes/no prediction.\n    n_bins : int\n        Number of bins to use in the reliability diagram.\n    min_count : int\n      Minimum number of samples required for each bin. A zero value is assigned\n      if the number of samples in a bin is smaller than bin_count.\n\n    Returns\n    -------\n    out : tuple\n      Two-element tuple containing the x- and y-coordinates of the points in\n      the reliability diagram.\n    \"\"\"\n\n    P_f = P_f.copy()\n    X_o = X_o.copy()\n    rdiag = reldiag_init(X_min, n_bins, min_count)\n    reldiag_accum(rdiag, P_f, X_o)\n    return reldiag_compute(rdiag)",
  "def reldiag_init(X_min, n_bins=10, min_count=10):\n    \"\"\"Initialize a reliability diagram object.\n\n    Parameters\n    ----------\n    X_min : float\n      Precipitation intensity threshold for yes/no prediction.\n    n_bins : int\n        Number of bins to use in the reliability diagram.\n    min_count : int\n      Minimum number of samples required for each bin. A zero value is assigned\n      if the number of samples in a bin is smaller than bin_count.\n\n    Returns\n    -------\n    out : dict\n      The reliability diagram object.\n\n    References\n    ----------\n    :cite:`BS2007`\n\n    \"\"\"\n    reldiag = {}\n\n    reldiag[\"X_min\"] = X_min\n    reldiag[\"bin_edges\"] = np.linspace(-1e-6, 1+1e-6, n_bins+1)\n    reldiag[\"n_bins\"] = n_bins\n    reldiag[\"X_sum\"] = np.zeros(n_bins)\n    reldiag[\"Y_sum\"] = np.zeros(n_bins, dtype=int)\n    reldiag[\"num_idx\"] = np.zeros(n_bins, dtype=int)\n    reldiag[\"sample_size\"] = np.zeros(n_bins, dtype=int)\n    reldiag[\"min_count\"] = min_count\n\n    return reldiag",
  "def reldiag_accum(reldiag, P_f, X_o):\n    \"\"\"Accumulate the given probability-observation pairs into the reliability\n    diagram.\n\n    Parameters\n    ----------\n    reldiag : dict\n      A reliability diagram object created with reldiag_init.\n    P_f : array-like\n      Forecast probabilities for exceeding the intensity threshold specified\n      in the reliability diagram object.\n    X_o : array-like\n      Observed values.\n\n    \"\"\"\n    mask = np.logical_and(np.isfinite(P_f), np.isfinite(X_o))\n\n    P_f = P_f[mask]\n    X_o = X_o[mask]\n\n    idx = np.digitize(P_f, reldiag[\"bin_edges\"], right=True)\n\n    x = []\n    y = []\n    num_idx = []\n    ss = []\n\n    for k in range(1, len(reldiag[\"bin_edges\"])):\n        I_k = np.where(idx == k)[0]\n        if len(I_k) >= reldiag[\"min_count\"]:\n            X_o_above_thr = (X_o[I_k] >= reldiag[\"X_min\"]).astype(int)\n            y.append(np.sum(X_o_above_thr))\n            x.append(np.sum(P_f[I_k]))\n            num_idx.append(len(I_k))\n            ss.append(len(I_k))\n        else:\n            y.append(0.0)\n            x.append(0.0)\n            num_idx.append(0.0)\n            ss.append(0)\n\n    reldiag[\"X_sum\"] += np.array(x)\n    reldiag[\"Y_sum\"] += np.array(y, dtype=int)\n    reldiag[\"num_idx\"] += np.array(num_idx, dtype=int)\n    reldiag[\"sample_size\"] += ss",
  "def reldiag_compute(reldiag):\n    \"\"\"Compute the x- and y- coordinates of the points in the reliability diagram.\n\n    Parameters\n    ----------\n    reldiag : dict\n      A reliability diagram object created with reldiag_init.\n\n    Returns\n    -------\n    out : tuple\n      Two-element tuple containing the x- and y-coordinates of the points in\n      the reliability diagram.\n    \"\"\"\n    f = 1.0 * reldiag[\"Y_sum\"] / reldiag[\"num_idx\"]\n    r = 1.0 * reldiag[\"X_sum\"] / reldiag[\"num_idx\"]\n\n    return r, f",
  "def ROC_curve(P_f, X_o, X_min, n_prob_thrs=10, compute_area=False):\n    \"\"\"Compute the ROC curve and its area from the given ROC object.\n\n    Parameters\n    ----------\n    P_f : array_like\n      Forecasted probabilities for exceeding the threshold specified in the ROC\n      object. Non-finite values are ignored.\n    X_o : array_like\n      Observed values. Non-finite values are ignored.\n    X_min : float\n      Precipitation intensity threshold for yes/no prediction.\n    n_prob_thrs : int\n      The number of probability thresholds to use.\n      The interval [0,1] is divided into n_prob_thrs evenly spaced values.\n    compute_area : bool\n      If True, compute the area under the ROC curve (between 0.5 and 1).\n\n    Returns\n    -------\n    out : tuple\n      A two-element tuple containing the probability of detection (POD) and\n      probability of false detection (POFD) for the probability thresholds\n      specified in the ROC curve object. If compute_area is True, return the\n      area under the ROC curve as the third element of the tuple.\n\n    \"\"\"\n\n    P_f = P_f.copy()\n    X_o = X_o.copy()\n    roc = ROC_curve_init(X_min, n_prob_thrs)\n    ROC_curve_accum(roc, P_f, X_o)\n    return ROC_curve_compute(roc, compute_area)",
  "def ROC_curve_init(X_min, n_prob_thrs=10):\n    \"\"\"Initialize a ROC curve object.\n\n    Parameters\n    ----------\n    X_min : float\n      Precipitation intensity threshold for yes/no prediction.\n    n_prob_thrs : int\n      The number of probability thresholds to use.\n      The interval [0,1] is divided into n_prob_thrs evenly spaced values.\n\n    Returns\n    -------\n    out : dict\n      The ROC curve object.\n\n    \"\"\"\n    ROC = {}\n\n    ROC[\"X_min\"] = X_min\n    ROC[\"hits\"] = np.zeros(n_prob_thrs, dtype=int)\n    ROC[\"misses\"] = np.zeros(n_prob_thrs, dtype=int)\n    ROC[\"false_alarms\"] = np.zeros(n_prob_thrs, dtype=int)\n    ROC[\"corr_neg\"] = np.zeros(n_prob_thrs, dtype=int)\n    ROC[\"prob_thrs\"] = np.linspace(0.0, 1.0, n_prob_thrs)\n\n    return ROC",
  "def ROC_curve_accum(ROC, P_f, X_o):\n    \"\"\"Accumulate the given probability-observation pairs into the given ROC\n    object.\n\n    Parameters\n    ----------\n    ROC : dict\n      A ROC curve object created with ROC_curve_init.\n    P_f : array_like\n      Forecasted probabilities for exceeding the threshold specified in the ROC\n      object. Non-finite values are ignored.\n    X_o : array_like\n      Observed values. Non-finite values are ignored.\n\n    \"\"\"\n    mask = np.logical_and(np.isfinite(P_f), np.isfinite(X_o))\n\n    P_f = P_f[mask]\n    X_o = X_o[mask]\n\n    for i, p in enumerate(ROC[\"prob_thrs\"]):\n        mask = np.logical_and(P_f >= p, X_o >= ROC[\"X_min\"])\n        ROC[\"hits\"][i] += np.sum(mask.astype(int))\n        mask = np.logical_and(P_f < p, X_o >= ROC[\"X_min\"])\n        ROC[\"misses\"][i] += np.sum(mask.astype(int))\n        mask = np.logical_and(P_f >= p, X_o < ROC[\"X_min\"])\n        ROC[\"false_alarms\"][i] += np.sum(mask.astype(int))\n        mask = np.logical_and(P_f < p, X_o < ROC[\"X_min\"])\n        ROC[\"corr_neg\"][i] += np.sum(mask.astype(int))",
  "def ROC_curve_compute(ROC, compute_area=False):\n    \"\"\"Compute the ROC curve and its area from the given ROC object.\n\n    Parameters\n    ----------\n    ROC : dict\n      A ROC curve object created with ROC_curve_init.\n    compute_area : bool\n      If True, compute the area under the ROC curve (between 0.5 and 1).\n\n    Returns\n    -------\n    out : tuple\n      A two-element tuple containing the probability of detection (POD) and\n      probability of false detection (POFD) for the probability thresholds\n      specified in the ROC curve object. If compute_area is True, return the\n      area under the ROC curve as the third element of the tuple.\n\n    \"\"\"\n    POD_vals = []\n    POFD_vals = []\n\n    for i in range(len(ROC[\"prob_thrs\"])):\n        POD_vals.append(1.0*ROC[\"hits\"][i] /\n                        (ROC[\"hits\"][i] + ROC[\"misses\"][i]))\n        POFD_vals.append(1.0*ROC[\"false_alarms\"][i] /\n                         (ROC[\"corr_neg\"][i] + ROC[\"false_alarms\"][i]))\n\n    if compute_area:\n        # Compute the total area of parallelepipeds under the ROC curve.\n        area = (1.0 - POFD_vals[0]) * (1.0 + POD_vals[0]) / 2.0\n        for i in range(len(ROC[\"prob_thrs\"])-1):\n            area += (POFD_vals[i] - POFD_vals[i+1]) * \\\n              (POD_vals[i+1] + POD_vals[i]) / 2.0\n        area += POFD_vals[-1] * POD_vals[-1] / 2.0\n\n        return POFD_vals, POD_vals, area\n    else:\n        return POFD_vals, POD_vals",
  "def ensemble_skill(X_f, X_o, metric, **kwargs):\n    \"\"\"Compute mean ensemble skill for a given skill metric.\n\n    Parameters\n    ----------\n    X_f : array-like\n        Array of shape (l,m,n) containing the forecast fields of shape (m,n)\n        from l ensemble members.\n    X_o : array_like\n        Array of shape (m,n) containing the observed field corresponding to\n        the forecast.\n    metric : str\n        The deterministic skill metric to be used (list available in\n        :func:`~pysteps.verification.interface.get_method`).\n\n    Returns\n    -------\n    out : float\n        The mean skill of all ensemble members that is used as defintion of\n        ensemble skill (as in Zacharov and Rezcova 2009 with the FSS).\n\n    References\n    ----------\n    :cite:`ZR2009`\n\n    \"\"\"\n\n    if len(X_f.shape) != 3:\n        raise ValueError(\"the number of dimensions of X_f must be equal to 3, \"\n                         + \"but %i dimensions were passed\"\n                         % len(X_f.shape))\n    if X_f.shape[1:] != X_o.shape:\n        raise ValueError(\"the shape of X_f does not match the shape of \"\n                         + \"X_o (%d,%d)!=(%d,%d)\"\n                         % (X_f.shape[1],\n                            X_f.shape[2],\n                            X_o.shape[0],\n                            X_o.shape[1]))\n\n    compute_skill = get_method(metric, type=\"deterministic\")\n\n    lolo = X_f.shape[0]\n    skill = []\n    for member in range(lolo):\n        skill_ = compute_skill(X_f[member, :, :], X_o, **kwargs)\n        if isinstance(skill_, dict):\n            skill_ = skill_[metric]\n        skill.append(skill_)\n\n    return np.mean(skill)",
  "def ensemble_spread(X_f, metric, **kwargs):\n    \"\"\"Compute mean ensemble spread for a given skill metric.\n\n    Parameters\n    ----------\n    X_f : array-like\n        Array of shape (l,m,n) containing the forecast fields of shape (m,n)\n        from l ensemble members.\n    metric : str\n        The deterministic skill metric to be used (list available in\n        :func:`~pysteps.verification.interface.get_method`).\n\n    Returns\n    -------\n    out : float\n        The mean skill compted between all possible pairs of\n        the ensemble members,\n        which can be used as definition of mean ensemble spread (as in Zacharov\n        and Rezcova 2009 with the FSS).\n\n    References\n    ----------\n    :cite:`ZR2009`\n\n    \"\"\"\n    if len(X_f.shape) != 3:\n        raise ValueError(\"the number of dimensions of X_f must be equal to 3, \"\n                         + \"but %i dimensions were passed\"\n                         % len(X_f.shape))\n    if X_f.shape[0] < 2:\n        raise ValueError(\"the number of members in X_f must be greater than 1,\"\n                         + \" but %i members were passed\"\n                         % X_f.shape[0])\n\n    compute_spread = get_method(metric, type=\"deterministic\")\n\n    lolo = X_f.shape[0]\n    spread = []\n    for member in range(lolo):\n        for othermember in range(member + 1, lolo):\n            spread_ = compute_spread(X_f[member, :, :],\n                                     X_f[othermember, :, :],\n                                     **kwargs)\n            if isinstance(spread_, dict):\n                spread_ = spread_[metric]\n            spread.append(spread_)\n\n    return np.mean(spread)",
  "def rankhist(X_f, X_o, X_min=None, normalize=True):\n    \"\"\"Compute a rank histogram counts and optionally normalize the histogram.\n\n    Parameters\n    ----------\n    X_f : array-like\n        Array of shape (k,m,n,...) containing the values from an ensemble\n        forecast of k members with shape (m,n,...).\n    X_o : array_like\n        Array of shape (m,n,...) containing the observed values corresponding\n        to the forecast.\n    X_min : {float,None}\n        Threshold for minimum intensity. Forecast-observation pairs, where all\n        ensemble members and verifying observations are below X_min, are not\n        counted in the rank histogram.\n        If set to None, thresholding is not used.\n    normalize : {bool, True}\n        If True, normalize the rank histogram so that\n        the bin counts sum to one.\n\n    \"\"\"\n\n    X_f = X_f.copy()\n    X_o = X_o.copy()\n    num_ens_members = X_f.shape[0]\n    rhist = rankhist_init(num_ens_members, X_min)\n    rankhist_accum(rhist, X_f, X_o)\n    return rankhist_compute(rhist, normalize)",
  "def rankhist_init(num_ens_members, X_min=None):\n    \"\"\"Initialize a rank histogram object.\n\n    Parameters\n    ----------\n    num_ens_members : int\n        Number ensemble members in the forecasts to accumulate into the rank\n        histogram.\n    X_min : {float,None}\n        Threshold for minimum intensity. Forecast-observation pairs, where all\n        ensemble members and verifying observations are below X_min, are not\n        counted in the rank histogram.\n        If set to None, thresholding is not used.\n\n    Returns\n    -------\n    out : dict\n        The rank histogram object.\n\n    \"\"\"\n    rankhist = {}\n\n    rankhist[\"num_ens_members\"] = num_ens_members\n    rankhist[\"n\"] = np.zeros(num_ens_members+1, dtype=int)\n    rankhist[\"X_min\"] = X_min\n\n    return rankhist",
  "def rankhist_accum(rankhist, X_f, X_o):\n    \"\"\"Accumulate forecast-observation pairs to the given rank histogram.\n\n    Parameters\n    ----------\n    rankhist : dict\n      The rank histogram object.\n    X_f : array-like\n        Array of shape (k,m,n,...) containing the values from an ensemble\n        forecast of k members with shape (m,n,...).\n    X_o : array_like\n        Array of shape (m,n,...) containing the observed values corresponding\n        to the forecast.\n\n    \"\"\"\n    if X_f.shape[0] != rankhist[\"num_ens_members\"]:\n        raise ValueError(\"the number of ensemble members in X_f does not \"\n                         + \"match the number of members in the rank \"\n                         + \"histogram (%d!=%d)\"\n                         % (X_f.shape[0], rankhist[\"num_ens_members\"]))\n\n    X_f = np.vstack([X_f[i, :].flatten() for i in range(X_f.shape[0])]).T\n    X_o = X_o.flatten()\n\n    X_min = rankhist[\"X_min\"]\n\n    mask = np.logical_and(np.isfinite(X_o), np.all(np.isfinite(X_f), axis=1))\n    # ignore pairs where the verifying observations and all ensemble members\n    # are below the threshold X_min\n    if X_min is not None:\n        mask_nz = np.logical_or(X_o >= X_min, np.any(X_f >= X_min, axis=1))\n        mask = np.logical_and(mask, mask_nz)\n\n    X_f = X_f[mask, :].copy()\n    X_o = X_o[mask].copy()\n    if X_min is not None:\n        X_f[X_f < X_min] = X_min - 1\n        X_o[X_o < X_min] = X_min - 1\n\n    X_o = np.reshape(X_o, (len(X_o), 1))\n\n    X_c = np.hstack([X_f, X_o])\n    X_c.sort(axis=1)\n\n    idx1 = np.where(X_c == X_o)\n    _, idx2, idx_counts = np.unique(idx1[0],\n                                    return_index=True,\n                                    return_counts=True)\n    bin_idx_1 = idx1[1][idx2]\n\n    bin_idx = list(bin_idx_1[np.where(idx_counts == 1)[0]])\n\n    # handle ties, where the verifying observation lies between ensemble\n    # members having the same value\n    idxdup = np.where(idx_counts > 1)[0]\n    if len(idxdup) > 0:\n        X_c_ = np.fliplr(X_c)\n        idx1 = np.where(X_c_ == X_o)\n        _, idx2 = np.unique(idx1[0], return_index=True)\n        bin_idx_2 = X_f.shape[1] - idx1[1][idx2]\n\n        idxr = np.random.uniform(low=0.0, high=1.0, size=len(idxdup))\n        idxr = bin_idx_1[idxdup] + idxr \\\n            * (bin_idx_2[idxdup] + 1 - bin_idx_1[idxdup])\n        bin_idx.extend(idxr.astype(int))\n\n    for bi in bin_idx:\n        rankhist[\"n\"][bi] += 1",
  "def rankhist_compute(rankhist, normalize=True):\n    \"\"\"Return the rank histogram counts and optionally normalize the histogram.\n\n    Parameters\n    ----------\n    rankhist : dict\n        A rank histogram object created with rankhist_init.\n    normalize : bool\n        If True, normalize the rank histogram so that\n        the bin counts sum to one.\n\n    Returns\n    -------\n    out : array_like\n        The counts for the n+1 bins in the rank histogram,\n        where n is the number of ensemble members.\n\n    \"\"\"\n    if normalize:\n        return 1.0*rankhist[\"n\"] / sum(rankhist[\"n\"])\n    else:\n        return rankhist[\"n\"]",
  "def lifetime(X_s, X_t, rule='1/e'):\n    \"\"\"\n    Compute the average lifetime by integrating the correlation function\n    as a function of lead time. When not using the 1/e rule, the correlation\n    function must be long enough to converge to 0, otherwise the lifetime is\n    underestimated. The correlation function can be either empirical or\n    theoretical, e.g. derived using the function 'ar_acf'\n    in timeseries/autoregression.py.\n\n    Parameters\n    ----------\n    X_s : array-like\n        Array with the correlation function.\n        Works also with other decaying scores that are defined\n        in the range [0,1]=[min_skill,max_skill].\n    X_t : array-like\n        Array with the forecast lead times in the desired unit,\n        e.g. [min, hour].\n    rule : str {'1/e', 'trapz', 'simpson'}, optional\n        Name of the method to integrate the correlation curve. \\n\n        '1/e' uses the 1/e rule and assumes an exponential decay. It linearly\n        interpolates the time when the correlation goes below the value 1/e.\n        When all values are > 1/e it returns the max lead time.\n        When all values are < 1/e it returns the min lead time. \\n\n        'trapz' uses the trapezoidal rule for integration.\\n\n        'simpson' uses the Simpson's rule for integration.\n\n    Returns\n    -------\n    lf : float\n        Estimated lifetime with same units of X_t.\n\n    \"\"\"\n    X_s = X_s.copy()\n    X_t = X_t.copy()\n    life = lifetime_init(rule)\n    lifetime_accum(life, X_s, X_t)\n    return lifetime_compute(life)",
  "def lifetime_init(rule='1/e'):\n    \"\"\"Initialize a lifetime object.\n\n    Parameters\n    ----------\n    rule : str {'1/e', 'trapz', 'simpson'}, optional\n        Name of the method to integrate the correlation curve. \\n\n        '1/e' uses the 1/e rule and assumes an exponential decay. It linearly\n        interpolates the time when the correlation goes below the value 1/e.\n        When all values are > 1/e it returns the max lead time.\n        When all values are < 1/e it returns the min lead time.\\n\n        'trapz' uses the trapezoidal rule for integration.\\n\n        'simpson' uses the Simpson's rule for integration.\n\n    Returns\n    -------\n    out : dict\n      The lifetime object.\n\n    \"\"\"\n    list_rules = ['trapz', 'simpson', '1/e']\n    if rule not in list_rules:\n        raise ValueError(\"Unknown rule %s for integration.\\n\" % rule\n                         + \"The available methods are: \"\n                         + str(list_rules))\n\n    lifetime = {}\n    lifetime[\"lifetime_sum\"] = 0.0\n    lifetime[\"n\"] = 0.0\n    lifetime[\"rule\"] = rule\n    return lifetime",
  "def lifetime_accum(lifetime, X_s, X_t):\n    \"\"\"\n    Compute the lifetime by integrating the correlation function\n    and accumulate the result into the given lifetime object.\n\n    Parameters\n    ----------\n    X_s : array-like\n        Array with the correlation function.\n        Works also with other decaying scores that are defined\n        in the range [0,1]=[min_skill,max_skill].\n    X_t : array-like\n        Array with the forecast lead times in the desired unit,\n        e.g. [min, hour].\n\n    \"\"\"\n    if lifetime[\"rule\"] == 'trapz':\n        lf = np.trapz(X_s, x=X_t)\n    elif lifetime[\"rule\"] == \"simpson\":\n        lf = simps(X_s, x=X_t)\n    elif lifetime[\"rule\"] == '1/e':\n        euler_number = 1.0/exp(1.0)\n        X_s_ = np.array(X_s)\n\n        is_euler_reached = np.sum(X_s_ <= euler_number) > 0\n        if is_euler_reached:\n            idx_b = np.argmax(X_s_ <= euler_number)\n            if idx_b > 0:\n                idx_a = idx_b - 1\n                fraction_score = (euler_number - X_s[idx_b])\\\n                    * (X_t[idx_a] - X_t[idx_b])\\\n                    / (X_s[idx_a] - X_s[idx_b])\n                lf = X_t[idx_b] + fraction_score\n            else:\n                # if all values are below the 1/e value, return min lead time\n                lf = np.min(X_t)\n        else:\n            # if all values are above the 1/e value, return max lead time\n            lf = np.max(X_t)\n\n    lifetime[\"lifetime_sum\"] += lf\n    lifetime[\"n\"] += 1",
  "def lifetime_compute(lifetime):\n    \"\"\"Compute the average value from the lifetime object.\n\n    Parameters\n    ----------\n    lifetime : dict\n      A lifetime object created with lifetime_init.\n\n    Returns\n    -------\n    out : float\n      The computed lifetime.\n\n    \"\"\"\n    return 1.0*lifetime[\"lifetime_sum\"]/lifetime[\"n\"]",
  "def intensity_scale(X_f, X_o, name, thrs, scales=None, wavelet=\"Haar\"):\n    \"\"\"Compute an intensity-scale verification score.\n\n    Parameters\n    ----------\n\n    X_f : array_like\n        Array of shape (m, n) containing the forecast field.\n\n    X_o : array_like\n        Array of shape (m, n) containing the verification observation field.\n\n    name : string\n        A string indicating the name of the spatial verification score\n        to be used:\n\n        +------------+--------------------------------------------------------+\n        | Name       | Description                                            |\n        +============+========================================================+\n        |  FSS       | Fractions skill score                                  |\n        +------------+--------------------------------------------------------+\n        |  BMSE      | Binary mean squared error                              |\n        +------------+--------------------------------------------------------+\n\n    thrs : float or array_like\n        Scalar or 1-D array of intensity thresholds for which to compute the\n        verification.\n\n    scales : float or array_like, optional\n        Scalar or 1-D array of spatial scales in pixels,\n        required if *name*=\"FSS\".\n\n    wavelet : str, optional\n        The name of the wavelet function to use in the BMSE.\n        Defaults to the Haar wavelet, as described in Casati et al. 2004.\n        See the documentation of PyWavelets for a list of available options.\n\n    Returns\n    -------\n\n    out : array_like\n        The two-dimensional array containing the intensity-scale skill scores\n        for each spatial scale and intensity threshold.\n\n    References\n    ----------\n\n    :cite:`CRS2004`, :cite:`RL2008`, :cite:`EWWM2013`\n\n    See also\n    --------\n\n    pysteps.verification.spatialscores.binary_mse,\n    pysteps.verification.spatialscores.fss\n\n    \"\"\"\n\n    intscale = intensity_scale_init(name, thrs, scales, wavelet)\n    intensity_scale_accum(intscale, X_f, X_o)\n    return intensity_scale_compute(intscale)",
  "def intensity_scale_init(name, thrs, scales=None, wavelet=\"Haar\"):\n    \"\"\"Initialize an intensity-scale verification object.\n\n    Parameters\n    ----------\n\n    name : string\n        A string indicating the name of the spatial verification score\n        to be used:\n\n        +------------+--------------------------------------------------------+\n        | Name       | Description                                            |\n        +============+========================================================+\n        |  FSS       | Fractions skill score                                  |\n        +------------+--------------------------------------------------------+\n        |  BMSE      | Binary mean squared error                              |\n        +------------+--------------------------------------------------------+\n\n    thrs : float or array_like\n        Scalar or 1-D array of intensity thresholds for which to compute the\n        verification.\n\n    scales : float or array_like, optional\n        Scalar or 1-D array of spatial scales in pixels,\n        required if *name*=\"FSS\".\n\n    wavelet : str, optional\n        The name of the wavelet function, required if *name*=\"BMSE\".\n        Defaults to the Haar wavelet, as described in Casati et al. 2004.\n        See the documentation of PyWavelets for a list of available options.\n\n    Returns\n    -------\n\n    out : dict\n        The intensity-scale object.\n\n    \"\"\"\n\n    if name.lower() == \"fss\" and scales is None:\n        message = \"an array of spatial scales must be provided for the FSS,\"\n        message += \" but %s was passed\" % scales\n        raise ValueError(message)\n\n    if name.lower() == \"bmse\" and wavelet is None:\n        message = \"the name of a wavelet must be provided for the BMSE,\"\n        message += \" but %s was passed\" % wavelet\n        raise ValueError(message)\n\n    # catch scalars when passed as arguments\n    def get_iterable(x):\n        if isinstance(x, collections.Iterable):\n            return np.copy(x)\n        else:\n            return np.copy((x,))\n\n    intscale = {}\n    intscale[\"name\"] = name\n    intscale[\"thrs\"] = np.sort(get_iterable(thrs))\n    if scales is not None:\n        intscale[\"scales\"] = np.sort(get_iterable(scales))[::-1]\n    else:\n        intscale[\"scales\"] = scales\n    intscale[\"wavelet\"] = wavelet\n\n    for i, thr in enumerate(intscale[\"thrs\"]):\n\n        if name.lower() == \"bmse\":\n            intscale[thr] = binary_mse_init(thr, intscale[\"wavelet\"])\n\n        elif name.lower() == \"fss\":\n            intscale[thr] = {}\n\n            for j, scale in enumerate(intscale[\"scales\"]):\n                intscale[thr][scale] = fss_init(thr, scale)\n\n    if name.lower() == \"fss\":\n        intscale[\"label\"] = \"Fractions skill score\"\n        del intscale[\"wavelet\"]\n\n    elif name.lower() == \"bmse\":\n        intscale[\"label\"] = \"Binary MSE skill score\"\n        intscale[\"scales\"] = None\n\n    else:\n        raise ValueError(\"unknown method %s\" % name)\n\n    return intscale",
  "def intensity_scale_accum(intscale, X_f, X_o):\n    \"\"\"Compute and update the intensity-scale verification scores.\n\n    Parameters\n    ----------\n\n    intscale : dict\n        The intensity-scale object initialized with\n        :py:func:`pysteps.verification.spatialscores.intensity_scale_init`.\n\n    X_f : array_like\n        Array of shape (m, n) containing the forecast field.\n\n    X_o : array_like\n        Array of shape (m, n) containing the verification observation field.\n    \"\"\"\n\n    name = intscale[\"name\"]\n    thrs = intscale[\"thrs\"]\n    scales = intscale[\"scales\"]\n\n    for i, thr in enumerate(thrs):\n\n        if name.lower() == \"bmse\":\n            binary_mse_accum(intscale[thr], X_f, X_o)\n\n        elif name.lower() == \"fss\":\n            for j, scale in enumerate(scales):\n                fss_accum(intscale[thr][scale], X_f, X_o)\n\n    if scales is None:\n        intscale[\"scales\"] = intscale[thrs[0]][\"scales\"]",
  "def intensity_scale_merge(intscale_1, intscale_2):\n    \"\"\"Merge two intensity-scale verification objects.\n\n    Parameters\n    ----------\n\n    intscale_1 : dict\n        Am intensity-scale object initialized with\n        :py:func:`pysteps.verification.spatialscores.intensity_scale_init`\n        and populated with\n        :py:func:`pysteps.verification.spatialscores.intensity_scale_accum`.\n\n    intscale_2 : dict\n        Another intensity-scale object initialized with\n        :py:func:`pysteps.verification.spatialscores.intensity_scale_init`\n        and populated with\n        :py:func:`pysteps.verification.spatialscores.intensity_scale_accum`.\n\n    Returns\n    -------\n\n    out : dict\n      The merged intensity-scale object.\n    \"\"\"\n\n    # checks\n    if intscale_1[\"name\"] != intscale_2[\"name\"]:\n        raise ValueError(\n            \"cannot merge: the intensity scale methods are not same %s!=%s\"\n            % (intscale_1[\"name\"], intscale_2[\"name\"])\n        )\n\n    intscale = intscale_1.copy()\n    name = intscale[\"name\"]\n    thrs = intscale[\"thrs\"]\n    scales = intscale[\"scales\"]\n\n    for i, thr in enumerate(thrs):\n\n        if name.lower() == \"bmse\":\n            intscale[thr] = binary_mse_merge(intscale[thr], intscale_2[thr])\n\n        elif name.lower() == \"fss\":\n            for j, scale in enumerate(scales):\n                intscale[thr][scale] = fss_merge(\n                    intscale[thr][scale], intscale_2[thr][scale]\n                )\n\n    return intscale",
  "def intensity_scale_compute(intscale):\n    \"\"\"Return the intensity scale matrix.\n\n    Parameters\n    ----------\n\n    intscale : dict\n        The intensity-scale object initialized with\n        :py:func:`pysteps.verification.spatialscores.intensity_scale_init`\n        and accumulated with\n        :py:func:`pysteps.verification.spatialscores.intensity_scale_accum`.\n\n    Returns\n    -------\n\n    out : array_like\n        The two-dimensional array of shape (j, k) containing\n        the intensity-scale skill scores for **j** spatial scales and\n        **k** intensity thresholds.\n\n    \"\"\"\n\n    name = intscale[\"name\"]\n    thrs = intscale[\"thrs\"]\n    scales = intscale[\"scales\"]\n\n    SS = np.zeros((scales.size, thrs.size))\n\n    for i, thr in enumerate(thrs):\n\n        if name.lower() == \"bmse\":\n            SS[:, i] = binary_mse_compute(intscale[thr], False)\n\n        elif name.lower() == \"fss\":\n            for j, scale in enumerate(scales):\n                SS[j, i] = fss_compute(intscale[thr][scale])\n\n    return SS",
  "def binary_mse(X_f, X_o, thr, wavelet=\"haar\", return_scales=True):\n    \"\"\"Compute the MSE of the binary error as a function of spatial scale.\n\n    This method uses PyWavelets for decomposing the error field between the\n    forecasts and observations into multiple spatial scales.\n\n    Parameters\n    ----------\n\n    X_f : array_like\n        Array of shape (m, n) containing the forecast field.\n\n    X_o : array_like\n        Array of shape (m, n) containing the verification observation field.\n\n    thr : sequence\n        The intensity threshold for which to compute the verification.\n\n    wavelet : str, optional\n        The name of the wavelet function to use. Defaults to the Haar wavelet,\n        as described in Casati et al. 2004. See the documentation of PyWavelets\n        for a list of available options.\n\n    return_scales : bool, optional\n        Whether to return the spatial scales resulting from the wavelet\n        decomposition.\n\n    Returns\n    -------\n\n    SS : array\n        One-dimensional array containing the binary MSE for each spatial scale.\n\n    scales : list, optional\n        If *return_scales*=True, return the spatial scales in pixels resulting\n        from the wavelet decomposition.\n\n    References\n    ----------\n\n    :cite:`CRS2004`\n    \"\"\"\n\n    bmse = binary_mse_init(thr, wavelet)\n    binary_mse_accum(bmse, X_f, X_o)\n    return binary_mse_compute(bmse, return_scales)",
  "def binary_mse_init(thr, wavelet=\"haar\"):\n    \"\"\"Initialize a binary MSE (BMSE) verification object.\n\n    Parameters\n    ----------\n\n    thr : float\n        The intensity threshold.\n\n    wavelet : str, optional\n        The name of the wavelet function to use. Defaults to the Haar wavelet,\n        as described in Casati et al. 2004. See the documentation of PyWavelets\n        for a list of available options.\n\n    Returns\n    -------\n    bmse : dict\n        The initialized BMSE verification object.\n    \"\"\"\n\n    bmse = {}\n\n    bmse[\"thr\"] = thr\n    bmse[\"wavelet\"] = wavelet\n    bmse[\"scales\"] = None\n\n    bmse[\"mse\"] = None\n    bmse[\"eps\"] = 0\n    bmse[\"n\"] = 0\n\n    return bmse",
  "def binary_mse_accum(bmse, X_f, X_o):\n    \"\"\"Accumulate forecast-observation pairs to an BMSE object.\n\n    Parameters\n    -----------\n\n    bmse : dict\n        The BMSE object initialized with\n        :py:func:`pysteps.verification.spatialscores.binary_mse_init`.\n\n    X_f : array_like\n        Array of shape (m, n) containing the forecast field.\n\n    X_o : array_like\n        Array of shape (m, n) containing the observation field.\n    \"\"\"\n    if not pywt_imported:\n        raise MissingOptionalDependency(\n            \"PyWavelets package is required for the binary MSE spatial \"\n            \"verification method but it is not installed\"\n        )\n\n    if len(X_f.shape) != 2 or len(X_o.shape) != 2 or X_f.shape != X_o.shape:\n        message = \"X_f and X_o must be two-dimensional arrays\"\n        message += \" having the same shape\"\n        raise ValueError(message)\n\n    thr = bmse[\"thr\"]\n    wavelet = bmse[\"wavelet\"]\n\n    X_f = X_f.copy()\n    X_f[~np.isfinite(X_f)] = thr - 1\n    X_o = X_o.copy()\n    X_o[~np.isfinite(X_o)] = thr - 1\n\n    w = pywt.Wavelet(wavelet)\n\n    I_f = (X_f >= thr).astype(float)\n    I_o = (X_o >= thr).astype(float)\n\n    E_decomp = _wavelet_decomp(I_f - I_o, w)\n\n    n_scales = len(E_decomp)\n    if bmse[\"scales\"] is None:\n        bmse[\"scales\"] = pow(2, np.arange(n_scales))[::-1]\n        bmse[\"mse\"] = np.zeros(n_scales)\n\n    # update eps\n    eps = 1.0 * np.sum((X_o >= thr).astype(int)) / X_o.size\n    if np.isfinite(eps):\n        bmse[\"eps\"] = (bmse[\"eps\"] * bmse[\"n\"] + eps) / (bmse[\"n\"] + 1)\n\n    # update mse\n    for j in range(n_scales):\n        mse = np.mean(E_decomp[j] ** 2)\n        if np.isfinite(mse):\n            bmse[\"mse\"][j] = (bmse[\"mse\"][j] * bmse[\"n\"] + mse) / (\n                bmse[\"n\"] + 1\n            )\n\n    bmse[\"n\"] += 1",
  "def binary_mse_merge(bmse_1, bmse_2):\n    \"\"\"Merge two BMSE objects.\n\n    Parameters\n    ----------\n\n    bmse_1 : dict\n      A BMSE object initialized with\n      :py:func:`pysteps.verification.spatialscores.binary_mse_init`.\n      and populated with\n      :py:func:`pysteps.verification.spatialscores.binary_mse_accum`.\n\n    bmse_2 : dict\n      Another BMSE object initialized with\n      :py:func:`pysteps.verification.spatialscores.binary_mse_init`.\n      and populated with\n      :py:func:`pysteps.verification.spatialscores.binary_mse_accum`.\n\n    Returns\n    -------\n\n    out : dict\n      The merged BMSE object.\n    \"\"\"\n\n    # checks\n    if bmse_1[\"thr\"] != bmse_2[\"thr\"]:\n        raise ValueError(\n            \"cannot merge: the thresholds are not same %s!=%s\"\n            % (bmse_1[\"thr\"], bmse_2[\"thr\"])\n        )\n    if bmse_1[\"wavelet\"] != bmse_2[\"wavelet\"]:\n        raise ValueError(\n            \"cannot merge: the wavelets are not same %s!=%s\"\n            % (bmse_1[\"wavelet\"], bmse_2[\"wavelet\"])\n        )\n    if list(bmse_1[\"scales\"]) != list(bmse_2[\"scales\"]):\n        raise ValueError(\n            \"cannot merge: the scales are not same %s!=%s\"\n            % (bmse_1[\"scales\"], bmse_2[\"scales\"])\n        )\n\n    # merge the BMSE objects\n    bmse = bmse_1.copy()\n    bmse[\"eps\"] = (bmse[\"eps\"] * bmse[\"n\"] + bmse_2[\"eps\"] * bmse_2[\"n\"]) / (\n        bmse[\"n\"] + bmse_2[\"n\"]\n    )\n    for j, scale in enumerate(bmse[\"scales\"]):\n        bmse[\"mse\"][j] = (\n            bmse[\"mse\"][j] * bmse[\"n\"] + bmse_2[\"mse\"][j] * bmse_2[\"n\"]\n        ) / (bmse[\"n\"] + bmse_2[\"n\"])\n    bmse[\"n\"] += bmse_2[\"n\"]\n\n    return bmse",
  "def binary_mse_compute(bmse, return_scales=True):\n    \"\"\"Compute the BMSE.\n\n    Parameters\n    ----------\n\n    bmse : dict\n        The BMSE object initialized with\n        :py:func:`pysteps.verification.spatialscores.binary_mse_init`\n        and accumulated with\n        :py:func:`pysteps.verification.spatialscores.binary_mse_accum`.\n\n    return_scales : bool, optional\n        Whether to return the spatial scales resulting from the wavelet\n        decomposition.\n\n    Returns\n    -------\n\n    BMSE : array_like\n        One-dimensional array containing the binary MSE for each spatial scale.\n\n    scales : list, optional\n        If *return_scales*=True, return the spatial scales in pixels resulting\n        from the wavelet decomposition.\n    \"\"\"\n\n    scales = bmse[\"scales\"]\n    n_scales = len(scales)\n    eps = bmse[\"eps\"]\n\n    BMSE = np.zeros(n_scales)\n    for j in range(n_scales):\n        mse = bmse[\"mse\"][j]\n        BMSE[j] = 1 - mse / (2 * eps * (1 - eps) / n_scales)\n\n    BMSE[~np.isfinite(BMSE)] = np.nan\n\n    if return_scales:\n        return BMSE, scales\n    else:\n        return BMSE",
  "def fss(X_f, X_o, thr, scale):\n    \"\"\"\n    Compute the fractions skill score (FSS) for a deterministic forecast field\n    and the corresponding observation field.\n\n    Parameters\n    ----------\n\n    X_f : array_like\n        Array of shape (m, n) containing the forecast field.\n\n    X_o : array_like\n        Array of shape (m, n) containing the observation field.\n\n    thr : float\n        The intensity threshold.\n\n    scale : int\n        The spatial scale in pixels. In practice, the scale represents the size\n        of the moving window that it is used to compute the fraction of pixels\n        above the threshold.\n\n    Returns\n    -------\n\n    out : float\n        The fractions skill score between 0 and 1.\n\n    References\n    ----------\n\n    :cite:`RL2008`, :cite:`EWWM2013`\n\n    \"\"\"\n\n    fss = fss_init(thr, scale)\n    fss_accum(fss, X_f, X_o)\n    return fss_compute(fss)",
  "def fss_init(thr, scale):\n    \"\"\"Initialize a fractions skill score (FSS) verification object.\n\n    Parameters\n    ----------\n\n    thr : float\n        The intensity threshold.\n\n    scale : float\n        The spatial scale in pixels. In practice, the scale represents the size\n        of the moving window that it is used to compute the fraction of pixels\n        above the threshold.\n\n    Returns\n    -------\n\n    fss : dict\n        The initialized FSS verification object.\n    \"\"\"\n    fss = {}\n\n    fss[\"thr\"] = thr\n    fss[\"scale\"] = scale\n    fss[\"sum_fct_sq\"] = 0.0\n    fss[\"sum_fct_obs\"] = 0.0\n    fss[\"sum_obs_sq\"] = 0.0\n\n    return fss",
  "def fss_accum(fss, X_f, X_o):\n    \"\"\"Accumulate forecast-observation pairs to an FSS object.\n\n    Parameters\n    -----------\n\n    fss : dict\n        The FSS object initialized with\n        :py:func:`pysteps.verification.spatialscores.fss_init`.\n\n    X_f : array_like\n        Array of shape (m, n) containing the forecast field.\n\n    X_o : array_like\n        Array of shape (m, n) containing the observation field.\n    \"\"\"\n    if len(X_f.shape) != 2 or len(X_o.shape) != 2 or X_f.shape != X_o.shape:\n        message = \"X_f and X_o must be two-dimensional arrays\"\n        message += \" having the same shape\"\n        raise ValueError(message)\n\n    X_f = X_f.copy()\n    X_f[~np.isfinite(X_f)] = fss[\"thr\"] - 1\n    X_o = X_o.copy()\n    X_o[~np.isfinite(X_o)] = fss[\"thr\"] - 1\n\n    # Convert to binary fields with the given intensity threshold\n    I_f = (X_f >= fss[\"thr\"]).astype(float)\n    I_o = (X_o >= fss[\"thr\"]).astype(float)\n\n    # Compute fractions of pixels above the threshold within a square\n    # neighboring area by applying a 2D moving average to the binary fields\n    if fss[\"scale\"] > 1:\n        S_f = uniform_filter(I_f, size=fss[\"scale\"], mode=\"constant\", cval=0.0)\n        S_o = uniform_filter(I_o, size=fss[\"scale\"], mode=\"constant\", cval=0.0)\n    else:\n        S_f = I_f\n        S_o = I_o\n\n    fss[\"sum_obs_sq\"] += np.nansum(S_o ** 2)\n    fss[\"sum_fct_obs\"] += np.nansum(S_f * S_o)\n    fss[\"sum_fct_sq\"] += np.nansum(S_f ** 2)",
  "def fss_merge(fss_1, fss_2):\n    \"\"\"Merge two FSS objects.\n\n    Parameters\n    ----------\n\n    fss_1 : dict\n      A FSS object initialized with\n      :py:func:`pysteps.verification.spatialscores.fss_init`.\n      and populated with\n      :py:func:`pysteps.verification.spatialscores.fss_accum`.\n\n    fss_2 : dict\n      Another FSS object initialized with\n      :py:func:`pysteps.verification.spatialscores.fss_init`.\n      and populated with\n      :py:func:`pysteps.verification.spatialscores.fss_accum`.\n\n    Returns\n    -------\n\n    out : dict\n      The merged FSS object.\n    \"\"\"\n\n    # checks\n    if fss_1[\"thr\"] != fss_2[\"thr\"]:\n        raise ValueError(\n            \"cannot merge: the thresholds are not same %s!=%s\"\n            % (fss_1[\"thr\"], fss_2[\"thr\"])\n        )\n    if fss_1[\"scale\"] != fss_2[\"scale\"]:\n        raise ValueError(\n            \"cannot merge: the scales are not same %s!=%s\"\n            % (fss_1[\"scale\"], fss_2[\"scale\"])\n        )\n\n    # merge the FSS objects\n    fss = fss_1.copy()\n    fss[\"sum_obs_sq\"] += fss_2[\"sum_obs_sq\"]\n    fss[\"sum_fct_obs\"] += fss_2[\"sum_fct_obs\"]\n    fss[\"sum_fct_sq\"] += fss_2[\"sum_fct_sq\"]\n\n    return fss",
  "def fss_compute(fss):\n    \"\"\"Compute the FSS.\n\n    Parameters\n    ----------\n\n    fss : dict\n       An FSS object initialized with\n       :py:func:`pysteps.verification.spatialscores.fss_init`\n       and accumulated with\n       :py:func:`pysteps.verification.spatialscores.fss_accum`.\n\n    Returns\n    -------\n\n    out : float\n        The computed FSS value.\n    \"\"\"\n    numer = fss[\"sum_fct_sq\"] - 2.0 * fss[\"sum_fct_obs\"] + fss[\"sum_obs_sq\"]\n    denom = fss[\"sum_fct_sq\"] + fss[\"sum_obs_sq\"]\n\n    return 1.0 - numer / denom",
  "def _wavelet_decomp(X, w):\n    c = pywt.wavedec2(X, w)\n\n    X_out = []\n    for k in range(len(c)):\n        c_ = c[:]\n        for k_ in set(range(len(c))).difference([k]):\n            c_[k_] = tuple([np.zeros_like(v) for v in c[k_]])\n        X_k = pywt.waverec2(c_, w)\n        X_out.append(X_k)\n\n    return X_out",
  "def get_iterable(x):\n        if isinstance(x, collections.Iterable):\n            return np.copy(x)\n        else:\n            return np.copy((x,))",
  "def plot_intensityscale(\n    intscale, fig=None, vminmax=None, kmperpixel=None, unit=None\n):\n    \"\"\"Plot a intensity-scale verification table with a color bar and axis\n    labels.\n\n    Parameters\n    ----------\n\n    intscale : dict\n        The intensity-scale object initialized with\n        :py:func:`pysteps.verification.spatialscores.intensity_scale_init`\n        and accumulated with\n        :py:func:`pysteps.verification.spatialscores.intensity_scale_accum`.\n\n    fig : matplotlib.figure.Figure, optional\n        The figure object to use for plotting. If not supplied, a new\n        figure is created.\n\n    vminmax : tuple of floats, optional\n       The minimum and maximum values for the intensity-scale skill score\n       in the plot.\n       Defaults to the data extent.\n\n    kmperpixel : float, optional\n       The conversion factor from pixels to kilometers. If supplied,\n       the unit of the shown spatial scales is km instead of pixels.\n\n    unit : string, optional\n       The unit of the intensity thresholds.\n    \"\"\"\n    if fig is None:\n        fig = plt.figure()\n\n    ax = fig.gca()\n\n    SS = spatialscores.intensity_scale_compute(intscale)\n\n    vmin = vmax = None\n    if vminmax is not None:\n        vmin = np.min(vminmax)\n        vmax = np.max(vminmax)\n    im = ax.imshow(\n        SS, vmin=vmin, vmax=vmax, interpolation=\"nearest\", cmap=cm.jet\n    )\n    cb = fig.colorbar(im)\n    cb.set_label(intscale[\"label\"])\n\n    if unit is None:\n        ax.set_xlabel(\"Intensity threshold\")\n    else:\n        ax.set_xlabel(\"Intensity threshold [%s]\" % unit)\n    if kmperpixel is None:\n        ax.set_ylabel(\"Spatial scale [pixels]\")\n    else:\n        ax.set_ylabel(\"Spatial scale [km]\")\n\n    ax.set_xticks(np.arange(SS.shape[1]))\n    ax.set_xticklabels(intscale[\"thrs\"])\n    ax.set_yticks(np.arange(SS.shape[0]))\n    if kmperpixel is None:\n        scales = intscale[\"scales\"]\n    else:\n        scales = np.array(intscale[\"scales\"]) * kmperpixel\n    ax.set_yticklabels(scales)",
  "def plot_rankhist(rankhist, ax=None):\n    \"\"\"Plot a rank histogram.\n\n    Parameters\n    ----------\n    rankhist : dict\n        A rank histogram object created by ensscores.rankhist_init.\n    ax : axis handle, optional\n        Axis handle for the figure. If set to None, the handle is taken from\n        the current figure (matplotlib.pylab.gca()).\n\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    r = ensscores.rankhist_compute(rankhist)\n    x = np.linspace(0, 1, rankhist[\"num_ens_members\"] + 1)\n    ax.bar(\n        x, r, width=1.0 / len(x), align=\"edge\", color=\"gray\", edgecolor=\"black\"\n    )\n\n    ax.set_xticks(x[::3] + (x[1] - x[0]))\n    ax.set_xticklabels(np.arange(1, len(x))[::3])\n    ax.set_xlim(0, 1 + 1.0 / len(x))\n    ax.set_ylim(0, np.max(r) * 1.25)\n\n    ax.set_xlabel(\"Rank of observation (among ensemble members)\")\n    ax.set_ylabel(\"Relative frequency\")\n\n    ax.grid(True, axis=\"y\", ls=\":\")",
  "def plot_reldiag(reldiag, ax=None):\n    \"\"\"Plot a reliability diagram.\n\n    Parameters\n    ----------\n    reldiag : dict\n        A reldiag object created by probscores.reldiag_init.\n    ax : axis handle, optional\n        Axis handle for the figure. If set to None, the handle is taken from\n        the current figure (matplotlib.pylab.gca()).\n\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    # Plot the reliability diagram.\n    f = 1.0 * reldiag[\"Y_sum\"] / reldiag[\"num_idx\"]\n    r = 1.0 * reldiag[\"X_sum\"] / reldiag[\"num_idx\"]\n\n    mask = np.logical_and(np.isfinite(r), np.isfinite(f))\n\n    ax.plot(r[mask], f[mask], \"kD-\")\n    ax.plot([0, 1], [0, 1], \"k--\")\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    ax.grid(True, ls=\":\")\n\n    ax.set_xlabel(\"Forecast probability\")\n    ax.set_ylabel(\"Observed relative frequency\")\n\n    # Plot sharpness diagram into an inset figure.\n    iax = inset_axes(ax, width=\"35%\", height=\"20%\", loc=4, borderpad=3.5)\n    bw = reldiag[\"bin_edges\"][2] - reldiag[\"bin_edges\"][1]\n    iax.bar(\n        reldiag[\"bin_edges\"][:-1],\n        reldiag[\"sample_size\"],\n        width=bw,\n        align=\"edge\",\n        color=\"gray\",\n        edgecolor=\"black\",\n    )\n    iax.set_yscale(\"log\", basey=10)\n    iax.set_xticks(reldiag[\"bin_edges\"])\n    iax.set_xticklabels([\"%.1f\" % max(v, 1e-6) for v in reldiag[\"bin_edges\"]])\n    yt_min = int(max(np.floor(np.log10(min(reldiag[\"sample_size\"][:-1]))), 1))\n    yt_max = int(np.ceil(np.log10(max(reldiag[\"sample_size\"][:-1]))))\n    t = [pow(10.0, k) for k in range(yt_min, yt_max)]\n\n    iax.set_yticks([int(t_) for t_ in t])\n    iax.set_xlim(0.0, 1.0)\n    iax.set_ylim(t[0], 5 * t[-1])\n    iax.set_ylabel(\"log10(samples)\")\n    iax.yaxis.tick_right()\n    iax.yaxis.set_label_position(\"right\")\n    iax.tick_params(axis=\"both\", which=\"major\", labelsize=6)",
  "def plot_ROC(ROC, ax=None, opt_prob_thr=False):\n    \"\"\"Plot a ROC curve.\n\n    Parameters\n    ----------\n    ROC : dict\n        A ROC curve object created by probscores.ROC_curve_init.\n    ax : axis handle, optional\n        Axis handle for the figure. If set to None, the handle is taken from\n        the current figure (matplotlib.pylab.gca()).\n    opt_prob_thr : bool, optional\n        If set to True, plot the optimal probability threshold that maximizes\n        the difference between the hit rate (POD) and false alarm rate (POFD).\n\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    POFD, POD, area = probscores.ROC_curve_compute(ROC, compute_area=True)\n    p_thr = ROC[\"prob_thrs\"]\n\n    ax.plot([0, 1], [0, 1], \"k--\")\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xlabel(\"False alarm rate (POFD)\")\n    ax.set_ylabel(\"Probability of detection (POD)\")\n    ax.grid(True, ls=\":\")\n\n    ax.plot(POFD, POD, \"kD-\")\n\n    if opt_prob_thr:\n        opt_prob_thr_idx = np.argmax(np.array(POD) - np.array(POFD))\n        ax.scatter(\n            [POFD[opt_prob_thr_idx]],\n            [POD[opt_prob_thr_idx]],\n            c=\"r\",\n            s=150,\n            facecolors=None,\n            edgecolors=\"r\",\n        )\n\n    for p_thr_, x, y in zip(p_thr, POFD, POD):\n        if p_thr_ > 0.05 and p_thr_ < 0.95:\n            ax.text(x + 0.02, y - 0.02, \"%.2f\" % p_thr_, fontsize=7)",
  "def det_cat_fct(pred, obs, thr, scores=\"\", axis=None):\n\n    \"\"\"Calculate simple and skill scores for deterministic categorical\n    (dichotomous) forecasts.\n\n    Parameters\n    ----------\n    pred : array_like\n        Array of predictions. NaNs are ignored.\n\n    obs : array_like\n        Array of verifying observations. NaNs are ignored.\n\n    thr : float\n        The threshold that is applied to predictions and observations in order\n        to define events vs no events (yes/no).\n\n    scores : {string, list of strings}, optional\n        The name(s) of the scores. The default, scores=\"\", will compute all\n        available scores.\n        The available score names are:\n\n        .. tabularcolumns:: |p{2cm}|L|\n\n        +------------+--------------------------------------------------------+\n        | Name       | Description                                            |\n        +============+========================================================+\n        |  ACC       | accuracy (proportion correct)                          |\n        +------------+--------------------------------------------------------+\n        |  BIAS      | frequency bias                                         |\n        +------------+--------------------------------------------------------+\n        |  CSI       | critical success index (threat score)                  |\n        +------------+--------------------------------------------------------+\n        |  F1        | the harmonic mean of precision and sensitivity         |\n        +------------+--------------------------------------------------------+\n        |  FA        | false alarm rate (prob. of false detection, fall-out,  |\n        |            | false positive rate)                                   |\n        +------------+--------------------------------------------------------+\n        |  FAR       | false alarm ratio (false discovery rate)               |\n        +------------+--------------------------------------------------------+\n        |  GSS       | Gilbert skill score (equitable threat score)           |\n        +------------+--------------------------------------------------------+\n        |  HK        | Hanssen-Kuipers discriminant (Pierce skill score)      |\n        +------------+--------------------------------------------------------+\n        |  HSS       | Heidke skill score                                     |\n        +------------+--------------------------------------------------------+\n        |  MCC       | Matthews correlation coefficient                       |\n        +------------+--------------------------------------------------------+\n        |  POD       | probability of detection (hit rate, sensitivity,       |\n        |            | recall, true positive rate)                            |\n        +------------+--------------------------------------------------------+\n        |  SEDI      | symmetric extremal dependency index                    |\n        +------------+--------------------------------------------------------+\n\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which a score is integrated. The default, axis=None,\n        will integrate all of the elements of the input arrays.\\n\n        If axis is -1 (or any negative integer),\n        the integration is not performed\n        and scores are computed on all of the elements in the input arrays.\\n\n        If axis is a tuple of ints, the integration is performed on all of the\n        axes specified in the tuple.\n\n    Returns\n    -------\n    result : dict\n        Dictionary containing the verification results.\n\n    See also\n    --------\n    pysteps.verification.detcontscores.det_cont_fct\n\n\n    \"\"\"\n\n    contab = det_cat_fct_init(thr, axis)\n    det_cat_fct_accum(contab, pred, obs)\n    return det_cat_fct_compute(contab, scores)",
  "def det_cat_fct_init(thr, axis=None):\n    \"\"\"Initialize a contingency table object.\n\n    Parameters\n    ----------\n\n    thr : float\n        threshold that is applied to predictions and observations in order\n        to define events vs no events (yes/no).\n\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which a score is integrated. The default, axis=None,\n        will integrate all of the elements of the input arrays.\\n\n        If axis is -1 (or any negative integer),\n        the integration is not performed\n        and scores are computed on all of the elements in the input arrays.\\n\n        If axis is a tuple of ints, the integration is performed on all of the\n        axes specified in the tuple.\n\n    Returns\n    -------\n\n    out : dict\n      The contingency table object.\n    \"\"\"\n\n    contab = {}\n\n    # catch case of axis passed as integer\n    def get_iterable(x):\n        if x is None or (\n            isinstance(x, collections.Iterable) and not isinstance(x, int)\n        ):\n            return x\n        else:\n            return (x,)\n\n    contab[\"thr\"] = thr\n    contab[\"axis\"] = get_iterable(axis)\n    contab[\"hits\"] = None\n    contab[\"false_alarms\"] = None\n    contab[\"misses\"] = None\n    contab[\"correct_negatives\"] = None\n\n    return contab",
  "def det_cat_fct_accum(contab, pred, obs):\n    \"\"\"Accumulate the frequency of \"yes\" and \"no\" forecasts and observations\n    in the contingency table.\n\n    Parameters\n    ----------\n\n    contab : dict\n      A contingency table object initialized with\n      pysteps.verification.detcatscores.det_cat_fct_init.\n\n    pred : array_like\n        Array of predictions. NaNs are ignored.\n\n    obs : array_like\n        Array of verifying observations. NaNs are ignored.\n\n    \"\"\"\n\n    pred = np.asarray(pred.copy())\n    obs = np.asarray(obs.copy())\n    axis = (\n        tuple(range(pred.ndim)) if contab[\"axis\"] is None else contab[\"axis\"]\n    )\n\n    # checks\n    if pred.shape != obs.shape:\n        raise ValueError(\n            \"the shape of pred does not match the shape of obs %s!=%s\"\n            % (pred.shape, obs.shape)\n        )\n\n    if pred.ndim <= np.max(axis):\n        raise ValueError(\n            \"axis %d is out of bounds for array of dimension %d\"\n            % (np.max(axis), len(pred.shape))\n        )\n\n    idims = [dim not in axis for dim in range(pred.ndim)]\n    nshape = tuple(np.array(pred.shape)[np.array(idims)])\n    if contab[\"hits\"] is None:\n        # initialize the count arrays in the contingency table\n        contab[\"hits\"] = np.zeros(nshape)\n        contab[\"false_alarms\"] = np.zeros(nshape)\n        contab[\"misses\"] = np.zeros(nshape)\n        contab[\"correct_negatives\"] = np.zeros(nshape)\n\n    else:\n        # check dimensions\n        if contab[\"hits\"].shape != nshape:\n            raise ValueError(\n                \"the shape of the input arrays does not match \"\n                + \"the shape of the \"\n                + \"contingency table %s!=%s\" % (nshape, contab[\"hits\"].shape)\n            )\n\n    # add dummy axis in case integration is not required\n    if np.max(axis) < 0:\n        pred = pred[None, :]\n        obs = obs[None, :]\n        axis = (0,)\n    axis = tuple([a for a in axis if a >= 0])\n\n    # apply threshold\n    predb = pred > contab[\"thr\"]\n    obsb = obs > contab[\"thr\"]\n\n    # calculate hits, misses, false positives, correct rejects\n    H_idx = np.logical_and(predb == 1, obsb == 1)\n    F_idx = np.logical_and(predb == 1, obsb == 0)\n    M_idx = np.logical_and(predb == 0, obsb == 1)\n    R_idx = np.logical_and(predb == 0, obsb == 0)\n\n    # accumulate in the contingency table\n    contab[\"hits\"] += np.nansum(H_idx.astype(int), axis=axis)\n    contab[\"misses\"] += np.nansum(M_idx.astype(int), axis=axis)\n    contab[\"false_alarms\"] += np.nansum(F_idx.astype(int), axis=axis)\n    contab[\"correct_negatives\"] += np.nansum(R_idx.astype(int), axis=axis)",
  "def det_cat_fct_merge(contab_1, contab_2):\n    \"\"\"Merge two contingency table objects.\n\n    Parameters\n    ----------\n\n    contab_1 : dict\n      A contingency table object initialized with\n      :py:func:`pysteps.verification.detcatscores.det_cat_fct_init`\n      and populated with\n      :py:func:`pysteps.verification.detcatscores.det_cat_fct_accum`.\n\n    contab_2 : dict\n      Another contingency table object initialized with\n      :py:func:`pysteps.verification.detcatscores.det_cat_fct_init`\n      and populated with\n      :py:func:`pysteps.verification.detcatscores.det_cat_fct_accum`.\n\n    Returns\n    -------\n\n    out : dict\n      The merged contingency table object.\n    \"\"\"\n\n    # checks\n    if contab_1[\"thr\"] != contab_2[\"thr\"]:\n        raise ValueError(\n            \"cannot merge: the thresholds are not same %s!=%s\"\n            % (contab_1[\"thr\"], contab_2[\"thr\"])\n        )\n    if contab_1[\"axis\"] != contab_2[\"axis\"]:\n        raise ValueError(\n            \"cannot merge: the axis are not same %s!=%s\"\n            % (contab_1[\"axis\"], contab_2[\"axis\"])\n        )\n    if contab_1[\"hits\"] is None or contab_2[\"hits\"] is None:\n        raise ValueError(\"cannot merge: no data found\")\n\n    # merge the contingency tables\n    contab = contab_1.copy()\n    contab[\"hits\"] += contab_2[\"hits\"]\n    contab[\"misses\"] += contab_2[\"misses\"]\n    contab[\"false_alarms\"] += contab_2[\"false_alarms\"]\n    contab[\"correct_negatives\"] += contab_2[\"correct_negatives\"]\n\n    return contab",
  "def det_cat_fct_compute(contab, scores=\"\"):\n    \"\"\"Compute simple and skill scores for deterministic categorical\n    (dichotomous) forecasts from a contingency table object.\n\n    Parameters\n    ----------\n    contab : dict\n      A contingency table object initialized with\n      pysteps.verification.detcatscores.det_cat_fct_init and populated with\n      pysteps.verification.detcatscores.det_cat_fct_accum.\n\n    scores : {string, list of strings}, optional\n        The name(s) of the scores. The default, scores=\"\", will compute all\n        available scores.\n        The available score names a\n\n        .. tabularcolumns:: |p{2cm}|L|\n\n        +------------+--------------------------------------------------------+\n        | Name       | Description                                            |\n        +============+========================================================+\n        |  ACC       | accuracy (proportion correct)                          |\n        +------------+--------------------------------------------------------+\n        |  BIAS      | frequency bias                                         |\n        +------------+--------------------------------------------------------+\n        |  CSI       | critical success index (threat score)                  |\n        +------------+--------------------------------------------------------+\n        |  F1        | the harmonic mean of precision and sensitivity         |\n        +------------+--------------------------------------------------------+\n        |  FA        | false alarm rate (prob. of false detection, fall-out,  |\n        |            | false positive rate)                                   |\n        +------------+--------------------------------------------------------+\n        |  FAR       | false alarm ratio (false discovery rate)               |\n        +------------+--------------------------------------------------------+\n        |  GSS       | Gilbert skill score (equitable threat score)           |\n        +------------+--------------------------------------------------------+\n        |  HK        | Hanssen-Kuipers discriminant (Pierce skill score)      |\n        +------------+--------------------------------------------------------+\n        |  HSS       | Heidke skill score                                     |\n        +------------+--------------------------------------------------------+\n        |  MCC       | Matthews correlation coefficient                       |\n        +------------+--------------------------------------------------------+\n        |  POD       | probability of detection (hit rate, sensitivity,       |\n        |            | recall, true positive rate)                            |\n        +------------+--------------------------------------------------------+\n        |  SEDI      | symmetric extremal dependency index                    |\n        +------------+--------------------------------------------------------+\n\n    Returns\n    -------\n    result : dict\n        Dictionary containing the verification results.\n\n    \"\"\"\n\n    # catch case of single score passed as string\n    def get_iterable(x):\n        if isinstance(x, collections.Iterable) and not isinstance(x, str):\n            return x\n        else:\n            return (x,)\n\n    scores = get_iterable(scores)\n\n    H = 1.0 * contab[\"hits\"]  # true positives\n    M = 1.0 * contab[\"misses\"]  # false negatives\n    F = 1.0 * contab[\"false_alarms\"]  # false positives\n    R = 1.0 * contab[\"correct_negatives\"]  # true negatives\n\n    result = {}\n    for score in scores:\n        # catch None passed as score\n        if score is None:\n            continue\n\n        score_ = score.lower()\n\n        # simple scores\n        POD = H / (H + M)\n        FAR = F / (H + F)\n        FA = F / (F + R)\n        s = (H + M) / (H + M + F + R)\n\n        if score_ in [\"pod\", \"\"]:\n            # probability of detection\n            result[\"POD\"] = POD\n        if score_ in [\"far\", \"\"]:\n            # false alarm ratio\n            result[\"FAR\"] = FAR\n        if score_ in [\"fa\", \"\"]:\n            # false alarm rate (prob of false detection)\n            result[\"FA\"] = FA\n        if score_ in [\"acc\", \"\"]:\n            # accuracy (fraction correct)\n            ACC = (H + R) / (H + M + F + R)\n            result[\"ACC\"] = ACC\n        if score_ in [\"csi\", \"\"]:\n            # critical success index\n            CSI = H / (H + M + F)\n            result[\"CSI\"] = CSI\n        if score_ in [\"bias\", \"\"]:\n            # frequency bias\n            B = (H + F) / (H + M)\n            result[\"BIAS\"] = B\n\n        # skill scores\n        if score_ in [\"hss\", \"\"]:\n            # Heidke Skill Score (-1 < HSS < 1) < 0 implies no skill\n            HSS = 2 * (H * R - F * M) / ((H + M) * (M + R) + (H + F) * (F + R))\n            result[\"HSS\"] = HSS\n        if score_ in [\"hk\", \"\"]:\n            # Hanssen-Kuipers Discriminant\n            HK = POD - FA\n            result[\"HK\"] = HK\n        if score_ in [\"gss\", \"ets\", \"\"]:\n            # Gilbert Skill Score\n            GSS = (POD - FA) / ((1 - s * POD) / (1 - s) + FA * (1 - s) / s)\n            if score_ == \"ets\":\n                result[\"ETS\"] = GSS\n            else:\n                result[\"GSS\"] = GSS\n        if score_ in [\"sedi\", \"\"]:\n            # Symmetric extremal dependence index\n            SEDI = (\n                np.log(FA) - np.log(POD) + np.log(1 - POD) - np.log(1 - FA)\n            ) / (np.log(FA) + np.log(POD) + np.log(1 - POD) + np.log(1 - FA))\n            result[\"SEDI\"] = SEDI\n        if score_ in [\"mcc\", \"\"]:\n            # Matthews correlation coefficient\n            MCC = (H * R - F * M) / np.sqrt(\n                (H + F) * (H + M) * (R + F) * (R + M)\n            )\n            result[\"MCC\"] = MCC\n        if score_ in [\"f1\", \"\"]:\n            # F1 score\n            F1 = 2 * H / (2 * H + F + M)\n            result[\"F1\"] = F1\n\n    return result",
  "def get_iterable(x):\n        if x is None or (\n            isinstance(x, collections.Iterable) and not isinstance(x, int)\n        ):\n            return x\n        else:\n            return (x,)",
  "def get_iterable(x):\n        if isinstance(x, collections.Iterable) and not isinstance(x, str):\n            return x\n        else:\n            return (x,)",
  "def get_method(name, type=\"deterministic\"):\n    \"\"\"Return a callable function for the method corresponding to the given\n    verification score.\n\n    Parameters\n    ----------\n    name : str\n        Name of the verification method. The available options are:\\n\\\n\n        type: deterministic\n\n        .. tabularcolumns:: |p{2cm}|L|\n\n        +------------+--------------------------------------------------------+\n        | Name       | Description                                            |\n        +============+========================================================+\n        |  ACC       | accuracy (proportion correct)                          |\n        +------------+--------------------------------------------------------+\n        |  BIAS      | frequency bias                                         |\n        +------------+--------------------------------------------------------+\n        |  CSI       | critical success index (threat score)                  |\n        +------------+--------------------------------------------------------+\n        |  F1        | the harmonic mean of precision and sensitivity         |\n        +------------+--------------------------------------------------------+\n        |  FA        | false alarm rate (prob. of false detection, fall-out,  |\n        |            | false positive rate)                                   |\n        +------------+--------------------------------------------------------+\n        |  FAR       | false alarm ratio (false discovery rate)               |\n        +------------+--------------------------------------------------------+\n        |  GSS       | Gilbert skill score (equitable threat score)           |\n        +------------+--------------------------------------------------------+\n        |  HK        | Hanssen-Kuipers discriminant (Pierce skill score)      |\n        +------------+--------------------------------------------------------+\n        |  HSS       | Heidke skill score                                     |\n        +------------+--------------------------------------------------------+\n        |  MCC       | Matthews correlation coefficient                       |\n        +------------+--------------------------------------------------------+\n        |  POD       | probability of detection (hit rate, sensitivity,       |\n        |            | recall, true positive rate)                            |\n        +------------+--------------------------------------------------------+\n        |  SEDI      | symmetric extremal dependency index                    |\n        +------------+--------------------------------------------------------+\n        |  beta1     | linear regression slope (type 1 conditional bias)      |\n        +------------+--------------------------------------------------------+\n        |  beta2     | linear regression slope (type 2 conditional bias)      |\n        +------------+--------------------------------------------------------+\n        |  corr_p    | pearson's correleation coefficien (linear correlation) |\n        +------------+--------------------------------------------------------+\n        |  corr_s*   | spearman's correlation coefficient (rank correlation)  |\n        +------------+--------------------------------------------------------+\n        |  DRMSE     | debiased root mean squared error                       |\n        +------------+--------------------------------------------------------+\n        |  MAE       | mean absolute error of residuals                       |\n        +------------+--------------------------------------------------------+\n        |  ME        | mean error or bias of residuals                        |\n        +------------+--------------------------------------------------------+\n        |  MSE       | mean squared error                                     |\n        +------------+--------------------------------------------------------+\n        |  NMSE      | normalized mean squared error                          |\n        +------------+--------------------------------------------------------+\n        |  RMSE      | root mean squared error                                |\n        +------------+--------------------------------------------------------+\n        |  RV        | reduction of variance                                  |\n        |            | (Brier Score, Nash-Sutcliffe Efficiency)               |\n        +------------+--------------------------------------------------------+\n        |  scatter*  | half the distance between the 16% and 84% percentiles  |\n        |            | of the weighted cumulative error distribution,         |\n        |            | where error = dB(pred/obs),                            |\n        |            | as in Germann et al. (2006)                            |\n        +------------+--------------------------------------------------------+\n        |  binary_mse| binary MSE                                             |\n        +------------+--------------------------------------------------------+\n        |  FSS       | fractions skill score                                  |\n        +------------+--------------------------------------------------------+\n\n        type: ensemble\n\n        .. tabularcolumns:: |p{2cm}|L|\n\n        +------------+--------------------------------------------------------+\n        | Name       | Description                                            |\n        +============+========================================================+\n        | ens_skill  | mean ensemble skill                                    |\n        +------------+--------------------------------------------------------+\n        | ens_spread | mean ensemble spread                                   |\n        +------------+--------------------------------------------------------+\n        | rankhist   | rank histogram                                         |\n        +------------+--------------------------------------------------------+\n\n        type: probabilistic\n\n        .. tabularcolumns:: |p{2cm}|L|\n\n        +------------+--------------------------------------------------------+\n        | Name       | Description                                            |\n        +============+========================================================+\n        |  CRPS      | continuous ranked probability score                    |\n        +------------+--------------------------------------------------------+\n        |  reldiag   | reliability diagram                                    |\n        +------------+--------------------------------------------------------+\n        |  ROC       | ROC curve                                              |\n        +------------+--------------------------------------------------------+\n\n    type : {'deterministic', 'ensemble', 'probabilistic'}, optional\n        Type of the verification method.\n\n    Notes\n    -----\n\n    Multiplicative scores can be computed by passing log-tranformed values.\n    Note that \"scatter\" is the only score that will be computed in dB units of\n    the multiplicative error, i.e.: 10*log10(pred/obs).\n\n    beta1 measures the degree of conditional bias of the observations given the\n    forecasts (type 1).\n\n    beta2 measures the degree of conditional bias of the forecasts given the\n    observations (type 2).\n\n    The normalized MSE is computed as\n    NMSE = E[(pred - obs)^2]/E[(pred + obs)^2].\n\n    The debiased RMSE is computed as DRMSE = sqrt(RMSE - ME^2).\n\n    The reduction of variance score is computed as RV = 1 - MSE/Var(obs).\n\n    Score names denoted by * can only be computed offline, meaning that the\n    these cannot be computed using _init, _accum and _compute methods of this\n    module.\n\n    References\n    ----------\n\n    Germann, U. , Galli, G. , Boscacci, M. and Bolliger, M. (2006), Radar\n    precipitation measurement in a mountainous region. Q.J.R. Meteorol. Soc.,\n    132: 1669-1692. doi:10.1256/qj.05.190\n\n    Potts, J. (2012), Chapter 2 - Basic concepts. Forecast verification: a\n    practitioner\u2019s guide in atmospheric sciences, I. T. Jolliffe, and D. B.\n    Stephenson, Eds., Wiley-Blackwell, 11\u201329.\n\n    \"\"\"\n\n    if name is None:\n        name = \"none\"\n    if type is None:\n        type = \"none\"\n\n    name = name.lower()\n    type = type.lower()\n\n    if type in [\"deterministic\"]:\n\n        from .detcatscores import det_cat_fct\n        from .detcontscores import det_cont_fct\n        from .spatialscores import fss, binary_mse\n\n        # categorical\n        if name in [\n            \"acc\",\n            \"csi\",\n            \"f1\",\n            \"fa\",\n            \"far\",\n            \"gss\",\n            \"hk\",\n            \"hss\",\n            \"mcc\",\n            \"pod\",\n            \"sedi\",\n        ]:\n\n            def f(fct, obs, **kwargs):\n                return det_cat_fct(fct, obs, kwargs.pop(\"thr\"), [name])\n\n            return f\n\n        # continuous\n        elif name in [\n            \"beta\",\n            \"beta1\",\n            \"beta2\",\n            \"corr_p\",\n            \"corr_s\",\n            \"drmse\",\n            \"mae\",\n            \"mse\",\n            \"me\",\n            \"nrmse\",\n            \"rmse\",\n            \"rv\",\n            \"scatter\",\n        ]:\n\n            def f(fct, obs, **kwargs):\n                return det_cont_fct(fct, obs, [name], **kwargs)\n\n            return f\n\n        # spatial\n        elif name in [\"binary_mse\"]:\n            return binary_mse\n        elif name in [\"fss\"]:\n            return fss\n\n        else:\n            raise ValueError(\"unknown deterministic method %s\" % name)\n\n    elif type in [\"ensemble\"]:\n\n        from .ensscores import ensemble_skill, ensemble_spread, rankhist\n\n        if name in [\"ens_skill\"]:\n            return ensemble_skill\n        elif name in [\"ens_spread\"]:\n            return ensemble_spread\n        elif name in [\"rankhist\"]:\n            return rankhist\n        else:\n            raise ValueError(\"unknown ensemble method %s\" % name)\n\n    elif type in [\"probabilistic\"]:\n\n        from .probscores import CRPS, reldiag, ROC_curve\n\n        if name in [\"crps\"]:\n            return CRPS\n        elif name in [\"reldiag\"]:\n            return reldiag\n        elif name in [\"roc\"]:\n            return ROC_curve\n        else:\n            raise ValueError(\"unknown probabilistic method %s\" % name)\n\n    else:\n        raise ValueError(\"unknown type %s\" % name)",
  "def f(fct, obs, **kwargs):\n                return det_cat_fct(fct, obs, kwargs.pop(\"thr\"), [name])",
  "def f(fct, obs, **kwargs):\n                return det_cont_fct(fct, obs, [name], **kwargs)",
  "def initialize_forecast_exporter_geotiff(outpath, outfnprefix, startdate,\n                                         timestep, n_timesteps, shape, metadata,\n                                         n_ens_members=1, incremental=None,\n                                         **kwargs):\n    \"\"\"Initialize a GeoTIFF forecast exporter.\n\n    The output files are named as '<outfnprefix>_<startdate>_<t>.tif', where\n    startdate is in YYmmddHHMM format and t is lead time (minutes). GDAL needs\n    to be installed to use this exporter.\n\n    Parameters\n    ----------\n    outpath : str\n        Output path.\n\n    outfnprefix : str\n        Prefix for output file names.\n\n    startdate : datetime.datetime\n        Start date of the forecast.\n\n    timestep : int\n        Time step of the forecast (minutes).\n\n    n_timesteps : int\n        Number of time steps in the forecast. This argument is ignored if\n        incremental is set to 'timestep'.\n\n    shape : tuple of int\n        Two-element tuple defining the shape (height,width) of the forecast\n        grids.\n\n    metadata: dict\n        Metadata dictionary containing the projection,x1,x2,y1,y2 and unit\n        attributes described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n\n    n_ens_members : int\n        Number of ensemble members in the forecast.\n\n    incremental : {None,'timestep'}, optional\n        Allow incremental writing of datasets into the GeoTIFF files. Set to\n        'timestep' to enable writing forecasts or forecast ensembles separately\n        for each time step. If set to None, incremental writing is disabled and\n        the whole forecast is written in a single function call. The 'member'\n        option is not currently implemented.\n\n    Returns\n    -------\n    exporter : dict\n        The return value is a dictionary containing an exporter object. This\n        can be used with :py:func:`pysteps.io.exporters.export_forecast_dataset`\n        to write the datasets.\n\n    \"\"\"\n\n    if not GDAL_IMPORTED:\n        raise MissingOptionalDependency(\n            \"gdal package is required for GeoTIFF \"\n            \"exporters but it is not installed\")\n\n    if incremental == \"member\":\n        raise ValueError(\"incremental writing of GeoTIFF files with the 'member' option is not supported\")\n\n    exporter = {}\n    exporter[\"method\"] = \"geotiff\"\n    exporter[\"outfnprefix\"] = outfnprefix\n    exporter[\"startdate\"] = startdate\n    exporter[\"timestep\"] = timestep\n    exporter[\"num_timesteps\"] = n_timesteps\n    exporter[\"shape\"] = shape\n    exporter[\"metadata\"] = metadata\n    exporter[\"num_ens_members\"] = n_ens_members\n    exporter[\"incremental\"] = incremental\n    exporter[\"dst\"] = []\n\n    driver = gdal.GetDriverByName(\"GTiff\")\n    exporter[\"driver\"] = driver\n\n    if incremental != \"timestep\":\n        for i in range(n_timesteps):\n            outfn = _get_geotiff_filename(outfnprefix, startdate, n_timesteps,\n                                          timestep, i)\n            outfn = os.path.join(outpath, outfn)\n            dst = _create_geotiff_file(outfn, driver, shape, metadata, n_ens_members)\n            exporter[\"dst\"].append(dst)\n    else:\n        exporter[\"num_files_written\"] = 0\n\n    return exporter",
  "def initialize_forecast_exporter_kineros(outpath, outfnprefix, startdate,\n                                         timestep, n_timesteps, shape, metadata,\n                                         n_ens_members=1, incremental=None,\n                                         **kwargs):\n    \"\"\"Initialize a KINEROS2 Rainfall .pre file as specified\n    in https://www.tucson.ars.ag.gov/kineros/.\n\n    Grid points are treated as individual rain gauges and a separate file is\n    produced for each ensemble member. The output files are named as\n    <outfnprefix>_N<n>.pre, where <n> is the index of ensemble member starting\n    from zero.\n\n    Parameters\n    ----------\n    outpath : str\n        Output path.\n\n    outfnprefix : str\n        Prefix for output file names.\n\n    startdate : datetime.datetime\n        Start date of the forecast.\n\n    timestep : int\n        Time step of the forecast (minutes).\n\n    n_timesteps : int\n        Number of time steps in the forecast this argument is ignored if\n        incremental is set to 'timestep'.\n\n    shape : tuple of int\n        Two-element tuple defining the shape (height,width) of the forecast\n        grids.\n\n    metadata: dict\n        Metadata dictionary containing the projection,x1,x2,y1,y2 and unit\n        attributes described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n\n    n_ens_members : int\n        Number of ensemble members in the forecast. This argument is ignored if\n        incremental is set to 'member'.\n\n    incremental : {None}, optional\n        Currently not implemented for this method.\n\n    Returns\n    -------\n    exporter : dict\n        The return value is a dictionary containing an exporter object. This c\n        an be used with :py:func:`pysteps.io.exporters.export_forecast_dataset`\n        to write datasets into the given file format.\n\n    \"\"\"\n\n    if incremental is not None:\n        raise ValueError(\"unknown option %s: incremental writing is not supported\" % incremental)\n\n    exporter = {}\n\n    # one file for each member\n    n_ens_members = np.min((99, n_ens_members))\n    fns = []\n    for i in range(n_ens_members):\n        outfn = \"%s_N%02d%s\" % (outfnprefix, i, \".pre\")\n        outfn = os.path.join(outpath, outfn)\n        with open(outfn, \"w\") as fd:\n            # write header\n            fd.writelines(\"! pysteps-generated nowcast.\\n\")\n            fd.writelines(\"! created the %s.\\n\" % datetime.now().strftime(\"%c\"))\n            # TODO(exporters): Add pySTEPS version here\n            fd.writelines(\"! Member = %02d.\\n\" % i)\n            fd.writelines(\"! Startdate = %s.\\n\" % startdate.strftime(\"%c\"))\n            fns.append(outfn)\n        fd.close()\n\n    h, w = shape\n\n    if metadata[\"unit\"] == \"mm/h\":\n        var_name = \"Intensity\"\n        var_long_name = \"Intensity in mm/hr\"\n        var_unit = \"mm/hr\"\n    elif metadata[\"unit\"] == \"mm\":\n        var_name = \"Depth\"\n        var_long_name = \"Accumulated depth in mm\"\n        var_unit = \"mm\"\n    else:\n        raise ValueError(\"unsupported unit %s\" % metadata[\"unit\"])\n\n    xr = np.linspace(metadata[\"x1\"], metadata[\"x2\"], w + 1)[:-1]\n    xr += 0.5 * (xr[1] - xr[0])\n    yr = np.linspace(metadata[\"y1\"], metadata[\"y2\"], h + 1)[:-1]\n    yr += 0.5 * (yr[1] - yr[0])\n\n    xy_coords = np.stack(np.meshgrid(xr, yr))\n\n    exporter[\"method\"] = \"kineros\"\n    exporter[\"ncfile\"] = fns\n    exporter[\"XY_coords\"] = xy_coords\n    exporter[\"var_name\"] = var_name\n    exporter[\"var_long_name\"] = var_long_name\n    exporter[\"var_unit\"] = var_unit\n    exporter[\"startdate\"] = startdate\n    exporter[\"timestep\"] = timestep\n    exporter[\"metadata\"] = metadata\n    exporter[\"incremental\"] = incremental\n    exporter[\"num_timesteps\"] = n_timesteps\n    exporter[\"num_ens_members\"] = n_ens_members\n    exporter[\"shape\"] = shape\n\n    return exporter",
  "def initialize_forecast_exporter_netcdf(outpath, outfnprefix, startdate,\n                                        timestep, n_timesteps, shape, metadata,\n                                        n_ens_members=1, incremental=None,\n                                        **kwargs):\n    \"\"\"Initialize a netCDF forecast exporter. All outputs are written to a\n    single file named as '<outfnprefix>_.nc'.\n\n    Parameters\n    ----------\n    outpath : str\n        Output path.\n\n    outfnprefix : str\n        Prefix for output file names.\n\n    startdate : datetime.datetime\n        Start date of the forecast.\n\n    timestep : int\n        Time step of the forecast (minutes).\n\n    n_timesteps : int\n        Number of time steps in the forecast this argument is ignored if\n        incremental is set to 'timestep'.\n\n    shape : tuple of int\n        Two-element tuple defining the shape (height,width) of the forecast\n        grids.\n\n    metadata: dict\n        Metadata dictionary containing the projection,x1,x2,y1,y2 and unit\n        attributes described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n\n    n_ens_members : int\n        Number of ensemble members in the forecast. This argument is ignored if\n        incremental is set to 'member'.\n\n    incremental : {None,'timestep','member'}, optional\n        Allow incremental writing of datasets into the netCDF files.\\n\n        The available options are: 'timestep' = write a forecast or a forecast\n        ensemble for  a given time step; 'member' = write a forecast sequence\n        for a given ensemble member. If set to None, incremental writing is\n        disabled.\n\n    Returns\n    -------\n    exporter : dict\n        The return value is a dictionary containing an exporter object. This c\n        an be used with :py:func:`pysteps.io.exporters.export_forecast_dataset`\n        to write datasets into the given file format.\n\n    \"\"\"\n    if not NETCDF4_IMPORTED:\n        raise MissingOptionalDependency(\n            \"netCDF4 package is required for netcdf \"\n            \"exporters but it is not installed\")\n\n    if not PYPROJ_IMPORTED:\n        raise MissingOptionalDependency(\n            \"pyproj package is required for netcdf \"\n            \"exporters but it is not installed\")\n\n    if incremental not in [None, \"timestep\", \"member\"]:\n        raise ValueError(\"unknown option %s: incremental must be 'timestep' or 'member'\" % incremental)\n\n    if incremental == \"timestep\":\n        n_timesteps = None\n    elif incremental == \"member\":\n        n_ens_members = None\n    elif incremental is not None:\n        raise ValueError(\"unknown argument value incremental='%s': must be 'timestep' or 'member'\" % str(incremental))\n\n    n_ens_gt_one = False\n    if n_ens_members is not None:\n        if n_ens_members > 1:\n            n_ens_gt_one = True\n\n    exporter = {}\n\n    outfn = os.path.join(outpath, outfnprefix + \".nc\")\n    ncf = netCDF4.Dataset(outfn, 'w', format=\"NETCDF4\")\n\n    ncf.Conventions = \"CF-1.7\"\n    ncf.title = \"pysteps-generated nowcast\"\n    ncf.institution = \"the pySTEPS community (https://pysteps.github.io)\"\n    ncf.source = \"pysteps\"  # TODO(exporters): Add pySTEPS version here\n    ncf.history = \"\"\n    ncf.references = \"\"\n    ncf.comment = \"\"\n\n    h, w = shape\n\n    ncf.createDimension(\"ens_number\", size=n_ens_members)\n    ncf.createDimension(\"time\", size=n_timesteps)\n    ncf.createDimension(\"y\", size=h)\n    ncf.createDimension(\"x\", size=w)\n\n    if metadata[\"unit\"] == \"mm/h\":\n        var_name = \"precip_intensity\"\n        var_standard_name = None\n        var_long_name = \"instantaneous precipitation rate\"\n        var_unit = \"mm h-1\"\n    elif metadata[\"unit\"] == \"mm\":\n        var_name = \"precip_accum\"\n        var_standard_name = None\n        var_long_name = \"accumulated precipitation\"\n        var_unit = \"mm\"\n    elif metadata[\"unit\"] == \"dBZ\":\n        var_name = \"reflectivity\"\n        var_long_name = \"equivalent reflectivity factor\"\n        var_standard_name = \"equivalent_reflectivity_factor\"\n        var_unit = \"dBZ\"\n    else:\n        raise ValueError(\"unknown unit %s\" % metadata[\"unit\"])\n\n    xr = np.linspace(metadata[\"x1\"], metadata[\"x2\"], w + 1)[:-1]\n    xr += 0.5 * (xr[1] - xr[0])\n    yr = np.linspace(metadata[\"y1\"], metadata[\"y2\"], h + 1)[:-1]\n    yr += 0.5 * (yr[1] - yr[0])\n\n    var_xc = ncf.createVariable(\"xc\", np.float32, dimensions=(\"x\",))\n    var_xc[:] = xr\n    var_xc.axis = 'X'\n    var_xc.standard_name = \"projection_x_coordinate\"\n    var_xc.long_name = \"x-coordinate in Cartesian system\"\n    # TODO(exporters): Don't hard-code the unit.\n    var_xc.units = 'm'\n\n    var_yc = ncf.createVariable(\"yc\", np.float32, dimensions=(\"y\",))\n    var_yc[:] = yr\n    var_yc.axis = 'Y'\n    var_yc.standard_name = \"projection_y_coordinate\"\n    var_yc.long_name = \"y-coordinate in Cartesian system\"\n    # TODO(exporters): Don't hard-code the unit.\n    var_yc.units = 'm'\n\n    x_2d, y_2d = np.meshgrid(xr, yr)\n    pr = pyproj.Proj(metadata[\"projection\"])\n    lon, lat = pr(x_2d.flatten(), y_2d.flatten(), inverse=True)\n\n    var_lon = ncf.createVariable(\"lon\", np.float, dimensions=(\"y\", \"x\"))\n    var_lon[:] = lon\n    var_lon.standard_name = \"longitude\"\n    var_lon.long_name = \"longitude coordinate\"\n    # TODO(exporters): Don't hard-code the unit.\n    var_lon.units = \"degrees_east\"\n\n    var_lat = ncf.createVariable(\"lat\", np.float, dimensions=(\"y\", \"x\"))\n    var_lat[:] = lat\n    var_lat.standard_name = \"latitude\"\n    var_lat.long_name = \"latitude coordinate\"\n    # TODO(exporters): Don't hard-code the unit.\n    var_lat.units = \"degrees_north\"\n\n    ncf.projection = metadata[\"projection\"]\n\n    grid_mapping_var_name, grid_mapping_name, grid_mapping_params = \\\n        _convert_proj4_to_grid_mapping(metadata[\"projection\"])\n    # skip writing the grid mapping if a matching name was not found\n    if grid_mapping_var_name is not None:\n        var_gm = ncf.createVariable(grid_mapping_var_name, np.int,\n                                    dimensions=())\n        var_gm.grid_mapping_name = grid_mapping_name\n        for i in grid_mapping_params.items():\n            var_gm.setncattr(i[0], i[1])\n\n    if incremental == \"member\" or n_ens_gt_one:\n        var_ens_num = ncf.createVariable(\"ens_number\", np.int,\n                                         dimensions=(\"ens_number\",)\n                                         )\n        if incremental != \"member\":\n            var_ens_num[:] = list(range(1, n_ens_members + 1))\n        var_ens_num.long_name = \"ensemble member\"\n        var_ens_num.units = \"\"\n\n    var_time = ncf.createVariable(\"time\", np.int, dimensions=(\"time\",))\n    if incremental != \"timestep\":\n        var_time[:] = [i * timestep * 60 for i in range(1, n_timesteps + 1)]\n    var_time.long_name = \"forecast time\"\n    startdate_str = datetime.strftime(startdate, \"%Y-%m-%d %H:%M:%S\")\n    var_time.units = \"seconds since %s\" % startdate_str\n\n    if incremental == \"member\" or n_ens_gt_one:\n        var_f = ncf.createVariable(var_name, np.float32,\n                                   dimensions=(\"ens_number\", \"time\", \"y\", \"x\"),\n                                   zlib=True, complevel=9)\n    else:\n        var_f = ncf.createVariable(var_name, np.float32,\n                                   dimensions=(\"time\", \"y\", \"x\"),\n                                   zlib=True, complevel=9)\n\n    if var_standard_name is not None:\n        var_f.standard_name = var_standard_name\n    var_f.long_name = var_long_name\n    var_f.coordinates = \"y x\"\n    var_f.units = var_unit\n\n    exporter[\"method\"] = \"netcdf\"\n    exporter[\"ncfile\"] = ncf\n    exporter[\"var_F\"] = var_f\n    if incremental == \"member\" or n_ens_gt_one:\n        exporter[\"var_ens_num\"] = var_ens_num\n    exporter[\"var_time\"] = var_time\n    exporter[\"var_name\"] = var_name\n    exporter[\"startdate\"] = startdate\n    exporter[\"timestep\"] = timestep\n    exporter[\"metadata\"] = metadata\n    exporter[\"incremental\"] = incremental\n    exporter[\"num_timesteps\"] = n_timesteps\n    exporter[\"num_ens_members\"] = n_ens_members\n    exporter[\"shape\"] = shape\n\n    return exporter",
  "def export_forecast_dataset(field, exporter):\n    \"\"\"Write a forecast array into a file.\n\n    If the exporter was initialized with n_ens_members>1, the written dataset\n    has dimensions (n_ens_members,num_timesteps,shape[0],shape[1]), where shape\n    refers to the shape of the two-dimensional forecast grids. Otherwise, the\n    dimensions are (num_timesteps,shape[0],shape[1]). If the exporter was\n    initialized with incremental!=None, the array is appended to the existing\n    dataset either along the ensemble member or time axis.\n\n    Parameters\n    ----------\n    exporter : dict\n        An exporter object created with any initialization method implemented\n        in :py:mod:`pysteps.io.exporters`.\n    field : array_like\n        The array to write. The required shape depends on the choice of the\n        'incremental' parameter the exporter was initialized with:\n\n        +-----------------+---------------------------------------------------+\n        |    incremental  |                    required shape                 |\n        +=================+===================================================+\n        |    None         | (num_ens_members,num_timesteps,shape[0],shape[1]) |\n        +-----------------+---------------------------------------------------+\n        |    'timestep'   | (num_ens_members,shape[0],shape[1])               |\n        +-----------------+---------------------------------------------------+\n        |    'member'     | (num_timesteps,shape[0],shape[1])                 |\n        +-----------------+---------------------------------------------------+\n\n        If the exporter was initialized with num_ens_members=1, the num_ens_members\n        dimension is dropped.\n\n    \"\"\"\n    if exporter[\"method\"] == \"netcdf\" and not NETCDF4_IMPORTED:\n        raise MissingOptionalDependency(\n            \"netCDF4 package is required for netcdf \"\n            \"exporters but it is not installed\")\n\n    if exporter[\"incremental\"] is None:\n        if exporter[\"num_ens_members\"] > 1:\n            shp = (exporter[\"num_ens_members\"], exporter[\"num_timesteps\"],\n                   exporter[\"shape\"][0], exporter[\"shape\"][1])\n        else:\n            shp = (exporter[\"num_timesteps\"], exporter[\"shape\"][0],\n                   exporter[\"shape\"][1])\n        if field.shape != shp:\n            raise ValueError(\"field has invalid shape: %s != %s\" % (str(field.shape), str(shp)))\n    elif exporter[\"incremental\"] == \"timestep\":\n        if exporter[\"num_ens_members\"] > 1:\n            shp = (exporter[\"num_ens_members\"], exporter[\"shape\"][0],\n                   exporter[\"shape\"][1])\n        else:\n            shp = exporter[\"shape\"]\n        if field.shape != shp:\n            raise ValueError(\"field has invalid shape: %s != %s\" % (str(field.shape), str(shp)))\n    elif exporter[\"incremental\"] == \"member\":\n        shp = (exporter[\"num_timesteps\"], exporter[\"shape\"][0],\n               exporter[\"shape\"][1])\n        if field.shape != shp:\n            raise ValueError(\"field has invalid shape: %s != %s\" % (str(field.shape), str(shp)))\n\n    if exporter[\"method\"] == \"geotiff\":\n        _export_geotiff(field, exporter)\n    elif exporter[\"method\"] == \"netcdf\":\n        _export_netcdf(field, exporter)\n    elif exporter[\"method\"] == \"kineros\":\n        _export_kineros(field, exporter)\n    else:\n        raise ValueError(\"unknown exporter method %s\" % exporter[\"method\"])",
  "def close_forecast_files(exporter):\n    \"\"\"Close the files associated with a forecast exporter.\n\n    Finish writing forecasts and close the output files opened by a forecast\n    exporter.\n\n    Parameters\n    ----------\n    exporter : dict\n        An exporter object created with any initialization method implemented\n        in :py:mod:`pysteps.io.exporters`.\n\n    \"\"\"\n    if exporter[\"method\"] == \"geotiff\":\n        pass  # NOTE: There is no explicit \"close\" method in GDAL.\n        # The files are closed when all objects referencing to the GDAL\n        # datasets are deleted (i.e. when the exporter object is deleted).\n    if exporter[\"method\"] == \"kineros\":\n        pass  # no need to close the file\n    else:\n        exporter[\"ncfile\"].close()",
  "def _export_geotiff(F, exporter):\n    def init_band(band):\n        band.SetScale(1.0)\n        band.SetOffset(0.0)\n        band.SetUnitType(exporter[\"metadata\"][\"unit\"])\n\n    if exporter[\"incremental\"] is None:\n        for i in range(exporter[\"num_timesteps\"]):\n            if exporter[\"num_ens_members\"] == 1:\n                band = exporter[\"dst\"][i].GetRasterBand(1)\n                init_band(band)\n                band.WriteArray(F[i, :, :])\n            else:\n                for j in range(exporter[\"num_ens_members\"]):\n                    band = exporter[\"dst\"][i].GetRasterBand(j + 1)\n                    init_band(band)\n                    band.WriteArray(F[j, i, :, :])\n    elif exporter[\"incremental\"] == \"timestep\":\n        i = exporter[\"num_files_written\"]\n\n        outfn = _get_geotiff_filename(exporter[\"outfnprefix\"],\n                                      exporter[\"startdate\"],\n                                      exporter[\"num_timesteps\"],\n                                      exporter[\"timestep\"], i)\n        dst = _create_geotiff_file(outfn, exporter[\"driver\"],\n                                   exporter[\"shape\"],\n                                   exporter[\"metadata\"],\n                                   exporter[\"num_ens_members\"])\n\n        for j in range(exporter[\"num_ens_members\"]):\n            band = dst.GetRasterBand(j + 1)\n            init_band(band)\n            if exporter[\"num_ens_members\"] > 1:\n                band.WriteArray(F[j, :, :])\n            else:\n                band.WriteArray(F)\n\n        exporter[\"num_files_written\"] += 1\n    elif exporter[\"incremental\"] == \"member\":\n        for i in range(exporter[\"num_timesteps\"]):\n            # NOTE: This does not work because the GeoTIFF driver does not\n            # support adding bands. An alternative solution needs to be\n            # implemented.\n            exporter[\"dst\"][i].AddBand(gdal.GDT_Float32)\n            band = exporter[\"dst\"][i].GetRasterBand(exporter[\"dst\"][i].RasterCount)\n            init_band(band)\n            band.WriteArray(F[i, :, :])",
  "def _export_kineros(field, exporter):\n    num_timesteps = exporter[\"num_timesteps\"]\n    num_ens_members = exporter[\"num_ens_members\"]\n\n    timestep = exporter[\"timestep\"]\n    xgrid = exporter[\"XY_coords\"][0, :, :].flatten()\n    ygrid = exporter[\"XY_coords\"][1, :, :].flatten()\n\n    timemin = [(t + 1) * timestep for t in range(num_timesteps)]\n\n    for n in range(num_ens_members):\n        file_name = exporter[\"ncfile\"][n]\n\n        field_tmp = field[n, :, :, :].reshape((num_timesteps, -1))\n\n        if exporter[\"var_name\"] == \"Depth\":\n            field_tmp = np.cumsum(field_tmp, axis=0)\n\n        with open(file_name, \"a\") as fd:\n            for m in range(field_tmp.shape[1]):\n                fd.writelines(\"BEGIN RG%03d\\n\" % (m + 1))\n                fd.writelines(\"  X = %.2f, Y = %.2f\\n\" % (xgrid[m], ygrid[m]))\n                fd.writelines(\"  N = %i\\n\" % num_timesteps)\n                fd.writelines(\"  TIME        %s\\n\" % exporter[\"var_name\"].upper())\n                fd.writelines(\"! (min)        (%s)\\n\" % exporter[\"var_unit\"])\n                for t in range(num_timesteps):\n                    line_new = \"{:6.1f}  {:11.2f}\\n\".format(timemin[t], field_tmp[t, m])\n                    fd.writelines(line_new)\n                fd.writelines(\"END\\n\\n\")",
  "def _export_netcdf(field, exporter):\n    var_f = exporter[\"var_F\"]\n\n    if exporter[\"incremental\"] is None:\n        var_f[:] = field\n    elif exporter[\"incremental\"] == \"timestep\":\n        if exporter[\"num_ens_members\"] > 1:\n            var_f[:, var_f.shape[1], :, :] = field\n        else:\n            var_f[var_f.shape[1], :, :] = field\n        var_time = exporter[\"var_time\"]\n        var_time[len(var_time) - 1] = len(var_time) * exporter[\"timestep\"] * 60\n    else:\n        var_f[var_f.shape[0], :, :, :] = field\n        var_ens_num = exporter[\"var_ens_num\"]\n        var_ens_num[len(var_ens_num) - 1] = len(var_ens_num)",
  "def _convert_proj4_to_grid_mapping(proj4str):\n    tokens = proj4str.split('+')\n\n    d = {}\n    for t in tokens[1:]:\n        t = t.split('=')\n        if len(t) > 1:\n            d[t[0]] = t[1].strip()\n\n    params = {}\n    # TODO(exporters): implement more projection types here\n    if d[\"proj\"] == \"stere\":\n        grid_mapping_var_name = \"polar_stereographic\"\n        grid_mapping_name = \"polar_stereographic\"\n        v = d[\"lon_0\"] if d[\"lon_0\"][-1] not in [\"E\", \"W\"] else d[\"lon_0\"][:-1]\n        params[\"straight_vertical_longitude_from_pole\"] = float(v)\n        v = d[\"lat_0\"] if d[\"lat_0\"][-1] not in [\"N\", \"S\"] else d[\"lat_0\"][:-1]\n        params[\"latitude_of_projection_origin\"] = float(v)\n        if \"lat_ts\" in list(d.keys()):\n            params[\"standard_parallel\"] = float(d[\"lat_ts\"])\n        elif \"k_0\" in list(d.keys()):\n            params[\"scale_factor_at_projection_origin\"] = float(d[\"k_0\"])\n        params[\"false_easting\"] = float(d[\"x_0\"])\n        params[\"false_northing\"] = float(d[\"y_0\"])\n    elif d[\"proj\"] == \"aea\":  # Albers Conical Equal Area\n        grid_mapping_var_name = \"proj\"\n        grid_mapping_name = \"albers_conical_equal_area\"\n        params[\"false_easting\"] = float(d[\"x_0\"]) if \"x_0\" in d else float(0)\n        params[\"false_northing\"] = float(d[\"y_0\"]) if \"y_0\" in d else float(0)\n        v = d[\"lon_0\"] if \"lon_0\" in d else float(0)\n        params[\"longitude_of_central_meridian\"] = float(v)\n        v = d[\"lat_0\"] if \"lat_0\" in d else float(0)\n        params[\"latitude_of_projection_origin\"] = float(v)\n        v1 = d[\"lat_1\"] if \"lat_1\" in d else float(0)\n        v2 = d[\"lat_2\"] if \"lat_2\" in d else float(0)\n        params[\"standard_parallel\"] = (float(v1), float(v2))\n    else:\n        print('unknown projection', d[\"proj\"])\n        return None, None, None\n\n    return grid_mapping_var_name, grid_mapping_name, params",
  "def _create_geotiff_file(outfn, driver, shape, metadata, num_bands):\n    dst = driver.Create(outfn, shape[1], shape[0], num_bands, gdal.GDT_Float32,\n                        [\"COMPRESS=DEFLATE\", \"PREDICTOR=3\"])\n\n    sx = (metadata[\"x2\"] - metadata[\"x1\"]) / shape[1]\n    sy = (metadata[\"y2\"] - metadata[\"y1\"]) / shape[0]\n    dst.SetGeoTransform([metadata[\"x1\"], sx, 0.0, metadata[\"y2\"], 0.0, -sy])\n\n    sr = osr.SpatialReference()\n    sr.ImportFromProj4(metadata[\"projection\"])\n    dst.SetProjection(sr.ExportToWkt())\n\n    return dst",
  "def _get_geotiff_filename(prefix, startdate, n_timesteps, timestep,\n                          timestep_index):\n    if n_timesteps * timestep == 0:\n        raise ValueError(\"n_timesteps x timestep can't be 0.\")\n\n    timestep_format_str = f\"{{time_str:0{int(np.floor(np.log10(n_timesteps * timestep))) + 1}d}}\"\n\n    startdate_str = datetime.strftime(startdate, \"%Y%m%d%H%M\")\n\n    timestep_str = timestep_format_str.format(time_str=(timestep_index + 1) * timestep)\n\n    return f\"{prefix}_{startdate_str}_{timestep_str}.tif\"",
  "def init_band(band):\n        band.SetScale(1.0)\n        band.SetOffset(0.0)\n        band.SetUnitType(exporter[\"metadata\"][\"unit\"])",
  "def import_netcdf_pysteps(filename, **kwargs):\n    \"\"\"Read a nowcast or a nowcast ensemble from a NetCDF file conforming to the\n    CF 1.7 specification.\"\"\"\n    if not NETCDF4_IMPORTED:\n        raise MissingOptionalDependency(\n            \"netCDF4 package is required to import pysteps netcdf \"\n            \"nowcasts but it is not installed\"\n        )\n    try:\n        ds = netCDF4.Dataset(filename, \"r\")\n\n        var_names = list(ds.variables.keys())\n\n        if \"precip_intensity\" in var_names:\n            precip = ds.variables[\"precip_intensity\"]\n            unit = \"mm/h\"\n            accutime = None\n            transform = None\n        elif \"precip_accum\" in var_names:\n            precip = ds.variables[\"precip_accum\"]\n            unit = \"mm\"\n            accutime = None\n            transform = None\n        elif \"hourly_precip_accum\" in var_names:\n            precip = ds.variables[\"hourly_precip_accum\"]\n            unit = \"mm\"\n            accutime = 60.0\n            transform = None\n        elif \"reflectivity\" in var_names:\n            precip = ds.variables[\"reflectivity\"]\n            unit = \"dBZ\"\n            accutime = None\n            transform = \"dB\"\n        else:\n            raise DataModelError(\n                \"Non CF compilant file: \"\n                \"the netCDF file does not contain any supported variable name.\\n\"\n                \"Supported names: 'precip_intensity', 'hourly_precip_accum', \"\n                \"or 'reflectivity'\\n\"\n                \"file: \" + filename\n            )\n\n        precip = precip[...].squeeze().astype(float)\n\n        metadata = {}\n\n        time_var = ds.variables[\"time\"]\n        leadtimes = time_var[:] / 60.0  # minutes leadtime\n        metadata[\"leadtimes\"] = leadtimes\n        timestamps = netCDF4.num2date(time_var[:], time_var.units)\n        metadata[\"timestamps\"] = timestamps\n\n        if \"polar_stereographic\" in var_names:\n            vn = \"polar_stereographic\"\n\n            attr_dict = {}\n            for attr_name in ds.variables[vn].ncattrs():\n                attr_dict[attr_name] = ds[vn].getncattr(attr_name)\n\n            proj_str = _convert_grid_mapping_to_proj4(attr_dict)\n            metadata[\"projection\"] = proj_str\n\n        # geodata\n        metadata[\"xpixelsize\"] = abs(ds.variables[\"xc\"][1] - ds.variables[\"xc\"][0])\n        metadata[\"ypixelsize\"] = abs(ds.variables[\"yc\"][1] - ds.variables[\"yc\"][0])\n\n        xmin = np.min(ds.variables[\"xc\"]) - 0.5 * metadata[\"xpixelsize\"]\n        xmax = np.max(ds.variables[\"xc\"]) + 0.5 * metadata[\"xpixelsize\"]\n        ymin = np.min(ds.variables[\"yc\"]) - 0.5 * metadata[\"ypixelsize\"]\n        ymax = np.max(ds.variables[\"yc\"]) + 0.5 * metadata[\"ypixelsize\"]\n\n        # TODO: this is only a quick solution\n        metadata[\"x1\"] = xmin\n        metadata[\"y1\"] = ymin\n        metadata[\"x2\"] = xmax\n        metadata[\"y2\"] = ymax\n\n        metadata[\"yorigin\"] = \"upper\"  # TODO: check this\n\n        # TODO: Read the metadata to the dictionary.\n        if accutime is None:\n            accutime = leadtimes[1] - leadtimes[0]\n        metadata[\"accutime\"] = accutime\n        metadata[\"unit\"] = unit\n        metadata[\"transform\"] = transform\n        metadata[\"zerovalue\"] = np.nanmin(precip)\n        metadata[\"threshold\"] = np.nanmin(precip[precip > np.nanmin(precip)])\n\n        ds.close()\n\n        return precip, metadata\n    except Exception as er:\n        print(\"There was an error processing the file\", er)\n        return None, None",
  "def _convert_grid_mapping_to_proj4(grid_mapping):\n    gm_keys = list(grid_mapping.keys())\n\n    # TODO: implement more projection types here\n    if grid_mapping[\"grid_mapping_name\"] == \"polar_stereographic\":\n        proj_str = \"+proj=stere\"\n        proj_str += \" +lon_0=%s\" % grid_mapping[\"straight_vertical_longitude_from_pole\"]\n        proj_str += \" +lat_0=%s\" % grid_mapping[\"latitude_of_projection_origin\"]\n        if \"standard_parallel\" in gm_keys:\n            proj_str += \" +lat_ts=%s\" % grid_mapping[\"standard_parallel\"]\n        if \"scale_factor_at_projection_origin\" in gm_keys:\n            proj_str += \" +k_0=%s\" % grid_mapping[\"scale_factor_at_projection_origin\"]\n        proj_str += \" +x_0=%s\" % grid_mapping[\"false_easting\"]\n        proj_str += \" +y_0=%s\" % grid_mapping[\"false_northing\"]\n\n        return proj_str\n    else:\n        return None",
  "def import_bom_rf3(filename, **kwargs):\n    \"\"\"Import a NetCDF radar rainfall product from the BoM Rainfields3.\n\n    Parameters\n    ----------\n\n    filename : str\n        Name of the file to import.\n\n    Returns\n    -------\n\n    out : tuple\n        A three-element tuple containing the rainfall field in mm/h imported\n        from the Bureau RF3 netcdf, the quality field and the metadata. The\n        quality field is currently set to None.\n    \"\"\"\n    if not NETCDF4_IMPORTED:\n        raise MissingOptionalDependency(\n            \"netCDF4 package is required to import BoM Rainfields3 products \"\n            \"but it is not installed\"\n        )\n\n    precip = _import_bom_rf3_data(filename)\n\n    geodata = _import_bom_rf3_geodata(filename)\n    metadata = geodata\n    # TODO(import_bom_rf3): Add missing georeferencing data.\n\n    metadata[\"transform\"] = None\n    metadata[\"zerovalue\"] = np.nanmin(precip)\n    if np.any(np.isfinite(precip)):\n        metadata[\"threshold\"] = np.nanmin(precip[precip > np.nanmin(precip)])\n    else:\n        metadata[\"threshold\"] = np.nan\n\n    return precip, None, metadata",
  "def _import_bom_rf3_data(filename):\n    ds_rainfall = netCDF4.Dataset(filename)\n    if \"precipitation\" in ds_rainfall.variables.keys():\n        precipitation = ds_rainfall.variables[\"precipitation\"][:]\n    else:\n        precipitation = None\n    ds_rainfall.close()\n\n    return precipitation",
  "def _import_bom_rf3_geodata(filename):\n    geodata = {}\n\n    ds_rainfall = netCDF4.Dataset(filename)\n\n    if \"proj\" in ds_rainfall.variables.keys():\n        projection = ds_rainfall.variables[\"proj\"]\n        if getattr(projection, \"grid_mapping_name\") == \"albers_conical_equal_area\":\n            projdef = \"+proj=aea \"\n            lon_0 = getattr(projection, \"longitude_of_central_meridian\")\n            projdef += \" +lon_0=\" + f\"{lon_0:.3f}\"\n            lat_0 = getattr(projection, \"latitude_of_projection_origin\")\n            projdef += \" +lat_0=\" + f\"{lat_0:.3f}\"\n            standard_parallels = getattr(projection, \"standard_parallel\")\n            projdef += \" +lat_1=\" + f\"{standard_parallels[0]:.3f}\"\n            projdef += \" +lat_2=\" + f\"{standard_parallels[1]:.3f}\"\n        else:\n            projdef = None\n    geodata[\"projection\"] = projdef\n\n    if \"valid_min\" in ds_rainfall.variables[\"x\"].ncattrs():\n        xmin = getattr(ds_rainfall.variables[\"x\"], \"valid_min\")\n        xmax = getattr(ds_rainfall.variables[\"x\"], \"valid_max\")\n        ymin = getattr(ds_rainfall.variables[\"y\"], \"valid_min\")\n        ymax = getattr(ds_rainfall.variables[\"y\"], \"valid_max\")\n    else:\n        xmin = min(ds_rainfall.variables[\"x\"])\n        xmax = max(ds_rainfall.variables[\"x\"])\n        ymin = min(ds_rainfall.variables[\"y\"])\n        ymax = max(ds_rainfall.variables[\"y\"])\n\n    xpixelsize = abs(ds_rainfall.variables[\"x\"][1] - ds_rainfall.variables[\"x\"][0])\n    ypixelsize = abs(ds_rainfall.variables[\"y\"][1] - ds_rainfall.variables[\"y\"][0])\n    factor_scale = 1.0\n    if \"units\" in ds_rainfall.variables[\"x\"].ncattrs():\n        if getattr(ds_rainfall.variables[\"x\"], \"units\") == \"km\":\n            factor_scale = 1000.0\n\n    geodata[\"x1\"] = xmin * factor_scale\n    geodata[\"y1\"] = ymin * factor_scale\n    geodata[\"x2\"] = xmax * factor_scale\n    geodata[\"y2\"] = ymax * factor_scale\n    geodata[\"xpixelsize\"] = xpixelsize * factor_scale\n    geodata[\"ypixelsize\"] = ypixelsize * factor_scale\n    geodata[\"yorigin\"] = \"upper\"  # TODO(_import_bom_rf3_geodata): check this\n\n    # get the accumulation period\n    valid_time = None\n\n    if \"valid_time\" in ds_rainfall.variables.keys():\n        times = ds_rainfall.variables[\"valid_time\"]\n        calendar = \"standard\"\n        if \"calendar\" in times.ncattrs():\n            calendar = times.calendar\n        valid_time = netCDF4.num2date(times[:], units=times.units, calendar=calendar)\n\n    start_time = None\n    if \"start_time\" in ds_rainfall.variables.keys():\n        times = ds_rainfall.variables[\"start_time\"]\n        calendar = \"standard\"\n        if \"calendar\" in times.ncattrs():\n            calendar = times.calendar\n        start_time = netCDF4.num2date(times[:], units=times.units, calendar=calendar)\n\n    time_step = None\n\n    if start_time is not None:\n        if valid_time is not None:\n            time_step = (valid_time - start_time).seconds // 60\n\n    geodata[\"accutime\"] = time_step\n\n    # get the unit of precipitation\n    if \"units\" in ds_rainfall.variables[\"precipitation\"].ncattrs():\n        units = getattr(ds_rainfall.variables[\"precipitation\"], \"units\")\n        if units in (\"kg m-2\", \"mm\"):\n            geodata[\"unit\"] = \"mm\"\n\n    geodata[\"institution\"] = \"Commonwealth of Australia, Bureau of Meteorology\"\n    ds_rainfall.close()\n\n    return geodata",
  "def import_fmi_geotiff(filename, **kwargs):\n    \"\"\"Import a reflectivity field (dBZ) from an FMI GeoTIFF file.\n\n    Parameters\n    ----------\n\n    filename : str\n        Name of the file to import.\n\n    Returns\n    -------\n\n    out : tuple\n        A three-element tuple containing the precipitation field, the associated\n        quality field and metadata. The quality field is currently set to None.\n    \"\"\"\n    if not GDAL_IMPORTED:\n        raise MissingOptionalDependency(\n            \"gdal package is required to import \"\n            \"FMI's radar reflectivity composite in GeoTIFF format \"\n            \"but it is not installed\"\n        )\n\n    f = gdal.Open(filename, gdalconst.GA_ReadOnly)\n\n    rb = f.GetRasterBand(1)\n    precip = rb.ReadAsArray()\n    mask = precip == 255\n    precip = precip.astype(float) * rb.GetScale() + rb.GetOffset()\n    precip = (precip - 64.0) / 2.0\n    precip[mask] = np.nan\n\n    sr = osr.SpatialReference()\n    pr = f.GetProjection()\n    sr.ImportFromWkt(pr)\n\n    projdef = sr.ExportToProj4()\n\n    gt = f.GetGeoTransform()\n\n    metadata = {}\n\n    metadata[\"projection\"] = projdef\n    metadata[\"x1\"] = gt[0]\n    metadata[\"y1\"] = gt[3] + gt[5] * f.RasterYSize\n    metadata[\"x2\"] = metadata[\"x1\"] + gt[1] * f.RasterXSize\n    metadata[\"y2\"] = gt[3]\n    metadata[\"xpixelsize\"] = abs(gt[1])\n    metadata[\"ypixelsize\"] = abs(gt[5])\n    if gt[5] < 0:\n        metadata[\"yorigin\"] = \"upper\"\n    else:\n        metadata[\"yorigin\"] = \"lower\"\n    metadata[\"institution\"] = \"Finnish Meteorological Institute\"\n    metadata[\"unit\"] = rb.GetUnitType()\n    metadata[\"transform\"] = None\n    metadata[\"accutime\"] = 5.0\n    precip_min = np.nanmin(precip)\n    metadata[\"threshold\"] = np.nanmin(precip[precip > precip_min])\n    metadata[\"zerovalue\"] = precip_min\n\n    return precip, None, metadata",
  "def import_fmi_pgm(filename, **kwargs):\n    \"\"\"Import a 8-bit PGM radar reflectivity composite from the FMI archive.\n\n    Parameters\n    ----------\n\n    filename : str\n        Name of the file to import.\n\n    Other Parameters\n    ----------------\n\n    gzipped : bool\n        If True, the input file is treated as a compressed gzip file.\n\n    Returns\n    -------\n\n    out : tuple\n        A three-element tuple containing the reflectivity composite in dBZ\n        and the associated quality field and metadata. The quality field is\n        currently set to None.\n    \"\"\"\n    if not PYPROJ_IMPORTED:\n        raise MissingOptionalDependency(\n            \"pyproj package is required to import \"\n            \"FMI's radar reflectivity composite \"\n            \"but it is not installed\"\n        )\n\n    gzipped = kwargs.get(\"gzipped\", False)\n\n    pgm_metadata = _import_fmi_pgm_metadata(filename, gzipped=gzipped)\n\n    if gzipped is False:\n        precip = imread(filename)\n    else:\n        precip = imread(gzip.open(filename, \"r\"))\n    geodata = _import_fmi_pgm_geodata(pgm_metadata)\n\n    mask = precip == pgm_metadata[\"missingval\"]\n    precip = precip.astype(float)\n    precip[mask] = np.nan\n    precip = (precip - 64.0) / 2.0\n\n    metadata = geodata\n    metadata[\"institution\"] = \"Finnish Meteorological Institute\"\n    metadata[\"accutime\"] = 5.0\n    metadata[\"unit\"] = \"dBZ\"\n    metadata[\"transform\"] = \"dB\"\n    metadata[\"zerovalue\"] = np.nanmin(precip)\n    if np.any(np.isfinite(precip)):\n        metadata[\"threshold\"] = np.nanmin(precip[precip > np.nanmin(precip)])\n    else:\n        metadata[\"threshold\"] = np.nan\n    metadata[\"zr_a\"] = 223.0\n    metadata[\"zr_b\"] = 1.53\n\n    return precip, None, metadata",
  "def _import_fmi_pgm_geodata(metadata):\n    geodata = {}\n\n    projdef = \"\"\n\n    if metadata[\"type\"][0] != \"stereographic\":\n        raise ValueError(\"unknown projection %s\" % metadata[\"type\"][0])\n    projdef += \"+proj=stere \"\n    projdef += \" +lon_0=\" + metadata[\"centrallongitude\"][0] + \"E\"\n    projdef += \" +lat_0=\" + metadata[\"centrallatitude\"][0] + \"N\"\n    projdef += \" +lat_ts=\" + metadata[\"truelatitude\"][0]\n    # These are hard-coded because the projection definition is missing from the\n    # PGM files.\n    projdef += \" +a=6371288\"\n    projdef += \" +x_0=380886.310\"\n    projdef += \" +y_0=3395677.920\"\n    projdef += \" +no_defs\"\n    #\n    geodata[\"projection\"] = projdef\n\n    ll_lon, ll_lat = [float(v) for v in metadata[\"bottomleft\"]]\n    ur_lon, ur_lat = [float(v) for v in metadata[\"topright\"]]\n\n    pr = pyproj.Proj(projdef)\n    x1, y1 = pr(ll_lon, ll_lat)\n    x2, y2 = pr(ur_lon, ur_lat)\n\n    geodata[\"x1\"] = x1\n    geodata[\"y1\"] = y1\n    geodata[\"x2\"] = x2\n    geodata[\"y2\"] = y2\n\n    geodata[\"xpixelsize\"] = float(metadata[\"metersperpixel_x\"][0])\n    geodata[\"ypixelsize\"] = float(metadata[\"metersperpixel_y\"][0])\n\n    geodata[\"yorigin\"] = \"upper\"\n\n    return geodata",
  "def _import_fmi_pgm_metadata(filename, gzipped=False):\n    metadata = {}\n\n    if not gzipped:\n        f = open(filename, \"rb\")\n    else:\n        f = gzip.open(filename, \"rb\")\n\n    file_line = f.readline()\n    while not file_line.startswith(b\"#\"):\n        file_line = f.readline()\n    while file_line.startswith(b\"#\"):\n        x = file_line.decode()\n        x = x[1:].strip().split(\" \")\n        if len(x) >= 2:\n            k = x[0]\n            v = x[1:]\n            metadata[k] = v\n        else:\n            file_line = f.readline()\n            continue\n        file_line = f.readline()\n    file_line = f.readline().decode()\n    metadata[\"missingval\"] = int(file_line)\n    f.close()\n\n    return metadata",
  "def import_mch_gif(filename, product, unit, accutime):\n    \"\"\"Import a 8-bit gif radar reflectivity composite from the MeteoSwiss\n    archive.\n\n    Parameters\n    ----------\n\n    filename : str\n        Name of the file to import.\n\n    product : {\"AQC\", \"CPC\", \"RZC\", \"AZC\"}\n        The name of the MeteoSwiss QPE product.\\n\n        Currently supported prducts:\n\n        +------+----------------------------+\n        | Name |          Product           |\n        +======+============================+\n        | AQC  |     Acquire                |\n        +------+----------------------------+\n        | CPC  |     CombiPrecip            |\n        +------+----------------------------+\n        | RZC  |     Precip                 |\n        +------+----------------------------+\n        | AZC  |     RZC accumulation       |\n        +------+----------------------------+\n\n    unit : {\"mm/h\", \"mm\", \"dBZ\"}\n        the physical unit of the data\n\n    accutime : float\n        the accumulation time in minutes of the data\n\n    Returns\n    -------\n\n    out : tuple\n        A three-element tuple containing the precipitation field in mm/h imported\n        from a MeteoSwiss gif file and the associated quality field and metadata.\n        The quality field is currently set to None.\n    \"\"\"\n    if not PIL_IMPORTED:\n        raise MissingOptionalDependency(\n            \"PIL package is required to import \"\n            \"radar reflectivity composite from MeteoSwiss\"\n            \"but it is not installed\"\n        )\n\n    geodata = _import_mch_geodata()\n\n    metadata = geodata\n\n    # import gif file\n    B = PIL.Image.open(filename)\n\n    if product.lower() in [\"azc\", \"rzc\", \"precip\"]:\n\n        # convert 8-bit GIF colortable to RGB values\n        Brgb = B.convert(\"RGB\")\n\n        # load lookup table\n        if product.lower() == \"azc\":\n            lut_filename = os.path.join(\n                os.path.dirname(__file__), \"mch_lut_8bit_Metranet_AZC_V104.txt\"\n            )\n        else:\n            lut_filename = os.path.join(\n                os.path.dirname(__file__), \"mch_lut_8bit_Metranet_v103.txt\"\n            )\n        lut = np.genfromtxt(lut_filename, skip_header=1)\n        lut = dict(zip(zip(lut[:, 1], lut[:, 2], lut[:, 3]), lut[:, -1]))\n\n        # apply lookup table conversion\n        precip = np.zeros(len(Brgb.getdata()))\n        for i, dn in enumerate(Brgb.getdata()):\n            precip[i] = lut.get(dn, np.nan)\n\n        # convert to original shape\n        width, height = B.size\n        precip = precip.reshape(height, width)\n\n        # set values outside observational range to NaN,\n        # and values in non-precipitating areas to zero.\n        precip[precip < 0] = 0\n        precip[precip > 9999] = np.nan\n\n    elif product.lower() in [\"aqc\", \"cpc\", \"acquire \", \"combiprecip\"]:\n\n        # convert digital numbers to physical values\n        B = np.array(B, dtype=int)\n\n        # build lookup table [mm/5min]\n        lut = np.zeros(256)\n        a = 316.0\n        b = 1.5\n        for i in range(256):\n            if (i < 2) or (i > 250 and i < 255):\n                lut[i] = 0.0\n            elif i == 255:\n                lut[i] = np.nan\n            else:\n                lut[i] = (10.0 ** ((i - 71.5) / 20.0) / a) ** (1.0 / b)\n\n        # apply lookup table\n        precip = lut[B]\n\n    else:\n        raise ValueError(\"unknown product %s\" % product)\n\n    metadata[\"accutime\"] = accutime\n    metadata[\"unit\"] = unit\n    metadata[\"transform\"] = None\n    metadata[\"zerovalue\"] = np.nanmin(precip)\n    if np.any(precip > np.nanmin(precip)):\n        metadata[\"threshold\"] = np.nanmin(precip[precip > np.nanmin(precip)])\n    else:\n        metadata[\"threshold\"] = np.nan\n    metadata[\"institution\"] = \"MeteoSwiss\"\n    metadata[\"product\"] = product\n    metadata[\"zr_a\"] = 316.0\n    metadata[\"zr_b\"] = 1.5\n\n    return precip, None, metadata",
  "def import_mch_hdf5(filename, **kwargs):\n    \"\"\"Import a precipitation field (and optionally the quality field) from a\n    MeteoSwiss HDF5 file conforming to the ODIM specification.\n\n    Parameters\n    ----------\n\n    filename : str\n        Name of the file to import.\n\n    Other Parameters\n    ----------------\n\n    qty : {'RATE', 'ACRR', 'DBZH'}\n        The quantity to read from the file. The currently supported identitiers\n        are: 'RATE'=instantaneous rain rate (mm/h), 'ACRR'=hourly rainfall\n        accumulation (mm) and 'DBZH'=max-reflectivity (dBZ). The default value\n        is 'RATE'.\n\n    Returns\n    -------\n\n    out : tuple\n        A three-element tuple containing the OPERA product for the requested\n        quantity and the associated quality field and metadata. The quality\n        field is read from the file if it contains a dataset whose quantity\n        identifier is 'QIND'.\n    \"\"\"\n    if not H5PY_IMPORTED:\n        raise MissingOptionalDependency(\n            \"h5py package is required to import \"\n            \"radar reflectivity composites using ODIM HDF5 specification \"\n            \"but it is not installed\"\n        )\n\n    qty = kwargs.get(\"qty\", \"RATE\")\n\n    if qty not in [\"ACRR\", \"DBZH\", \"RATE\"]:\n        raise ValueError(\n            \"unknown quantity %s: the available options are 'ACRR', 'DBZH' and 'RATE'\"\n        )\n\n    f = h5py.File(filename, \"r\")\n\n    precip = None\n    quality = None\n\n    for dsg in f.items():\n        if dsg[0].startswith(\"dataset\"):\n            what_grp_found = False\n            # check if the \"what\" group is in the \"dataset\" group\n            if \"what\" in list(dsg[1].keys()):\n                qty_, gain, offset, nodata, undetect = _read_mch_hdf5_what_group(\n                    dsg[1][\"what\"]\n                )\n                what_grp_found = True\n\n            for dg in dsg[1].items():\n                if dg[0][0:4] == \"data\":\n                    # check if the \"what\" group is in the \"data\" group\n                    if \"what\" in list(dg[1].keys()):\n                        qty_, gain, offset, nodata, undetect = _read_mch_hdf5_what_group(\n                            dg[1][\"what\"]\n                        )\n                    elif not what_grp_found:\n                        raise DataModelError(\n                            \"Non ODIM compilant file: \"\n                            \"no what group found from {} \"\n                            \"or its subgroups\".format(dg[0])\n                        )\n\n                    if qty_.decode() in [qty, \"QIND\"]:\n                        arr = dg[1][\"data\"][...]\n                        mask_n = arr == nodata\n                        mask_u = arr == undetect\n                        mask = np.logical_and(~mask_u, ~mask_n)\n\n                        if qty_.decode() == qty:\n                            precip = np.empty(arr.shape)\n                            precip[mask] = arr[mask] * gain + offset\n                            precip[mask_u] = np.nan\n                            precip[mask_n] = np.nan\n                        elif qty_.decode() == \"QIND\":\n                            quality = np.empty(arr.shape, dtype=float)\n                            quality[mask] = arr[mask]\n                            quality[~mask] = np.nan\n\n    if precip is None:\n        raise IOError(\"requested quantity %s not found\" % qty)\n\n    where = f[\"where\"]\n    proj4str = where.attrs[\"projdef\"].decode()  # is empty ...\n\n    geodata = _import_mch_geodata()\n    metadata = geodata\n\n    # TODO: use those from the hdf5 file instead\n    # xpixelsize = where.attrs[\"xscale\"] * 1000.0\n    # ypixelsize = where.attrs[\"yscale\"] * 1000.0\n    # xsize = where.attrs[\"xsize\"]\n    # ysize = where.attrs[\"ysize\"]\n\n    if qty == \"ACRR\":\n        unit = \"mm\"\n        transform = None\n    elif qty == \"DBZH\":\n        unit = \"dBZ\"\n        transform = \"dB\"\n    else:\n        unit = \"mm/h\"\n        transform = None\n\n    if np.any(np.isfinite(precip)):\n        thr = np.nanmin(precip[precip > np.nanmin(precip)])\n    else:\n        thr = np.nan\n\n    metadata.update(\n        {\n            \"yorigin\": \"upper\",\n            \"institution\": \"MeteoSwiss\",\n            \"accutime\": 5.0,\n            \"unit\": unit,\n            \"transform\": transform,\n            \"zerovalue\": np.nanmin(precip),\n            \"threshold\": thr,\n            \"zr_a\": 316.0,\n            \"zr_b\": 1.5,\n        }\n    )\n\n    f.close()\n\n    return precip, quality, metadata",
  "def import_mch_metranet(filename, product, unit, accutime):\n    \"\"\"Import a 8-bit bin radar reflectivity composite from the MeteoSwiss\n    archive.\n\n    Parameters\n    ----------\n\n    filename : str\n        Name of the file to import.\n\n    product : {\"AQC\", \"CPC\", \"RZC\", \"AZC\"}\n        The name of the MeteoSwiss QPE product.\\n\n        Currently supported prducts:\n\n        +------+----------------------------+\n        | Name |          Product           |\n        +======+============================+\n        | AQC  |     Acquire                |\n        +------+----------------------------+\n        | CPC  |     CombiPrecip            |\n        +------+----------------------------+\n        | RZC  |     Precip                 |\n        +------+----------------------------+\n        | AZC  |     RZC accumulation       |\n        +------+----------------------------+\n\n    unit : {\"mm/h\", \"mm\", \"dBZ\"}\n        the physical unit of the data\n\n    accutime : float\n        the accumulation time in minutes of the data\n\n    Returns\n    -------\n\n    out : tuple\n        A three-element tuple containing the precipitation field in mm/h imported\n        from a MeteoSwiss gif file and the associated quality field and metadata.\n        The quality field is currently set to None.\n    \"\"\"\n    if not METRANET_IMPORTED:\n        raise MissingOptionalDependency(\n            \"metranet package needed for importing MeteoSwiss \"\n            \"radar composites but it is not installed\"\n        )\n\n    ret = metranet.read_file(filename, physic_value=True, verbose=False)\n    precip = ret.data\n\n    geodata = _import_mch_geodata()\n\n    # read metranet\n    metadata = geodata\n    metadata[\"institution\"] = \"MeteoSwiss\"\n    metadata[\"accutime\"] = accutime\n    metadata[\"unit\"] = unit\n    metadata[\"transform\"] = None\n    metadata[\"zerovalue\"] = np.nanmin(precip)\n    if np.isnan(metadata[\"zerovalue\"]):\n        metadata[\"threshold\"] = np.nan\n    else:\n        metadata[\"threshold\"] = np.nanmin(precip[precip > metadata[\"zerovalue\"]])\n    metadata[\"zr_a\"] = 316.0\n    metadata[\"zr_b\"] = 1.5\n\n    return precip, None, metadata",
  "def _import_mch_geodata():\n    \"\"\"Swiss radar domain CCS4\n    These are all hard-coded because the georeferencing is missing from the gif files.\n    \"\"\"\n\n    geodata = {}\n\n    # LV03 Swiss projection definition in Proj4\n    projdef = \"\"\n    projdef += \"+proj=somerc \"\n    projdef += \" +lon_0=7.43958333333333\"\n    projdef += \" +lat_0=46.9524055555556\"\n    projdef += \" +k_0=1\"\n    projdef += \" +x_0=600000\"\n    projdef += \" +y_0=200000\"\n    projdef += \" +ellps=bessel\"\n    projdef += \" +towgs84=674.374,15.056,405.346,0,0,0,0\"\n    projdef += \" +units=m\"\n    projdef += \" +no_defs\"\n    geodata[\"projection\"] = projdef\n\n    geodata[\"x1\"] = 255000.0\n    geodata[\"y1\"] = -160000.0\n    geodata[\"x2\"] = 965000.0\n    geodata[\"y2\"] = 480000.0\n\n    geodata[\"xpixelsize\"] = 1000.0\n    geodata[\"ypixelsize\"] = 1000.0\n\n    geodata[\"yorigin\"] = \"upper\"\n\n    return geodata",
  "def import_opera_hdf5(filename, **kwargs):\n    \"\"\"Import a precipitation field (and optionally the quality field) from an\n    OPERA HDF5 file conforming to the ODIM specification.\n\n    Parameters\n    ----------\n\n    filename : str\n        Name of the file to import.\n\n    Other Parameters\n    ----------------\n\n    qty : {'RATE', 'ACRR', 'DBZH'}\n        The quantity to read from the file. The currently supported identitiers\n        are: 'RATE'=instantaneous rain rate (mm/h), 'ACRR'=hourly rainfall\n        accumulation (mm) and 'DBZH'=max-reflectivity (dBZ). The default value\n        is 'RATE'.\n\n    Returns\n    -------\n\n    out : tuple\n        A three-element tuple containing the OPERA product for the requested\n        quantity and the associated quality field and metadata. The quality\n        field is read from the file if it contains a dataset whose quantity\n        identifier is 'QIND'.\n    \"\"\"\n    if not H5PY_IMPORTED:\n        raise MissingOptionalDependency(\n            \"h5py package is required to import \"\n            \"radar reflectivity composites using ODIM HDF5 specification \"\n            \"but it is not installed\"\n        )\n\n    qty = kwargs.get(\"qty\", \"RATE\")\n\n    if qty not in [\"ACRR\", \"DBZH\", \"RATE\"]:\n        raise ValueError(\n            \"unknown quantity %s: the available options are 'ACRR', 'DBZH' and 'RATE'\"\n        )\n\n    f = h5py.File(filename, \"r\")\n\n    precip = None\n    quality = None\n\n    for dsg in f.items():\n        if dsg[0].startswith(\"dataset\"):\n            what_grp_found = False\n            # check if the \"what\" group is in the \"dataset\" group\n            if \"what\" in list(dsg[1].keys()):\n                qty_, gain, offset, nodata, undetect = _read_opera_hdf5_what_group(\n                    dsg[1][\"what\"]\n                )\n                what_grp_found = True\n\n            for dg in dsg[1].items():\n                if dg[0][0:4] == \"data\":\n                    # check if the \"what\" group is in the \"data\" group\n                    if \"what\" in list(dg[1].keys()):\n                        qty_, gain, offset, nodata, undetect = _read_opera_hdf5_what_group(\n                            dg[1][\"what\"]\n                        )\n                    elif not what_grp_found:\n                        raise DataModelError(\n                            \"Non ODIM compilant file: \"\n                            \"no what group found from {} \"\n                            \"or its subgroups\".format(dg[0])\n                        )\n\n                    if qty_.decode() in [qty, \"QIND\"]:\n                        arr = dg[1][\"data\"][...]\n                        mask_n = arr == nodata\n                        mask_u = arr == undetect\n                        mask = np.logical_and(~mask_u, ~mask_n)\n\n                        if qty_.decode() == qty:\n                            precip = np.empty(arr.shape)\n                            precip[mask] = arr[mask] * gain + offset\n                            precip[mask_u] = 0.0\n                            precip[mask_n] = np.nan\n                        elif qty_.decode() == \"QIND\":\n                            quality = np.empty(arr.shape, dtype=float)\n                            quality[mask] = arr[mask]\n                            quality[~mask] = np.nan\n\n    if precip is None:\n        raise IOError(\"requested quantity %s not found\" % qty)\n\n    where = f[\"where\"]\n    proj4str = where.attrs[\"projdef\"].decode()\n    pr = pyproj.Proj(proj4str)\n\n    ll_lat = where.attrs[\"LL_lat\"]\n    ll_lon = where.attrs[\"LL_lon\"]\n    ur_lat = where.attrs[\"UR_lat\"]\n    ur_lon = where.attrs[\"UR_lon\"]\n    if (\n            \"LR_lat\" in where.attrs.keys()\n            and \"LR_lon\" in where.attrs.keys()\n            and \"UL_lat\" in where.attrs.keys()\n            and \"UL_lon\" in where.attrs.keys()\n    ):\n        lr_lat = float(where.attrs[\"LR_lat\"])\n        lr_lon = float(where.attrs[\"LR_lon\"])\n        ul_lat = float(where.attrs[\"UL_lat\"])\n        ul_lon = float(where.attrs[\"UL_lon\"])\n        full_cornerpts = True\n    else:\n        full_cornerpts = False\n\n    ll_x, ll_y = pr(ll_lon, ll_lat)\n    ur_x, ur_y = pr(ur_lon, ur_lat)\n\n    if full_cornerpts:\n        lr_x, lr_y = pr(lr_lon, lr_lat)\n        ul_x, ul_y = pr(ul_lon, ul_lat)\n        x1 = min(ll_x, ul_x)\n        y1 = min(ll_y, lr_y)\n        x2 = max(lr_x, ur_x)\n        y2 = max(ul_y, ur_y)\n    else:\n        x1 = ll_x\n        y1 = ll_y\n        x2 = ur_x\n        y2 = ur_y\n\n    if \"xscale\" in where.attrs.keys() and \"yscale\" in where.attrs.keys():\n        xpixelsize = where.attrs[\"xscale\"]\n        ypixelsize = where.attrs[\"yscale\"]\n    else:\n        xpixelsize = None\n        ypixelsize = None\n\n    if qty == \"ACRR\":\n        unit = \"mm\"\n        transform = None\n    elif qty == \"DBZH\":\n        unit = \"dBZ\"\n        transform = \"dB\"\n    else:\n        unit = \"mm/h\"\n        transform = None\n\n    if np.any(np.isfinite(precip)):\n        thr = np.nanmin(precip[precip > np.nanmin(precip)])\n    else:\n        thr = np.nan\n\n    metadata = {\n        \"projection\": proj4str,\n        \"ll_lon\": ll_lon,\n        \"ll_lat\": ll_lat,\n        \"ur_lon\": ur_lon,\n        \"ur_lat\": ur_lat,\n        \"x1\": x1,\n        \"y1\": y1,\n        \"x2\": x2,\n        \"y2\": y2,\n        \"xpixelsize\": xpixelsize,\n        \"ypixelsize\": ypixelsize,\n        \"yorigin\": \"upper\",\n        \"institution\": \"Odyssey datacentre\",\n        \"accutime\": 15.0,\n        \"unit\": unit,\n        \"transform\": transform,\n        \"zerovalue\": np.nanmin(precip),\n        \"threshold\": thr,\n    }\n\n    f.close()\n\n    return precip, quality, metadata",
  "def _read_mch_hdf5_what_group(whatgrp):\n    qty = whatgrp.attrs[\"quantity\"] if \"quantity\" in whatgrp.attrs.keys() else \"RATE\"\n    gain = whatgrp.attrs[\"gain\"] if \"gain\" in whatgrp.attrs.keys() else 1.0\n    offset = whatgrp.attrs[\"offset\"] if \"offset\" in whatgrp.attrs.keys() else 0.0\n    nodata = whatgrp.attrs[\"nodata\"] if \"nodata\" in whatgrp.attrs.keys() else 0\n    undetect = whatgrp.attrs[\"undetect\"] if \"undetect\" in whatgrp.attrs.keys() else -1.0\n\n    return qty, gain, offset, nodata, undetect",
  "def _read_opera_hdf5_what_group(whatgrp):\n    qty = whatgrp.attrs[\"quantity\"]\n    gain = whatgrp.attrs[\"gain\"] if \"gain\" in whatgrp.attrs.keys() else 1.0\n    offset = whatgrp.attrs[\"offset\"] if \"offset\" in whatgrp.attrs.keys() else 0.0\n    nodata = whatgrp.attrs[\"nodata\"] if \"nodata\" in whatgrp.attrs.keys() else np.nan\n    undetect = whatgrp.attrs[\"undetect\"] if \"undetect\" in whatgrp.attrs.keys() else 0.0\n\n    return qty, gain, offset, nodata, undetect",
  "def import_knmi_hdf5(filename, **kwargs):\n    \"\"\"Import a precipitation or reflectivity field (and optionally the quality\n    field) from a HDF5 file conforming to the KNMI Data Centre specification.\n\n    Parameters\n    ----------\n\n    filename : str\n        Name of the file to import.\n\n    Other Parameters\n    ----------------\n\n    qty : {'ACRR', 'DBZH'}\n        The quantity to read from the file. The currently supported identifiers\n        are: 'ACRR'=hourly rainfall accumulation (mm) and 'DBZH'=max-reflectivity\n        (dBZ). The default value is 'ACRR'.\n\n    accutime : float\n        The accumulation time of the dataset in minutes. A 5 min accumulation\n        is used as default, but hourly, daily and monthly accumulations\n        are also available.\n\n    pixelsize: float\n        The pixel size of a raster cell in meters. The default value for the KNMI\n        datasets is 1000 m grid cell size, but datasets with 2400 m pixel size\n        are also available.\n\n    Returns\n    -------\n\n    out : tuple\n        A three-element tuple containing precipitation accumulation [mm] /\n        reflectivity [dBZ] of the KNMI product, the associated quality field\n        and metadata. The quality field is currently set to None.\n\n    Notes\n    -----\n\n    Every KNMI data type has a slightly different naming convention. The\n    standard setup is based on the accumulated rainfall product on 1 km2 spatial\n    and 5 min temporal resolution.\n    See https://data.knmi.nl/datasets?q=radar for a list of all available KNMI\n    radar data.\n    \"\"\"\n\n    # TODO: Add quality field.\n\n    if not H5PY_IMPORTED:\n        raise MissingOptionalDependency(\n            \"h5py package is required to import \"\n            \"KNMI's radar datasets \"\n            \"but it is not installed\"\n        )\n\n    ###\n    # Options for kwargs.get\n    ###\n\n    # The unit in the 2D fields: either hourly rainfall accumulation (ACRR) or\n    # reflectivity (DBZH)\n    qty = kwargs.get(\"qty\", \"ACRR\")\n\n    if qty not in [\"ACRR\", \"DBZH\"]:\n        raise ValueError(\n            \"unknown quantity %s: the available options are 'ACRR' and 'DBZH' \"\n        )\n\n    # The time step. Generally, the 5 min data is used, but also hourly, daily\n    # and monthly accumulations are present.\n    accutime = kwargs.get(\"accutime\", 5.0)\n    # The pixel size. Recommended is to use KNMI datasets with 1 km grid cell size.\n    # 1.0 or 2.4 km datasets are available - give pixelsize in meters\n    pixelsize = kwargs.get(\"pixelsize\", 1000.0)\n\n    ####\n    # Precipitation fields\n    ####\n\n    f = h5py.File(filename, \"r\")\n    dset = f[\"image1\"][\"image_data\"]\n    precip_intermediate = np.copy(dset)  # copy the content\n\n    # In case R is a rainfall accumulation (ACRR), R is divided by 100.0,\n    # because the data is saved as hundreds of mm (so, as integers). 65535 is\n    # the no data value. The precision of the data is two decimals (0.01 mm).\n    if qty == \"ACRR\":\n        precip = np.where(precip_intermediate == 65535,\n                          np.NaN,\n                          precip_intermediate / 100.0)\n\n    # In case reflectivities are imported, the no data value is 255. Values are\n    # saved as integers. The reflectivities are not directly saved in dBZ, but\n    # as: dBZ = 0.5 * pixel_value - 32.0 (this used to be 31.5).\n    if qty == \"DBZH\":\n        precip = np.where(precip_intermediate == 255,\n                          np.NaN,\n                          precip_intermediate * 0.5 - 32.0)\n\n    if precip is None:\n        raise IOError(\"requested quantity not found\")\n\n    # TODO: Check if the reflectivity conversion equation is still up to date (unfortunately not well documented)\n\n    ####\n    # Meta data\n    ####\n\n    metadata = {}\n\n    if qty == \"ACRR\":\n        unit = \"mm\"\n        transform = None\n    elif qty == \"DBZH\":\n        unit = \"dBZ\"\n        transform = \"dB\"\n\n    # The 'where' group of mch- and Opera-data, is called 'geographic' in the\n    # KNMI data.\n    geographic = f[\"geographic\"]\n    proj4str = geographic[\"map_projection\"].attrs[\"projection_proj4_params\"].decode()\n    pr = pyproj.Proj(proj4str)\n    metadata[\"projection\"] = proj4str\n\n    # Get coordinates\n    latlon_corners = geographic.attrs[\"geo_product_corners\"]\n    ll_lat = latlon_corners[1]\n    ll_lon = latlon_corners[0]\n    ur_lat = latlon_corners[5]\n    ur_lon = latlon_corners[4]\n    lr_lat = latlon_corners[7]\n    lr_lon = latlon_corners[6]\n    ul_lat = latlon_corners[3]\n    ul_lon = latlon_corners[2]\n\n    ll_x, ll_y = pr(ll_lon, ll_lat)\n    ur_x, ur_y = pr(ur_lon, ur_lat)\n    lr_x, lr_y = pr(lr_lon, lr_lat)\n    ul_x, ul_y = pr(ul_lon, ul_lat)\n    x1 = min(ll_x, ul_x)\n    y2 = min(ll_y, lr_y)\n    x2 = max(lr_x, ur_x)\n    y1 = max(ul_y, ur_y)\n\n    # Fill in the metadata\n    metadata[\"x1\"] = x1 * 1000.0\n    metadata[\"y1\"] = y1 * 1000.0\n    metadata[\"x2\"] = x2 * 1000.0\n    metadata[\"y2\"] = y2 * 1000.0\n    metadata[\"xpixelsize\"] = pixelsize\n    metadata[\"ypixelsize\"] = pixelsize\n    metadata[\"yorigin\"] = \"upper\"\n    metadata[\"institution\"] = \"KNMI - Royal Netherlands Meteorological Institute\"\n    metadata[\"accutime\"] = accutime\n    metadata[\"unit\"] = unit\n    metadata[\"transform\"] = transform\n    metadata[\"zerovalue\"] = 0.0\n    metadata[\"threshold\"] = np.nanmin(precip[precip > np.nanmin(precip)])\n    metadata[\"zr_a\"] = 200.0\n    metadata[\"zr_b\"] = 1.6\n\n    f.close()\n\n    return precip, None, metadata",
  "def find_by_date(date, root_path, path_fmt, fn_pattern, fn_ext, timestep,\n                 num_prev_files=0, num_next_files=0):\n    \"\"\"List input files whose timestamp matches the given date.\n\n    Parameters\n    ----------\n    date : datetime.datetime\n        The given date.\n    root_path : str\n        The root path to search the input files.\n    path_fmt : str\n        Path format. It may consist of directory names separated by '/',\n        date/time specifiers beginning with '%' (e.g. %Y/%m/%d) and wildcards\n        (?) that match any single character.\n    fn_pattern : str\n        The name pattern of the input files without extension. The pattern can\n        contain time specifiers (e.g. %H, %M and %S).\n    fn_ext : str\n        Extension of the input files.\n    timestep : float\n        Time step between consecutive input files (minutes).\n    num_prev_files : int\n        Optional, number of previous files to find before the given timestamp.\n    num_next_files : int\n        Optional, number of future files to find after the given timestamp.\n\n    Returns\n    -------\n    out : tuple\n        If num_prev_files=0 and num_next_files=0, return a pair containing the\n        found file name and the corresponding timestamp as a datetime.datetime\n        object. Otherwise, return a tuple of two lists, the first one for the\n        file names and the second one for the corresponding timestemps. The lists\n        are sorted in ascending order with respect to timestamp. A None value is\n        assigned if a file name corresponding to a given timestamp is not found.\n\n    \"\"\"\n    filenames = []\n    timestamps = []\n\n    for i in range(num_prev_files + num_next_files + 1):\n        curdate = date + timedelta(minutes=num_next_files * timestep) - timedelta(minutes=i * timestep)\n        fn = _find_matching_filename(curdate, root_path, path_fmt, fn_pattern, fn_ext)\n        filenames.append(fn)\n\n        timestamps.append(curdate)\n\n    if all(filename is None for filename in filenames):\n        raise IOError(\"no input data found in %s\" % root_path)\n\n    if (num_prev_files + num_next_files) > 0:\n        return filenames[::-1], timestamps[::-1]\n    else:\n        return filenames, timestamps",
  "def _find_matching_filename(date, root_path, path_fmt, fn_pattern, fn_ext):\n    path = _generate_path(date, root_path, path_fmt)\n    fn = None\n\n    if os.path.exists(path):\n        fn = datetime.strftime(date, fn_pattern) + '.' + fn_ext\n\n        # test for wildcars\n        if '?' in fn:\n            filenames = os.listdir(path)\n            if len(filenames) > 0:\n                for filename in filenames:\n                    if fnmatch.fnmatch(filename, fn):\n                        fn = filename\n                        break\n\n        fn = os.path.join(path, fn)\n\n        if os.path.exists(fn):\n            fn = fn\n        else:\n            print('file not found: %s' % fn)\n            fn = None\n    else:\n        print('path', path, 'not found.')\n\n    return fn",
  "def _generate_path(date, root_path, path_format):\n    \"\"\"Generate file path.\"\"\"\n    if not isinstance(date, datetime):\n        raise TypeError(\"The input 'date' argument must be a datetime object\")\n\n    if path_format != \"\":\n        sub_path = date.strftime(path_format)\n        return os.path.join(root_path, sub_path)\n    else:\n        return root_path",
  "def read_timeseries(inputfns, importer, **kwargs):\n    \"\"\"Read a time series of input files using the methods implemented in the \n    :py:mod:`pysteps.io.importers` module and stack them into a 3d array of \n    shape (num_timesteps, height, width).\n\n    Parameters\n    ----------\n    inputfns : tuple\n        Input files returned by a function implemented in the \n        :py:mod:`pysteps.io.archive` module.\n    importer : function\n        A function implemented in the :py:mod:`pysteps.io.importers` module.\n    kwargs : dict\n        Optional keyword arguments for the importer.\n\n    Returns\n    -------\n    out : tuple\n        A three-element tuple containing the read data and quality rasters and\n        associated metadata. If an input file name is None, the corresponding\n        precipitation and quality fields are filled with nan values. If all\n        input file names are None or if the length of the file name list is\n        zero, a three-element tuple containing None values is returned.\n\n    \"\"\"\n\n    # check for missing data\n    precip_ref = None\n    if all(ifn is None for ifn in inputfns):\n        return None, None, None\n    else:\n        if len(inputfns[0]) == 0:\n            return None, None, None\n        for ifn in inputfns[0]:\n            if ifn is not None:\n                precip_ref, quality_ref, metadata = importer(ifn, **kwargs)\n                break\n\n    if precip_ref is None:\n        return None, None, None\n\n    precip = []\n    quality = []\n    timestamps = []\n    for i, ifn in enumerate(inputfns[0]):\n        if ifn is not None:\n            precip_, quality_, _ = importer(ifn, **kwargs)\n            precip.append(precip_)\n            quality.append(quality_)\n            timestamps.append(inputfns[1][i])\n        else:\n            precip.append(precip_ref * np.nan)\n            if quality_ref is not None:\n                quality.append(quality_ref * np.nan)\n            else:\n                quality.append(None)\n            timestamps.append(inputfns[1][i])\n\n    # Replace this with stack?\n    precip = np.concatenate([precip_[None, :, :] for precip_ in precip])\n    # TODO: Q should be organized as R, but this is not trivial as Q_ can be also None or a scalar\n    metadata[\"timestamps\"] = np.array(timestamps)\n\n    return precip, quality, metadata",
  "def get_method(name, method_type):\n    \"\"\"Return a callable function for the method corresponding to the given \n    name.\n\n    Parameters\n    ----------\n    name : str\n        Name of the method. The available options are:\\n\n\n        Importers:\n\n        .. tabularcolumns:: |p{2cm}|L|\n\n        +--------------+-------------------------------------------------------+\n        |     Name     |              Description                              |\n        +==============+=======================================================+\n        | bom_rf3      |  NefCDF files used in the Boreau of Meterorology      |\n        |              |  archive containing precipitation intensity           |\n        |              |  composites.                                          |\n        +--------------+-------------------------------------------------------+\n        | fmi_geotiff  |  GeoTIFF files used in the Finnish Meteorological     |\n        |              |  Institute (FMI) archive, containing reflectivity     |\n        |              |  composites (dBZ).                                    |\n        +--------------+-------------------------------------------------------+\n        | fmi_pgm      |  PGM files used in the Finnish Meteorological         |\n        |              |  Institute (FMI) archive, containing reflectivity     |\n        |              |  composites (dBZ).                                    |\n        +--------------+-------------------------------------------------------+\n        | mch_gif      | GIF files in the MeteoSwiss (MCH) archive containing  |\n        |              | precipitation composites.                             |\n        +--------------+-------------------------------------------------------+\n        | mch_hdf5     | HDF5 file format used by MeteoSiss (MCH).             |\n        +--------------+-------------------------------------------------------+\n        | mch_metranet | metranet files in the MeteoSwiss (MCH) archive        |\n        |              | containing precipitation composites.                  |\n        +--------------+-------------------------------------------------------+\n        | opera_hdf5   | ODIM HDF5 file format used by Eumetnet/OPERA.         |\n        +--------------+-------------------------------------------------------+\n        | knmi_hdf5    |  HDF5 file format used by KNMI.                       |\n        +--------------+-------------------------------------------------------+\n\n        Exporters:\n        \n        .. tabularcolumns:: |p{2cm}|L|\n\n        +-------------+--------------------------------------------------------+\n        |     Name    |              Description                               |\n        +=============+========================================================+\n        | kineros     | KINEROS2 Rainfall file as specified in                 |\n        |             | https://www.tucson.ars.ag.gov/kineros/.                |\n        |             | Grid points are treated as individual rain gauges.     |\n        |             | A separate file is produced for each ensemble member.  |\n        +-------------+--------------------------------------------------------+\n        | netcdf      | NetCDF files conforming to the CF 1.7 specification.   |\n        +-------------+--------------------------------------------------------+\n\n    method_type : str\n        Type of the method. The available options are 'importer' and 'exporter'.\n\n    \"\"\"\n\n    if isinstance(method_type, str):\n        method_type = method_type.lower()\n    else:\n        raise TypeError(\"Only strings supported for for the method_type\"\n                        + \" argument\\n\"\n                        + \"The available types are: 'importer' and 'exporter'\"\n                        ) from None\n\n    if isinstance(name, str):\n        name = name.lower()\n    else:\n        raise TypeError(\"Only strings supported for the method's names.\\n\"\n                        + \"Available importers names:\"\n                        + str(list(_importer_methods.keys()))\n                        + \"\\nAvailable exporters names:\"\n                        + str(list(_exporter_methods.keys()))) from None\n\n    if method_type == \"importer\":\n        methods_dict = _importer_methods\n    elif method_type == \"exporter\":\n        methods_dict = _exporter_methods\n    else:\n        raise ValueError(\"Unknown method type {}\\n\".format(name)\n                         + \"The available types are: 'importer' and 'exporter'\"\n                         ) from None\n\n    try:\n        return methods_dict[name]\n    except KeyError:\n        raise ValueError(\"Unknown {} method {}\\n\".format(method_type, name)\n                         + \"The available methods are:\"\n                         + str(list(methods_dict.keys()))) from None",
  "def filter_uniform(shape, n):\n    \"\"\"A dummy filter with one frequency band covering the whole domain. The\n    weights are set to one.\n\n    Parameters\n    ----------\n    shape : int or tuple\n        The dimensions (height, width) of the input field. If shape is an int,\n        the domain is assumed to have square shape.\n    n : int\n        Not used. Needed for compatibility with the filter interface.\n\n    \"\"\"\n    del n  # Unused\n\n    result = {}\n\n    try:\n        height, width = shape\n    except TypeError:\n        height, width = (shape, shape)\n\n    r_max = int(max(width, height) / 2) + 1\n\n    result[\"weights_1d\"] = np.ones((1, r_max))\n    result[\"weights_2d\"] = np.ones((1, height, int(width / 2) + 1))\n    result[\"central_freqs\"] = None\n    result[\"central_wavenumbers\"] = None\n\n    return result",
  "def filter_gaussian(shape, n, l_0=3, gauss_scale=0.5, gauss_scale_0=0.5, d=1.0,\n                    normalize=True):\n    \"\"\"Implements a set of Gaussian bandpass filters in logarithmic frequency\n    scale.\n\n    Parameters\n    ----------\n    shape : int or tuple\n        The dimensions (height, width) of the input field. If shape is an int,\n        the domain is assumed to have square shape.\n    n : int\n        The number of frequency bands to use. Must be greater than 2.\n    l_0 : int\n        Central frequency of the second band (the first band is always centered\n        at zero).\n    gauss_scale : float\n        Optional scaling prameter. Proportional to the standard deviation of\n        the Gaussian weight functions.\n    gauss_scale_0 : float\n        Optional scaling parameter for the Gaussian function corresponding to\n        the first frequency band.\n    d : scalar, optional\n        Sample spacing (inverse of the sampling rate). Defaults to 1.\n    normalize : bool\n        If True, normalize the weights so that for any given wavenumber\n        they sum to one.\n\n    Returns\n    -------\n    out : dict\n        A dictionary containing the bandpass filters corresponding to the\n        specified frequency bands.\n\n    References\n    ----------\n    :cite:`PCH2018`\n\n    \"\"\"\n    if n < 3:\n        raise ValueError(\"n must be greater than 2\")\n\n    try:\n        height, width = shape\n    except TypeError:\n        height, width = (shape, shape)\n\n    rx = np.s_[:int(width / 2) + 1]\n\n    if (height % 2) == 1:\n        ry = np.s_[-int(height / 2):int(height / 2) + 1]\n    else:\n        ry = np.s_[-int(height / 2):int(height / 2)]\n\n    y_grid, x_grid = np.ogrid[ry, rx]\n    dy = int(height / 2) if height % 2 == 0 else int(height / 2) + 1\n\n    r_2d = np.roll(np.sqrt(x_grid * x_grid + y_grid * y_grid), dy, axis=0)\n\n    max_length = max(width, height)\n\n    r_max = int(max_length / 2) + 1\n    r_1d = np.arange(r_max)\n\n    wfs, central_wavenumbers = _gaussweights_1d(max_length, n, l_0=l_0,\n                                                gauss_scale=gauss_scale,\n                                                gauss_scale_0=gauss_scale_0)\n\n    weights_1d = np.empty((n, r_max))\n    weights_2d = np.empty((n, height, int(width / 2) + 1))\n\n    for i, wf in enumerate(wfs):\n        weights_1d[i, :] = wf(r_1d)\n        weights_2d[i, :, :] = wf(r_2d)\n\n    if normalize:\n        weights_1d_sum = np.sum(weights_1d, axis=0)\n        weights_2d_sum = np.sum(weights_2d, axis=0)\n        for k in range(weights_2d.shape[0]):\n            weights_1d[k, :] /= weights_1d_sum\n            weights_2d[k, :, :] /= weights_2d_sum\n\n    result = {\"weights_1d\": weights_1d, \"weights_2d\": weights_2d}\n\n    central_wavenumbers = np.array(central_wavenumbers)\n    result[\"central_wavenumbers\"] = central_wavenumbers\n\n    # Compute frequencies\n    central_freqs = 1.0 * central_wavenumbers / max_length\n    central_freqs[0] = 1.0 / max_length\n    central_freqs[-1] = 0.5  # Nyquist freq\n    central_freqs = 1.0 * d * central_freqs\n    result[\"central_freqs\"] = central_freqs\n\n    return result",
  "def _gaussweights_1d(l, n, l_0=3, gauss_scale=0.5, gauss_scale_0=0.5):\n    e = pow(0.5 * l / l_0, 1.0 / (n - 2))\n    r = [(l_0 * pow(e, k - 1), l_0 * pow(e, k)) for k in range(1, n - 1)]\n\n    def log_e(x):\n        if len(np.shape(x)) > 0:\n            res = np.empty(x.shape)\n            res[x == 0] = 0.0\n            res[x > 0] = np.log(x[x > 0]) / np.log(e)\n        else:\n            if x == 0.0:\n                res = 0.0\n            else:\n                res = np.log(x) / np.log(e)\n\n        return res\n\n    class GaussFunc:\n\n        def __init__(self, c, s):\n            self.c = c\n            self.s = s\n\n        def __call__(self, x):\n            x = log_e(x) - self.c\n            return np.exp(-x ** 2.0 / (2.0 * self.s ** 2.0))\n\n    weight_funcs = []\n    central_wavenumbers = [0.0]\n\n    weight_funcs.append(GaussFunc(0.0, gauss_scale_0))\n\n    for i, ri in enumerate(r):\n        rc = log_e(ri[0])\n        weight_funcs.append(GaussFunc(rc, gauss_scale))\n        central_wavenumbers.append(ri[0])\n\n    gf = GaussFunc(log_e(l / 2), gauss_scale)\n\n    def g(x):\n        res = np.ones(x.shape)\n        mask = x <= l / 2\n        res[mask] = gf(x[mask])\n\n        return res\n\n    weight_funcs.append(g)\n    central_wavenumbers.append(l / 2)\n\n    return weight_funcs, central_wavenumbers",
  "def log_e(x):\n        if len(np.shape(x)) > 0:\n            res = np.empty(x.shape)\n            res[x == 0] = 0.0\n            res[x > 0] = np.log(x[x > 0]) / np.log(e)\n        else:\n            if x == 0.0:\n                res = 0.0\n            else:\n                res = np.log(x) / np.log(e)\n\n        return res",
  "class GaussFunc:\n\n        def __init__(self, c, s):\n            self.c = c\n            self.s = s\n\n        def __call__(self, x):\n            x = log_e(x) - self.c\n            return np.exp(-x ** 2.0 / (2.0 * self.s ** 2.0))",
  "def g(x):\n        res = np.ones(x.shape)\n        mask = x <= l / 2\n        res[mask] = gf(x[mask])\n\n        return res",
  "def __init__(self, c, s):\n            self.c = c\n            self.s = s",
  "def __call__(self, x):\n            x = log_e(x) - self.c\n            return np.exp(-x ** 2.0 / (2.0 * self.s ** 2.0))",
  "def decomposition_fft(field, bp_filter, **kwargs):\n    \"\"\"Decompose a 2d input field into multiple spatial scales by using the Fast\n    Fourier Transform (FFT) and a bandpass filter.\n\n    Parameters\n    ----------\n    field : array_like\n        Two-dimensional array containing the input field. All values are\n        required to be finite.\n    bp_filter : dict\n        A filter returned by a method implemented in\n        :py:mod:`pysteps.cascade.bandpass_filters`.\n\n    Other Parameters\n    ----------------\n    fft_method : str or tuple\n        A string or a (function,kwargs) tuple defining the FFT method to use\n        (see :py:func:`pysteps.utils.interface.get_method`).\n        Defaults to \"numpy\".\n    MASK : array_like\n        Optional mask to use for computing the statistics for the cascade\n        levels. Pixels with MASK==False are excluded from the computations.\n\n    Returns\n    -------\n    out : ndarray\n        A dictionary described in the module documentation.\n        The number of cascade levels is determined from the filter\n        (see :py:mod:`pysteps.cascade.bandpass_filters`).\n\n    \"\"\"\n    fft = kwargs.get(\"fft_method\", \"numpy\")\n    if type(fft) == str:\n        fft = utils.get_method(fft, shape=field.shape)\n\n    mask = kwargs.get(\"MASK\", None)\n\n    if len(field.shape) != 2:\n        raise ValueError(\"The input is not two-dimensional array\")\n\n    if mask is not None and mask.shape != field.shape:\n        raise ValueError(\"Dimension mismatch between X and MASK:\"\n                         + \"X.shape=\" + str(field.shape)\n                         + \",mask.shape\" + str(mask.shape))\n\n    if field.shape[0] != bp_filter[\"weights_2d\"].shape[1]:\n        raise ValueError(\n            \"dimension mismatch between X and filter: \"\n            + \"X.shape[0]=%d , \" % field.shape[0]\n            + \"filter['weights_2d'].shape[1]\"\n              \"=%d\" % bp_filter[\"weights_2d\"].shape[1])\n\n    if int(field.shape[1] / 2) + 1 != bp_filter[\"weights_2d\"].shape[2]:\n        raise ValueError(\n            \"Dimension mismatch between X and filter: \"\n            \"int(X.shape[1]/2)+1=%d , \" % (int(field.shape[1] / 2) + 1)\n            + \"filter['weights_2d'].shape[2]\"\n              \"=%d\" % bp_filter[\"weights_2d\"].shape[2])\n\n    if np.any(~np.isfinite(field)):\n        raise ValueError(\"X contains non-finite values\")\n\n    result = {}\n    means = []\n    stds = []\n\n    field_decomp = []\n\n    for k in range(len(bp_filter[\"weights_1d\"])):\n\n        _decomp_field = fft.irfft2(fft.rfft2(field) * bp_filter[\"weights_2d\"][k, :, :])\n\n        field_decomp.append(_decomp_field)\n\n        if mask is not None:\n            _decomp_field = _decomp_field[mask]\n        means.append(np.mean(_decomp_field))\n        stds.append(np.std(_decomp_field))\n\n    result[\"cascade_levels\"] = np.stack(field_decomp)\n    result[\"means\"] = means\n    result[\"stds\"] = stds\n\n    return result",
  "def get_method(name):\n    \"\"\"\n    Return a callable function for the bandpass filter or decomposition method\n    corresponding to the given name.\n\n    Filter methods:\n\n    +-------------------+------------------------------------------------------+\n    |     Name          |              Description                             |\n    +===================+======================================================+\n    |  gaussian         | implementation of a bandpass filter using Gaussian   |\n    |                   | weights                                              |\n    +-------------------+------------------------------------------------------+\n    |  uniform          | implementation of a filter where all weights are set |\n    |                   | to one                                               |\n    +-------------------+------------------------------------------------------+\n\n    Decomposition methods:\n\n    +-------------------+------------------------------------------------------+\n    |     Name          |              Description                             |\n    +===================+======================================================+\n    |  fft              | decomposition based on Fast Fourier Transform (FFT)  |\n    |                   | and a bandpass filter                                |\n    +-------------------+------------------------------------------------------+\n\n    \"\"\"\n\n    if isinstance(name, str):\n        name = name.lower()\n    else:\n        raise TypeError(\"Only strings supported for the method's names.\\n\"\n                        + \"Available names:\"\n                        + str(list(_cascade_methods.keys()))) from None\n    try:\n        return _cascade_methods[name]\n    except KeyError:\n        raise ValueError(\"Unknown method {}\\n\".format(name)\n                         + \"The available methods are:\"\n                         + str(list(_cascade_methods.keys()))) from None",
  "def rapsd(Z, fft_method=None, return_freq=False, d=1.0, **fft_kwargs):\n    \"\"\"Compute radially averaged power spectral density (RAPSD) from the given\n    2D input field.\n\n    Parameters\n    ----------\n    Z : array_like\n      A 2d array of shape (M,N) containing the input field.\n    fft_method : object\n      A module or object implementing the same methods as numpy.fft and\n      scipy.fftpack. If set to None, Z is assumed to represent the shifted\n      discrete Fourier transform of the input field, where the origin is at\n      the center of the array\n      (see numpy.fft.fftshift or scipy.fftpack.fftshift).\n    return_freq: bool\n      Whether to also return the Fourier frequencies.\n    d: scalar\n      Sample spacing (inverse of the sampling rate). Defaults to 1.\n      Applicable if return_freq is 'True'.\n\n    Returns\n    -------\n    out : ndarray\n      One-dimensional array containing the RAPSD. The length of the array is\n      int(L/2)+1 (if L is even) or int(L/2) (if L is odd), where L=max(M,N).\n    freq : ndarray\n      One-dimensional array containing the Fourier frequencies.\n\n    References\n    ----------\n    :cite:`RC2011`\n\n    \"\"\"\n\n    if len(Z.shape) != 2:\n        raise ValueError(\"%i dimensions are found, but the number \"\n                         + \"of dimensions should be 2\"\n                         % len(Z.shape))\n\n    if np.sum(np.isnan(Z)) > 0:\n        raise ValueError('input array Z should not contain nans')\n\n    M, N = Z.shape\n\n    YC, XC = arrays.compute_centred_coord_array(M, N)\n    R = np.sqrt(XC*XC + YC*YC).round()\n    L = max(Z.shape[0], Z.shape[1])\n\n    if L % 2 == 0:\n        r_range = np.arange(0, int(L/2)+1)\n    else:\n        r_range = np.arange(0, int(L/2))\n\n    if fft_method is not None:\n        F = fft_method.fftshift(fft_method.fft2(Z, **fft_kwargs))\n        F = np.abs(F)**2/F.size\n    else:\n        F = Z\n\n    result = []\n    for r in r_range:\n        MASK = R == r\n        F_vals = F[MASK]\n        result.append(np.mean(F_vals))\n\n    if return_freq:\n        freq = np.fft.fftfreq(L, d=d)\n        freq = freq[r_range]\n        return np.array(result), freq\n    else:\n        return np.array(result)",
  "def remove_rain_norain_discontinuity(R):\n    \"\"\"Function to remove the rain/no-rain discontinuity.\n    It can be used before computing Fourier filters to reduce\n    the artificial increase of power at high frequencies\n    caused by the discontinuity.\n\n    Parameters\n    ----------\n    R : array-like\n        Array of any shape to be transformed.\n\n    Returns\n    -------\n    R : array-like\n        Array of any shape containing the transformed data.\n    \"\"\"\n    R = R.copy()\n    zerovalue = np.nanmin(R)\n    threshold = np.nanmin(R[R > zerovalue])\n    R[R > zerovalue] -= (threshold - zerovalue)\n    R -= np.nanmin(R)\n\n    return R",
  "def ShiTomasi_detection(input_image, max_corners=500, quality_level=0.1,\n                        min_distance=3, block_size=15, buffer_mask=0,\n                        use_harris=False, k=0.04,\n                        verbose=False,\n                        **kwargs):\n    \"\"\"\n    Interface to the OpenCV `Shi-Tomasi`_ features detection method to detect\n    corners in an image.\n\n    Corners are used for local tracking methods.\n\n    .. _`Shi-Tomasi`:\\\n        https://docs.opencv.org/3.4.1/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541\n\n    .. _MaskedArray:\\\n        https://docs.scipy.org/doc/numpy/reference/maskedarray.baseclass.html#numpy.ma.MaskedArray\n\n    .. _`Harris detector`:\\\n        https://docs.opencv.org/3.4.1/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345\n\n    .. _cornerMinEigenVal:\\\n        https://docs.opencv.org/3.4.1/dd/d1a/group__imgproc__feature.html#ga3dbce297c1feb859ee36707e1003e0a8\n\n\n    Parameters\n    ----------\n\n    input_image : array_like or MaskedArray_\n        Array of shape (m, n) containing the input image.\n\n        In case of array_like, invalid values (Nans or infs) are masked,\n        otherwise the mask of the MaskedArray_ is used. Such mask defines a\n        region where features are not detected.\n\n        The fill value for the masked pixels is taken as the minimum of all\n        valid pixels.\n\n    max_corners : int, optional\n        The **maxCorners** parameter in the `Shi-Tomasi`_ corner detection\n        method.\n        It represents the maximum number of points to be tracked (corners).\n        If set to zero, all detected corners are used.\n\n    quality_level : float, optional\n        The **qualityLevel** parameter in the `Shi-Tomasi`_ corner detection\n        method.\n        It represents the minimal accepted quality for the image corners.\n\n    min_distance : int, optional\n        The **minDistance** parameter in the `Shi-Tomasi`_ corner detection\n        method.\n        It represents minimum possible Euclidean distance in pixels between\n        corners.\n\n    block_size : int, optional\n        The **blockSize** parameter in the `Shi-Tomasi`_ corner detection\n        method.\n        It represents the window size in pixels used for computing a derivative\n        covariation matrix over each pixel neighborhood.\n\n    use_harris : bool, optional\n        Whether to use a `Harris detector`_  or cornerMinEigenVal_.\n\n    k : float, optional\n        Free parameter of the Harris detector.\n\n    buffer_mask : int, optional\n        A mask buffer width in pixels. This extends the input mask (if any)\n        to limit edge effects.\n\n    verbose : bool, optional\n        Print the number of features detected.\n\n    Returns\n    -------\n\n    points : array_like\n        Array of shape (p, 2) indicating the pixel coordinates of *p* detected\n        corners.\n\n    References\n    ----------\n\n    Jianbo Shi and Carlo Tomasi. Good features to track. In Computer Vision and\n    Pattern Recognition, 1994. Proceedings CVPR'94., 1994 IEEE Computer Society\n    Conference on, pages 593\u2013600. IEEE, 1994.\n    \"\"\"\n    if not CV2_IMPORTED:\n        raise MissingOptionalDependency(\n            \"opencv package is required for the goodFeaturesToTrack() \"\n            \"routine but it is not installed\"\n        )\n\n    input_image = np.copy(input_image)\n\n    if input_image.ndim != 2:\n        raise ValueError(\"input_image must be a two-dimensional array\")\n\n    # masked array\n    if ~isinstance(input_image, MaskedArray):\n        input_image = np.ma.masked_invalid(input_image)\n    np.ma.set_fill_value(input_image, input_image.min())\n\n    # buffer the quality mask to ensure that no vectors are computed nearby\n    # the edges of the radar mask\n    mask = np.ma.getmaskarray(input_image).astype(\"uint8\")\n    if buffer_mask > 0:\n        mask = cv2.dilate(\n            mask, np.ones((int(buffer_mask), int(buffer_mask)), np.uint8), 1\n        )\n        input_image[mask] = np.ma.masked\n\n    # scale image between 0 and 255\n    input_image = (\n        (input_image.filled() - input_image.min())\n        / (input_image.max() - input_image.min())\n        * 255\n    )\n\n    # convert to 8-bit\n    input_image = np.ndarray.astype(input_image, \"uint8\")\n    mask = (-1 * mask + 1).astype(\"uint8\")\n\n    params = dict(\n        maxCorners=max_corners,\n        qualityLevel=quality_level,\n        minDistance=min_distance,\n        useHarrisDetector=use_harris,\n        k=k,\n    )\n    points = cv2.goodFeaturesToTrack(input_image, mask=mask, **params)\n    if points is None:\n        points = np.empty(shape=(0, 2))\n    else:\n        points = points.squeeze()\n\n    if verbose:\n        print(\"--- %i good features to track detected ---\" % points.shape[0])\n\n    return points",
  "def morph_opening(input_image, thr, n):\n    \"\"\"Filter out small scale noise on the image by applying a binary\n    morphological opening, that is, erosion followed by dilation.\n\n    Parameters\n    ----------\n\n    input_image : array_like\n        Array of shape (m, n) containing the input image.\n\n    thr : float\n        The threshold used to convert the image into a binary image.\n\n    n : int\n        The structuring element size [pixels].\n\n    Returns\n    -------\n\n    input_image : array_like\n        Array of shape (m,n) containing the filtered image.\n    \"\"\"\n    if not CV2_IMPORTED:\n        raise MissingOptionalDependency(\n            \"opencv package is required for the morphologyEx \"\n            \"routine but it is not installed\"\n        )\n\n    # Convert to binary image\n    field_bin = np.ndarray.astype(input_image > thr, \"uint8\")\n\n    # Build a structuring element of size n\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (n, n))\n\n    # Apply morphological opening (i.e. erosion then dilation)\n    field_bin_out = cv2.morphologyEx(field_bin, cv2.MORPH_OPEN, kernel)\n\n    # Build mask to be applied on the original image\n    mask = (field_bin - field_bin_out) > 0\n\n    # Filter out small isolated pixels based on mask\n    input_image[mask] = np.nanmin(input_image)\n\n    return input_image",
  "def rbfinterp2d(\n    coord,\n    input_array,\n    xgrid,\n    ygrid,\n    rbfunction=\"gaussian\",\n    epsilon=5,\n    k=50,\n    nchunks=5,\n):\n    \"\"\"Fast 2-D grid interpolation of a sparse (multivariate) array using a\n    radial basis function.\n\n    Parameters\n    ----------\n\n    coord : array_like\n        Array of shape (n, 2) containing the coordinates of the data points\n        into a 2-dimensional space.\n\n    input_array : array_like\n        Array of shape (n) or (n, m) containing the values of the data points,\n        where *n* is the number of data points and *m* the number of co-located\n        variables.\n        All values in **input_array** are required to have finite values.\n\n    xgrid, ygrid : array_like\n        1D arrays representing the coordinates of the 2-D output grid.\n\n    rbfunction : {\"gaussian\", \"multiquadric\", \"inverse quadratic\",\n        \"inverse multiquadric\", \"bump\"}, optional\n        The name of one of the available radial basis function based on a\n        normalized Euclidian norm.\n\n        See also the Notes section below.\n\n    epsilon : float, optional\n        The shape parameter used to scale the input to the radial kernel.\n\n        A smaller value for **epsilon** produces a smoother interpolation. More\n        details provided in the wikipedia reference page.\n\n    k : int or None, optional\n        The number of nearest neighbours used to speed-up the interpolation.\n        If set to None, it interpolates based on all the data points.\n\n    nchunks : int, optional\n        The number of chunks in which the grid points are split to limit the\n        memory usage during the interpolation.\n\n    Returns\n    -------\n\n    output_array : array_like\n        The interpolated field(s) having shape (m, ygrid.size, xgrid.size).\n\n    Notes\n    -----\n\n    The coordinates are normalized before computing the Euclidean norms:\n\n        x = (x - min(x)) / max[max(x) - min(x), max(y) - min(y)],\\n\n        y = (y - min(y)) / max[max(x) - min(x), max(y) - min(y)],\n\n    where the min and max values are taken as the 2nd and 98th percentiles.\n\n    References\n    ----------\n\n    Wikipedia contributors, \"Radial basis function,\"\n    Wikipedia, The Free Encyclopedia,\n    https://en.wikipedia.org/w/index.php?title=Radial_basis_function&oldid=906155047\n    (accessed August 19, 2019).\n    \"\"\"\n\n    _rbfunctions = [\n        \"nearest\",\n        \"gaussian\",\n        \"inverse quadratic\",\n        \"inverse multiquadric\",\n        \"bump\",\n    ]\n\n    input_array = np.copy(input_array)\n\n    if np.any(~np.isfinite(input_array)):\n        raise ValueError(\"input_array contains non-finite values\")\n\n    if input_array.ndim == 1:\n        nvar = 1\n        input_array = input_array[:, None]\n\n    elif input_array.ndim == 2:\n        nvar = input_array.shape[1]\n\n    else:\n        raise ValueError(\n            \"input_array must have 1 (n) or 2 dimensions (n, m), but it has %i\"\n            % input_array.ndim\n        )\n\n    npoints = input_array.shape[0]\n\n    coord = np.copy(coord)\n\n    if coord.ndim != 2:\n        raise ValueError(\n            \"coord must have 2 dimensions (n, 2), but it has %i\" % coord.ndim\n        )\n\n    if npoints != coord.shape[0]:\n        raise ValueError(\n            \"the number of samples in the input_array does not match the \"\n            + \"number of coordinates %i!=%i\" % (npoints, coord.shape[0])\n        )\n\n    # normalize coordinates\n    qcoord = np.percentile(coord, [2, 98], axis=0)\n    dextent = np.max(np.diff(qcoord, axis=0))\n    coord = (coord - qcoord[0, :]) / dextent\n\n    rbfunction = rbfunction.lower()\n    if rbfunction not in _rbfunctions:\n        raise ValueError(\n            \"Unknown rbfunction '{}'\\n\".format(rbfunction)\n            + \"The available rbfunctions are: \"\n            + str(_rbfunctions)\n        ) from None\n\n    # generate the target grid\n    X, Y = np.meshgrid(xgrid, ygrid)\n    grid = np.column_stack((X.ravel(), Y.ravel()))\n    # normalize the grid coordinates\n    grid = (grid - qcoord[0, :]) / dextent\n\n    # k-nearest interpolation\n    if k is not None and k > 0:\n        k = int(np.min((k, npoints)))\n\n        # create cKDTree object to represent source grid\n        tree = scipy.spatial.cKDTree(coord)\n\n    else:\n        k = 0\n\n    # split grid points in n chunks\n    if nchunks > 1:\n        subgrids = np.array_split(grid, nchunks, 0)\n        subgrids = [x for x in subgrids if x.size > 0]\n\n    else:\n        subgrids = [grid]\n\n    # loop subgrids\n    i0 = 0\n    output_array = np.zeros((grid.shape[0], nvar))\n    for i, subgrid in enumerate(subgrids):\n        idelta = subgrid.shape[0]\n\n        if k == 0:\n            # use all points\n            d = scipy.spatial.distance.cdist(\n                coord, subgrid, \"euclidean\"\n            ).transpose()\n            inds = np.arange(npoints)[None, :] * np.ones(\n                (subgrid.shape[0], npoints)\n            ).astype(int)\n\n        else:\n            # use k-nearest neighbours\n            d, inds = tree.query(subgrid, k=k)\n\n        if k == 1:\n            # nearest neighbour\n            output_array[i0: (i0 + idelta), :] = input_array[inds, :]\n\n        else:\n\n            # the interpolation weights\n            if rbfunction == \"gaussian\":\n                w = np.exp(-(d * epsilon) ** 2)\n\n            elif rbfunction == \"inverse quadratic\":\n                w = 1.0 / (1 + (epsilon * d) ** 2)\n\n            elif rbfunction == \"inverse multiquadric\":\n                w = 1.0 / np.sqrt(1 + (epsilon * d) ** 2)\n\n            elif rbfunction == \"bump\":\n                w = np.exp(-1.0 / (1 - (epsilon * d) ** 2))\n                w[d >= 1 / epsilon] = 0.0\n\n            if not np.all(np.sum(w, axis=1)):\n                w[np.sum(w, axis=1) == 0, :] = 1.0\n\n            # interpolate\n            for j in range(nvar):\n                output_array[i0: (i0 + idelta), j] = np.sum(\n                    w * input_array[inds, j], axis=1\n                ) / np.sum(w, axis=1)\n\n        i0 += idelta\n\n    # reshape to final grid size\n    output_array = output_array.reshape(ygrid.size, xgrid.size, nvar)\n\n    return np.moveaxis(output_array, -1, 0).squeeze()",
  "def get_numpy(shape, fftn_shape=None, **kwargs):\n    import numpy.fft as numpy_fft\n\n    f = {\"fft2\": numpy_fft.fft2,\n         \"ifft2\": numpy_fft.ifft2,\n         \"rfft2\": numpy_fft.rfft2,\n         \"irfft2\": lambda X: numpy_fft.irfft2(X, s=shape),\n         \"fftshift\": numpy_fft.fftshift,\n         \"ifftshift\": numpy_fft.ifftshift,\n         \"fftfreq\": numpy_fft.fftfreq}\n    if fftn_shape is not None:\n        f[\"fftn\"] = numpy_fft.fftn\n    fft = SimpleNamespace(**f)\n\n    return fft",
  "def get_scipy(shape, fftn_shape=None, **kwargs):\n    import numpy.fft as numpy_fft\n    import scipy.fftpack as scipy_fft\n\n    # use numpy implementation of rfft2/irfft2 because they have not been\n    # implemented in scipy.fftpack\n    f = {\"fft2\": scipy_fft.fft2,\n         \"ifft2\": scipy_fft.ifft2,\n         \"rfft2\": numpy_fft.rfft2,\n         \"irfft2\": lambda X: numpy_fft.irfft2(X, s=shape),\n         \"fftshift\": scipy_fft.fftshift,\n         \"ifftshift\": scipy_fft.ifftshift,\n         \"fftfreq\": scipy_fft.fftfreq}\n    if fftn_shape is not None:\n        f[\"fftn\"] = scipy_fft.fftn\n    fft = SimpleNamespace(**f)\n\n    return fft",
  "def get_pyfftw(shape, fftn_shape=None, n_threads=1, **kwargs):\n    try:\n        import pyfftw.interfaces.numpy_fft as pyfftw_fft\n        import pyfftw\n        pyfftw.interfaces.cache.enable()\n    except ImportError:\n        raise MissingOptionalDependency(\"pyfftw is required but not installed\")\n\n    X = pyfftw.empty_aligned(shape, dtype=\"complex128\")\n    F = pyfftw.empty_aligned(shape, dtype=\"complex128\")\n\n    fft_obj = pyfftw.FFTW(X, F, flags=[\"FFTW_ESTIMATE\"],\n                          direction=\"FFTW_FORWARD\", axes=(0, 1),\n                          threads=n_threads)\n    ifft_obj = pyfftw.FFTW(F, X, flags=[\"FFTW_ESTIMATE\"],\n                           direction=\"FFTW_BACKWARD\", axes=(0, 1),\n                           threads=n_threads)\n\n    if fftn_shape is not None:\n        X = pyfftw.empty_aligned(fftn_shape, dtype=\"complex128\")\n        F = pyfftw.empty_aligned(fftn_shape, dtype=\"complex128\")\n\n        fftn_obj = pyfftw.FFTW(X, F, flags=[\"FFTW_ESTIMATE\"],\n                               direction=\"FFTW_FORWARD\",\n                               axes=list(range(len(fftn_shape))),\n                               threads=n_threads)\n\n    X = pyfftw.empty_aligned(shape, dtype=\"float64\")\n    output_shape = list(shape[:-1])\n    output_shape.append(int(shape[-1]/2)+1)\n    output_shape = tuple(output_shape)\n    F = pyfftw.empty_aligned(output_shape, dtype=\"complex128\")\n\n    rfft_obj = pyfftw.FFTW(X, F, flags=[\"FFTW_ESTIMATE\"],\n                           direction=\"FFTW_FORWARD\", axes=(0, 1),\n                           threads=n_threads)\n    irfft_obj = pyfftw.FFTW(F, X, flags=[\"FFTW_ESTIMATE\"],\n                            direction=\"FFTW_BACKWARD\", axes=(0, 1),\n                            threads=n_threads)\n\n    f = {\"fft2\": lambda X: fft_obj(input_array=X.copy()).copy(),\n         \"ifft2\": lambda X: ifft_obj(input_array=X.copy()).copy(),\n         \"rfft2\": lambda X: rfft_obj(input_array=X.copy()).copy(),\n         \"irfft2\": lambda X: irfft_obj(input_array=X.copy()).copy(),\n         \"fftshift\": pyfftw_fft.fftshift,\n         \"ifftshift\": pyfftw_fft.ifftshift,\n         \"fftfreq\": pyfftw_fft.fftfreq}\n    if fftn_shape is not None:\n        f[\"fftn\"] = lambda X: fftn_obj(input_array=X).copy()\n    fft = SimpleNamespace(**f)\n\n    return fft",
  "def compute_mask_window_function(mask, func, **kwargs):\n    \"\"\"Compute window function for a two-dimensional area defined by a\n    non-rectangular mask. The window function is computed based on the distance\n    to the nearest boundary point of the mask. Window function-specific\n    parameters are given as keyword arguments.\n\n    Parameters\n    ----------\n    mask : array_like\n        Two-dimensional boolean array containing the mask.\n        Pixels with True/False are inside/outside the mask.\n    func : str\n        The name of the window function. The currently implemented function is\n        'tukey'.\n\n    Returns\n    -------\n    out : array\n        Array containing the tapering weights.\n    \"\"\"\n    R = _compute_mask_distances(mask)\n\n    if func == \"hann\":\n        raise NotImplementedError(\"Hann function has not been implemented\")\n    elif func == \"tukey\":\n        r_max = kwargs.get(\"r_max\", 10.0)\n\n        return _tukey_masked(R, r_max, np.isfinite(R))\n    else:\n        raise ValueError(\"invalid window function '%s'\" % func)",
  "def compute_window_function(m, n, func, **kwargs):\n    \"\"\"Compute window function for a two-dimensional rectangular region. Window\n    function-specific parameters are given as keyword arguments.\n\n    Parameters\n    ----------\n    m : int\n        Height of the array.\n    n : int\n        Width of the array.\n    func : str\n        The name of the window function.\n        The currently implemented functions are\n        'hann' and 'tukey'.\n\n    Other Parameters\n    ----------------\n    alpha : float\n        Applicable if func is 'tukey'.\n\n    Notes\n    -----\n    Two-dimensional tapering weights are computed from one-dimensional window\n    functions using w(r), where r is the distance from the center of the\n    region.\n\n    Returns\n    -------\n    out : array\n        Array of shape (m, n) containing the tapering weights.\n    \"\"\"\n    X, Y = np.meshgrid(np.arange(n), np.arange(m))\n    R = np.sqrt((X - int(n/2))**2 + (Y - int(m/2))**2)\n\n    if func == \"hann\":\n        return _hann(R)\n    elif func == \"tukey\":\n        alpha = kwargs.get(\"alpha\", 0.2)\n\n        return _tukey(R, alpha)\n    else:\n        raise ValueError(\"invalid window function '%s'\" % func)",
  "def _compute_mask_distances(mask):\n    X, Y = np.meshgrid(np.arange(mask.shape[1]), np.arange(mask.shape[0]))\n\n    tree = cKDTree(np.vstack([X[~mask], Y[~mask]]).T)\n    r, i = tree.query(np.vstack([X[mask], Y[mask]]).T, k=1)\n\n    R = np.ones(mask.shape) * np.nan\n    R[Y[mask], X[mask]] = r\n\n    return R",
  "def _hann(R):\n    W = np.ones_like(R)\n    N = min(R.shape[0], R.shape[1])\n    mask = R > int(N / 2)\n\n    W[mask] = 0.0\n    W[~mask] = 0.5 * (1.0 - np.cos(2.0 * np.pi * (R[~mask] + int(N/2)) / N))\n\n    return W",
  "def _tukey(R, alpha):\n    W = np.ones_like(R)\n    N = min(R.shape[0], R.shape[1])\n\n    mask1 = R < int(N/2)\n    mask2 = R > int(N / 2) * (1.0 - alpha)\n    mask = np.logical_and(mask1, mask2)\n    W[mask] = 0.5 * (1.0 + np.cos(np.pi*(R[mask] /\n                                  (alpha * 0.5 * N) - 1.0/alpha + 1.0)))\n    mask = R >= int(N/2)\n    W[mask] = 0.0\n\n    return W",
  "def _tukey_masked(R, r_max, mask):\n    W = np.ones_like(R)\n\n    mask_r = R < r_max\n    mask_ = np.logical_and(mask, mask_r)\n    W[mask_] = 0.5 * (1.0 + np.cos(np.pi * (R[mask_] / r_max - 1.0)))\n    W[~mask] = np.nan\n\n    return W",
  "def boxcox_transform(\n    R, metadata=None, Lambda=None, threshold=None, zerovalue=None,\n    inverse=False,\n):\n    \"\"\"The one-parameter Box-Cox transformation.\n\n    The Box-Cox transform is a well-known power transformation introduced by\n    Box and Cox (1964). In its one-parameter version, the Box-Cox transform\n    takes the form T(x) = ln(x) for Lambda = 0,\n    or T(x) = (x**Lambda - 1)/Lambda otherwise.\n\n    Default parameters will produce a log transform (i.e. Lambda=0).\n\n    Parameters\n    ----------\n    R : array-like\n        Array of any shape to be transformed.\n\n    metadata : dict, optional\n        Metadata dictionary containing the transform, zerovalue and threshold\n        attributes as described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n\n    Lambda : float, optional\n        Parameter Lambda of the Box-Cox transformation.\n        It is 0 by default, which produces the log transformation.\n\n        Choose Lambda < 1 for positively skewed data, Lambda > 1 for negatively\n        skewed data.\n\n    threshold : float, optional\n        The value that is used for thresholding with the same units as R.\n        If None, the threshold contained in metadata is used.\n        If no threshold is found in the metadata,\n        a value of 0.1 is used as default.\n\n    zerovalue : float, optional\n        The value to be assigned to no rain pixels as defined by the threshold.\n        It is equal to the threshold - 1 by default.\n\n    inverse : bool, optional\n        If set to True, it performs the inverse transform. False by default.\n\n    Returns\n    -------\n    R : array-like\n        Array of any shape containing the (back-)transformed units.\n    metadata : dict\n        The metadata with updated attributes.\n\n    References\n    ----------\n    Box, G. E. and Cox, D. R. (1964), An Analysis of Transformations. Journal\n    of the Royal Statistical Society: Series B (Methodological), 26: 211-243.\n    doi:10.1111/j.2517-6161.1964.tb00553.x\n\n    \"\"\"\n\n    R = R.copy()\n\n    if metadata is None:\n        if inverse:\n            metadata = {\"transform\": \"BoxCox\"}\n        else:\n            metadata = {\"transform\": None}\n\n    else:\n        metadata = metadata.copy()\n\n    if not inverse:\n\n        if metadata[\"transform\"] == \"BoxCox\":\n            return R, metadata\n\n        if Lambda is None:\n            Lambda = metadata.get(\"BoxCox_lambda\", 0.0)\n\n        if threshold is None:\n            threshold = metadata.get(\"threshold\", 0.1)\n\n        zeros = R < threshold\n\n        # Apply Box-Cox transform\n        if Lambda == 0.0:\n            R[~zeros] = np.log(R[~zeros])\n            threshold = np.log(threshold)\n\n        else:\n            R[~zeros] = (R[~zeros] ** Lambda - 1) / Lambda\n            threshold = (threshold ** Lambda - 1) / Lambda\n\n        # Set value for zeros\n        if zerovalue is None:\n            zerovalue = threshold - 1  # TODO: set to a more meaningful value\n        R[zeros] = zerovalue\n\n        metadata[\"transform\"] = \"BoxCox\"\n        metadata[\"BoxCox_lambda\"] = Lambda\n        metadata[\"zerovalue\"] = zerovalue\n        metadata[\"threshold\"] = threshold\n\n    elif inverse:\n\n        if metadata[\"transform\"] not in [\"BoxCox\", \"log\"]:\n            return R, metadata\n\n        if Lambda is None:\n            Lambda = metadata.pop(\"BoxCox_lambda\", 0.0)\n        if threshold is None:\n            threshold = metadata.get(\"threshold\", -10.0)\n        if zerovalue is None:\n            zerovalue = 0.0\n\n        # Apply inverse Box-Cox transform\n        if Lambda == 0.0:\n            R = np.exp(R)\n            threshold = np.exp(threshold)\n\n        else:\n            R = np.exp(np.log(Lambda * R + 1) / Lambda)\n            threshold = np.exp(np.log(Lambda * threshold + 1) / Lambda)\n\n        R[R < threshold] = zerovalue\n\n        metadata[\"transform\"] = None\n        metadata[\"zerovalue\"] = zerovalue\n        metadata[\"threshold\"] = threshold\n\n    return R, metadata",
  "def dB_transform(R, metadata=None, threshold=None, zerovalue=None,\n                 inverse=False,):\n    \"\"\"Methods to transform precipitation intensities to/from dB units.\n\n    Parameters\n    ----------\n    R : array-like\n        Array of any shape to be (back-)transformed.\n    metadata : dict, optional\n        Metadata dictionary containing the transform, zerovalue and threshold\n        attributes as described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n    threshold : float, optional\n        Optional value that is used for thresholding with the same units as R.\n        If None, the threshold contained in metadata is used.\n        If no threshold is found in the metadata,\n        a value of 0.1 is used as default.\n    zerovalue : float, optional\n        The value to be assigned to no rain pixels as defined by the threshold.\n        It is equal to the threshold - 1 by default.\n    inverse : bool, optional\n        If set to True, it performs the inverse transform. False by default.\n\n    Returns\n    -------\n    R : array-like\n        Array of any shape containing the (back-)transformed units.\n    metadata : dict\n        The metadata with updated attributes.\n\n    \"\"\"\n\n    R = R.copy()\n\n    if metadata is None:\n        if inverse:\n            metadata = {\"transform\": \"dB\"}\n        else:\n            metadata = {\"transform\": None}\n\n    else:\n        metadata = metadata.copy()\n\n    # to dB units\n    if not inverse:\n\n        if metadata[\"transform\"] == \"dB\":\n            return R, metadata\n\n        if threshold is None:\n            threshold = metadata.get(\"threshold\", 0.1)\n\n        zeros = R < threshold\n\n        # Convert to dB\n        R[~zeros] = 10.0 * np.log10(R[~zeros])\n        threshold = 10.0 * np.log10(threshold)\n\n        # Set value for zeros\n        if zerovalue is None:\n            zerovalue = threshold - 5  # TODO: set to a more meaningful value\n        R[zeros] = zerovalue\n\n        metadata[\"transform\"] = \"dB\"\n        metadata[\"zerovalue\"] = zerovalue\n        metadata[\"threshold\"] = threshold\n\n        return R, metadata\n\n    # from dB units\n    elif inverse:\n\n        if metadata[\"transform\"] != \"dB\":\n            return R, metadata\n\n        if threshold is None:\n            threshold = metadata.get(\"threshold\", -10.0)\n        if zerovalue is None:\n            zerovalue = 0.0\n\n        R = 10.0 ** (R / 10.0)\n        threshold = 10.0 ** (threshold / 10.0)\n        R[R < threshold] = zerovalue\n\n        metadata[\"transform\"] = None\n        metadata[\"threshold\"] = threshold\n        metadata[\"zerovalue\"] = zerovalue\n\n        return R, metadata",
  "def NQ_transform(R, metadata=None, inverse=False, **kwargs):\n    \"\"\"The normal quantile transformation as in Bogner et al (2012).\n    Zero rain vales are set to zero in norm space.\n\n    Parameters\n    ----------\n    R : array-like\n        Array of any shape to be transformed.\n    metadata : dict, optional\n        Metadata dictionary containing the transform, zerovalue and threshold\n        attributes as described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n    inverse : bool, optional\n        If set to True, it performs the inverse transform. False by default.\n\n    Other Parameters\n    ----------------\n    a : float, optional\n        The offset fraction to be used for plotting positions;\n        typically in (0,1).\n        The default is 0., that is, it spaces the points evenly in the uniform\n        distribution.\n\n    Returns\n    -------\n    R : array-like\n        Array of any shape containing the (back-)transformed units.\n    metadata : dict\n        The metadata with updated attributes.\n\n    References\n    ----------\n    Bogner, K., Pappenberger, field., and Cloke, H. L.: Technical Note: The normal\n    quantile transformation and its application in a flood forecasting system,\n    Hydrol. Earth Syst. Sci., 16, 1085-1094,\n    https://doi.org/10.5194/hess-16-1085-2012, 2012.\n\n\n    \"\"\"\n\n    # defaults\n    a = kwargs.get(\"a\", 0.0)\n\n    R = R.copy()\n    shape0 = R.shape\n    R = R.ravel().astype(float)\n    idxNan = np.isnan(R)\n    R_ = R[~idxNan]\n\n    if metadata is None:\n        if inverse:\n            metadata = {\"transform\": \"NQT\"}\n        else:\n            metadata = {\"transform\": None}\n        metadata[\"zerovalue\"] = np.min(R_)\n\n    else:\n        metadata = metadata.copy()\n\n    if not inverse:\n\n        # Plotting positions\n        # https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot#Plotting_position\n        n = R_.size\n        Rpp = ((np.arange(n) + 1 - a) / (n + 1 - 2 * a)).reshape(R_.shape)\n\n        # NQ transform\n        Rqn = scipy_stats.norm.ppf(Rpp)\n        R__ = np.interp(R_, R_[np.argsort(R_)], Rqn)\n\n        # set zero rain to 0 in norm space\n        R__[R[~idxNan] == metadata[\"zerovalue\"]] = 0\n\n        # build inverse transform\n        metadata[\"inqt\"] = interp1d(\n            Rqn, R_[np.argsort(R_)],\n            bounds_error=False, fill_value=(R_.min(), R_.max())\n        )\n\n        metadata[\"transform\"] = \"NQT\"\n        metadata[\"zerovalue\"] = 0\n        metadata[\"threshold\"] = R__[R__ > 0].min()\n\n    else:\n\n        f = metadata.pop(\"inqt\")\n        R__ = f(R_)\n        metadata[\"transform\"] = None\n        metadata[\"zerovalue\"] = R__.min()\n        metadata[\"threshold\"] = R__[R__ > R__.min()].min()\n\n    R[~idxNan] = R__\n\n    return R.reshape(shape0), metadata",
  "def sqrt_transform(R, metadata=None, inverse=False, **kwargs):\n    \"\"\"Square-root transform.\n\n    Parameters\n    ----------\n    R : array-like\n        Array of any shape to be transformed.\n    metadata : dict, optional\n        Metadata dictionary containing the transform, zerovalue and threshold\n        attributes as described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n    inverse : bool, optional\n        If set to True, it performs the inverse transform. False by default.\n\n    Returns\n    -------\n    R : array-like\n        Array of any shape containing the (back-)transformed units.\n    metadata : dict\n        The metadata with updated attributes.\n\n    \"\"\"\n\n    R = R.copy()\n\n    if metadata is None:\n        if inverse:\n            metadata = {\"transform\": \"sqrt\"}\n        else:\n            metadata = {\"transform\": None}\n        metadata[\"zerovalue\"] = np.nan\n        metadata[\"threshold\"] = np.nan\n\n    else:\n        metadata = metadata.copy()\n\n    if not inverse:\n\n        # sqrt transform\n        R = np.sqrt(R)\n\n        metadata[\"transform\"] = \"sqrt\"\n        metadata[\"zerovalue\"] = np.sqrt(metadata[\"zerovalue\"])\n        metadata[\"threshold\"] = np.sqrt(metadata[\"threshold\"])\n\n    else:\n\n        # inverse sqrt transform\n        R = R ** 2\n\n        metadata[\"transform\"] = None\n        metadata[\"zerovalue\"] = metadata[\"zerovalue\"] ** 2\n        metadata[\"threshold\"] = metadata[\"threshold\"] ** 2\n\n    return R, metadata",
  "def compute_centred_coord_array(M, N):\n    \"\"\"Compute a 2D coordinate array, where the origin is at the center.\n\n    Parameters\n    ----------\n    M : int\n      The height of the array.\n    N : int\n      The width of the array.\n\n    Returns\n    -------\n    out : ndarray\n      The coordinate array.\n\n    Examples\n    --------\n    >>> compute_centred_coord_array(2, 2)\n\n    (array([[-2],\\n\n        [-1],\\n\n        [ 0],\\n\n        [ 1],\\n\n        [ 2]]), array([[-2, -1,  0,  1,  2]]))\n\n    \"\"\"\n\n    if M % 2 == 1:\n        s1 = np.s_[-int(M/2):int(M/2)+1]\n    else:\n        s1 = np.s_[-int(M/2):int(M/2)]\n\n    if N % 2 == 1:\n        s2 = np.s_[-int(N/2):int(N/2)+1]\n    else:\n        s2 = np.s_[-int(N/2):int(N/2)]\n\n    YC, XC = np.ogrid[s1, s2]\n\n    return YC, XC",
  "def to_rainrate(R, metadata, zr_a=None, zr_b=None):\n    \"\"\"Convert to rain rate [mm/h].\n\n    Parameters\n    ----------\n    R : array-like\n        Array of any shape to be (back-)transformed.\n    metadata : dict\n        Metadata dictionary containing the accutime, transform, unit, threshold\n        and zerovalue attributes as described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n\n        Additionally, in case of conversion to/from reflectivity units, the\n        zr_a and zr_b attributes are also required,\n        but only if zr_a = zr_b = None.\n        If missing, it defaults to Marshall\u2013Palmer relation,\n        that is, zr_a = 200.0 and zr_b = 1.6.\n    zr_a, zr_b : float, optional\n        The a and b coefficients of the Z-R relationship (Z = a*R^b).\n\n    Returns\n    -------\n    R : array-like\n        Array of any shape containing the converted units.\n    metadata : dict\n        The metadata with updated attributes.\n\n    \"\"\"\n\n    R = R.copy()\n    metadata = metadata.copy()\n\n    if metadata[\"transform\"] is not None:\n\n        if metadata[\"transform\"] == \"dB\":\n\n            R, metadata = transformation.dB_transform(R, metadata,\n                                                      inverse=True)\n\n        elif metadata[\"transform\"] in [\"BoxCox\", \"log\"]:\n\n            R, metadata = transformation.boxcox_transform(R, metadata,\n                                                          inverse=True)\n\n        elif metadata[\"transform\"] == \"NQT\":\n\n            R, metadata = transformation.NQ_transform(R, metadata,\n                                                      inverse=True)\n\n        elif metadata[\"transform\"] == \"sqrt\":\n\n            R, metadata = transformation.sqrt_transform(R, metadata,\n                                                        inverse=True)\n\n        else:\n\n            raise ValueError(\"Unknown transformation %s\"\n                             % metadata[\"transform\"])\n\n    if metadata[\"unit\"] == \"mm/h\":\n\n        pass\n\n    elif metadata[\"unit\"] == \"mm\":\n\n        threshold = metadata[\"threshold\"]  # convert the threshold, too\n        zerovalue = metadata[\"zerovalue\"]  # convert the zerovalue, too\n\n        R = R / float(metadata[\"accutime\"]) * 60.0\n        threshold = threshold / float(metadata[\"accutime\"]) * 60.0\n        zerovalue = zerovalue / float(metadata[\"accutime\"]) * 60.0\n\n        metadata[\"threshold\"] = threshold\n        metadata[\"zerovalue\"] = zerovalue\n\n    elif metadata[\"unit\"] == \"dBZ\":\n\n        threshold = metadata[\"threshold\"]  # convert the threshold, too\n        zerovalue = metadata[\"zerovalue\"]  # convert the zerovalue, too\n\n        # Z to R\n        if zr_a is None:\n            zr_a = metadata.get(\"zr_a\", 200.0)  # default to Marshall\u2013Palmer\n        if zr_b is None:\n            zr_b = metadata.get(\"zr_b\", 1.6)  # default to Marshall\u2013Palmer\n        R = (R / zr_a) ** (1.0 / zr_b)\n        threshold = (threshold / zr_a) ** (1.0 / zr_b)\n        zerovalue = (zerovalue / zr_a) ** (1.0 / zr_b)\n\n        metadata[\"zr_a\"] = zr_a\n        metadata[\"zr_b\"] = zr_b\n        metadata[\"threshold\"] = threshold\n        metadata[\"zerovalue\"] = zerovalue\n\n    else:\n        raise ValueError(\n            \"Cannot convert unit %s and transform %s to mm/h\"\n            % (metadata[\"unit\"], metadata[\"transform\"])\n        )\n\n    metadata[\"unit\"] = \"mm/h\"\n\n    return R, metadata",
  "def to_raindepth(R, metadata, zr_a=None, zr_b=None):\n    \"\"\"Convert to rain depth [mm].\n\n    Parameters\n    ----------\n    R : array-like\n        Array of any shape to be (back-)transformed.\n    metadata : dict\n        Metadata dictionary containing the accutime, transform, unit, threshold\n        and zerovalue attributes as described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n\n        Additionally, in case of conversion to/from reflectivity units, the\n        zr_a and zr_b attributes are also required,\n        but only if zr_a = zr_b = None.\n        If missing, it defaults to Marshall\u2013Palmer relation, that is,\n        zr_a = 200.0 and zr_b = 1.6.\n    zr_a, zr_b : float, optional\n        The a and b coefficients of the Z-R relationship (Z = a*R^b).\n\n    Returns\n    -------\n    R : array-like\n        Array of any shape containing the converted units.\n    metadata : dict\n        The metadata with updated attributes.\n\n    \"\"\"\n\n    R = R.copy()\n    metadata = metadata.copy()\n\n    if metadata[\"transform\"] is not None:\n\n        if metadata[\"transform\"] == \"dB\":\n\n            R, metadata = transformation.dB_transform(R, metadata,\n                                                      inverse=True)\n\n        elif metadata[\"transform\"] in [\"BoxCox\", \"log\"]:\n\n            R, metadata = transformation.boxcox_transform(R, metadata,\n                                                          inverse=True)\n\n        elif metadata[\"transform\"] == \"NQT\":\n\n            R, metadata = transformation.NQ_transform(R, metadata,\n                                                      inverse=True)\n\n        elif metadata[\"transform\"] == \"sqrt\":\n\n            R, metadata = transformation.sqrt_transform(R, metadata,\n                                                        inverse=True)\n\n        else:\n            raise ValueError(\"Unknown transformation %s\"\n                             % metadata[\"transform\"])\n\n    if metadata[\"unit\"] == \"mm\" and metadata[\"transform\"] is None:\n        pass\n\n    elif metadata[\"unit\"] == \"mm/h\":\n\n        threshold = metadata[\"threshold\"]  # convert the threshold, too\n        zerovalue = metadata[\"zerovalue\"]  # convert the zerovalue, too\n\n        R = R / 60.0 * metadata[\"accutime\"]\n        threshold = threshold / 60.0 * metadata[\"accutime\"]\n        zerovalue = zerovalue / 60.0 * metadata[\"accutime\"]\n\n        metadata[\"threshold\"] = threshold\n        metadata[\"zerovalue\"] = zerovalue\n\n    elif metadata[\"unit\"] == \"dBZ\":\n\n        threshold = metadata[\"threshold\"]  # convert the threshold, too\n        zerovalue = metadata[\"zerovalue\"]  # convert the zerovalue, too\n\n        # Z to R\n        if zr_a is None:\n            zr_a = metadata.get(\"zr_a\", 200.0)  # Default to Marshall\u2013Palmer\n        if zr_b is None:\n            zr_b = metadata.get(\"zr_b\", 1.6)  # Default to Marshall\u2013Palmer\n        R = (R / zr_a) ** (1.0 / zr_b) / 60.0 * metadata[\"accutime\"]\n        threshold = (threshold / zr_a) ** (1.0 / zr_b) / 60.0 \\\n            * metadata[\"accutime\"]\n        zerovalue = (zerovalue / zr_a) ** (1.0 / zr_b) / 60.0 \\\n            * metadata[\"accutime\"]\n\n        metadata[\"zr_a\"] = zr_a\n        metadata[\"zr_b\"] = zr_b\n        metadata[\"threshold\"] = threshold\n        metadata[\"zerovalue\"] = zerovalue\n\n    else:\n        raise ValueError(\n            \"Cannot convert unit %s and transform %s to mm\"\n            % (metadata[\"unit\"], metadata[\"transform\"])\n        )\n\n    metadata[\"unit\"] = \"mm\"\n\n    return R, metadata",
  "def to_reflectivity(R, metadata, zr_a=None, zr_b=None):\n    \"\"\"Convert to reflectivity [dBZ].\n\n    Parameters\n    ----------\n    R : array-like\n        Array of any shape to be (back-)transformed.\n    metadata : dict\n        Metadata dictionary containing the accutime, transform, unit, threshold\n        and zerovalue attributes as described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n\n        Additionally, in case of conversion to/from reflectivity units, the\n        zr_a and zr_b attributes are also required,\n        but only if zr_a = zr_b = None.\n        If missing, it defaults to Marshall\u2013Palmer relation, that is,\n        zr_a = 200.0 and zr_b = 1.6.\n    zr_a, zr_b : float, optional\n        The a and b coefficients of the Z-R relationship (Z = a*R^b).\n\n    Returns\n    -------\n    R : array-like\n        Array of any shape containing the converted units.\n    metadata : dict\n        The metadata with updated attributes.\n\n    \"\"\"\n\n    R = R.copy()\n    metadata = metadata.copy()\n\n    if metadata[\"transform\"] is not None:\n\n        if metadata[\"transform\"] == \"dB\":\n\n            R, metadata = transformation.dB_transform(R, metadata,\n                                                      inverse=True)\n\n        elif metadata[\"transform\"] in [\"BoxCox\", \"log\"]:\n\n            R, metadata = transformation.boxcox_transform(R, metadata,\n                                                          inverse=True)\n\n        elif metadata[\"transform\"] == \"NQT\":\n\n            R, metadata = transformation.NQ_transform(R, metadata,\n                                                      inverse=True)\n\n        elif metadata[\"transform\"] == \"sqrt\":\n\n            R, metadata = transformation.sqrt_transform(R, metadata,\n                                                        inverse=True)\n\n        else:\n\n            raise ValueError(\"Unknown transformation %s\"\n                             % metadata[\"transform\"])\n\n    if metadata[\"unit\"] == \"mm/h\":\n\n        # Z to R\n        if zr_a is None:\n            zr_a = metadata.get(\"zr_a\", 200.0)  # Default to Marshall\u2013Palmer\n        if zr_b is None:\n            zr_b = metadata.get(\"zr_b\", 1.6)  # Default to Marshall\u2013Palmer\n\n        R = zr_a * R ** zr_b\n        metadata[\"threshold\"] = zr_a * metadata[\"threshold\"] ** zr_b\n        metadata[\"zerovalue\"] = zr_a * metadata[\"zerovalue\"] ** zr_b\n        metadata[\"zr_a\"] = zr_a\n        metadata[\"zr_b\"] = zr_b\n\n        # Z to dBZ\n        R, metadata = transformation.dB_transform(R, metadata)\n\n    elif metadata[\"unit\"] == \"mm\":\n\n        # depth to rate\n        R, metadata = to_rainrate(R, metadata)\n\n        # Z to R\n        if zr_a is None:\n            zr_a = metadata.get(\"zr_a\", 200.0)  # Default to Marshall-Palmer\n        if zr_b is None:\n            zr_b = metadata.get(\"zr_b\", 1.6)  # Default to Marshall-Palmer\n        R = zr_a * R ** zr_b\n        metadata[\"threshold\"] = zr_a * metadata[\"threshold\"] ** zr_b\n        metadata[\"zerovalue\"] = zr_a * metadata[\"zerovalue\"] ** zr_b\n        metadata[\"zr_a\"] = zr_a\n        metadata[\"zr_b\"] = zr_b\n\n        # Z to dBZ\n        R, metadata = transformation.dB_transform(R, metadata)\n\n    elif metadata[\"unit\"] == \"dBZ\":\n\n        # Z to dBZ\n        R, metadata = transformation.dB_transform(R, metadata)\n\n    else:\n\n        raise ValueError(\n            \"Cannot convert unit %s and transform %s to mm/h\"\n            % (metadata[\"unit\"], metadata[\"transform\"])\n        )\n\n    metadata[\"unit\"] = \"dBZ\"\n\n    return R, metadata",
  "def aggregate_fields_time(R, metadata, time_window_min, ignore_nan=False):\n    \"\"\"Aggregate fields in time.\n\n    Parameters\n    ----------\n    R : array-like\n        Array of shape (t,m,n) or (l,t,m,n) containing\n        a time series of (ensemble) input fields.\n        They must be evenly spaced in time.\n    metadata : dict\n        Metadata dictionary containing the timestamps and unit attributes as\n        described in the documentation of :py:mod:`pysteps.io.importers`.\n    time_window_min : float or None\n        The length in minutes of the time window that is used to\n        aggregate the fields.\n        The time spanned by the t dimension of R must be a multiple of\n        time_window_min.\n        If set to None, it returns a copy of the original R and metadata.\n    ignore_nan : bool, optional\n        If True, ignore nan values.\n\n    Returns\n    -------\n    outputarray : array-like\n        The new array of aggregated fields of shape (k,m,n) or (l,k,m,n), where\n        k = t*delta/time_window_min and delta is the time interval between two\n        successive timestamps.\n    metadata : dict\n        The metadata with updated attributes.\n\n    See also\n    --------\n    pysteps.utils.dimension.aggregate_fields_space,\n    pysteps.utils.dimension.aggregate_fields\n\n    \"\"\"\n\n    R = R.copy()\n    metadata = metadata.copy()\n\n    if time_window_min is None:\n        return R, metadata\n\n    unit = metadata[\"unit\"]\n    timestamps = metadata[\"timestamps\"]\n    if \"leadtimes\" in metadata:\n        leadtimes = metadata[\"leadtimes\"]\n\n    if len(R.shape) < 3:\n        raise ValueError(\"The number of dimension must be > 2\")\n    if len(R.shape) == 3:\n        axis = 0\n    if len(R.shape) == 4:\n        axis = 1\n    if len(R.shape) > 4:\n        raise ValueError(\"The number of dimension must be <= 4\")\n\n    if R.shape[axis] != len(timestamps):\n        raise ValueError(\"The list of timestamps has length %i, \"\n                         + \"but R contains %i frames\"\n                         % (len(timestamps), R.shape[axis]))\n\n    # assumes that frames are evenly spaced\n    delta = (timestamps[1] - timestamps[0]).seconds/60\n    if delta == time_window_min:\n        return R, metadata\n    if (R.shape[axis]*delta) % time_window_min:\n        raise ValueError('time_window_size does not equally split R')\n\n    nframes = int(time_window_min/delta)\n\n    # specify the operator to be used to aggregate\n    # the values within the time window\n    if unit == \"mm/h\":\n        method = \"mean\"\n    elif unit == \"mm\":\n        method = \"sum\"\n    else:\n        raise ValueError(\"can only aggregate units of 'mm/h' or 'mm'\"\n                         + \" not %s\" % unit)\n\n    if ignore_nan:\n        method = \"\".join((\"nan\", method))\n\n    R = aggregate_fields(R, nframes, axis=axis, method=method)\n\n    metadata[\"accutime\"] = time_window_min\n    metadata[\"timestamps\"] = timestamps[nframes-1::nframes]\n    if \"leadtimes\" in metadata:\n        metadata[\"leadtimes\"] = leadtimes[nframes-1::nframes]\n\n    return R, metadata",
  "def aggregate_fields_space(R, metadata, space_window, ignore_nan=False):\n    \"\"\"Upscale fields in space.\n\n    Parameters\n    ----------\n    R : array-like\n        Array of shape (m,n), (t,m,n) or (l,t,m,n) containing a single field or\n        a time series of (ensemble) input fields.\n    metadata : dict\n        Metadata dictionary containing the xpixelsize, ypixelsize and unit\n        attributes as described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n    space_window : float or None\n        The length of the space window that is used to upscale the fields.\n        The space_window unit is the same used in the geographical projection\n        of R\n        and hence the same as for the xpixelsize and ypixelsize attributes.\n        The space spanned by the m and n dimensions of R must be a multiple of\n        space_window.\n        If set to None, it returns a copy of the original R and metadata.\n    ignore_nan : bool, optional\n        If True, ignore nan values.\n\n    Returns\n    -------\n    outputarray : array-like\n        The new array of aggregated fields of shape (k,j), (t,k,j)\n        or (l,t,k,j),\n        where k = m*ypixelsize/space_window and j = n*xpixelsize/space_window.\n    metadata : dict\n        The metadata with updated attributes.\n\n    See also\n    --------\n    pysteps.utils.dimension.aggregate_fields_time,\n    pysteps.utils.dimension.aggregate_fields\n\n    \"\"\"\n\n    R = R.copy()\n    metadata = metadata.copy()\n\n    if space_window is None:\n        return R, metadata\n\n    unit = metadata[\"unit\"]\n    ypixelsize = metadata[\"ypixelsize\"]\n    xpixelsize = metadata[\"xpixelsize\"]\n\n    if len(R.shape) < 2:\n        raise ValueError(\"The number of dimensions must be >= 2\")\n    if len(R.shape) == 2:\n        axes = [0, 1]\n    if len(R.shape) == 3:\n        axes = [1, 2]\n    if len(R.shape) == 4:\n        axes = [2, 3]\n    if len(R.shape) > 4:\n        raise ValueError(\"The number of dimensions must be <= 4\")\n\n    # assumes that frames are evenly spaced\n    if ypixelsize == space_window and xpixelsize == space_window:\n        return R, metadata\n    if (R.shape[axes[0]]*ypixelsize) % space_window or \\\n       (R.shape[axes[1]]*xpixelsize) % space_window:\n        raise ValueError('space_window does not equally split R')\n\n    nframes = [int(space_window/ypixelsize), int(space_window/xpixelsize)]\n\n    # specify the operator to be used to aggregate the values\n    # within the space window\n    if unit == \"mm/h\":\n        method = \"mean\"\n    elif unit == \"mm\":\n        method = \"sum\"\n    else:\n        raise ValueError(\"can only aggregate units of 'mm/h' or 'mm' \"\n                         + \"not %s\" % unit)\n\n    if ignore_nan:\n        method = \"\".join((\"nan\", method))\n\n    R = aggregate_fields(R, nframes[0], axis=axes[0], method=method)\n    R = aggregate_fields(R, nframes[1], axis=axes[1], method=method)\n\n    metadata[\"ypixelsize\"] = space_window\n    metadata[\"xpixelsize\"] = space_window\n\n    return R, metadata",
  "def aggregate_fields(R, window_size, axis=0, method=\"mean\"):\n    \"\"\"Aggregate fields.\n    It attemps to aggregate the given R axis in an integer number of sections\n    of length = window_size.\n    If such a aggregation is not possible, an error is raised.\n\n    Parameters\n    ----------\n    R : array-like\n        Array of any shape containing the input fields.\n    window_size : int\n        The length of the window that is used to aggregate the fields.\n    axis : int, optional\n        The axis where to perform the aggregation.\n    method : string, optional\n        Optional argument that specifies the operation to use\n        to aggregate the values within the window.\n        Default to mean operator.\n\n    Returns\n    -------\n    outputarray : array-like\n        The new aggregated array with shape[axis] = k,\n        where k = R.shape[axis] / window_size\n\n    See also\n    --------\n    pysteps.utils.dimension.aggregate_fields_time,\n    pysteps.utils.dimension.aggregate_fields_space\n    \"\"\"\n\n    N = R.shape[axis]\n    if N % window_size:\n        raise ValueError('window_size %i does not equally split '\n                         + 'R.shape[axis] %i' % (window_size, N))\n\n    R = R.copy().swapaxes(axis, 0)\n    shape = list(R.shape)\n    R_ = R.reshape((N, -1))\n\n    if method.lower() == \"sum\":\n        R__ = R_.reshape(int(N/window_size),\n                         window_size,\n                         R_.shape[1]).sum(axis=1)\n    elif method.lower() == \"mean\":\n        R__ = R_.reshape(int(N/window_size),\n                         window_size,\n                         R_.shape[1]).mean(axis=1)\n    elif method.lower() == \"nansum\":\n        R__ = np.nansum(R_.reshape(int(N/window_size),\n                                   window_size,\n                                   R_.shape[1]), axis=1)\n    elif method.lower() == \"nanmean\":\n        R__ = np.nanmean(R_.reshape(int(N/window_size),\n                                    window_size,\n                                    R_.shape[1]), axis=1)\n    else:\n        raise ValueError(\"unknown method %s\" % method)\n\n    shape[0] = int(N/window_size)\n    R = R__.reshape(shape).swapaxes(axis, 0)\n\n    return R",
  "def clip_domain(R, metadata, extent=None):\n    \"\"\"Clip the field domain by geographical coordinates.\n\n    Parameters\n    ----------\n    R : array-like\n        Array of shape (m,n) or (t,m,n) containing the input fields.\n    metadata : dict\n        Metadata dictionary containing the x1, x2, y1, y2,\n        xpixelsize, ypixelsize,\n        zerovalue and yorigin attributes as described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n    extent : scalars (left, right, bottom, top), optional\n        The extent of the bounding box in data coordinates to be used to clip\n        the data.\n        Note that the direction of the vertical axis and thus the default\n        values for top and bottom depend on origin. We follow the same\n        convention as in the imshow method of matplotlib:\n        https://matplotlib.org/tutorials/intermediate/imshow_extent.html\n\n    Returns\n    -------\n    R : array-like\n        the clipped array\n    metadata : dict\n        the metadata with updated attributes.\n\n    \"\"\"\n\n    R = R.copy()\n    R_shape = np.array(R.shape)\n    metadata = metadata.copy()\n\n    if extent is None:\n        return R, metadata\n\n    if len(R.shape) < 2:\n        raise ValueError(\"The number of dimension must be > 1\")\n    if len(R.shape) == 2:\n        R = R[None, None, :, :]\n    if len(R.shape) == 3:\n        R = R[None, :, :, :]\n    if len(R.shape) > 4:\n        raise ValueError(\"The number of dimension must be <= 4\")\n\n    # extract original domain coordinates\n    left = metadata[\"x1\"]\n    right = metadata[\"x2\"]\n    bottom = metadata[\"y1\"]\n    top = metadata[\"y2\"]\n\n    # extract bounding box coordinates\n    left_ = extent[0]\n    right_ = extent[1]\n    bottom_ = extent[2]\n    top_ = extent[3]\n\n    # compute its extent in pixels\n    dim_x_ = int((right_ - left_)/metadata[\"xpixelsize\"])\n    dim_y_ = int((top_ - bottom_)/metadata[\"ypixelsize\"])\n    R_ = np.ones((R.shape[0], R.shape[1], dim_y_, dim_x_)) \\\n        * metadata[\"zerovalue\"]\n\n    # build set of coordinates for the original domain\n    y_coord = np.linspace(bottom, top - metadata[\"ypixelsize\"], R.shape[2]) \\\n        + metadata[\"ypixelsize\"]/2.\n    x_coord = np.linspace(left, right - metadata[\"xpixelsize\"], R.shape[3]) \\\n        + metadata[\"xpixelsize\"]/2.\n\n    # build set of coordinates for the new domain\n    y_coord_ = np.linspace(bottom_, top_ - metadata[\"ypixelsize\"],\n                           R_.shape[2]) \\\n        + metadata[\"ypixelsize\"]/2.\n    x_coord_ = np.linspace(left_, right_ - metadata[\"xpixelsize\"],\n                           R_.shape[3]) \\\n        + metadata[\"xpixelsize\"]/2.\n\n    # origin='upper' reverses the vertical axes direction\n    if metadata[\"yorigin\"] == \"upper\":\n        y_coord = y_coord[::-1]\n        y_coord_ = y_coord_[::-1]\n\n    # extract original domain\n    idx_y = np.where(np.logical_and(y_coord < top_, y_coord > bottom_))[0]\n    idx_x = np.where(np.logical_and(x_coord < right_, x_coord > left_))[0]\n\n    # extract new domain\n    idx_y_ = np.where(np.logical_and(y_coord_ < top, y_coord_ > bottom))[0]\n    idx_x_ = np.where(np.logical_and(x_coord_ < right, x_coord_ > left))[0]\n\n    # compose the new array\n    R_[:, :, idx_y_[0]:(idx_y_[-1] + 1), idx_x_[0]:(idx_x_[-1] + 1)] = \\\n        R[:, :, idx_y[0]:(idx_y[-1] + 1), idx_x[0]:(idx_x[-1] + 1)]\n\n    # update coordinates\n    metadata[\"y1\"] = bottom_\n    metadata[\"y2\"] = top_\n    metadata[\"x1\"] = left_\n    metadata[\"x2\"] = right_\n\n    R_shape[-2] = R_.shape[-2]\n    R_shape[-1] = R_.shape[-1]\n\n    return R_.reshape(R_shape), metadata",
  "def square_domain(R, metadata, method=\"pad\", inverse=False):\n    \"\"\"Either pad or crop a field to obtain a square domain.\n\n    Parameters\n    ----------\n    R : array-like\n        Array of shape (m,n) or (t,m,n) containing the input fields.\n    metadata : dict\n        Metadata dictionary containing the x1, x2, y1, y2,\n        xpixelsize, ypixelsize,\n        attributes as described in the documentation of\n        :py:mod:`pysteps.io.importers`.\n    method : {'pad', 'crop'}, optional\n        Either pad or crop.\n        If pad, an equal number of zeros is added to both ends of its shortest\n        side in order to produce a square domain.\n        If crop, an equal number of pixels is removed\n        to both ends of its longest side in order to produce a square domain.\n        Note that the crop method involves an irreversible loss of data.\n    inverse : bool, optional\n        Perform the inverse method to recover the original domain shape.\n        After a crop, the inverse is performed by padding the field with zeros.\n\n    Returns\n    -------\n    R : array-like\n        the reshape dataset\n    metadata : dict\n        the metadata with updated attributes.\n\n    \"\"\"\n\n    R = R.copy()\n    R_shape = np.array(R.shape)\n    metadata = metadata.copy()\n\n    if not inverse:\n\n        if len(R.shape) < 2:\n            raise ValueError(\"The number of dimension must be > 1\")\n        if len(R.shape) == 2:\n            R = R[None, None, :]\n        if len(R.shape) == 3:\n            R = R[None, :]\n        if len(R.shape) > 4:\n            raise ValueError(\"The number of dimension must be <= 4\")\n\n        if R.shape[2] == R.shape[3]:\n            return R.squeeze()\n\n        orig_dim = (R.shape)\n        orig_dim_n = orig_dim[0]\n        orig_dim_t = orig_dim[1]\n        orig_dim_y = orig_dim[2]\n        orig_dim_x = orig_dim[3]\n\n        if method == \"pad\":\n\n            new_dim = np.max(orig_dim[2:])\n            R_ = np.ones((orig_dim_n, orig_dim_t, new_dim, new_dim))*R.min()\n\n            if(orig_dim_x < new_dim):\n                idx_buffer = int((new_dim - orig_dim_x)/2.)\n                R_[:, :, :, idx_buffer:(idx_buffer + orig_dim_x)] = R\n                metadata[\"x1\"] -= idx_buffer*metadata[\"xpixelsize\"]\n                metadata[\"x2\"] += idx_buffer*metadata[\"xpixelsize\"]\n\n            elif(orig_dim_y < new_dim):\n                idx_buffer = int((new_dim - orig_dim_y)/2.)\n                R_[:, :, idx_buffer:(idx_buffer + orig_dim_y), :] = R\n                metadata[\"y1\"] -= idx_buffer*metadata[\"ypixelsize\"]\n                metadata[\"y2\"] += idx_buffer*metadata[\"ypixelsize\"]\n\n        elif method == \"crop\":\n\n            new_dim = np.min(orig_dim[2:])\n            R_ = np.zeros((orig_dim_n, orig_dim_t, new_dim, new_dim))\n\n            if(orig_dim_x > new_dim):\n                idx_buffer = int((orig_dim_x - new_dim)/2.)\n                R_ = R[:, :, :, idx_buffer:(idx_buffer + new_dim)]\n                metadata[\"x1\"] += idx_buffer*metadata[\"xpixelsize\"]\n                metadata[\"x2\"] -= idx_buffer*metadata[\"xpixelsize\"]\n\n            elif(orig_dim_y > new_dim):\n                idx_buffer = int((orig_dim_y - new_dim)/2.)\n                R_ = R[:, :, idx_buffer:(idx_buffer + new_dim), :]\n                metadata[\"y1\"] += idx_buffer*metadata[\"ypixelsize\"]\n                metadata[\"y2\"] -= idx_buffer*metadata[\"ypixelsize\"]\n\n        else:\n            raise ValueError(\"Unknown type\")\n\n        metadata[\"orig_domain\"] = (orig_dim_y, orig_dim_x)\n        metadata[\"square_method\"] = method\n\n        R_shape[-2] = R_.shape[-2]\n        R_shape[-1] = R_.shape[-1]\n\n        return R_.reshape(R_shape), metadata\n\n    elif inverse:\n\n        if len(R.shape) < 2:\n            raise ValueError(\"The number of dimension must be > 2\")\n        if len(R.shape) == 2:\n            R = R[None, None, :]\n        if len(R.shape) == 3:\n            R = R[None, :]\n        if len(R.shape) > 4:\n            raise ValueError(\"The number of dimension must be <= 4\")\n\n        method = metadata.pop(\"square_method\")\n        shape = metadata.pop(\"orig_domain\")\n\n        if R.shape[2] == shape[0] and R.shape[3] == shape[1]:\n            return R.squeeze(), metadata\n\n        R_ = np.zeros((R.shape[0], R.shape[1], shape[0], shape[1]))\n\n        if method == \"pad\":\n\n            if R.shape[2] == shape[0]:\n                idx_buffer = int((R.shape[3] - shape[1])/2.)\n                R_ = R[:, :, :, idx_buffer:(idx_buffer + shape[1])]\n                metadata[\"x1\"] += idx_buffer*metadata[\"xpixelsize\"]\n                metadata[\"x2\"] -= idx_buffer*metadata[\"xpixelsize\"]\n\n            elif R.shape[3] == shape[1]:\n                idx_buffer = int((R.shape[2] - shape[0])/2.)\n                R_ = R[:, :, idx_buffer:(idx_buffer + shape[0]), :]\n                metadata[\"y1\"] += idx_buffer*metadata[\"ypixelsize\"]\n                metadata[\"y2\"] -= idx_buffer*metadata[\"ypixelsize\"]\n\n        elif method == \"crop\":\n\n            if R.shape[2] == shape[0]:\n                idx_buffer = int((shape[1] - R.shape[3])/2.)\n                R_[:, :, :, idx_buffer:(idx_buffer + R.shape[3])] = R\n                metadata[\"x1\"] -= idx_buffer*metadata[\"xpixelsize\"]\n                metadata[\"x2\"] += idx_buffer*metadata[\"xpixelsize\"]\n\n            elif R.shape[3] == shape[1]:\n                idx_buffer = int((shape[0] - R.shape[2])/2.)\n                R_[:, :, idx_buffer:(idx_buffer + R.shape[2]), :] = R\n                metadata[\"y1\"] -= idx_buffer*metadata[\"ypixelsize\"]\n                metadata[\"y2\"] += idx_buffer*metadata[\"ypixelsize\"]\n\n        R_shape[-2] = R_.shape[-2]\n        R_shape[-1] = R_.shape[-1]\n\n        return R_.reshape(R_shape), metadata",
  "def decluster(coord, input_array, scale, min_samples=1, verbose=False):\n    \"\"\"Decluster a set of sparse data points by aggregating, that is, taking\n    the median value of all values lying within a certain distance (i.e., a\n    cluster).\n\n    Parameters\n    ----------\n\n    coord : array_like\n        Array of shape (n, d) containing the coordinates of the input data into\n        a space of *d* dimensions.\n\n    input_array : array_like\n        Array of shape (n) or (n, m), where *n* is the number of samples and\n        *m* the number of variables.\n        All values in **input_array** are required to have finite values.\n\n    scale : float or array_like\n        The **scale** parameter in the same units of **coord**.\n        It can be a scalar or an array_like of shape (d).\n        Data points within the declustering **scale** are aggregated.\n\n    min_samples : int, optional\n        The minimum number of samples for computing the median within a given\n        cluster.\n\n    verbose : bool, optional\n        Print out information.\n\n    Returns\n    -------\n\n    out : tuple of ndarrays\n        A two-element tuple (**out_coord**, **output_array**) containing the\n        declustered coordinates (l, d) and **input_array** (l, m), where *l* is\n        the new number of samples with *l* <= *n*.\n    \"\"\"\n\n    coord = np.copy(coord)\n    input_array = np.copy(input_array)\n\n    # check inputs\n    if np.any(~np.isfinite(input_array)):\n        raise ValueError(\"input_array contains non-finite values\")\n\n    if input_array.ndim == 1:\n        nvar = 1\n        input_array = input_array[:, None]\n    elif input_array.ndim == 2:\n        nvar = input_array.shape[1]\n    else:\n        raise ValueError(\n            \"input_array must have 1 (n) or 2 dimensions (n, m), but it has %i\"\n            % input_array.ndim\n        )\n\n    if coord.ndim != 2:\n        raise ValueError(\n            \"coord must have 2 dimensions (n, d), but it has %i\" % coord.ndim\n        )\n    if coord.shape[0] != input_array.shape[0]:\n        raise ValueError(\n            \"the number of samples in the input_array does not match the \"\n            + \"number of coordinates %i!=%i\"\n            % (input_array.shape[0], coord.shape[0])\n        )\n\n    if np.isscalar(scale):\n        scale = np.float(scale)\n    else:\n        scale = np.copy(scale)\n        if scale.ndim != 1:\n            raise ValueError(\n                \"scale must have 1 dimension (d), but it has %i\" % scale.ndim\n            )\n        if scale.shape[0] != coord.shape[1]:\n            raise ValueError(\n                \"scale must have %i elements, but it has %i\"\n                % (coord.shape[1], scale.shape[0])\n            )\n        scale = scale[None, :]\n\n    # reduce original coordinates\n    coord_ = np.floor(coord / scale)\n\n    # keep only unique pairs of the reduced coordinates\n    coordb_ = np.ascontiguousarray(coord_).view(\n        np.dtype((np.void, coord_.dtype.itemsize * coord_.shape[1]))\n    )\n    __, idx = np.unique(coordb_, return_index=True)\n    ucoord_ = coord_[idx]\n    # TODO: why not simply using np.unique(coord_, axis=0) ?\n\n    # loop through these unique values and average data points which belong to\n    # the same cluster\n    dinput = np.empty(shape=(0, nvar))\n    dcoord = np.empty(shape=(0, coord.shape[1]))\n    for i in range(ucoord_.shape[0]):\n        idx = np.all(coord_ == ucoord_[i, :], axis=1)\n        npoints = np.sum(idx)\n        if npoints >= min_samples:\n            dinput = np.append(\n                dinput, np.median(input_array[idx, :], axis=0)[None, :], axis=0\n            )\n            dcoord = np.append(\n                dcoord, np.median(coord[idx, :], axis=0)[None, :], axis=0\n            )\n\n    if verbose:\n        print(\"--- %i samples left after declustering ---\" % dinput.shape[0])\n\n    return dcoord.squeeze(), dinput",
  "def detect_outliers(input_array, thr, coord=None, k=None, verbose=False):\n    \"\"\"Detect outliers in a (multivariate and georeferenced) dataset.\n\n    Assume a (multivariate) Gaussian distribution and detect outliers based on\n    the number of standard deviations from the mean.\n\n    If spatial information is provided through coordinates, the outlier\n    detection can be localized by considering only the k-nearest neighbours\n    when computing the local mean and standard deviation.\n\n    Parameters\n    ----------\n\n    input_array : array_like\n        Array of shape (n) or (n, m), where *n* is the number of samples and\n        *m* the number of variables. If *m* > 1, the Mahalanobis distance\n        is used.\n        All values in **input_array** are required to have finite values.\n\n    thr : float\n        The number of standard deviations from the mean\n        that defines an outlier.\n\n    coord : array_like, optional\n        Array of shape (n, d) containing the coordinates of the input data into\n        a space of *d* dimensions.\n        Passing **coord** requires that **k** is not None.\n\n    k : int or None, optional\n        The number of nearest neighbours used to localize the outlier\n        detection.\n        If set to None (the default), it employs all the data points (global\n        detection). Setting **k** requires that **coord** is not None.\n\n    verbose : bool, optional\n        Print out information.\n\n    Returns\n    -------\n\n    out : array_like\n        A boolean array of the same shape as **input_array**, with True values\n        indicating the outliers detected in **input_array**.\n    \"\"\"\n\n    input_array = np.copy(input_array)\n\n    if np.any(~np.isfinite(input_array)):\n        raise ValueError(\"input_array contains non-finite values\")\n\n    if input_array.ndim == 1:\n        nvar = 1\n    elif input_array.ndim == 2:\n        nvar = input_array.shape[1]\n    else:\n        raise ValueError(\n            \"input_array must have 1 (n) or 2 dimensions (n, m), but it has %i\"\n            % coord.ndim\n        )\n\n    if coord is not None:\n\n        coord = np.copy(coord)\n        if coord.ndim == 1:\n            coord = coord[:, None]\n\n        elif coord.ndim > 2:\n            raise ValueError(\n                \"coord must have 2 dimensions (n, d), but it has %i\"\n                % coord.ndim\n            )\n\n        if coord.shape[0] != input_array.shape[0]:\n            raise ValueError(\n                \"the number of samples in input_array does not match the \"\n                + \"number of coordinates %i!=%i\"\n                % (input_array.shape[0], coord.shape[0])\n            )\n\n        if k is None:\n            raise ValueError(\"coord is set but k is None\")\n\n        k = np.min((coord.shape[0], k + 1))\n\n    else:\n        if k is not None:\n            raise ValueError(\"k is set but coord=None\")\n\n    # global\n\n    if k is None:\n\n        if nvar == 1:\n\n            # univariate\n\n            zdata = (input_array - np.mean(input_array)) / np.std(input_array)\n            outliers = zdata > thr\n\n        else:\n\n            # multivariate (mahalanobis distance)\n\n            zdata = input_array - np.mean(input_array, axis=0)\n            V = np.cov(zdata.T)\n            VI = np.linalg.inv(V)\n            try:\n                VI = np.linalg.inv(V)\n                MD = np.sqrt(np.dot(np.dot(zdata, VI), zdata.T).diagonal())\n            except np.linalg.LinAlgError:\n                MD = np.zeros(input_array.shape)\n            outliers = MD > thr\n\n    # local\n\n    else:\n\n        tree = scipy.spatial.cKDTree(coord)\n        __, inds = tree.query(coord, k=k)\n        outliers = np.empty(shape=0, dtype=bool)\n        for i in range(inds.shape[0]):\n\n            if nvar == 1:\n\n                # univariate\n\n                thisdata = input_array[i]\n                neighbours = input_array[inds[i, 1:]]\n                thiszdata = (thisdata - np.mean(neighbours)) / np.std(\n                    neighbours\n                )\n                outliers = np.append(outliers, thiszdata > thr)\n\n            else:\n\n                # multivariate (mahalanobis distance)\n\n                thisdata = input_array[i, :]\n                neighbours = input_array[inds[i, 1:], :].copy()\n                thiszdata = thisdata - np.mean(neighbours, axis=0)\n                neighbours = neighbours - np.mean(neighbours, axis=0)\n                V = np.cov(neighbours.T)\n                try:\n                    VI = np.linalg.inv(V)\n                    MD = np.sqrt(np.dot(np.dot(thiszdata, VI), thiszdata.T))\n                except np.linalg.LinAlgError:\n                    MD = 0\n                outliers = np.append(outliers, MD > thr)\n\n    if verbose:\n        print(\"--- %i outliers detected ---\" % np.sum(outliers))\n\n    return outliers",
  "def get_method(name, **kwargs):\n    \"\"\"Return a callable function for the utility method corresponding to the\n    given name.\\n\\\n\n    Arrays methods:\n\n    +-------------------+-----------------------------------------------------+\n    |     Name          |              Description                            |\n    +===================+=====================================================+\n    | centred_coord     | compute a 2D coordinate array                       |\n    +-------------------+-----------------------------------------------------+\n\n    Cleansing methods:\n\n    +-------------------+-----------------------------------------------------+\n    |     Name          |              Description                            |\n    +===================+=====================================================+\n    | decluster         | decluster a set of sparse data points               |\n    +-------------------+-----------------------------------------------------+\n    | detect_outliers   | detect outliers in a dataset                        |\n    +-------------------+-----------------------------------------------------+\n\n    Conversion methods:\n\n    +-------------------+-----------------------------------------------------+\n    |     Name          |              Description                            |\n    +===================+=====================================================+\n    | mm/h or rainrate  | convert to rain rate [mm/h]                         |\n    +-------------------+-----------------------------------------------------+\n    | mm or raindepth   | convert to rain depth [mm]                          |\n    +-------------------+-----------------------------------------------------+\n    | dbz or            | convert to reflectivity [dBZ]                       |\n    | reflectivity      |                                                     |\n    +-------------------+-----------------------------------------------------+\n\n    Dimension methods:\n\n    +-------------------+-----------------------------------------------------+\n    |     Name          |              Description                            |\n    +===================+=====================================================+\n    |  accumulate       | aggregate fields in time                            |\n    +-------------------+-----------------------------------------------------+\n    |  clip             | resize the field domain by geographical coordinates |\n    +-------------------+-----------------------------------------------------+\n    |  square           | either pad or crop the data to get a square domain  |\n    +-------------------+-----------------------------------------------------+\n    |  upscale          | upscale the field                                   |\n    +-------------------+-----------------------------------------------------+\n\n    FFT methods (wrappers to different implementations):\n\n    +-------------------+-----------------------------------------------------+\n    |     Name          |              Description                            |\n    +===================+=====================================================+\n    |  numpy            | numpy.fft                                           |\n    +-------------------+-----------------------------------------------------+\n    |  scipy            | scipy.fftpack                                       |\n    +-------------------+-----------------------------------------------------+\n    |  pyfftw           | pyfftw.interfaces.numpy_fft                         |\n    +-------------------+-----------------------------------------------------+\n\n    Image processing methods:\n\n    +-------------------+-----------------------------------------------------+\n    |     Name          |              Description                            |\n    +===================+=====================================================+\n    |  ShiTomasi        | Shi-Tomasi corner detection on an image             |\n    +-------------------+-----------------------------------------------------+\n    |  morph_opening    | filter small scale noise on an image                |\n    +-------------------+-----------------------------------------------------+\n\n    Interpolation methods:\n\n    +-------------------+-----------------------------------------------------+\n    |     Name          |              Description                            |\n    +===================+=====================================================+\n    |  rbfinterp2d      | fast kernel interpolation of a (multivariate) array |\n    |                   | over a 2D grid using a radial basis function        |\n    +-------------------+-----------------------------------------------------+\n\n    Additional keyword arguments are passed to the initializer of the FFT\n    methods, see utils.fft.\n\n    Spectral methods:\n\n    +-------------------+-----------------------------------------------------+\n    |     Name          |              Description                            |\n    +===================+=====================================================+\n    |  rapsd            | Compute radially averaged power spectral density    |\n    +-------------------+-----------------------------------------------------+\n    |  rm_rdisc         | remove the rain / no-rain discontinuity             |\n    +-------------------+-----------------------------------------------------+\n\n    Transformation methods:\n\n    +-------------------+-----------------------------------------------------+\n    |     Name          |              Description                            |\n    +===================+=====================================================+\n    | boxcox or box-cox | one-parameter Box-Cox transform                     |\n    +-------------------+-----------------------------------------------------+\n    | db or decibel     | transform to units of decibel                       |\n    +-------------------+-----------------------------------------------------+\n    | log               | log transform                                       |\n    +-------------------+-----------------------------------------------------+\n    | nqt               | Normal Quantile Transform                           |\n    +-------------------+-----------------------------------------------------+\n    | sqrt              | square-root transform                               |\n    +-------------------+-----------------------------------------------------+\n\n    \"\"\"\n\n    if name is None:\n        name = \"none\"\n\n    name = name.lower()\n\n    def donothing(R, metadata=None, *args, **kwargs):\n        return R.copy(), {} if metadata is None else metadata.copy()\n\n    methods_objects = dict()\n    methods_objects[\"none\"] = donothing\n\n    # arrays methods\n    methods_objects[\"centred_coord\"] = arrays.compute_centred_coord_array\n\n    # cleansing methods\n    methods_objects[\"decluster\"] = cleansing.decluster\n    methods_objects[\"detect_outliers\"] = cleansing.detect_outliers\n\n    # conversion methods\n    methods_objects[\"mm/h\"] = conversion.to_rainrate\n    methods_objects[\"rainrate\"] = conversion.to_rainrate\n    methods_objects[\"mm\"] = conversion.to_raindepth\n    methods_objects[\"raindepth\"] = conversion.to_raindepth\n    methods_objects[\"dbz\"] = conversion.to_reflectivity\n    methods_objects[\"reflectivity\"] = conversion.to_reflectivity\n\n    # dimension methods\n    methods_objects[\"accumulate\"] = dimension.aggregate_fields_time\n    methods_objects[\"clip\"] = dimension.clip_domain\n    methods_objects[\"square\"] = dimension.square_domain\n    methods_objects[\"upscale\"] = dimension.aggregate_fields_space\n\n    # image processing methods\n    methods_objects[\"shitomasi\"] = images.ShiTomasi_detection\n    methods_objects[\"morph_opening\"] = images.morph_opening\n\n    # interpolation methods\n    methods_objects[\"rbfinterp2d\"] = interpolate.rbfinterp2d\n\n    # spectral methods\n    methods_objects[\"rapsd\"] = spectral.rapsd\n    methods_objects[\"rm_rdisc\"] = spectral.remove_rain_norain_discontinuity\n\n    # transformation methods\n    methods_objects[\"boxcox\"] = transformation.boxcox_transform\n    methods_objects[\"box-cox\"] = transformation.boxcox_transform\n    methods_objects[\"db\"] = transformation.dB_transform\n    methods_objects[\"decibel\"] = transformation.dB_transform\n    methods_objects[\"log\"] = transformation.boxcox_transform\n    methods_objects[\"nqt\"] = transformation.NQ_transform\n    methods_objects[\"sqrt\"] = transformation.sqrt_transform\n\n    # FFT methods\n    if name in [\"numpy\", \"pyfftw\", \"scipy\"]:\n        if \"shape\" not in kwargs.keys():\n            raise KeyError(\"mandatory keyword argument shape not given\")\n        return _get_fft_method(name, **kwargs)\n    else:\n        try:\n            return methods_objects[name]\n        except KeyError as e:\n            raise ValueError(\n                \"Unknown method %s\\n\" % e\n                + \"Supported methods:%s\" % str(methods_objects.keys())\n            )",
  "def _get_fft_method(name, **kwargs):\n    kwargs = kwargs.copy()\n    shape = kwargs[\"shape\"]\n    kwargs.pop(\"shape\")\n\n    if name == \"numpy\":\n        return fft.get_numpy(shape, **kwargs)\n    elif name == \"scipy\":\n        return fft.get_scipy(shape, **kwargs)\n    elif name == \"pyfftw\":\n        return fft.get_pyfftw(shape, **kwargs)\n    else:\n        raise ValueError(\n            \"Unknown method {}\\n\".format(name)\n            + \"The available methods are:\"\n            + str([\"numpy\", \"pyfftw\", \"scipy\"])\n        ) from None",
  "def donothing(R, metadata=None, *args, **kwargs):\n        return R.copy(), {} if metadata is None else metadata.copy()",
  "def parse_proj4_string(proj4str):\n    \"\"\"Construct a dictionary from a PROJ.4 projection string.\n\n    Parameters\n    ----------\n    proj4str : str\n      A PROJ.4-compatible projection string.\n\n    Returns\n    -------\n    out : dict\n      Dictionary, where keys and values are parsed from the projection\n      parameter tokens beginning with '+'.\n\n    \"\"\"\n    tokens = proj4str.split('+')\n\n    result = {}\n    for t in tokens[1:]:\n        if '=' in t:\n            k, v = t.split('=')\n            result[k] = v.strip()\n\n    return result",
  "def proj4_to_basemap(proj4str):\n    \"\"\"Convert a PROJ.4 projection string into a dictionary that can be expanded\n    as keyword arguments to mpl_toolkits.basemap.Basemap.__init__.\n\n    Parameters\n    ----------\n    proj4str : str\n        A PROJ.4-compatible projection string.\n\n    Returns\n    -------\n    out : dict\n        The output dictionary.\n\n    \"\"\"\n    pdict = parse_proj4_string(proj4str)\n    odict = {}\n\n    for k, v in list(pdict.items()):\n        if k == \"proj\":\n            # TODO: Make sure that the proj.4 projection type is in all cases\n            # mapped to the corresponding (or closest matching) Basemap\n            # projection.\n            if v == \"somerc\":\n                raise UnsupportedSomercProjection(\"unsupported projection:\"\n                                                  \" somerc\")\n            if v not in [\"latlon\", \"latlong\", \"lonlat\", \"longlat\"]:\n                odict[\"projection\"] = v\n            else:\n                odict[\"projection\"] = \"cyl\"\n        elif k == \"lon_0\" or k == \"lat_0\" or k == \"lat_ts\":\n            # TODO: Check that east/west and north/south hemispheres are\n            # handled correctly.\n            if v[-1] in [\"E\", \"N\", \"S\", \"W\"]:\n                v = v[:-1]\n            odict[k] = float(v)\n        elif k == \"ellps\":\n            odict[k] = v\n        elif k == \"R\":\n            odict[\"rsphere\"] = float(v)\n        elif k in [\"k\", \"k0\"]:\n            odict[\"k_0\"] = float(v)\n\n    return odict",
  "def proj4_to_cartopy(proj4str):\n    \"\"\"Convert a PROJ.4 projection string into a Cartopy coordinate reference\n    system (crs) object.\n\n    Parameters\n    ----------\n    proj4str : str\n        A PROJ.4-compatible projection string.\n\n    Returns\n    -------\n    out : object\n        Instance of a crs class defined in cartopy.crs.\n\n    \"\"\"\n    if not cartopy_imported:\n        raise MissingOptionalDependency(\n            \"cartopy package is required for proj4_to_cartopy function \"\n            \"utility but it is not installed\")\n\n    if not pyproj_imported:\n        raise MissingOptionalDependency(\n            \"pyproj package is required for proj4_to_cartopy function utility \"\n            \"but it is not installed\")\n\n    proj = pyproj.Proj(proj4str)\n\n    if proj.is_latlong():\n        return ccrs.PlateCarree()\n\n    km_proj = {\"lon_0\": \"central_longitude\",\n               \"lat_0\": \"central_latitude\",\n               \"lat_ts\": \"true_scale_latitude\",\n               \"x_0\": \"false_easting\",\n               \"y_0\": \"false_northing\",\n               \"k\": \"scale_factor\",\n               \"zone\": \"zone\"}\n    km_globe = {'a': \"semimajor_axis\",\n                'b': \"semiminor_axis\"}\n    km_std = {\"lat_1\": \"lat_1\",\n              \"lat_2\": \"lat_2\"}\n\n    kw_proj = {}\n    kw_globe = {}\n    kw_std = {}\n\n    for s in proj.srs.split('+'):\n        s = s.split('=')\n        if len(s) != 2:\n            continue\n        k = s[0].strip()\n        v = s[1].strip()\n        try:\n            v = float(v)\n        except Exception:\n            pass\n\n        if k == \"proj\":\n            if v == \"tmerc\":\n                cl = ccrs.TransverseMercator\n            elif v == \"laea\":\n                cl = ccrs.LambertAzimuthalEqualArea\n            elif v == \"lcc\":\n                cl = ccrs.LambertConformal\n            elif v == \"merc\":\n                cl = ccrs.Mercator\n            elif v == \"utm\":\n                cl = ccrs.UTM\n            elif v == \"stere\":\n                cl = ccrs.Stereographic\n            elif v == \"aea\":\n                cl = ccrs.AlbersEqualArea\n            elif v == \"somerc\":\n                raise UnsupportedSomercProjection(\"unsupported projection:\"\n                                                  \" somerc\")\n            else:\n                raise ValueError(\"unsupported projection: %s\" % v)\n        elif k in km_proj:\n            kw_proj[km_proj[k]] = v\n        elif k in km_globe:\n            kw_globe[km_globe[k]] = v\n        elif k in km_std:\n            kw_std[km_std[k]] = v\n\n    globe = None\n    if kw_globe:\n        globe = ccrs.Globe(**kw_globe)\n    if kw_std:\n        kw_proj[\"standard_parallels\"] = (kw_std[\"lat_1\"], kw_std[\"lat_2\"])\n\n    if cl.__name__ == \"Mercator\":\n        kw_proj.pop(\"false_easting\",  None)\n        kw_proj.pop(\"false_northing\", None)\n\n    return cl(globe=globe, **kw_proj)",
  "def reproject_geodata(geodata, t_proj4str, return_grid=None):\n    \"\"\"\n    Reproject geodata and optionally create a grid in a new projection.\n\n    Parameters\n    ----------\n    geodata : dictionary\n        Dictionary containing geographical information about the field.\n        It must contain the attributes projection, x1, x2, y1, y2, xpixelsize,\n        ypixelsize, as defined in the documentation of pysteps.io.importers.\n    t_proj4str: str\n        The target PROJ.4-compatible projection string (fallback).\n    return_grid : {None, 'coords', 'quadmesh'}, optional\n        Whether to return the coordinates of the projected grid.\n        The default return_grid=None does not compute the grid,\n        return_grid='coords' returns the centers of projected grid points,\n        return_grid='quadmesh' returns the coordinates of the quadrilaterals\n        (e.g. to be used by pcolormesh).\n\n    Returns\n    -------\n    geodata : dictionary\n        Dictionary containing the reprojected geographical information\n        and optionally the required X_grid and Y_grid. \\n\n        It also includes a fixed boolean attribute\n        regular_grid=False to indicate\n        that the reprojected grid has no regular spacing.\n    \"\"\"\n    if not pyproj_imported:\n        raise MissingOptionalDependency(\n            \"pyproj package is required for reproject_geodata function utility\"\n            \" but it is not installed\")\n\n    geodata = geodata.copy()\n    s_proj4str = geodata[\"projection\"]\n    extent = (geodata[\"x1\"], geodata[\"x2\"], geodata[\"y1\"], geodata[\"y2\"])\n    shape = (int((geodata[\"y2\"]-geodata[\"y1\"])/geodata[\"ypixelsize\"]),\n             int((geodata[\"x2\"]-geodata[\"x1\"])/geodata[\"xpixelsize\"]))\n\n    s_srs = pyproj.Proj(s_proj4str)\n    t_srs = pyproj.Proj(t_proj4str)\n\n    x1 = extent[0]\n    x2 = extent[1]\n    y1 = extent[2]\n    y2 = extent[3]\n\n    # Reproject grid on fall-back projection\n    if return_grid is not None:\n        if return_grid == \"coords\":\n            y_coord = np.linspace(y1, y2, shape[0]) + geodata[\"ypixelsize\"]/2.0\n            x_coord = np.linspace(x1, x2, shape[1]) + geodata[\"xpixelsize\"]/2.0\n        elif return_grid == \"quadmesh\":\n            y_coord = np.linspace(y1, y2, shape[0] + 1)\n            x_coord = np.linspace(x1, x2, shape[1] + 1)\n        else:\n            raise ValueError(\"unknown return_grid value %s\" % return_grid)\n\n        X, Y = np.meshgrid(x_coord, y_coord)\n\n        X, Y = pyproj.transform(s_srs, t_srs, X.flatten(), Y.flatten())\n        X = X.reshape((y_coord.size, x_coord.size))\n        Y = Y.reshape((y_coord.size, x_coord.size))\n\n    # Reproject extent on fall-back projection\n    x1, y1 = pyproj.transform(s_srs, t_srs, x1, y1)\n    x2, y2 = pyproj.transform(s_srs, t_srs, x2, y2)\n\n    # update geodata\n    geodata[\"projection\"] = t_proj4str\n    geodata[\"x1\"] = x1\n    geodata[\"x2\"] = x2\n    geodata[\"y1\"] = y1\n    geodata[\"y2\"] = y2\n    geodata[\"regular_grid\"] = False\n    geodata[\"xpixelsize\"] = None\n    geodata[\"ypixelsize\"] = None\n    geodata[\"X_grid\"] = X\n    geodata[\"Y_grid\"] = Y\n\n    return geodata",
  "def plot_spectrum1d(fft_freq, fft_power, x_units=None, y_units=None,\n                    wavelength_ticks=None, color='k', lw=1.0, label=None,\n                    ax=None, **kwargs):\n    \"\"\"\n    Function to plot in log-log a radially averaged Fourier spectrum.\n\n    Parameters\n    ----------\n    fft_freq: array-like\n        1d array containing the Fourier frequencies computed by the function\n        'rapsd' in utils/spectral.py\n    fft_power: array-like\n        1d array containing the radially averaged Fourier power spectrum\n        computed by the function 'rapsd' in utils/spectral.py\n    x_units: str, optional\n        Units of the X variable (distance, e.g. km)\n    y_units: str, optional\n        Units of the Y variable (amplitude, e.g. dBR)\n    wavelength_ticks: array-like, optional\n        List of wavelengths where to show xticklabels\n    color: str, optional\n        Line color\n    lw: float, optional\n        Line width\n    label: str, optional\n        Label (for legend)\n    ax: Axes, optional\n        Plot axes\n\n    Returns\n    -------\n    ax: Axes\n        Plot axes\n    \"\"\"\n\n    # Check input dimensions\n    n_freq = len(fft_freq)\n    n_pow = len(fft_power)\n    if n_freq != n_pow:\n        raise ValueError(\"Dimensions of the 1d input arrays must be equal. \"\n                         + \"%s vs %s\" % (n_freq, n_pow))\n\n    if ax is None:\n        ax = plt.subplot(111)\n\n    # Plot spectrum in log-log scale\n    ax.plot(10.0*np.log10(fft_freq), 10.0*np.log10(fft_power),\n            color=color, linewidth=lw, label=label, **kwargs)\n\n    # X-axis\n    if wavelength_ticks is not None:\n        wavelength_ticks = np.array(wavelength_ticks)\n        freq_ticks = 1.0/wavelength_ticks\n        ax.set_xticks(10.0*np.log10(freq_ticks))\n        ax.set_xticklabels(wavelength_ticks)\n        if x_units is not None:\n            ax.set_xlabel('Wavelength [' + x_units + ']')\n    else:\n        if x_units is not None:\n            ax.set_xlabel('Frequency [1/' + x_units + ']')\n\n    # Y-axis\n    if y_units is not None:\n        ax.set_ylabel(r'Power [10log$_{10}(\\frac{' + y_units + '^2}{'\n                      + x_units + '})$]')\n\n    return(ax)",
  "def plot_geography(map, proj4str, extent, shape=None, lw=0.5,\n                   drawlonlatlines=False,\n                   **kwargs):\n    \"\"\"\n    Plot geographical map using either cartopy_ or basemap_ in a chosen projection.\n\n    .. _cartopy: https://scitools.org.uk/cartopy/docs/latest\n\n    .. _basemap: https://matplotlib.org/basemap\n\n    .. _SubplotSpec: https://matplotlib.org/api/_as_gen/matplotlib.gridspec.SubplotSpec.html\n\n    Parameters\n    ----------\n    map : {'cartopy', 'basemap'}\n        The type of basemap.\n    proj4str : str\n        The PROJ.4-compatible projection string.\n    extent: scalars (left, right, bottom, top)\n        The bounding box in proj4str coordinates.\n    shape: scalars (n_rows, n_cols)\n        The dimensions of the image.\n        Only used if needing a fallback projection.\n    lw: float, optional\n        Linewidth of the map (administrative boundaries and coastlines).\n    drawlonlatlines : bool, optional\n        If set to True, draw longitude and latitude lines. Applicable if map is\n        'basemap' or 'cartopy'.\n\n    Other parameters\n    ----------------\n    resolution : str, optional\n        The resolution of the map, see the documentation of\n        `mpl_toolkits.basemap`_.\n        Applicable if map is 'basemap'. Default ``'l'``.\n    scale_args : list, optional\n        If not None, a map scale bar is drawn with basemap_scale_args supplied\n        to mpl_toolkits.basemap.Basemap.drawmapscale. Applicable if map is\n        'basemap'. Default ``None``.\n    scale : {'10m', '50m', '110m'}, optional\n        The scale (resolution) of the map. The available options are '10m',\n        '50m', and '110m'. Applicable if map is 'cartopy'. Default ``'50m'``\n    subplot : tuple or SubplotSpec_ instance, optional\n        The cartopy subplot to plot into. Applicable if map is 'cartopy'.\n        Default ``'(1, 1, 1)'``\n\n    Returns\n    -------\n    ax : fig Axes_\n        Cartopy or Basemap axes.\n    regular_grid : bool\n        Whether the projection allows plotting a regular grid.\n        Returns False in case a fall-back projection is used.\n    \"\"\"\n    if map is not None and map not in [\"basemap\", \"cartopy\"]:\n        raise ValueError(\"unknown map method %s: must be\"\n                         + \" 'basemap' or 'cartopy'\" % map)\n    if map == \"basemap\" and not basemap_imported:\n        raise MissingOptionalDependency(\n            \"map='basemap' option passed to plot_geography function\"\n            \"but the basemap package is not installed\")\n    if map == \"cartopy\" and not cartopy_imported:\n        raise MissingOptionalDependency(\n            \"map='cartopy' option passed to plot_geography function\"\n            \"but the cartopy package is not installed\")\n    if map is not None and not pyproj_imported:\n        raise MissingOptionalDependency(\n            \"map!=None option passed to plot_geography function\"\n            \"but the pyproj package is not installed\")\n\n    if map == \"basemap\":\n        basemap_resolution = kwargs.get(\"resolution\", \"l\")\n        basemap_scale_args = kwargs.get(\"scale_args\", None)\n    if map == \"cartopy\":\n        cartopy_scale = kwargs.get(\"scale\", \"50m\")\n        cartopy_subplot = kwargs.get(\"subplot\", (1, 1, 1))\n\n    if map == \"basemap\":\n        pr = pyproj.Proj(proj4str)\n        x1, x2, y1, y2 = extent[0], extent[1], extent[2], extent[3]\n        ll_lon, ll_lat = pr(x1, y1, inverse=True)\n        ur_lon, ur_lat = pr(x2, y2, inverse=True)\n\n        bm_params = utils.proj4_to_basemap(proj4str)\n        bm_params[\"llcrnrlon\"] = ll_lon\n        bm_params[\"llcrnrlat\"] = ll_lat\n        bm_params[\"urcrnrlon\"] = ur_lon\n        bm_params[\"urcrnrlat\"] = ur_lat\n        bm_params[\"resolution\"] = basemap_resolution\n\n        ax = plot_map_basemap(bm_params,\n                              drawlonlatlines=drawlonlatlines, lw=lw)\n\n        if basemap_scale_args is not None:\n            ax.drawmapscale(*basemap_scale_args, fontsize=6, yoffset=10000)\n    else:\n        crs = utils.proj4_to_cartopy(proj4str)\n\n        ax = plot_map_cartopy(crs, extent, cartopy_scale,\n                              drawlonlatlines=drawlonlatlines, lw=lw,\n                              subplot=cartopy_subplot)\n\n    return ax",
  "def plot_map_basemap(bm_params, drawlonlatlines=False,\n                     coastlinecolor=(0.3, 0.3, 0.3),\n                     countrycolor=(0.3, 0.3, 0.3),\n                     continentcolor=(0.95, 0.95, 0.85),\n                     lakecolor=(0.65, 0.75, 0.9),\n                     rivercolor=(0.65, 0.75, 0.9),\n                     mapboundarycolor=(0.65, 0.75, 0.9),\n                     lw=0.5):\n    \"\"\"\n    Plot coastlines, countries, rivers and meridians/parallels using Basemap.\n\n    Parameters\n    ----------\n    bm_params : optional\n        Optional arguments for the Basemap class constructor:\n        https://basemaptutorial.readthedocs.io/en/latest/basemap.html\n    drawlonlatlines : bool\n        Whether to plot longitudes and latitudes.\n    coastlinecolor : scalars (r, g, b)\n        Coastline color.\n    countrycolor : scalars (r, g, b)\n        Countrycolor color.\n    continentcolor : scalars (r, g, b)\n        Continentcolor color.\n    lakecolor : scalars (r, g, b)\n        Lakecolor color.\n    rivercolor : scalars (r, g, b)\n        Rivercolor color.\n    mapboundarycolor : scalars (r, g, b)\n        Mapboundarycolor color.\n    lw : float\n        Line width.\n\n    Returns\n    -------\n    ax : axes\n        Basemap axes.\n    \"\"\"\n    if not basemap_imported:\n        raise MissingOptionalDependency(\n            \"map='basemap' option passed to plot_map_basemap function\"\n            \"but the basemap package is not installed\")\n\n    ax = Basemap(**bm_params)\n\n    if coastlinecolor is not None:\n        ax.drawcoastlines(color=coastlinecolor, linewidth=lw, zorder=0.1)\n    if countrycolor is not None:\n        ax.drawcountries(color=countrycolor, linewidth=lw, zorder=0.2)\n    if rivercolor is not None:\n        ax.drawrivers(zorder=0.2, color=rivercolor)\n    if continentcolor is not None:\n        ax.fillcontinents(color=continentcolor, lake_color=lakecolor, zorder=0)\n    if mapboundarycolor is not None:\n        ax.drawmapboundary(fill_color=mapboundarycolor, zorder=-1)\n    if drawlonlatlines:\n        ax.drawmeridians(np.linspace(ax.llcrnrlon, ax.urcrnrlon, 10),\n                         color=(0.5, 0.5, 0.5), linewidth=0.25,\n                         labels=[0, 0, 0, 1],\n                         fmt=\"%.1f\", fontsize=6)\n        ax.drawparallels(np.linspace(ax.llcrnrlat, ax.urcrnrlat, 10),\n                         color=(0.5, 0.5, 0.5), linewidth=0.25,\n                         labels=[1, 0, 0, 0],\n                         fmt=\"%.1f\", fontsize=6)\n\n    return ax",
  "def plot_map_cartopy(crs, extent, scale, drawlonlatlines=False,\n                     lw=0.5, subplot=(1, 1, 1)):\n    \"\"\"\n    Plot coastlines, countries, rivers and meridians/parallels using Cartopy.\n\n    Parameters\n    ----------\n    crs : object\n        Instance of a crs class defined in cartopy.crs.\n        It can be created using utils.proj4_to_cartopy.\n    extent : scalars (left, right, bottom, top)\n        The coordinates of the bounding box.\n    drawlonlatlines : bool\n        Whether to plot longitudes and latitudes.\n    scale : {'10m', '50m', '110m'}\n        The scale (resolution) of the map. The available options are '10m',\n        '50m', and '110m'.\n    lw : float\n        Line width.\n    subplot : scalars (nrows, ncols, index)\n        Subplot dimensions (n_rows, n_cols) and subplot number (index).\n\n    Returns\n    -------\n    ax : axes\n        Cartopy axes. Compatible with matplotlib.\n    \"\"\"\n    if not cartopy_imported:\n        raise MissingOptionalDependency(\n            \"map='cartopy' option passed to plot_map_cartopy function\"\n            \"but the cartopy package is not installed\")\n\n    if isinstance(subplot, gridspec.SubplotSpec):\n        ax = plt.subplot(subplot, projection=crs)\n    else:\n        ax = plt.subplot(subplot[0], subplot[1], subplot[2], projection=crs)\n\n    ax.add_feature(\n        cfeature.NaturalEarthFeature(\"physical\", \"ocean\",\n                                     scale=\"50m\" if scale == \"10m\" else scale,\n                                     edgecolor=\"none\",\n                                     facecolor=np.array([0.59375, 0.71484375, 0.8828125])), zorder=0)\n    ax.add_feature(\n        cfeature.NaturalEarthFeature(\"physical\", \"land\",\n                                     scale=scale, edgecolor=\"none\",\n                                     facecolor=np.array([0.9375, 0.9375, 0.859375])), zorder=0)\n    ax.add_feature(\n        cfeature.NaturalEarthFeature(\"physical\", \"coastline\", scale=scale,\n                                     edgecolor=\"black\", facecolor=\"none\",\n                                     linewidth=lw), zorder=2)\n    ax.add_feature(\n        cfeature.NaturalEarthFeature(\"physical\", \"lakes\", scale=scale,\n                                     edgecolor=\"none\",\n                                     facecolor=np.array([0.59375, 0.71484375, 0.8828125])), zorder=0)\n    ax.add_feature(\n        cfeature.NaturalEarthFeature(\"physical\", \"rivers_lake_centerlines\",\n                                     scale=scale,\n                                     edgecolor=np.array([0.59375, 0.71484375, 0.8828125]),\n                                     facecolor=\"none\"), zorder=0)\n    ax.add_feature(\n        cfeature.NaturalEarthFeature(\"cultural\", \"admin_0_boundary_lines_land\",\n                                     scale=scale, edgecolor=\"black\",\n                                     facecolor=\"none\", linewidth=lw), zorder=2)\n\n    if drawlonlatlines:\n        ax.gridlines(crs=ccrs.PlateCarree())\n\n    ax.set_extent(extent, crs)\n\n    return ax",
  "def plot_precip_field(R, type=\"intensity\", map=None, geodata=None,\n                      units='mm/h', bbox=None,\n                      colorscale='pysteps', probthr=None, title=None,\n                      colorbar=True, drawlonlatlines=False, lw=0.5, axis=\"on\",\n                      cax=None, **kwargs):\n    \"\"\"\n    Function to plot a precipitation intensity or probability field with a\n    colorbar.\n\n    .. _Axes: https://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes\n\n    .. _SubplotSpec: https://matplotlib.org/api/_as_gen/matplotlib.gridspec.SubplotSpec.html\n\n    .. _cartopy: https://scitools.org.uk/cartopy/docs/latest\n\n    .. _mpl_toolkits.basemap: https://matplotlib.org/basemap\n\n    Parameters\n    ----------\n    R : array-like\n        Two-dimensional array containing the input precipitation field or an\n        exceedance probability map.\n    type : {'intensity', 'depth', 'prob'}, optional\n        Type of the map to plot: 'intensity' = precipitation intensity field,\n        'depth' = precipitation depth (accumulation) field,\n        'prob' = exceedance probability field.\n    map : {'basemap', 'cartopy'}, optional\n        Optional method for plotting a map: 'basemap' or 'cartopy'. The former\n        uses `mpl_toolkits.basemap`_, while the latter uses cartopy_.\n    geodata : dictionary, optional\n        Optional dictionary containing geographical information about\n        the field. Required is map is not None.\n\n        If geodata is not None, it must contain the following key-value pairs:\n\n        .. tabularcolumns:: |p{1.5cm}|L|\n\n        +-----------------+---------------------------------------------------+\n        |        Key      |                  Value                            |\n        +=================+===================================================+\n        |    projection   | PROJ.4-compatible projection definition           |\n        +-----------------+---------------------------------------------------+\n        |    x1           | x-coordinate of the lower-left corner of the data |\n        |                 | raster (meters)                                   |\n        +-----------------+---------------------------------------------------+\n        |    y1           | y-coordinate of the lower-left corner of the data |\n        |                 | raster (meters)                                   |\n        +-----------------+---------------------------------------------------+\n        |    x2           | x-coordinate of the upper-right corner of the     |\n        |                 | data raster (meters)                              |\n        +-----------------+---------------------------------------------------+\n        |    y2           | y-coordinate of the upper-right corner of the     |\n        |                 | data raster (meters)                              |\n        +-----------------+---------------------------------------------------+\n        |    yorigin      | a string specifying the location of the first     |\n        |                 | element in the data raster w.r.t. y-axis:         |\n        |                 | 'upper' = upper border, 'lower' = lower border    |\n        +-----------------+---------------------------------------------------+\n    units : {'mm/h', 'mm', 'dBZ'}, optional\n        Units of the input array. If type is 'prob', this specifies the unit of\n        the intensity threshold.\n    bbox : tuple, optional\n        Four-element tuple specifying the coordinates of the bounding box. Use\n        this for plotting a subdomain inside the input grid. The coordinates are\n        of the form (lower left x,lower left y,upper right x,upper right y). If\n        map is not None, the x- and y-coordinates are longitudes and latitudes.\n        Otherwise they represent image pixels.\n    colorscale : {'pysteps', 'STEPS-BE', 'BOM-RF3'}, optional\n        Which colorscale to use. Applicable if units is 'mm/h', 'mm' or 'dBZ'.\n    probthr : float, optional\n        Intensity threshold to show in the color bar of the exceedance\n        probability map.\n        Required if type is \"prob\" and colorbar is True.\n    title : str, optional\n        If not None, print the title on top of the plot.\n    colorbar : bool, optional\n        If set to True, add a colorbar on the right side of the plot.\n    drawlonlatlines : bool, optional\n        If set to True, draw longitude and latitude lines. Applicable if map is\n        'basemap' or 'cartopy'.\n    lw: float, optional\n        Linewidth of the map (administrative boundaries and coastlines).\n    axis : {'off','on'}, optional\n        Whether to turn off or on the x and y axis.\n    cax : Axes_ object, optional\n        Axes into which the colorbar will be drawn. If no axes is provided\n        the colorbar axes are created next to the plot.\n\n    Other parameters\n    ----------------\n    Optional parameters are contained in **kwargs. See basemaps.plot_geography.\n\n    Returns\n    -------\n    ax : fig Axes_\n        Figure axes. Needed if one wants to add e.g. text inside the plot.\n\n    \"\"\"\n    if type not in [\"intensity\", \"depth\", \"prob\"]:\n        raise ValueError(\"invalid type '%s', must be \"\n                         + \"'intensity', 'depth' or 'prob'\" % type)\n    if units not in [\"mm/h\", \"mm\", \"dBZ\"]:\n        raise ValueError(\"invalid units '%s', must be \"\n                         + \"'mm/h', 'mm' or 'dBZ'\" % units)\n    if type == \"prob\" and colorbar and probthr is None:\n        raise ValueError(\"type='prob' but probthr not specified\")\n    if map is not None and geodata is None:\n        raise ValueError(\"map!=None but geodata=None\")\n    if len(R.shape) != 2:\n        raise ValueError(\"the input is not two-dimensional array\")\n\n    # get colormap and color levels\n    cmap, norm, clevs, clevsStr = get_colormap(type, units, colorscale)\n\n    # extract extent and origin\n    if geodata is not None:\n        field_extent = (geodata['x1'], geodata['x2'], geodata['y1'], geodata['y2'])\n        if bbox is None:\n            bm_extent = field_extent\n        else:\n            if not PYPROJ_IMPORTED:\n                raise MissingOptionalDependency(\n                    \"pyproj package is required to import \"\n                    \"FMI's radar reflectivity composite \"\n                    \"but it is not installed\"\n                )\n            pr = pyproj.Proj(geodata[\"projection\"])\n            x1, y1 = pr(bbox[0], bbox[1])\n            x2, y2 = pr(bbox[2], bbox[3])\n            bm_extent = (x1, x2, y1, y2)\n        origin = geodata[\"yorigin\"]\n    else:\n        field_extent = (0, R.shape[1]-1, 0, R.shape[0]-1)\n        origin = \"upper\"\n\n    # plot geography\n    if map is not None:\n        try:\n            ax = basemaps.plot_geography(map, geodata[\"projection\"],\n                                    bm_extent, R.shape, lw,\n                                    drawlonlatlines, **kwargs)\n            regular_grid = True\n        except UnsupportedSomercProjection:\n            # Define default fall-back projection for Swiss data(EPSG:3035)\n            # This will work reasonably well for Europe only.\n            t_proj4str = \"+proj=laea +lat_0=52 +lon_0=10 \"\n            t_proj4str += \"+x_0=4321000 +y_0=3210000 +ellps=GRS80 \"\n            t_proj4str += \"+units=m +no_defs\"\n            geodata = utils.reproject_geodata(geodata, t_proj4str,\n                                              return_grid=\"quadmesh\")\n            bm_extent = (geodata['x1'], geodata['x2'],\n                         geodata['y1'], geodata['y2'])\n            X, Y = geodata[\"X_grid\"], geodata[\"Y_grid\"]\n            regular_grid = geodata[\"regular_grid\"]\n\n            ax = basemaps.plot_geography(map, geodata[\"projection\"],\n                                         bm_extent, R.shape, lw,\n                                         drawlonlatlines, **kwargs)\n    else:\n        regular_grid = True\n\n    if bbox is not None and map is not None:\n        x1, y1 = pr(geodata[\"x1\"], geodata[\"y1\"], inverse=True)\n        x2, y2 = pr(geodata[\"x2\"], geodata[\"y2\"], inverse=True)\n        if map == \"basemap\":\n            x1, y1 = ax(x1, y1)\n            x2, y2 = ax(x2, y2)\n        else:\n            x1, y1 = pr(x1, y1)\n            x2, y2 = pr(x2, y2)\n        field_extent = (x1, x2, y1, y2)\n\n    # plot rainfield\n    if regular_grid:\n        ax = plt.gca()\n        im = _plot_field(R, ax, type, units, colorscale,\n                         extent=field_extent, origin=origin)\n    else:\n        if origin == \"upper\":\n            Y = np.flipud(Y)\n        im = _plot_field_pcolormesh(X, Y, R, ax, type, units, colorscale)\n\n    # plot radar domain mask\n    mask = np.ones(R.shape)\n    mask[~np.isnan(R)] = np.nan  # Fully transparent within the radar domain\n    ax.imshow(mask, cmap=colors.ListedColormap(['gray']), alpha=0.5,\n              zorder=1e6, extent=field_extent, origin=origin)\n\n    # ax.pcolormesh(X, Y, np.flipud(mask),\n    #               cmap=colors.ListedColormap(['gray']),\n    #               alpha=0.5, zorder=1e6)\n    # TODO: pcolormesh doesn't work properly with the alpha parameter\n\n    if title is not None:\n        plt.title(title)\n\n    # add colorbar\n    if colorbar:\n        if type in [\"intensity\", \"depth\"]:\n            extend = \"max\"\n        else:\n            extend = \"neither\"\n        cbar = plt.colorbar(im, ticks=clevs, spacing='uniform', norm=norm,\n                            extend=extend,\n                            shrink=0.8, cax=cax)\n        if clevsStr is not None:\n            cbar.ax.set_yticklabels(clevsStr)\n\n        if type == \"intensity\":\n            cbar.ax.set_title(units, fontsize=10)\n            cbar.set_label(\"Precipitation intensity\")\n        elif type == \"depth\":\n            cbar.ax.set_title(units, fontsize=10)\n            cbar.set_label(\"Precipitation depth\")\n        else:\n            cbar.set_label(\"P(R > %.1f %s)\" % (probthr, units))\n\n    if map is None and bbox is not None:\n        ax = plt.gca()\n        ax.set_xlim(bbox[0], bbox[2])\n        ax.set_ylim(bbox[1], bbox[3])\n\n    if geodata is None or axis == \"off\":\n        axes = plt.gca()\n        axes.xaxis.set_ticks([])\n        axes.xaxis.set_ticklabels([])\n        axes.yaxis.set_ticks([])\n        axes.yaxis.set_ticklabels([])\n\n    return plt.gca()",
  "def _plot_field(R, ax, type, units, colorscale, extent, origin=None):\n    R = R.copy()\n\n    # Get colormap and color levels\n    cmap, norm, clevs, clevsStr = get_colormap(type, units, colorscale)\n\n    # Plot precipitation field\n    # transparent where no precipitation or the probability is zero\n    if type in [\"intensity\", \"depth\"]:\n        if units in ['mm/h', 'mm']:\n            R[R < 0.1] = np.nan\n        elif units == 'dBZ':\n            R[R < 10] = np.nan\n    else:\n        R[R < 1e-3] = np.nan\n\n    vmin, vmax = [None, None] if type in [\"intensity\", \"depth\"] else [0.0, 1.0]\n\n    im = ax.imshow(R, cmap=cmap, norm=norm, extent=extent,\n                   interpolation='nearest',\n                   vmin=vmin, vmax=vmax, origin=origin, zorder=1)\n\n    return im",
  "def _plot_field_pcolormesh(X, Y, R, ax, type, units, colorscale):\n    R = R.copy()\n\n    # Get colormap and color levels\n    cmap, norm, clevs, clevsStr = get_colormap(type, units, colorscale)\n\n    # Plot precipitation field\n    # transparent where no precipitation or the probability is zero\n    if type in [\"intensity\", \"depth\"]:\n        if units in ['mm/h', 'mm']:\n            R[R < 0.1] = np.nan\n        elif units == 'dBZ':\n            R[R < 10] = np.nan\n    else:\n        R[R < 1e-3] = np.nan\n\n    vmin, vmax = [None, None] if type in [\"intensity\", \"depth\"] else [0.0, 1.0]\n\n    im = ax.pcolormesh(X, Y, R, cmap=cmap, norm=norm,\n                       vmin=vmin, vmax=vmax, zorder=1)\n\n    return im",
  "def get_colormap(type, units='mm/h', colorscale='pysteps'):\n    \"\"\"Function to generate a colormap (cmap) and norm.\n\n    Parameters\n    ----------\n    type : {'intensity', 'depth', 'prob'}, optional\n        Type of the map to plot: 'intensity' = precipitation intensity field,\n        'depth' = precipitation depth (accumulation) field,\n        'prob' = exceedance probability field.\n    units : {'mm/h', 'mm', 'dBZ'}, optional\n        Units of the input array. If type is 'prob', this specifies the unit of\n        the intensity threshold.\n    colorscale : {'pysteps', 'STEPS-BE', 'BOM-RF3'}, optional\n        Which colorscale to use. Applicable if units is 'mm/h', 'mm' or 'dBZ'.\n\n    Returns\n    -------\n    cmap : Colormap instance\n        colormap\n    norm : colors.Normalize object\n        Colors norm\n    clevs: list(float)\n        List of precipitation values defining the color limits.\n    clevsStr: list(str)\n        List of precipitation values defining the color limits (with correct\n        number of decimals).\n\n    \"\"\"\n    if type in [\"intensity\", \"depth\"]:\n        # Get list of colors\n        color_list, clevs, clevsStr = _get_colorlist(units, colorscale)\n\n        cmap = colors.LinearSegmentedColormap.from_list(\"cmap\",\n                                                        color_list,\n                                                        len(clevs)-1)\n\n        if colorscale == 'BOM-RF3':\n            cmap.set_over('black', 1)\n        if colorscale == 'pysteps':\n            cmap.set_over('darkred', 1)\n        if colorscale == 'STEPS-BE':\n            cmap.set_over('black', 1)\n        norm = colors.BoundaryNorm(clevs, cmap.N)\n\n        return cmap, norm, clevs, clevsStr\n\n    elif type == \"prob\":\n        cmap = plt.get_cmap(\"OrRd\", 10)\n        return cmap, colors.Normalize(vmin=0, vmax=1), None, None\n    else:\n        return cm.jet, colors.Normalize(), None, None",
  "def _get_colorlist(units='mm/h', colorscale='pysteps'):\n    \"\"\"\n    Function to get a list of colors to generate the colormap.\n\n    Parameters\n    ----------\n    units : str\n        Units of the input array (mm/h, mm or dBZ)\n    colorscale : str\n        Which colorscale to use (BOM-RF3, pysteps, STEPS-BE)\n\n    Returns\n    -------\n    color_list : list(str)\n        List of color strings.\n\n    clevs : list(float)\n        List of precipitation values defining the color limits.\n\n    clevsStr : list(str)\n        List of precipitation values defining the color limits\n        (with correct number of decimals).\n\n    \"\"\"\n\n    if colorscale == \"BOM-RF3\":\n        color_list = np.array([(255, 255, 255),  # 0.0\n                               (245, 245, 255),  # 0.2\n                               (180, 180, 255),  # 0.5\n                               (120, 120, 255),  # 1.5\n                               (20,  20, 255),   # 2.5\n                               (0, 216, 195),    # 4.0\n                               (0, 150, 144),    # 6.0\n                               (0, 102, 102),    # 10\n                               (255, 255,   0),  # 15\n                               (255, 200,   0),  # 20\n                               (255, 150,   0),  # 30\n                               (255, 100,   0),  # 40\n                               (255,   0,   0),  # 50\n                               (200,   0,   0),  # 60\n                               (120,   0,   0),  # 75\n                               (40,   0,   0)])  # > 100\n        color_list = color_list/255.\n        if units == 'mm/h':\n            clevs = [0., 0.2, 0.5, 1.5, 2.5, 4, 6, 10, 15, 20, 30, 40, 50, 60,\n                     75, 100, 150]\n        elif units == \"mm\":\n            clevs = [0., 0.2, 0.5, 1.5, 2.5, 4, 5, 7, 10, 15, 20, 25, 30, 35,\n                     40, 45, 50]\n        else:\n            raise ValueError('Wrong units in get_colorlist: %s' % units)\n    elif colorscale == 'pysteps':\n        # pinkHex = '#%02x%02x%02x' % (232, 215, 242)\n        redgreyHex = '#%02x%02x%02x' % (156, 126, 148)\n        color_list = [redgreyHex, \"#640064\", \"#AF00AF\", \"#DC00DC\", \"#3232C8\",\n                      \"#0064FF\", \"#009696\", \"#00C832\", \"#64FF00\", \"#96FF00\",\n                      \"#C8FF00\", \"#FFFF00\", \"#FFC800\", \"#FFA000\", \"#FF7D00\",\n                      \"#E11900\"]\n        if units in ['mm/h', 'mm']:\n            clevs = [0.08, 0.16, 0.25, 0.40, 0.63, 1, 1.6, 2.5, 4, 6.3, 10,\n                     16, 25, 40, 63, 100, 160]\n        elif units == 'dBZ':\n            clevs = np.arange(10, 65, 5)\n        else:\n            raise ValueError('Wrong units in get_colorlist: %s' % units)\n    elif colorscale == 'STEPS-BE':\n        color_list = ['cyan', 'deepskyblue', 'dodgerblue', 'blue',\n                      'chartreuse', 'limegreen', 'green', 'darkgreen',\n                      'yellow', 'gold', 'orange', 'red', 'magenta',\n                      'darkmagenta']\n        if units in ['mm/h', 'mm']:\n            clevs = [0.1, 0.25, 0.4, 0.63, 1, 1.6, 2.5, 4, 6.3, 10, 16, 25, 40,\n                     63, 100]\n        elif units == 'dBZ':\n            clevs = np.arange(10, 65, 5)\n        else:\n            raise ValueError('Wrong units in get_colorlist: %s' % units)\n\n    else:\n        print('Invalid colorscale', colorscale)\n        raise ValueError(\"Invalid colorscale \" + colorscale)\n\n    # Generate color level strings with correct amount of decimal places\n    clevsStr = []\n    clevsStr = _dynamic_formatting_floats(clevs, )\n\n    return color_list, clevs, clevsStr",
  "def _dynamic_formatting_floats(floatArray, colorscale='pysteps'):\n    \"\"\"\n    Function to format the floats defining the class limits of the colorbar.\n    \"\"\"\n    floatArray = np.array(floatArray, dtype=float)\n\n    labels = []\n    for label in floatArray:\n        if label >= 0.1 and label < 1:\n            if colorscale == 'pysteps':\n                formatting = ',.2f'\n            else:\n                formatting = ',.1f'\n        elif label >= 0.01 and label < 0.1:\n            formatting = ',.2f'\n        elif label >= 0.001 and label < 0.01:\n            formatting = ',.3f'\n        elif label >= 0.0001 and label < 0.001:\n            formatting = ',.4f'\n        elif label >= 1 and label.is_integer():\n            formatting = 'i'\n        else:\n            formatting = ',.1f'\n\n        if formatting != 'i':\n            labels.append(format(label, formatting))\n        else:\n            labels.append(str(int(label)))\n\n    return labels",
  "def quiver(UV, ax=None, map=None, geodata=None, drawlonlatlines=False,\n           basemap_resolution='l', cartopy_scale=\"50m\", lw=0.5,\n           cartopy_subplot=(1, 1, 1), axis=\"on\", **kwargs):\n    \"\"\"Function to plot a motion field as arrows.\n\n    Parameters\n    ----------\n    UV : array-like\n        Array of shape (2,m,n) containing the input motion field.\n    ax : axis object\n        Optional axis object to use for plotting.\n    map : {'basemap', 'cartopy'}, optional\n        Optional method for plotting a map: 'basemap' or 'cartopy'. The former\n        uses `mpl_toolkits.basemap`_, while the latter uses cartopy_.\n    geodata : dictionary\n        Optional dictionary containing geographical information about\n        the field.\n        If geodata is not None, it must contain the following key-value pairs:\n    drawlonlatlines : bool, optional\n        If set to True, draw longitude and latitude lines. Applicable if map is\n        'basemap' or 'cartopy'.\n    basemap_resolution : str, optional\n        The resolution of the basemap, see the documentation of\n        `mpl_toolkits.basemap`_.\n        Applicable if map is 'basemap'.\n    cartopy_scale : {'10m', '50m', '110m'}, optional\n        The scale (resolution) of the map. The available options are '10m',\n        '50m', and '110m'. Applicable if map is 'cartopy'.\n    lw: float, optional\n        Linewidth of the map (administrative boundaries and coastlines).\n    cartopy_subplot : tuple or SubplotSpec_ instance, optional\n        Cartopy subplot. Applicable if map is 'cartopy'.\n    axis : {'off','on'}, optional\n        Whether to turn off or on the x and y axis.\n\n        .. tabularcolumns:: |p{1.5cm}|L|\n\n        +----------------+----------------------------------------------------+\n        |        Key     |                  Value                             |\n        +================+====================================================+\n        |   projection   | PROJ.4-compatible projection definition            |\n        +----------------+----------------------------------------------------+\n        |    x1          | x-coordinate of the lower-left corner of the data  |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    y1          | y-coordinate of the lower-left corner of the data  |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    x2          | x-coordinate of the upper-right corner of the data |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    y2          | y-coordinate of the upper-right corner of the data |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    yorigin     | a string specifying the location of the first      |\n        |                | element in the data raster w.r.t. y-axis:          |\n        |                | 'upper' = upper border, 'lower' = lower border     |\n        +----------------+----------------------------------------------------+\n\n    Other Parameters\n    ----------------\n    step : int\n        Optional resample step to control the density of the arrows.\n        Default : 20\n    color : string\n        Optional color of the arrows. This is a synonym for the PolyCollection\n        facecolor kwarg in matplotlib.collections.\n        Default : black\n\n    Returns\n    -------\n    out : axis object\n        Figure axes. Needed if one wants to add e.g. text inside the plot.\n\n    \"\"\"\n    if map is not None and geodata is None:\n        raise ValueError(\"map!=None but geodata=None\")\n\n    # defaults\n    step = kwargs.get(\"step\", 20)\n\n    quiver_keys = [\"scale\", \"scale_units\", \"width\", \"headwidth\", \"headlength\",\n                   \"headaxislength\", \"minshaft\", \"minlength\", \"pivot\", \"color\"]\n    kwargs_quiver = {k: kwargs[k] for k in set(quiver_keys).intersection(kwargs)}\n\n    kwargs_quiver[\"color\"] = kwargs.get(\"color\", \"black\")\n\n    # prepare x y coordinates\n    reproject = False\n    if geodata is not None:\n        x = np.linspace(geodata['x1'], geodata['x2'],\n                        UV.shape[2]) + geodata[\"xpixelsize\"]/2.0\n        y = np.linspace(geodata['y1'], geodata['y2'],\n                        UV.shape[1]) + geodata[\"ypixelsize\"]/2.0\n        extent = (geodata['x1'], geodata['x2'], geodata['y1'], geodata['y2'])\n\n        # check geodata and project if different from axes\n        if ax is not None and map is None:\n            if type(ax).__name__ == 'GeoAxesSubplot':\n                try:\n                    ccrs = utils.proj4_to_cartopy(geodata[\"projection\"])\n                except UnsupportedSomercProjection:\n                    # Define fall-back projection for Swiss data(EPSG:3035)\n                    # This will work reasonably well for Europe only.\n                    t_proj4str = \"+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs\"\n                    reproject = True\n            elif type(ax).__name__ == 'Basemap':\n                utils.proj4_to_basemap(geodata[\"projection\"])\n\n            if reproject:\n                geodata = utils.reproject_geodata(geodata, t_proj4str,\n                                                  return_grid=\"coords\")\n                extent = (geodata['x1'], geodata['x2'],\n                          geodata['y1'], geodata['y2'])\n                X, Y = geodata[\"X_grid\"], geodata[\"Y_grid\"]\n    else:\n        x = np.arange(UV.shape[2])\n        y = np.arange(UV.shape[1])\n\n    if not reproject:\n        X, Y = np.meshgrid(x, y)\n\n    # draw basemaps\n    if map is not None:\n        try:\n            ax = basemaps.plot_geography(map, geodata[\"projection\"],\n                                         extent, UV.shape[1:], drawlonlatlines,\n                                         basemap_resolution,\n                                         cartopy_scale, lw, cartopy_subplot)\n        except UnsupportedSomercProjection:\n            # Define default fall-back projection for Swiss data(EPSG:3035)\n            # This will work reasonably well for Europe only.\n            t_proj4str = \"+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs\"\n            geodata = utils.reproject_geodata(geodata, t_proj4str,\n                                              return_grid=\"coords\")\n            extent = (geodata['x1'], geodata['x2'],\n                      geodata['y1'], geodata['y2'])\n            X, Y = geodata[\"X_grid\"], geodata[\"Y_grid\"]\n\n            ax = basemaps.plot_geography(map, geodata[\"projection\"],\n                                         extent, UV.shape[1:], drawlonlatlines,\n                                         basemap_resolution,\n                                         cartopy_scale, lw, cartopy_subplot)\n    else:\n        ax = plt.gca()\n\n    # reduce number of vectors to plot\n    skip = (slice(None, None, step), slice(None, None, step))\n    dx = UV[0, :, :]\n    dy = UV[1, :, :]\n\n    # plot quiver\n    ax.quiver(X[skip], np.flipud(Y[skip]), dx[skip], -dy[skip], angles=\"xy\",\n              zorder=1e6, **kwargs_quiver)\n\n    if geodata is None or axis == \"off\":\n        axes = plt.gca()\n        axes.xaxis.set_ticks([])\n        axes.xaxis.set_ticklabels([])\n        axes.yaxis.set_ticks([])\n        axes.yaxis.set_ticklabels([])\n\n    return plt.gca()",
  "def streamplot(UV, ax=None, map=None, geodata=None, drawlonlatlines=False,\n               basemap_resolution='l', cartopy_scale=\"50m\", lw=0.5,\n               cartopy_subplot=(1, 1, 1), axis=\"on\", **kwargs):\n    \"\"\"Function to plot a motion field as streamlines.\n\n    Parameters\n    ----------\n    UV : array-like\n        Array of shape (2, m,n) containing the input motion field.\n    ax : axis object\n        Optional axis object to use for plotting.\n    map : {'basemap', 'cartopy'}, optional\n        Optional method for plotting a map: 'basemap' or 'cartopy'. The former\n        uses `mpl_toolkits.basemap`_, while the latter uses cartopy_.\n    geodata : dictionary\n        Optional dictionary containing geographical information about\n        the field.\n        If geodata is not None, it must contain the following key-value pairs:\n    drawlonlatlines : bool, optional\n        If set to True, draw longitude and latitude lines. Applicable if map is\n        'basemap' or 'cartopy'.\n    basemap_resolution : str, optional\n        The resolution of the basemap, see the documentation of\n        `mpl_toolkits.basemap`_.\n        Applicable if map is 'basemap'.\n    cartopy_scale : {'10m', '50m', '110m'}, optional\n        The scale (resolution) of the map. The available options are '10m',\n        '50m', and '110m'. Applicable if map is 'cartopy'.\n    lw: float, optional\n        Linewidth of the map (administrative boundaries and coastlines).\n    cartopy_subplot : tuple or SubplotSpec_ instance, optional\n        Cartopy subplot. Applicable if map is 'cartopy'.\n    axis : {'off','on'}, optional\n        Whether to turn off or on the x and y axis.\n\n        .. tabularcolumns:: |p{1.5cm}|L|\n\n        +----------------+----------------------------------------------------+\n        |        Key     |                  Value                             |\n        +================+====================================================+\n        |   projection   | PROJ.4-compatible projection definition            |\n        +----------------+----------------------------------------------------+\n        |    x1          | x-coordinate of the lower-left corner of the data  |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    y1          | y-coordinate of the lower-left corner of the data  |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    x2          | x-coordinate of the upper-right corner of the data |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    y2          | y-coordinate of the upper-right corner of the data |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    yorigin     | a string specifying the location of the first      |\n        |                | element in the data raster w.r.t. y-axis:          |\n        |                | 'upper' = upper border, 'lower' = lower border     |\n        +----------------+----------------------------------------------------+\n\n    Other Parameters\n    ----------------\n    density : float\n        Controls the closeness of streamlines.\n        Default : 1.5\n    color : string\n        Optional streamline color. This is a synonym for the PolyCollection\n        facecolor kwarg in matplotlib.collections.\n        Default : black\n\n    Returns\n    -------\n    out : axis object\n        Figure axes. Needed if one wants to add e.g. text inside the plot.\n\n    \"\"\"\n    if map is not None and geodata is None:\n        raise ValueError(\"map!=None but geodata=None\")\n\n    # defaults\n    density = kwargs.get(\"density\", 1.5)\n    color = kwargs.get(\"color\", \"black\")\n\n    # prepare x y coordinates\n    reproject = False\n    if geodata is not None:\n        x = np.linspace(geodata['x1'], geodata['x2'],\n                        UV.shape[2]) + geodata[\"xpixelsize\"]/2.0\n        y = np.linspace(geodata['y1'], geodata['y2'],\n                        UV.shape[1]) + geodata[\"ypixelsize\"]/2.0\n        extent = (geodata['x1'], geodata['x2'],\n                  geodata['y1'], geodata['y2'])\n\n        # check geodata and project if different from axes\n        if ax is not None and map is None:\n            if type(ax).__name__ == 'GeoAxesSubplot':\n                try:\n                    ccrs = utils.proj4_to_cartopy(geodata[\"projection\"])\n                except UnsupportedSomercProjection:\n                    # Define fall-back projection for Swiss data(EPSG:3035)\n                    # This will work reasonably well for Europe only.\n                    t_proj4str = \"+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs\"\n                    reproject = True\n            elif type(ax).__name__ == 'Basemap':\n                utils.proj4_to_basemap(geodata[\"projection\"])\n\n            if reproject:\n                geodata = utils.reproject_geodata(geodata, t_proj4str,\n                                                  return_grid=\"coords\")\n                extent = (geodata['x1'], geodata['x2'],\n                          geodata['y1'], geodata['y2'])\n                X, Y = geodata[\"X_grid\"], geodata[\"Y_grid\"]\n    else:\n        x = np.arange(UV.shape[2])\n        y = np.arange(UV.shape[1])\n\n    if not reproject:\n        X, Y = np.meshgrid(x, y)\n\n    # draw basemaps\n    if map is not None:\n        try:\n            ax = basemaps.plot_geography(map, geodata[\"projection\"],\n                                         extent, UV.shape[1:], drawlonlatlines,\n                                         basemap_resolution,\n                                         cartopy_scale, lw, cartopy_subplot)\n        except UnsupportedSomercProjection:\n            # Define default fall-back projection for Swiss data(EPSG:3035)\n            # This will work reasonably well for Europe only.\n            t_proj4str = \"+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs\"\n            geodata = utils.reproject_geodata(geodata, t_proj4str,\n                                              return_grid=\"coords\")\n            extent = (geodata['x1'], geodata['x2'],\n                      geodata['y1'], geodata['y2'])\n            X, Y = geodata[\"X_grid\"], geodata[\"Y_grid\"]\n\n            ax = basemaps.plot_geography(map, geodata[\"projection\"],\n                                         extent, UV.shape[1:], drawlonlatlines,\n                                         basemap_resolution,\n                                         cartopy_scale, lw, cartopy_subplot)\n    else:\n        ax = plt.gca()\n\n    # plot streamplot\n    ax.streamplot(x, np.flipud(y), UV[0, :, :], -UV[1, :, :], density=density,\n                  color=color, zorder=1e6)\n\n    if geodata is None or axis == \"off\":\n        axes = plt.gca()\n        axes.xaxis.set_ticks([])\n        axes.xaxis.set_ticklabels([])\n        axes.yaxis.set_ticks([])\n        axes.yaxis.set_ticklabels([])\n\n    return plt.gca()",
  "def animate(R_obs, nloops=2, timestamps=None, R_fct=None, timestep_min=5,\n            UV=None, motion_plot=\"quiver\", geodata=None, map=None,\n            colorscale=\"pysteps\", units=\"mm/h\", colorbar=True, type=\"ensemble\",\n            prob_thr=None, plotanimation=True, savefig=False,\n            fig_dpi=150, fig_format=\"png\", path_outputs=\"\", **kwargs):\n    \"\"\"Function to animate observations and forecasts in pysteps.\n\n    Parameters\n    ----------\n    R_obs : array-like\n        Three-dimensional array containing the time series of observed\n        precipitation fields.\n\n    Other parameters\n    ----------------\n    nloops : int\n        Optional, the number of loops in the animation.\n    R_fct : array-like\n        Optional, the three or four-dimensional (for ensembles) array\n        containing the time series of forecasted precipitation field.\n    timestep_min : float\n        The time resolution in minutes of the forecast.\n    UV : array-like\n        Optional, the motion field used for the forecast.\n    motion_plot : string\n        The method to plot the motion field.\n    geodata : dictionary\n        Optional dictionary containing geographical information about\n        the field.\n        If geodata is not None, it must contain the following key-value pairs:\n\n        .. tabularcolumns:: |p{1.5cm}|L|\n\n        +----------------+----------------------------------------------------+\n        |        Key     |                  Value                             |\n        +================+====================================================+\n        |   projection   | PROJ.4-compatible projection definition            |\n        +----------------+----------------------------------------------------+\n        |    x1          | x-coordinate of the lower-left corner of the data  |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    y1          | y-coordinate of the lower-left corner of the data  |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    x2          | x-coordinate of the upper-right corner of the data |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    y2          | y-coordinate of the upper-right corner of the data |\n        |                | raster (meters)                                    |\n        +----------------+----------------------------------------------------+\n        |    yorigin     | a string specifying the location of the first      |\n        |                | element in the data raster w.r.t. y-axis:          |\n        |                | 'upper' = upper border, 'lower' = lower border     |\n        +----------------+----------------------------------------------------+\n\n    map : str\n        Optional method for plotting a map.\n        See pysteps.visualization.precipifields.plot_precip.field.\n    units : str\n        Units of the input array (mm/h or dBZ)\n    colorscale : str\n        Which colorscale to use.\n    title : str\n        If not None, print the title on top of the plot.\n    colorbar : bool\n        If set to True, add a colorbar on the right side of the plot.\n    type : {'ensemble', 'mean', 'prob'}, str\n        Type of the map to animate. 'ensemble' = ensemble members,\n        'mean' = ensemble mean, 'prob' = exceedance probability\n        (using threshold defined in prob_thrs).\n    prob_thr : float\n        Intensity threshold for the exceedance probability maps. Applicable\n        if type = 'prob'.\n    plotanimation : bool\n        If set to True, visualize the animation (useful when one is only\n        interested in saving the individual frames).\n    savefig : bool\n        If set to True, save the individual frames to path_outputs.\n    fig_dpi : scalar > 0\n        Resolution of the output figures, see the documentation of\n        matplotlib.pyplot.savefig. Applicable if savefig is True.\n    path_outputs : string\n        Path to folder where to save the frames.\n    kwargs : dict\n        Optional keyword arguments that are supplied to plot_precip_field\n        and quiver/streamplot.\n\n    Returns\n    -------\n    ax : fig axes\n        Figure axes. Needed if one wants to add e.g. text inside the plot.\n    \"\"\"\n    if timestamps is not None:\n        startdate_str = timestamps[-1].strftime(\"%Y%m%d%H%M\")\n    else:\n        startdate_str = None\n\n    if R_fct is not None:\n        if len(R_fct.shape) == 3:\n            R_fct = R_fct[None, :, :, :]\n\n    if R_fct is not None:\n        n_lead_times = R_fct.shape[1]\n        n_members = R_fct.shape[0]\n    else:\n        n_lead_times = 0\n        n_members = 1\n\n    if type == \"prob\" and prob_thr is None:\n        raise ValueError(\"type 'prob' needs a prob_thr value\")\n\n    if type != \"ensemble\":\n        n_members = 1\n\n    n_obs = R_obs.shape[0]\n\n    loop = 0\n    while loop < nloops:\n        for n in range(n_members):\n            for i in range(n_obs + n_lead_times):\n                plt.clf()\n\n                # Observations\n                if i < n_obs and (plotanimation or n == 0):\n\n                    title = \"\"\n                    if timestamps is not None:\n                        title += timestamps[i].strftime(\"%Y-%m-%d %H:%M\\n\")\n\n                    plt.clf()\n                    if type == \"prob\":\n                        title += \"Observed Probability\"\n                        R_obs_ = R_obs[np.newaxis, ::]\n                        P_obs = st.postprocessing.ensemblestats.excprob(R_obs_[:, i, :, :], prob_thr)\n                        ax = st.plt.plot_precip_field(P_obs, type=\"prob\",\n                                                      map=map,\n                                                      geodata=geodata,\n                                                      units=units,\n                                                      probthr=prob_thr,\n                                                      title=title,\n                                                      **kwargs)\n                    else:\n                        title += \"Observed Rainfall\"\n                        ax = st.plt.plot_precip_field(R_obs[i, :, :], map=map,\n                                                      geodata=geodata,\n                                                      units=units,\n                                                      colorscale=colorscale,\n                                                      title=title,\n                                                      colorbar=colorbar,\n                                                      **kwargs)\n\n                    if UV is not None and motion_plot is not None:\n                        if motion_plot.lower() == \"quiver\":\n                            st.plt.quiver(UV, ax=ax, geodata=geodata, **kwargs)\n                        elif motion_plot.lower() == \"streamplot\":\n                            st.plt.streamplot(UV, ax=ax, geodata=geodata,\n                                              **kwargs)\n\n                    if savefig & (loop == 0):\n                        if type == \"prob\":\n                            figname = \"%s/%s_frame_%02d_binmap_%.1f.%s\" % \\\n                                    (path_outputs, startdate_str, i,\n                                     prob_thr, fig_format)\n                        else:\n                            figname = \"%s/%s_frame_%02d.%s\" % \\\n                                (path_outputs, startdate_str, i, fig_format)\n                        plt.savefig(figname, bbox_inches=\"tight\", dpi=fig_dpi)\n                        print(figname, 'saved.')\n\n                # Forecasts\n                elif i >= n_obs and R_fct is not None:\n\n                    title = \"\"\n                    if timestamps is not None:\n                        title += timestamps[-1].strftime(\"%Y-%m-%d %H:%M\\n\")\n                    leadtime = \"+%02d min\" % ((1 + i - n_obs)*timestep_min)\n\n                    plt.clf()\n                    if type == \"prob\":\n                        title += \"Forecast Probability\"\n                        P = st.postprocessing.ensemblestats.excprob(R_fct[:, i - n_obs, :, :], prob_thr)\n                        ax = st.plt.plot_precip_field(P, type=\"prob\", map=map,\n                                                      geodata=geodata,\n                                                      units=units,\n                                                      probthr=prob_thr,\n                                                      title=title,\n                                                      **kwargs)\n                    elif type == \"mean\":\n                        title += \"Forecast Ensemble Mean\"\n                        EM = st.postprocessing.ensemblestats.mean(R_fct[:, i - n_obs, :, :])\n                        ax = st.plt.plot_precip_field(EM, map=map,\n                                                      geodata=geodata,\n                                                      units=units,\n                                                      title=title,\n                                                      colorscale=colorscale,\n                                                      colorbar=colorbar,\n                                                      **kwargs)\n                    else:\n                        title += \"Forecast Rainfall\"\n                        ax = st.plt.plot_precip_field(R_fct[n, i - n_obs, :, :],\n                                                      map=map,\n                                                      geodata=geodata,\n                                                      units=units,\n                                                      title=title,\n                                                      colorscale=colorscale,\n                                                      colorbar=colorbar,\n                                                      **kwargs)\n\n                    if UV is not None and motion_plot is not None:\n                        if motion_plot.lower() == \"quiver\":\n                            st.plt.quiver(UV, ax=ax, geodata=geodata, **kwargs)\n                        elif motion_plot.lower() == \"streamplot\":\n                            st.plt.streamplot(UV, ax=ax, geodata=geodata,\n                                              **kwargs)\n\n                    if leadtime is not None:\n                        plt.text(0.99, 0.99, leadtime, transform=ax.transAxes,\n                                 ha=\"right\", va=\"top\")\n                    if type == \"ensemble\" and n_members > 1:\n                        plt.text(0.01, 0.99, \"m %02d\" % (n+1),\n                                 transform=ax.transAxes,\n                                 ha=\"left\", va=\"top\")\n\n                    if savefig & (loop == 0):\n                        if type == \"prob\":\n                            figname = \"%s/%s_frame_%02d_probmap_%.1f.%s\" % \\\n                                (path_outputs, startdate_str, i,\n                                 prob_thr, fig_format)\n                        elif type == \"mean\":\n                            figname = \"%s/%s_frame_%02d_ensmean.%s\" % \\\n                                (path_outputs, startdate_str, i, fig_format)\n                        else:\n                            figname = \"%s/%s_member_%02d_frame_%02d.%s\" % \\\n                                (path_outputs, startdate_str, (n+1), i,\n                                 fig_format)\n                        plt.savefig(figname, bbox_inches=\"tight\", dpi=fig_dpi)\n                        print(figname, \"saved.\")\n\n                if plotanimation:\n                    plt.pause(.2)\n\n            if plotanimation:\n                plt.pause(.5)\n\n        loop += 1\n\n    plt.close()"
]