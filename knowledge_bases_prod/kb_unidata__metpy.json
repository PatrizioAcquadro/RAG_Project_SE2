[
  "def pytest_report_header(config, startdir):\n    \"\"\"Add dependency information to pytest output.\"\"\"\n    return (f'Dep Versions: Matplotlib {matplotlib.__version__}, '\n            f'NumPy {numpy.__version__}, Pandas {pandas.__version__}, '\n            f'Pint {pint.__version__}, Pooch {pooch_version}\\n'\n            f'\\tPyProj {pyproj.__version__}, SciPy {scipy.__version__}, '\n            f'Traitlets {traitlets.__version__}, Xarray {xarray.__version__}')",
  "def doctest_available_modules(doctest_namespace):\n    \"\"\"Make modules available automatically to doctests.\"\"\"\n    doctest_namespace['metpy'] = metpy\n    doctest_namespace['metpy.calc'] = metpy.calc\n    doctest_namespace['np'] = numpy\n    doctest_namespace['plt'] = matplotlib.pyplot\n    doctest_namespace['units'] = metpy.units.units",
  "def ccrs():\n    \"\"\"Provide access to the ``cartopy.crs`` module through a global fixture.\n\n    Any testing function/fixture that needs access to ``cartopy.crs`` can simply add this to\n    their parameter list.\n\n    \"\"\"\n    return pytest.importorskip('cartopy.crs')",
  "def cfeature():\n    \"\"\"Provide access to the ``cartopy.feature`` module through a global fixture.\n\n    Any testing function/fixture that needs access to ``cartopy.feature`` can simply add this\n    to their parameter list.\n\n    \"\"\"\n    return pytest.importorskip('cartopy.feature')",
  "def test_da_lonlat():\n    \"\"\"Return a DataArray with a lon/lat grid and no time coordinate for use in tests.\"\"\"\n    data = numpy.linspace(300, 250, 3 * 4 * 4).reshape((3, 4, 4))\n    ds = xarray.Dataset(\n        {'temperature': (['isobaric', 'lat', 'lon'], data)},\n        coords={\n            'isobaric': xarray.DataArray(\n                numpy.array([850., 700., 500.]),\n                name='isobaric',\n                dims=['isobaric'],\n                attrs={'units': 'hPa'}\n            ),\n            'lat': xarray.DataArray(\n                numpy.linspace(30, 40, 4),\n                name='lat',\n                dims=['lat'],\n                attrs={'units': 'degrees_north'}\n            ),\n            'lon': xarray.DataArray(\n                numpy.linspace(260, 270, 4),\n                name='lon',\n                dims=['lon'],\n                attrs={'units': 'degrees_east'}\n            )\n        }\n    )\n    ds['temperature'].attrs['units'] = 'kelvin'\n\n    return ds.metpy.parse_cf('temperature')",
  "def test_da_xy():\n    \"\"\"Return a DataArray with a x/y grid and a time coordinate for use in tests.\"\"\"\n    data = numpy.linspace(300, 250, 3 * 3 * 4 * 4).reshape((3, 3, 4, 4))\n    ds = xarray.Dataset(\n        {'temperature': (['time', 'isobaric', 'y', 'x'], data),\n         'lambert_conformal': ([], '')},\n        coords={\n            'time': xarray.DataArray(\n                numpy.array([numpy.datetime64('2018-07-01T00:00'),\n                             numpy.datetime64('2018-07-01T06:00'),\n                             numpy.datetime64('2018-07-01T12:00')]),\n                name='time',\n                dims=['time']\n            ),\n            'isobaric': xarray.DataArray(\n                numpy.array([850., 700., 500.]),\n                name='isobaric',\n                dims=['isobaric'],\n                attrs={'units': 'hPa'}\n            ),\n            'y': xarray.DataArray(\n                numpy.linspace(-1000, 500, 4),\n                name='y',\n                dims=['y'],\n                attrs={'units': 'km'}\n            ),\n            'x': xarray.DataArray(\n                numpy.linspace(0, 1500, 4),\n                name='x',\n                dims=['x'],\n                attrs={'units': 'km'}\n            )\n        }\n    )\n    ds['temperature'].attrs = {\n        'units': 'kelvin',\n        'grid_mapping': 'lambert_conformal'\n    }\n    ds['lambert_conformal'].attrs = {\n        'grid_mapping_name': 'lambert_conformal_conic',\n        'standard_parallel': 50.0,\n        'longitude_of_central_meridian': -107.0,\n        'latitude_of_projection_origin': 50.0,\n        'earth_shape': 'spherical',\n        'earth_radius': 6367470.21484375\n    }\n\n    return ds.metpy.parse_cf('temperature')",
  "def set_agg_backend():\n    \"\"\"Fixture to ensure the Agg backend is active.\"\"\"\n    prev_backend = matplotlib.pyplot.get_backend()\n    try:\n        matplotlib.pyplot.switch_backend('agg')\n        yield\n    finally:\n        matplotlib.pyplot.switch_backend(prev_backend)",
  "def array_type(request):\n    \"\"\"Return an array type for testing calc functions.\"\"\"\n    quantity = metpy.units.units.Quantity\n    if request.param == 'dask':\n        dask_array = pytest.importorskip('dask.array', reason='dask.array is not available')\n        marker = request.node.get_closest_marker('xfail_dask')\n        if marker is not None:\n            request.applymarker(pytest.mark.xfail(reason=marker.args[0]))\n        return lambda d, u, *, mask=None: quantity(dask_array.array(d), u)\n    elif request.param == 'xarray':\n        return lambda d, u, *, mask=None: xarray.DataArray(d, attrs={'units': u})\n    elif request.param == 'masked':\n        return lambda d, u, *, mask=None: quantity(numpy.ma.array(d, mask=mask), u)\n    elif request.param == 'numpy':\n        return lambda d, u, *, mask=None: quantity(numpy.array(d), u)\n    else:\n        raise ValueError(f'Unsupported array_type option {request.param}')",
  "def geog_data(request):\n    \"\"\"Create data to use for testing calculations on geographic coordinates.\"\"\"\n    # Generate a field of u and v on a lat/lon grid\n    crs = pyproj.CRS(request.param)\n    proj = pyproj.Proj(crs)\n    a = numpy.arange(4)[None, :]\n    arr = numpy.r_[a, a, a] * metpy.units.units('m/s')\n    lons = numpy.array([-100, -90, -80, -70]) * metpy.units.units.degree\n    lats = numpy.array([45, 55, 65]) * metpy.units.units.degree\n    lon_arr, lat_arr = numpy.meshgrid(lons.m_as('degree'), lats.m_as('degree'))\n    factors = proj.get_factors(lon_arr, lat_arr)\n\n    return (crs, lons, lats, arr, arr, factors.parallel_scale, factors.meridional_scale,\n            metpy.calc.lat_lon_grid_deltas(lons.m, numpy.zeros_like(lons.m),\n                                           geod=crs.get_geod())[0][0],\n            metpy.calc.lat_lon_grid_deltas(numpy.zeros_like(lats.m), lats.m,\n                                           geod=crs.get_geod())[1][:, 0])",
  "class MetpyDeprecationWarning(UserWarning):\n    \"\"\"A class for issuing deprecation warnings for MetPy users.\n\n    In light of the fact that Python builtin DeprecationWarnings are ignored\n    by default as of Python 2.7 (see link below), this class was put in to\n    allow for the signaling of deprecation, but via UserWarnings which are not\n    ignored by default. Borrowed with love from matplotlib.\n\n    https://docs.python.org/dev/whatsnew/2.7.html#the-future-for-python-2-x\n    \"\"\"",
  "def _generate_deprecation_message(since, message='', name='',\n                                  alternative='', pending=False,\n                                  obj_type='attribute',\n                                  addendum=''):\n\n    if not message:\n\n        if pending:\n            message = (\n                'The {} {} will be deprecated in a '\n                'future version.'.format(name, obj_type))\n        else:\n            message = (\n                'The {} {} was deprecated in version '\n                '{}.'.format(name, obj_type, since))\n\n    altmessage = ''\n    if alternative:\n        altmessage = f' Use {alternative} instead.'\n\n    message = message + altmessage\n\n    if addendum:\n        message += addendum\n\n    return message",
  "def warn_deprecated(since, message='', name='', alternative='', pending=False,\n                    obj_type='attribute', addendum=''):\n    \"\"\"Display deprecation warning in a standard way.\n\n    Parameters\n    ----------\n    since : str\n        The release at which this API became deprecated.\n\n    message : str, optional\n        Override the default deprecation message.  The format\n        specifier `%(name)s` may be used for the name of the function,\n        and `%(alternative)s` may be used in the deprecation message\n        to insert the name of an alternative to the deprecated\n        function.  `%(obj_type)s` may be used to insert a friendly name\n        for the type of object being deprecated.\n\n    name : str, optional\n        The name of the deprecated object.\n\n    alternative : str, optional\n        An alternative function that the user may use in place of the\n        deprecated function.  The deprecation warning will tell the user\n        about this alternative if provided.\n\n    pending : bool, optional\n        If True, uses a PendingDeprecationWarning instead of a\n        DeprecationWarning.\n\n    obj_type : str, optional\n        The object type being deprecated.\n\n    addendum : str, optional\n        Additional text appended directly to the final message.\n\n    Examples\n    --------\n        Basic example::\n\n            # To warn of the deprecation of \"metpy.name_of_module\"\n            warn_deprecated('0.6.0', name='metpy.name_of_module',\n                            obj_type='module')\n\n    \"\"\"\n    message = _generate_deprecation_message(since, message, name, alternative,\n                                            pending, obj_type)\n\n    warnings.warn(message, metpyDeprecation, stacklevel=1)",
  "def deprecated(since, message='', name='', alternative='', pending=False,\n               obj_type=None, addendum=''):\n    \"\"\"Mark a function or a class as deprecated.\n\n    Parameters\n    ----------\n    since : str\n        The release at which this API became deprecated.  This is\n        required.\n\n    message : str, optional\n        Override the default deprecation message.  The format\n        specifier `%(name)s` may be used for the name of the object,\n        and `%(alternative)s` may be used in the deprecation message\n        to insert the name of an alternative to the deprecated\n        object.  `%(obj_type)s` may be used to insert a friendly name\n        for the type of object being deprecated.\n\n    name : str, optional\n        The name of the deprecated object; if not provided the name\n        is automatically determined from the passed in object,\n        though this is useful in the case of renamed functions, where\n        the new function is just assigned to the name of the\n        deprecated function.  For example::\n\n            def new_function():\n                ...\n            oldFunction = new_function\n\n    alternative : str, optional\n        An alternative object that the user may use in place of the\n        deprecated object.  The deprecation warning will tell the user\n        about this alternative if provided.\n\n    pending : bool, optional\n        If True, uses a PendingDeprecationWarning instead of a\n        DeprecationWarning.\n\n    addendum : str, optional\n        Additional text appended directly to the final message.\n\n    Examples\n    --------\n        Basic example::\n\n            @deprecated('1.4.0')\n            def the_function_to_deprecate():\n                pass\n\n    \"\"\"\n    def deprecate(obj, message=message, name=name, alternative=alternative,\n                  pending=pending, addendum=addendum):\n        import textwrap\n\n        if not name:\n            name = obj.__name__\n\n        if isinstance(obj, type):\n            obj_type = 'class'\n            old_doc = obj.__doc__\n            func = obj.__init__\n\n            def finalize(wrapper, new_doc):\n                obj.__init__ = wrapper\n                return obj\n        else:\n            obj_type = 'function'\n            func = obj\n            old_doc = func.__doc__\n\n            def finalize(wrapper, new_doc):\n                return functools.wraps(func)(wrapper)\n\n        message = _generate_deprecation_message(since, message, name,\n                                                alternative, pending,\n                                                obj_type, addendum)\n\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, metpyDeprecation, stacklevel=2)\n            return func(*args, **kwargs)\n\n        old_doc = textwrap.dedent(old_doc or '').strip('\\n')\n        message = message.strip()\n        new_doc = ('\\n.. deprecated:: {}'\n                   '\\n    {}\\n\\n'.format(since, message) + old_doc)\n        if not old_doc:\n            # This is to prevent a spurious 'unexected unindent' warning from\n            # docutils when the original docstring was blank.\n            new_doc += r'\\ '\n\n        return finalize(wrapper, new_doc)\n\n    return deprecate",
  "def deprecate(obj, message=message, name=name, alternative=alternative,\n                  pending=pending, addendum=addendum):\n        import textwrap\n\n        if not name:\n            name = obj.__name__\n\n        if isinstance(obj, type):\n            obj_type = 'class'\n            old_doc = obj.__doc__\n            func = obj.__init__\n\n            def finalize(wrapper, new_doc):\n                obj.__init__ = wrapper\n                return obj\n        else:\n            obj_type = 'function'\n            func = obj\n            old_doc = func.__doc__\n\n            def finalize(wrapper, new_doc):\n                return functools.wraps(func)(wrapper)\n\n        message = _generate_deprecation_message(since, message, name,\n                                                alternative, pending,\n                                                obj_type, addendum)\n\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, metpyDeprecation, stacklevel=2)\n            return func(*args, **kwargs)\n\n        old_doc = textwrap.dedent(old_doc or '').strip('\\n')\n        message = message.strip()\n        new_doc = ('\\n.. deprecated:: {}'\n                   '\\n    {}\\n\\n'.format(since, message) + old_doc)\n        if not old_doc:\n            # This is to prevent a spurious 'unexected unindent' warning from\n            # docutils when the original docstring was blank.\n            new_doc += r'\\ '\n\n        return finalize(wrapper, new_doc)",
  "def wrapper(*args, **kwargs):\n            warnings.warn(message, metpyDeprecation, stacklevel=2)\n            return func(*args, **kwargs)",
  "def finalize(wrapper, new_doc):\n                obj.__init__ = wrapper\n                return obj",
  "def finalize(wrapper, new_doc):\n                return functools.wraps(func)(wrapper)",
  "def _fix_udunits_powers(string):\n    \"\"\"Replace UDUNITS-style powers (m2 s-2) with exponent symbols (m**2 s**-2).\"\"\"\n    return _UDUNIT_POWER.sub('**', string)",
  "def _fix_udunits_div(string):\n    return 's**-1' if string == '/s' else string",
  "def setup_registry(reg):\n    \"\"\"Set up a given registry with MetPy's default tweaks and settings.\"\"\"\n    reg.autoconvert_offset_to_baseunit = True\n\n    # For Pint 0.18.0, need to deal with the fact that the wrapper isn't forwarding on setting\n    # the attribute.\n    with contextlib.suppress(AttributeError):\n        reg.get().autoconvert_offset_to_baseunit = True\n\n    for pre in _unit_preprocessors:\n        if pre not in reg.preprocessors:\n            reg.preprocessors.append(pre)\n\n    # Add a percent unit if it's not already present, it was added in 0.21\n    if 'percent' not in reg:\n        reg.define('percent = 0.01 = %')\n\n    # Define commonly encountered units not defined by pint\n    reg.define('degrees_north = degree = degrees_N = degreesN = degree_north = degree_N '\n               '= degreeN')\n    reg.define('degrees_east = degree = degrees_E = degreesE = degree_east = degree_E '\n               '= degreeE')\n\n    # Alias geopotential meters (gpm) to just meters\n    reg.define('@alias meter = gpm')\n\n    # Enable pint's built-in matplotlib support\n    reg.setup_matplotlib()\n\n    return reg",
  "def pandas_dataframe_to_unit_arrays(df, column_units=None):\n    \"\"\"Attach units to data in pandas dataframes and return quantities.\n\n    Parameters\n    ----------\n    df : `pandas.DataFrame`\n        Data in pandas dataframe.\n\n    column_units : dict\n        Dictionary of units to attach to columns of the dataframe. Overrides\n        the units attribute if it is attached to the dataframe.\n\n    Returns\n    -------\n        Dictionary containing `Quantity` instances with keys corresponding to the dataframe\n        column names.\n\n    \"\"\"\n    if not column_units:\n        try:\n            column_units = df.units\n        except AttributeError:\n            raise ValueError('No units attribute attached to pandas '\n                             'dataframe and col_units not given.') from None\n\n    # Iterate through columns attaching units if we have them, if not, don't touch it\n    res = {}\n    for column in df:\n        if column in column_units and column_units[column]:\n            res[column] = units.Quantity(df[column].values, column_units[column])\n        else:\n            res[column] = df[column].values\n    return res",
  "def is_quantity(*args):\n    \"\"\"Check whether an instance is a quantity.\"\"\"\n    return all(isinstance(a, pint.Quantity) for a in args)",
  "def concatenate(arrs, axis=0):\n    r\"\"\"Concatenate multiple values into a new quantity.\n\n    This is essentially a scalar-/masked array-aware version of `numpy.concatenate`. All items\n    must be able to be converted to the same units. If an item has no units, it will be given\n    those of the rest of the collection, without conversion. The first units found in the\n    arguments is used as the final output units.\n\n    Parameters\n    ----------\n    arrs : Sequence[pint.Quantity or numpy.ndarray]\n        The items to be joined together\n\n    axis : int, optional\n        The array axis along which to join the arrays. Defaults to 0 (the first dimension)\n\n    Returns\n    -------\n    `pint.Quantity`\n        New container with the value passed in and units corresponding to the first item.\n\n    \"\"\"\n    dest = 'dimensionless'\n    for a in arrs:\n        if hasattr(a, 'units'):\n            dest = a.units\n            break\n\n    data = []\n    for a in arrs:\n        if hasattr(a, 'to'):\n            a = a.to(dest).magnitude\n        data.append(np.atleast_1d(a))\n\n    # Use masked array concatenate to ensure masks are preserved, but convert to an\n    # array if there are no masked values.\n    data = np.ma.concatenate(data, axis=axis)\n    if not np.any(data.mask):\n        data = np.asarray(data)\n\n    return units.Quantity(data, dest)",
  "def masked_array(data, data_units=None, **kwargs):\n    \"\"\"Create a :class:`numpy.ma.MaskedArray` with units attached.\n\n    This is a thin wrapper around :class:`numpy.ma.MaskedArray` that ensures that\n    units are properly attached to the result (otherwise units are silently lost). Units\n    are taken from the ``data_units`` argument, or if this is ``None``, the units on ``data``\n    are used.\n\n    Parameters\n    ----------\n    data : array-like\n        The source data. If ``data_units`` is `None`, this should be a `pint.Quantity` with\n        the desired units.\n    data_units : str or `pint.Unit`, optional\n        The units for the resulting `pint.Quantity`\n    kwargs\n        Arbitrary keyword arguments passed to `numpy.ma.masked_array`, optional\n\n    Returns\n    -------\n    `pint.Quantity`\n\n    \"\"\"\n    if data_units is None:\n        data_units = data.units\n    return units.Quantity(np.ma.masked_array(data, **kwargs), data_units)",
  "def _mutate_arguments(bound_args, check_type, mutate_arg):\n    \"\"\"Handle adjusting bound arguments.\n\n    Calls ``mutate_arg`` on every argument, including those passed as ``*args``, if they are\n    of type ``check_type``.\n    \"\"\"\n    for arg_name, arg_val in bound_args.arguments.items():\n        if isinstance(arg_val, check_type):\n            bound_args.arguments[arg_name] = mutate_arg(arg_val, arg_name)\n\n    if isinstance(bound_args.arguments.get('args'), tuple):\n        bound_args.arguments['args'] = tuple(\n            mutate_arg(arg_val, '(unnamed)') if isinstance(arg_val, check_type) else arg_val\n            for arg_val in bound_args.arguments['args'])",
  "def _check_argument_units(args, defaults, dimensionality):\n    \"\"\"Yield arguments with improper dimensionality.\"\"\"\n    for arg, val in args.items():\n        # Get the needed dimensionality (for printing) as well as cached, parsed version\n        # for this argument.\n        try:\n            need, parsed = dimensionality[arg]\n        except KeyError:\n            # Argument did not have units specified in decorator\n            continue\n\n        if arg in defaults and (defaults[arg] is not None or val is None):\n            check = val == defaults[arg]\n            if np.all(check):\n                continue\n\n        # See if the value passed in is appropriate\n        try:\n            if val.dimensionality != parsed:\n                yield arg, val, val.units, need\n        # No dimensionality\n        except AttributeError:\n            # If this argument is dimensionless, don't worry\n            if parsed != '':\n                yield arg, val, 'none', need",
  "def _get_changed_version(docstring):\n    \"\"\"Find the most recent version in which the docs say a function changed.\"\"\"\n    matches = re.findall(r'.. versionchanged:: ([\\d.]+)', docstring)\n    return max(matches) if matches else None",
  "def _check_units_outer_helper(func, *args, **kwargs):\n    \"\"\"Get dims and defaults from function signature and specified dimensionalities.\"\"\"\n    # Match the signature of the function to the arguments given to the decorator\n    sig = signature(func)\n    bound_units = sig.bind_partial(*args, **kwargs)\n\n    # Convert our specified dimensionality (e.g. \"[pressure]\") to one used by\n    # pint directly (e.g. \"[mass] / [length] / [time]**2). This is for both efficiency\n    # reasons and to ensure that problems with the decorator are caught at import,\n    # rather than runtime.\n    dims = {name: (orig, units.get_dimensionality(orig.replace('dimensionless', '')))\n            for name, orig in bound_units.arguments.items()}\n\n    defaults = {name: sig.parameters[name].default for name in sig.parameters\n                if sig.parameters[name].default is not Parameter.empty}\n\n    return sig, dims, defaults",
  "def _check_units_inner_helper(func, sig, defaults, dims, *args, **kwargs):\n    \"\"\"Check bound arguments for unit correctness.\"\"\"\n    # Match all passed in value to their proper arguments so we can check units\n    bound_args = sig.bind(*args, **kwargs)\n    bad = list(_check_argument_units(bound_args.arguments, defaults, dims))\n\n    # If there are any bad units, emit a proper error message making it clear\n    # what went wrong.\n    if bad:\n        msg = f'`{func.__name__}` given arguments with incorrect units: '\n        msg += ', '.join(\n            f'`{arg}` requires \"{req}\" but given \"{given}\"' for arg, _, given, req in bad\n        )\n        if 'none' in msg:\n            if any(isinstance(x, np.ma.core.MaskedArray) for _, x, _, _ in bad):\n                msg += ('\\nA masked array `m` can be assigned a unit as follows:\\n'\n                        '    from metpy.units import units\\n'\n                        '    m = units.Quantity(m, \"m/s\")')\n            else:\n                msg += ('\\nA xarray DataArray or numpy array `x` can be assigned a unit as '\n                        'follows:\\n'\n                        '    from metpy.units import units\\n'\n                        '    x = x * units(\"m/s\")')\n            msg += ('\\nFor more information see the Units Tutorial: '\n                    'https://unidata.github.io/MetPy/latest/tutorials/unit_tutorial.html')\n\n        # If function has changed, mention that fact\n        if func.__doc__:\n            changed_version = _get_changed_version(func.__doc__)\n            if changed_version:\n                msg = (\n                    f'This function changed in {changed_version}--double check '\n                    'that the function is being called properly.\\n'\n                ) + msg\n        raise ValueError(msg)\n\n    # Return the bound arguments for reuse\n    return bound_args",
  "def check_units(*units_by_pos, **units_by_name):\n    \"\"\"Create a decorator to check units of function arguments.\"\"\"\n    def dec(func):\n        sig, dims, defaults = _check_units_outer_helper(func, *units_by_pos, **units_by_name)\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            _check_units_inner_helper(func, sig, defaults, dims, *args, **kwargs)\n            return func(*args, **kwargs)\n\n        return wrapper\n    return dec",
  "def process_units(\n    input_dimensionalities,\n    output_dimensionalities,\n    output_to=None,\n    ignore_inputs_for_output=None\n):\n    \"\"\"Wrap a non-Quantity-using function in base units to fully handle units.\"\"\"\n    def dec(func):\n        sig, dims, defaults = _check_units_outer_helper(func, **input_dimensionalities)\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            bound_args = _check_units_inner_helper(func, sig, defaults, dims, *args, **kwargs)\n\n            # Determine unit(s) with which to wrap output (first, since we mutate the bound\n            # args)\n            if isinstance(output_dimensionalities, tuple):\n                multiple_output = True\n                outputs = output_dimensionalities\n            else:\n                multiple_output = False\n                outputs = (output_dimensionalities,)\n            output_control = []\n            for i, output in enumerate(outputs):\n                convert_to = (\n                    output_to if not multiple_output or output_to is None else output_to[i]\n                )\n                # Find matching input, if it exists\n                if convert_to is None:\n                    for name, (this_dim, _) in dims.items():\n                        if (\n                            this_dim == output\n                            and (\n                                ignore_inputs_for_output is None\n                                or name not in ignore_inputs_for_output\n                            )\n                        ):\n                            try:\n                                convert_to = bound_args.arguments[name].units\n                            except AttributeError:\n                                # We don't have units, so given prior check, is dimensionless\n                                convert_to = ''\n                            break\n\n                output_control.append((_base_unit_of_dimensionality[output], convert_to))\n\n            # Convert all inputs as specified, assuming dimensionality is fine based on above\n            _mutate_arguments(bound_args, units.Quantity, lambda val, _: val.to_base_units().m)\n\n            # Evaluate inner calculation\n            result = func(*bound_args.args, **bound_args.kwargs)\n\n            # Wrap output\n            if multiple_output:\n                wrapped_result = []\n                for this_result, this_output_control in zip(result, output_control):\n                    q = units.Quantity(this_result, this_output_control[0])\n                    if this_output_control[1] is not None:\n                        q = q.to(this_output_control[1])\n                    wrapped_result.append(q)\n                return tuple(wrapped_result)\n            else:\n                q = units.Quantity(result, output_control[0][0])\n                if output_control[0][1] is not None:\n                    q = q.to(output_control[0][1])\n                return q\n\n        # Attach the unwrapped func for internal use\n        wrapper._nounit = func\n\n        return wrapper\n    return dec",
  "def dec(func):\n        sig, dims, defaults = _check_units_outer_helper(func, *units_by_pos, **units_by_name)\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            _check_units_inner_helper(func, sig, defaults, dims, *args, **kwargs)\n            return func(*args, **kwargs)\n\n        return wrapper",
  "def dec(func):\n        sig, dims, defaults = _check_units_outer_helper(func, **input_dimensionalities)\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            bound_args = _check_units_inner_helper(func, sig, defaults, dims, *args, **kwargs)\n\n            # Determine unit(s) with which to wrap output (first, since we mutate the bound\n            # args)\n            if isinstance(output_dimensionalities, tuple):\n                multiple_output = True\n                outputs = output_dimensionalities\n            else:\n                multiple_output = False\n                outputs = (output_dimensionalities,)\n            output_control = []\n            for i, output in enumerate(outputs):\n                convert_to = (\n                    output_to if not multiple_output or output_to is None else output_to[i]\n                )\n                # Find matching input, if it exists\n                if convert_to is None:\n                    for name, (this_dim, _) in dims.items():\n                        if (\n                            this_dim == output\n                            and (\n                                ignore_inputs_for_output is None\n                                or name not in ignore_inputs_for_output\n                            )\n                        ):\n                            try:\n                                convert_to = bound_args.arguments[name].units\n                            except AttributeError:\n                                # We don't have units, so given prior check, is dimensionless\n                                convert_to = ''\n                            break\n\n                output_control.append((_base_unit_of_dimensionality[output], convert_to))\n\n            # Convert all inputs as specified, assuming dimensionality is fine based on above\n            _mutate_arguments(bound_args, units.Quantity, lambda val, _: val.to_base_units().m)\n\n            # Evaluate inner calculation\n            result = func(*bound_args.args, **bound_args.kwargs)\n\n            # Wrap output\n            if multiple_output:\n                wrapped_result = []\n                for this_result, this_output_control in zip(result, output_control):\n                    q = units.Quantity(this_result, this_output_control[0])\n                    if this_output_control[1] is not None:\n                        q = q.to(this_output_control[1])\n                    wrapped_result.append(q)\n                return tuple(wrapped_result)\n            else:\n                q = units.Quantity(result, output_control[0][0])\n                if output_control[0][1] is not None:\n                    q = q.to(output_control[0][1])\n                return q\n\n        # Attach the unwrapped func for internal use\n        wrapper._nounit = func\n\n        return wrapper",
  "def wrapper(*args, **kwargs):\n            _check_units_inner_helper(func, sig, defaults, dims, *args, **kwargs)\n            return func(*args, **kwargs)",
  "def wrapper(*args, **kwargs):\n            bound_args = _check_units_inner_helper(func, sig, defaults, dims, *args, **kwargs)\n\n            # Determine unit(s) with which to wrap output (first, since we mutate the bound\n            # args)\n            if isinstance(output_dimensionalities, tuple):\n                multiple_output = True\n                outputs = output_dimensionalities\n            else:\n                multiple_output = False\n                outputs = (output_dimensionalities,)\n            output_control = []\n            for i, output in enumerate(outputs):\n                convert_to = (\n                    output_to if not multiple_output or output_to is None else output_to[i]\n                )\n                # Find matching input, if it exists\n                if convert_to is None:\n                    for name, (this_dim, _) in dims.items():\n                        if (\n                            this_dim == output\n                            and (\n                                ignore_inputs_for_output is None\n                                or name not in ignore_inputs_for_output\n                            )\n                        ):\n                            try:\n                                convert_to = bound_args.arguments[name].units\n                            except AttributeError:\n                                # We don't have units, so given prior check, is dimensionless\n                                convert_to = ''\n                            break\n\n                output_control.append((_base_unit_of_dimensionality[output], convert_to))\n\n            # Convert all inputs as specified, assuming dimensionality is fine based on above\n            _mutate_arguments(bound_args, units.Quantity, lambda val, _: val.to_base_units().m)\n\n            # Evaluate inner calculation\n            result = func(*bound_args.args, **bound_args.kwargs)\n\n            # Wrap output\n            if multiple_output:\n                wrapped_result = []\n                for this_result, this_output_control in zip(result, output_control):\n                    q = units.Quantity(this_result, this_output_control[0])\n                    if this_output_control[1] is not None:\n                        q = q.to(this_output_control[1])\n                    wrapped_result.append(q)\n                return tuple(wrapped_result)\n            else:\n                q = units.Quantity(result, output_control[0][0])\n                if output_control[0][1] is not None:\n                    q = q.to(output_control[0][1])\n                return q",
  "class Exporter:\n    \"\"\"Manages exporting of symbols from the module.\n\n    Grabs a reference to `globals()` for a module and provides a decorator to add\n    functions and classes to `__all__` rather than requiring a separately maintained list.\n    Also provides a context manager to do this for instances by adding all instances added\n    within a block to `__all__`.\n    \"\"\"\n\n    def __init__(self, globls):\n        \"\"\"Initialize the Exporter.\"\"\"\n        self.globls = globls\n        self.exports = globls.setdefault('__all__', [])\n\n    def export(self, defn):\n        \"\"\"Declare a function or class as exported.\"\"\"\n        self.exports.append(defn.__name__)\n        return defn\n\n    def __enter__(self):\n        \"\"\"Start a block tracking all instances created at global scope.\"\"\"\n        self.start_vars = set(self.globls)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Exit the instance tracking block.\"\"\"\n        self.exports.extend(set(self.globls) - self.start_vars)\n        del self.start_vars",
  "def set_module(globls):\n    \"\"\"Set the module for all functions in ``__all__``.\n\n    This sets the ``__module__`` attribute of all items within the ``__all__`` list\n    for the calling module.\n\n    This supports our hoisting of functions out of individual modules, which are\n    considered implementation details, into the namespace of the top-level subpackage.\n\n    Parameters\n    ----------\n    globls : Dict[str, object]\n        Mapping of all global variables for the module. This contains all needed\n        python special (\"dunder\") variables needed to be modified.\n\n    \"\"\"\n    for item in globls['__all__']:\n        obj = globls[item]\n        if hasattr(obj, '__module__'):\n            obj.__module__ = globls['__name__']",
  "def __init__(self, globls):\n        \"\"\"Initialize the Exporter.\"\"\"\n        self.globls = globls\n        self.exports = globls.setdefault('__all__', [])",
  "def export(self, defn):\n        \"\"\"Declare a function or class as exported.\"\"\"\n        self.exports.append(defn.__name__)\n        return defn",
  "def __enter__(self):\n        \"\"\"Start a block tracking all instances created at global scope.\"\"\"\n        self.start_vars = set(self.globls)",
  "def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Exit the instance tracking block.\"\"\"\n        self.exports.extend(set(self.globls) - self.start_vars)\n        del self.start_vars",
  "def _find_stack_level():\n    \"\"\"Find and return the stack level where we're outside the library.\"\"\"\n    import metpy\n\n    frame = inspect.currentframe()\n    n = 0\n    while frame:\n        if inspect.getfile(frame).startswith(metpy.__path__[0]):\n            n += 1\n            frame = frame.f_back\n        else:\n            break\n    return n",
  "def warn(*args, **kwargs):\n    \"\"\"Wrap `warnings.warn` and automatically set the stack level if not given.\"\"\"\n    level = kwargs.get('stacklevel')\n    if level is None:\n        level = _find_stack_level()\n    warnings.warn(*args, **kwargs, stacklevel=level)",
  "def preprocess_pandas(func):\n    \"\"\"Decorate a function to convert all data series arguments to `np.ndarray`.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # not using hasattr(a, values) because it picks up dict.values()\n        # and this is more explicitly handling pandas\n        args = tuple(a.values if isinstance(a, pd.Series) else a for a in args)\n        kwargs = {name: (v.values if isinstance(v, pd.Series) else v)\n                  for name, v in kwargs.items()}\n        return func(*args, **kwargs)\n    return wrapper",
  "def wrapper(*args, **kwargs):\n        # not using hasattr(a, values) because it picks up dict.values()\n        # and this is more explicitly handling pandas\n        args = tuple(a.values if isinstance(a, pd.Series) else a for a in args)\n        kwargs = {name: (v.values if isinstance(v, pd.Series) else v)\n                  for name, v in kwargs.items()}\n        return func(*args, **kwargs)",
  "def needs_module(module):\n    \"\"\"Decorate a test function or fixture as requiring a module.\n\n    Will skip the decorated test, or any test using the decorated fixture, if ``module``\n    is unable to be imported.\n    \"\"\"\n    def dec(test_func):\n        @functools.wraps(test_func)\n        def wrapped(*args, **kwargs):\n            pytest.importorskip(module)\n            return test_func(*args, **kwargs)\n        return wrapped\n    return dec",
  "def get_upper_air_data(date, station):\n    \"\"\"Get upper air observations from the test data cache.\n\n    Parameters\n    ----------\n    time : `~datetime.datetime`\n          The date and time of the desired observation.\n    station : str\n         The three letter ICAO identifier of the station for which data should be\n         downloaded.\n\n    Returns\n    -------\n        dict : upper air data\n\n    \"\"\"\n    sounding_key = f'{date:%Y-%m-%dT%HZ}_{station}'\n    sounding_files = {'2016-05-22T00Z_DDC': 'may22_sounding.txt',\n                      '2013-01-20T12Z_OUN': 'jan20_sounding.txt',\n                      '1999-05-04T00Z_OUN': 'may4_sounding.txt',\n                      '2002-11-11T00Z_BNA': 'nov11_sounding.txt',\n                      '2010-12-09T12Z_BOI': 'dec9_sounding.txt'}\n\n    # Initiate lists for variables\n    arr_data = []\n\n    def to_float(s):\n        # Remove all whitespace and replace empty values with NaN\n        if not s.strip():\n            s = 'nan'\n        return float(s)\n\n    with contextlib.closing(get_test_data(sounding_files[sounding_key])) as fobj:\n        # Skip dashes, column names, units, and more dashes\n        for _ in range(4):\n            fobj.readline()\n\n        # Read all lines of data and append to lists only if there is some data\n        for row in fobj:\n            level = to_float(row[0:7])\n            values = (to_float(row[7:14]), to_float(row[14:21]), to_float(row[21:28]),\n                      to_float(row[42:49]), to_float(row[49:56]))\n\n            if any(np.invert(np.isnan(values[1:]))):\n                arr_data.append((level,) + values)\n\n    p, z, t, td, direc, spd = np.array(arr_data).T\n\n    p = units.Quantity(p, 'hPa')\n    z = units.Quantity(z, 'meters')\n    t = units.Quantity(t, 'degC')\n    td = units.Quantity(td, 'degC')\n    direc = units.Quantity(direc, 'degrees')\n    spd = units.Quantity(spd, 'knots')\n\n    u, v = wind_components(spd, direc)\n\n    return {'pressure': p, 'height': z, 'temperature': t,\n            'dewpoint': td, 'direction': direc, 'speed': spd, 'u_wind': u, 'v_wind': v}",
  "def check_and_drop_units(actual, desired):\n    r\"\"\"Check that the units on the passed in arrays are compatible; return the magnitudes.\n\n    Parameters\n    ----------\n    actual : `pint.Quantity` or array-like\n\n    desired : `pint.Quantity` or array-like\n\n    Returns\n    -------\n    actual, desired\n        array-like versions of `actual` and `desired` once they have been\n        coerced to compatible units.\n\n    Raises\n    ------\n    AssertionError\n        If the units on the passed in objects are not compatible.\n\n    \"\"\"\n    try:\n        # Convert DataArrays to Quantities\n        if isinstance(desired, xr.DataArray):\n            desired = desired.metpy.unit_array\n        if isinstance(actual, xr.DataArray):\n            actual = actual.metpy.unit_array\n        # If the desired result has units, add dimensionless units if necessary, then\n        # ensure that this is compatible to the desired result.\n        if hasattr(desired, 'units'):\n            if not hasattr(actual, 'units'):\n                actual = units.Quantity(actual, 'dimensionless')\n            actual = actual.to(desired.units)\n        # Otherwise, the desired result has no units. Convert the actual result to\n        # dimensionless units if it is a quantity.\n        else:\n            if hasattr(actual, 'units'):\n                actual = actual.to('dimensionless')\n    except DimensionalityError:\n        raise AssertionError('Units are not compatible: {} should be {}'.format(\n            actual.units, getattr(desired, 'units', 'dimensionless'))) from None\n\n    if hasattr(actual, 'magnitude'):\n        actual = actual.magnitude\n    if hasattr(desired, 'magnitude'):\n        desired = desired.magnitude\n\n    return actual, desired",
  "def check_mask(actual, desired):\n    \"\"\"Check that two arrays have the same mask.\n\n    This handles the fact that `~numpy.testing.assert_array_equal` ignores masked values\n    in either of the arrays. This ensures that the masks are identical.\n    \"\"\"\n    actual_mask = getattr(actual, 'mask', np.full(np.asarray(actual).shape, False))\n    desired_mask = getattr(desired, 'mask', np.full(np.asarray(desired).shape, False))\n    np.testing.assert_array_equal(actual_mask, desired_mask)",
  "def assert_nan(value, value_units):\n    \"\"\"Check for nan with proper units.\"\"\"\n    value, _ = check_and_drop_units(value, units.Quantity(np.nan, value_units))\n    assert np.isnan(value)",
  "def assert_almost_equal(actual, desired, decimal=7):\n    \"\"\"Check that values are almost equal, including units.\n\n    Wrapper around :func:`numpy.testing.assert_almost_equal`\n    \"\"\"\n    actual, desired = check_and_drop_units(actual, desired)\n    numpy.testing.assert_almost_equal(actual, desired, decimal)",
  "def assert_array_almost_equal(actual, desired, decimal=7):\n    \"\"\"Check that arrays are almost equal, including units.\n\n    Wrapper around :func:`numpy.testing.assert_array_almost_equal`\n    \"\"\"\n    actual, desired = check_and_drop_units(actual, desired)\n    check_mask(actual, desired)\n    numpy.testing.assert_array_almost_equal(actual, desired, decimal)",
  "def assert_array_equal(actual, desired):\n    \"\"\"Check that arrays are equal, including units.\n\n    Wrapper around :func:`numpy.testing.assert_array_equal`\n    \"\"\"\n    actual, desired = check_and_drop_units(actual, desired)\n    check_mask(actual, desired)\n    numpy.testing.assert_array_equal(actual, desired)",
  "def assert_xarray_allclose(actual, desired):\n    \"\"\"Check that the xarrays are almost equal, including coordinates and attributes.\"\"\"\n    xr.testing.assert_allclose(actual, desired)\n    assert desired.metpy.coordinates_identical(actual)\n    assert desired.attrs == actual.attrs",
  "def check_and_silence_warning(warn_type):\n    \"\"\"Decorate a function to swallow some warning type, making sure they are present.\n\n    This should be used on function tests to make sure the warnings are not printing in the\n    tests, but checks that the warning is present and makes sure the function still works as\n    intended.\n    \"\"\"\n    def dec(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with pytest.warns(warn_type):\n                return func(*args, **kwargs)\n        return wrapper\n    return dec",
  "def dec(test_func):\n        @functools.wraps(test_func)\n        def wrapped(*args, **kwargs):\n            pytest.importorskip(module)\n            return test_func(*args, **kwargs)\n        return wrapped",
  "def to_float(s):\n        # Remove all whitespace and replace empty values with NaN\n        if not s.strip():\n            s = 'nan'\n        return float(s)",
  "def dec(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with pytest.warns(warn_type):\n                return func(*args, **kwargs)\n        return wrapper",
  "def wrapped(*args, **kwargs):\n            pytest.importorskip(module)\n            return test_func(*args, **kwargs)",
  "def wrapper(*args, **kwargs):\n            with pytest.warns(warn_type):\n                return func(*args, **kwargs)",
  "class MetPyDataArrayAccessor:\n    r\"\"\"Provide custom attributes and methods on xarray DataArrays for MetPy functionality.\n\n    This accessor provides several convenient attributes and methods through the ``.metpy``\n    attribute on a DataArray. For example, MetPy can identify the coordinate corresponding\n    to a particular axis (given sufficient metadata):\n\n        >>> import xarray as xr\n        >>> from metpy.units import units\n        >>> temperature = xr.DataArray([[0, 1], [2, 3]] * units.degC, dims=('lat', 'lon'),\n        ...                            coords={'lat': [40, 41], 'lon': [-105, -104]})\n        >>> temperature.metpy.x\n        <xarray.DataArray 'lon' (lon: 2)>\n        array([-105, -104])\n        Coordinates:\n          * lon      (lon) int64 -105 -104\n        Attributes:\n            _metpy_axis:  x,longitude\n\n    \"\"\"\n\n    def __init__(self, data_array):  # noqa: D107\n        # Initialize accessor with a DataArray. (Do not use directly).\n        self._data_array = data_array\n\n    @property\n    def units(self):\n        \"\"\"Return the units of this DataArray as a `pint.Unit`.\"\"\"\n        if is_quantity(self._data_array.variable._data):\n            return self._data_array.variable._data.units\n        else:\n            axis = self._data_array.attrs.get('_metpy_axis', '')\n            if 'latitude' in axis or 'longitude' in axis:\n                default_unit = 'degrees'\n            else:\n                default_unit = 'dimensionless'\n            return units.parse_units(self._data_array.attrs.get('units', default_unit))\n\n    @property\n    def magnitude(self):\n        \"\"\"Return the magnitude of the data values of this DataArray (i.e., without units).\"\"\"\n        if is_quantity(self._data_array.data):\n            return self._data_array.data.magnitude\n        else:\n            return self._data_array.data\n\n    @property\n    def unit_array(self):\n        \"\"\"Return the data values of this DataArray as a `pint.Quantity`.\n\n        Notes\n        -----\n        If not already existing as a `pint.Quantity` or Dask array, the data of this DataArray\n        will be loaded into memory by this operation. Do not utilize on moderate- to\n        large-sized remote datasets before subsetting!\n        \"\"\"\n        if is_quantity(self._data_array.data):\n            return self._data_array.data\n        else:\n            return units.Quantity(self._data_array.data, self.units)\n\n    def convert_units(self, units):\n        \"\"\"Return new DataArray with values converted to different units.\n\n        See Also\n        --------\n        convert_coordinate_units\n\n        Notes\n        -----\n        Any cached/lazy-loaded data (except that in a Dask array) will be loaded into memory\n        by this operation. Do not utilize on moderate- to large-sized remote datasets before\n        subsetting!\n\n        \"\"\"\n        return self.quantify().copy(data=self.unit_array.to(units))\n\n    def convert_to_base_units(self):\n        \"\"\"Return new DataArray with values converted to base units.\n\n        See Also\n        --------\n        convert_units\n\n        Notes\n        -----\n        Any cached/lazy-loaded data (except that in a Dask array) will be loaded into memory\n        by this operation. Do not utilize on moderate- to large-sized remote datasets before\n        subsetting!\n\n        \"\"\"\n        return self.quantify().copy(data=self.unit_array.to_base_units())\n\n    def convert_coordinate_units(self, coord, units):\n        \"\"\"Return new DataArray with specified coordinate converted to different units.\n\n        This operation differs from ``.convert_units`` since xarray coordinate indexes do not\n        yet support unit-aware arrays (even though unit-aware *data* arrays are).\n\n        See Also\n        --------\n        convert_units\n\n        Notes\n        -----\n        Any cached/lazy-loaded coordinate data (except that in a Dask array) will be loaded\n        into memory by this operation.\n\n        \"\"\"\n        new_coord_var = self._data_array[coord].copy(\n            data=self._data_array[coord].metpy.unit_array.m_as(units)\n        )\n        new_coord_var.attrs['units'] = str(units)\n        return self._data_array.assign_coords(coords={coord: new_coord_var})\n\n    def quantify(self):\n        \"\"\"Return a new DataArray with the data converted to a `pint.Quantity`.\n\n        Notes\n        -----\n        Any cached/lazy-loaded data (except that in a Dask array) will be loaded into memory\n        by this operation. Do not utilize on moderate- to large-sized remote datasets before\n        subsetting!\n        \"\"\"\n        if (\n            not is_quantity(self._data_array.data)\n            and np.issubdtype(self._data_array.data.dtype, np.number)\n        ):\n            # Only quantify if not already quantified and is quantifiable\n            quantified_dataarray = self._data_array.copy(data=self.unit_array)\n            if 'units' in quantified_dataarray.attrs:\n                del quantified_dataarray.attrs['units']\n        else:\n            quantified_dataarray = self._data_array\n        return quantified_dataarray\n\n    def dequantify(self):\n        \"\"\"Return a new DataArray with the data as magnitude and the units as an attribute.\"\"\"\n        if is_quantity(self._data_array.data):\n            # Only dequantify if quantified\n            dequantified_dataarray = self._data_array.copy(\n                data=self._data_array.data.magnitude\n            )\n            dequantified_dataarray.attrs['units'] = str(self.units)\n        else:\n            dequantified_dataarray = self._data_array\n        return dequantified_dataarray\n\n    @property\n    def crs(self):\n        \"\"\"Return the coordinate reference system (CRS) as a CFProjection object.\"\"\"\n        if 'metpy_crs' in self._data_array.coords:\n            return self._data_array.coords['metpy_crs'].item()\n        raise AttributeError('crs attribute is not available. You may need to use the'\n                             ' `parse_cf` or `assign_crs` methods. Consult the \"xarray'\n                             ' with MetPy Tutorial\" for more details.')\n\n    @property\n    def cartopy_crs(self):\n        \"\"\"Return the coordinate reference system (CRS) as a cartopy object.\"\"\"\n        return self.crs.to_cartopy()\n\n    @property\n    def cartopy_globe(self):\n        \"\"\"Return the globe belonging to the coordinate reference system (CRS).\"\"\"\n        return self.crs.cartopy_globe\n\n    @property\n    def cartopy_geodetic(self):\n        \"\"\"Return the cartopy Geodetic CRS associated with the native CRS globe.\"\"\"\n        return self.crs.cartopy_geodetic\n\n    @property\n    def pyproj_crs(self):\n        \"\"\"Return the coordinate reference system (CRS) as a pyproj object.\"\"\"\n        return self.crs.to_pyproj()\n\n    @property\n    def pyproj_proj(self):\n        \"\"\"Return the Proj object corresponding to the coordinate reference system (CRS).\"\"\"\n        return Proj(self.pyproj_crs)\n\n    def _fixup_coordinate_map(self, coord_map):\n        \"\"\"Ensure sure we have coordinate variables in map, not coordinate names.\"\"\"\n        new_coord_map = {}\n        for axis in coord_map:\n            if coord_map[axis] is not None and not isinstance(coord_map[axis], xr.DataArray):\n                new_coord_map[axis] = self._data_array[coord_map[axis]]\n            else:\n                new_coord_map[axis] = coord_map[axis]\n\n        return new_coord_map\n\n    def assign_coordinates(self, coordinates):\n        \"\"\"Return new DataArray with given coordinates assigned to the given MetPy axis types.\n\n        Parameters\n        ----------\n        coordinates : dict or None\n            Mapping from axis types ('time', 'vertical', 'y', 'latitude', 'x', 'longitude') to\n            coordinates of this DataArray. Coordinates can either be specified directly or by\n            their name. If ``None``, clears the `_metpy_axis` attribute on all coordinates,\n            which will trigger reparsing of all coordinates on next access.\n\n        \"\"\"\n        coord_updates = {}\n        if coordinates:\n            # Assign the _metpy_axis attributes according to supplied mapping\n            coordinates = self._fixup_coordinate_map(coordinates)\n            for axis in coordinates:\n                if coordinates[axis] is not None:\n                    coord_updates[coordinates[axis].name] = (\n                        coordinates[axis].assign_attrs(\n                            _assign_axis(coordinates[axis].attrs.copy(), axis)\n                        )\n                    )\n        else:\n            # Clear _metpy_axis attribute on all coordinates\n            for coord_name, coord_var in self._data_array.coords.items():\n                coord_updates[coord_name] = coord_var.copy(deep=False)\n\n                # Some coordinates remained linked in old form under other coordinates. We\n                # need to remove from these.\n                sub_coords = coord_updates[coord_name].coords\n                for sub_coord in sub_coords:\n                    coord_updates[coord_name].coords[sub_coord].attrs.pop('_metpy_axis', None)\n\n                # Now we can remove the _metpy_axis attr from the coordinate itself\n                coord_updates[coord_name].attrs.pop('_metpy_axis', None)\n\n        return self._data_array.assign_coords(coord_updates)\n\n    def _generate_coordinate_map(self):\n        \"\"\"Generate a coordinate map via CF conventions and other methods.\"\"\"\n        coords = self._data_array.coords.values()\n        # Parse all the coordinates, attempting to identify x, longitude, y, latitude,\n        # vertical, time\n        coord_lists = {'time': [], 'vertical': [], 'y': [], 'latitude': [], 'x': [],\n                       'longitude': []}\n        for coord_var in coords:\n            # Identify the coordinate type using check_axis helper\n            for axis in coord_lists:\n                if check_axis(coord_var, axis):\n                    coord_lists[axis].append(coord_var)\n\n        # Fill in x/y with longitude/latitude if x/y not otherwise present\n        for geometric, graticule in (('y', 'latitude'), ('x', 'longitude')):\n            if len(coord_lists[geometric]) == 0 and len(coord_lists[graticule]) > 0:\n                coord_lists[geometric] = coord_lists[graticule]\n\n        # Filter out multidimensional coordinates where not allowed\n        require_1d_coord = ['time', 'vertical', 'y', 'x']\n        for axis in require_1d_coord:\n            coord_lists[axis] = [coord for coord in coord_lists[axis] if coord.ndim <= 1]\n\n        # Resolve any coordinate type duplication\n        axis_duplicates = [axis for axis in coord_lists if len(coord_lists[axis]) > 1]\n        for axis in axis_duplicates:\n            self._resolve_axis_duplicates(axis, coord_lists)\n\n        # Collapse the coord_lists to a coord_map\n        return {axis: (coord_lists[axis][0] if len(coord_lists[axis]) > 0 else None)\n                for axis in coord_lists}\n\n    def _resolve_axis_duplicates(self, axis, coord_lists):\n        \"\"\"Handle coordinate duplication for an axis type if it arises.\"\"\"\n        # If one and only one of the possible axes is a dimension, use it\n        dimension_coords = [coord_var for coord_var in coord_lists[axis] if\n                            coord_var.name in coord_var.dims]\n        if len(dimension_coords) == 1:\n            coord_lists[axis] = dimension_coords\n            return\n\n        # Ambiguous axis, raise warning and do not parse\n        varname = (' \"' + self._data_array.name + '\"'\n                   if self._data_array.name is not None else '')\n        _warnings.warn(f'More than one {axis} coordinate present for variable {varname}.')\n        coord_lists[axis] = []\n\n    def _metpy_axis_search(self, metpy_axis):\n        \"\"\"Search for cached _metpy_axis attribute on the coordinates, otherwise parse.\"\"\"\n        # Search for coord with proper _metpy_axis\n        coords = self._data_array.coords.values()\n        for coord_var in coords:\n            if metpy_axis in coord_var.attrs.get('_metpy_axis', '').split(','):\n                return coord_var\n\n        # Opportunistically parse all coordinates, and assign if not already assigned\n        # Note: since this is generally called by way of the coordinate properties, to cache\n        # the coordinate parsing results in coord_map on the coordinates means modifying the\n        # DataArray in-place (an exception to the usual behavior of MetPy's accessor). This is\n        # considered safe because it only effects the \"_metpy_axis\" attribute on the\n        # coordinates, and nothing else.\n        coord_map = self._generate_coordinate_map()\n        for axis, coord_var in coord_map.items():\n            if coord_var is not None and all(\n                axis not in coord.attrs.get('_metpy_axis', '').split(',')\n                for coord in coords\n            ):\n\n                _assign_axis(coord_var.attrs, axis)\n\n        # Return parsed result (can be None if none found)\n        return coord_map[metpy_axis]\n\n    def _axis(self, axis):\n        \"\"\"Return the coordinate variable corresponding to the given individual axis type.\"\"\"\n        if axis not in metpy_axes:\n            raise AttributeError(\"'\" + axis + \"' is not an interpretable axis.\")\n\n        coord_var = self._metpy_axis_search(axis)\n        if coord_var is None:\n            raise AttributeError(axis + ' attribute is not available.')\n        else:\n            return coord_var\n\n    def coordinates(self, *args):\n        \"\"\"Return the coordinate variables corresponding to the given axes types.\n\n        Parameters\n        ----------\n        args : str\n            Strings describing the axes type(s) to obtain. Currently understood types are\n            'time', 'vertical', 'y', 'latitude', 'x', and 'longitude'.\n\n        Notes\n        -----\n        This method is designed for use with multiple coordinates; it returns a generator. To\n        access a single coordinate, use the appropriate attribute on the accessor, or use tuple\n        unpacking.\n\n        If latitude and/or longitude are requested here, and yet are not present on the\n        DataArray, an on-the-fly computation from the CRS and y/x dimension coordinates is\n        attempted.\n\n        \"\"\"\n        latitude = None\n        longitude = None\n        for arg in args:\n            try:\n                yield self._axis(arg)\n            except AttributeError as exc:\n                if (\n                    (arg == 'latitude' and latitude is None)\n                    or (arg == 'longitude' and longitude is None)\n                ):\n                    # Try to compute on the fly\n                    try:\n                        latitude, longitude = _build_latitude_longitude(self._data_array)\n                    except Exception:\n                        # Attempt failed, re-raise original error\n                        raise exc from None\n                    # Otherwise, warn and yield result\n                    _warnings.warn(\n                        'Latitude and longitude computed on-demand, which may be an '\n                        'expensive operation. To avoid repeating this computation, assign '\n                        'these coordinates ahead of time with '\n                        '.metpy.assign_latitude_longitude().'\n                    )\n                    if arg == 'latitude':\n                        yield latitude\n                    else:\n                        yield longitude\n                elif arg == 'latitude' and latitude is not None:\n                    # We have this from previous computation\n                    yield latitude\n                elif arg == 'longitude' and longitude is not None:\n                    # We have this from previous computation\n                    yield longitude\n                else:\n                    raise exc\n\n    @property\n    def time(self):\n        \"\"\"Return the time coordinate.\"\"\"\n        return self._axis('time')\n\n    @property\n    def vertical(self):\n        \"\"\"Return the vertical coordinate.\"\"\"\n        return self._axis('vertical')\n\n    @property\n    def y(self):\n        \"\"\"Return the y coordinate.\"\"\"\n        return self._axis('y')\n\n    @property\n    def latitude(self):\n        \"\"\"Return the latitude coordinate (if it exists).\"\"\"\n        return self._axis('latitude')\n\n    @property\n    def x(self):\n        \"\"\"Return the x coordinate.\"\"\"\n        return self._axis('x')\n\n    @property\n    def longitude(self):\n        \"\"\"Return the longitude coordinate (if it exists).\"\"\"\n        return self._axis('longitude')\n\n    def coordinates_identical(self, other):\n        \"\"\"Return whether the coordinates of other match this DataArray's.\"\"\"\n        return (len(self._data_array.coords) == len(other.coords)\n                and all(coord_name in other.coords and other[coord_name].identical(coord_var)\n                        for coord_name, coord_var in self._data_array.coords.items()))\n\n    @property\n    def time_deltas(self):\n        \"\"\"Return the time difference of the data in seconds (to microsecond precision).\"\"\"\n        us_diffs = np.diff(self._data_array.values).astype('timedelta64[us]').astype('int64')\n        return units.Quantity(us_diffs / 1e6, 's')\n\n    @property\n    def grid_deltas(self):\n        \"\"\"Return the horizontal dimensional grid deltas suitable for vector derivatives.\"\"\"\n        if (\n                (hasattr(self, 'crs')\n                 and self.crs._attrs['grid_mapping_name'] == 'latitude_longitude')\n                or (hasattr(self, 'longitude') and self.longitude.squeeze().ndim == 1\n                    and hasattr(self, 'latitude') and self.latitude.squeeze().ndim == 1)\n        ):\n            # Calculate dx and dy on ellipsoid (on equator and 0 deg meridian, respectively)\n            from .calc.tools import nominal_lat_lon_grid_deltas\n            crs = getattr(self, 'pyproj_crs', CRS('+proj=latlon'))\n            dx, dy = nominal_lat_lon_grid_deltas(\n                self.longitude.metpy.unit_array,\n                self.latitude.metpy.unit_array,\n                crs.get_geod()\n            )\n        else:\n            # Calculate dx and dy in projection space\n            try:\n                dx = np.diff(self.x.metpy.unit_array)\n                dy = np.diff(self.y.metpy.unit_array)\n            except AttributeError:\n                raise AttributeError(\n                    'Grid deltas cannot be calculated since horizontal dimension coordinates '\n                    'cannot be found.'\n                ) from None\n\n        return {'dx': dx, 'dy': dy}\n\n    def find_axis_name(self, axis):\n        \"\"\"Return the name of the axis corresponding to the given identifier.\n\n        Parameters\n        ----------\n        axis : str or int\n            Identifier for an axis. Can be an axis number (integer), dimension coordinate\n            name (string) or a standard axis type (string).\n\n        \"\"\"\n        if isinstance(axis, int):\n            # If an integer, use the corresponding dimension\n            return self._data_array.dims[axis]\n        elif axis not in self._data_array.dims and axis in metpy_axes:\n            # If not a dimension name itself, but a valid axis type, get the name of the\n            # coordinate corresponding to that axis type\n            return self._axis(axis).name\n        elif axis in self._data_array.dims and axis in self._data_array.coords:\n            # If this is a dimension coordinate name, use it directly\n            return axis\n        else:\n            # Otherwise, not valid\n            raise ValueError(_axis_identifier_error)\n\n    def find_axis_number(self, axis):\n        \"\"\"Return the dimension number of the axis corresponding to the given identifier.\n\n        Parameters\n        ----------\n        axis : str or int\n            Identifier for an axis. Can be an axis number (integer), dimension coordinate\n            name (string) or a standard axis type (string).\n\n        \"\"\"\n        if isinstance(axis, int):\n            # If an integer, use it directly\n            return axis\n        elif axis in self._data_array.dims:\n            # Simply index into dims\n            return self._data_array.dims.index(axis)\n        elif axis in metpy_axes:\n            # If not a dimension name itself, but a valid axis type, first determine if this\n            # standard axis type is present as a dimension coordinate\n            try:\n                name = self._axis(axis).name\n                return self._data_array.dims.index(name)\n            except AttributeError as exc:\n                # If x, y, or vertical requested, but not available, attempt to interpret dim\n                # names using regular expressions from coordinate parsing to allow for\n                # multidimensional lat/lon without y/x dimension coordinates, and basic\n                # vertical dim recognition\n                if axis in ('vertical', 'y', 'x'):\n                    for i, dim in enumerate(self._data_array.dims):\n                        if coordinate_criteria['regular_expression'][axis].match(dim.lower()):\n                            return i\n                raise exc\n            except ValueError:\n                # Intercept ValueError when axis type found but not dimension coordinate\n                raise AttributeError(f'Requested {axis} dimension coordinate but {axis} '\n                                     f'coordinate {name} is not a dimension') from None\n        else:\n            # Otherwise, not valid\n            raise ValueError(_axis_identifier_error)\n\n    class _LocIndexer:\n        \"\"\"Provide the unit-wrapped .loc indexer for data arrays.\"\"\"\n\n        def __init__(self, data_array):\n            self.data_array = data_array\n\n        def expand(self, key):\n            \"\"\"Parse key using xarray utils to ensure we have dimension names.\"\"\"\n            if not is_dict_like(key):\n                labels = expanded_indexer(key, self.data_array.ndim)\n                key = dict(zip(self.data_array.dims, labels))\n            return key\n\n        def __getitem__(self, key):\n            key = _reassign_quantity_indexer(self.data_array, self.expand(key))\n            return self.data_array.loc[key]\n\n        def __setitem__(self, key, value):\n            key = _reassign_quantity_indexer(self.data_array, self.expand(key))\n            self.data_array.loc[key] = value\n\n    @property\n    def loc(self):\n        \"\"\"Wrap DataArray.loc with an indexer to handle units and coordinate types.\"\"\"\n        return self._LocIndexer(self._data_array)\n\n    def sel(self, indexers=None, method=None, tolerance=None, drop=False, **indexers_kwargs):\n        \"\"\"Wrap DataArray.sel to handle units and coordinate types.\"\"\"\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'sel')\n        indexers = _reassign_quantity_indexer(self._data_array, indexers)\n        return self._data_array.sel(indexers, method=method, tolerance=tolerance, drop=drop)\n\n    def assign_crs(self, cf_attributes=None, **kwargs):\n        \"\"\"Assign a CRS to this DataArray based on CF projection attributes.\n\n        Specify a coordinate reference system/grid mapping following the Climate and\n        Forecasting (CF) conventions (see `Appendix F: Grid Mappings\n        <http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html#appendix-grid-mappings>`_\n        ) and store in the ``metpy_crs`` coordinate.\n\n        This method is only required if your data do not come from a dataset that follows CF\n        conventions with respect to grid mappings (in which case the ``.parse_cf`` method will\n        parse for the CRS metadata automatically).\n\n        Parameters\n        ----------\n        cf_attributes : dict, optional\n            Dictionary of CF projection attributes\n        kwargs : optional\n            CF projection attributes specified as keyword arguments\n\n        Returns\n        -------\n        `xarray.DataArray`\n            New xarray DataArray with CRS coordinate assigned\n\n        Notes\n        -----\n        CF projection arguments should be supplied as a dictionary or collection of kwargs,\n        but not both.\n\n        \"\"\"\n        return _assign_crs(self._data_array, cf_attributes, kwargs)\n\n    def assign_latitude_longitude(self, force=False):\n        \"\"\"Assign 2D latitude and longitude coordinates derived from 1D y and x coordinates.\n\n        Parameters\n        ----------\n        force : bool, optional\n            If force is true, overwrite latitude and longitude coordinates if they exist,\n            otherwise, raise a RuntimeError if such coordinates exist.\n\n        Returns\n        -------\n        `xarray.DataArray`\n            New xarray DataArray with latitude and longtiude auxiliary coordinates assigned.\n\n        Notes\n        -----\n        A valid CRS coordinate must be present (as assigned by ``.parse_cf`` or\n        ``.assign_crs``). PyProj is used for the coordinate transformations.\n\n        \"\"\"\n        # Check for existing latitude and longitude coords\n        if (not force and (self._metpy_axis_search('latitude') is not None\n                           or self._metpy_axis_search('longitude'))):\n            raise RuntimeError('Latitude/longitude coordinate(s) are present. If you wish to '\n                               'overwrite these, specify force=True.')\n\n        # Build new latitude and longitude DataArrays\n        latitude, longitude = _build_latitude_longitude(self._data_array)\n\n        # Assign new coordinates, refresh MetPy's parsed axis attribute, and return result\n        new_dataarray = self._data_array.assign_coords(latitude=latitude, longitude=longitude)\n        return new_dataarray.metpy.assign_coordinates(None)\n\n    def assign_y_x(self, force=False, tolerance=None):\n        \"\"\"Assign 1D y and x dimension coordinates derived from 2D latitude and longitude.\n\n        Parameters\n        ----------\n        force : bool, optional\n            If force is true, overwrite y and x coordinates if they exist, otherwise, raise a\n            RuntimeError if such coordinates exist.\n        tolerance : `pint.Quantity`\n            Maximum range tolerated when collapsing projected y and x coordinates from 2D to\n            1D. Defaults to 1 meter.\n\n        Returns\n        -------\n        `xarray.DataArray`\n            New xarray DataArray with y and x dimension coordinates assigned.\n\n        Notes\n        -----\n        A valid CRS coordinate must be present (as assigned by ``.parse_cf`` or\n        ``.assign_crs``) for the y/x projection space. PyProj is used for the coordinate\n        transformations.\n\n        \"\"\"\n        # Check for existing latitude and longitude coords\n        if (not force and (self._metpy_axis_search('y') is not None\n                           or self._metpy_axis_search('x'))):\n            raise RuntimeError('y/x coordinate(s) are present. If you wish to overwrite '\n                               'these, specify force=True.')\n\n        # Build new y and x DataArrays\n        y, x = _build_y_x(self._data_array, tolerance)\n\n        # Assign new coordinates, refresh MetPy's parsed axis attribute, and return result\n        new_dataarray = self._data_array.assign_coords(**{y.name: y, x.name: x})\n        return new_dataarray.metpy.assign_coordinates(None)",
  "class MetPyDatasetAccessor:\n    \"\"\"Provide custom attributes and methods on XArray Datasets for MetPy functionality.\n\n    This accessor provides parsing of CF grid mapping metadata, generating missing coordinate\n    types, and unit-/coordinate-type-aware operations.\n\n        >>> import xarray as xr\n        >>> from metpy.cbook import get_test_data\n        >>> ds = xr.open_dataset(get_test_data('narr_example.nc', False)).metpy.parse_cf()\n        >>> print(ds['metpy_crs'].item())\n        Projection: lambert_conformal_conic\n\n    \"\"\"\n\n    def __init__(self, dataset):  # noqa: D107\n        # Initialize accessor with a Dataset. (Do not use directly).\n        self._dataset = dataset\n\n    def parse_cf(self, varname=None, coordinates=None):\n        \"\"\"Parse dataset for coordinate system metadata according to CF conventions.\n\n        Interpret the grid mapping metadata in the dataset according to the Climate and\n        Forecasting (CF) conventions (see `Appendix F: Grid Mappings\n        <http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html#appendix-grid-mappings>`_\n        ) and store in the ``metpy_crs`` coordinate. Also, gives option to manually specify\n        coordinate types with the ``coordinates`` keyword argument.\n\n        If your dataset does not follow the CF conventions, you can manually supply the grid\n        mapping metadata with the ``.assign_crs`` method.\n\n        This method operates on individual data variables within the dataset, so do not be\n        surprised if information not associated with individual data variables is not\n        preserved.\n\n        Parameters\n        ----------\n        varname : str or Sequence[str], optional\n            Name of the variable(s) to extract from the dataset while parsing for CF metadata.\n            Defaults to all variables.\n        coordinates : dict, optional\n            Dictionary mapping CF axis types to coordinates of the variable(s). Only specify\n            if you wish to override MetPy's automatic parsing of some axis type(s).\n\n        Returns\n        -------\n        `xarray.DataArray` or `xarray.Dataset`\n            Parsed DataArray (if varname is a string) or Dataset\n\n        See Also\n        --------\n        assign_crs\n\n        \"\"\"\n        from .plots.mapping import CFProjection\n\n        if varname is None:\n            # If no varname is given, parse all variables in the dataset\n            varname = list(self._dataset.data_vars)\n\n        if np.iterable(varname) and not isinstance(varname, str):\n            # If non-string iterable is given, apply recursively across the varnames\n            subset = xr.merge([self.parse_cf(single_varname, coordinates=coordinates)\n                               for single_varname in varname])\n            subset.attrs = self._dataset.attrs\n            return subset\n\n        var = self._dataset[varname]\n\n        # Check for crs conflict\n        if varname == 'metpy_crs':\n            _warnings.warn(\n                'Attempting to parse metpy_crs as a data variable. Unexpected merge conflicts '\n                'may occur.'\n            )\n        elif 'metpy_crs' in var.coords and (var.coords['metpy_crs'].size > 1 or not isinstance(\n                var.coords['metpy_crs'].item(), CFProjection)):\n            _warnings.warn(\n                'metpy_crs already present as a non-CFProjection coordinate. Unexpected '\n                'merge conflicts may occur.'\n            )\n\n        # Assign coordinates if the coordinates argument is given\n        if coordinates is not None:\n            var = var.metpy.assign_coordinates(coordinates)\n\n        # Attempt to build the crs coordinate\n        crs = None\n        if 'grid_mapping' in var.attrs:\n            # Use given CF grid_mapping\n            proj_name = var.attrs['grid_mapping']\n            try:\n                proj_var = self._dataset.variables[proj_name]\n            except KeyError:\n                log.warning(\n                    'Could not find variable corresponding to the value of grid_mapping: %s',\n                    proj_name)\n            else:\n                crs = CFProjection(proj_var.attrs)\n\n        if crs is None:\n            # This isn't a lat or lon coordinate itself, so determine if we need to fall back\n            # to creating a latitude_longitude CRS. We do so if there exists valid *at most\n            # 1D* coordinates for latitude and longitude (usually dimension coordinates, but\n            # that is not strictly required, for example, for DSG's). What is required is that\n            # x == latitude and y == latitude (so that all assumptions about grid coordinates\n            # and CRS line up).\n            try:\n                latitude, y, longitude, x = var.metpy.coordinates(\n                    'latitude',\n                    'y',\n                    'longitude',\n                    'x'\n                )\n            except AttributeError:\n                # This means that we don't even have sufficient coordinates, so skip\n                pass\n            else:\n                if latitude.identical(y) and longitude.identical(x):\n                    crs = CFProjection({'grid_mapping_name': 'latitude_longitude'})\n                    log.debug('Found valid latitude/longitude coordinates, assuming '\n                              'latitude_longitude for projection grid_mapping variable')\n\n        # Rebuild the coordinates of the dataarray, and return quantified DataArray\n        var = self._rebuild_coords(var, crs)\n        if crs is not None:\n            var = var.assign_coords(coords={'metpy_crs': crs})\n        return var\n\n    def _rebuild_coords(self, var, crs):\n        \"\"\"Clean up the units on the coordinate variables.\"\"\"\n        for coord_name, coord_var in var.coords.items():\n            if (check_axis(coord_var, 'x', 'y')\n                    and not check_axis(coord_var, 'longitude', 'latitude')):\n                try:\n                    var = var.metpy.convert_coordinate_units(coord_name, 'meters')\n                except DimensionalityError:\n                    # Radians! Attempt to use perspective point height conversion\n                    if crs is not None:\n                        height = crs['perspective_point_height']\n                        new_coord_var = coord_var.copy(\n                            data=(\n                                coord_var.metpy.unit_array\n                                * units.Quantity(height, 'meter')\n                            ).m_as('meter')\n                        )\n                        new_coord_var.attrs['units'] = 'meter'\n                        var = var.assign_coords(coords={coord_name: new_coord_var})\n\n        return var\n\n    class _LocIndexer:\n        \"\"\"Provide the unit-wrapped .loc indexer for datasets.\"\"\"\n\n        def __init__(self, dataset):\n            self.dataset = dataset\n\n        def __getitem__(self, key):\n            parsed_key = _reassign_quantity_indexer(self.dataset, key)\n            return self.dataset.loc[parsed_key]\n\n    @property\n    def loc(self):\n        \"\"\"Wrap Dataset.loc with an indexer to handle units and coordinate types.\"\"\"\n        return self._LocIndexer(self._dataset)\n\n    def sel(self, indexers=None, method=None, tolerance=None, drop=False, **indexers_kwargs):\n        \"\"\"Wrap Dataset.sel to handle units.\"\"\"\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'sel')\n        indexers = _reassign_quantity_indexer(self._dataset, indexers)\n        return self._dataset.sel(indexers, method=method, tolerance=tolerance, drop=drop)\n\n    def assign_crs(self, cf_attributes=None, **kwargs):\n        \"\"\"Assign a CRS to this Dataset based on CF projection attributes.\n\n        Specify a coordinate reference system/grid mapping following the Climate and\n        Forecasting (CF) conventions (see `Appendix F: Grid Mappings\n        <http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html#appendix-grid-mappings>`_\n        ) and store in the ``metpy_crs`` coordinate.\n\n        This method is only required if your dataset does not already follow CF conventions\n        with respect to grid mappings (in which case the ``.parse_cf`` method will parse for\n        the CRS metadata automatically).\n\n        Parameters\n        ----------\n        cf_attributes : dict, optional\n            Dictionary of CF projection attributes\n        kwargs : optional\n            CF projection attributes specified as keyword arguments\n\n        Returns\n        -------\n        `xarray.Dataset`\n            New xarray Dataset with CRS coordinate assigned\n\n        See Also\n        --------\n        parse_cf\n\n        Notes\n        -----\n        CF projection arguments should be supplied as a dictionary or collection of kwargs,\n        but not both.\n\n        \"\"\"\n        return _assign_crs(self._dataset, cf_attributes, kwargs)\n\n    def assign_latitude_longitude(self, force=False):\n        \"\"\"Assign latitude and longitude coordinates derived from y and x coordinates.\n\n        Parameters\n        ----------\n        force : bool, optional\n            If force is true, overwrite latitude and longitude coordinates if they exist,\n            otherwise, raise a RuntimeError if such coordinates exist.\n\n        Returns\n        -------\n        `xarray.Dataset`\n            New xarray Dataset with latitude and longitude coordinates assigned to all\n            variables with y and x coordinates.\n\n        Notes\n        -----\n        A valid CRS coordinate must be present (as assigned by ``.parse_cf`` or\n        ``.assign_crs``). PyProj is used for the coordinate transformations.\n\n        \"\"\"\n        # Determine if there is a valid grid prototype from which to compute the coordinates,\n        # while also checking for existing lat/lon coords\n        grid_prototype = None\n        for data_var in self._dataset.data_vars.values():\n            if hasattr(data_var.metpy, 'y') and hasattr(data_var.metpy, 'x'):\n                if grid_prototype is None:\n                    grid_prototype = data_var\n                if (not force and (hasattr(data_var.metpy, 'latitude')\n                                   or hasattr(data_var.metpy, 'longitude'))):\n                    raise RuntimeError('Latitude/longitude coordinate(s) are present. If you '\n                                       'wish to overwrite these, specify force=True.')\n\n        # Calculate latitude and longitude from grid_prototype, if it exists, and assign\n        if grid_prototype is None:\n            _warnings.warn('No latitude and longitude assigned since horizontal coordinates '\n                           'were not found')\n            return self._dataset\n        else:\n            latitude, longitude = _build_latitude_longitude(grid_prototype)\n            return self._dataset.assign_coords(latitude=latitude, longitude=longitude)\n\n    def assign_y_x(self, force=False, tolerance=None):\n        \"\"\"Assign y and x dimension coordinates derived from 2D latitude and longitude.\n\n        Parameters\n        ----------\n        force : bool, optional\n            If force is true, overwrite y and x coordinates if they exist, otherwise, raise a\n            RuntimeError if such coordinates exist.\n        tolerance : `pint.Quantity`\n            Maximum range tolerated when collapsing projected y and x coordinates from 2D to\n            1D. Defaults to 1 meter.\n\n        Returns\n        -------\n        `xarray.Dataset`\n            New xarray Dataset with y and x dimension coordinates assigned to all variables\n            with valid latitude and longitude coordinates.\n\n        Notes\n        -----\n        A valid CRS coordinate must be present (as assigned by ``.parse_cf`` or\n        ``.assign_crs``). PyProj is used for the coordinate transformations.\n\n        \"\"\"\n        # Determine if there is a valid grid prototype from which to compute the coordinates,\n        # while also checking for existing y and x coords\n        grid_prototype = None\n        for data_var in self._dataset.data_vars.values():\n            if hasattr(data_var.metpy, 'latitude') and hasattr(data_var.metpy, 'longitude'):\n                if grid_prototype is None:\n                    grid_prototype = data_var\n                if (not force and (hasattr(data_var.metpy, 'y')\n                                   or hasattr(data_var.metpy, 'x'))):\n                    raise RuntimeError('y/x coordinate(s) are present. If you wish to '\n                                       'overwrite these, specify force=True.')\n\n        # Calculate y and x from grid_prototype, if it exists, and assign\n        if grid_prototype is None:\n            _warnings.warn('No y and x coordinates assigned since horizontal coordinates '\n                           'were not found')\n            return self._dataset\n        else:\n            y, x = _build_y_x(grid_prototype, tolerance)\n            return self._dataset.assign_coords(**{y.name: y, x.name: x})\n\n    def update_attribute(self, attribute, mapping):\n        \"\"\"Return new Dataset with specified attribute updated on all Dataset variables.\n\n        Parameters\n        ----------\n        attribute : str,\n            Name of attribute to update\n        mapping : dict or Callable\n            Either a dict, with keys as variable names and values as attribute values to set,\n            or a callable, which must accept one positional argument (variable name) and\n            arbitrary keyword arguments (all existing variable attributes). If a variable name\n            is not present/the callable returns None, the attribute will not be updated.\n\n        Returns\n        -------\n        `xarray.Dataset`\n            New Dataset with attribute updated\n\n        \"\"\"\n        # Make mapping uniform\n        if not callable(mapping):\n            old_mapping = mapping\n\n            def mapping(varname, **kwargs):\n                return old_mapping.get(varname)\n\n        # Define mapping function for Dataset.map\n        def mapping_func(da):\n            new_value = mapping(da.name, **da.attrs)\n            if new_value is None:\n                return da\n            else:\n                return da.assign_attrs(**{attribute: new_value})\n\n        # Apply across all variables and coordinates\n        return (\n            self._dataset\n            .map(mapping_func)\n            .assign_coords({\n                coord_name: mapping_func(coord_var)\n                for coord_name, coord_var in self._dataset.coords.items()\n            })\n        )\n\n    def quantify(self):\n        \"\"\"Return new dataset with all numeric variables quantified and cached data loaded.\n\n        Notes\n        -----\n        Any cached/lazy-loaded data (except that in a Dask array) will be loaded into memory\n        by this operation. Do not utilize on moderate- to large-sized remote datasets before\n        subsetting!\n        \"\"\"\n        return self._dataset.map(lambda da: da.metpy.quantify()).assign_attrs(\n            self._dataset.attrs\n        )\n\n    def dequantify(self):\n        \"\"\"Return new dataset with variables cast to magnitude and units on attribute.\"\"\"\n        return self._dataset.map(lambda da: da.metpy.dequantify()).assign_attrs(\n            self._dataset.attrs\n        )",
  "def _assign_axis(attributes, axis):\n    \"\"\"Assign the given axis to the _metpy_axis attribute.\"\"\"\n    existing_axes = attributes.get('_metpy_axis', '').split(',')\n    if ((axis == 'y' and 'latitude' in existing_axes)\n            or (axis == 'latitude' and 'y' in existing_axes)):\n        # Special case for combined y/latitude handling\n        attributes['_metpy_axis'] = 'y,latitude'\n    elif ((axis == 'x' and 'longitude' in existing_axes)\n            or (axis == 'longitude' and 'x' in existing_axes)):\n        # Special case for combined x/longitude handling\n        attributes['_metpy_axis'] = 'x,longitude'\n    else:\n        # Simply add it/overwrite past value\n        attributes['_metpy_axis'] = axis\n    return attributes",
  "def check_axis(var, *axes):\n    \"\"\"Check if the criteria for any of the given axes are satisfied.\n\n    Parameters\n    ----------\n    var : `xarray.DataArray`\n        DataArray belonging to the coordinate to be checked\n    axes : str\n        Axis type(s) to check for. Currently can check for 'time', 'vertical', 'y', 'latitude',\n        'x', and 'longitude'.\n\n    \"\"\"\n    for axis in axes:\n        # Check for\n        #   - standard name (CF option)\n        #   - _CoordinateAxisType (from THREDDS)\n        #   - axis (CF option)\n        #   - positive (CF standard for non-pressure vertical coordinate)\n        if any(var.attrs.get(criterion, 'absent')\n               in coordinate_criteria[criterion].get(axis, set())\n               for criterion in ('standard_name', '_CoordinateAxisType', 'axis', 'positive')):\n            return True\n\n        # Check for units, either by dimensionality or name\n        with contextlib.suppress(UndefinedUnitError):\n            if (axis in coordinate_criteria['units'] and (\n                    (\n                        coordinate_criteria['units'][axis]['match'] == 'dimensionality'\n                        and (units.get_dimensionality(var.metpy.units)\n                             == units.get_dimensionality(\n                                 coordinate_criteria['units'][axis]['units']))\n                    ) or (\n                        coordinate_criteria['units'][axis]['match'] == 'name'\n                        and str(var.metpy.units)\n                        in coordinate_criteria['units'][axis]['units']\n                    ))):\n                return True\n\n        # Check if name matches regular expression (non-CF failsafe)\n        if coordinate_criteria['regular_expression'][axis].match(var.name.lower()):\n            return True\n\n    # If no match has been made, return False (rather than None)\n    return False",
  "def _assign_crs(xarray_object, cf_attributes, cf_kwargs):\n    from .plots.mapping import CFProjection\n\n    # Handle argument options\n    if cf_attributes is not None and len(cf_kwargs) > 0:\n        raise ValueError('Cannot specify both attribute dictionary and kwargs.')\n    elif cf_attributes is None and len(cf_kwargs) == 0:\n        raise ValueError('Must specify either attribute dictionary or kwargs.')\n    attrs = cf_attributes if cf_attributes is not None else cf_kwargs\n\n    # Assign crs coordinate to xarray object\n    return xarray_object.assign_coords(metpy_crs=CFProjection(attrs))",
  "def _build_latitude_longitude(da):\n    \"\"\"Build latitude/longitude coordinates from DataArray's y/x coordinates.\"\"\"\n    y, x = da.metpy.coordinates('y', 'x')\n    xx, yy = np.meshgrid(x.values, y.values)\n    lonlats = np.stack(da.metpy.pyproj_proj(xx, yy, inverse=True, radians=False), axis=-1)\n    longitude = xr.DataArray(lonlats[..., 0], dims=(y.name, x.name),\n                             coords={y.name: y, x.name: x},\n                             attrs={'units': 'degrees_east', 'standard_name': 'longitude'})\n    latitude = xr.DataArray(lonlats[..., 1], dims=(y.name, x.name),\n                            coords={y.name: y, x.name: x},\n                            attrs={'units': 'degrees_north', 'standard_name': 'latitude'})\n    return latitude, longitude",
  "def _build_y_x(da, tolerance):\n    \"\"\"Build y/x coordinates from DataArray's latitude/longitude coordinates.\"\"\"\n    # Initial sanity checks\n    latitude, longitude = da.metpy.coordinates('latitude', 'longitude')\n    if latitude.dims != longitude.dims:\n        raise ValueError('Latitude and longitude must have same dimensionality')\n    elif latitude.ndim != 2:\n        raise ValueError('To build 1D y/x coordinates via assign_y_x, latitude/longitude '\n                         'must be 2D')\n\n    # Convert to projected y/x\n    xxyy = np.stack(da.metpy.pyproj_proj(\n        longitude.values,\n        latitude.values,\n        inverse=False,\n        radians=False\n    ), axis=-1)\n\n    # Handle tolerance\n    tolerance = 1 if tolerance is None else tolerance.m_as('m')\n\n    # If within tolerance, take median to collapse to 1D\n    try:\n        y_dim = latitude.metpy.find_axis_number('y')\n        x_dim = latitude.metpy.find_axis_number('x')\n    except AttributeError:\n        _warnings.warn('y and x dimensions unable to be identified. Assuming [..., y, x] '\n                       'dimension order.')\n        y_dim, x_dim = 0, 1\n    if (np.all(np.ptp(xxyy[..., 0], axis=y_dim) < tolerance)\n            and np.all(np.ptp(xxyy[..., 1], axis=x_dim) < tolerance)):\n        x = np.median(xxyy[..., 0], axis=y_dim)\n        y = np.median(xxyy[..., 1], axis=x_dim)\n        x = xr.DataArray(x, name=latitude.dims[x_dim], dims=(latitude.dims[x_dim],),\n                         coords={latitude.dims[x_dim]: x},\n                         attrs={'units': 'meter', 'standard_name': 'projection_x_coordinate'})\n        y = xr.DataArray(y, name=latitude.dims[y_dim], dims=(latitude.dims[y_dim],),\n                         coords={latitude.dims[y_dim]: y},\n                         attrs={'units': 'meter', 'standard_name': 'projection_y_coordinate'})\n        return y, x\n    else:\n        raise ValueError('Projected y and x coordinates cannot be collapsed to 1D within '\n                         'tolerance. Verify that your latitude and longitude coordinates '\n                         'correspond to your CRS coordinate.')",
  "def preprocess_and_wrap(broadcast=None, wrap_like=None, match_unit=False, to_magnitude=False):\n    \"\"\"Return decorator to wrap array calculations for type flexibility.\n\n    Assuming you have a calculation that works internally with `pint.Quantity` or\n    `numpy.ndarray`, this will wrap the function to be able to handle `xarray.DataArray` and\n    `pint.Quantity` as well (assuming appropriate match to one of the input arguments).\n\n    Parameters\n    ----------\n    broadcast : Sequence[str] or None\n        Iterable of string labels for arguments to broadcast against each other using xarray,\n        assuming they are supplied as `xarray.DataArray`. No automatic broadcasting will occur\n        with default of None.\n    wrap_like : str or array-like or tuple of str or tuple of array-like or None\n        Wrap the calculation output following a particular input argument (if str) or data\n        object (if array-like). If tuple, will assume output is in the form of a tuple,\n        and wrap iteratively according to the str or array-like contained within. If None,\n        will not wrap output.\n    match_unit : bool\n        If true, force the unit of the final output to be that of wrapping object (as\n        determined by wrap_like), no matter the original calculation output. Defaults to\n        False.\n    to_magnitude : bool\n        If true, downcast xarray and Pint arguments to their magnitude. If false, downcast\n        xarray arguments to Quantity, and do not change other array-like arguments.\n    \"\"\"\n    def decorator(func):\n        sig = signature(func)\n        if broadcast is not None:\n            for arg_name in broadcast:\n                if arg_name not in sig.parameters:\n                    raise ValueError(\n                        f'Cannot broadcast argument {arg_name} as it is not in function '\n                        'signature'\n                    )\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            bound_args = sig.bind(*args, **kwargs)\n\n            # Auto-broadcast select xarray arguments, and update bound_args\n            if broadcast is not None:\n                arg_names_to_broadcast = tuple(\n                    arg_name for arg_name in broadcast\n                    if arg_name in bound_args.arguments\n                    and isinstance(\n                        bound_args.arguments[arg_name],\n                        (xr.DataArray, xr.Variable)\n                    )\n                )\n                broadcasted_args = xr.broadcast(\n                    *(bound_args.arguments[arg_name] for arg_name in arg_names_to_broadcast)\n                )\n                for i, arg_name in enumerate(arg_names_to_broadcast):\n                    bound_args.arguments[arg_name] = broadcasted_args[i]\n\n            # Cast all Variables to their data and warn\n            # (need to do before match finding, since we don't want to rewrap as Variable)\n            def cast_variables(arg, arg_name):\n                _warnings.warn(\n                    f'Argument {arg_name} given as xarray Variable...casting to its data. '\n                    'xarray DataArrays are recommended instead.'\n                )\n                return arg.data\n            _mutate_arguments(bound_args, xr.Variable, cast_variables)\n\n            # Obtain proper match if referencing an input\n            match = list(wrap_like) if isinstance(wrap_like, tuple) else wrap_like\n            if isinstance(wrap_like, str):\n                match = bound_args.arguments[wrap_like]\n            elif isinstance(wrap_like, tuple):\n                for i, arg in enumerate(wrap_like):\n                    if isinstance(arg, str):\n                        match[i] = bound_args.arguments[arg]\n\n            # Cast all DataArrays to Pint Quantities\n            _mutate_arguments(bound_args, xr.DataArray, lambda arg, _: arg.metpy.unit_array)\n\n            # Optionally cast all Quantities to their magnitudes\n            if to_magnitude:\n                _mutate_arguments(bound_args, units.Quantity, lambda arg, _: arg.m)\n\n            # Evaluate inner calculation\n            result = func(*bound_args.args, **bound_args.kwargs)\n\n            # Wrap output based on match and match_unit\n            if match is None:\n                return result\n            else:\n                if match_unit:\n                    wrapping = _wrap_output_like_matching_units\n                else:\n                    wrapping = _wrap_output_like_not_matching_units\n\n                if isinstance(match, list):\n                    return tuple(wrapping(*args) for args in zip(result, match))\n                else:\n                    return wrapping(result, match)\n        return wrapper\n    return decorator",
  "def _wrap_output_like_matching_units(result, match):\n    \"\"\"Convert result to be like match with matching units for output wrapper.\"\"\"\n    output_xarray = isinstance(match, xr.DataArray)\n    match_units = str(match.metpy.units if output_xarray else getattr(match, 'units', ''))\n\n    if isinstance(result, xr.DataArray):\n        result = result.metpy.convert_units(match_units)\n        return result if output_xarray else result.metpy.unit_array\n    else:\n        result = (\n            result.to(match_units) if is_quantity(result)\n            else units.Quantity(result, match_units)\n        )\n        return (\n            xr.DataArray(result, coords=match.coords, dims=match.dims) if output_xarray\n            else result\n        )",
  "def _wrap_output_like_not_matching_units(result, match):\n    \"\"\"Convert result to be like match without matching units for output wrapper.\"\"\"\n    output_xarray = isinstance(match, xr.DataArray)\n    if isinstance(result, xr.DataArray):\n        return result if output_xarray else result.metpy.unit_array\n    # Determine if need to upcast to Quantity\n    if (\n        not is_quantity(result) and (\n            is_quantity(match) or (output_xarray and is_quantity(match.data))\n        )\n    ):\n        result = units.Quantity(result)\n    return (\n        xr.DataArray(result, coords=match.coords, dims=match.dims)\n        if output_xarray and result is not None\n        else result\n    )",
  "def check_matching_coordinates(func):\n    \"\"\"Decorate a function to make sure all given DataArrays have matching coordinates.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        data_arrays = ([a for a in args if isinstance(a, xr.DataArray)]\n                       + [a for a in kwargs.values() if isinstance(a, xr.DataArray)])\n        if len(data_arrays) > 1:\n            first = data_arrays[0]\n            for other in data_arrays[1:]:\n                if not first.metpy.coordinates_identical(other):\n                    raise ValueError('Input DataArray arguments must be on same coordinates.')\n        return func(*args, **kwargs)\n    return wrapper",
  "def _reassign_quantity_indexer(data, indexers):\n    \"\"\"Reassign a units.Quantity indexer to units of relevant coordinate.\"\"\"\n    def _to_magnitude(val, unit):\n        try:\n            return val.m_as(unit)\n        except AttributeError:\n            return val\n\n    # Update indexers keys for axis type -> coord name replacement\n    indexers = {(key if not isinstance(data, xr.DataArray) or key in data.dims\n                 or key not in metpy_axes else\n                 next(data.metpy.coordinates(key)).name): indexers[key]\n                for key in indexers}\n\n    # Update indexers to handle quantities and slices of quantities\n    reassigned_indexers = {}\n    for coord_name in indexers:\n        coord_units = data[coord_name].metpy.units\n        if isinstance(indexers[coord_name], slice):\n            # Handle slices of quantities\n            start = _to_magnitude(indexers[coord_name].start, coord_units)\n            stop = _to_magnitude(indexers[coord_name].stop, coord_units)\n            step = _to_magnitude(indexers[coord_name].step, coord_units)\n            reassigned_indexers[coord_name] = slice(start, stop, step)\n        else:\n            # Handle quantities\n            reassigned_indexers[coord_name] = _to_magnitude(indexers[coord_name], coord_units)\n\n    return reassigned_indexers",
  "def grid_deltas_from_dataarray(f, kind='default'):\n    \"\"\"Calculate the horizontal deltas between grid points of a DataArray.\n\n    Calculate the signed delta distance between grid points of a DataArray in the horizontal\n    directions, using actual (real distance) or nominal (in projection space) deltas.\n\n    Parameters\n    ----------\n    f : `xarray.DataArray`\n        Parsed DataArray (``metpy_crs`` coordinate must be available for kind=\"actual\")\n    kind : str\n        Type of grid delta to calculate. \"actual\" returns true distances as calculated from\n        longitude and latitude via `lat_lon_grid_deltas`. \"nominal\" returns horizontal\n        differences in the data's coordinate space, either in degrees (for lat/lon CRS) or\n        meters (for y/x CRS). \"default\" behaves like \"actual\" for datasets with a lat/lon CRS\n        and like \"nominal\" for all others. Defaults to \"default\".\n\n    Returns\n    -------\n    dx, dy:\n        arrays of signed deltas between grid points in the x and y directions with dimensions\n        matching those of `f`.\n\n    See Also\n    --------\n    `~metpy.calc.lat_lon_grid_deltas`\n\n    \"\"\"\n    from metpy.calc import lat_lon_grid_deltas\n\n    # Determine behavior\n    if (\n        kind == 'default'\n        and (\n            not hasattr(f.metpy, 'crs')\n            or f.metpy.crs['grid_mapping_name'] == 'latitude_longitude'\n        )\n    ):\n        # Use actual grid deltas by default with latitude_longitude or missing CRS\n        kind = 'actual'\n    elif kind == 'default':\n        # Otherwise, use grid deltas in projected grid space by default\n        kind = 'nominal'\n    elif kind not in ('actual', 'nominal'):\n        raise ValueError('\"kind\" argument must be specified as \"default\", \"actual\", or '\n                         '\"nominal\"')\n\n    if kind == 'actual':\n        # Get latitude/longitude coordinates and find dim order\n        latitude, longitude = xr.broadcast(*f.metpy.coordinates('latitude', 'longitude'))\n        try:\n            y_dim = latitude.metpy.find_axis_number('y')\n            x_dim = latitude.metpy.find_axis_number('x')\n        except AttributeError:\n            _warnings.warn('y and x dimensions unable to be identified. Assuming [..., y, x] '\n                           'dimension order.')\n            y_dim, x_dim = -2, -1\n\n        # Get geod if it exists, otherwise fall back to PyProj default\n        try:\n            geod = f.metpy.pyproj_crs.get_geod()\n        except AttributeError:\n            geod = CRS.from_cf({'grid_mapping_name': 'latitude_longitude'}).get_geod()\n        # Obtain grid deltas as xarray Variables\n        (dx_var, dx_units), (dy_var, dy_units) = (\n            (xr.Variable(dims=latitude.dims, data=deltas.magnitude), deltas.units)\n            for deltas in lat_lon_grid_deltas(longitude, latitude, x_dim=x_dim, y_dim=y_dim,\n                                              geod=geod))\n    else:\n        # Obtain y/x coordinate differences\n        y, x = f.metpy.coordinates('y', 'x')\n        dx_var = x.diff(x.dims[0]).variable\n        dx_units = units(x.attrs.get('units'))\n        dy_var = y.diff(y.dims[0]).variable\n        dy_units = units(y.attrs.get('units'))\n\n    # Broadcast to input and attach units\n    dx_var = dx_var.set_dims(f.dims, shape=[dx_var.sizes[dim] if dim in dx_var.dims else 1\n                                            for dim in f.dims])\n    dx = units.Quantity(dx_var.data, dx_units)\n    dy_var = dy_var.set_dims(f.dims, shape=[dy_var.sizes[dim] if dim in dy_var.dims else 1\n                                            for dim in f.dims])\n    dy = units.Quantity(dy_var.data, dy_units)\n\n    return dx, dy",
  "def dataarray_arguments(bound_args):\n    \"\"\"Get any dataarray arguments in the bound function arguments.\"\"\"\n    for value in chain(bound_args.args, bound_args.kwargs.values()):\n        if isinstance(value, xr.DataArray):\n            yield value",
  "def add_vertical_dim_from_xarray(func):\n    \"\"\"Fill in optional vertical_dim from DataArray argument.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        bound_args = signature(func).bind(*args, **kwargs)\n        bound_args.apply_defaults()\n\n        # Fill in vertical_dim\n        if 'vertical_dim' in bound_args.arguments:\n            a = next(dataarray_arguments(bound_args), None)\n            if a is not None:\n                try:\n                    bound_args.arguments['vertical_dim'] = a.metpy.find_axis_number('vertical')\n                except AttributeError:\n                    # If axis number not found, fall back to default but warn.\n                    _warnings.warn(\n                        'Vertical dimension number not found. Defaulting to initial dimension.'\n                    )\n\n        return func(*bound_args.args, **bound_args.kwargs)\n    return wrapper",
  "def __init__(self, data_array):  # noqa: D107\n        # Initialize accessor with a DataArray. (Do not use directly).\n        self._data_array = data_array",
  "def units(self):\n        \"\"\"Return the units of this DataArray as a `pint.Unit`.\"\"\"\n        if is_quantity(self._data_array.variable._data):\n            return self._data_array.variable._data.units\n        else:\n            axis = self._data_array.attrs.get('_metpy_axis', '')\n            if 'latitude' in axis or 'longitude' in axis:\n                default_unit = 'degrees'\n            else:\n                default_unit = 'dimensionless'\n            return units.parse_units(self._data_array.attrs.get('units', default_unit))",
  "def magnitude(self):\n        \"\"\"Return the magnitude of the data values of this DataArray (i.e., without units).\"\"\"\n        if is_quantity(self._data_array.data):\n            return self._data_array.data.magnitude\n        else:\n            return self._data_array.data",
  "def unit_array(self):\n        \"\"\"Return the data values of this DataArray as a `pint.Quantity`.\n\n        Notes\n        -----\n        If not already existing as a `pint.Quantity` or Dask array, the data of this DataArray\n        will be loaded into memory by this operation. Do not utilize on moderate- to\n        large-sized remote datasets before subsetting!\n        \"\"\"\n        if is_quantity(self._data_array.data):\n            return self._data_array.data\n        else:\n            return units.Quantity(self._data_array.data, self.units)",
  "def convert_units(self, units):\n        \"\"\"Return new DataArray with values converted to different units.\n\n        See Also\n        --------\n        convert_coordinate_units\n\n        Notes\n        -----\n        Any cached/lazy-loaded data (except that in a Dask array) will be loaded into memory\n        by this operation. Do not utilize on moderate- to large-sized remote datasets before\n        subsetting!\n\n        \"\"\"\n        return self.quantify().copy(data=self.unit_array.to(units))",
  "def convert_to_base_units(self):\n        \"\"\"Return new DataArray with values converted to base units.\n\n        See Also\n        --------\n        convert_units\n\n        Notes\n        -----\n        Any cached/lazy-loaded data (except that in a Dask array) will be loaded into memory\n        by this operation. Do not utilize on moderate- to large-sized remote datasets before\n        subsetting!\n\n        \"\"\"\n        return self.quantify().copy(data=self.unit_array.to_base_units())",
  "def convert_coordinate_units(self, coord, units):\n        \"\"\"Return new DataArray with specified coordinate converted to different units.\n\n        This operation differs from ``.convert_units`` since xarray coordinate indexes do not\n        yet support unit-aware arrays (even though unit-aware *data* arrays are).\n\n        See Also\n        --------\n        convert_units\n\n        Notes\n        -----\n        Any cached/lazy-loaded coordinate data (except that in a Dask array) will be loaded\n        into memory by this operation.\n\n        \"\"\"\n        new_coord_var = self._data_array[coord].copy(\n            data=self._data_array[coord].metpy.unit_array.m_as(units)\n        )\n        new_coord_var.attrs['units'] = str(units)\n        return self._data_array.assign_coords(coords={coord: new_coord_var})",
  "def quantify(self):\n        \"\"\"Return a new DataArray with the data converted to a `pint.Quantity`.\n\n        Notes\n        -----\n        Any cached/lazy-loaded data (except that in a Dask array) will be loaded into memory\n        by this operation. Do not utilize on moderate- to large-sized remote datasets before\n        subsetting!\n        \"\"\"\n        if (\n            not is_quantity(self._data_array.data)\n            and np.issubdtype(self._data_array.data.dtype, np.number)\n        ):\n            # Only quantify if not already quantified and is quantifiable\n            quantified_dataarray = self._data_array.copy(data=self.unit_array)\n            if 'units' in quantified_dataarray.attrs:\n                del quantified_dataarray.attrs['units']\n        else:\n            quantified_dataarray = self._data_array\n        return quantified_dataarray",
  "def dequantify(self):\n        \"\"\"Return a new DataArray with the data as magnitude and the units as an attribute.\"\"\"\n        if is_quantity(self._data_array.data):\n            # Only dequantify if quantified\n            dequantified_dataarray = self._data_array.copy(\n                data=self._data_array.data.magnitude\n            )\n            dequantified_dataarray.attrs['units'] = str(self.units)\n        else:\n            dequantified_dataarray = self._data_array\n        return dequantified_dataarray",
  "def crs(self):\n        \"\"\"Return the coordinate reference system (CRS) as a CFProjection object.\"\"\"\n        if 'metpy_crs' in self._data_array.coords:\n            return self._data_array.coords['metpy_crs'].item()\n        raise AttributeError('crs attribute is not available. You may need to use the'\n                             ' `parse_cf` or `assign_crs` methods. Consult the \"xarray'\n                             ' with MetPy Tutorial\" for more details.')",
  "def cartopy_crs(self):\n        \"\"\"Return the coordinate reference system (CRS) as a cartopy object.\"\"\"\n        return self.crs.to_cartopy()",
  "def cartopy_globe(self):\n        \"\"\"Return the globe belonging to the coordinate reference system (CRS).\"\"\"\n        return self.crs.cartopy_globe",
  "def cartopy_geodetic(self):\n        \"\"\"Return the cartopy Geodetic CRS associated with the native CRS globe.\"\"\"\n        return self.crs.cartopy_geodetic",
  "def pyproj_crs(self):\n        \"\"\"Return the coordinate reference system (CRS) as a pyproj object.\"\"\"\n        return self.crs.to_pyproj()",
  "def pyproj_proj(self):\n        \"\"\"Return the Proj object corresponding to the coordinate reference system (CRS).\"\"\"\n        return Proj(self.pyproj_crs)",
  "def _fixup_coordinate_map(self, coord_map):\n        \"\"\"Ensure sure we have coordinate variables in map, not coordinate names.\"\"\"\n        new_coord_map = {}\n        for axis in coord_map:\n            if coord_map[axis] is not None and not isinstance(coord_map[axis], xr.DataArray):\n                new_coord_map[axis] = self._data_array[coord_map[axis]]\n            else:\n                new_coord_map[axis] = coord_map[axis]\n\n        return new_coord_map",
  "def assign_coordinates(self, coordinates):\n        \"\"\"Return new DataArray with given coordinates assigned to the given MetPy axis types.\n\n        Parameters\n        ----------\n        coordinates : dict or None\n            Mapping from axis types ('time', 'vertical', 'y', 'latitude', 'x', 'longitude') to\n            coordinates of this DataArray. Coordinates can either be specified directly or by\n            their name. If ``None``, clears the `_metpy_axis` attribute on all coordinates,\n            which will trigger reparsing of all coordinates on next access.\n\n        \"\"\"\n        coord_updates = {}\n        if coordinates:\n            # Assign the _metpy_axis attributes according to supplied mapping\n            coordinates = self._fixup_coordinate_map(coordinates)\n            for axis in coordinates:\n                if coordinates[axis] is not None:\n                    coord_updates[coordinates[axis].name] = (\n                        coordinates[axis].assign_attrs(\n                            _assign_axis(coordinates[axis].attrs.copy(), axis)\n                        )\n                    )\n        else:\n            # Clear _metpy_axis attribute on all coordinates\n            for coord_name, coord_var in self._data_array.coords.items():\n                coord_updates[coord_name] = coord_var.copy(deep=False)\n\n                # Some coordinates remained linked in old form under other coordinates. We\n                # need to remove from these.\n                sub_coords = coord_updates[coord_name].coords\n                for sub_coord in sub_coords:\n                    coord_updates[coord_name].coords[sub_coord].attrs.pop('_metpy_axis', None)\n\n                # Now we can remove the _metpy_axis attr from the coordinate itself\n                coord_updates[coord_name].attrs.pop('_metpy_axis', None)\n\n        return self._data_array.assign_coords(coord_updates)",
  "def _generate_coordinate_map(self):\n        \"\"\"Generate a coordinate map via CF conventions and other methods.\"\"\"\n        coords = self._data_array.coords.values()\n        # Parse all the coordinates, attempting to identify x, longitude, y, latitude,\n        # vertical, time\n        coord_lists = {'time': [], 'vertical': [], 'y': [], 'latitude': [], 'x': [],\n                       'longitude': []}\n        for coord_var in coords:\n            # Identify the coordinate type using check_axis helper\n            for axis in coord_lists:\n                if check_axis(coord_var, axis):\n                    coord_lists[axis].append(coord_var)\n\n        # Fill in x/y with longitude/latitude if x/y not otherwise present\n        for geometric, graticule in (('y', 'latitude'), ('x', 'longitude')):\n            if len(coord_lists[geometric]) == 0 and len(coord_lists[graticule]) > 0:\n                coord_lists[geometric] = coord_lists[graticule]\n\n        # Filter out multidimensional coordinates where not allowed\n        require_1d_coord = ['time', 'vertical', 'y', 'x']\n        for axis in require_1d_coord:\n            coord_lists[axis] = [coord for coord in coord_lists[axis] if coord.ndim <= 1]\n\n        # Resolve any coordinate type duplication\n        axis_duplicates = [axis for axis in coord_lists if len(coord_lists[axis]) > 1]\n        for axis in axis_duplicates:\n            self._resolve_axis_duplicates(axis, coord_lists)\n\n        # Collapse the coord_lists to a coord_map\n        return {axis: (coord_lists[axis][0] if len(coord_lists[axis]) > 0 else None)\n                for axis in coord_lists}",
  "def _resolve_axis_duplicates(self, axis, coord_lists):\n        \"\"\"Handle coordinate duplication for an axis type if it arises.\"\"\"\n        # If one and only one of the possible axes is a dimension, use it\n        dimension_coords = [coord_var for coord_var in coord_lists[axis] if\n                            coord_var.name in coord_var.dims]\n        if len(dimension_coords) == 1:\n            coord_lists[axis] = dimension_coords\n            return\n\n        # Ambiguous axis, raise warning and do not parse\n        varname = (' \"' + self._data_array.name + '\"'\n                   if self._data_array.name is not None else '')\n        _warnings.warn(f'More than one {axis} coordinate present for variable {varname}.')\n        coord_lists[axis] = []",
  "def _metpy_axis_search(self, metpy_axis):\n        \"\"\"Search for cached _metpy_axis attribute on the coordinates, otherwise parse.\"\"\"\n        # Search for coord with proper _metpy_axis\n        coords = self._data_array.coords.values()\n        for coord_var in coords:\n            if metpy_axis in coord_var.attrs.get('_metpy_axis', '').split(','):\n                return coord_var\n\n        # Opportunistically parse all coordinates, and assign if not already assigned\n        # Note: since this is generally called by way of the coordinate properties, to cache\n        # the coordinate parsing results in coord_map on the coordinates means modifying the\n        # DataArray in-place (an exception to the usual behavior of MetPy's accessor). This is\n        # considered safe because it only effects the \"_metpy_axis\" attribute on the\n        # coordinates, and nothing else.\n        coord_map = self._generate_coordinate_map()\n        for axis, coord_var in coord_map.items():\n            if coord_var is not None and all(\n                axis not in coord.attrs.get('_metpy_axis', '').split(',')\n                for coord in coords\n            ):\n\n                _assign_axis(coord_var.attrs, axis)\n\n        # Return parsed result (can be None if none found)\n        return coord_map[metpy_axis]",
  "def _axis(self, axis):\n        \"\"\"Return the coordinate variable corresponding to the given individual axis type.\"\"\"\n        if axis not in metpy_axes:\n            raise AttributeError(\"'\" + axis + \"' is not an interpretable axis.\")\n\n        coord_var = self._metpy_axis_search(axis)\n        if coord_var is None:\n            raise AttributeError(axis + ' attribute is not available.')\n        else:\n            return coord_var",
  "def coordinates(self, *args):\n        \"\"\"Return the coordinate variables corresponding to the given axes types.\n\n        Parameters\n        ----------\n        args : str\n            Strings describing the axes type(s) to obtain. Currently understood types are\n            'time', 'vertical', 'y', 'latitude', 'x', and 'longitude'.\n\n        Notes\n        -----\n        This method is designed for use with multiple coordinates; it returns a generator. To\n        access a single coordinate, use the appropriate attribute on the accessor, or use tuple\n        unpacking.\n\n        If latitude and/or longitude are requested here, and yet are not present on the\n        DataArray, an on-the-fly computation from the CRS and y/x dimension coordinates is\n        attempted.\n\n        \"\"\"\n        latitude = None\n        longitude = None\n        for arg in args:\n            try:\n                yield self._axis(arg)\n            except AttributeError as exc:\n                if (\n                    (arg == 'latitude' and latitude is None)\n                    or (arg == 'longitude' and longitude is None)\n                ):\n                    # Try to compute on the fly\n                    try:\n                        latitude, longitude = _build_latitude_longitude(self._data_array)\n                    except Exception:\n                        # Attempt failed, re-raise original error\n                        raise exc from None\n                    # Otherwise, warn and yield result\n                    _warnings.warn(\n                        'Latitude and longitude computed on-demand, which may be an '\n                        'expensive operation. To avoid repeating this computation, assign '\n                        'these coordinates ahead of time with '\n                        '.metpy.assign_latitude_longitude().'\n                    )\n                    if arg == 'latitude':\n                        yield latitude\n                    else:\n                        yield longitude\n                elif arg == 'latitude' and latitude is not None:\n                    # We have this from previous computation\n                    yield latitude\n                elif arg == 'longitude' and longitude is not None:\n                    # We have this from previous computation\n                    yield longitude\n                else:\n                    raise exc",
  "def time(self):\n        \"\"\"Return the time coordinate.\"\"\"\n        return self._axis('time')",
  "def vertical(self):\n        \"\"\"Return the vertical coordinate.\"\"\"\n        return self._axis('vertical')",
  "def y(self):\n        \"\"\"Return the y coordinate.\"\"\"\n        return self._axis('y')",
  "def latitude(self):\n        \"\"\"Return the latitude coordinate (if it exists).\"\"\"\n        return self._axis('latitude')",
  "def x(self):\n        \"\"\"Return the x coordinate.\"\"\"\n        return self._axis('x')",
  "def longitude(self):\n        \"\"\"Return the longitude coordinate (if it exists).\"\"\"\n        return self._axis('longitude')",
  "def coordinates_identical(self, other):\n        \"\"\"Return whether the coordinates of other match this DataArray's.\"\"\"\n        return (len(self._data_array.coords) == len(other.coords)\n                and all(coord_name in other.coords and other[coord_name].identical(coord_var)\n                        for coord_name, coord_var in self._data_array.coords.items()))",
  "def time_deltas(self):\n        \"\"\"Return the time difference of the data in seconds (to microsecond precision).\"\"\"\n        us_diffs = np.diff(self._data_array.values).astype('timedelta64[us]').astype('int64')\n        return units.Quantity(us_diffs / 1e6, 's')",
  "def grid_deltas(self):\n        \"\"\"Return the horizontal dimensional grid deltas suitable for vector derivatives.\"\"\"\n        if (\n                (hasattr(self, 'crs')\n                 and self.crs._attrs['grid_mapping_name'] == 'latitude_longitude')\n                or (hasattr(self, 'longitude') and self.longitude.squeeze().ndim == 1\n                    and hasattr(self, 'latitude') and self.latitude.squeeze().ndim == 1)\n        ):\n            # Calculate dx and dy on ellipsoid (on equator and 0 deg meridian, respectively)\n            from .calc.tools import nominal_lat_lon_grid_deltas\n            crs = getattr(self, 'pyproj_crs', CRS('+proj=latlon'))\n            dx, dy = nominal_lat_lon_grid_deltas(\n                self.longitude.metpy.unit_array,\n                self.latitude.metpy.unit_array,\n                crs.get_geod()\n            )\n        else:\n            # Calculate dx and dy in projection space\n            try:\n                dx = np.diff(self.x.metpy.unit_array)\n                dy = np.diff(self.y.metpy.unit_array)\n            except AttributeError:\n                raise AttributeError(\n                    'Grid deltas cannot be calculated since horizontal dimension coordinates '\n                    'cannot be found.'\n                ) from None\n\n        return {'dx': dx, 'dy': dy}",
  "def find_axis_name(self, axis):\n        \"\"\"Return the name of the axis corresponding to the given identifier.\n\n        Parameters\n        ----------\n        axis : str or int\n            Identifier for an axis. Can be an axis number (integer), dimension coordinate\n            name (string) or a standard axis type (string).\n\n        \"\"\"\n        if isinstance(axis, int):\n            # If an integer, use the corresponding dimension\n            return self._data_array.dims[axis]\n        elif axis not in self._data_array.dims and axis in metpy_axes:\n            # If not a dimension name itself, but a valid axis type, get the name of the\n            # coordinate corresponding to that axis type\n            return self._axis(axis).name\n        elif axis in self._data_array.dims and axis in self._data_array.coords:\n            # If this is a dimension coordinate name, use it directly\n            return axis\n        else:\n            # Otherwise, not valid\n            raise ValueError(_axis_identifier_error)",
  "def find_axis_number(self, axis):\n        \"\"\"Return the dimension number of the axis corresponding to the given identifier.\n\n        Parameters\n        ----------\n        axis : str or int\n            Identifier for an axis. Can be an axis number (integer), dimension coordinate\n            name (string) or a standard axis type (string).\n\n        \"\"\"\n        if isinstance(axis, int):\n            # If an integer, use it directly\n            return axis\n        elif axis in self._data_array.dims:\n            # Simply index into dims\n            return self._data_array.dims.index(axis)\n        elif axis in metpy_axes:\n            # If not a dimension name itself, but a valid axis type, first determine if this\n            # standard axis type is present as a dimension coordinate\n            try:\n                name = self._axis(axis).name\n                return self._data_array.dims.index(name)\n            except AttributeError as exc:\n                # If x, y, or vertical requested, but not available, attempt to interpret dim\n                # names using regular expressions from coordinate parsing to allow for\n                # multidimensional lat/lon without y/x dimension coordinates, and basic\n                # vertical dim recognition\n                if axis in ('vertical', 'y', 'x'):\n                    for i, dim in enumerate(self._data_array.dims):\n                        if coordinate_criteria['regular_expression'][axis].match(dim.lower()):\n                            return i\n                raise exc\n            except ValueError:\n                # Intercept ValueError when axis type found but not dimension coordinate\n                raise AttributeError(f'Requested {axis} dimension coordinate but {axis} '\n                                     f'coordinate {name} is not a dimension') from None\n        else:\n            # Otherwise, not valid\n            raise ValueError(_axis_identifier_error)",
  "class _LocIndexer:\n        \"\"\"Provide the unit-wrapped .loc indexer for data arrays.\"\"\"\n\n        def __init__(self, data_array):\n            self.data_array = data_array\n\n        def expand(self, key):\n            \"\"\"Parse key using xarray utils to ensure we have dimension names.\"\"\"\n            if not is_dict_like(key):\n                labels = expanded_indexer(key, self.data_array.ndim)\n                key = dict(zip(self.data_array.dims, labels))\n            return key\n\n        def __getitem__(self, key):\n            key = _reassign_quantity_indexer(self.data_array, self.expand(key))\n            return self.data_array.loc[key]\n\n        def __setitem__(self, key, value):\n            key = _reassign_quantity_indexer(self.data_array, self.expand(key))\n            self.data_array.loc[key] = value",
  "def loc(self):\n        \"\"\"Wrap DataArray.loc with an indexer to handle units and coordinate types.\"\"\"\n        return self._LocIndexer(self._data_array)",
  "def sel(self, indexers=None, method=None, tolerance=None, drop=False, **indexers_kwargs):\n        \"\"\"Wrap DataArray.sel to handle units and coordinate types.\"\"\"\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'sel')\n        indexers = _reassign_quantity_indexer(self._data_array, indexers)\n        return self._data_array.sel(indexers, method=method, tolerance=tolerance, drop=drop)",
  "def assign_crs(self, cf_attributes=None, **kwargs):\n        \"\"\"Assign a CRS to this DataArray based on CF projection attributes.\n\n        Specify a coordinate reference system/grid mapping following the Climate and\n        Forecasting (CF) conventions (see `Appendix F: Grid Mappings\n        <http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html#appendix-grid-mappings>`_\n        ) and store in the ``metpy_crs`` coordinate.\n\n        This method is only required if your data do not come from a dataset that follows CF\n        conventions with respect to grid mappings (in which case the ``.parse_cf`` method will\n        parse for the CRS metadata automatically).\n\n        Parameters\n        ----------\n        cf_attributes : dict, optional\n            Dictionary of CF projection attributes\n        kwargs : optional\n            CF projection attributes specified as keyword arguments\n\n        Returns\n        -------\n        `xarray.DataArray`\n            New xarray DataArray with CRS coordinate assigned\n\n        Notes\n        -----\n        CF projection arguments should be supplied as a dictionary or collection of kwargs,\n        but not both.\n\n        \"\"\"\n        return _assign_crs(self._data_array, cf_attributes, kwargs)",
  "def assign_latitude_longitude(self, force=False):\n        \"\"\"Assign 2D latitude and longitude coordinates derived from 1D y and x coordinates.\n\n        Parameters\n        ----------\n        force : bool, optional\n            If force is true, overwrite latitude and longitude coordinates if they exist,\n            otherwise, raise a RuntimeError if such coordinates exist.\n\n        Returns\n        -------\n        `xarray.DataArray`\n            New xarray DataArray with latitude and longtiude auxiliary coordinates assigned.\n\n        Notes\n        -----\n        A valid CRS coordinate must be present (as assigned by ``.parse_cf`` or\n        ``.assign_crs``). PyProj is used for the coordinate transformations.\n\n        \"\"\"\n        # Check for existing latitude and longitude coords\n        if (not force and (self._metpy_axis_search('latitude') is not None\n                           or self._metpy_axis_search('longitude'))):\n            raise RuntimeError('Latitude/longitude coordinate(s) are present. If you wish to '\n                               'overwrite these, specify force=True.')\n\n        # Build new latitude and longitude DataArrays\n        latitude, longitude = _build_latitude_longitude(self._data_array)\n\n        # Assign new coordinates, refresh MetPy's parsed axis attribute, and return result\n        new_dataarray = self._data_array.assign_coords(latitude=latitude, longitude=longitude)\n        return new_dataarray.metpy.assign_coordinates(None)",
  "def assign_y_x(self, force=False, tolerance=None):\n        \"\"\"Assign 1D y and x dimension coordinates derived from 2D latitude and longitude.\n\n        Parameters\n        ----------\n        force : bool, optional\n            If force is true, overwrite y and x coordinates if they exist, otherwise, raise a\n            RuntimeError if such coordinates exist.\n        tolerance : `pint.Quantity`\n            Maximum range tolerated when collapsing projected y and x coordinates from 2D to\n            1D. Defaults to 1 meter.\n\n        Returns\n        -------\n        `xarray.DataArray`\n            New xarray DataArray with y and x dimension coordinates assigned.\n\n        Notes\n        -----\n        A valid CRS coordinate must be present (as assigned by ``.parse_cf`` or\n        ``.assign_crs``) for the y/x projection space. PyProj is used for the coordinate\n        transformations.\n\n        \"\"\"\n        # Check for existing latitude and longitude coords\n        if (not force and (self._metpy_axis_search('y') is not None\n                           or self._metpy_axis_search('x'))):\n            raise RuntimeError('y/x coordinate(s) are present. If you wish to overwrite '\n                               'these, specify force=True.')\n\n        # Build new y and x DataArrays\n        y, x = _build_y_x(self._data_array, tolerance)\n\n        # Assign new coordinates, refresh MetPy's parsed axis attribute, and return result\n        new_dataarray = self._data_array.assign_coords(**{y.name: y, x.name: x})\n        return new_dataarray.metpy.assign_coordinates(None)",
  "def __init__(self, dataset):  # noqa: D107\n        # Initialize accessor with a Dataset. (Do not use directly).\n        self._dataset = dataset",
  "def parse_cf(self, varname=None, coordinates=None):\n        \"\"\"Parse dataset for coordinate system metadata according to CF conventions.\n\n        Interpret the grid mapping metadata in the dataset according to the Climate and\n        Forecasting (CF) conventions (see `Appendix F: Grid Mappings\n        <http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html#appendix-grid-mappings>`_\n        ) and store in the ``metpy_crs`` coordinate. Also, gives option to manually specify\n        coordinate types with the ``coordinates`` keyword argument.\n\n        If your dataset does not follow the CF conventions, you can manually supply the grid\n        mapping metadata with the ``.assign_crs`` method.\n\n        This method operates on individual data variables within the dataset, so do not be\n        surprised if information not associated with individual data variables is not\n        preserved.\n\n        Parameters\n        ----------\n        varname : str or Sequence[str], optional\n            Name of the variable(s) to extract from the dataset while parsing for CF metadata.\n            Defaults to all variables.\n        coordinates : dict, optional\n            Dictionary mapping CF axis types to coordinates of the variable(s). Only specify\n            if you wish to override MetPy's automatic parsing of some axis type(s).\n\n        Returns\n        -------\n        `xarray.DataArray` or `xarray.Dataset`\n            Parsed DataArray (if varname is a string) or Dataset\n\n        See Also\n        --------\n        assign_crs\n\n        \"\"\"\n        from .plots.mapping import CFProjection\n\n        if varname is None:\n            # If no varname is given, parse all variables in the dataset\n            varname = list(self._dataset.data_vars)\n\n        if np.iterable(varname) and not isinstance(varname, str):\n            # If non-string iterable is given, apply recursively across the varnames\n            subset = xr.merge([self.parse_cf(single_varname, coordinates=coordinates)\n                               for single_varname in varname])\n            subset.attrs = self._dataset.attrs\n            return subset\n\n        var = self._dataset[varname]\n\n        # Check for crs conflict\n        if varname == 'metpy_crs':\n            _warnings.warn(\n                'Attempting to parse metpy_crs as a data variable. Unexpected merge conflicts '\n                'may occur.'\n            )\n        elif 'metpy_crs' in var.coords and (var.coords['metpy_crs'].size > 1 or not isinstance(\n                var.coords['metpy_crs'].item(), CFProjection)):\n            _warnings.warn(\n                'metpy_crs already present as a non-CFProjection coordinate. Unexpected '\n                'merge conflicts may occur.'\n            )\n\n        # Assign coordinates if the coordinates argument is given\n        if coordinates is not None:\n            var = var.metpy.assign_coordinates(coordinates)\n\n        # Attempt to build the crs coordinate\n        crs = None\n        if 'grid_mapping' in var.attrs:\n            # Use given CF grid_mapping\n            proj_name = var.attrs['grid_mapping']\n            try:\n                proj_var = self._dataset.variables[proj_name]\n            except KeyError:\n                log.warning(\n                    'Could not find variable corresponding to the value of grid_mapping: %s',\n                    proj_name)\n            else:\n                crs = CFProjection(proj_var.attrs)\n\n        if crs is None:\n            # This isn't a lat or lon coordinate itself, so determine if we need to fall back\n            # to creating a latitude_longitude CRS. We do so if there exists valid *at most\n            # 1D* coordinates for latitude and longitude (usually dimension coordinates, but\n            # that is not strictly required, for example, for DSG's). What is required is that\n            # x == latitude and y == latitude (so that all assumptions about grid coordinates\n            # and CRS line up).\n            try:\n                latitude, y, longitude, x = var.metpy.coordinates(\n                    'latitude',\n                    'y',\n                    'longitude',\n                    'x'\n                )\n            except AttributeError:\n                # This means that we don't even have sufficient coordinates, so skip\n                pass\n            else:\n                if latitude.identical(y) and longitude.identical(x):\n                    crs = CFProjection({'grid_mapping_name': 'latitude_longitude'})\n                    log.debug('Found valid latitude/longitude coordinates, assuming '\n                              'latitude_longitude for projection grid_mapping variable')\n\n        # Rebuild the coordinates of the dataarray, and return quantified DataArray\n        var = self._rebuild_coords(var, crs)\n        if crs is not None:\n            var = var.assign_coords(coords={'metpy_crs': crs})\n        return var",
  "def _rebuild_coords(self, var, crs):\n        \"\"\"Clean up the units on the coordinate variables.\"\"\"\n        for coord_name, coord_var in var.coords.items():\n            if (check_axis(coord_var, 'x', 'y')\n                    and not check_axis(coord_var, 'longitude', 'latitude')):\n                try:\n                    var = var.metpy.convert_coordinate_units(coord_name, 'meters')\n                except DimensionalityError:\n                    # Radians! Attempt to use perspective point height conversion\n                    if crs is not None:\n                        height = crs['perspective_point_height']\n                        new_coord_var = coord_var.copy(\n                            data=(\n                                coord_var.metpy.unit_array\n                                * units.Quantity(height, 'meter')\n                            ).m_as('meter')\n                        )\n                        new_coord_var.attrs['units'] = 'meter'\n                        var = var.assign_coords(coords={coord_name: new_coord_var})\n\n        return var",
  "class _LocIndexer:\n        \"\"\"Provide the unit-wrapped .loc indexer for datasets.\"\"\"\n\n        def __init__(self, dataset):\n            self.dataset = dataset\n\n        def __getitem__(self, key):\n            parsed_key = _reassign_quantity_indexer(self.dataset, key)\n            return self.dataset.loc[parsed_key]",
  "def loc(self):\n        \"\"\"Wrap Dataset.loc with an indexer to handle units and coordinate types.\"\"\"\n        return self._LocIndexer(self._dataset)",
  "def sel(self, indexers=None, method=None, tolerance=None, drop=False, **indexers_kwargs):\n        \"\"\"Wrap Dataset.sel to handle units.\"\"\"\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'sel')\n        indexers = _reassign_quantity_indexer(self._dataset, indexers)\n        return self._dataset.sel(indexers, method=method, tolerance=tolerance, drop=drop)",
  "def assign_crs(self, cf_attributes=None, **kwargs):\n        \"\"\"Assign a CRS to this Dataset based on CF projection attributes.\n\n        Specify a coordinate reference system/grid mapping following the Climate and\n        Forecasting (CF) conventions (see `Appendix F: Grid Mappings\n        <http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html#appendix-grid-mappings>`_\n        ) and store in the ``metpy_crs`` coordinate.\n\n        This method is only required if your dataset does not already follow CF conventions\n        with respect to grid mappings (in which case the ``.parse_cf`` method will parse for\n        the CRS metadata automatically).\n\n        Parameters\n        ----------\n        cf_attributes : dict, optional\n            Dictionary of CF projection attributes\n        kwargs : optional\n            CF projection attributes specified as keyword arguments\n\n        Returns\n        -------\n        `xarray.Dataset`\n            New xarray Dataset with CRS coordinate assigned\n\n        See Also\n        --------\n        parse_cf\n\n        Notes\n        -----\n        CF projection arguments should be supplied as a dictionary or collection of kwargs,\n        but not both.\n\n        \"\"\"\n        return _assign_crs(self._dataset, cf_attributes, kwargs)",
  "def assign_latitude_longitude(self, force=False):\n        \"\"\"Assign latitude and longitude coordinates derived from y and x coordinates.\n\n        Parameters\n        ----------\n        force : bool, optional\n            If force is true, overwrite latitude and longitude coordinates if they exist,\n            otherwise, raise a RuntimeError if such coordinates exist.\n\n        Returns\n        -------\n        `xarray.Dataset`\n            New xarray Dataset with latitude and longitude coordinates assigned to all\n            variables with y and x coordinates.\n\n        Notes\n        -----\n        A valid CRS coordinate must be present (as assigned by ``.parse_cf`` or\n        ``.assign_crs``). PyProj is used for the coordinate transformations.\n\n        \"\"\"\n        # Determine if there is a valid grid prototype from which to compute the coordinates,\n        # while also checking for existing lat/lon coords\n        grid_prototype = None\n        for data_var in self._dataset.data_vars.values():\n            if hasattr(data_var.metpy, 'y') and hasattr(data_var.metpy, 'x'):\n                if grid_prototype is None:\n                    grid_prototype = data_var\n                if (not force and (hasattr(data_var.metpy, 'latitude')\n                                   or hasattr(data_var.metpy, 'longitude'))):\n                    raise RuntimeError('Latitude/longitude coordinate(s) are present. If you '\n                                       'wish to overwrite these, specify force=True.')\n\n        # Calculate latitude and longitude from grid_prototype, if it exists, and assign\n        if grid_prototype is None:\n            _warnings.warn('No latitude and longitude assigned since horizontal coordinates '\n                           'were not found')\n            return self._dataset\n        else:\n            latitude, longitude = _build_latitude_longitude(grid_prototype)\n            return self._dataset.assign_coords(latitude=latitude, longitude=longitude)",
  "def assign_y_x(self, force=False, tolerance=None):\n        \"\"\"Assign y and x dimension coordinates derived from 2D latitude and longitude.\n\n        Parameters\n        ----------\n        force : bool, optional\n            If force is true, overwrite y and x coordinates if they exist, otherwise, raise a\n            RuntimeError if such coordinates exist.\n        tolerance : `pint.Quantity`\n            Maximum range tolerated when collapsing projected y and x coordinates from 2D to\n            1D. Defaults to 1 meter.\n\n        Returns\n        -------\n        `xarray.Dataset`\n            New xarray Dataset with y and x dimension coordinates assigned to all variables\n            with valid latitude and longitude coordinates.\n\n        Notes\n        -----\n        A valid CRS coordinate must be present (as assigned by ``.parse_cf`` or\n        ``.assign_crs``). PyProj is used for the coordinate transformations.\n\n        \"\"\"\n        # Determine if there is a valid grid prototype from which to compute the coordinates,\n        # while also checking for existing y and x coords\n        grid_prototype = None\n        for data_var in self._dataset.data_vars.values():\n            if hasattr(data_var.metpy, 'latitude') and hasattr(data_var.metpy, 'longitude'):\n                if grid_prototype is None:\n                    grid_prototype = data_var\n                if (not force and (hasattr(data_var.metpy, 'y')\n                                   or hasattr(data_var.metpy, 'x'))):\n                    raise RuntimeError('y/x coordinate(s) are present. If you wish to '\n                                       'overwrite these, specify force=True.')\n\n        # Calculate y and x from grid_prototype, if it exists, and assign\n        if grid_prototype is None:\n            _warnings.warn('No y and x coordinates assigned since horizontal coordinates '\n                           'were not found')\n            return self._dataset\n        else:\n            y, x = _build_y_x(grid_prototype, tolerance)\n            return self._dataset.assign_coords(**{y.name: y, x.name: x})",
  "def update_attribute(self, attribute, mapping):\n        \"\"\"Return new Dataset with specified attribute updated on all Dataset variables.\n\n        Parameters\n        ----------\n        attribute : str,\n            Name of attribute to update\n        mapping : dict or Callable\n            Either a dict, with keys as variable names and values as attribute values to set,\n            or a callable, which must accept one positional argument (variable name) and\n            arbitrary keyword arguments (all existing variable attributes). If a variable name\n            is not present/the callable returns None, the attribute will not be updated.\n\n        Returns\n        -------\n        `xarray.Dataset`\n            New Dataset with attribute updated\n\n        \"\"\"\n        # Make mapping uniform\n        if not callable(mapping):\n            old_mapping = mapping\n\n            def mapping(varname, **kwargs):\n                return old_mapping.get(varname)\n\n        # Define mapping function for Dataset.map\n        def mapping_func(da):\n            new_value = mapping(da.name, **da.attrs)\n            if new_value is None:\n                return da\n            else:\n                return da.assign_attrs(**{attribute: new_value})\n\n        # Apply across all variables and coordinates\n        return (\n            self._dataset\n            .map(mapping_func)\n            .assign_coords({\n                coord_name: mapping_func(coord_var)\n                for coord_name, coord_var in self._dataset.coords.items()\n            })\n        )",
  "def quantify(self):\n        \"\"\"Return new dataset with all numeric variables quantified and cached data loaded.\n\n        Notes\n        -----\n        Any cached/lazy-loaded data (except that in a Dask array) will be loaded into memory\n        by this operation. Do not utilize on moderate- to large-sized remote datasets before\n        subsetting!\n        \"\"\"\n        return self._dataset.map(lambda da: da.metpy.quantify()).assign_attrs(\n            self._dataset.attrs\n        )",
  "def dequantify(self):\n        \"\"\"Return new dataset with variables cast to magnitude and units on attribute.\"\"\"\n        return self._dataset.map(lambda da: da.metpy.dequantify()).assign_attrs(\n            self._dataset.attrs\n        )",
  "def decorator(func):\n        sig = signature(func)\n        if broadcast is not None:\n            for arg_name in broadcast:\n                if arg_name not in sig.parameters:\n                    raise ValueError(\n                        f'Cannot broadcast argument {arg_name} as it is not in function '\n                        'signature'\n                    )\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            bound_args = sig.bind(*args, **kwargs)\n\n            # Auto-broadcast select xarray arguments, and update bound_args\n            if broadcast is not None:\n                arg_names_to_broadcast = tuple(\n                    arg_name for arg_name in broadcast\n                    if arg_name in bound_args.arguments\n                    and isinstance(\n                        bound_args.arguments[arg_name],\n                        (xr.DataArray, xr.Variable)\n                    )\n                )\n                broadcasted_args = xr.broadcast(\n                    *(bound_args.arguments[arg_name] for arg_name in arg_names_to_broadcast)\n                )\n                for i, arg_name in enumerate(arg_names_to_broadcast):\n                    bound_args.arguments[arg_name] = broadcasted_args[i]\n\n            # Cast all Variables to their data and warn\n            # (need to do before match finding, since we don't want to rewrap as Variable)\n            def cast_variables(arg, arg_name):\n                _warnings.warn(\n                    f'Argument {arg_name} given as xarray Variable...casting to its data. '\n                    'xarray DataArrays are recommended instead.'\n                )\n                return arg.data\n            _mutate_arguments(bound_args, xr.Variable, cast_variables)\n\n            # Obtain proper match if referencing an input\n            match = list(wrap_like) if isinstance(wrap_like, tuple) else wrap_like\n            if isinstance(wrap_like, str):\n                match = bound_args.arguments[wrap_like]\n            elif isinstance(wrap_like, tuple):\n                for i, arg in enumerate(wrap_like):\n                    if isinstance(arg, str):\n                        match[i] = bound_args.arguments[arg]\n\n            # Cast all DataArrays to Pint Quantities\n            _mutate_arguments(bound_args, xr.DataArray, lambda arg, _: arg.metpy.unit_array)\n\n            # Optionally cast all Quantities to their magnitudes\n            if to_magnitude:\n                _mutate_arguments(bound_args, units.Quantity, lambda arg, _: arg.m)\n\n            # Evaluate inner calculation\n            result = func(*bound_args.args, **bound_args.kwargs)\n\n            # Wrap output based on match and match_unit\n            if match is None:\n                return result\n            else:\n                if match_unit:\n                    wrapping = _wrap_output_like_matching_units\n                else:\n                    wrapping = _wrap_output_like_not_matching_units\n\n                if isinstance(match, list):\n                    return tuple(wrapping(*args) for args in zip(result, match))\n                else:\n                    return wrapping(result, match)\n        return wrapper",
  "def wrapper(*args, **kwargs):\n        data_arrays = ([a for a in args if isinstance(a, xr.DataArray)]\n                       + [a for a in kwargs.values() if isinstance(a, xr.DataArray)])\n        if len(data_arrays) > 1:\n            first = data_arrays[0]\n            for other in data_arrays[1:]:\n                if not first.metpy.coordinates_identical(other):\n                    raise ValueError('Input DataArray arguments must be on same coordinates.')\n        return func(*args, **kwargs)",
  "def _to_magnitude(val, unit):\n        try:\n            return val.m_as(unit)\n        except AttributeError:\n            return val",
  "def wrapper(*args, **kwargs):\n        bound_args = signature(func).bind(*args, **kwargs)\n        bound_args.apply_defaults()\n\n        # Fill in vertical_dim\n        if 'vertical_dim' in bound_args.arguments:\n            a = next(dataarray_arguments(bound_args), None)\n            if a is not None:\n                try:\n                    bound_args.arguments['vertical_dim'] = a.metpy.find_axis_number('vertical')\n                except AttributeError:\n                    # If axis number not found, fall back to default but warn.\n                    _warnings.warn(\n                        'Vertical dimension number not found. Defaulting to initial dimension.'\n                    )\n\n        return func(*bound_args.args, **bound_args.kwargs)",
  "def __init__(self, data_array):\n            self.data_array = data_array",
  "def expand(self, key):\n            \"\"\"Parse key using xarray utils to ensure we have dimension names.\"\"\"\n            if not is_dict_like(key):\n                labels = expanded_indexer(key, self.data_array.ndim)\n                key = dict(zip(self.data_array.dims, labels))\n            return key",
  "def __getitem__(self, key):\n            key = _reassign_quantity_indexer(self.data_array, self.expand(key))\n            return self.data_array.loc[key]",
  "def __setitem__(self, key, value):\n            key = _reassign_quantity_indexer(self.data_array, self.expand(key))\n            self.data_array.loc[key] = value",
  "def __init__(self, dataset):\n            self.dataset = dataset",
  "def __getitem__(self, key):\n            parsed_key = _reassign_quantity_indexer(self.dataset, key)\n            return self.dataset.loc[parsed_key]",
  "def mapping_func(da):\n            new_value = mapping(da.name, **da.attrs)\n            if new_value is None:\n                return da\n            else:\n                return da.assign_attrs(**{attribute: new_value})",
  "def wrapper(*args, **kwargs):\n            bound_args = sig.bind(*args, **kwargs)\n\n            # Auto-broadcast select xarray arguments, and update bound_args\n            if broadcast is not None:\n                arg_names_to_broadcast = tuple(\n                    arg_name for arg_name in broadcast\n                    if arg_name in bound_args.arguments\n                    and isinstance(\n                        bound_args.arguments[arg_name],\n                        (xr.DataArray, xr.Variable)\n                    )\n                )\n                broadcasted_args = xr.broadcast(\n                    *(bound_args.arguments[arg_name] for arg_name in arg_names_to_broadcast)\n                )\n                for i, arg_name in enumerate(arg_names_to_broadcast):\n                    bound_args.arguments[arg_name] = broadcasted_args[i]\n\n            # Cast all Variables to their data and warn\n            # (need to do before match finding, since we don't want to rewrap as Variable)\n            def cast_variables(arg, arg_name):\n                _warnings.warn(\n                    f'Argument {arg_name} given as xarray Variable...casting to its data. '\n                    'xarray DataArrays are recommended instead.'\n                )\n                return arg.data\n            _mutate_arguments(bound_args, xr.Variable, cast_variables)\n\n            # Obtain proper match if referencing an input\n            match = list(wrap_like) if isinstance(wrap_like, tuple) else wrap_like\n            if isinstance(wrap_like, str):\n                match = bound_args.arguments[wrap_like]\n            elif isinstance(wrap_like, tuple):\n                for i, arg in enumerate(wrap_like):\n                    if isinstance(arg, str):\n                        match[i] = bound_args.arguments[arg]\n\n            # Cast all DataArrays to Pint Quantities\n            _mutate_arguments(bound_args, xr.DataArray, lambda arg, _: arg.metpy.unit_array)\n\n            # Optionally cast all Quantities to their magnitudes\n            if to_magnitude:\n                _mutate_arguments(bound_args, units.Quantity, lambda arg, _: arg.m)\n\n            # Evaluate inner calculation\n            result = func(*bound_args.args, **bound_args.kwargs)\n\n            # Wrap output based on match and match_unit\n            if match is None:\n                return result\n            else:\n                if match_unit:\n                    wrapping = _wrap_output_like_matching_units\n                else:\n                    wrapping = _wrap_output_like_not_matching_units\n\n                if isinstance(match, list):\n                    return tuple(wrapping(*args) for args in zip(result, match))\n                else:\n                    return wrapping(result, match)",
  "def mapping(varname, **kwargs):\n                return old_mapping.get(varname)",
  "def cast_variables(arg, arg_name):\n                _warnings.warn(\n                    f'Argument {arg_name} given as xarray Variable...casting to its data. '\n                    'xarray DataArrays are recommended instead.'\n                )\n                return arg.data",
  "def get_version():\n    \"\"\"Get MetPy's version.\n\n    Either get it from package metadata, or get it using version control information if\n    a development install.\n    \"\"\"\n    try:\n        from setuptools_scm import get_version\n        return get_version(root='../..', relative_to=__file__,\n                           version_scheme='post-release')\n    except (ImportError, LookupError):\n        from importlib.metadata import PackageNotFoundError, version\n\n        try:\n            return version(__package__)\n        except PackageNotFoundError:\n            return 'Unknown'",
  "def get_test_data(fname, as_file_obj=True, mode='rb'):\n    \"\"\"Access a file from MetPy's collection of test data.\"\"\"\n    path = POOCH.fetch(fname)\n    # If we want a file object, open it, trying to guess whether this should be binary mode\n    # or not\n    if as_file_obj:\n        return open(path, mode)  # noqa: SIM115\n\n    return path",
  "def example_data():\n    \"\"\"Create a sample xarray Dataset with 2D variables.\"\"\"\n    import xarray as xr\n\n    # make data based on Matplotlib example data for wind barbs\n    x, y = np.meshgrid(np.linspace(-3, 3, 25), np.linspace(-3, 3, 25))\n    z = (1 - x / 2 + x**5 + y**3) * np.exp(-x**2 - y**2)\n\n    # make u and v out of the z equation\n    u = -np.diff(z[:, 1:], axis=0) * 100 + 10\n    v = np.diff(z[1:, :], axis=1) * 100 + 10\n\n    # make t as colder air to the north\n    t = (np.linspace(15, 5, 24) * np.ones((24, 24))).T\n\n    # Make lat/lon data over the mid-latitudes\n    lats = np.linspace(30, 40, 24)\n    lons = np.linspace(360 - 100, 360 - 90, 24)\n\n    # place data into an xarray dataset object\n    lat = xr.DataArray(lats, attrs={'standard_name': 'latitude', 'units': 'degrees_north'})\n    lon = xr.DataArray(lons, attrs={'standard_name': 'longitude', 'units': 'degrees_east'})\n    uwind = xr.DataArray(u, coords=(lat, lon), dims=['lat', 'lon'],\n                         attrs={'standard_name': 'u-component_of_wind', 'units': 'm s-1'})\n    vwind = xr.DataArray(v, coords=(lat, lon), dims=['lat', 'lon'],\n                         attrs={'standard_name': 'u-component_of_wind', 'units': 'm s-1'})\n    temperature = xr.DataArray(t, coords=(lat, lon), dims=['lat', 'lon'],\n                               attrs={'standard_name': 'temperature', 'units': 'degC'})\n    return xr.Dataset({'uwind': uwind,\n                       'vwind': vwind,\n                       'temperature': temperature})",
  "class Registry:\n    \"\"\"Provide a generic function registry.\n\n    This provides a class to instantiate, which then has a `register` method that can\n    be used as a decorator on functions to register them under a particular name.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize an empty registry.\"\"\"\n        self._registry = {}\n\n    def register(self, name):\n        \"\"\"Register a callable with the registry under a particular name.\n\n        Parameters\n        ----------\n        name : str\n            The name under which to register a function\n\n        Returns\n        -------\n        dec : Callable\n            A decorator that takes a function and will register it under the name.\n\n        \"\"\"\n        def dec(func):\n            self._registry[name] = func\n            return func\n        return dec\n\n    def __getitem__(self, name):\n        \"\"\"Return any callable registered under name.\"\"\"\n        return self._registry[name]",
  "def broadcast_indices(indices, shape, axis):\n    \"\"\"Calculate index values to properly broadcast index array within data array.\n\n    The purpose of this function is work around the challenges trying to work with arrays of\n    indices that need to be \"broadcast\" against full slices for other dimensions.\n\n    See usage in `interpolate_1d` or `isentropic_interpolation`.\n    \"\"\"\n    ret = []\n    ndim = len(shape)\n    for dim in range(ndim):\n        if dim == axis:\n            ret.append(indices)\n        else:\n            broadcast_slice = [np.newaxis] * ndim\n            broadcast_slice[dim] = slice(None)\n            dim_inds = np.arange(shape[dim])\n            ret.append(dim_inds[tuple(broadcast_slice)])\n    return tuple(ret)",
  "def __init__(self):\n        \"\"\"Initialize an empty registry.\"\"\"\n        self._registry = {}",
  "def register(self, name):\n        \"\"\"Register a callable with the registry under a particular name.\n\n        Parameters\n        ----------\n        name : str\n            The name under which to register a function\n\n        Returns\n        -------\n        dec : Callable\n            A decorator that takes a function and will register it under the name.\n\n        \"\"\"\n        def dec(func):\n            self._registry[name] = func\n            return func\n        return dec",
  "def __getitem__(self, name):\n        \"\"\"Return any callable registered under name.\"\"\"\n        return self._registry[name]",
  "def dec(func):\n            self._registry[name] = func\n            return func",
  "def relative_humidity_from_dewpoint(temperature, dewpoint):\n    r\"\"\"Calculate the relative humidity.\n\n    Uses temperature and dewpoint to calculate relative humidity as the ratio of vapor\n    pressure to saturation vapor pressures.\n\n    Parameters\n    ----------\n    temperature : `pint.Quantity`\n        Air temperature\n\n    dewpoint : `pint.Quantity`\n        Dewpoint temperature\n\n    Returns\n    -------\n    `pint.Quantity`\n        Relative humidity\n\n    Examples\n    --------\n    >>> from metpy.calc import relative_humidity_from_dewpoint\n    >>> from metpy.units import units\n    >>> relative_humidity_from_dewpoint(25 * units.degC, 12 * units.degC).to('percent')\n    <Quantity(44.2484765, 'percent')>\n\n    See Also\n    --------\n    saturation_vapor_pressure\n\n    Notes\n    -----\n    .. math:: rh = \\frac{e(T_d)}{e_s(T)}\n\n    .. versionchanged:: 1.0\n       Renamed ``dewpt`` parameter to ``dewpoint``\n\n    \"\"\"\n    e = saturation_vapor_pressure(dewpoint)\n    e_s = saturation_vapor_pressure(temperature)\n    return (e / e_s)",
  "def exner_function(pressure, reference_pressure=mpconsts.P0):\n    r\"\"\"Calculate the Exner function.\n\n    .. math:: \\Pi = \\left( \\frac{p}{p_0} \\right)^\\kappa\n\n    This can be used to calculate potential temperature from temperature (and visa-versa),\n    since:\n\n    .. math:: \\Pi = \\frac{T}{\\theta}\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Total atmospheric pressure\n\n    reference_pressure : `pint.Quantity`, optional\n        The reference pressure against which to calculate the Exner function, defaults to\n        metpy.constants.P0\n\n    Returns\n    -------\n    `pint.Quantity`\n        Value of the Exner function at the given pressure\n\n    See Also\n    --------\n    potential_temperature\n    temperature_from_potential_temperature\n\n    \"\"\"\n    return (pressure / reference_pressure).to('dimensionless')**mpconsts.kappa",
  "def potential_temperature(pressure, temperature):\n    r\"\"\"Calculate the potential temperature.\n\n    Uses the Poisson equation to calculation the potential temperature\n    given `pressure` and `temperature`.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature : `pint.Quantity`\n        Air temperature\n\n    Returns\n    -------\n    `pint.Quantity`\n        Potential temperature corresponding to the temperature and pressure\n\n    See Also\n    --------\n    dry_lapse\n\n    Notes\n    -----\n    Formula:\n\n    .. math:: \\Theta = T (P_0 / P)^\\kappa\n\n    Examples\n    --------\n    >>> from metpy.units import units\n    >>> metpy.calc.potential_temperature(800. * units.mbar, 273. * units.kelvin)\n    <Quantity(290.972015, 'kelvin')>\n\n    \"\"\"\n    return temperature / exner_function(pressure)",
  "def temperature_from_potential_temperature(pressure, potential_temperature):\n    r\"\"\"Calculate the temperature from a given potential temperature.\n\n    Uses the inverse of the Poisson equation to calculate the temperature from a\n    given potential temperature at a specific pressure level.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Total atmospheric pressure\n\n    potential_temperature : `pint.Quantity`\n        Potential temperature\n\n    Returns\n    -------\n    `pint.Quantity`\n        Temperature corresponding to the potential temperature and pressure\n\n    See Also\n    --------\n    dry_lapse\n    potential_temperature\n\n    Notes\n    -----\n    Formula:\n\n    .. math:: T = \\Theta (P / P_0)^\\kappa\n\n    Examples\n    --------\n    >>> from metpy.units import units\n    >>> from metpy.calc import temperature_from_potential_temperature\n    >>> # potential temperature\n    >>> theta = np.array([ 286.12859679, 288.22362587]) * units.kelvin\n    >>> p = 850 * units.mbar\n    >>> T = temperature_from_potential_temperature(p, theta)\n\n    .. versionchanged:: 1.0\n       Renamed ``theta`` parameter to ``potential_temperature``\n\n    \"\"\"\n    return potential_temperature * exner_function(pressure)",
  "def dry_lapse(pressure, temperature, reference_pressure=None, vertical_dim=0):\n    r\"\"\"Calculate the temperature at a level assuming only dry processes.\n\n    This function lifts a parcel starting at ``temperature``, conserving\n    potential temperature. The starting pressure can be given by ``reference_pressure``.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure level(s) of interest\n\n    temperature : `pint.Quantity`\n        Starting temperature\n\n    reference_pressure : `pint.Quantity`, optional\n        Reference pressure; if not given, it defaults to the first element of the\n        pressure array.\n\n    Returns\n    -------\n    `pint.Quantity`\n       The parcel's resulting temperature at levels given by ``pressure``\n\n    Examples\n    --------\n    >>> from metpy.calc import dry_lapse\n    >>> from metpy.units import units\n    >>> plevs = [1000, 925, 850, 700] * units.hPa\n    >>> dry_lapse(plevs, 15 * units.degC).to('degC')\n    <Quantity([ 15.           8.65249458   1.92593808 -12.91786723], 'degree_Celsius')>\n\n    See Also\n    --------\n    moist_lapse : Calculate parcel temperature assuming liquid saturation processes\n    parcel_profile : Calculate complete parcel profile\n    potential_temperature\n\n    Notes\n    -----\n    Only reliably functions on 1D profiles (not higher-dimension vertical cross sections or\n    grids) unless reference_pressure is specified.\n\n    .. versionchanged:: 1.0\n       Renamed ``ref_pressure`` parameter to ``reference_pressure``\n\n    \"\"\"\n    if reference_pressure is None:\n        reference_pressure = pressure[0]\n    return temperature * (pressure / reference_pressure)**mpconsts.kappa",
  "def moist_lapse(pressure, temperature, reference_pressure=None):\n    r\"\"\"Calculate the temperature at a level assuming liquid saturation processes.\n\n    This function lifts a parcel starting at `temperature`. The starting pressure can\n    be given by `reference_pressure`. Essentially, this function is calculating moist\n    pseudo-adiabats.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure level(s) of interest\n\n    temperature : `pint.Quantity`\n        Starting temperature\n\n    reference_pressure : `pint.Quantity`, optional\n        Reference pressure; if not given, it defaults to the first element of the\n        pressure array.\n\n    Returns\n    -------\n    `pint.Quantity`\n       The resulting parcel temperature at levels given by `pressure`\n\n    Examples\n    --------\n    >>> from metpy.calc import moist_lapse\n    >>> from metpy.units import units\n    >>> plevs = [925, 850, 700, 500, 300, 200] * units.hPa\n    >>> moist_lapse(plevs, 5 * units.degC).to('degC')\n    <Quantity([  5.           0.99716773  -8.88545598 -28.37637988 -60.11086751\n    -83.33806983], 'degree_Celsius')>\n\n    See Also\n    --------\n    dry_lapse : Calculate parcel temperature assuming dry adiabatic processes\n    parcel_profile : Calculate complete parcel profile\n\n    Notes\n    -----\n    This function is implemented by integrating the following differential\n    equation:\n\n    .. math:: \\frac{dT}{dP} = \\frac{1}{P} \\frac{R_d T + L_v r_s}\n                                {C_{pd} + \\frac{L_v^2 r_s \\epsilon}{R_d T^2}}\n\n    This equation comes from [Bakhshaii2013]_.\n\n    Only reliably functions on 1D profiles (not higher-dimension vertical cross sections or\n    grids).\n\n    .. versionchanged:: 1.0\n       Renamed ``ref_pressure`` parameter to ``reference_pressure``\n\n    \"\"\"\n    def dt(p, t):\n        rs = saturation_mixing_ratio._nounit(p, t)\n        frac = (\n            (mpconsts.nounit.Rd * t + mpconsts.nounit.Lv * rs)\n            / (mpconsts.nounit.Cp_d + (\n                mpconsts.nounit.Lv * mpconsts.nounit.Lv * rs * mpconsts.nounit.epsilon\n                / (mpconsts.nounit.Rd * t**2)\n            ))\n        )\n        return frac / p\n\n    temperature = np.atleast_1d(temperature)\n    pressure = np.atleast_1d(pressure)\n    if reference_pressure is None:\n        reference_pressure = pressure[0]\n\n    if np.isnan(reference_pressure) or np.all(np.isnan(temperature)):\n        return np.full((temperature.size, pressure.size), np.nan)\n\n    pres_decreasing = (pressure[0] > pressure[-1])\n    if pres_decreasing:\n        # Everything is easier if pressures are in increasing order\n        pressure = pressure[::-1]\n\n    # It would be preferable to use a regular solver like RK45, but as of scipy 1.8.0\n    # anything other than LSODA goes into an infinite loop when given NaNs for y0.\n    solver_args = {'fun': dt, 'y0': temperature,\n                   'method': 'LSODA', 'atol': 1e-7, 'rtol': 1.5e-8}\n\n    # Need to handle close points to avoid an error in the solver\n    close = np.isclose(pressure, reference_pressure)\n    if np.any(close):\n        ret = np.broadcast_to(temperature[:, np.newaxis], (temperature.size, np.sum(close)))\n    else:\n        ret = np.empty((temperature.size, 0), dtype=temperature.dtype)\n\n    # Do we have any points above the reference pressure\n    points_above = (pressure < reference_pressure) & ~close\n    if np.any(points_above):\n        # Integrate upward--need to flip so values are properly ordered from ref to min\n        press_side = pressure[points_above][::-1]\n\n        # Flip on exit so t values correspond to increasing pressure\n        result = si.solve_ivp(t_span=(reference_pressure, press_side[-1]),\n                              t_eval=press_side, **solver_args)\n        if result.success:\n            ret = np.concatenate((result.y[..., ::-1], ret), axis=-1)\n        else:\n            raise ValueError('ODE Integration failed. This is likely due to trying to '\n                             'calculate at too small values of pressure.')\n\n    # Do we have any points below the reference pressure\n    points_below = ~points_above & ~close\n    if np.any(points_below):\n        # Integrate downward\n        press_side = pressure[points_below]\n        result = si.solve_ivp(t_span=(reference_pressure, press_side[-1]),\n                              t_eval=press_side, **solver_args)\n        if result.success:\n            ret = np.concatenate((ret, result.y), axis=-1)\n        else:\n            raise ValueError('ODE Integration failed. This is likely due to trying to '\n                             'calculate at too small values of pressure.')\n\n    if pres_decreasing:\n        ret = ret[..., ::-1]\n\n    return ret.squeeze()",
  "def lcl(pressure, temperature, dewpoint, max_iters=50, eps=1e-5):\n    r\"\"\"Calculate the lifted condensation level (LCL) from the starting point.\n\n    The starting state for the parcel is defined by `temperature`, `dewpoint`,\n    and `pressure`. If these are arrays, this function will return a LCL\n    for every index. This function does work with surface grids as a result.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Starting atmospheric pressure\n\n    temperature : `pint.Quantity`\n        Starting temperature\n\n    dewpoint : `pint.Quantity`\n        Starting dewpoint\n\n    Returns\n    -------\n    `pint.Quantity`\n        LCL pressure\n\n    `pint.Quantity`\n        LCL temperature\n\n    Other Parameters\n    ----------------\n    max_iters : int, optional\n        The maximum number of iterations to use in calculation, defaults to 50.\n\n    eps : float, optional\n        The desired relative error in the calculated value, defaults to 1e-5.\n\n    Examples\n    --------\n    >>> from metpy.calc import lcl\n    >>> from metpy.units import units\n    >>> lcl(943 * units.hPa, 33 * units.degC, 28 * units.degC)\n    (<Quantity(877.563323, 'hectopascal')>, <Quantity(26.7734921, 'degree_Celsius')>)\n\n    See Also\n    --------\n    parcel_profile\n\n    Notes\n    -----\n    This function is implemented using an iterative approach to solve for the\n    LCL. The basic algorithm is:\n\n    1. Find the dewpoint from the LCL pressure and starting mixing ratio\n    2. Find the LCL pressure from the starting temperature and dewpoint\n    3. Iterate until convergence\n\n    The function is guaranteed to finish by virtue of the `max_iters` counter.\n\n    .. versionchanged:: 1.0\n       Renamed ``dewpt`` parameter to ``dewpoint``\n\n    \"\"\"\n    def _lcl_iter(p, p0, w, t):\n        nonlocal nan_mask\n        td = globals()['dewpoint']._nounit(vapor_pressure._nounit(p, w))\n        p_new = (p0 * (td / t) ** (1. / mpconsts.nounit.kappa))\n        nan_mask = nan_mask | np.isnan(p_new)\n        return np.where(np.isnan(p_new), p, p_new)\n\n    # Handle nans by creating a mask that gets set by our _lcl_iter function if it\n    # ever encounters a nan, at which point pressure is set to p, stopping iteration.\n    nan_mask = False\n    w = mixing_ratio._nounit(saturation_vapor_pressure._nounit(dewpoint), pressure)\n    lcl_p = so.fixed_point(_lcl_iter, pressure, args=(pressure, w, temperature),\n                           xtol=eps, maxiter=max_iters)\n    lcl_p = np.where(nan_mask, np.nan, lcl_p)\n\n    # np.isclose needed if surface is LCL due to precision error with np.log in dewpoint.\n    # Causes issues with parcel_profile_with_lcl if removed. Issue #1187\n    lcl_p = np.where(np.isclose(lcl_p, pressure), pressure, lcl_p)\n\n    return lcl_p, globals()['dewpoint']._nounit(vapor_pressure._nounit(lcl_p, w))",
  "def ccl(pressure, temperature, dewpoint, height=None, mixed_layer_depth=None, which='top'):\n    r\"\"\"Calculate the convective condensation level (CCL) and convective temperature.\n\n    This function is implemented directly based on the definition of the CCL,\n    as in [USAF1990]_, and finding where the ambient temperature profile intersects\n    the line of constant mixing ratio starting at the surface, using the surface dewpoint\n    or the average dewpoint of a shallow layer near the surface.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile. This array must be from high to low pressure.\n\n    temperature : `pint.Quantity`\n        Temperature at the levels given by `pressure`\n\n    dewpoint : `pint.Quantity`\n        Dewpoint at the levels given by `pressure`\n\n    height : `pint.Quantity`, optional\n        Atmospheric heights at the levels given by `pressure`.\n        Only needed when specifying a mixed layer depth as a height.\n\n    mixed_layer_depth : `pint.Quantity`, optional\n        The thickness of the mixed layer as a pressure or height above the bottom\n        of the layer (default None).\n\n    which: str, optional\n        Pick which CCL value to return; must be one of 'top', 'bottom', or 'all'.\n        'top' returns the lowest-pressure CCL (default),\n        'bottom' returns the highest-pressure CCL,\n        'all' returns every CCL in a `Pint.Quantity` array.\n\n    Returns\n    -------\n    `pint.Quantity`\n        CCL Pressure\n\n    `pint.Quantity`\n        CCL Temperature\n\n    `pint.Quantity`\n        Convective Temperature\n\n    See Also\n    --------\n    lcl, lfc, el\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    Examples\n    --------\n    >>> import metpy.calc as mpcalc\n    >>> from metpy.units import units\n    >>> pressure = [993, 957, 925, 886, 850, 813, 798, 732, 716, 700] * units.mbar\n    >>> temperature = [34.6, 31.1, 27.8, 24.3, 21.4, 19.6, 18.7, 13, 13.5, 13] * units.degC\n    >>> dewpoint = [19.6, 18.7, 17.8, 16.3, 12.4, -0.4, -3.8, -6, -13.2, -11] * units.degC\n    >>> ccl_p, ccl_t, t_c = mpcalc.ccl(pressure, temperature, dewpoint)\n    >>> ccl_p, t_c\n    (<Quantity(758.348093, 'millibar')>, <Quantity(38.4336274, 'degree_Celsius')>)\n    \"\"\"\n    pressure, temperature, dewpoint = _remove_nans(pressure, temperature, dewpoint)\n    _check_pressure_error(pressure)\n\n    # If the mixed layer is not defined, take the starting dewpoint to be the\n    # first element of the dewpoint array and calculate the corresponding mixing ratio.\n    if mixed_layer_depth is None:\n        p_start, dewpoint_start = pressure[0], dewpoint[0]\n        vapor_pressure_start = saturation_vapor_pressure(dewpoint_start)\n        r_start = mixing_ratio(vapor_pressure_start, p_start)\n\n    # Else, calculate the mixing ratio of the mixed layer.\n    else:\n        vapor_pressure_profile = saturation_vapor_pressure(dewpoint)\n        r_profile = mixing_ratio(vapor_pressure_profile, pressure)\n        r_start = mixed_layer(pressure, r_profile, height=height,\n                              depth=mixed_layer_depth)[0]\n\n    # rt_profile is the temperature-pressure profile with a fixed mixing ratio\n    rt_profile = globals()['dewpoint'](vapor_pressure(pressure, r_start))\n\n    x, y = find_intersections(pressure, rt_profile, temperature,\n                              direction='increasing', log_x=True)\n\n    # In the case of multiple CCLs, select which to return\n    if which == 'top':\n        x, y = x[-1], y[-1]\n    elif which == 'bottom':\n        x, y = x[0], y[0]\n    elif which not in ['top', 'bottom', 'all']:\n        raise ValueError(f'Invalid option for \"which\": {which}. Valid options are '\n                         '\"top\", \"bottom\", and \"all\".')\n\n    x, y = x.to(pressure.units), y.to(temperature.units)\n    return x, y, dry_lapse(pressure[0], y, x).to(temperature.units)",
  "def lfc(pressure, temperature, dewpoint, parcel_temperature_profile=None, dewpoint_start=None,\n        which='top'):\n    r\"\"\"Calculate the level of free convection (LFC).\n\n    This works by finding the first intersection of the ideal parcel path and\n    the measured parcel temperature. If this intersection occurs below the LCL,\n    the LFC is determined to be the same as the LCL, based upon the conditions\n    set forth in [USAF1990]_, pg 4-14, where a parcel must be lifted dry adiabatically\n    to saturation before it can freely rise.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile. This array must be from high to low pressure.\n\n    temperature : `pint.Quantity`\n        Temperature at the levels given by `pressure`\n\n    dewpoint : `pint.Quantity`\n        Dewpoint at the levels given by `pressure`\n\n    parcel_temperature_profile: `pint.Quantity`, optional\n        The parcel's temperature profile from which to calculate the LFC. Defaults to the\n        surface parcel profile.\n\n    dewpoint_start: `pint.Quantity`, optional\n        Dewpoint of the parcel for which to calculate the LFC. Defaults to the surface\n        dewpoint.\n\n    which: str, optional\n        Pick which LFC to return. Options are 'top', 'bottom', 'wide', 'most_cape', and 'all';\n        'top' returns the lowest-pressure LFC (default),\n        'bottom' returns the highest-pressure LFC,\n        'wide' returns the LFC whose corresponding EL is farthest away,\n        'most_cape' returns the LFC that results in the most CAPE in the profile.\n\n    Returns\n    -------\n    `pint.Quantity`\n        LFC pressure, or array of same if which='all'\n\n    `pint.Quantity`\n        LFC temperature, or array of same if which='all'\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, lfc\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # calculate LFC\n    >>> lfc(p, T, Td)\n    (<Quantity(968.171757, 'hectopascal')>, <Quantity(25.8362857, 'degree_Celsius')>)\n\n    See Also\n    --------\n    parcel_profile\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``dewpt``,``dewpoint_start`` parameters to ``dewpoint``, ``dewpoint_start``\n\n    \"\"\"\n    # Default to surface parcel if no profile or starting pressure level is given\n    if parcel_temperature_profile is None:\n        pressure, temperature, dewpoint = _remove_nans(pressure, temperature, dewpoint)\n        new_profile = parcel_profile_with_lcl(pressure, temperature, dewpoint)\n        pressure, temperature, dewpoint, parcel_temperature_profile = new_profile\n        parcel_temperature_profile = parcel_temperature_profile.to(temperature.units)\n    else:\n        new_profile = _remove_nans(pressure, temperature, dewpoint, parcel_temperature_profile)\n        pressure, temperature, dewpoint, parcel_temperature_profile = new_profile\n\n    if dewpoint_start is None:\n        dewpoint_start = dewpoint[0]\n\n    # The parcel profile and data may have the same first data point.\n    # If that is the case, ignore that point to get the real first\n    # intersection for the LFC calculation. Use logarithmic interpolation.\n    if np.isclose(parcel_temperature_profile[0].to(temperature.units).m, temperature[0].m):\n        x, y = find_intersections(pressure[1:], parcel_temperature_profile[1:],\n                                  temperature[1:], direction='increasing', log_x=True)\n    else:\n        x, y = find_intersections(pressure, parcel_temperature_profile,\n                                  temperature, direction='increasing', log_x=True)\n\n    # Compute LCL for this parcel for future comparisons\n    this_lcl = lcl(pressure[0], parcel_temperature_profile[0], dewpoint_start)\n\n    # The LFC could:\n    # 1) Not exist\n    # 2) Exist but be equal to the LCL\n    # 3) Exist and be above the LCL\n\n    # LFC does not exist or is LCL\n    if len(x) == 0:\n        # Is there any positive area above the LCL?\n        mask = pressure < this_lcl[0]\n        if np.all(_less_or_close(parcel_temperature_profile[mask], temperature[mask])):\n            # LFC doesn't exist\n            x = units.Quantity(np.nan, pressure.units)\n            y = units.Quantity(np.nan, temperature.units)\n        else:  # LFC = LCL\n            x, y = this_lcl\n        return x, y\n\n    # LFC exists. Make sure it is no lower than the LCL\n    else:\n        idx = x < this_lcl[0]\n        # LFC height < LCL height, so set LFC = LCL\n        if not any(idx):\n            el_pressure, _ = find_intersections(pressure[1:], parcel_temperature_profile[1:],\n                                                temperature[1:], direction='decreasing',\n                                                log_x=True)\n            if np.min(el_pressure) > this_lcl[0]:\n                x = units.Quantity(np.nan, pressure.units)\n                y = units.Quantity(np.nan, temperature.units)\n            else:\n                x, y = this_lcl\n            return x, y\n        # Otherwise, find all LFCs that exist above the LCL\n        # What is returned depends on which flag as described in the docstring\n        else:\n            return _multiple_el_lfc_options(x, y, idx, which, pressure,\n                                            parcel_temperature_profile, temperature,\n                                            dewpoint, intersect_type='LFC')",
  "def _multiple_el_lfc_options(intersect_pressures, intersect_temperatures, valid_x,\n                             which, pressure, parcel_temperature_profile, temperature,\n                             dewpoint, intersect_type):\n    \"\"\"Choose which ELs and LFCs to return from a sounding.\"\"\"\n    p_list, t_list = intersect_pressures[valid_x], intersect_temperatures[valid_x]\n    if which == 'all':\n        x, y = p_list, t_list\n    elif which == 'bottom':\n        x, y = p_list[0], t_list[0]\n    elif which == 'top':\n        x, y = p_list[-1], t_list[-1]\n    elif which == 'wide':\n        x, y = _wide_option(intersect_type, p_list, t_list, pressure,\n                            parcel_temperature_profile, temperature)\n    elif which == 'most_cape':\n        x, y = _most_cape_option(intersect_type, p_list, t_list, pressure, temperature,\n                                 dewpoint, parcel_temperature_profile)\n    else:\n        raise ValueError('Invalid option for \"which\". Valid options are \"top\", \"bottom\", '\n                         '\"wide\", \"most_cape\", and \"all\".')\n    return x, y",
  "def _wide_option(intersect_type, p_list, t_list, pressure, parcel_temperature_profile,\n                 temperature):\n    \"\"\"Calculate the LFC or EL that produces the greatest distance between these points.\"\"\"\n    # zip the LFC and EL lists together and find greatest difference\n    if intersect_type == 'LFC':\n        # Find EL intersection pressure values\n        lfc_p_list = p_list\n        el_p_list, _ = find_intersections(pressure[1:], parcel_temperature_profile[1:],\n                                          temperature[1:], direction='decreasing',\n                                          log_x=True)\n    else:  # intersect_type == 'EL'\n        el_p_list = p_list\n        # Find LFC intersection pressure values\n        lfc_p_list, _ = find_intersections(pressure, parcel_temperature_profile,\n                                           temperature, direction='increasing',\n                                           log_x=True)\n    diff = [lfc_p.m - el_p.m for lfc_p, el_p in zip(lfc_p_list, el_p_list)]\n    return (p_list[np.where(diff == np.max(diff))][0],\n            t_list[np.where(diff == np.max(diff))][0])",
  "def _most_cape_option(intersect_type, p_list, t_list, pressure, temperature, dewpoint,\n                      parcel_temperature_profile):\n    \"\"\"Calculate the LFC or EL that produces the most CAPE in the profile.\"\"\"\n    # Need to loop through all possible combinations of cape, find greatest cape profile\n    cape_list, pair_list = [], []\n    for which_lfc in ['top', 'bottom']:\n        for which_el in ['top', 'bottom']:\n            cape, _ = cape_cin(pressure, temperature, dewpoint, parcel_temperature_profile,\n                               which_lfc=which_lfc, which_el=which_el)\n            cape_list.append(cape.m)\n            pair_list.append([which_lfc, which_el])\n    (lfc_chosen, el_chosen) = pair_list[np.where(cape_list == np.max(cape_list))[0][0]]\n    if intersect_type == 'LFC':\n        if lfc_chosen == 'top':\n            x, y = p_list[-1], t_list[-1]\n        else:  # 'bottom' is returned\n            x, y = p_list[0], t_list[0]\n    else:  # EL is returned\n        if el_chosen == 'top':\n            x, y = p_list[-1], t_list[-1]\n        else:\n            x, y = p_list[0], t_list[0]\n    return x, y",
  "def el(pressure, temperature, dewpoint, parcel_temperature_profile=None, which='top'):\n    r\"\"\"Calculate the equilibrium level.\n\n    This works by finding the last intersection of the ideal parcel path and\n    the measured environmental temperature. If there is one or fewer intersections, there is\n    no equilibrium level.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile. This array must be from high to low pressure.\n\n    temperature : `pint.Quantity`\n        Temperature at the levels given by `pressure`\n\n    dewpoint : `pint.Quantity`\n        Dewpoint at the levels given by `pressure`\n\n    parcel_temperature_profile: `pint.Quantity`, optional\n        The parcel's temperature profile from which to calculate the EL. Defaults to the\n        surface parcel profile.\n\n    which: str, optional\n        Pick which EL to return. Options are 'top', 'bottom', 'wide', 'most_cape', and 'all'.\n        'top' returns the lowest-pressure EL, default.\n        'bottom' returns the highest-pressure EL.\n        'wide' returns the EL whose corresponding LFC is farthest away.\n        'most_cape' returns the EL that results in the most CAPE in the profile.\n\n    Returns\n    -------\n    `pint.Quantity`\n        EL pressure, or array of same if which='all'\n\n    `pint.Quantity`\n        EL temperature, or array of same if which='all'\n\n    Examples\n    --------\n    >>> from metpy.calc import el, dewpoint_from_relative_humidity, parcel_profile\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # compute parcel profile temperature\n    >>> prof = parcel_profile(p, T[0], Td[0]).to('degC')\n    >>> # calculate EL\n    >>> el(p, T, Td, prof)\n    (<Quantity(111.739463, 'hectopascal')>, <Quantity(-76.3112792, 'degree_Celsius')>)\n\n    See Also\n    --------\n    parcel_profile\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``dewpt`` parameter to ``dewpoint``\n\n    \"\"\"\n    # Default to surface parcel if no profile or starting pressure level is given\n    if parcel_temperature_profile is None:\n        pressure, temperature, dewpoint = _remove_nans(pressure, temperature, dewpoint)\n        new_profile = parcel_profile_with_lcl(pressure, temperature, dewpoint)\n        pressure, temperature, dewpoint, parcel_temperature_profile = new_profile\n        parcel_temperature_profile = parcel_temperature_profile.to(temperature.units)\n    else:\n        new_profile = _remove_nans(pressure, temperature, dewpoint, parcel_temperature_profile)\n        pressure, temperature, dewpoint, parcel_temperature_profile = new_profile\n\n    # If the top of the sounding parcel is warmer than the environment, there is no EL\n    if parcel_temperature_profile[-1] > temperature[-1]:\n        return (units.Quantity(np.nan, pressure.units),\n                units.Quantity(np.nan, temperature.units))\n\n    # Interpolate in log space to find the appropriate pressure - units have to be stripped\n    # and reassigned to allow np.log() to function properly.\n    x, y = find_intersections(pressure[1:], parcel_temperature_profile[1:], temperature[1:],\n                              direction='decreasing', log_x=True)\n    lcl_p, _ = lcl(pressure[0], temperature[0], dewpoint[0])\n    if len(x) > 0 and x[-1] < lcl_p:\n        idx = x < lcl_p\n        return _multiple_el_lfc_options(x, y, idx, which, pressure,\n                                        parcel_temperature_profile, temperature, dewpoint,\n                                        intersect_type='EL')\n    else:\n        return (units.Quantity(np.nan, pressure.units),\n                units.Quantity(np.nan, temperature.units))",
  "def parcel_profile(pressure, temperature, dewpoint):\n    r\"\"\"Calculate the profile a parcel takes through the atmosphere.\n\n    The parcel starts at `temperature`, and `dewpoint`, lifted up\n    dry adiabatically to the LCL, and then moist adiabatically from there.\n    `pressure` specifies the pressure levels for the profile.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure level(s) of interest. This array must be from\n        high to low pressure.\n\n    temperature : `pint.Quantity`\n        Starting temperature\n\n    dewpoint : `pint.Quantity`\n        Starting dewpoint\n\n    Returns\n    -------\n    `pint.Quantity`\n        The parcel's temperatures at the specified pressure levels\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, parcel_profile\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # computer parcel temperature\n    >>> parcel_profile(p, T[0], Td[0]).to('degC')\n    <Quantity([  29.3          28.61221952   25.22214738   23.46097535   21.5835928\n    19.57260398   17.40636185   15.05748615   12.49064866    9.6592539\n        6.50023491    2.92560365   -1.19172846   -6.04257884  -11.92497517\n    -19.3176536   -28.97672464  -41.94444385  -50.01173076  -59.30936248\n    -70.02760604  -82.53084923  -94.2966713  -100.99074331 -108.40829933\n    -116.77024489 -126.42910222 -138.00649584 -144.86615886 -152.78967029], 'degree_Celsius')>\n\n    See Also\n    --------\n    lcl, moist_lapse, dry_lapse, parcel_profile_with_lcl, parcel_profile_with_lcl_as_dataset\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Duplicate pressure levels return duplicate parcel temperatures. Consider preprocessing\n    low-precision, high frequency profiles with tools like `scipy.medfilt`,\n    `pandas.drop_duplicates`, or `numpy.unique`.\n\n    Will only return Pint Quantities, even when given xarray DataArray profiles. To\n    obtain a xarray Dataset instead, use `parcel_profile_with_lcl_as_dataset` instead.\n\n    .. versionchanged:: 1.0\n       Renamed ``dewpt`` parameter to ``dewpoint``\n\n    \"\"\"\n    _, _, _, t_l, _, t_u = _parcel_profile_helper(pressure, temperature, dewpoint)\n    return concatenate((t_l, t_u))",
  "def parcel_profile_with_lcl(pressure, temperature, dewpoint):\n    r\"\"\"Calculate the profile a parcel takes through the atmosphere.\n\n    The parcel starts at `temperature`, and `dewpoint`, lifted up\n    dry adiabatically to the LCL, and then moist adiabatically from there.\n    `pressure` specifies the pressure levels for the profile. This function returns\n    a profile that includes the LCL.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure level(s) of interest. This array must be from\n        high to low pressure.\n\n    temperature : `pint.Quantity`\n        Atmospheric temperature at the levels in `pressure`. The first entry should be at\n        the same level as the first `pressure` data point.\n\n    dewpoint : `pint.Quantity`\n        Atmospheric dewpoint at the levels in `pressure`. The first entry should be at\n        the same level as the first `pressure` data point.\n\n    Returns\n    -------\n    pressure : `pint.Quantity`\n        The parcel profile pressures, which includes the specified levels and the LCL\n\n    ambient_temperature : `pint.Quantity`\n        Atmospheric temperature values, including the value interpolated to the LCL level\n\n    ambient_dew_point : `pint.Quantity`\n        Atmospheric dewpoint values, including the value interpolated to the LCL level\n\n    profile_temperature : `pint.Quantity`\n        The parcel profile temperatures at all of the levels in the returned pressures array,\n        including the LCL\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, parcel_profile_with_lcl\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # compute parcel temperature\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> p_wLCL, T_wLCL, Td_wLCL, prof_wLCL = parcel_profile_with_lcl(p, T, Td)\n\n    See Also\n    --------\n    lcl, moist_lapse, dry_lapse, parcel_profile, parcel_profile_with_lcl_as_dataset\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Duplicate pressure levels return duplicate parcel temperatures. Consider preprocessing\n    low-precision, high frequency profiles with tools like `scipy.medfilt`,\n    `pandas.drop_duplicates`, or `numpy.unique`.\n\n    Will only return Pint Quantities, even when given xarray DataArray profiles. To\n    obtain a xarray Dataset instead, use `parcel_profile_with_lcl_as_dataset` instead.\n\n    .. versionchanged:: 1.0\n       Renamed ``dewpt`` parameter to ``dewpoint``\n\n    \"\"\"\n    p_l, p_lcl, p_u, t_l, t_lcl, t_u = _parcel_profile_helper(pressure, temperature[0],\n                                                              dewpoint[0])\n    new_press = concatenate((p_l, p_lcl, p_u))\n    prof_temp = concatenate((t_l, t_lcl, t_u))\n    new_temp = _insert_lcl_level(pressure, temperature, p_lcl)\n    new_dewp = _insert_lcl_level(pressure, dewpoint, p_lcl)\n    return new_press, new_temp, new_dewp, prof_temp",
  "def parcel_profile_with_lcl_as_dataset(pressure, temperature, dewpoint):\n    r\"\"\"Calculate the profile a parcel takes through the atmosphere, returning a Dataset.\n\n    The parcel starts at `temperature`, and `dewpoint`, lifted up\n    dry adiabatically to the LCL, and then moist adiabatically from there.\n    `pressure` specifies the pressure levels for the profile. This function returns\n    a profile that includes the LCL.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        The atmospheric pressure level(s) of interest. This array must be from\n        high to low pressure.\n    temperature : `pint.Quantity`\n        The atmospheric temperature at the levels in `pressure`. The first entry should be at\n        the same level as the first `pressure` data point.\n    dewpoint : `pint.Quantity`\n        The atmospheric dewpoint at the levels in `pressure`. The first entry should be at\n        the same level as the first `pressure` data point.\n\n    Returns\n    -------\n    profile : `xarray.Dataset`\n        The interpolated profile with three data variables: ambient_temperature,\n        ambient_dew_point, and profile_temperature, all of which are on an isobaric\n        coordinate.\n\n    See Also\n    --------\n    lcl, moist_lapse, dry_lapse, parcel_profile, parcel_profile_with_lcl\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n\n    \"\"\"\n    p, ambient_temperature, ambient_dew_point, profile_temperature = parcel_profile_with_lcl(\n        pressure,\n        temperature,\n        dewpoint\n    )\n    return xr.Dataset(\n        {\n            'ambient_temperature': (\n                ('isobaric',),\n                ambient_temperature,\n                {'standard_name': 'air_temperature'}\n            ),\n            'ambient_dew_point': (\n                ('isobaric',),\n                ambient_dew_point,\n                {'standard_name': 'dew_point_temperature'}\n            ),\n            'parcel_temperature': (\n                ('isobaric',),\n                profile_temperature,\n                {'long_name': 'air_temperature_of_lifted_parcel'}\n            )\n        },\n        coords={\n            'isobaric': (\n                'isobaric',\n                p.m,\n                {'units': str(p.units), 'standard_name': 'air_pressure'}\n            )\n        }\n    )",
  "def _check_pressure(pressure):\n    \"\"\"Check that pressure does not increase.\n\n    Returns True if the pressure does not increase from one level to the next;\n    otherwise, returns False.\n\n    \"\"\"\n    return np.all(pressure[:-1] >= pressure[1:])",
  "def _check_pressure_error(pressure):\n    \"\"\"Raise an `InvalidSoundingError` if _check_pressure returns False.\"\"\"\n    if not _check_pressure(pressure):\n        raise InvalidSoundingError('Pressure increases between at least two points in '\n                                   'your sounding. Using scipy.signal.medfilt may fix this.')",
  "def _parcel_profile_helper(pressure, temperature, dewpoint):\n    \"\"\"Help calculate parcel profiles.\n\n    Returns the temperature and pressure, above, below, and including the LCL. The\n    other calculation functions decide what to do with the pieces.\n\n    \"\"\"\n    # Check that pressure does not increase.\n    _check_pressure_error(pressure)\n\n    # Find the LCL\n    press_lcl, temp_lcl = lcl(pressure[0], temperature, dewpoint)\n    press_lcl = press_lcl.to(pressure.units)\n\n    # Find the dry adiabatic profile, *including* the LCL. We need >= the LCL in case the\n    # LCL is included in the levels. It's slightly redundant in that case, but simplifies\n    # the logic for removing it later.\n    press_lower = concatenate((pressure[pressure >= press_lcl], press_lcl))\n    temp_lower = dry_lapse(press_lower, temperature)\n\n    # If the pressure profile doesn't make it to the lcl, we can stop here\n    if _greater_or_close(np.nanmin(pressure), press_lcl):\n        return (press_lower[:-1], press_lcl, units.Quantity(np.array([]), press_lower.units),\n                temp_lower[:-1], temp_lcl, units.Quantity(np.array([]), temp_lower.units))\n\n    # Establish profile above LCL\n    press_upper = concatenate((press_lcl, pressure[pressure < press_lcl]))\n\n    # Remove duplicate pressure values from remaining profile. Needed for solve_ivp in\n    # moist_lapse. unique will return remaining values sorted ascending.\n    unique, indices, counts = np.unique(press_upper.m, return_inverse=True, return_counts=True)\n    unique = units.Quantity(unique, press_upper.units)\n    if np.any(counts > 1):\n        _warnings.warn(f'Duplicate pressure(s) {unique[counts > 1]:~P} provided. '\n                       'Output profile includes duplicate temperatures as a result.')\n\n    # Find moist pseudo-adiabatic profile starting at the LCL, reversing above sorting\n    temp_upper = moist_lapse(unique[::-1], temp_lower[-1]).to(temp_lower.units)\n    temp_upper = temp_upper[::-1][indices]\n\n    # Return profile pieces\n    return (press_lower[:-1], press_lcl, press_upper[1:],\n            temp_lower[:-1], temp_lcl, temp_upper[1:])",
  "def _insert_lcl_level(pressure, temperature, lcl_pressure):\n    \"\"\"Insert the LCL pressure into the profile.\"\"\"\n    interp_temp = interpolate_1d(lcl_pressure, pressure, temperature)\n\n    # Pressure needs to be increasing for searchsorted, so flip it and then convert\n    # the index back to the original array\n    loc = pressure.size - pressure[::-1].searchsorted(lcl_pressure)\n    return units.Quantity(np.insert(temperature.m, loc, interp_temp.m), temperature.units)",
  "def vapor_pressure(pressure, mixing_ratio):\n    r\"\"\"Calculate water vapor (partial) pressure.\n\n    Given total ``pressure`` and water vapor ``mixing_ratio``, calculates the\n    partial pressure of water vapor.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Total atmospheric pressure\n\n    mixing_ratio : `pint.Quantity`\n        Dimensionless mass mixing ratio\n\n    Returns\n    -------\n    `pint.Quantity`\n        Ambient water vapor (partial) pressure in the same units as ``pressure``\n\n    Examples\n    --------\n    >>> from metpy.calc import vapor_pressure\n    >>> from metpy.units import units\n    >>> vapor_pressure(988 * units.hPa, 18 * units('g/kg')).to('hPa')\n    <Quantity(27.789371, 'hectopascal')>\n\n    See Also\n    --------\n    saturation_vapor_pressure, dewpoint\n\n    Notes\n    -----\n    This function is a straightforward implementation of the equation given in many places,\n    such as [Hobbs1977]_ pg.71:\n\n    .. math:: e = p \\frac{r}{r + \\epsilon}\n\n    .. versionchanged:: 1.0\n       Renamed ``mixing`` parameter to ``mixing_ratio``\n\n    \"\"\"\n    return pressure * mixing_ratio / (mpconsts.nounit.epsilon + mixing_ratio)",
  "def saturation_vapor_pressure(temperature):\n    r\"\"\"Calculate the saturation water vapor (partial) pressure.\n\n    Parameters\n    ----------\n    temperature : `pint.Quantity`\n        Air temperature\n\n    Returns\n    -------\n    `pint.Quantity`\n        Saturation water vapor (partial) pressure\n\n    Examples\n    --------\n    >>> from metpy.calc import saturation_vapor_pressure\n    >>> from metpy.units import units\n    >>> saturation_vapor_pressure(25 * units.degC).to('hPa')\n    <Quantity(31.6742944, 'hectopascal')>\n\n    See Also\n    --------\n    vapor_pressure, dewpoint\n\n    Notes\n    -----\n    Instead of temperature, dewpoint may be used in order to calculate\n    the actual (ambient) water vapor (partial) pressure.\n\n    The formula used is that from [Bolton1980]_ for T in degrees Celsius:\n\n    .. math:: 6.112 e^\\frac{17.67T}{T + 243.5}\n\n    \"\"\"\n    # Converted from original in terms of C to use kelvin.\n    return mpconsts.nounit.sat_pressure_0c * np.exp(\n        17.67 * (temperature - 273.15) / (temperature - 29.65)\n    )",
  "def dewpoint_from_relative_humidity(temperature, relative_humidity):\n    r\"\"\"Calculate the ambient dewpoint given air temperature and relative humidity.\n\n    Parameters\n    ----------\n    temperature : `pint.Quantity`\n        Air temperature\n\n    relative_humidity : `pint.Quantity`\n        Relative humidity expressed as a ratio in the range 0 < relative_humidity <= 1\n\n    Returns\n    -------\n    `pint.Quantity`\n        Dewpoint temperature\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity\n    >>> from metpy.units import units\n    >>> dewpoint_from_relative_humidity(10 * units.degC, 50 * units.percent)\n    <Quantity(0.0536760815, 'degree_Celsius')>\n\n    .. versionchanged:: 1.0\n       Renamed ``rh`` parameter to ``relative_humidity``\n\n    See Also\n    --------\n    dewpoint, saturation_vapor_pressure\n\n    \"\"\"\n    if np.any(relative_humidity > 1.2):\n        _warnings.warn('Relative humidity >120%, ensure proper units.')\n    return dewpoint(relative_humidity * saturation_vapor_pressure(temperature))",
  "def dewpoint(vapor_pressure):\n    r\"\"\"Calculate the ambient dewpoint given the vapor pressure.\n\n    Parameters\n    ----------\n    vapor_pressure : `pint.Quantity`\n        Water vapor partial pressure\n\n    Returns\n    -------\n    `pint.Quantity`\n        Dewpoint temperature\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint\n    >>> from metpy.units import units\n    >>> dewpoint(22 * units.hPa)\n    <Quantity(19.0291018, 'degree_Celsius')>\n\n    See Also\n    --------\n    dewpoint_from_relative_humidity, saturation_vapor_pressure, vapor_pressure\n\n    Notes\n    -----\n    This function inverts the [Bolton1980]_ formula for saturation vapor\n    pressure to instead calculate the temperature. This yields the following formula for\n    dewpoint in degrees Celsius, where :math:`e` is the ambient vapor pressure in millibars:\n\n    .. math:: T = \\frac{243.5 \\log(e / 6.112)}{17.67 - \\log(e / 6.112)}\n\n    .. versionchanged:: 1.0\n       Renamed ``e`` parameter to ``vapor_pressure``\n\n    \"\"\"\n    val = np.log(vapor_pressure / mpconsts.nounit.sat_pressure_0c)\n    return mpconsts.nounit.zero_degc + 243.5 * val / (17.67 - val)",
  "def mixing_ratio(partial_press, total_press, molecular_weight_ratio=mpconsts.nounit.epsilon):\n    r\"\"\"Calculate the mixing ratio of a gas.\n\n    This calculates mixing ratio given its partial pressure and the total pressure of\n    the air. There are no required units for the input arrays, other than that\n    they have the same units.\n\n    Parameters\n    ----------\n    partial_press : `pint.Quantity`\n        Partial pressure of the constituent gas\n\n    total_press : `pint.Quantity`\n        Total air pressure\n\n    molecular_weight_ratio : `pint.Quantity` or float, optional\n        The ratio of the molecular weight of the constituent gas to that assumed\n        for air. Defaults to the ratio for water vapor to dry air\n        (:math:`\\epsilon\\approx0.622`).\n\n    Returns\n    -------\n    `pint.Quantity`\n        The (mass) mixing ratio, dimensionless (e.g. Kg/Kg or g/g)\n\n    Examples\n    --------\n    >>> from metpy.calc import mixing_ratio\n    >>> from metpy.units import units\n    >>> mixing_ratio(25 * units.hPa, 1000 * units.hPa).to('g/kg')\n    <Quantity(15.9476131, 'gram / kilogram')>\n\n    See Also\n    --------\n    saturation_mixing_ratio, vapor_pressure\n\n    Notes\n    -----\n    This function is a straightforward implementation of the equation given in many places,\n    such as [Hobbs1977]_ pg.73:\n\n    .. math:: r = \\epsilon \\frac{e}{p - e}\n\n    .. versionchanged:: 1.0\n       Renamed ``part_press``, ``tot_press`` parameters to ``partial_press``, ``total_press``\n\n    \"\"\"\n    return molecular_weight_ratio * partial_press / (total_press - partial_press)",
  "def saturation_mixing_ratio(total_press, temperature):\n    r\"\"\"Calculate the saturation mixing ratio of water vapor.\n\n    This calculation is given total atmospheric pressure and air temperature.\n\n    Parameters\n    ----------\n    total_press: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Air temperature\n\n    Returns\n    -------\n    `pint.Quantity`\n        Saturation mixing ratio, dimensionless\n\n    Examples\n    --------\n    >>> from metpy.calc import saturation_mixing_ratio\n    >>> from metpy.units import units\n    >>> saturation_mixing_ratio(983 * units.hPa, 25 * units.degC).to('g/kg')\n    <Quantity(20.7079932, 'gram / kilogram')>\n\n    Notes\n    -----\n    This function is a straightforward implementation of the equation given in many places,\n    such as [Hobbs1977]_ pg.73:\n\n    .. math:: r_s = \\epsilon \\frac{e_s}{p - e_s}\n\n    .. versionchanged:: 1.0\n       Renamed ``tot_press`` parameter to ``total_press``\n\n    \"\"\"\n    return mixing_ratio._nounit(saturation_vapor_pressure._nounit(temperature), total_press)",
  "def equivalent_potential_temperature(pressure, temperature, dewpoint):\n    r\"\"\"Calculate equivalent potential temperature.\n\n    This calculation must be given an air parcel's pressure, temperature, and dewpoint.\n    The implementation uses the formula outlined in [Bolton1980]_:\n\n    First, the LCL temperature is calculated:\n\n    .. math:: T_{L}=\\frac{1}{\\frac{1}{T_{D}-56}+\\frac{ln(T_{K}/T_{D})}{800}}+56\n\n    Which is then used to calculate the potential temperature at the LCL:\n\n    .. math:: \\theta_{DL}=T_{K}\\left(\\frac{1000}{p-e}\\right)^k\n              \\left(\\frac{T_{K}}{T_{L}}\\right)^{.28r}\n\n    Both of these are used to calculate the final equivalent potential temperature:\n\n    .. math:: \\theta_{E}=\\theta_{DL}\\exp\\left[\\left(\\frac{3036.}{T_{L}}\n                                              -1.78\\right)*r(1+.448r)\\right]\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Temperature of parcel\n\n    dewpoint: `pint.Quantity`\n        Dewpoint of parcel\n\n    Returns\n    -------\n    `pint.Quantity`\n        Equivalent potential temperature of the parcel\n\n    Examples\n    --------\n    >>> from metpy.calc import equivalent_potential_temperature\n    >>> from metpy.units import units\n    >>> equivalent_potential_temperature(850*units.hPa, 20*units.degC, 18*units.degC)\n    <Quantity(353.937994, 'kelvin')>\n\n    Notes\n    -----\n    [Bolton1980]_ formula for Theta-e is used, since according to\n    [DaviesJones2009]_ it is the most accurate non-iterative formulation\n    available.\n\n    \"\"\"\n    t = temperature.to('kelvin').magnitude\n    td = dewpoint.to('kelvin').magnitude\n    r = saturation_mixing_ratio(pressure, dewpoint).magnitude\n    e = saturation_vapor_pressure(dewpoint)\n\n    t_l = 56 + 1. / (1. / (td - 56) + np.log(t / td) / 800.)\n    th_l = potential_temperature(pressure - e, temperature) * (t / t_l) ** (0.28 * r)\n    return th_l * np.exp(r * (1 + 0.448 * r) * (3036. / t_l - 1.78))",
  "def saturation_equivalent_potential_temperature(pressure, temperature):\n    r\"\"\"Calculate saturation equivalent potential temperature.\n\n    This calculation must be given an air parcel's pressure and temperature.\n    The implementation uses the formula outlined in [Bolton1980]_ for the\n    equivalent potential temperature, and assumes a saturated process.\n\n    First, because we assume a saturated process, the temperature at the LCL is\n    equivalent to the current temperature. Therefore the following equation.\n\n    .. math:: T_{L}=\\frac{1}{\\frac{1}{T_{D}-56}+\\frac{ln(T_{K}/T_{D})}{800}}+56\n\n    reduces to:\n\n    .. math:: T_{L} = T_{K}\n\n    Then the potential temperature at the temperature/LCL is calculated:\n\n    .. math:: \\theta_{DL}=T_{K}\\left(\\frac{1000}{p-e}\\right)^k\n              \\left(\\frac{T_{K}}{T_{L}}\\right)^{.28r}\n\n    However, because:\n\n    .. math:: T_{L} = T_{K}\n\n    it follows that:\n\n    .. math:: \\theta_{DL}=T_{K}\\left(\\frac{1000}{p-e}\\right)^k\n\n    Both of these are used to calculate the final equivalent potential temperature:\n\n    .. math:: \\theta_{E}=\\theta_{DL}\\exp\\left[\\left(\\frac{3036.}{T_{K}}\n                                              -1.78\\right)*r(1+.448r)\\right]\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Temperature of parcel\n\n    Returns\n    -------\n    `pint.Quantity`\n        Saturation equivalent potential temperature of the parcel\n\n    Examples\n    --------\n    >>> from metpy.calc import saturation_equivalent_potential_temperature\n    >>> from metpy.units import units\n    >>> saturation_equivalent_potential_temperature(500 * units.hPa, -20 * units.degC)\n    <Quantity(313.804174, 'kelvin')>\n\n    Notes\n    -----\n    [Bolton1980]_ formula for Theta-e is used (for saturated case), since according to\n    [DaviesJones2009]_ it is the most accurate non-iterative formulation\n    available.\n\n    \"\"\"\n    t = temperature.to('kelvin').magnitude\n    p = pressure.to('hPa').magnitude\n    e = saturation_vapor_pressure(temperature).to('hPa').magnitude\n    r = saturation_mixing_ratio(pressure, temperature).magnitude\n\n    th_l = t * (1000 / (p - e)) ** mpconsts.kappa\n    th_es = th_l * np.exp((3036. / t - 1.78) * r * (1 + 0.448 * r))\n\n    return units.Quantity(th_es, units.kelvin)",
  "def virtual_temperature(\n    temperature, mixing_ratio, molecular_weight_ratio=mpconsts.nounit.epsilon\n):\n    r\"\"\"Calculate virtual temperature.\n\n    This calculation must be given an air parcel's temperature and mixing ratio.\n    The implementation uses the formula outlined in [Hobbs2006]_ pg.80.\n\n    Parameters\n    ----------\n    temperature: `pint.Quantity`\n        Air temperature\n\n    mixing_ratio : `pint.Quantity`\n        Mass mixing ratio (dimensionless)\n\n    molecular_weight_ratio : `pint.Quantity` or float, optional\n        The ratio of the molecular weight of the constituent gas to that assumed\n        for air. Defaults to the ratio for water vapor to dry air.\n        (:math:`\\epsilon\\approx0.622`)\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding virtual temperature of the parcel\n\n    Examples\n    --------\n    >>> from metpy.calc import virtual_temperature\n    >>> from metpy.units import units\n    >>> virtual_temperature(283 * units.K, 12 * units('g/kg'))\n    <Quantity(285.039709, 'kelvin')>\n\n    Notes\n    -----\n    .. math:: T_v = T \\frac{\\text{w} + \\epsilon}{\\epsilon\\,(1 + \\text{w})}\n\n    .. versionchanged:: 1.0\n       Renamed ``mixing`` parameter to ``mixing_ratio``\n\n    \"\"\"\n    return temperature * ((mixing_ratio + molecular_weight_ratio)\n                          / (molecular_weight_ratio * (1 + mixing_ratio)))",
  "def virtual_temperature_from_dewpoint(\n    pressure, temperature, dewpoint, molecular_weight_ratio=mpconsts.nounit.epsilon\n):\n    r\"\"\"Calculate virtual temperature.\n\n    This calculation must be given an air parcel's temperature and mixing ratio.\n    The implementation uses the formula outlined in [Hobbs2006]_ pg.80.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Air temperature\n\n    dewpoint : `pint.Quantity`\n        Dewpoint temperature\n\n    molecular_weight_ratio : `pint.Quantity` or float, optional\n        The ratio of the molecular weight of the constituent gas to that assumed\n        for air. Defaults to the ratio for water vapor to dry air.\n        (:math:`\\epsilon\\approx0.622`)\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding virtual temperature of the parcel\n\n    Examples\n    --------\n    >>> from metpy.calc import virtual_temperature_from_dewpoint\n    >>> from metpy.units import units\n    >>> virtual_temperature_from_dewpoint(1000 * units.hPa, 30 * units.degC, 25 * units.degC)\n    <Quantity(33.6739865, 'degree_Celsius')>\n\n    Notes\n    -----\n    .. math:: T_v = T \\frac{\\text{w} + \\epsilon}{\\epsilon\\,(1 + \\text{w})}\n\n    .. versionchanged:: 1.0\n       Renamed ``mixing`` parameter to ``mixing_ratio``\n\n    \"\"\"\n    # Convert dewpoint to mixing ratio\n    mixing_ratio = saturation_mixing_ratio(pressure, dewpoint)\n\n    # Calculate virtual temperature with given parameters\n    return virtual_temperature(temperature, mixing_ratio, molecular_weight_ratio)",
  "def virtual_potential_temperature(pressure, temperature, mixing_ratio,\n                                  molecular_weight_ratio=mpconsts.epsilon):\n    r\"\"\"Calculate virtual potential temperature.\n\n    This calculation must be given an air parcel's pressure, temperature, and mixing ratio.\n    The implementation uses the formula outlined in [Markowski2010]_ pg.13.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Air temperature\n\n    mixing_ratio : `pint.Quantity`\n        Dimensionless mass mixing ratio\n\n    molecular_weight_ratio : `pint.Quantity` or float, optional\n        The ratio of the molecular weight of the constituent gas to that assumed\n        for air. Defaults to the ratio for water vapor to dry air.\n        (:math:`\\epsilon\\approx0.622`)\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding virtual potential temperature of the parcel\n\n    Examples\n    --------\n    >>> from metpy.calc import virtual_potential_temperature\n    >>> from metpy.units import units\n    >>> virtual_potential_temperature(500 * units.hPa, -15 * units.degC, 1 * units('g/kg'))\n    <Quantity(314.87946, 'kelvin')>\n\n    Notes\n    -----\n    .. math:: \\Theta_v = \\Theta \\frac{\\text{w} + \\epsilon}{\\epsilon\\,(1 + \\text{w})}\n\n    .. versionchanged:: 1.0\n       Renamed ``mixing`` parameter to ``mixing_ratio``\n\n    \"\"\"\n    pottemp = potential_temperature(pressure, temperature)\n    return virtual_temperature(pottemp, mixing_ratio, molecular_weight_ratio)",
  "def density(pressure, temperature, mixing_ratio, molecular_weight_ratio=mpconsts.epsilon):\n    r\"\"\"Calculate density.\n\n    This calculation must be given an air parcel's pressure, temperature, and mixing ratio.\n    The implementation uses the formula outlined in [Hobbs2006]_ pg.67.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Air temperature (or the virtual temperature if the mixing ratio is set to 0)\n\n    mixing_ratio : `pint.Quantity`\n        Mass mixing ratio (dimensionless)\n\n    molecular_weight_ratio : `pint.Quantity` or float, optional\n        The ratio of the molecular weight of the constituent gas to that assumed\n        for air. Defaults to the ratio for water vapor to dry air.\n        (:math:`\\epsilon\\approx0.622`)\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding density of the parcel\n\n    Examples\n    --------\n    >>> from metpy.calc import density\n    >>> from metpy.units import units\n    >>> density(1000 * units.hPa, 10 * units.degC, 24 * units('g/kg'))\n    <Quantity(1.21307146, 'kilogram / meter ** 3')>\n\n    Notes\n    -----\n    .. math:: \\rho = \\frac{\\epsilon p\\,(1+w)}{R_dT\\,(w+\\epsilon)}\n\n    .. versionchanged:: 1.0\n       Renamed ``mixing`` parameter to ``mixing_ratio``\n\n    \"\"\"\n    virttemp = virtual_temperature(temperature, mixing_ratio, molecular_weight_ratio)\n    return (pressure / (mpconsts.Rd * virttemp)).to('kg/m**3')",
  "def relative_humidity_wet_psychrometric(pressure, dry_bulb_temperature, wet_bulb_temperature,\n                                        **kwargs):\n    r\"\"\"Calculate the relative humidity with wet bulb and dry bulb temperatures.\n\n    This uses a psychrometric relationship as outlined in [WMO8]_, with\n    coefficients from [Fan1987]_.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    dry_bulb_temperature: `pint.Quantity`\n        Dry bulb temperature\n\n    wet_bulb_temperature: `pint.Quantity`\n        Wet bulb temperature\n\n    Returns\n    -------\n    `pint.Quantity`\n        Relative humidity\n\n    Examples\n    --------\n    >>> from metpy.calc import relative_humidity_wet_psychrometric\n    >>> from metpy.units import units\n    >>> relative_humidity_wet_psychrometric(1000 * units.hPa, 19 * units.degC,\n    ...                                     10 * units.degC).to('percent')\n    <Quantity(30.4311332, 'percent')>\n\n    See Also\n    --------\n    psychrometric_vapor_pressure_wet, saturation_vapor_pressure\n\n    Notes\n    -----\n    .. math:: RH = \\frac{e}{e_s}\n\n    * :math:`RH` is relative humidity as a unitless ratio\n    * :math:`e` is vapor pressure from the wet psychrometric calculation\n    * :math:`e_s` is the saturation vapor pressure\n\n    .. versionchanged:: 1.0\n       Changed signature from\n       ``(dry_bulb_temperature, web_bulb_temperature, pressure, **kwargs)``\n\n    \"\"\"\n    return (psychrometric_vapor_pressure_wet(pressure, dry_bulb_temperature,\n                                             wet_bulb_temperature, **kwargs)\n            / saturation_vapor_pressure(dry_bulb_temperature))",
  "def psychrometric_vapor_pressure_wet(pressure, dry_bulb_temperature, wet_bulb_temperature,\n                                     psychrometer_coefficient=None):\n    r\"\"\"Calculate the vapor pressure with wet bulb and dry bulb temperatures.\n\n    This uses a psychrometric relationship as outlined in [WMO8]_, with\n    coefficients from [Fan1987]_.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    dry_bulb_temperature: `pint.Quantity`\n        Dry bulb temperature\n\n    wet_bulb_temperature: `pint.Quantity`\n        Wet bulb temperature\n\n    psychrometer_coefficient: `pint.Quantity`, optional\n        Psychrometer coefficient. Defaults to 6.21e-4 K^-1.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Vapor pressure\n\n    Examples\n    --------\n    >>> from metpy.calc import psychrometric_vapor_pressure_wet, saturation_vapor_pressure\n    >>> from metpy.units import units\n    >>> vp = psychrometric_vapor_pressure_wet(958 * units.hPa, 25 * units.degC,\n    ...                                       12 * units.degC)\n    >>> print(f'Vapor Pressure: {vp:.2f}')\n    Vapor Pressure: 628.15 pascal\n    >>> rh = (vp / saturation_vapor_pressure(25 * units.degC)).to('percent')\n    >>> print(f'RH: {rh:.2f}')\n    RH: 19.83 percent\n\n    See Also\n    --------\n    saturation_vapor_pressure\n\n    Notes\n    -----\n    .. math:: e' = e'_w(T_w) - A p (T - T_w)\n\n    * :math:`e'` is vapor pressure\n    * :math:`e'_w(T_w)` is the saturation vapor pressure with respect to water at temperature\n      :math:`T_w`\n    * :math:`p` is the pressure of the wet bulb\n    * :math:`T` is the temperature of the dry bulb\n    * :math:`T_w` is the temperature of the wet bulb\n    * :math:`A` is the psychrometer coefficient\n\n    Psychrometer coefficient depends on the specific instrument being used and the ventilation\n    of the instrument.\n\n    .. versionchanged:: 1.0\n       Changed signature from\n       ``(dry_bulb_temperature, wet_bulb_temperature, pressure, psychrometer_coefficient)``\n\n    \"\"\"\n    if psychrometer_coefficient is None:\n        psychrometer_coefficient = units.Quantity(6.21e-4, '1/K')\n    return (saturation_vapor_pressure(wet_bulb_temperature) - psychrometer_coefficient\n            * pressure * (dry_bulb_temperature - wet_bulb_temperature).to('kelvin'))",
  "def mixing_ratio_from_relative_humidity(pressure, temperature, relative_humidity):\n    r\"\"\"Calculate the mixing ratio from relative humidity, temperature, and pressure.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Air temperature\n\n    relative_humidity: array-like\n        The relative humidity expressed as a unitless ratio in the range [0, 1]. Can also pass\n        a percentage if proper units are attached.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Mixing ratio (dimensionless)\n\n    Examples\n    --------\n    >>> from metpy.calc import mixing_ratio_from_relative_humidity\n    >>> from metpy.units import units\n    >>> p = 1000. * units.hPa\n    >>> T = 28.1 * units.degC\n    >>> rh = .65\n    >>> mixing_ratio_from_relative_humidity(p, T, rh).to('g/kg')\n    <Quantity(15.9828362, 'gram / kilogram')>\n\n    See Also\n    --------\n    relative_humidity_from_mixing_ratio, saturation_mixing_ratio\n\n    Notes\n    -----\n    Formula adapted from [Hobbs1977]_ pg. 74.\n\n    .. math:: w = (rh)(w_s)\n\n    * :math:`w` is mixing ratio\n    * :math:`rh` is relative humidity as a unitless ratio\n    * :math:`w_s` is the saturation mixing ratio\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(relative_humidity, temperature, pressure)``\n\n    \"\"\"\n    return (relative_humidity\n            * saturation_mixing_ratio(pressure, temperature)).to('dimensionless')",
  "def relative_humidity_from_mixing_ratio(pressure, temperature, mixing_ratio):\n    r\"\"\"Calculate the relative humidity from mixing ratio, temperature, and pressure.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Air temperature\n\n    mixing_ratio: `pint.Quantity`\n        Dimensionless mass mixing ratio\n\n    Returns\n    -------\n    `pint.Quantity`\n        Relative humidity\n\n    Examples\n    --------\n    >>> from metpy.calc import relative_humidity_from_mixing_ratio\n    >>> from metpy.units import units\n    >>> relative_humidity_from_mixing_ratio(1013.25 * units.hPa,\n    ...                                     30 * units.degC, 18/1000).to('percent')\n    <Quantity(66.1763544, 'percent')>\n\n    See Also\n    --------\n    mixing_ratio_from_relative_humidity, saturation_mixing_ratio\n\n    Notes\n    -----\n    Formula based on that from [Hobbs1977]_ pg. 74.\n\n    .. math:: rh = \\frac{w}{w_s}\n\n    * :math:`rh` is relative humidity as a unitless ratio\n    * :math:`w` is mixing ratio\n    * :math:`w_s` is the saturation mixing ratio\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(mixing_ratio, temperature, pressure)``\n\n    \"\"\"\n    return mixing_ratio / saturation_mixing_ratio(pressure, temperature)",
  "def mixing_ratio_from_specific_humidity(specific_humidity):\n    r\"\"\"Calculate the mixing ratio from specific humidity.\n\n    Parameters\n    ----------\n    specific_humidity: `pint.Quantity`\n        Specific humidity of air\n\n    Returns\n    -------\n    `pint.Quantity`\n        Mixing ratio\n\n    Examples\n    --------\n    >>> from metpy.calc import mixing_ratio_from_specific_humidity\n    >>> from metpy.units import units\n    >>> sh = [4.77, 12.14, 6.16, 15.29, 12.25] * units('g/kg')\n    >>> mixing_ratio_from_specific_humidity(sh).to('g/kg')\n    <Quantity([ 4.79286195 12.28919078  6.19818079 15.52741416 12.40192356],\n    'gram / kilogram')>\n\n    See Also\n    --------\n    mixing_ratio, specific_humidity_from_mixing_ratio\n\n    Notes\n    -----\n    Formula from [Salby1996]_ pg. 118.\n\n    .. math:: w = \\frac{q}{1-q}\n\n    * :math:`w` is mixing ratio\n    * :math:`q` is the specific humidity\n\n    \"\"\"\n    return specific_humidity / (1 - specific_humidity)",
  "def specific_humidity_from_mixing_ratio(mixing_ratio):\n    r\"\"\"Calculate the specific humidity from the mixing ratio.\n\n    Parameters\n    ----------\n    mixing_ratio: `pint.Quantity`\n        Mixing ratio\n\n    Returns\n    -------\n    `pint.Quantity`\n        Specific humidity\n\n    Examples\n    --------\n    >>> from metpy.calc import specific_humidity_from_mixing_ratio\n    >>> from metpy.units import units\n    >>> specific_humidity_from_mixing_ratio(19 * units('g/kg'))\n    <Quantity(18.6457311, 'gram / kilogram')>\n\n    See Also\n    --------\n    mixing_ratio, mixing_ratio_from_specific_humidity\n\n    Notes\n    -----\n    Formula from [Salby1996]_ pg. 118.\n\n    .. math:: q = \\frac{w}{1+w}\n\n    * :math:`w` is mixing ratio\n    * :math:`q` is the specific humidity\n\n    \"\"\"\n    return mixing_ratio / (1 + mixing_ratio)",
  "def relative_humidity_from_specific_humidity(pressure, temperature, specific_humidity):\n    r\"\"\"Calculate the relative humidity from specific humidity, temperature, and pressure.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Air temperature\n\n    specific_humidity: `pint.Quantity`\n        Specific humidity of air\n\n    Returns\n    -------\n    `pint.Quantity`\n        Relative humidity\n\n    Examples\n    --------\n    >>> from metpy.calc import relative_humidity_from_specific_humidity\n    >>> from metpy.units import units\n    >>> relative_humidity_from_specific_humidity(1013.25 * units.hPa,\n    ...                                          30 * units.degC, 18/1000).to('percent')\n    <Quantity(67.3893629, 'percent')>\n\n    See Also\n    --------\n    relative_humidity_from_mixing_ratio\n\n    Notes\n    -----\n    Formula based on that from [Hobbs1977]_ pg. 74. and [Salby1996]_ pg. 118.\n\n    .. math:: RH = \\frac{q}{(1-q)w_s}\n\n    * :math:`RH` is relative humidity as a unitless ratio\n    * :math:`q` is specific humidity\n    * :math:`w_s` is the saturation mixing ratio\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(specific_humidity, temperature, pressure)``\n\n    \"\"\"\n    return (mixing_ratio_from_specific_humidity(specific_humidity)\n            / saturation_mixing_ratio(pressure, temperature))",
  "def cape_cin(pressure, temperature, dewpoint, parcel_profile, which_lfc='bottom',\n             which_el='top'):\n    r\"\"\"Calculate CAPE and CIN.\n\n    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)\n    of a given upper air profile and parcel path. CIN is integrated between the surface and\n    LFC, CAPE is integrated between the LFC and EL (or top of sounding). Intersection points\n    of the measured temperature profile and parcel profile are logarithmically interpolated.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure level(s) of interest, in order from highest to\n        lowest pressure\n\n    temperature : `pint.Quantity`\n        Atmospheric temperature corresponding to pressure\n\n    dewpoint : `pint.Quantity`\n        Atmospheric dewpoint corresponding to pressure\n\n    parcel_profile : `pint.Quantity`\n        Temperature profile of the parcel\n\n    which_lfc : str\n        Choose which LFC to integrate from. Valid options are 'top', 'bottom', 'wide',\n        and 'most_cape'. Default is 'bottom'.\n\n    which_el : str\n        Choose which EL to integrate to. Valid options are 'top', 'bottom', 'wide',\n        and 'most_cape'. Default is 'top'.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Convective Available Potential Energy (CAPE)\n\n    `pint.Quantity`\n        Convective Inhibition (CIN)\n\n    Examples\n    --------\n    >>> from metpy.calc import cape_cin, dewpoint_from_relative_humidity, parcel_profile\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # compture parcel temperature\n    >>> prof = parcel_profile(p, T[0], Td[0]).to('degC')\n    >>> # calculate surface based CAPE/CIN\n    >>> cape_cin(p, T, Td, prof)\n    (<Quantity(4703.77306, 'joule / kilogram')>, <Quantity(0, 'joule / kilogram')>)\n\n    See Also\n    --------\n    lfc, el\n\n    Notes\n    -----\n    Formula adopted from [Hobbs1977]_.\n\n    .. math:: \\text{CAPE} = -R_d \\int_{LFC}^{EL}\n            (T_{{v}_{parcel}} - T_{{v}_{env}}) d\\text{ln}(p)\n\n    .. math:: \\text{CIN} = -R_d \\int_{SFC}^{LFC}\n            (T_{{v}_{parcel}} - T_{{v}_{env}}) d\\text{ln}(p)\n\n\n    * :math:`CAPE` is convective available potential energy\n    * :math:`CIN` is convective inhibition\n    * :math:`LFC` is pressure of the level of free convection\n    * :math:`EL` is pressure of the equilibrium level\n    * :math:`SFC` is the level of the surface or beginning of parcel path\n    * :math:`R_d` is the gas constant\n    * :math:`g` is gravitational acceleration\n    * :math:`T_{{v}_{parcel}}` is the parcel virtual temperature\n    * :math:`T_{{v}_{env}}` is environment virtual temperature\n    * :math:`p` is atmospheric pressure\n\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``dewpt`` parameter to ``dewpoint``\n\n    \"\"\"\n    pressure, temperature, dewpoint, parcel_profile = _remove_nans(pressure, temperature,\n                                                                   dewpoint, parcel_profile)\n\n    pressure_lcl, _ = lcl(pressure[0], temperature[0], dewpoint[0])\n    below_lcl = pressure > pressure_lcl\n\n    # The mixing ratio of the parcel comes from the dewpoint below the LCL, is saturated\n    # based on the temperature above the LCL\n    parcel_mixing_ratio = np.where(below_lcl, saturation_mixing_ratio(pressure, dewpoint),\n                                   saturation_mixing_ratio(pressure, temperature))\n\n    # Convert the temperature/parcel profile to virtual temperature\n    temperature = virtual_temperature_from_dewpoint(pressure, temperature, dewpoint)\n    parcel_profile = virtual_temperature(parcel_profile, parcel_mixing_ratio)\n\n    # Calculate LFC limit of integration\n    lfc_pressure, _ = lfc(pressure, temperature, dewpoint,\n                          parcel_temperature_profile=parcel_profile, which=which_lfc)\n\n    # If there is no LFC, no need to proceed.\n    if np.isnan(lfc_pressure):\n        return units.Quantity(0, 'J/kg'), units.Quantity(0, 'J/kg')\n    else:\n        lfc_pressure = lfc_pressure.magnitude\n\n    # Calculate the EL limit of integration\n    el_pressure, _ = el(pressure, temperature, dewpoint,\n                        parcel_temperature_profile=parcel_profile, which=which_el)\n\n    # No EL and we use the top reading of the sounding.\n    if np.isnan(el_pressure):\n        el_pressure = pressure[-1].magnitude\n    else:\n        el_pressure = el_pressure.magnitude\n\n    # Difference between the parcel path and measured temperature profiles\n    y = (parcel_profile - temperature).to(units.degK)\n\n    # Estimate zero crossings\n    x, y = _find_append_zero_crossings(np.copy(pressure), y)\n\n    # CAPE\n    # Only use data between the LFC and EL for calculation\n    p_mask = _less_or_close(x.m, lfc_pressure) & _greater_or_close(x.m, el_pressure)\n    x_clipped = x[p_mask].magnitude\n    y_clipped = y[p_mask].magnitude\n    cape = (mpconsts.Rd\n            * units.Quantity(np.trapz(y_clipped, np.log(x_clipped)), 'K')).to(units('J/kg'))\n\n    # CIN\n    # Only use data between the surface and LFC for calculation\n    p_mask = _greater_or_close(x.m, lfc_pressure)\n    x_clipped = x[p_mask].magnitude\n    y_clipped = y[p_mask].magnitude\n    cin = (mpconsts.Rd\n           * units.Quantity(np.trapz(y_clipped, np.log(x_clipped)), 'K')).to(units('J/kg'))\n\n    # Set CIN to 0 if it's returned as a positive value (#1190)\n    if cin > units.Quantity(0, 'J/kg'):\n        cin = units.Quantity(0, 'J/kg')\n    return cape, cin",
  "def _find_append_zero_crossings(x, y):\n    r\"\"\"\n    Find and interpolate zero crossings.\n\n    Estimate the zero crossings of an x,y series and add estimated crossings to series,\n    returning a sorted array with no duplicate values.\n\n    Parameters\n    ----------\n    x : `pint.Quantity`\n        X values of data\n\n    y : `pint.Quantity`\n        Y values of data\n\n    Returns\n    -------\n    x : `pint.Quantity`\n        X values of data\n    y : `pint.Quantity`\n        Y values of data\n\n    \"\"\"\n    crossings = find_intersections(x[1:], y[1:],\n                                   units.Quantity(np.zeros_like(y[1:]), y.units), log_x=True)\n    x = concatenate((x, crossings[0]))\n    y = concatenate((y, crossings[1]))\n\n    # Resort so that data are in order\n    sort_idx = np.argsort(x)\n    x = x[sort_idx]\n    y = y[sort_idx]\n\n    # Remove duplicate data points if there are any\n    keep_idx = np.ediff1d(x.magnitude, to_end=[1]) > 1e-6\n    x = x[keep_idx]\n    y = y[keep_idx]\n    return x, y",
  "def most_unstable_parcel(pressure, temperature, dewpoint, height=None, bottom=None,\n                         depth=None):\n    \"\"\"\n    Determine the most unstable parcel in a layer.\n\n    Determines the most unstable parcel of air by calculating the equivalent\n    potential temperature and finding its maximum in the specified layer.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Atmospheric pressure profile\n\n    temperature: `pint.Quantity`\n        Atmospheric temperature profile\n\n    dewpoint: `pint.Quantity`\n        Atmospheric dewpoint profile\n\n    height: `pint.Quantity`, optional\n        Atmospheric height profile. Standard atmosphere assumed when None (the default).\n\n    bottom: `pint.Quantity`, optional\n        Bottom of the layer to consider for the calculation in pressure or height.\n        Defaults to using the bottom pressure or height.\n\n    depth: `pint.Quantity`, optional\n        Depth of the layer to consider for the calculation in pressure or height. Defaults\n        to 300 hPa.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Pressure of the most unstable parcel in the profile\n\n    `pint.Quantity`\n        Temperature of the most unstable parcel in the profile\n\n    `pint.Quantity`\n        Dewpoint of the most unstable parcel in the profile\n\n    int\n        The index of the most unstable parcel within the original data\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, most_unstable_parcel\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # find most unstable parcel of depth 50 hPa\n    >>> most_unstable_parcel(p, T, Td, depth=50*units.hPa)\n    (<Quantity(1008.0, 'hectopascal')>, <Quantity(29.3, 'degree_Celsius')>,\n    <Quantity(26.5176931, 'degree_Celsius')>, 0)\n\n    See Also\n    --------\n    get_layer\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``heights`` parameter to ``height``\n\n    \"\"\"\n    if depth is None:\n        depth = units.Quantity(300, 'hPa')\n    p_layer, t_layer, td_layer = get_layer(pressure, temperature, dewpoint, bottom=bottom,\n                                           depth=depth, height=height, interpolate=False)\n    theta_e = equivalent_potential_temperature(p_layer, t_layer, td_layer)\n    max_idx = np.argmax(theta_e)\n    return p_layer[max_idx], t_layer[max_idx], td_layer[max_idx], max_idx",
  "def isentropic_interpolation(levels, pressure, temperature, *args, vertical_dim=0,\n                             temperature_out=False, max_iters=50, eps=1e-6,\n                             bottom_up_search=True, **kwargs):\n    r\"\"\"Interpolate data in isobaric coordinates to isentropic coordinates.\n\n    Parameters\n    ----------\n    levels : array-like\n        One-dimensional array of desired potential temperature surfaces\n\n    pressure : array-like\n        One-dimensional array of pressure levels\n\n    temperature : array-like\n        Array of temperature\n\n    args : array-like, optional\n        Any additional variables will be interpolated to each isentropic level.\n\n    Returns\n    -------\n    list\n        List with pressure at each isentropic level, followed by each additional\n        argument interpolated to isentropic coordinates.\n\n    Other Parameters\n    ----------------\n    vertical_dim : int, optional\n        The axis corresponding to the vertical in the temperature array, defaults to 0.\n\n    temperature_out : bool, optional\n        If true, will calculate temperature and output as the last item in the output list.\n        Defaults to False.\n\n    max_iters : int, optional\n        Maximum number of iterations to use in calculation, defaults to 50.\n\n    eps : float, optional\n        The desired absolute error in the calculated value, defaults to 1e-6.\n\n    bottom_up_search : bool, optional\n        Controls whether to search for levels bottom-up (starting at lower indices),\n        or top-down (starting at higher indices). Defaults to True, which is bottom-up search.\n\n    See Also\n    --------\n    potential_temperature, isentropic_interpolation_as_dataset\n\n    Notes\n    -----\n    Input variable arrays must have the same number of vertical levels as the pressure levels\n    array. Pressure is calculated on isentropic surfaces by assuming that temperature varies\n    linearly with the natural log of pressure. Linear interpolation is then used in the\n    vertical to find the pressure at each isentropic level. Interpolation method from\n    [Ziv1994]_. Any additional arguments are assumed to vary linearly with temperature and will\n    be linearly interpolated to the new isentropic levels.\n\n    Will only return Pint Quantities, even when given xarray DataArray profiles. To\n    obtain a xarray Dataset instead, use `isentropic_interpolation_as_dataset` instead.\n\n    .. versionchanged:: 1.0\n       Renamed ``theta_levels``, ``axis`` parameters to ``levels``, ``vertical_dim``\n\n    \"\"\"\n    # iteration function to be used later\n    # Calculates theta from linearly interpolated temperature and solves for pressure\n    def _isen_iter(iter_log_p, isentlevs_nd, ka, a, b, pok):\n        exner = pok * np.exp(-ka * iter_log_p)\n        t = a * iter_log_p + b\n        # Newton-Raphson iteration\n        f = isentlevs_nd - t * exner\n        fp = exner * (ka * t - a)\n        return iter_log_p - (f / fp)\n\n    # Convert units\n    pressure = pressure.to('hPa')\n    temperature = temperature.to('kelvin')\n\n    slices = [np.newaxis] * temperature.ndim\n    slices[vertical_dim] = slice(None)\n    slices = tuple(slices)\n    pressure = units.Quantity(np.broadcast_to(pressure[slices].magnitude, temperature.shape),\n                              pressure.units)\n\n    # Sort input data\n    sort_pressure = np.argsort(pressure.m, axis=vertical_dim)\n    sort_pressure = np.swapaxes(np.swapaxes(sort_pressure, 0, vertical_dim)[::-1], 0,\n                                vertical_dim)\n    sorter = broadcast_indices(sort_pressure, temperature.shape, vertical_dim)\n    levs = pressure[sorter]\n    tmpk = temperature[sorter]\n\n    levels = np.asarray(levels.m_as('kelvin')).reshape(-1)\n    isentlevels = levels[np.argsort(levels)]\n\n    # Make the desired isentropic levels the same shape as temperature\n    shape = list(temperature.shape)\n    shape[vertical_dim] = isentlevels.size\n    isentlevs_nd = np.broadcast_to(isentlevels[slices], shape)\n\n    # exponent to Poisson's Equation, which is imported above\n    ka = mpconsts.kappa.m_as('dimensionless')\n\n    # calculate theta for each point\n    pres_theta = potential_temperature(levs, tmpk)\n\n    # Raise error if input theta level is larger than pres_theta max\n    if np.max(pres_theta.m) < np.max(levels):\n        raise ValueError('Input theta level out of data bounds')\n\n    # Find log of pressure to implement assumption of linear temperature dependence on\n    # ln(p)\n    log_p = np.log(levs.m)\n\n    # Calculations for interpolation routine\n    pok = mpconsts.P0 ** ka\n\n    # index values for each point for the pressure level nearest to the desired theta level\n    above, below, good = find_bounding_indices(pres_theta.m, levels, vertical_dim,\n                                               from_below=bottom_up_search)\n\n    # calculate constants for the interpolation\n    a = (tmpk.m[above] - tmpk.m[below]) / (log_p[above] - log_p[below])\n    b = tmpk.m[above] - a * log_p[above]\n\n    # calculate first guess for interpolation\n    isentprs = 0.5 * (log_p[above] + log_p[below])\n\n    # Make sure we ignore any nans in the data for solving; checking a is enough since it\n    # combines log_p and tmpk.\n    good &= ~np.isnan(a)\n\n    # iterative interpolation using scipy.optimize.fixed_point and _isen_iter defined above\n    log_p_solved = so.fixed_point(_isen_iter, isentprs[good],\n                                  args=(isentlevs_nd[good], ka, a[good], b[good], pok.m),\n                                  xtol=eps, maxiter=max_iters)\n\n    # get back pressure from log p\n    isentprs[good] = np.exp(log_p_solved)\n\n    # Mask out points we know are bad as well as points that are beyond the max pressure\n    isentprs[~(good & _less_or_close(isentprs, np.max(pressure.m)))] = np.nan\n\n    # create list for storing output data\n    ret = [units.Quantity(isentprs, 'hPa')]\n\n    # if temperature_out = true, calculate temperature and output as last item in list\n    if temperature_out:\n        ret.append(units.Quantity((isentlevs_nd / ((mpconsts.P0.m / isentprs) ** ka)), 'K'))\n\n    # do an interpolation for each additional argument\n    if args:\n        others = interpolate_1d(isentlevels, pres_theta.m, *(arr[sorter] for arr in args),\n                                axis=vertical_dim, return_list_always=True)\n        ret.extend(others)\n\n    return ret",
  "def isentropic_interpolation_as_dataset(\n    levels,\n    temperature,\n    *args,\n    max_iters=50,\n    eps=1e-6,\n    bottom_up_search=True\n):\n    r\"\"\"Interpolate xarray data in isobaric coords to isentropic coords, returning a Dataset.\n\n    Parameters\n    ----------\n    levels : `pint.Quantity`\n        One-dimensional array of desired potential temperature surfaces\n    temperature : `xarray.DataArray`\n        Array of temperature\n    args : `xarray.DataArray`, optional\n        Any other given variables will be interpolated to each isentropic level. Must have\n        names in order to have a well-formed output Dataset.\n    max_iters : int, optional\n        The maximum number of iterations to use in calculation, defaults to 50.\n    eps : float, optional\n        The desired absolute error in the calculated value, defaults to 1e-6.\n    bottom_up_search : bool, optional\n        Controls whether to search for levels bottom-up (starting at lower indices),\n        or top-down (starting at higher indices). Defaults to True, which is bottom-up search.\n\n    Returns\n    -------\n    xarray.Dataset\n        Dataset with pressure, temperature, and each additional argument, all on the specified\n        isentropic coordinates.\n\n    See Also\n    --------\n    potential_temperature, isentropic_interpolation\n\n    Notes\n    -----\n    Input variable arrays must have the same number of vertical levels as the pressure levels\n    array. Pressure is calculated on isentropic surfaces by assuming that temperature varies\n    linearly with the natural log of pressure. Linear interpolation is then used in the\n    vertical to find the pressure at each isentropic level. Interpolation method from\n    [Ziv1994]_. Any additional arguments are assumed to vary linearly with temperature and will\n    be linearly interpolated to the new isentropic levels.\n\n    This formulation relies upon xarray functionality. If using Pint Quantities, use\n    `isentropic_interpolation` instead.\n\n    \"\"\"\n    # Ensure matching coordinates by broadcasting\n    all_args = xr.broadcast(temperature, *args)\n\n    # Obtain result as list of Quantities\n    ret = isentropic_interpolation(\n        levels,\n        all_args[0].metpy.vertical,\n        all_args[0].metpy.unit_array,\n        *(arg.metpy.unit_array for arg in all_args[1:]),\n        vertical_dim=all_args[0].metpy.find_axis_number('vertical'),\n        temperature_out=True,\n        max_iters=max_iters,\n        eps=eps,\n        bottom_up_search=bottom_up_search\n    )\n\n    # Reconstruct coordinates and dims (add isentropic levels, remove isobaric levels)\n    vertical_dim = all_args[0].metpy.find_axis_name('vertical')\n    new_coords = {\n        'isentropic_level': xr.DataArray(\n            levels.m,\n            dims=('isentropic_level',),\n            coords={'isentropic_level': levels.m},\n            name='isentropic_level',\n            attrs={\n                'units': str(levels.units),\n                'positive': 'up'\n            }\n        ),\n        **{\n            key: value\n            for key, value in all_args[0].coords.items()\n            if key != vertical_dim\n        }\n    }\n    new_dims = [\n        dim if dim != vertical_dim else 'isentropic_level' for dim in all_args[0].dims\n    ]\n\n    # Build final dataset from interpolated Quantities and original DataArrays\n    return xr.Dataset(\n        {\n            'pressure': (\n                new_dims,\n                ret[0],\n                {'standard_name': 'air_pressure'}\n            ),\n            'temperature': (\n                new_dims,\n                ret[1],\n                {'standard_name': 'air_temperature'}\n            ),\n            **{\n                all_args[i].name: (new_dims, ret[i + 1], all_args[i].attrs)\n                for i in range(1, len(all_args))\n            }\n        },\n        coords=new_coords\n    )",
  "def surface_based_cape_cin(pressure, temperature, dewpoint):\n    r\"\"\"Calculate surface-based CAPE and CIN.\n\n    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)\n    of a given upper air profile for a surface-based parcel. CIN is integrated\n    between the surface and LFC, CAPE is integrated between the LFC and EL (or top of\n    sounding). Intersection points of the measured temperature profile and parcel profile are\n    logarithmically interpolated.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile. The first entry should be the starting\n        (surface) observation, with the array going from high to low pressure.\n\n    temperature : `pint.Quantity`\n        Temperature profile corresponding to the `pressure` profile\n\n    dewpoint : `pint.Quantity`\n        Dewpoint profile corresponding to the `pressure` profile\n\n    Returns\n    -------\n    `pint.Quantity`\n        Surface based Convective Available Potential Energy (CAPE)\n\n    `pint.Quantity`\n        Surface based Convective Inhibition (CIN)\n\n    See Also\n    --------\n    cape_cin, parcel_profile\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    \"\"\"\n    pressure, temperature, dewpoint = _remove_nans(pressure, temperature, dewpoint)\n    p, t, td, profile = parcel_profile_with_lcl(pressure, temperature, dewpoint)\n    return cape_cin(p, t, td, profile)",
  "def most_unstable_cape_cin(pressure, temperature, dewpoint, **kwargs):\n    r\"\"\"Calculate most unstable CAPE/CIN.\n\n    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)\n    of a given upper air profile and most unstable parcel path. CIN is integrated between the\n    surface and LFC, CAPE is integrated between the LFC and EL (or top of sounding).\n    Intersection points of the measured temperature profile and parcel profile are\n    logarithmically interpolated.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressure profile\n\n    temperature : `pint.Quantity`\n        Temperature profile\n\n    dewpoint : `pint.Quantity`\n        Dew point profile\n\n    kwargs\n        Additional keyword arguments to pass to `most_unstable_parcel`\n\n    Returns\n    -------\n    `pint.Quantity`\n        Most unstable Convective Available Potential Energy (CAPE)\n\n    `pint.Quantity`\n        Most unstable Convective Inhibition (CIN)\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, most_unstable_cape_cin\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # calculate most unstbale CAPE/CIN\n    >>> most_unstable_cape_cin(p, T, Td)\n    (<Quantity(4703.77306, 'joule / kilogram')>, <Quantity(0, 'joule / kilogram')>)\n\n    See Also\n    --------\n    cape_cin, most_unstable_parcel, parcel_profile\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    \"\"\"\n    pressure, temperature, dewpoint = _remove_nans(pressure, temperature, dewpoint)\n    _, _, _, parcel_idx = most_unstable_parcel(pressure, temperature, dewpoint, **kwargs)\n    p, t, td, mu_profile = parcel_profile_with_lcl(pressure[parcel_idx:],\n                                                   temperature[parcel_idx:],\n                                                   dewpoint[parcel_idx:])\n    return cape_cin(p, t, td, mu_profile)",
  "def mixed_layer_cape_cin(pressure, temperature, dewpoint, **kwargs):\n    r\"\"\"Calculate mixed-layer CAPE and CIN.\n\n    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)\n    of a given upper air profile and mixed-layer parcel path. CIN is integrated between the\n    surface and LFC, CAPE is integrated between the LFC and EL (or top of sounding).\n    Intersection points of the measured temperature profile and parcel profile are\n    logarithmically interpolated. Kwargs for `mixed_parcel` can be provided, such as `depth`.\n    Default mixed-layer depth is 100 hPa.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressure profile\n\n    temperature : `pint.Quantity`\n        Temperature profile\n\n    dewpoint : `pint.Quantity`\n        Dewpoint profile\n\n    kwargs\n        Additional keyword arguments to pass to `mixed_parcel`\n\n    Returns\n    -------\n    `pint.Quantity`\n        Mixed-layer Convective Available Potential Energy (CAPE)\n    `pint.Quantity`\n        Mixed-layer Convective INhibition (CIN)\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, mixed_layer_cape_cin\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 25.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .75, .56, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> mixed_layer_cape_cin(p, T, Td, depth=50 * units.hPa)\n    (<Quantity(711.239032, 'joule / kilogram')>, <Quantity(-5.48053989, 'joule / kilogram')>)\n\n    See Also\n    --------\n    cape_cin, mixed_parcel, parcel_profile\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    \"\"\"\n    depth = kwargs.get('depth', units.Quantity(100, 'hPa'))\n    parcel_pressure, parcel_temp, parcel_dewpoint = mixed_parcel(pressure, temperature,\n                                                                 dewpoint, **kwargs)\n\n    # Remove values below top of mixed layer and add in the mixed layer values\n    pressure_prof = pressure[pressure < (pressure[0] - depth)]\n    temp_prof = temperature[pressure < (pressure[0] - depth)]\n    dew_prof = dewpoint[pressure < (pressure[0] - depth)]\n    pressure_prof = concatenate([parcel_pressure, pressure_prof])\n    temp_prof = concatenate([parcel_temp, temp_prof])\n    dew_prof = concatenate([parcel_dewpoint, dew_prof])\n\n    p, t, td, ml_profile = parcel_profile_with_lcl(pressure_prof, temp_prof, dew_prof)\n    return cape_cin(p, t, td, ml_profile)",
  "def mixed_parcel(pressure, temperature, dewpoint, parcel_start_pressure=None,\n                 height=None, bottom=None, depth=None, interpolate=True):\n    r\"\"\"Calculate the properties of a parcel mixed from a layer.\n\n    Determines the properties of an air parcel that is the result of complete mixing of a\n    given atmospheric layer.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile\n\n    temperature : `pint.Quantity`\n        Atmospheric temperature profile\n\n    dewpoint : `pint.Quantity`\n        Atmospheric dewpoint profile\n\n    parcel_start_pressure : `pint.Quantity`, optional\n        Pressure at which the mixed parcel should begin (default None)\n\n    height: `pint.Quantity`, optional\n        Atmospheric heights corresponding to the given pressures (default None)\n\n    bottom : `pint.Quantity`, optional\n        The bottom of the layer as a pressure or height above the surface pressure\n        (default None)\n\n    depth : `pint.Quantity`, optional\n        The thickness of the layer as a pressure or height above the bottom of the layer\n        (default 100 hPa)\n\n    interpolate : bool, optional\n        Interpolate the top and bottom points if they are not in the given data\n\n    Returns\n    -------\n    `pint.Quantity`\n        Pressure of the mixed parcel\n    `pint.Quantity`\n        Temperature of the mixed parcel\n\n    `pint.Quantity`\n        Dewpoint of the mixed parcel\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, mixed_parcel\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # find the mixed parcel of depth 50 hPa\n    >>> mixed_parcel(p, T, Td, depth=50 * units.hPa)\n    (<Quantity(1008.0, 'hectopascal')>, <Quantity(28.750033, 'degree_Celsius')>,\n    <Quantity(18.1998736, 'degree_Celsius')>)\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``p``, ``dewpt``, ``heights`` parameters to\n       ``pressure``, ``dewpoint``, ``height``\n\n    \"\"\"\n    # If a parcel starting pressure is not provided, use the surface\n    if not parcel_start_pressure:\n        parcel_start_pressure = pressure[0]\n\n    if depth is None:\n        depth = units.Quantity(100, 'hPa')\n\n    # Calculate the potential temperature and mixing ratio over the layer\n    theta = potential_temperature(pressure, temperature)\n    mixing_ratio = saturation_mixing_ratio(pressure, dewpoint)\n\n    # Mix the variables over the layer\n    mean_theta, mean_mixing_ratio = mixed_layer(pressure, theta, mixing_ratio, bottom=bottom,\n                                                height=height, depth=depth,\n                                                interpolate=interpolate)\n\n    # Convert back to temperature\n    mean_temperature = mean_theta * exner_function(parcel_start_pressure)\n\n    # Convert back to dewpoint\n    mean_vapor_pressure = vapor_pressure(parcel_start_pressure, mean_mixing_ratio)\n\n    # Using globals() here allows us to keep the dewpoint parameter but still call the\n    # function of the same name.\n    mean_dewpoint = globals()['dewpoint'](mean_vapor_pressure)\n\n    return (parcel_start_pressure, mean_temperature.to(temperature.units),\n            mean_dewpoint.to(dewpoint.units))",
  "def mixed_layer(pressure, *args, height=None, bottom=None, depth=None, interpolate=True):\n    r\"\"\"Mix variable(s) over a layer, yielding a mass-weighted average.\n\n    This function will integrate a data variable with respect to pressure and determine the\n    average value using the mean value theorem.\n\n    Parameters\n    ----------\n    pressure : array-like\n        Atmospheric pressure profile\n\n    datavar : array-like\n        Atmospheric variable measured at the given pressures\n\n    height: array-like, optional\n        Atmospheric heights corresponding to the given pressures (default None)\n\n    bottom : `pint.Quantity`, optional\n        The bottom of the layer as a pressure or height above the surface pressure\n        (default None)\n\n    depth : `pint.Quantity`, optional\n        The thickness of the layer as a pressure or height above the bottom of the layer\n        (default 100 hPa)\n\n    interpolate : bool, optional\n        Interpolate the top and bottom points if they are not in the given data (default True)\n\n    Returns\n    -------\n    `pint.Quantity`\n        The mixed value of the data variable\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, mixed_layer\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # find mixed layer T and Td of depth 50 hPa\n    >>> mixed_layer(p, T, Td, depth=50 * units.hPa)\n    [<Quantity(26.5798571, 'degree_Celsius')>, <Quantity(16.675935, 'degree_Celsius')>]\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``p``, ``heights`` parameters to ``pressure``, ``height``\n\n    \"\"\"\n    if depth is None:\n        depth = units.Quantity(100, 'hPa')\n    layer = get_layer(pressure, *args, height=height, bottom=bottom,\n                      depth=depth, interpolate=interpolate)\n    p_layer = layer[0]\n    datavars_layer = layer[1:]\n\n    ret = []\n    for datavar_layer in datavars_layer:\n        actual_depth = abs(p_layer[0] - p_layer[-1])\n        ret.append(units.Quantity(np.trapz(datavar_layer.m, p_layer.m) / -actual_depth.m,\n                   datavar_layer.units))\n    return ret",
  "def dry_static_energy(height, temperature):\n    r\"\"\"Calculate the dry static energy of parcels.\n\n    This function will calculate the dry static energy following the first two terms of\n    equation 3.72 in [Hobbs2006]_.\n\n    Parameters\n    ----------\n    height : `pint.Quantity`\n        Atmospheric height\n\n    temperature : `pint.Quantity`\n        Air temperature\n\n    Returns\n    -------\n    `pint.Quantity`\n        Dry static energy\n\n    Examples\n    --------\n    >>> from metpy.calc import dry_static_energy\n    >>> from metpy.units import units\n    >>> dry_static_energy(1000 * units.meters, 8 * units.degC)\n    <Quantity(292.268557, 'kilojoule / kilogram')>\n\n    See Also\n    --------\n    montgomery_streamfunction\n\n    Notes\n    -----\n    .. math:: \\text{dry static energy} = c_{pd} T + gz\n\n    * :math:`T` is temperature\n    * :math:`z` is height\n\n    .. versionchanged:: 1.0\n       Renamed ``heights`` parameter to ``height``\n\n    \"\"\"\n    return (mpconsts.g * height + mpconsts.Cp_d * temperature).to('kJ/kg')",
  "def moist_static_energy(height, temperature, specific_humidity):\n    r\"\"\"Calculate the moist static energy of parcels.\n\n    This function will calculate the moist static energy following\n    equation 3.72 in [Hobbs2006]_.\n\n    Parameters\n    ----------\n    height : `pint.Quantity`\n        Atmospheric height\n\n    temperature : `pint.Quantity`\n        Air temperature\n\n    specific_humidity : `pint.Quantity`\n        Atmospheric specific humidity\n\n    Returns\n    -------\n    `pint.Quantity`\n        Moist static energy\n\n    Examples\n    --------\n    >>> from metpy.calc import moist_static_energy\n    >>> from metpy.units import units\n    >>> moist_static_energy(1000 * units.meters, 8 * units.degC, 8 * units('g/kg'))\n    <Quantity(312.275277, 'kilojoule / kilogram')>\n\n    Notes\n    -----\n    .. math:: \\text{moist static energy} = c_{pd} T + gz + L_v q\n\n    * :math:`T` is temperature\n    * :math:`z` is height\n    * :math:`q` is specific humidity\n\n    .. versionchanged:: 1.0\n       Renamed ``heights`` parameter to ``height``\n\n    \"\"\"\n    return (dry_static_energy(height, temperature)\n            + mpconsts.Lv * specific_humidity.to('dimensionless')).to('kJ/kg')",
  "def thickness_hydrostatic(pressure, temperature, mixing_ratio=None,\n                          molecular_weight_ratio=mpconsts.nounit.epsilon, bottom=None,\n                          depth=None):\n    r\"\"\"Calculate the thickness of a layer via the hypsometric equation.\n\n    This thickness calculation uses the pressure and temperature profiles (and optionally\n    mixing ratio) via the hypsometric equation with virtual temperature adjustment.\n\n    .. math:: Z_2 - Z_1 = -\\frac{R_d}{g} \\int_{p_1}^{p_2} T_v d\\ln p,\n\n    Which is based off of Equation 3.24 in [Hobbs2006]_.\n\n    This assumes a hydrostatic atmosphere. Layer bottom and depth specified in pressure.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile\n\n    temperature : `pint.Quantity`\n        Atmospheric temperature profile\n\n    mixing_ratio : `pint.Quantity`, optional\n        Profile of dimensionless mass mixing ratio. If none is given, virtual temperature\n        is simply set to be the given temperature.\n\n    molecular_weight_ratio : `pint.Quantity` or float, optional\n        The ratio of the molecular weight of the constituent gas to that assumed\n        for air. Defaults to the ratio for water vapor to dry air.\n        (:math:`\\epsilon\\approx0.622`)\n\n    bottom : `pint.Quantity`, optional\n        The bottom of the layer in pressure. Defaults to the first observation.\n\n    depth : `pint.Quantity`, optional\n        The depth of the layer in hPa. Defaults to the full profile if bottom is not given,\n        and 100 hPa if bottom is given.\n\n    Returns\n    -------\n    `pint.Quantity`\n        The thickness of the layer in meters\n\n    Examples\n    --------\n    >>> import metpy.calc as mpcalc\n    >>> from metpy.units import units\n    >>> temperature = [278, 275, 270] * units.kelvin\n    >>> pressure = [950, 925, 900] * units.millibar\n    >>> mpcalc.thickness_hydrostatic(pressure, temperature)\n    <Quantity(434.376889, 'meter')>\n\n    >>> bottom, depth = 950 * units.millibar, 25 * units.millibar\n    >>> mpcalc.thickness_hydrostatic(pressure, temperature, bottom=bottom, depth=depth)\n    <Quantity(215.835404, 'meter')>\n\n    To include the mixing ratio in the calculation:\n\n    >>> r = [0.005, 0.006, 0.002] * units.dimensionless\n    >>> mpcalc.thickness_hydrostatic(pressure, temperature, mixing_ratio=r,\n    ...                              bottom=bottom, depth=depth)\n    <Quantity(216.552623, 'meter')>\n\n    Compute the 1000-500 hPa Thickness\n\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # specify a layer\n    >>> layer = (p <= 1000 * units.hPa) & (p >= 500 * units.hPa)\n    >>> # compute the hydrostatic thickness\n    >>> mpcalc.thickness_hydrostatic(p[layer], T[layer])\n    <Quantity(5755.94719, 'meter')>\n\n    See Also\n    --------\n    thickness_hydrostatic_from_relative_humidity, pressure_to_height_std, virtual_temperature\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``mixing`` parameter to ``mixing_ratio``\n\n    \"\"\"\n    # Get the data for the layer, conditional upon bottom/depth being specified and mixing\n    # ratio being given\n    if bottom is None and depth is None:\n        if mixing_ratio is None:\n            layer_p, layer_virttemp = pressure, temperature\n        else:\n            layer_p = pressure\n            layer_virttemp = virtual_temperature._nounit(\n                temperature, mixing_ratio, molecular_weight_ratio\n            )\n    else:\n        if mixing_ratio is None:\n            # Note: get_layer works on *args and has arguments that make the function behave\n            # differently depending on units, making a unit-free version nontrivial. For now,\n            # since optimized path doesn't use this conditional branch at all, we can safely\n            # sacrifice performance by reattaching and restripping units to use unit-aware\n            # get_layer\n            layer_p, layer_virttemp = get_layer(\n                units.Quantity(pressure, 'Pa'),\n                units.Quantity(temperature, 'K'),\n                bottom=units.Quantity(bottom, 'Pa') if bottom is not None else None,\n                depth=units.Quantity(depth, 'Pa') if depth is not None else None\n            )\n            layer_p = layer_p.m_as('Pa')\n            layer_virttemp = layer_virttemp.m_as('K')\n        else:\n            layer_p, layer_temp, layer_w = get_layer(\n                units.Quantity(pressure, 'Pa'),\n                units.Quantity(temperature, 'K'),\n                units.Quantity(mixing_ratio, ''),\n                bottom=units.Quantity(bottom, 'Pa') if bottom is not None else None,\n                depth=units.Quantity(depth, 'Pa') if depth is not None else None\n            )\n            layer_p = layer_p.m_as('Pa')\n            layer_virttemp = virtual_temperature._nounit(\n                layer_temp.m_as('K'), layer_w.m_as(''), molecular_weight_ratio\n            )\n\n    # Take the integral\n    return (\n        -mpconsts.nounit.Rd / mpconsts.nounit.g\n        * np.trapz(layer_virttemp, np.log(layer_p))\n    )",
  "def thickness_hydrostatic_from_relative_humidity(pressure, temperature, relative_humidity,\n                                                 bottom=None, depth=None):\n    r\"\"\"Calculate the thickness of a layer given pressure, temperature and relative humidity.\n\n    Similar to ``thickness_hydrostatic``, this thickness calculation uses the pressure,\n    temperature, and relative humidity profiles via the hypsometric equation with virtual\n    temperature adjustment\n\n    .. math:: Z_2 - Z_1 = -\\frac{R_d}{g} \\int_{p_1}^{p_2} T_v d\\ln p,\n\n    which is based off of Equation 3.24 in [Hobbs2006]_. Virtual temperature is calculated\n    from the profiles of temperature and relative humidity.\n\n    This assumes a hydrostatic atmosphere.\n\n    Layer bottom and depth specified in pressure.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile\n\n    temperature : `pint.Quantity`\n        Atmospheric temperature profile\n\n    relative_humidity : `pint.Quantity`\n        Atmospheric relative humidity profile. The relative humidity is expressed as a\n        unitless ratio in the range [0, 1]. Can also pass a percentage if proper units are\n        attached.\n\n    bottom : `pint.Quantity`, optional\n        The bottom of the layer in pressure. Defaults to the first observation.\n\n    depth : `pint.Quantity`, optional\n        The depth of the layer in hPa. Defaults to the full profile if bottom is not given,\n        and 100 hPa if bottom is given.\n\n    Returns\n    -------\n    `pint.Quantity`\n        The thickness of the layer in meters\n\n    Examples\n    --------\n    >>> from metpy.calc import thickness_hydrostatic_from_relative_humidity\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> ip1000_500 = (p <= 1000 * units.hPa) & (p >= 500 * units.hPa)\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # compute hydrostatic thickness from RH\n    >>> thickness_hydrostatic_from_relative_humidity(p[ip1000_500],\n    ...                                              T[ip1000_500],\n    ...                                              rh[ip1000_500])\n    <Quantity(5781.35394, 'meter')>\n\n    See Also\n    --------\n    thickness_hydrostatic, pressure_to_height_std, virtual_temperature,\n    mixing_ratio_from_relative_humidity\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    \"\"\"\n    mixing = mixing_ratio_from_relative_humidity(pressure, temperature, relative_humidity)\n\n    return thickness_hydrostatic(pressure, temperature, mixing_ratio=mixing, bottom=bottom,\n                                 depth=depth)",
  "def brunt_vaisala_frequency_squared(height, potential_temperature, vertical_dim=0):\n    r\"\"\"Calculate the square of the Brunt-Vaisala frequency.\n\n    Brunt-Vaisala frequency squared (a measure of atmospheric stability) is given by the\n    formula:\n\n    .. math:: N^2 = \\frac{g}{\\theta} \\frac{d\\theta}{dz}\n\n    This formula is based off of Equations 3.75 and 3.77 in [Hobbs2006]_.\n\n    Parameters\n    ----------\n    height : `xarray.DataArray` or `pint.Quantity`\n        Atmospheric (geopotential) height\n\n    potential_temperature : `xarray.DataArray` or `pint.Quantity`\n        Atmospheric potential temperature\n\n    vertical_dim : int, optional\n        The axis corresponding to vertical in the potential temperature array, defaults to 0,\n        unless `height` and `potential_temperature` given as `xarray.DataArray`, in which case\n        it is automatically determined from the coordinate metadata.\n\n    Returns\n    -------\n    `pint.Quantity` or `xarray.DataArray`\n        The square of the Brunt-Vaisala frequency. Given as `pint.Quantity`, unless both\n        `height` and `potential_temperature` arguments are given as `xarray.DataArray`, in\n        which case will be `xarray.DataArray`.\n\n\n    .. versionchanged:: 1.0\n       Renamed ``heights``, ``axis`` parameters to ``height``, ``vertical_dim``\n\n    See Also\n    --------\n    brunt_vaisala_frequency, brunt_vaisala_period, potential_temperature\n\n    \"\"\"\n    # Ensure validity of temperature units\n    potential_temperature = potential_temperature.to('K')\n\n    # Calculate and return the square of Brunt-Vaisala frequency\n    return mpconsts.g / potential_temperature * first_derivative(\n        potential_temperature,\n        x=height,\n        axis=vertical_dim\n    )",
  "def brunt_vaisala_frequency(height, potential_temperature, vertical_dim=0):\n    r\"\"\"Calculate the Brunt-Vaisala frequency.\n\n    This function will calculate the Brunt-Vaisala frequency as follows:\n\n    .. math:: N = \\left( \\frac{g}{\\theta} \\frac{d\\theta}{dz} \\right)^\\frac{1}{2}\n\n    This formula based off of Equations 3.75 and 3.77 in [Hobbs2006]_.\n\n    This function is a wrapper for `brunt_vaisala_frequency_squared` that filters out negative\n    (unstable) quantities and takes the square root.\n\n    Parameters\n    ----------\n    height : `xarray.DataArray` or `pint.Quantity`\n        Atmospheric (geopotential) height\n\n    potential_temperature : `xarray.DataArray` or `pint.Quantity`\n        Atmospheric potential temperature\n\n    vertical_dim : int, optional\n        The axis corresponding to vertical in the potential temperature array, defaults to 0,\n        unless `height` and `potential_temperature` given as `xarray.DataArray`, in which case\n        it is automatically determined from the coordinate metadata.\n\n    Returns\n    -------\n    `pint.Quantity` or `xarray.DataArray`\n        Brunt-Vaisala frequency. Given as `pint.Quantity`, unless both\n        `height` and `potential_temperature` arguments are given as `xarray.DataArray`, in\n        which case will be `xarray.DataArray`.\n\n\n    .. versionchanged:: 1.0\n       Renamed ``heights``, ``axis`` parameters to ``height``, ``vertical_dim``\n\n    See Also\n    --------\n    brunt_vaisala_frequency_squared, brunt_vaisala_period, potential_temperature\n\n    \"\"\"\n    bv_freq_squared = brunt_vaisala_frequency_squared(height, potential_temperature,\n                                                      vertical_dim=vertical_dim)\n    bv_freq_squared[bv_freq_squared.magnitude < 0] = np.nan\n\n    return np.sqrt(bv_freq_squared)",
  "def brunt_vaisala_period(height, potential_temperature, vertical_dim=0):\n    r\"\"\"Calculate the Brunt-Vaisala period.\n\n    This function is a helper function for `brunt_vaisala_frequency` that calculates the\n    period of oscillation as in Exercise 3.13 of [Hobbs2006]_:\n\n    .. math:: \\tau = \\frac{2\\pi}{N}\n\n    Returns `NaN` when :math:`N^2 > 0`.\n\n    Parameters\n    ----------\n    height : `xarray.DataArray` or `pint.Quantity`\n        Atmospheric (geopotential) height\n\n    potential_temperature : `xarray.DataArray` or `pint.Quantity`\n        Atmospheric potential temperature\n\n    vertical_dim : int, optional\n        The axis corresponding to vertical in the potential temperature array, defaults to 0,\n        unless `height` and `potential_temperature` given as `xarray.DataArray`, in which case\n        it is automatically determined from the coordinate metadata.\n\n    Returns\n    -------\n    `pint.Quantity` or `xarray.DataArray`\n        Brunt-Vaisala period. Given as `pint.Quantity`, unless both\n        `height` and `potential_temperature` arguments are given as `xarray.DataArray`, in\n        which case will be `xarray.DataArray`.\n\n\n    .. versionchanged:: 1.0\n       Renamed ``heights``, ``axis`` parameters to ``height``, ``vertical_dim``\n\n    See Also\n    --------\n    brunt_vaisala_frequency, brunt_vaisala_frequency_squared, potential_temperature\n\n    \"\"\"\n    bv_freq_squared = brunt_vaisala_frequency_squared(height, potential_temperature,\n                                                      vertical_dim=vertical_dim)\n    bv_freq_squared[bv_freq_squared.magnitude <= 0] = np.nan\n\n    return 2 * np.pi / np.sqrt(bv_freq_squared)",
  "def wet_bulb_temperature(pressure, temperature, dewpoint):\n    \"\"\"Calculate the wet-bulb temperature using Normand's rule.\n\n    This function calculates the wet-bulb temperature using the Normand method. The LCL is\n    computed, and that parcel brought down to the starting pressure along a moist adiabat.\n    The Normand method (and others) are described and compared by [Knox2017]_.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Initial atmospheric pressure\n\n    temperature : `pint.Quantity`\n        Initial atmospheric temperature\n\n    dewpoint : `pint.Quantity`\n        Initial atmospheric dewpoint\n\n    Returns\n    -------\n    `pint.Quantity`\n        Wet-bulb temperature\n\n    Examples\n    --------\n    >>> from metpy.calc import wet_bulb_temperature\n    >>> from metpy.units import units\n    >>> wet_bulb_temperature(993 * units.hPa, 32 * units.degC, 15 * units.degC)\n    <Quantity(20.3770228, 'degree_Celsius')>\n\n    See Also\n    --------\n    lcl, moist_lapse\n\n    Notes\n    -----\n    Since this function iteratively applies a parcel calculation, it should be used with\n    caution on large arrays.\n\n    \"\"\"\n    if not getattr(pressure, 'shape', False):\n        pressure = np.atleast_1d(pressure)\n        temperature = np.atleast_1d(temperature)\n        dewpoint = np.atleast_1d(dewpoint)\n\n    lcl_press, lcl_temp = lcl(pressure, temperature, dewpoint)\n\n    it = np.nditer([pressure.magnitude, lcl_press.magnitude, lcl_temp.magnitude, None],\n                   op_dtypes=['float', 'float', 'float', 'float'],\n                   flags=['buffered'])\n\n    for press, lpress, ltemp, ret in it:\n        moist_adiabat_temperatures = moist_lapse(units.Quantity(press, pressure.units),\n                                                 units.Quantity(ltemp, lcl_temp.units),\n                                                 units.Quantity(lpress, lcl_press.units))\n        ret[...] = moist_adiabat_temperatures.m_as(temperature.units)\n\n    # If we started with a scalar, return a scalar\n    ret = it.operands[3]\n    if ret.size == 1:\n        ret = ret[0]\n    return units.Quantity(ret, temperature.units)",
  "def static_stability(pressure, temperature, vertical_dim=0):\n    r\"\"\"Calculate the static stability within a vertical profile.\n\n    .. math:: \\sigma = -\\frac{RT}{p} \\frac{\\partial \\ln \\theta}{\\partial p}\n\n    This formula is based on equation 4.3.6 in [Bluestein1992]_.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Profile of atmospheric pressure\n\n    temperature : `pint.Quantity`\n        Profile of temperature\n\n    vertical_dim : int, optional\n        The axis corresponding to vertical in the pressure and temperature arrays, defaults\n        to 0.\n\n    Returns\n    -------\n    `pint.Quantity`\n        The profile of static stability\n\n    Examples\n    --------\n    >>> from metpy.calc import static_stability\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # Static Stability Parameter\n    >>> static_stability(p, T).to('m^2 s^-2 Pa^-2')\n    <Quantity([-2.06389302e-06 -1.60051176e-06  5.29948840e-07  1.35399713e-06\n    1.62475780e-06  1.80616992e-06  1.95909329e-06  2.12257341e-06\n    2.35051280e-06  2.86326649e-06  3.44288781e-06  3.95797199e-06\n    4.15532473e-06  4.32460872e-06  4.70381191e-06  4.60700187e-06\n    4.80962228e-06  7.72162917e-06  1.13637163e-05  1.89412484e-05\n    5.12162481e-05  1.59883754e-04  3.74228296e-04  5.30145977e-04\n    7.20889325e-04  1.00335001e-03  1.48043778e-03  2.32777913e-03\n    3.43878993e-03  5.74908298e-03], 'meter ** 2 / pascal ** 2 / second ** 2')>\n\n    .. versionchanged:: 1.0\n       Renamed ``axis`` parameter ``vertical_dim``\n\n    \"\"\"\n    theta = potential_temperature(pressure, temperature)\n\n    return - mpconsts.Rd * temperature / pressure * first_derivative(\n        np.log(theta.m_as('K')),\n        x=pressure,\n        axis=vertical_dim\n    )",
  "def dewpoint_from_specific_humidity(pressure, temperature, specific_humidity):\n    r\"\"\"Calculate the dewpoint from specific humidity, temperature, and pressure.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Air temperature\n\n    specific_humidity: `pint.Quantity`\n        Specific humidity of air\n\n    Returns\n    -------\n    `pint.Quantity`\n        Dew point temperature\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_specific_humidity\n    >>> from metpy.units import units\n    >>> dewpoint_from_specific_humidity(1000 * units.hPa, 10 * units.degC, 5 * units('g/kg'))\n    <Quantity(3.73203192, 'degree_Celsius')>\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(specific_humidity, temperature, pressure)``\n\n    See Also\n    --------\n    relative_humidity_from_mixing_ratio, dewpoint_from_relative_humidity\n\n    \"\"\"\n    return dewpoint_from_relative_humidity(temperature,\n                                           relative_humidity_from_specific_humidity(\n                                               pressure, temperature, specific_humidity))",
  "def vertical_velocity_pressure(w, pressure, temperature, mixing_ratio=0):\n    r\"\"\"Calculate omega from w assuming hydrostatic conditions.\n\n    This function converts vertical velocity with respect to height\n    :math:`\\left(w = \\frac{Dz}{Dt}\\right)` to that\n    with respect to pressure :math:`\\left(\\omega = \\frac{Dp}{Dt}\\right)`\n    assuming hydrostatic conditions on the synoptic scale.\n    By Equation 7.33 in [Hobbs2006]_,\n\n    .. math:: \\omega \\simeq -\\rho g w\n\n    Density (:math:`\\rho`) is calculated using the :func:`density` function,\n    from the given pressure and temperature. If `mixing_ratio` is given, the virtual\n    temperature correction is used, otherwise, dry air is assumed.\n\n    Parameters\n    ----------\n    w: `pint.Quantity`\n        Vertical velocity in terms of height\n\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Air temperature\n\n    mixing_ratio: `pint.Quantity`, optional\n        Mixing ratio of air\n\n    Returns\n    -------\n    `pint.Quantity`\n        Vertical velocity in terms of pressure (in Pascals / second)\n\n    Examples\n    --------\n    >>> from metpy.calc import vertical_velocity_pressure\n    >>> from metpy.units import units\n    >>> vertical_velocity_pressure(0.5 * units('cm/s'), 700 * units.hPa, 5 * units.degC)\n    <Quantity(-0.0429888572, 'pascal / second')>\n\n    See Also\n    --------\n    density, vertical_velocity\n\n    \"\"\"\n    rho = density(pressure, temperature, mixing_ratio)\n    return (-mpconsts.g * rho * w).to('Pa/s')",
  "def vertical_velocity(omega, pressure, temperature, mixing_ratio=0):\n    r\"\"\"Calculate w from omega assuming hydrostatic conditions.\n\n    This function converts vertical velocity with respect to pressure\n    :math:`\\left(\\omega = \\frac{Dp}{Dt}\\right)` to that with respect to height\n    :math:`\\left(w = \\frac{Dz}{Dt}\\right)` assuming hydrostatic conditions on\n    the synoptic scale. By Equation 7.33 in [Hobbs2006]_,\n\n    .. math:: \\omega \\simeq -\\rho g w\n\n    so that\n\n    .. math:: w \\simeq \\frac{- \\omega}{\\rho g}\n\n    Density (:math:`\\rho`) is calculated using the :func:`density` function,\n    from the given pressure and temperature. If `mixing_ratio` is given, the virtual\n    temperature correction is used, otherwise, dry air is assumed.\n\n    Parameters\n    ----------\n    omega: `pint.Quantity`\n        Vertical velocity in terms of pressure\n\n    pressure: `pint.Quantity`\n        Total atmospheric pressure\n\n    temperature: `pint.Quantity`\n        Air temperature\n\n    mixing_ratio: `pint.Quantity`, optional\n        Mixing ratio of air\n\n    Returns\n    -------\n    `pint.Quantity`\n        Vertical velocity in terms of height (in meters / second)\n\n    Examples\n    --------\n    >>> from metpy.calc import vertical_velocity\n    >>> from metpy.units import units\n    >>> vertical_velocity(-15 * units('Pa/s'), 700 * units.hPa, 5 * units.degC)\n    <Quantity(1.74463814, 'meter / second')>\n\n    See Also\n    --------\n    density, vertical_velocity_pressure\n\n    \"\"\"\n    rho = density(pressure, temperature, mixing_ratio)\n    return (omega / (- mpconsts.g * rho)).to('m/s')",
  "def specific_humidity_from_dewpoint(pressure, dewpoint):\n    r\"\"\"Calculate the specific humidity from the dewpoint temperature and pressure.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Pressure\n\n    dewpoint: `pint.Quantity`\n        Dewpoint temperature\n\n    Returns\n    -------\n    `pint.Quantity`\n        Specific humidity\n\n    Examples\n    --------\n    >>> from metpy.calc import specific_humidity_from_dewpoint\n    >>> from metpy.units import units\n    >>> specific_humidity_from_dewpoint(988 * units.hPa, 15 * units.degC).to('g/kg')\n    <Quantity(10.7975828, 'gram / kilogram')>\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(dewpoint, pressure)``\n\n    See Also\n    --------\n    mixing_ratio, saturation_mixing_ratio\n\n    \"\"\"\n    mixing_ratio = saturation_mixing_ratio._nounit(pressure, dewpoint)\n    return specific_humidity_from_mixing_ratio._nounit(mixing_ratio)",
  "def lifted_index(pressure, temperature, parcel_profile, vertical_dim=0):\n    \"\"\"Calculate Lifted Index from the pressure temperature and parcel profile.\n\n    Lifted index formula derived from [Galway1956]_ and referenced by [DoswellSchultz2006]_:\n\n    .. math:: LI = T500 - Tp500\n\n    where:\n\n    * :math:`T500` is the measured temperature at 500 hPa\n    * :math:`Tp500` is the temperature of the lifted parcel at 500 hPa\n\n    Calculation of the lifted index is defined as the temperature difference between the\n    observed 500 hPa temperature and the temperature of a parcel lifted from the\n    surface to 500 hPa.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure level(s) of interest, in order from highest to\n        lowest pressure\n\n    temperature : `pint.Quantity`\n        Atmospheric temperature corresponding to pressure\n\n    parcel_profile : `pint.Quantity`\n        Temperature profile of the parcel\n\n    vertical_dim : int, optional\n        The axis corresponding to vertical, defaults to 0. Automatically determined from\n        xarray DataArray arguments.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Lifted Index\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, lifted_index, parcel_profile\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # compute the parcel temperatures from surface parcel\n    >>> prof = parcel_profile(p, T[0], Td[0])\n    >>> # calculate the LI\n    >>> lifted_index(p, T, prof)\n    <Quantity([-7.42560365], 'delta_degree_Celsius')>\n\n    \"\"\"\n    # find the measured temperature and parcel profile temperature at 500 hPa.\n    t500, tp500 = interpolate_1d(units.Quantity(500, 'hPa'),\n                                 pressure, temperature, parcel_profile, axis=vertical_dim)\n\n    # calculate the lifted index.\n    return t500 - tp500",
  "def k_index(pressure, temperature, dewpoint, vertical_dim=0):\n    \"\"\"Calculate K Index from the pressure temperature and dewpoint.\n\n    K Index formula derived from [George1960]_:\n\n    .. math:: K = (T850 - T500) + Td850 - (T700 - Td700)\n\n    where:\n\n    * :math:`T850` is the temperature at 850 hPa\n    * :math:`T700` is the temperature at 700 hPa\n    * :math:`T500` is the temperature at 500 hPa\n    * :math:`Td850` is the dewpoint at 850 hPa\n    * :math:`Td700` is the dewpoint at 700 hPa\n\n    Calculation of the K Index is defined as the temperature difference between\n    the static instability between 850 hPa and 500 hPa, add with the moisture\n    at 850hPa, then subtract from the dryness of the airmass at 700 hPa.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressure level(s), in order from highest to lowest pressure\n\n    temperature : `pint.Quantity`\n        Temperature corresponding to pressure\n\n    dewpoint : `pint.Quantity`\n        Dewpoint temperature corresponding to pressure\n\n    vertical_dim : int, optional\n        The axis corresponding to vertical, defaults to 0. Automatically determined from\n        xarray DataArray arguments.\n\n    Returns\n    -------\n    `pint.Quantity`\n        K Index\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, k_index\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> k_index(p, T, Td)\n    <Quantity(35.9395759, 'degree_Celsius')>\n\n    \"\"\"\n    # Find temperature and dewpoint at 850, 700, and 500 hPa\n    (t850, t700, t500), (td850, td700, _) = interpolate_1d(\n        units.Quantity([850, 700, 500], 'hPa'), pressure, temperature, dewpoint,\n        axis=vertical_dim)\n\n    # Calculate k index.\n    return ((t850 - t500) + td850 - (t700 - td700)).to(units.degC)",
  "def gradient_richardson_number(height, potential_temperature, u, v, vertical_dim=0):\n    r\"\"\"Calculate the gradient Richardson number.\n\n    .. math:: Ri = \\frac{g}{\\theta} \\frac{\\left(\\partial \\theta/\\partial z\\right)}\n             {\\left(\\partial u / \\partial z\\right)^2 + \\left(\\partial v / \\partial z\\right)^2}\n\n    See [Holton2004]_ pg. 121-122. As noted by [Holton2004]_, flux Richardson\n    number values below 0.25 indicate turbulence.\n\n    Parameters\n    ----------\n    height : `pint.Quantity`\n        Atmospheric height\n\n    potential_temperature : `pint.Quantity`\n        Atmospheric potential temperature\n\n    u : `pint.Quantity`\n        X component of the wind\n\n    v : `pint.Quantity`\n        y component of the wind\n\n    vertical_dim : int, optional\n        The axis corresponding to vertical, defaults to 0. Automatically determined from\n        xarray DataArray arguments.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Gradient Richardson number\n    \"\"\"\n    dthetadz = first_derivative(potential_temperature, x=height, axis=vertical_dim)\n    dudz = first_derivative(u, x=height, axis=vertical_dim)\n    dvdz = first_derivative(v, x=height, axis=vertical_dim)\n\n    return (mpconsts.g / potential_temperature) * (dthetadz / (dudz ** 2 + dvdz ** 2))",
  "def scale_height(temperature_bottom, temperature_top):\n    r\"\"\"Calculate the scale height of a layer.\n\n    .. math:: H = \\frac{R_d \\overline{T}}{g}\n\n    This function assumes dry air, but can be used with the virtual temperature\n    to account for moisture.\n\n    Parameters\n    ----------\n    temperature_bottom : `pint.Quantity`\n        Temperature at bottom of layer\n\n    temperature_top : `pint.Quantity`\n        Temperature at top of layer\n\n    Returns\n    -------\n    `pint.Quantity`\n        Scale height of layer\n\n    Examples\n    --------\n    >>> from metpy.calc import scale_height\n    >>> from metpy.units import units\n    >>> scale_height(20 * units.degC, -50 * units.degC)\n    <Quantity(7556.2307, 'meter')>\n\n    \"\"\"\n    t_bar = 0.5 * (temperature_bottom + temperature_top)\n    return (mpconsts.nounit.Rd * t_bar) / mpconsts.nounit.g",
  "def showalter_index(pressure, temperature, dewpoint):\n    \"\"\"Calculate Showalter Index.\n\n    Showalter Index derived from [Galway1956]_:\n\n    .. math:: SI = T500 - Tp500\n\n    where:\n\n    * :math:`T500` is the measured temperature at 500 hPa\n    * :math:`Tp500` is the temperature of the parcel at 500 hPa when lifted from 850 hPa\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure, in order from highest to lowest pressure\n\n    temperature : `pint.Quantity`\n        Ambient temperature corresponding to ``pressure``\n\n    dewpoint : `pint.Quantity`\n        Ambient dew point temperatures corresponding to ``pressure``\n\n    Returns\n    -------\n    `pint.Quantity`\n        Showalter index\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, showalter_index\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # compute the showalter index\n    >>> showalter_index(p, T, Td)\n    <Quantity([0.48421285], 'delta_degree_Celsius')>\n\n    \"\"\"\n    # find the measured temperature and dew point temperature at 850 hPa.\n    t850, td850 = interpolate_1d(units.Quantity(850, 'hPa'), pressure, temperature, dewpoint)\n\n    # find the parcel profile temperature at 500 hPa.\n    tp500 = interpolate_1d(units.Quantity(500, 'hPa'), pressure, temperature)\n\n    # Lift parcel from 850 to 500, handling any needed dry vs. saturated adiabatic processes\n    prof = parcel_profile(units.Quantity([850., 500.], 'hPa'), t850, td850)\n\n    # Calculate the Showalter index\n    return tp500 - prof[-1]",
  "def total_totals_index(pressure, temperature, dewpoint, vertical_dim=0):\n    \"\"\"Calculate Total Totals Index from the pressure temperature and dewpoint.\n\n    Total Totals Index formula derived from [Miller1972]_:\n\n    .. math:: TT = (T850 + Td850) - (2 * T500)\n\n    where:\n\n    * :math:`T850` is the temperature at 850 hPa\n    * :math:`T500` is the temperature at 500 hPa\n    * :math:`Td850` is the dewpoint at 850 hPa\n\n    Calculation of the Total Totals Index is defined as the temperature at 850 hPa plus\n    the dewpoint at 850 hPa, minus twice the temperature at 500 hPa. This index consists of\n    two components, the Vertical Totals (VT) and the Cross Totals (CT).\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressure level(s), in order from highest to lowest pressure\n\n    temperature : `pint.Quantity`\n        Temperature corresponding to pressure\n\n    dewpoint : `pint.Quantity`\n        Dewpoint temperature corresponding to pressure\n\n    vertical_dim : int, optional\n        The axis corresponding to vertical, defaults to 0. Automatically determined from\n        xarray DataArray arguments.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Total Totals Index\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, total_totals_index\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # compute the TT index\n    >>> total_totals_index(p, T, Td)\n    <Quantity(42.6741081, 'delta_degree_Celsius')>\n\n    \"\"\"\n    # Find temperature and dewpoint at 850 and 500 hPa.\n    (t850, t500), (td850, _) = interpolate_1d(units.Quantity([850, 500], 'hPa'),\n                                              pressure, temperature, dewpoint,\n                                              axis=vertical_dim)\n\n    # Calculate total totals index.\n    return (t850 - t500) + (td850 - t500)",
  "def vertical_totals(pressure, temperature, vertical_dim=0):\n    \"\"\"Calculate Vertical Totals from the pressure and temperature.\n\n    Vertical Totals formula derived from [Miller1972]_:\n\n    .. math:: VT = T850 - T500\n\n    where:\n\n    * :math:`T850` is the temperature at 850 hPa\n    * :math:`T500` is the temperature at 500 hPa\n\n    Calculation of the Vertical Totals is defined as the temperature difference between\n    850 hPa and 500 hPa. This is a part of the Total Totals Index.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressure level(s), in order from highest to lowest pressure\n\n    temperature : `pint.Quantity`\n        Temperature corresponding to pressure\n\n    vertical_dim : int, optional\n        The axis corresponding to vertical, defaults to 0. Automatically determined from\n        xarray DataArray arguments.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Vertical Totals\n\n    Examples\n    --------\n    >>> from metpy.calc import vertical_totals\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # compute vertical totals index\n    >>> vertical_totals(p, T)\n    <Quantity(22.9, 'delta_degree_Celsius')>\n\n    \"\"\"\n    # Find temperature at 850 and 500 hPa.\n    (t850, t500) = interpolate_1d(units.Quantity([850, 500], 'hPa'),\n                                  pressure, temperature, axis=vertical_dim)\n\n    # Calculate vertical totals.\n    return t850 - t500",
  "def cross_totals(pressure, temperature, dewpoint, vertical_dim=0):\n    \"\"\"Calculate Cross Totals from the pressure temperature and dewpoint.\n\n    Cross Totals formula derived from [Miller1972]_:\n\n    .. math:: CT = Td850 - T500\n\n    where:\n\n    * :math:`Td850` is the dewpoint at 850 hPa\n    * :math:`T500` is the temperature at 500 hPa\n\n    Calculation of the Cross Totals is defined as the difference between dewpoint\n    at 850 hPa and temperature at 500 hPa. This is a part of the Total Totals Index.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressure level(s), in order from highest to lowest pressure\n\n    temperature : `pint.Quantity`\n        Temperature corresponding to pressure\n\n    dewpoint : `pint.Quantity`\n        Dewpoint temperature corresponding to pressure\n\n    vertical_dim : int, optional\n        The axis corresponding to vertical, defaults to 0. Automatically determined from\n        xarray DataArray arguments.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Cross Totals\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, cross_totals\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # compute the cross totals index\n    >>> cross_totals(p, T, Td)\n    <Quantity(19.7741081, 'delta_degree_Celsius')>\n\n    \"\"\"\n    # Find temperature and dewpoint at 850 and 500 hPa\n    (_, t500), (td850, _) = interpolate_1d(units.Quantity([850, 500], 'hPa'),\n                                           pressure, temperature, dewpoint, axis=vertical_dim)\n\n    # Calculate vertical totals.\n    return td850 - t500",
  "def sweat_index(pressure, temperature, dewpoint, speed, direction, vertical_dim=0):\n    \"\"\"Calculate SWEAT Index.\n\n    SWEAT Index derived from [Miller1972]_:\n\n    .. math:: SWEAT = 12Td_{850} + 20(TT - 49) + 2f_{850} + f_{500} + 125(S + 0.2)\n\n    where:\n\n    * :math:`Td_{850}` is the dewpoint at 850 hPa; the first term is set to zero\n      if :math:`Td_{850}` is negative.\n    * :math:`TT` is the total totals index; the second term is set to zero\n      if :math:`TT` is less than 49\n    * :math:`f_{850}` is the wind speed at 850 hPa\n    * :math:`f_{500}` is the wind speed at 500 hPa\n    * :math:`S` is the shear term: :math:`sin{(dd_{850} - dd_{500})}`, where\n      :math:`dd_{850}` and :math:`dd_{500}` are the wind directions at 850 hPa and 500 hPa,\n      respectively. It is set to zero if any of the following conditions are not met:\n\n    1. :math:`dd_{850}` is between 130 - 250 degrees\n    2. :math:`dd_{500}` is between 210 - 310 degrees\n    3. :math:`dd_{500} - dd_{850} > 0`\n    4. both the wind speeds are greater than or equal to 15 kts\n\n    Calculation of the SWEAT Index consists of a low-level moisture, instability,\n    and the vertical wind shear (both speed and direction). This index aim to\n    determine the likeliness of severe weather and tornadoes.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressure level(s), in order from highest to lowest pressure\n\n    temperature : `pint.Quantity`\n        Temperature corresponding to pressure\n\n    dewpoint : `pint.Quantity`\n        Dewpoint temperature corresponding to pressure\n\n    speed : `pint.Quantity`\n        Wind speed corresponding to pressure\n\n    direction : `pint.Quantity`\n        Wind direction corresponding to pressure\n\n    vertical_dim : int, optional\n        The axis corresponding to vertical, defaults to 0. Automatically determined from\n        xarray DataArray arguments.\n\n    Returns\n    -------\n    `pint.Quantity`\n        SWEAT Index\n\n    \"\"\"\n    # Find dewpoint at 850 hPa.\n    td850 = interpolate_1d(units.Quantity(850, 'hPa'), pressure, dewpoint, axis=vertical_dim)\n\n    # Find total totals index.\n    tt = total_totals_index(pressure, temperature, dewpoint, vertical_dim=vertical_dim)\n\n    # Find wind speed and direction at 850 and 500 hPa\n    (f850, f500), (dd850, dd500) = interpolate_1d(units.Quantity([850, 500], 'hPa'),\n                                                  pressure, speed, direction,\n                                                  axis=vertical_dim)\n\n    # First term is set to zero if Td850 is negative\n    first_term = 12 * np.clip(td850.m_as('degC'), 0, None)\n\n    # Second term is set to zero if TT is less than 49\n    second_term = 20 * np.clip(tt.m_as('degC') - 49, 0, None)\n\n    # Shear term is set to zero if any of four conditions are not met\n    required = ((units.Quantity(130, 'deg') <= dd850) & (dd850 <= units.Quantity(250, 'deg'))\n                & (units.Quantity(210, 'deg') <= dd500) & (dd500 <= units.Quantity(310, 'deg'))\n                & (dd500 - dd850 > 0)\n                & (f850 >= units.Quantity(15, 'knots'))\n                & (f500 >= units.Quantity(15, 'knots')))\n    shear_term = np.atleast_1d(125 * (np.sin(dd500 - dd850) + 0.2))\n    shear_term[~required] = 0\n\n    # Calculate sweat index.\n    return first_term + second_term + (2 * f850.m) + f500.m + shear_term",
  "def dt(p, t):\n        rs = saturation_mixing_ratio._nounit(p, t)\n        frac = (\n            (mpconsts.nounit.Rd * t + mpconsts.nounit.Lv * rs)\n            / (mpconsts.nounit.Cp_d + (\n                mpconsts.nounit.Lv * mpconsts.nounit.Lv * rs * mpconsts.nounit.epsilon\n                / (mpconsts.nounit.Rd * t**2)\n            ))\n        )\n        return frac / p",
  "def _lcl_iter(p, p0, w, t):\n        nonlocal nan_mask\n        td = globals()['dewpoint']._nounit(vapor_pressure._nounit(p, w))\n        p_new = (p0 * (td / t) ** (1. / mpconsts.nounit.kappa))\n        nan_mask = nan_mask | np.isnan(p_new)\n        return np.where(np.isnan(p_new), p, p_new)",
  "def _isen_iter(iter_log_p, isentlevs_nd, ka, a, b, pok):\n        exner = pok * np.exp(-ka * iter_log_p)\n        t = a * iter_log_p + b\n        # Newton-Raphson iteration\n        f = isentlevs_nd - t * exner\n        fp = exner * (ka * t - a)\n        return iter_log_p - (f / fp)",
  "class InvalidSoundingError(ValueError):\n    \"\"\"Raise when a sounding does not meet thermodynamic calculation expectations.\"\"\"",
  "def distances_from_cross_section(cross):\n    \"\"\"Calculate the distances in the x and y directions along a cross-section.\n\n    Parameters\n    ----------\n    cross : `xarray.DataArray`\n        The input DataArray of a cross-section from which to obtain geometric distances in\n        the x and y directions.\n\n    Returns\n    -------\n    x, y : tuple of `xarray.DataArray`\n        A tuple of the x and y distances as DataArrays\n\n    \"\"\"\n    if check_axis(cross.metpy.x, 'longitude') and check_axis(cross.metpy.y, 'latitude'):\n        # Use pyproj to obtain x and y distances\n        g = cross.metpy.pyproj_crs.get_geod()\n        lon = cross.metpy.x\n        lat = cross.metpy.y\n\n        forward_az, _, distance = g.inv(lon[0].values * np.ones_like(lon),\n                                        lat[0].values * np.ones_like(lat),\n                                        lon.values,\n                                        lat.values)\n        x = distance * np.sin(np.deg2rad(forward_az))\n        y = distance * np.cos(np.deg2rad(forward_az))\n\n        # Build into DataArrays\n        x = xr.DataArray(units.Quantity(x, 'meter'), coords=lon.coords, dims=lon.dims)\n        y = xr.DataArray(units.Quantity(y, 'meter'), coords=lat.coords, dims=lat.dims)\n\n    elif check_axis(cross.metpy.x, 'x') and check_axis(cross.metpy.y, 'y'):\n\n        # Simply return what we have\n        x = cross.metpy.x\n        y = cross.metpy.y\n\n    else:\n        raise AttributeError('Sufficient horizontal coordinates not defined.')\n\n    return x, y",
  "def latitude_from_cross_section(cross):\n    \"\"\"Calculate the latitude of points in a cross-section.\n\n    Parameters\n    ----------\n    cross : `xarray.DataArray`\n        The input DataArray of a cross-section from which to obtain latitudes\n\n    Returns\n    -------\n    latitude : `xarray.DataArray`\n        Latitude of points\n\n    \"\"\"\n    y = cross.metpy.y\n    if check_axis(y, 'latitude'):\n        return y\n    else:\n        from pyproj import Proj\n        latitude = Proj(cross.metpy.pyproj_crs)(\n            cross.metpy.x.values,\n            y.values,\n            inverse=True,\n            radians=False\n        )[1]\n        return xr.DataArray(units.Quantity(latitude, 'degrees_north'), coords=y.coords,\n                            dims=y.dims)",
  "def unit_vectors_from_cross_section(cross, index='index'):\n    r\"\"\"Calculate the unit tangent and unit normal vectors from a cross-section.\n\n    Given a path described parametrically by :math:`\\vec{l}(i) = (x(i), y(i))`, we can find\n    the unit tangent vector by the formula:\n\n    .. math:: \\vec{T}(i) =\n        \\frac{1}{\\sqrt{\\left( \\frac{dx}{di} \\right)^2 + \\left( \\frac{dy}{di} \\right)^2}}\n        \\left( \\frac{dx}{di}, \\frac{dy}{di} \\right)\n\n    From this, because this is a two-dimensional path, the normal vector can be obtained by a\n    simple :math:`\\frac{\\pi}{2}` rotation.\n\n    Parameters\n    ----------\n    cross : `xarray.DataArray`\n        The input DataArray of a cross-section from which to obtain latitudes\n\n    index : str or int, optional\n        Denotes the index coordinate of the cross-section, defaults to 'index' as\n        set by `metpy.interpolate.cross_section`\n\n    Returns\n    -------\n    unit_tangent_vector, unit_normal_vector : tuple of `numpy.ndarray`\n        Arrays describing the unit tangent and unit normal vectors (in x,y) for all points\n        along the cross-section\n\n    \"\"\"\n    x, y = distances_from_cross_section(cross)\n    dx_di = first_derivative(x, axis=index).values\n    dy_di = first_derivative(y, axis=index).values\n    tangent_vector_mag = np.hypot(dx_di, dy_di)\n    unit_tangent_vector = np.vstack([dx_di / tangent_vector_mag, dy_di / tangent_vector_mag])\n    unit_normal_vector = np.vstack([-dy_di / tangent_vector_mag, dx_di / tangent_vector_mag])\n    return unit_tangent_vector, unit_normal_vector",
  "def cross_section_components(data_x, data_y, index='index'):\n    r\"\"\"Obtain the tangential and normal components of a cross-section of a vector field.\n\n    Parameters\n    ----------\n    data_x : `xarray.DataArray`\n        The input DataArray of the x-component (in terms of data projection) of the vector\n        field.\n\n    data_y : `xarray.DataArray`\n        The input DataArray of the y-component (in terms of data projection) of the vector\n        field.\n\n    index : str or int, optional\n        Denotes the index coordinate of the cross-section, defaults to 'index' as\n        set by `metpy.interpolate.cross_section`\n\n    Returns\n    -------\n    component_tangential, component_normal: tuple of `xarray.DataArray`\n        Components of the vector field in the tangential and normal directions, respectively\n\n    See Also\n    --------\n    tangential_component, normal_component\n\n    Notes\n    -----\n    The coordinates of `data_x` and `data_y` must match.\n\n    \"\"\"\n    # Get the unit vectors\n    unit_tang, unit_norm = unit_vectors_from_cross_section(data_x, index=index)\n\n    # Take the dot products\n    component_tang = data_x * unit_tang[0] + data_y * unit_tang[1]\n    component_norm = data_x * unit_norm[0] + data_y * unit_norm[1]\n\n    return component_tang, component_norm",
  "def normal_component(data_x, data_y, index='index'):\n    r\"\"\"Obtain the normal component of a cross-section of a vector field.\n\n    Parameters\n    ----------\n    data_x : `xarray.DataArray`\n        The input DataArray of the x-component (in terms of data projection) of the vector\n        field.\n\n    data_y : `xarray.DataArray`\n        The input DataArray of the y-component (in terms of data projection) of the vector\n        field.\n\n    index : str or int, optional\n        Denotes the index coordinate of the cross-section, defaults to 'index' as\n        set by `metpy.interpolate.cross_section`\n\n    Returns\n    -------\n    component_normal: `xarray.DataArray`\n        Component of the vector field in the normal directions\n\n    See Also\n    --------\n    cross_section_components, tangential_component\n\n    Notes\n    -----\n    The coordinates of `data_x` and `data_y` must match.\n\n    \"\"\"\n    # Get the unit vectors\n    _, unit_norm = unit_vectors_from_cross_section(data_x, index=index)\n\n    # Take the dot products\n    component_norm = data_x * unit_norm[0] + data_y * unit_norm[1]\n\n    # Reattach only reliable attributes after operation\n    if 'grid_mapping' in data_x.attrs:\n        component_norm.attrs['grid_mapping'] = data_x.attrs['grid_mapping']\n\n    return component_norm",
  "def tangential_component(data_x, data_y, index='index'):\n    r\"\"\"Obtain the tangential component of a cross-section of a vector field.\n\n    Parameters\n    ----------\n    data_x : `xarray.DataArray`\n        The input DataArray of the x-component (in terms of data projection) of the vector\n        field\n\n    data_y : `xarray.DataArray`\n        The input DataArray of the y-component (in terms of data projection) of the vector\n        field\n\n    index : str or int, optional\n        Denotes the index coordinate of the cross-section, defaults to 'index' as\n        set by `metpy.interpolate.cross_section`\n\n    Returns\n    -------\n    component_tangential: `xarray.DataArray`\n        Component of the vector field in the tangential directions\n\n    See Also\n    --------\n    cross_section_components, normal_component\n\n    Notes\n    -----\n    The coordinates of `data_x` and `data_y` must match.\n\n    \"\"\"\n    # Get the unit vectors\n    unit_tang, _ = unit_vectors_from_cross_section(data_x, index=index)\n\n    # Take the dot products\n    component_tang = data_x * unit_tang[0] + data_y * unit_tang[1]\n\n    # Reattach only reliable attributes after operation\n    if 'grid_mapping' in data_x.attrs:\n        component_tang.attrs['grid_mapping'] = data_x.attrs['grid_mapping']\n\n    return component_tang",
  "def absolute_momentum(u, v, index='index'):\n    r\"\"\"Calculate cross-sectional absolute momentum (also called pseudoangular momentum).\n\n    The cross-sectional absolute momentum is calculated given u- and v-components of the wind\n    along a 2 dimensional vertical cross-section. The coordinates of `u` and `v` must match.\n\n    Parameters\n    ----------\n    u : `xarray.DataArray`\n        The input DataArray of the x-component (in terms of data projection) of the wind.\n\n    v : `xarray.DataArray`\n        The input DataArray of the y-component (in terms of data projection) of the wind.\n\n    index : str or int, optional\n        Denotes the index coordinate of the cross-section, defaults to 'index' as\n        set by `metpy.interpolate.cross_section`\n\n    Returns\n    -------\n    absolute_momentum: `xarray.DataArray`\n        Absolute momentum\n\n    See Also\n    --------\n    metpy.interpolate.cross_section, cross_section_components\n\n    Notes\n    -----\n    As given in [Schultz1999]_, absolute momentum (also called pseudoangular momentum) is\n    given by:\n\n    .. math:: M = v + fx\n\n    where :math:`v` is the along-front component of the wind and :math:`x` is the cross-front\n    distance. Applied to a cross-section taken perpendicular to the front, :math:`v` becomes\n    the normal component of the wind and :math:`x` the tangential distance.\n\n    If using this calculation in assessing symmetric instability, geostrophic wind should be\n    used so that geostrophic absolute momentum :math:`\\left(M_g\\right)` is obtained, as\n    described in [Schultz1999]_.\n\n    .. versionchanged:: 1.0\n       Renamed ``u_wind``, ``v_wind`` parameters to ``u``, ``v``\n\n    \"\"\"\n    # Get the normal component of the wind\n    norm_wind = normal_component(u.metpy.quantify(), v.metpy.quantify(), index=index)\n\n    # Get other pieces of calculation (all as ndarrays matching shape of norm_wind)\n    latitude = latitude_from_cross_section(norm_wind)\n    _, latitude = xr.broadcast(norm_wind, latitude)\n    f = coriolis_parameter(latitude)\n    x, y = distances_from_cross_section(norm_wind)\n    _, x, y = xr.broadcast(norm_wind, x, y)\n    distance = np.hypot(x.metpy.quantify(), y.metpy.quantify())\n\n    return (norm_wind + f * distance).metpy.convert_units('m/s')",
  "def precipitable_water(pressure, dewpoint, *, bottom=None, top=None):\n    r\"\"\"Calculate precipitable water through the depth of a sounding.\n\n    Formula used is:\n\n    .. math::  -\\frac{1}{\\rho_l g} \\int\\limits_{p_\\text{bottom}}^{p_\\text{top}} r dp\n\n    from [Salby1996]_, p. 28\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile\n\n    dewpoint : `pint.Quantity`\n        Atmospheric dewpoint profile\n\n    bottom: `pint.Quantity`, optional\n        Bottom of the layer, specified in pressure. Defaults to None (highest pressure).\n\n    top: `pint.Quantity`, optional\n        Top of the layer, specified in pressure. Defaults to None (lowest pressure).\n\n    Returns\n    -------\n    `pint.Quantity`\n        Precipitable water in the layer\n\n    Examples\n    --------\n    >>> pressure = np.array([1000, 950, 900]) * units.hPa\n    >>> dewpoint = np.array([20, 15, 10]) * units.degC\n    >>> pw = precipitable_water(pressure, dewpoint)\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n\n    .. versionchanged:: 1.0\n       Signature changed from ``(dewpt, pressure, bottom=None, top=None)``\n\n    \"\"\"\n    # Sort pressure and dewpoint to be in decreasing pressure order (increasing height)\n    sort_inds = np.argsort(pressure)[::-1]\n    pressure = pressure[sort_inds]\n    dewpoint = dewpoint[sort_inds]\n\n    pressure, dewpoint = _remove_nans(pressure, dewpoint)\n\n    min_pressure = np.nanmin(pressure)\n    max_pressure = np.nanmax(pressure)\n\n    if top is None:\n        top = min_pressure\n    elif not min_pressure <= top <= max_pressure:\n        raise ValueError(f'The pressure and dewpoint profile ranges from {max_pressure} to '\n                         f'{min_pressure}, after removing missing values. {top} is outside '\n                         'this range.')\n\n    if bottom is None:\n        bottom = max_pressure\n    elif not min_pressure <= bottom <= max_pressure:\n        raise ValueError(f'The pressure and dewpoint profile ranges from {max_pressure} to '\n                         f'{min_pressure}, after removing missing values. {bottom} is outside '\n                         'this range.')\n\n    pres_layer, dewpoint_layer = get_layer(pressure, dewpoint, bottom=bottom,\n                                           depth=bottom - top)\n\n    w = mixing_ratio(saturation_vapor_pressure(dewpoint_layer), pres_layer)\n\n    # Since pressure is in decreasing order, pw will be the opposite sign of that expected.\n    pw = -np.trapz(w, pres_layer) / (mpconsts.g * mpconsts.rho_l)\n    return pw.to('millimeters')",
  "def mean_pressure_weighted(pressure, *args, height=None, bottom=None, depth=None):\n    r\"\"\"Calculate pressure-weighted mean of an arbitrary variable through a layer.\n\n    Layer bottom and depth specified in height or pressure.\n\n    .. math::  MPW = \\frac{\\int_{p_s}^{p_b} A p dp}{\\int_{p_s}^{p_b} p dp}\n\n    where:\n\n    * :math:`MPW` is the pressure-weighted mean of a variable.\n    * :math:`p_b` is the bottom pressure level.\n    * :math:`p_s` is the top pressure level.\n    * :math:`A` is the variable whose pressure-weighted mean is being calculated.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile.\n\n    args : `pint.Quantity`\n        Parameters for which the weighted-continuous mean is to be calculated.\n\n    height : `pint.Quantity`, optional\n        Heights from sounding. Standard atmosphere heights assumed (if needed)\n        if no heights are given.\n\n    bottom: `pint.Quantity`, optional\n        The bottom of the layer in either the provided height coordinate\n        or in pressure. Don't provide in meters AGL unless the provided\n        height coordinate is meters AGL. Default is the first observation,\n        assumed to be the surface.\n\n    depth: `pint.Quantity`, optional\n        Depth of the layer in meters or hPa. Defaults to 100 hPa.\n\n    Returns\n    -------\n    list of `pint.Quantity`\n        list of layer mean value for each profile in args.\n\n    See Also\n    --------\n    weighted_continuous_average\n\n    Examples\n    --------\n    >>> from metpy.calc import mean_pressure_weighted\n    >>> from metpy.units import units\n    >>> p = [1000, 850, 700, 500] * units.hPa\n    >>> T = [30, 15, 5, -5] * units.degC\n    >>> mean_pressure_weighted(p, T)\n    [<Quantity(298.54368, 'kelvin')>]\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``heights`` parameter to ``height``\n\n    \"\"\"\n    # Split pressure profile from other variables to average\n    pres_prof, *others = get_layer(pressure, *args, height=height, bottom=bottom, depth=depth)\n\n    # Taking the integral of the weights (pressure) to feed into the weighting\n    # function. Said integral works out to this function:\n    pres_int = 0.5 * (pres_prof[-1] ** 2 - pres_prof[0] ** 2)\n\n    # Perform integration on the profile for each variable\n    return [np.trapz(var_prof * pres_prof, x=pres_prof) / pres_int for var_prof in others]",
  "def weighted_continuous_average(pressure, *args, height=None, bottom=None, depth=None):\n    r\"\"\"Calculate weighted-continuous mean of an arbitrary variable through a layer.\n\n    Layer top and bottom specified in height or pressure.\n\n    Formula based on that from [Holton2004]_ pg. 76 and the NCL function ``wgt_vertical_n``\n\n    .. math::  WCA = \\frac{\\int_{p_s}^{p_b} A dp}{\\int_{p_s}^{p_b} dp}\n\n    where:\n\n    * :math:`WCA` is the weighted continuous average of a variable.\n    * :math:`p_b` is the bottom pressure level.\n    * :math:`p_s` is the top pressure level.\n    * :math:`A` is the variable whose weighted continuous average is being calculated.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile.\n\n    args : `pint.Quantity`\n        Parameters for which the weighted-continuous mean is to be calculated.\n\n    height : `pint.Quantity`, optional\n        Heights from sounding. Standard atmosphere heights assumed (if needed)\n        if no heights are given.\n\n    bottom: `pint.Quantity`, optional\n        The bottom of the layer in either the provided height coordinate\n        or in pressure. Don't provide in meters AGL unless the provided\n        height coordinate is meters AGL. Default is the first observation,\n        assumed to be the surface.\n\n    depth: `pint.Quantity`, optional\n        Depth of the layer in meters or hPa.\n\n    Returns\n    -------\n    list of `pint.Quantity`\n        list of layer mean value for each profile in args.\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    \"\"\"\n    # Split pressure profile from other variables to average\n    pres_prof, *others = get_layer(\n        pressure, *args, height=height, bottom=bottom, depth=depth\n    )\n\n    return [np.trapz(var_prof, x=pres_prof) / (pres_prof[-1] - pres_prof[0])\n            for var_prof in others]",
  "def bunkers_storm_motion(pressure, u, v, height):\n    r\"\"\"Calculate right-mover and left-mover supercell storm motions using the Bunkers method.\n\n    This is a physically based, shear-relative, and Galilean invariant method for predicting\n    supercell motion. Full atmospheric profiles of wind components, as well as pressure and\n    heights, need to be provided so that calculation can properly calculate the required\n    surface to 6 km mean flow.\n\n    The calculation in summary is (from [Bunkers2000]_):\n\n    * surface to 6 km non-pressure-weighted mean wind\n    * a deviation from the sfc to 6 km mean wind of 7.5 m s\u22121\n    * a 5.5 to 6 km mean wind for the head of the vertical wind shear vector\n    * a surface to 0.5 km mean wind for the tail of the vertical wind shear vector\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressure from full profile\n\n    u : `pint.Quantity`\n        Full profile of the U-component of the wind\n\n    v : `pint.Quantity`\n        Full profile of the V-component of the wind\n\n    height : `pint.Quantity`\n        Full profile of height\n\n    Returns\n    -------\n    right_mover: (`pint.Quantity`, `pint.Quantity`)\n        Scalar U- and V- components of Bunkers right-mover storm motion\n\n    left_mover: (`pint.Quantity`, `pint.Quantity`)\n        Scalar U- and V- components of Bunkers left-mover storm motion\n\n    wind_mean: (`pint.Quantity`, `pint.Quantity`)\n        Scalar U- and V- components of surface to 6 km mean flow\n\n    Examples\n    --------\n    >>> from metpy.calc import bunkers_storm_motion, wind_components\n    >>> from metpy.units import units\n    >>> p = [1000, 925, 850, 700, 500, 400] * units.hPa\n    >>> h = [250, 700, 1500, 3100, 5720, 7120] * units.meters\n    >>> wdir = [165, 180, 190, 210, 220, 250] * units.degree\n    >>> sped = [5, 15, 20, 30, 50, 60] * units.knots\n    >>> u, v = wind_components(sped, wdir)\n    >>> bunkers_storm_motion(p, u, v, h)\n    (<Quantity([22.09618172 12.43406736], 'knot')>,\n    <Quantity([ 6.02861839 36.76517865], 'knot')>,\n    <Quantity([14.06240005 24.599623  ], 'knot')>)\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``heights`` parameter to ``height``\n\n    \"\"\"\n    # remove nans from input data\n    pressure, u, v, height = _remove_nans(pressure, u, v, height)\n\n    # mean wind from sfc-6km\n    wind_mean = weighted_continuous_average(pressure, u, v, height=height,\n                                            depth=units.Quantity(6000, 'meter'))\n\n    wind_mean = units.Quantity.from_list(wind_mean)\n\n    # mean wind from sfc-500m\n    wind_500m = weighted_continuous_average(pressure, u, v, height=height,\n                                            depth=units.Quantity(500, 'meter'))\n\n    wind_500m = units.Quantity.from_list(wind_500m)\n\n    # mean wind from 5.5-6km\n    wind_5500m = weighted_continuous_average(\n        pressure, u, v, height=height,\n        depth=units.Quantity(500, 'meter'),\n        bottom=height[0] + units.Quantity(5500, 'meter'))\n\n    wind_5500m = units.Quantity.from_list(wind_5500m)\n\n    # Calculate the shear vector from sfc-500m to 5.5-6km\n    shear = wind_5500m - wind_500m\n\n    # Take the cross product of the wind shear and k, and divide by the vector magnitude and\n    # multiply by the deviation empirically calculated in Bunkers (2000) (7.5 m/s)\n    shear_cross = concatenate([shear[1], -shear[0]])\n    shear_mag = np.hypot(*shear)\n    rdev = shear_cross * (units.Quantity(7.5, 'm/s').to(u.units) / shear_mag)\n\n    # Add the deviations to the layer average wind to get the RM motion\n    right_mover = wind_mean + rdev\n\n    # Subtract the deviations to get the LM motion\n    left_mover = wind_mean - rdev\n\n    return right_mover, left_mover, wind_mean",
  "def bulk_shear(pressure, u, v, height=None, bottom=None, depth=None):\n    r\"\"\"Calculate bulk shear through a layer.\n\n    Layer top and bottom specified in meters or pressure.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile\n\n    u : `pint.Quantity`\n        U-component of wind\n\n    v : `pint.Quantity`\n        V-component of wind\n\n    height : `pint.Quantity`, optional\n        Heights from sounding\n\n    depth: `pint.Quantity`, optional\n        The depth of the layer in meters or hPa. Defaults to 100 hPa.\n\n    bottom: `pint.Quantity`, optional\n        The bottom of the layer in height or pressure coordinates.\n        If using a height, it must be in the same coordinates as the given\n        heights (i.e., don't use meters AGL unless given heights\n        are in meters AGL.) Defaults to the highest pressure or lowest height given.\n\n    Returns\n    -------\n    u_shr: `pint.Quantity`\n        U-component of layer bulk shear\n    v_shr: `pint.Quantity`\n        V-component of layer bulk shear\n\n    Examples\n    --------\n    >>> from metpy.calc import bulk_shear, wind_components\n    >>> from metpy.units import units\n    >>> p = [1000, 925, 850, 700, 500] * units.hPa\n    >>> wdir = [165, 180, 190, 210, 220] * units.degree\n    >>> sped = [5, 15, 20, 30, 50] * units.knots\n    >>> u, v = wind_components(sped, wdir)\n    >>> bulk_shear(p, u, v)\n    (<Quantity(2.41943319, 'knot')>, <Quantity(11.6920573, 'knot')>)\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``heights`` parameter to ``height``\n\n    \"\"\"\n    _, u_layer, v_layer = get_layer(pressure, u, v, height=height,\n                                    bottom=bottom, depth=depth)\n\n    u_shr = u_layer[-1] - u_layer[0]\n    v_shr = v_layer[-1] - v_layer[0]\n\n    return u_shr, v_shr",
  "def supercell_composite(mucape, effective_storm_helicity, effective_shear):\n    r\"\"\"Calculate the supercell composite parameter.\n\n    The supercell composite parameter is designed to identify\n    environments favorable for the development of supercells,\n    and is calculated using the formula developed by\n    [Thompson2004]_:\n\n    .. math::  \\text{SCP} = \\frac{\\text{MUCAPE}}{1000 \\text{J/kg}} *\n               \\frac{\\text{Effective SRH}}{50 \\text{m}^2/\\text{s}^2} *\n               \\frac{\\text{Effective Shear}}{20 \\text{m/s}}\n\n    The effective_shear term is set to zero below 10 m/s and\n    capped at 1 when effective_shear exceeds 20 m/s.\n\n    Parameters\n    ----------\n    mucape : `pint.Quantity`\n        Most-unstable CAPE\n\n    effective_storm_helicity : `pint.Quantity`\n        Effective-layer storm-relative helicity\n\n    effective_shear : `pint.Quantity`\n        Effective bulk shear\n\n    Returns\n    -------\n    `pint.Quantity`\n        Supercell composite\n\n    Examples\n    --------\n    >>> from metpy.calc import supercell_composite\n    >>> from metpy.units import units\n    >>> supercell_composite(2500 * units('J/kg'), 125 * units('m^2/s^2'),\n    ...                     50 * units.knot).to_base_units()\n    <Quantity([6.25], 'dimensionless')>\n\n    \"\"\"\n    effective_shear = np.clip(np.atleast_1d(effective_shear), None, units.Quantity(20, 'm/s'))\n    effective_shear[effective_shear < units.Quantity(10, 'm/s')] = units.Quantity(0, 'm/s')\n    effective_shear = effective_shear / units.Quantity(20, 'm/s')\n\n    return ((mucape / units.Quantity(1000, 'J/kg'))\n            * (effective_storm_helicity / units.Quantity(50, 'm^2/s^2'))\n            * effective_shear).to('dimensionless')",
  "def significant_tornado(sbcape, surface_based_lcl_height, storm_helicity_1km, shear_6km):\n    r\"\"\"Calculate the significant tornado parameter (fixed layer).\n\n    The significant tornado parameter is designed to identify\n    environments favorable for the production of significant\n    tornadoes contingent upon the development of supercells.\n    It's calculated according to the formula used on the SPC\n    mesoanalysis page, updated in [Thompson2004]_:\n\n    .. math::  \\text{SIGTOR} = \\frac{\\text{SBCAPE}}{1500 \\text{J/kg}} * \\frac{(2000 \\text{m} -\n               \\text{LCL}_\\text{SB})}{1000 \\text{m}} *\n               \\frac{SRH_{\\text{1km}}}{150 \\text{m}^\\text{s}/\\text{s}^2} *\n               \\frac{\\text{Shear}_\\text{6km}}{20 \\text{m/s}}\n\n    The lcl height is set to zero when the lcl is above 2000m and\n    capped at 1 when below 1000m, and the shr6 term is set to 0\n    when shr6 is below 12.5 m/s and maxed out at 1.5 when shr6\n    exceeds 30 m/s.\n\n    Parameters\n    ----------\n    sbcape : `pint.Quantity`\n        Surface-based CAPE\n\n    surface_based_lcl_height : `pint.Quantity`\n        Surface-based lifted condensation level\n\n    storm_helicity_1km : `pint.Quantity`\n        Surface-1km storm-relative helicity\n\n    shear_6km : `pint.Quantity`\n        Surface-6km bulk shear\n\n    Returns\n    -------\n    `pint.Quantity`\n        Significant tornado parameter\n\n    Examples\n    --------\n    >>> from metpy.calc import significant_tornado\n    >>> from metpy.units import units\n    >>> significant_tornado(3000 * units('J/kg'), 750 * units.meters,\n    ...                     150 * units('m^2/s^2'), 25 * units.knot).to_base_units()\n    <Quantity([1.28611111], 'dimensionless')>\n\n    \"\"\"\n    surface_based_lcl_height = np.clip(np.atleast_1d(surface_based_lcl_height),\n                                       units.Quantity(1000., 'm'), units.Quantity(2000., 'm'))\n    surface_based_lcl_height = ((units.Quantity(2000., 'm') - surface_based_lcl_height)\n                                / units.Quantity(1000., 'm'))\n    shear_6km = np.clip(np.atleast_1d(shear_6km), None, units.Quantity(30, 'm/s'))\n    shear_6km[shear_6km < units.Quantity(12.5, 'm/s')] = units.Quantity(0, 'm/s')\n    shear_6km /= units.Quantity(20, 'm/s')\n\n    return ((sbcape / units.Quantity(1500., 'J/kg'))\n            * surface_based_lcl_height\n            * (storm_helicity_1km / units.Quantity(150., 'm^2/s^2'))\n            * shear_6km)",
  "def critical_angle(pressure, u, v, height, u_storm, v_storm):\n    r\"\"\"Calculate the critical angle.\n\n    The critical angle is the angle between the 10m storm-relative inflow vector\n    and the 10m-500m shear vector. A critical angle near 90 degrees indicates\n    that a storm in this environment on the indicated storm motion vector\n    is likely ingesting purely streamwise vorticity into its updraft, and [Esterheld2008]_\n    showed that significantly tornadic supercells tend to occur in environments\n    with critical angles near 90 degrees.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressures from sounding\n\n    u : `pint.Quantity`\n        U-component of sounding winds\n\n    v : `pint.Quantity`\n        V-component of sounding winds\n\n    height : `pint.Quantity`\n        Heights from sounding\n\n    u_storm : `pint.Quantity`\n        U-component of storm motion\n\n    v_storm : `pint.Quantity`\n        V-component of storm motion\n\n    Returns\n    -------\n    `pint.Quantity`\n        Critical angle in degrees\n\n    Examples\n    --------\n    >>> from metpy.calc import critical_angle, wind_components\n    >>> from metpy.units import units\n    >>> p = [1000, 925, 850, 700, 500, 400] * units.hPa\n    >>> h = [250, 700, 1500, 3100, 5720, 7120] * units.meters\n    >>> wdir = [165, 180, 190, 210, 220, 250] * units.degree\n    >>> sped = [5, 15, 20, 30, 50, 60] * units.knots\n    >>> u, v = wind_components(sped, wdir)\n    >>> critical_angle(p, u, v, h, 7 * units.knots, 7 * units.knots)\n    <Quantity(67.0942521, 'degree')>\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``heights`` parameter to ``height``\n\n    \"\"\"\n    # Convert everything to m/s\n    u = u.to('m/s')\n    v = v.to('m/s')\n    u_storm = u_storm.to('m/s')\n    v_storm = v_storm.to('m/s')\n\n    sort_inds = np.argsort(pressure[::-1])\n    pressure = pressure[sort_inds]\n    height = height[sort_inds]\n    u = u[sort_inds]\n    v = v[sort_inds]\n\n    # Calculate sfc-500m shear vector\n    shr5 = bulk_shear(pressure, u, v, height=height, depth=units.Quantity(500, 'meter'))\n\n    # Make everything relative to the sfc wind orientation\n    umn = u_storm - u[0]\n    vmn = v_storm - v[0]\n\n    vshr = np.asarray([shr5[0].magnitude, shr5[1].magnitude])\n    vsm = np.asarray([umn.magnitude, vmn.magnitude])\n    angle_c = np.dot(vshr, vsm) / (np.linalg.norm(vshr) * np.linalg.norm(vsm))\n    critical_angle = units.Quantity(np.arccos(angle_c), 'radian')\n\n    return critical_angle.to('degrees')",
  "def vorticity(\n    u, v, *, dx=None, dy=None, x_dim=-1, y_dim=-2,\n    parallel_scale=None, meridional_scale=None\n):\n    r\"\"\"Calculate the vertical vorticity of the horizontal wind.\n\n    Parameters\n    ----------\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the wind\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the wind\n\n    Returns\n    -------\n    (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        vertical vorticity\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input. Also optional if one-dimensional\n        longitude and latitude arguments are given for your data on a non-projected grid.\n        Keyword-only argument.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input. Also optional if one-dimensional\n        longitude and latitude arguments are given for your data on a non-projected grid.\n        Keyword-only argument.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`. Keyword-only argument.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`. Keyword-only argument.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n    See Also\n    --------\n    divergence, absolute_vorticity\n\n    Notes\n    -----\n    This implements a numerical version of the typical vertical vorticity equation:\n\n    .. math:: \\zeta = \\frac{\\partial v}{\\partial x} - \\frac{\\partial u}{\\partial y}\n\n    If sufficient grid projection information is provided, these partial derivatives are\n    taken from the projection-correct derivative matrix of the vector wind. Otherwise, they\n    are evaluated as scalar derivatives on a Cartesian grid.\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(u, v, dx, dy)``\n\n    \"\"\"\n    dudy, dvdx = vector_derivative(\n        u, v, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim, parallel_scale=parallel_scale,\n        meridional_scale=meridional_scale, return_only=('du/dy', 'dv/dx')\n    )\n    return dvdx - dudy",
  "def divergence(u, v, *, dx=None, dy=None, x_dim=-1, y_dim=-2,\n               parallel_scale=None, meridional_scale=None):\n    r\"\"\"Calculate the horizontal divergence of a vector.\n\n    Parameters\n    ----------\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the vector\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the vector\n\n    Returns\n    -------\n    (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        The horizontal divergence\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input. Keyword-only argument.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input. Keyword-only argument.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`. Keyword-only argument.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`. Keyword-only argument.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n    See Also\n    --------\n    vorticity\n\n    Notes\n    -----\n    This implements a numerical version of the typical equation of 2D divergence of a vector in\n    Cartesian coordinates:\n\n    .. math:: \\nabla \\cdot \\vec{U} =\n        \\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y}\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(u, v, dx, dy)``\n\n    \"\"\"\n    dudx, dvdy = vector_derivative(\n        u, v, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim, parallel_scale=parallel_scale,\n        meridional_scale=meridional_scale, return_only=('du/dx', 'dv/dy')\n    )\n    return dudx + dvdy",
  "def shearing_deformation(u, v, dx=None, dy=None, x_dim=-1, y_dim=-2, *,\n                         parallel_scale=None, meridional_scale=None):\n    r\"\"\"Calculate the shearing deformation of the horizontal wind.\n\n    Parameters\n    ----------\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the wind\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the wind\n\n    Returns\n    -------\n    (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        Shearing Deformation\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(u, v, dx, dy)``\n\n    See Also\n    --------\n    stretching_deformation, total_deformation\n\n    \"\"\"\n    dudy, dvdx = vector_derivative(\n        u, v, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim, parallel_scale=parallel_scale,\n        meridional_scale=meridional_scale, return_only=('du/dy', 'dv/dx')\n    )\n    return dvdx + dudy",
  "def stretching_deformation(u, v, dx=None, dy=None, x_dim=-1, y_dim=-2, *,\n                           parallel_scale=None, meridional_scale=None):\n    r\"\"\"Calculate the stretching deformation of the horizontal wind.\n\n    Parameters\n    ----------\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the wind\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the wind\n\n    Returns\n    -------\n    (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        Stretching Deformation\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(u, v, dx, dy)``\n\n    See Also\n    --------\n    shearing_deformation, total_deformation\n\n    \"\"\"\n    dudx, dvdy = vector_derivative(\n        u, v, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim, parallel_scale=parallel_scale,\n        meridional_scale=meridional_scale, return_only=('du/dx', 'dv/dy')\n    )\n    return dudx - dvdy",
  "def total_deformation(u, v, dx=None, dy=None, x_dim=-1, y_dim=-2, *,\n                      parallel_scale=None, meridional_scale=None):\n    r\"\"\"Calculate the total deformation of the horizontal wind.\n\n    Parameters\n    ----------\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the wind\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the wind\n\n    Returns\n    -------\n    (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        Total Deformation\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n    See Also\n    --------\n    shearing_deformation, stretching_deformation\n\n    Notes\n    -----\n    If inputs have more than two dimensions, they are assumed to have either leading dimensions\n    of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(u, v, dx, dy)``\n\n    \"\"\"\n    dudx, dudy, dvdx, dvdy = vector_derivative(\n        u, v, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim, parallel_scale=parallel_scale,\n        meridional_scale=meridional_scale\n    )\n    return np.sqrt((dvdx + dudy)**2 + (dudx - dvdy)**2)",
  "def advection(\n    scalar,\n    u=None,\n    v=None,\n    w=None,\n    *,\n    dx=None,\n    dy=None,\n    dz=None,\n    x_dim=-1,\n    y_dim=-2,\n    vertical_dim=-3,\n    parallel_scale=None,\n    meridional_scale=None\n):\n    r\"\"\"Calculate the advection of a scalar field by 1D, 2D, or 3D winds.\n\n    If ``scalar`` is a `xarray.DataArray`, only ``u``, ``v``, and/or ``w`` are required\n    to compute advection. The horizontal and vertical spacing (``dx``, ``dy``, and ``dz``)\n    and axis numbers (``x_dim``, ``y_dim``, and ``z_dim``) are automatically inferred from\n    ``scalar``. But if ``scalar`` is a `pint.Quantity`, the horizontal and vertical\n    spacing ``dx``, ``dy``, and ``dz`` needs to be provided, and each array should have one\n    item less than the size of ``scalar`` along the applicable axis. Additionally, ``x_dim``,\n    ``y_dim``, and ``z_dim`` are required if ``scalar`` does not have the default\n    [..., Z, Y, X] ordering. ``dx``, ``dy``, ``dz``, ``x_dim``, ``y_dim``, and ``z_dim``\n    are keyword-only arguments.\n\n    ``parallel_scale`` and ``meridional_scale`` specify the parallel and meridional scale of\n    map projection at data coordinate, respectively. They are optional when (a)\n    `xarray.DataArray` with latitude/longitude coordinates and MetPy CRS are used as input\n    or (b) longitude, latitude, and crs are given. If otherwise omitted, calculation\n    will be carried out on a Cartesian, rather than geospatial, grid. Both are keyword-only\n    arguments.\n\n    Parameters\n    ----------\n    scalar : `pint.Quantity` or `xarray.DataArray`\n        The quantity (an N-dimensional array) to be advected.\n    u : `pint.Quantity` or `xarray.DataArray` or None\n        The wind component in the x dimension. An N-dimensional array.\n    v : `pint.Quantity` or `xarray.DataArray` or None\n        The wind component in the y dimension. An N-dimensional array.\n    w : `pint.Quantity` or `xarray.DataArray` or None\n        The wind component in the z dimension. An N-dimensional array.\n\n    Returns\n    -------\n    `pint.Quantity` or `xarray.DataArray`\n        An N-dimensional array containing the advection at all grid points.\n\n    Other Parameters\n    ----------------\n    dx: `pint.Quantity` or None, optional\n        Grid spacing in the x dimension.\n    dy: `pint.Quantity` or None, optional\n        Grid spacing in the y dimension.\n    dz: `pint.Quantity` or None, optional\n        Grid spacing in the z dimension.\n    x_dim: int or None, optional\n        Axis number in the x dimension. Defaults to -1 for (..., Z, Y, X) dimension ordering.\n    y_dim: int or None, optional\n        Axis number in the y dimension. Defaults to -2 for (..., Z, Y, X) dimension ordering.\n    vertical_dim: int or None, optional\n        Axis number in the z dimension. Defaults to -3 for (..., Z, Y, X) dimension ordering.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate.\n\n    Notes\n    -----\n    This implements the advection of a scalar quantity by wind:\n\n    .. math:: -\\mathbf{u} \\cdot \\nabla = -(u \\frac{\\partial}{\\partial x}\n              + v \\frac{\\partial}{\\partial y} + w \\frac{\\partial}{\\partial z})\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(scalar, wind, deltas)``\n\n    \"\"\"\n    # Set up vectors of provided components\n    wind_vector = {key: value\n                   for key, value in {'u': u, 'v': v, 'w': w}.items()\n                   if value is not None}\n    return_only_horizontal = {key: value\n                              for key, value in {'u': 'df/dx', 'v': 'df/dy'}.items()\n                              if key in wind_vector}\n    gradient_vector = ()\n\n    # Calculate horizontal components of gradient, if needed\n    if return_only_horizontal:\n        gradient_vector = geospatial_gradient(scalar, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim,\n                                              parallel_scale=parallel_scale,\n                                              meridional_scale=meridional_scale,\n                                              return_only=return_only_horizontal.values())\n\n    # Calculate vertical component of gradient, if needed\n    if 'w' in wind_vector:\n        gradient_vector = (*gradient_vector,\n                           first_derivative(scalar, axis=vertical_dim, delta=dz))\n\n    return -sum(\n        wind * gradient\n        for wind, gradient in zip(wind_vector.values(), gradient_vector)\n    )",
  "def frontogenesis(potential_temperature, u, v, dx=None, dy=None, x_dim=-1, y_dim=-2,\n                  *, parallel_scale=None, meridional_scale=None):\n    r\"\"\"Calculate the 2D kinematic frontogenesis of a temperature field.\n\n    The implementation is a form of the Petterssen Frontogenesis and uses the formula\n    outlined in [Bluestein1993]_ pg.248-253\n\n    .. math:: F=\\frac{1}{2}\\left|\\nabla \\theta\\right|[D cos(2\\beta)-\\delta]\n\n    * :math:`F` is 2D kinematic frontogenesis\n    * :math:`\\theta` is potential temperature\n    * :math:`D` is the total deformation\n    * :math:`\\beta` is the angle between the axis of dilatation and the isentropes\n    * :math:`\\delta` is the divergence\n\n    Parameters\n    ----------\n    potential_temperature : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        Potential temperature\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the wind\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the wind\n\n    Returns\n    -------\n    (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        2D Frontogenesis in [temperature units]/m/s\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n    Notes\n    -----\n    To convert from [temperature units]/m/s to [temperature units]/100km/3h, multiply by\n    :math:`1.08e9`\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(thta, u, v, dx, dy, dim_order='yx')``\n\n    \"\"\"\n    # Get gradients of potential temperature in both x and y\n    ddy_theta, ddx_theta = geospatial_gradient(potential_temperature, dx=dx, dy=dy,\n                                               x_dim=x_dim, y_dim=y_dim,\n                                               parallel_scale=parallel_scale,\n                                               meridional_scale=meridional_scale,\n                                               return_only=('df/dy', 'df/dx'))\n\n    # Compute the magnitude of the potential temperature gradient\n    mag_theta = np.sqrt(ddx_theta**2 + ddy_theta**2)\n\n    # Get the shearing, stretching, and total deformation of the wind field\n    shrd = shearing_deformation(u, v, dx, dy, x_dim=x_dim, y_dim=y_dim,\n                                parallel_scale=parallel_scale,\n                                meridional_scale=meridional_scale)\n    strd = stretching_deformation(u, v, dx, dy, x_dim=x_dim, y_dim=y_dim,\n                                  parallel_scale=parallel_scale,\n                                  meridional_scale=meridional_scale)\n    tdef = total_deformation(u, v, dx, dy, x_dim=x_dim, y_dim=y_dim,\n                             parallel_scale=parallel_scale,\n                             meridional_scale=meridional_scale)\n\n    # Get the divergence of the wind field\n    div = divergence(u, v, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim,\n                     parallel_scale=parallel_scale, meridional_scale=meridional_scale)\n\n    # Compute the angle (beta) between the wind field and the gradient of potential temperature\n    psi = 0.5 * np.arctan2(shrd, strd)\n    beta = np.arcsin((-ddx_theta * np.cos(psi) - ddy_theta * np.sin(psi)) / mag_theta)\n\n    return 0.5 * mag_theta * (tdef * np.cos(2 * beta) - div)",
  "def geostrophic_wind(height, dx=None, dy=None, latitude=None, x_dim=-1, y_dim=-2,\n                     *, parallel_scale=None, meridional_scale=None):\n    r\"\"\"Calculate the geostrophic wind given from the height or geopotential.\n\n    Parameters\n    ----------\n    height : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        The height or geopotential field.\n\n    Returns\n    -------\n    A 2-item tuple of arrays\n        A tuple of the u-component and v-component of the geostrophic wind.\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    latitude : `xarray.DataArray` or `pint.Quantity`\n        The latitude, which is used to calculate the Coriolis parameter. Its dimensions must\n        be broadcastable with those of height. Optional if `xarray.DataArray` with latitude\n        coordinate used as input. Note that an argument without units is treated as\n        dimensionless, which is equivalent to radians.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(heights, f, dx, dy)``\n\n    \"\"\"\n    f = coriolis_parameter(latitude)\n    if height.dimensionality['[length]'] == 2.0:\n        norm_factor = 1. / f\n    else:\n        norm_factor = mpconsts.g / f\n\n    dhdx, dhdy = geospatial_gradient(height, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim,\n                                     parallel_scale=parallel_scale,\n                                     meridional_scale=meridional_scale)\n    return -norm_factor * dhdy, norm_factor * dhdx",
  "def ageostrophic_wind(height, u, v, dx=None, dy=None, latitude=None, x_dim=-1, y_dim=-2,\n                      *, parallel_scale=None, meridional_scale=None):\n    r\"\"\"Calculate the ageostrophic wind given from the height or geopotential.\n\n    Parameters\n    ----------\n    height : (M, N) `xarray.DataArray` or `pint.Quantity`\n        The height or geopotential field.\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        The u wind field.\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        The u wind field.\n\n    Returns\n    -------\n    A 2-item tuple of arrays\n        A tuple of the u-component and v-component of the ageostrophic wind\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    latitude : `xarray.DataArray` or `pint.Quantity`\n        The latitude, which is used to calculate the Coriolis parameter. Its dimensions must\n        be broadcastable with those of height. Optional if `xarray.DataArray` with latitude\n        coordinate used as input. Note that an argument without units is treated as\n        dimensionless, which is equivalent to radians.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(heights, f, dx, dy, u, v, dim_order='yx')``\n\n    \"\"\"\n    u_geostrophic, v_geostrophic = geostrophic_wind(\n        height,\n        dx,\n        dy,\n        latitude,\n        x_dim=x_dim,\n        y_dim=y_dim,\n        parallel_scale=parallel_scale,\n        meridional_scale=meridional_scale\n    )\n    return u - u_geostrophic, v - v_geostrophic",
  "def montgomery_streamfunction(height, temperature):\n    r\"\"\"Compute the Montgomery Streamfunction on isentropic surfaces.\n\n    The Montgomery Streamfunction is the streamfunction of the geostrophic wind on an\n    isentropic surface. Its gradient can be interpreted similarly to the pressure gradient in\n    isobaric coordinates.\n\n    Parameters\n    ----------\n    height : `pint.Quantity` or `xarray.DataArray`\n        Array of geopotential height of isentropic surfaces\n    temperature : `pint.Quantity` or `xarray.DataArray`\n        Array of temperature on isentropic surfaces\n\n    Returns\n    -------\n    stream_func : `pint.Quantity` or `xarray.DataArray`\n\n    See Also\n    --------\n    dry_static_energy\n\n    Notes\n    -----\n    The formula used is that from [Lackmann2011]_ p. 69.\n\n    .. math:: \\Psi = gZ + C_pT\n\n    * :math:`\\Psi` is Montgomery Streamfunction\n    * :math:`g` is avg. gravitational acceleration on Earth\n    * :math:`Z` is geopotential height of the isentropic surface\n    * :math:`C_p` is specific heat at constant pressure for dry air\n    * :math:`T` is temperature of the isentropic surface\n\n    \"\"\"\n    from . import dry_static_energy\n    return dry_static_energy(height, temperature)",
  "def storm_relative_helicity(height, u, v, depth, *, bottom=None, storm_u=None, storm_v=None):\n    # Partially adapted from similar SharpPy code\n    r\"\"\"Calculate storm relative helicity.\n\n    Calculates storm relative helicity following [Markowski2010]_ pg.230-231\n\n    .. math:: \\int\\limits_0^d (\\bar v - c) \\cdot \\bar\\omega_{h} \\,dz\n\n    This is applied to the data from a hodograph with the following summation:\n\n    .. math:: \\sum_{n = 1}^{N-1} [(u_{n+1} - c_{x})(v_{n} - c_{y}) -\n                                  (u_{n} - c_{x})(v_{n+1} - c_{y})]\n\n    Parameters\n    ----------\n    u : array-like\n        U component winds\n\n    v : array-like\n        V component winds\n\n    height : array-like\n        Atmospheric height, will be converted to AGL\n\n    depth : float or int\n        Depth of the layer\n\n    bottom : float or int\n        Height of layer bottom AGL (default is surface)\n\n    storm_u : float or int\n        U component of storm motion (default is 0 m/s)\n\n    storm_v : float or int\n        V component of storm motion (default is 0 m/s)\n\n    Returns\n    -------\n    `pint.Quantity`\n        Positive storm-relative helicity\n\n    `pint.Quantity`\n        Negative storm-relative helicity\n\n    `pint.Quantity`\n        Total storm-relative helicity\n\n    Examples\n    --------\n    >>> from metpy.calc import storm_relative_helicity, wind_components\n    >>> from metpy.units import units\n    >>> # set needed values of pressure, height, wind direction/speed\n    >>> p = [1000, 925, 850, 700, 500, 400] * units.hPa\n    >>> h = [250, 700, 1500, 3100, 5720, 7120] * units.meters\n    >>> wdir = [165, 180, 190, 210, 220, 250] * units.degree\n    >>> sped = [5, 15, 20, 30, 50, 60] * units.knots\n    >>> # compute wind components\n    >>> u, v = wind_components(sped, wdir)\n    >>> # compute SRH with a storm vector\n    >>> storm_relative_helicity(h, u, v, depth=1 * units.km,\n    ...                         storm_u=7 * units('m/s'), storm_v=7 * units('m/s'))\n    (<Quantity(49.6086162, 'meter ** 2 / second ** 2')>,\n    <Quantity(0.0, 'meter ** 2 / second ** 2')>,\n    <Quantity(49.6086162, 'meter ** 2 / second ** 2')>)\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``heights`` parameter to ``height`` and converted ``bottom``, ``storm_u``, and\n       ``storm_v`` parameters to keyword-only arguments\n\n    \"\"\"\n    if bottom is None:\n        bottom = units.Quantity(0, 'm')\n    if storm_u is None:\n        storm_u = units.Quantity(0, 'm/s')\n    if storm_v is None:\n        storm_v = units.Quantity(0, 'm/s')\n\n    _, u, v = get_layer_heights(height, depth, u, v, with_agl=True, bottom=bottom)\n\n    storm_relative_u = u - storm_u\n    storm_relative_v = v - storm_v\n\n    int_layers = (storm_relative_u[1:] * storm_relative_v[:-1]\n                  - storm_relative_u[:-1] * storm_relative_v[1:])\n\n    # Need to manually check for masked value because sum() on masked array with non-default\n    # mask will return a masked value rather than 0. See numpy/numpy#11736\n    positive_srh = int_layers[int_layers.magnitude > 0.].sum()\n    if np.ma.is_masked(positive_srh):\n        positive_srh = units.Quantity(0.0, 'meter**2 / second**2')\n    negative_srh = int_layers[int_layers.magnitude < 0.].sum()\n    if np.ma.is_masked(negative_srh):\n        negative_srh = units.Quantity(0.0, 'meter**2 / second**2')\n\n    return (positive_srh.to('meter ** 2 / second ** 2'),\n            negative_srh.to('meter ** 2 / second ** 2'),\n            (positive_srh + negative_srh).to('meter ** 2 / second ** 2'))",
  "def absolute_vorticity(u, v, dx=None, dy=None, latitude=None, x_dim=-1, y_dim=-2, *,\n                       parallel_scale=None, meridional_scale=None):\n    \"\"\"Calculate the absolute vorticity of the horizontal wind.\n\n    Parameters\n    ----------\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the wind\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the wind\n\n    Returns\n    -------\n    (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        absolute vorticity\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    latitude : `pint.Quantity`, optional\n        Latitude of the wind data. Optional if `xarray.DataArray` with latitude/longitude\n        coordinates used as input. Note that an argument without units is treated as\n        dimensionless, which translates to radians.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n    See Also\n    --------\n    vorticity, coriolis_parameter\n\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(u, v, dx, dy, lats, dim_order='yx')``\n\n    \"\"\"\n    f = coriolis_parameter(latitude)\n    relative_vorticity = vorticity(u, v, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim,\n                                   parallel_scale=parallel_scale,\n                                   meridional_scale=meridional_scale)\n    return relative_vorticity + f",
  "def potential_vorticity_baroclinic(\n    potential_temperature,\n    pressure,\n    u,\n    v,\n    dx=None,\n    dy=None,\n    latitude=None,\n    x_dim=-1,\n    y_dim=-2,\n    vertical_dim=-3,\n    *,\n    parallel_scale=None,\n    meridional_scale=None\n):\n    r\"\"\"Calculate the baroclinic potential vorticity.\n\n    .. math:: PV = -g \\left(\\frac{\\partial u}{\\partial p}\\frac{\\partial \\theta}{\\partial y}\n              - \\frac{\\partial v}{\\partial p}\\frac{\\partial \\theta}{\\partial x}\n              + \\frac{\\partial \\theta}{\\partial p}(\\zeta + f) \\right)\n\n    This formula is based on equation 4.5.93 [Bluestein1993]_\n\n    Parameters\n    ----------\n    potential_temperature : (..., P, M, N) `xarray.DataArray` or `pint.Quantity`\n        potential temperature\n    pressure : (..., P, M, N) `xarray.DataArray` or `pint.Quantity`\n        vertical pressures\n    u : (..., P, M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the wind\n    v : (..., P, M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the wind\n\n    Returns\n    -------\n    (..., P, M, N) `xarray.DataArray` or `pint.Quantity`\n        baroclinic potential vorticity\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    latitude : `pint.Quantity`, optional\n        Latitude of the wind data. Optional if `xarray.DataArray` with latitude/longitude\n        coordinates used as input. Note that an argument without units is treated as\n        dimensionless, which translates to radians.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Z, Y, X] order).\n        Automatically parsed from input if using `xarray.DataArray`.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Z, Y, X] order).\n        Automatically parsed from input if using `xarray.DataArray`.\n    vertical_dim : int, optional\n        Axis number of vertical dimension. Defaults to -3 (implying [..., Z, Y, X] order).\n        Automatically parsed from input if using `xarray.DataArray`.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n    Notes\n    -----\n    The same function can be used for isobaric and isentropic PV analysis. Provide winds\n    for vorticity calculations on the desired isobaric or isentropic surface. At least three\n    layers of pressure/potential temperature are required in order to calculate the vertical\n    derivative (one above and below the desired surface). The first two terms will be zero\n    if isentropic level data is used. This is because the gradient of theta in both the x\n    and y-directions is zero when you are on an isentropic surface.\n\n    This function expects pressure/isentropic level to increase with increasing array element\n    (e.g., from higher in the atmosphere to closer to the surface. If the pressure array is\n    one-dimensional, and not given as `xarray.DataArray`, p[:, None, None] can be used to make\n    it appear multi-dimensional.)\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(potential_temperature, pressure, u, v, dx, dy, lats)``\n\n    \"\"\"\n    if (\n        np.shape(potential_temperature)[vertical_dim] < 3\n        or np.shape(pressure)[vertical_dim] < 3\n        or np.shape(potential_temperature)[vertical_dim] != np.shape(pressure)[vertical_dim]\n    ):\n        raise ValueError('Length of potential temperature along the vertical axis '\n                         f'{vertical_dim} must be at least 3.')\n\n    avor = absolute_vorticity(u, v, dx, dy, latitude, x_dim=x_dim, y_dim=y_dim,\n                              parallel_scale=parallel_scale, meridional_scale=meridional_scale)\n    dthetadp = first_derivative(potential_temperature, x=pressure, axis=vertical_dim)\n\n    if (\n        (np.shape(potential_temperature)[y_dim] == 1)\n        and (np.shape(potential_temperature)[x_dim] == 1)\n    ):\n        dthetady = units.Quantity(0, 'K/m')  # axis=y_dim only has one dimension\n        dthetadx = units.Quantity(0, 'K/m')  # axis=x_dim only has one dimension\n    else:\n        dthetadx, dthetady = geospatial_gradient(potential_temperature, dx=dx, dy=dy,\n                                                 x_dim=x_dim, y_dim=y_dim,\n                                                 parallel_scale=parallel_scale,\n                                                 meridional_scale=meridional_scale)\n    dudp = first_derivative(u, x=pressure, axis=vertical_dim)\n    dvdp = first_derivative(v, x=pressure, axis=vertical_dim)\n\n    return (-mpconsts.g * (dudp * dthetady - dvdp * dthetadx\n                           + avor * dthetadp)).to('K * m**2 / (s * kg)')",
  "def potential_vorticity_barotropic(\n    height,\n    u,\n    v,\n    dx=None,\n    dy=None,\n    latitude=None,\n    x_dim=-1,\n    y_dim=-2,\n    *,\n    parallel_scale=None,\n    meridional_scale=None\n):\n    r\"\"\"Calculate the barotropic (Rossby) potential vorticity.\n\n    .. math:: PV = \\frac{f + \\zeta}{H}\n\n    This formula is based on equation 7.27 [Hobbs2006]_.\n\n    Parameters\n    ----------\n    height : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        atmospheric height\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the wind\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the wind\n\n    Returns\n    -------\n    (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        barotropic potential vorticity\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    latitude : `pint.Quantity`, optional\n        Latitude of the wind data. Optional if `xarray.DataArray` with latitude/longitude\n        coordinates used as input. Note that an argument without units is treated as\n        dimensionless, which translates to radians.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(heights, u, v, dx, dy, lats, dim_order='yx')``\n\n    \"\"\"\n    avor = absolute_vorticity(u, v, dx, dy, latitude, x_dim=x_dim, y_dim=y_dim,\n                              parallel_scale=parallel_scale, meridional_scale=meridional_scale)\n    return (avor / height).to('meter**-1 * second**-1')",
  "def inertial_advective_wind(\n    u,\n    v,\n    u_geostrophic,\n    v_geostrophic,\n    dx=None,\n    dy=None,\n    latitude=None,\n    x_dim=-1,\n    y_dim=-2,\n    *,\n    parallel_scale=None,\n    meridional_scale=None\n):\n    r\"\"\"Calculate the inertial advective wind.\n\n    .. math:: \\frac{\\hat k}{f} \\times (\\vec V \\cdot \\nabla)\\hat V_g\n\n    .. math:: \\frac{\\hat k}{f} \\times \\left[ \\left( u \\frac{\\partial u_g}{\\partial x} + v\n              \\frac{\\partial u_g}{\\partial y} \\right) \\hat i + \\left( u \\frac{\\partial v_g}\n              {\\partial x} + v \\frac{\\partial v_g}{\\partial y} \\right) \\hat j \\right]\n\n    .. math:: \\left[ -\\frac{1}{f}\\left(u \\frac{\\partial v_g}{\\partial x} + v\n              \\frac{\\partial v_g}{\\partial y} \\right) \\right] \\hat i + \\left[ \\frac{1}{f}\n              \\left( u \\frac{\\partial u_g}{\\partial x} + v \\frac{\\partial u_g}{\\partial y}\n              \\right) \\right] \\hat j\n\n    This formula is based on equation 27 of [Rochette2006]_.\n\n    Parameters\n    ----------\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the advecting wind\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the advecting wind\n    u_geostrophic : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the geostrophic (advected) wind\n    v_geostrophic : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the geostrophic (advected) wind\n\n    Returns\n    -------\n    (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of inertial advective wind\n    (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of inertial advective wind\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    latitude : `pint.Quantity`, optional\n        Latitude of the wind data. Optional if `xarray.DataArray` with latitude/longitude\n        coordinates used as input. Note that an argument without units is treated as\n        dimensionless, which translates to radians.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n    Notes\n    -----\n    Many forms of the inertial advective wind assume the advecting and advected\n    wind to both be the geostrophic wind. To do so, pass the x and y components\n    of the geostrophic wind for u and u_geostrophic/v and v_geostrophic.\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(u, v, u_geostrophic, v_geostrophic, dx, dy, lats)``\n\n    \"\"\"\n    f = coriolis_parameter(latitude)\n\n    dugdx, dugdy = geospatial_gradient(u_geostrophic, dx=dx, dy=dy,\n                                       x_dim=x_dim, y_dim=y_dim,\n                                       parallel_scale=parallel_scale,\n                                       meridional_scale=meridional_scale)\n    dvgdx, dvgdy = geospatial_gradient(v_geostrophic, dx=dx, dy=dy,\n                                       x_dim=x_dim, y_dim=y_dim,\n                                       parallel_scale=parallel_scale,\n                                       meridional_scale=meridional_scale)\n\n    u_component = -(u * dvgdx + v * dvgdy) / f\n    v_component = (u * dugdx + v * dugdy) / f\n\n    return u_component, v_component",
  "def q_vector(\n    u,\n    v,\n    temperature,\n    pressure,\n    dx=None,\n    dy=None,\n    static_stability=1,\n    x_dim=-1,\n    y_dim=-2,\n    *,\n    parallel_scale=None,\n    meridional_scale=None\n):\n    r\"\"\"Calculate Q-vector at a given pressure level using the u, v winds and temperature.\n\n    .. math:: \\vec{Q} = (Q_1, Q_2)\n                      =  - \\frac{R}{\\sigma p}\\left(\n                               \\frac{\\partial \\vec{v}_g}{\\partial x} \\cdot \\nabla_p T,\n                               \\frac{\\partial \\vec{v}_g}{\\partial y} \\cdot \\nabla_p T\n                           \\right)\n\n    This formula follows equation 5.7.55 from [Bluestein1992]_, and can be used with the\n    the below form of the quasigeostrophic omega equation to assess vertical motion\n    ([Bluestein1992]_ equation 5.7.54):\n\n    .. math:: \\left( \\nabla_p^2 + \\frac{f_0^2}{\\sigma} \\frac{\\partial^2}{\\partial p^2}\n                  \\right) \\omega =\n              - 2 \\nabla_p \\cdot \\vec{Q} -\n                  \\frac{R}{\\sigma p} \\beta \\frac{\\partial T}{\\partial x}\n\n    Parameters\n    ----------\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the wind (geostrophic in QG-theory)\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the wind (geostrophic in QG-theory)\n    temperature : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        Array of temperature at pressure level\n    pressure : `pint.Quantity`\n        Pressure at level\n\n    Returns\n    -------\n    tuple of (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        The components of the Q-vector in the u- and v-directions respectively\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input.\n    static_stability : `pint.Quantity`, optional\n        The static stability at the pressure level. Defaults to 1 if not given to calculate\n        the Q-vector without factoring in static stability.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(u, v, temperature, pressure, dx, dy, static_stability=1)``\n\n    See Also\n    --------\n    static_stability\n\n    \"\"\"\n    dudx, dudy, dvdx, dvdy = vector_derivative(\n        u, v, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim,\n        parallel_scale=parallel_scale, meridional_scale=meridional_scale)\n\n    dtempdx, dtempdy = geospatial_gradient(\n        temperature, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim,\n        parallel_scale=parallel_scale, meridional_scale=meridional_scale)\n\n    q1 = -mpconsts.Rd / (pressure * static_stability) * (dudx * dtempdx + dvdx * dtempdy)\n    q2 = -mpconsts.Rd / (pressure * static_stability) * (dudy * dtempdx + dvdy * dtempdy)\n\n    return q1.to_base_units(), q2.to_base_units()",
  "def geospatial_laplacian(f, *, dx=None, dy=None, x_dim=-1, y_dim=-2,\n                         parallel_scale=None, meridional_scale=None):\n    r\"\"\"Calculate the projection-correct laplacian of a 2D scalar field.\n\n    Parameters\n    ----------\n    f : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        scalar field for which the horizontal gradient should be calculated\n    return_only : str or Sequence[str], optional\n        Sequence of which components of the gradient to compute and return. If none,\n        returns the gradient tuple ('df/dx', 'df/dy'). Otherwise, matches the return\n        pattern of the given strings. Only valid strings are 'df/dx', 'df/dy'.\n\n    Returns\n    -------\n    `pint.Quantity`, tuple of `pint.Quantity`, or tuple of pairs of `pint.Quantity`\n        Component(s) of vector derivative\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input. Also optional if one-dimensional\n        longitude and latitude arguments are given for your data on a non-projected grid.\n        Keyword-only argument.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input. Also optional if one-dimensional\n        longitude and latitude arguments are given for your data on a non-projected grid.\n        Keyword-only argument.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`. Keyword-only argument.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`. Keyword-only argument.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n    See Also\n    --------\n    vector_derivative, geospatial_gradient, laplacian\n\n    \"\"\"\n    grad_u, grad_y = geospatial_gradient(f, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim,\n                                         parallel_scale=parallel_scale,\n                                         meridional_scale=meridional_scale)\n    return divergence(grad_u, grad_y, dx=dx, dy=dy, x_dim=x_dim, y_dim=y_dim,\n                      parallel_scale=parallel_scale, meridional_scale=meridional_scale)",
  "def get_perturbation(ts, axis=-1):\n    r\"\"\"Compute the perturbation from the mean of a time series.\n\n    Parameters\n    ----------\n    ts : array-like\n         The time series from which you wish to find the perturbation\n         time series (perturbation from the mean).\n\n    Returns\n    -------\n    array-like\n        The perturbation time series.\n\n    Other Parameters\n    ----------------\n    axis : int\n           The index of the time axis. Default is -1\n\n    Notes\n    -----\n    The perturbation time series produced by this function is defined as\n    the perturbations about the mean:\n\n    .. math:: x(t)^{\\prime} = x(t) - \\overline{x(t)}\n\n    \"\"\"\n    mean = ts.mean(axis=axis)[make_take(ts.ndim, axis)(None)]\n    return ts - mean",
  "def tke(u, v, w, perturbation=False, axis=-1):\n    r\"\"\"Compute turbulence kinetic energy.\n\n    Compute the turbulence kinetic energy (e) from the time series of the\n    velocity components.\n\n    Parameters\n    ----------\n    u : array-like\n        The wind component along the x-axis\n    v : array-like\n        The wind component along the y-axis\n    w : array-like\n        The wind component along the z-axis\n    perturbation : bool, optional\n                   True if the `u`, `v`, and `w` components of wind speed\n                   supplied to the function are perturbation velocities.\n                   If False, perturbation velocities will be calculated by\n                   removing the mean value from each component.\n\n    Returns\n    -------\n    array-like\n        The corresponding turbulence kinetic energy value\n\n    Other Parameters\n    ----------------\n    axis : int\n           The index of the time axis. Default is -1\n\n    See Also\n    --------\n    get_perturbation : Used to compute perturbations if `perturbation`\n                       is False.\n\n    Notes\n    -----\n    Turbulence Kinetic Energy is computed as:\n\n    .. math:: e = 0.5 \\sqrt{\\overline{u^{\\prime2}} +\n                            \\overline{v^{\\prime2}} +\n                            \\overline{w^{\\prime2}}},\n\n    where the velocity components\n\n    .. math:: u^{\\prime}, v^{\\prime}, u^{\\prime}\n\n    are perturbation velocities. For more information on the subject, please\n    see [Garratt1994]_.\n\n    \"\"\"\n    if not perturbation:\n        u = get_perturbation(u, axis=axis)\n        v = get_perturbation(v, axis=axis)\n        w = get_perturbation(w, axis=axis)\n\n    u_cont = np.mean(u**2, axis=axis)\n    v_cont = np.mean(v**2, axis=axis)\n    w_cont = np.mean(w**2, axis=axis)\n\n    return 0.5 * np.sqrt(u_cont + v_cont + w_cont)",
  "def kinematic_flux(vel, b, perturbation=False, axis=-1):\n    r\"\"\"Compute the kinematic flux from two time series.\n\n    Compute the kinematic flux from the time series of two variables `vel`\n    and b. Note that to be a kinematic flux, at least one variable must be\n    a component of velocity.\n\n    Parameters\n    ----------\n    vel : array-like\n        A component of velocity\n\n    b : array-like\n        May be a component of velocity or a scalar variable (e.g. Temperature)\n\n    perturbation : bool, optional\n        `True` if the `vel` and `b` variables are perturbations. If `False`, perturbations\n        will be calculated by removing the mean value from each variable. Defaults to `False`.\n\n    Returns\n    -------\n    array-like\n        The corresponding kinematic flux\n\n    Other Parameters\n    ----------------\n    axis : int, optional\n           The index of the time axis, along which the calculations will be\n           performed. Defaults to -1\n\n    Notes\n    -----\n    A kinematic flux is computed as\n\n    .. math:: \\overline{u^{\\prime} s^{\\prime}}\n\n    where at the prime notation denotes perturbation variables, and at least\n    one variable is perturbation velocity. For example, the vertical kinematic\n    momentum flux (two velocity components):\n\n    .. math:: \\overline{u^{\\prime} w^{\\prime}}\n\n    or the vertical kinematic heat flux (one velocity component, and one\n    scalar):\n\n    .. math:: \\overline{w^{\\prime} T^{\\prime}}\n\n    If perturbation variables are passed into this function (i.e.\n    `perturbation` is True), the kinematic flux is computed using the equation\n    above.\n\n    However, the equation above can be rewritten as\n\n    .. math:: \\overline{us} - \\overline{u}~\\overline{s}\n\n    which is computationally more efficient. This is how the kinematic flux\n    is computed in this function if `perturbation` is False.\n\n    For more information on the subject, please see [Garratt1994]_.\n\n    \"\"\"\n    kf = np.mean(vel * b, axis=axis)\n    if not perturbation:\n        kf -= np.mean(vel, axis=axis) * np.mean(b, axis=axis)\n    return np.atleast_1d(kf)",
  "def friction_velocity(u, w, v=None, perturbation=False, axis=-1):\n    r\"\"\"Compute the friction velocity from the time series of velocity components.\n\n    Compute the friction velocity from the time series of the x, z,\n    and optionally y, velocity components.\n\n    Parameters\n    ----------\n    u : array-like\n        The wind component along the x-axis\n    w : array-like\n        The wind component along the z-axis\n    v : array-like, optional\n        The wind component along the y-axis.\n\n    perturbation : bool, optional\n                   True if the `u`, `w`, and `v` components of wind speed\n                   supplied to the function are perturbation velocities.\n                   If False, perturbation velocities will be calculated by\n                   removing the mean value from each component.\n\n    Returns\n    -------\n    array-like\n        The corresponding friction velocity\n\n    Other Parameters\n    ----------------\n    axis : int\n           The index of the time axis. Default is -1\n\n    See Also\n    --------\n    kinematic_flux : Used to compute the x-component and y-component\n                     vertical kinematic momentum flux(es) used in the\n                     computation of the friction velocity.\n\n    Notes\n    -----\n    The Friction Velocity is computed as:\n\n    .. math:: u_{*} = \\sqrt[4]{\\left(\\overline{u^{\\prime}w^{\\prime}}\\right)^2 +\n                               \\left(\\overline{v^{\\prime}w^{\\prime}}\\right)^2},\n\n    where :math: \\overline{u^{\\prime}w^{\\prime}} and\n    :math: \\overline{v^{\\prime}w^{\\prime}}\n    are the x-component and y-components of the vertical kinematic momentum\n    flux, respectively. If the optional v component of velocity is not\n    supplied to the function, the computation of the friction velocity is\n    reduced to\n\n    .. math:: u_{*} = \\sqrt[4]{\\left(\\overline{u^{\\prime}w^{\\prime}}\\right)^2}\n\n    For more information on the subject, please see [Garratt1994]_.\n\n    \"\"\"\n    uw = kinematic_flux(u, w, perturbation=perturbation, axis=axis)\n    kf = uw**2\n    if v is not None:\n        vw = kinematic_flux(v, w, perturbation=perturbation, axis=axis)\n        kf += vw**2\n    # the friction velocity is the 4th root of the kinematic momentum flux\n    # As an optimization, first do inplace square root, then return the\n    # square root of that. This is faster than np.power(..., 0.25)\n    np.sqrt(kf, out=kf)\n    return np.sqrt(kf)",
  "def resample_nn_1d(a, centers):\n    \"\"\"Return one-dimensional nearest-neighbor indexes based on user-specified centers.\n\n    Parameters\n    ----------\n    a : array-like\n        1-dimensional array of numeric values from which to extract indexes of\n        nearest-neighbors\n    centers : array-like\n        1-dimensional array of numeric values representing a subset of values to approximate\n\n    Returns\n    -------\n        A list of indexes (in type given by `array.argmin()`) representing values closest to\n        given array values.\n\n    \"\"\"\n    ix = []\n    for center in centers:\n        index = (np.abs(a - center)).argmin()\n        if index not in ix:\n            ix.append(index)\n    return ix",
  "def nearest_intersection_idx(a, b):\n    \"\"\"Determine the index of the point just before two lines with common x values.\n\n    Parameters\n    ----------\n    a : array-like\n        1-dimensional array of y-values for line 1\n    b : array-like\n        1-dimensional array of y-values for line 2\n\n    Returns\n    -------\n        An array of indexes representing the index of the values\n        just before the intersection(s) of the two lines.\n\n    \"\"\"\n    # Difference in the two y-value sets\n    difference = a - b\n\n    # Determine the point just before the intersection of the lines\n    # Will return multiple points for multiple intersections\n    sign_change_idx, = np.nonzero(np.diff(np.sign(difference)))\n\n    return sign_change_idx",
  "def find_intersections(x, a, b, direction='all', log_x=False):\n    \"\"\"Calculate the best estimate of intersection.\n\n    Calculates the best estimates of the intersection of two y-value\n    data sets that share a common x-value set.\n\n    Parameters\n    ----------\n    x : array-like\n        1-dimensional array of numeric x-values\n    a : array-like\n        1-dimensional array of y-values for line 1\n    b : array-like\n        1-dimensional array of y-values for line 2\n    direction : str, optional\n        specifies direction of crossing. 'all', 'increasing' (a becoming greater than b),\n        or 'decreasing' (b becoming greater than a). Defaults to 'all'.\n    log_x : bool, optional\n        Use logarithmic interpolation along the `x` axis (i.e. for finding intersections\n        in pressure coordinates). Default is False.\n\n    Returns\n    -------\n        A tuple (x, y) of array-like with the x and y coordinates of the\n        intersections of the lines.\n\n    Notes\n    -----\n    This function implicitly converts `xarray.DataArray` to `pint.Quantity`, with the results\n    given as `pint.Quantity`.\n\n    \"\"\"\n    # Change x to logarithmic if log_x=True\n    if log_x is True:\n        x = np.log(x)\n\n    # Find the index of the points just before the intersection(s)\n    nearest_idx = nearest_intersection_idx(a, b)\n    next_idx = nearest_idx + 1\n\n    # Determine the sign of the change\n    sign_change = np.sign(a[next_idx] - b[next_idx])\n\n    # x-values around each intersection\n    _, x0 = _next_non_masked_element(x, nearest_idx)\n    _, x1 = _next_non_masked_element(x, next_idx)\n\n    # y-values around each intersection for the first line\n    _, a0 = _next_non_masked_element(a, nearest_idx)\n    _, a1 = _next_non_masked_element(a, next_idx)\n\n    # y-values around each intersection for the second line\n    _, b0 = _next_non_masked_element(b, nearest_idx)\n    _, b1 = _next_non_masked_element(b, next_idx)\n\n    # Calculate the x-intersection. This comes from finding the equations of the two lines,\n    # one through (x0, a0) and (x1, a1) and the other through (x0, b0) and (x1, b1),\n    # finding their intersection, and reducing with a bunch of algebra.\n    delta_y0 = a0 - b0\n    delta_y1 = a1 - b1\n    intersect_x = (delta_y1 * x0 - delta_y0 * x1) / (delta_y1 - delta_y0)\n\n    # Calculate the y-intersection of the lines. Just plug the x above into the equation\n    # for the line through the a points. One could solve for y like x above, but this\n    # causes weirder unit behavior and seems a little less good numerically.\n    intersect_y = ((intersect_x - x0) / (x1 - x0)) * (a1 - a0) + a0\n\n    # If there's no intersections, return\n    if len(intersect_x) == 0:\n        return intersect_x, intersect_y\n\n    # Return x to linear if log_x is True\n    if log_x is True:\n        intersect_x = np.exp(intersect_x)\n\n    # Check for duplicates\n    duplicate_mask = (np.ediff1d(intersect_x, to_end=1) != 0)\n\n    # Make a mask based on the direction of sign change desired\n    if direction == 'increasing':\n        mask = sign_change > 0\n    elif direction == 'decreasing':\n        mask = sign_change < 0\n    elif direction == 'all':\n        return intersect_x[duplicate_mask], intersect_y[duplicate_mask]\n    else:\n        raise ValueError(f'Unknown option for direction: {direction}')\n\n    return intersect_x[mask & duplicate_mask], intersect_y[mask & duplicate_mask]",
  "def _next_non_masked_element(a, idx):\n    \"\"\"Return the next non masked element of a masked array.\n\n    If an array is masked, return the next non-masked element (if the given index is masked).\n    If no other unmasked points are after the given masked point, returns none.\n\n    Parameters\n    ----------\n    a : array-like\n        1-dimensional array of numeric values\n    idx : integer\n        Index of requested element\n\n    Returns\n    -------\n        Index of next non-masked element and next non-masked element\n\n    \"\"\"\n    try:\n        next_idx = idx + a[idx:].mask.argmin()\n        if ma.is_masked(a[next_idx]):\n            return None, None\n        else:\n            return next_idx, a[next_idx]\n    except (AttributeError, TypeError, IndexError):\n        return idx, a[idx]",
  "def _delete_masked_points(*arrs):\n    \"\"\"Delete masked points from arrays.\n\n    Takes arrays and removes masked points to help with calculations and plotting.\n\n    Parameters\n    ----------\n    arrs : one or more array-like\n        Source arrays\n\n    Returns\n    -------\n    arrs : one or more array-like\n        Arrays with masked elements removed\n\n    \"\"\"\n    if any(hasattr(a, 'mask') for a in arrs):\n        keep = ~functools.reduce(np.logical_or, (np.ma.getmaskarray(a) for a in arrs))\n        return tuple(a[keep] for a in arrs)\n    else:\n        return arrs",
  "def reduce_point_density(points, radius, priority=None):\n    r\"\"\"Return a mask to reduce the density of points in irregularly-spaced data.\n\n    This function is used to down-sample a collection of scattered points (e.g. surface\n    data), returning a mask that can be used to select the points from one or more arrays\n    (e.g. arrays of temperature and dew point). The points selected can be controlled by\n    providing an array of ``priority`` values (e.g. rainfall totals to ensure that\n    stations with higher precipitation remain in the mask). The points and radius can be\n    specified with units. If none are provided, meters are assumed.\n\n    Points with at least one non-finite (i.e. NaN or Inf) value are ignored and returned with\n    a value of ``False`` (meaning don't keep).\n\n    Parameters\n    ----------\n    points : (N, M) array-like\n        N locations of the points in M dimensional space\n    radius : `pint.Quantity` or float\n        Minimum radius allowed between points. If units are not provided, meters is assumed.\n    priority : (N, M) array-like, optional\n        If given, this should have the same shape as ``points``; these values will\n        be used to control selection priority for points.\n\n    Returns\n    -------\n        (N,) array-like of boolean values indicating whether points should be kept. This\n        can be used directly to index numpy arrays to return only the desired points.\n\n    Examples\n    --------\n    >>> metpy.calc.reduce_point_density(np.array([1, 2, 3]), 1.)\n    array([ True, False, True])\n    >>> metpy.calc.reduce_point_density(np.array([1, 2, 3]), 1.,\n    ... priority=np.array([0.1, 0.9, 0.3]))\n    array([False, True, False])\n\n    \"\"\"\n    # Handle input with units. Assume meters if units are not specified\n    if hasattr(radius, 'units'):\n        radius = radius.to('m').m\n\n    if hasattr(points, 'units'):\n        points = points.to('m').m\n\n    # Handle 1D input\n    if points.ndim < 2:\n        points = points.reshape(-1, 1)\n\n    # Identify good points--finite values (e.g. not NaN or inf). Set bad 0, but we're going to\n    # ignore anyway. It's easier for managing indices to keep the original points in the group.\n    good_vals = np.isfinite(points)\n    points = np.where(good_vals, points, 0)\n\n    # Make a kd-tree to speed searching of data.\n    tree = cKDTree(points)\n\n    # Need to use sorted indices rather than sorting the position\n    # so that the keep mask matches *original* order.\n    if priority is not None:\n        # Need to sort the locations in decreasing priority.\n        sorted_indices = np.argsort(priority)[::-1]\n    else:\n        # Take advantage of iterator nature of range here to avoid making big lists\n        sorted_indices = range(len(points))\n\n    # Keep all good points initially\n    keep = np.logical_and.reduce(good_vals, axis=-1)\n\n    # Loop over all the potential points\n    for ind in sorted_indices:\n        # Only proceed if we haven't already excluded this point\n        if keep[ind]:\n            # Find the neighbors and eliminate them\n            neighbors = tree.query_ball_point(points[ind], radius)\n            keep[neighbors] = False\n\n            # We just removed ourselves, so undo that\n            keep[ind] = True\n\n    return keep",
  "def _get_bound_pressure_height(pressure, bound, height=None, interpolate=True):\n    \"\"\"Calculate the bounding pressure and height in a layer.\n\n    Given pressure, optional heights and a bound, return either the closest pressure/height\n    or interpolated pressure/height. If no heights are provided, a standard atmosphere\n    ([NOAA1976]_) is assumed.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressures\n    bound : `pint.Quantity`\n        Bound to retrieve (in pressure or height)\n    height : `pint.Quantity`, optional\n        Atmospheric heights associated with the pressure levels. Defaults to using\n        heights calculated from ``pressure`` assuming a standard atmosphere.\n    interpolate : boolean, optional\n        Interpolate the bound or return the nearest. Defaults to True.\n\n    Returns\n    -------\n    `pint.Quantity`\n        The bound pressure and height\n\n    \"\"\"\n    # avoid circular import if basic.py ever imports something from tools.py\n    from .basic import height_to_pressure_std, pressure_to_height_std\n\n    # Make sure pressure is monotonically decreasing\n    sort_inds = np.argsort(pressure)[::-1]\n    pressure = pressure[sort_inds]\n    if height is not None:\n        height = height[sort_inds]\n\n    # Bound is given in pressure\n    if bound.check('[length]**-1 * [mass] * [time]**-2'):\n        # If the bound is in the pressure data, we know the pressure bound exactly\n        if bound in pressure:\n            # By making sure this is at least a 1D array we avoid the behavior in numpy\n            # (at least up to 1.19.4) that float32 scalar * Python float -> float64, which\n            # can wreak havok with floating point comparisons.\n            bound_pressure = np.atleast_1d(bound)\n            # If we have heights, we know the exact height value, otherwise return standard\n            # atmosphere height for the pressure\n            if height is not None:\n                bound_height = height[pressure == bound_pressure]\n            else:\n                bound_height = pressure_to_height_std(bound_pressure)\n        # If bound is not in the data, return the nearest or interpolated values\n        else:\n            if interpolate:\n                bound_pressure = bound  # Use the user specified bound\n                if height is not None:  # Interpolate heights from the height data\n                    bound_height = log_interpolate_1d(bound_pressure, pressure, height)\n                else:  # If not heights given, use the standard atmosphere\n                    bound_height = pressure_to_height_std(bound_pressure)\n            else:  # No interpolation, find the closest values\n                idx = (np.abs(pressure - bound)).argmin()\n                bound_pressure = pressure[idx]\n                if height is not None:\n                    bound_height = height[idx]\n                else:\n                    bound_height = pressure_to_height_std(bound_pressure)\n\n    # Bound is given in height\n    elif bound.check('[length]'):\n        # If there is height data, see if we have the bound or need to interpolate/find nearest\n        if height is not None:\n            if bound in height:  # Bound is in the height data\n                bound_height = bound\n                bound_pressure = pressure[height == bound]\n            else:  # Bound is not in the data\n                if interpolate:\n                    bound_height = bound\n\n                    # Need to cast back to the input type since interp (up to at least numpy\n                    # 1.13 always returns float64. This can cause upstream users problems,\n                    # resulting in something like np.append() to upcast.\n                    bound_pressure = np.interp(np.atleast_1d(bound),\n                                               height, pressure).astype(np.result_type(bound))\n                else:\n                    idx = (np.abs(height - bound)).argmin()\n                    bound_pressure = pressure[idx]\n                    bound_height = height[idx]\n        else:  # Don't have heights, so assume a standard atmosphere\n            bound_height = bound\n            bound_pressure = height_to_pressure_std(bound)\n            # If interpolation is on, this is all we need, if not, we need to go back and\n            # find the pressure closest to this and refigure the bounds\n            if not interpolate:\n                idx = (np.abs(pressure - bound_pressure)).argmin()\n                bound_pressure = pressure[idx]\n                bound_height = pressure_to_height_std(bound_pressure)\n\n    # Bound has invalid units\n    else:\n        raise ValueError('Bound must be specified in units of length or pressure.')\n\n    # If the bound is out of the range of the data, we shouldn't extrapolate\n    if not (_greater_or_close(bound_pressure, np.nanmin(pressure))\n            and _less_or_close(bound_pressure, np.nanmax(pressure))):\n        raise ValueError('Specified bound is outside pressure range.')\n    if height is not None and not (_less_or_close(bound_height, np.nanmax(height))\n                                   and _greater_or_close(bound_height, np.nanmin(height))):\n        raise ValueError('Specified bound is outside height range.')\n\n    return bound_pressure, bound_height",
  "def get_layer_heights(height, depth, *args, bottom=None, interpolate=True, with_agl=False):\n    \"\"\"Return an atmospheric layer from upper air data with the requested bottom and depth.\n\n    This function will subset an upper air dataset to contain only the specified layer using\n    the height only.\n\n    Parameters\n    ----------\n    height : array-like\n        Atmospheric height\n    depth : `pint.Quantity`\n        Thickness of the layer\n    args : array-like\n        Atmospheric variable(s) measured at the given pressures\n    bottom : `pint.Quantity`, optional\n        Bottom of the layer\n    interpolate : bool, optional\n        Interpolate the top and bottom points if they are not in the given data. Defaults\n        to True.\n    with_agl : bool, optional\n        Returns the height as above ground level by subtracting the minimum height in the\n        provided height. Defaults to False.\n\n    Returns\n    -------\n    `pint.Quantity, pint.Quantity`\n        Height and data variables of the layer\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Also, this will return Pint Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``heights`` parameter to ``height``\n\n    \"\"\"\n    # Make sure pressure and datavars are the same length\n    for datavar in args:\n        if len(height) != len(datavar):\n            raise ValueError('Height and data variables must have the same length.')\n\n    # If we want things in AGL, subtract the minimum height from all height values\n    if with_agl:\n        sfc_height = np.min(height)\n        height = height - sfc_height\n\n    # If the bottom is not specified, make it the surface\n    if bottom is None:\n        bottom = height[0]\n\n    # Make heights and arguments base units\n    height = height.to_base_units()\n    bottom = bottom.to_base_units()\n\n    # Calculate the top of the layer\n    top = bottom + depth\n\n    ret = []  # returned data variables in layer\n\n    # Ensure heights are sorted in ascending order\n    sort_inds = np.argsort(height)\n    height = height[sort_inds]\n\n    # Mask based on top and bottom\n    inds = _greater_or_close(height, bottom) & _less_or_close(height, top)\n    heights_interp = height[inds]\n\n    # Interpolate heights at bounds if necessary and sort\n    if interpolate:\n        # If we don't have the bottom or top requested, append them\n        if top not in heights_interp:\n            heights_interp = units.Quantity(np.sort(np.append(heights_interp.m, top.m)),\n                                            height.units)\n        if bottom not in heights_interp:\n            heights_interp = units.Quantity(np.sort(np.append(heights_interp.m, bottom.m)),\n                                            height.units)\n\n    ret.append(heights_interp)\n\n    for datavar in args:\n        # Ensure that things are sorted in ascending order\n        datavar = datavar[sort_inds]\n\n        if interpolate:\n            # Interpolate for the possibly missing bottom/top values\n            datavar_interp = interpolate_1d(heights_interp, height, datavar)\n            datavar = datavar_interp\n        else:\n            datavar = datavar[inds]\n\n        ret.append(datavar)\n    return ret",
  "def get_layer(pressure, *args, height=None, bottom=None, depth=None, interpolate=True):\n    r\"\"\"Return an atmospheric layer from upper air data with the requested bottom and depth.\n\n    This function will subset an upper air dataset to contain only the specified layer. The\n    bottom of the layer can be specified with a pressure or height above the surface\n    pressure. The bottom defaults to the surface pressure. The depth of the layer can be\n    specified in terms of pressure or height above the bottom of the layer. If the top and\n    bottom of the layer are not in the data, they are interpolated by default.\n\n    Parameters\n    ----------\n    pressure : array-like\n        Atmospheric pressure profile\n    args : array-like\n        Atmospheric variable(s) measured at the given pressures\n    height: array-like, optional\n        Atmospheric heights corresponding to the given pressures. Defaults to using\n        heights calculated from ``pressure`` assuming a standard atmosphere [NOAA1976]_.\n    bottom : `pint.Quantity`, optional\n        Bottom of the layer as a pressure or height above the surface pressure. Defaults\n        to the highest pressure or lowest height given.\n    depth : `pint.Quantity`, optional\n        Thickness of the layer as a pressure or height above the bottom of the layer.\n        Defaults to 100 hPa.\n    interpolate : bool, optional\n        Interpolate the top and bottom points if they are not in the given data. Defaults\n        to True.\n\n    Returns\n    -------\n    `pint.Quantity, pint.Quantity`\n        The pressure and data variables of the layer\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Also, this will return Pint Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``heights`` parameter to ``height``\n\n    \"\"\"\n    # If we get the depth kwarg, but it's None, set it to the default as well\n    if depth is None:\n        depth = units.Quantity(100, 'hPa')\n\n    # Make sure pressure and datavars are the same length\n    for datavar in args:\n        if len(pressure) != len(datavar):\n            raise ValueError('Pressure and data variables must have the same length.')\n\n    # If the bottom is not specified, make it the surface pressure\n    if bottom is None:\n        bottom = np.nanmax(pressure)\n\n    bottom_pressure, bottom_height = _get_bound_pressure_height(pressure, bottom,\n                                                                height=height,\n                                                                interpolate=interpolate)\n\n    # Calculate the top in whatever units depth is in\n    if depth.check('[length]**-1 * [mass] * [time]**-2'):\n        top = bottom_pressure - depth\n    elif depth.check('[length]'):\n        top = bottom_height + depth\n    else:\n        raise ValueError('Depth must be specified in units of length or pressure')\n\n    top_pressure, _ = _get_bound_pressure_height(pressure, top, height=height,\n                                                 interpolate=interpolate)\n\n    ret = []  # returned data variables in layer\n\n    # Ensure pressures are sorted in ascending order\n    sort_inds = np.argsort(pressure)\n    pressure = pressure[sort_inds]\n\n    # Mask based on top and bottom pressure\n    inds = (_less_or_close(pressure, bottom_pressure)\n            & _greater_or_close(pressure, top_pressure))\n    p_interp = pressure[inds]\n\n    # Interpolate pressures at bounds if necessary and sort\n    if interpolate:\n        # If we don't have the bottom or top requested, append them\n        if not np.any(np.isclose(top_pressure, p_interp)):\n            p_interp = units.Quantity(np.sort(np.append(p_interp.m, top_pressure.m)),\n                                      pressure.units)\n        if not np.any(np.isclose(bottom_pressure, p_interp)):\n            p_interp = units.Quantity(np.sort(np.append(p_interp.m, bottom_pressure.m)),\n                                      pressure.units)\n\n    ret.append(p_interp[::-1])\n\n    for datavar in args:\n        # Ensure that things are sorted in ascending order\n        datavar = datavar[sort_inds]\n\n        if interpolate:\n            # Interpolate for the possibly missing bottom/top values\n            datavar_interp = log_interpolate_1d(p_interp, pressure, datavar)\n            datavar = datavar_interp\n        else:\n            datavar = datavar[inds]\n\n        ret.append(datavar[::-1])\n    return ret",
  "def find_bounding_indices(arr, values, axis, from_below=True):\n    \"\"\"Find the indices surrounding the values within arr along axis.\n\n    Returns a set of above, below, good. Above and below are lists of arrays of indices.\n    These lists are formulated such that they can be used directly to index into a numpy\n    array and get the expected results (no extra slices or ellipsis necessary). `good` is\n    a boolean array indicating the \"columns\" that actually had values to bound the desired\n    value(s).\n\n    Parameters\n    ----------\n    arr : array-like\n        Array to search for values\n\n    values: array-like\n        One or more values to search for in `arr`\n\n    axis : int\n        Dimension of `arr` along which to search\n\n    from_below : bool, optional\n        Whether to search from \"below\" (i.e. low indices to high indices). If `False`,\n        the search will instead proceed from high indices to low indices. Defaults to `True`.\n\n    Returns\n    -------\n    above : list of arrays\n        List of broadcasted indices to the location above the desired value\n\n    below : list of arrays\n        List of broadcasted indices to the location below the desired value\n\n    good : array\n        Boolean array indicating where the search found proper bounds for the desired value\n\n    \"\"\"\n    # The shape of generated indices is the same as the input, but with the axis of interest\n    # replaced by the number of values to search for.\n    indices_shape = list(arr.shape)\n    indices_shape[axis] = len(values)\n\n    # Storage for the found indices and the mask for good locations\n    indices = np.empty(indices_shape, dtype=int)\n    good = np.empty(indices_shape, dtype=bool)\n\n    # Used to put the output in the proper location\n    take = make_take(arr.ndim, axis)\n\n    # Loop over all of the values and for each, see where the value would be found from a\n    # linear search\n    for level_index, value in enumerate(values):\n        # Look for changes in the value of the test for <= value in consecutive points\n        # Taking abs() because we only care if there is a flip, not which direction.\n        switches = np.abs(np.diff((arr <= value).astype(int), axis=axis))\n\n        # Good points are those where it's not just 0's along the whole axis\n        good_search = np.any(switches, axis=axis)\n\n        if from_below:\n            # Look for the first switch; need to add 1 to the index since argmax is giving the\n            # index within the difference array, which is one smaller.\n            index = switches.argmax(axis=axis) + 1\n        else:\n            # Generate a list of slices to reverse the axis of interest so that searching from\n            # 0 to N is starting at the \"top\" of the axis.\n            arr_slice = [slice(None)] * arr.ndim\n            arr_slice[axis] = slice(None, None, -1)\n\n            # Same as above, but we use the slice to come from the end; then adjust those\n            # indices to measure from the front.\n            index = arr.shape[axis] - 1 - switches[tuple(arr_slice)].argmax(axis=axis)\n\n        # Set all indices where the results are not good to 0\n        index[~good_search] = 0\n\n        # Put the results in the proper slice\n        store_slice = take(level_index)\n        indices[store_slice] = index\n        good[store_slice] = good_search\n\n    # Create index values for broadcasting arrays\n    above = broadcast_indices(indices, arr.shape, axis)\n    below = broadcast_indices(indices - 1, arr.shape, axis)\n\n    return above, below, good",
  "def _greater_or_close(a, value, **kwargs):\n    r\"\"\"Compare values for greater or close to boolean masks.\n\n    Returns a boolean mask for values greater than or equal to a target within a specified\n    absolute or relative tolerance (as in :func:`numpy.isclose`).\n\n    Parameters\n    ----------\n    a : array-like\n        Array of values to be compared\n    value : float\n        Comparison value\n\n    Returns\n    -------\n    array-like\n        Boolean array where values are greater than or nearly equal to value.\n\n    \"\"\"\n    return (a > value) | np.isclose(a, value, **kwargs)",
  "def _less_or_close(a, value, **kwargs):\n    r\"\"\"Compare values for less or close to boolean masks.\n\n    Returns a boolean mask for values less than or equal to a target within a specified\n    absolute or relative tolerance (as in :func:`numpy.isclose`).\n\n    Parameters\n    ----------\n    a : array-like\n        Array of values to be compared\n    value : float\n        Comparison value\n\n    Returns\n    -------\n    array-like\n        Boolean array where values are less than or nearly equal to value\n\n    \"\"\"\n    return (a < value) | np.isclose(a, value, **kwargs)",
  "def make_take(ndims, slice_dim):\n    \"\"\"Generate a take function to index in a particular dimension.\"\"\"\n    def take(indexer):\n        return tuple(indexer if slice_dim % ndims == i else slice(None)  # noqa: S001\n                     for i in range(ndims))\n    return take",
  "def lat_lon_grid_deltas(longitude, latitude, x_dim=-1, y_dim=-2, geod=None):\n    r\"\"\"Calculate the actual delta between grid points that are in latitude/longitude format.\n\n    Parameters\n    ----------\n    longitude : array-like\n        Array of longitudes defining the grid. If not a `pint.Quantity`, assumed to be in\n        degrees.\n\n    latitude : array-like\n        Array of latitudes defining the grid. If not a `pint.Quantity`, assumed to be in\n        degrees.\n\n    x_dim: int\n        axis number for the x dimension, defaults to -1.\n\n    y_dim : int\n        axis number for the y dimension, defaults to -2.\n\n    geod : `pyproj.Geod` or ``None``\n        PyProj Geod to use for forward azimuth and distance calculations. If ``None``, use a\n        default spherical ellipsoid.\n\n    Returns\n    -------\n    dx, dy:\n        At least two dimensional arrays of signed deltas between grid points in the x and y\n        direction\n\n    Notes\n    -----\n    Accepts 1D, 2D, or higher arrays for latitude and longitude\n    Assumes [..., Y, X] dimension order for input and output, unless keyword arguments `y_dim`\n    and `x_dim` are otherwise specified.\n\n    This function will only return `pint.Quantity` arrays (not `xarray.DataArray` or another\n    array-like type). It will also \"densify\" your data if using Dask or lazy-loading.\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(longitude, latitude, **kwargs)``\n\n    \"\"\"\n    # Inputs must be the same number of dimensions\n    if latitude.ndim != longitude.ndim:\n        raise ValueError('Latitude and longitude must have the same number of dimensions.')\n\n    # If we were given 1D arrays, make a mesh grid\n    if latitude.ndim < 2:\n        longitude, latitude = np.meshgrid(longitude, latitude)\n\n    # pyproj requires ndarrays, not Quantities\n    try:\n        longitude = np.asarray(longitude.m_as('degrees'))\n        latitude = np.asarray(latitude.m_as('degrees'))\n    except AttributeError:\n        longitude = np.asarray(longitude)\n        latitude = np.asarray(latitude)\n\n    # Determine dimension order for offset slicing\n    take_y = make_take(latitude.ndim, y_dim)\n    take_x = make_take(latitude.ndim, x_dim)\n\n    g = Geod(ellps='sphere') if geod is None else geod\n    forward_az, _, dy = g.inv(longitude[take_y(slice(None, -1))],\n                              latitude[take_y(slice(None, -1))],\n                              longitude[take_y(slice(1, None))],\n                              latitude[take_y(slice(1, None))])\n    dy[(forward_az < -90.) | (forward_az > 90.)] *= -1\n\n    forward_az, _, dx = g.inv(longitude[take_x(slice(None, -1))],\n                              latitude[take_x(slice(None, -1))],\n                              longitude[take_x(slice(1, None))],\n                              latitude[take_x(slice(1, None))])\n    dx[(forward_az < 0.) | (forward_az > 180.)] *= -1\n\n    return units.Quantity(dx, 'meter'), units.Quantity(dy, 'meter')",
  "def nominal_lat_lon_grid_deltas(longitude, latitude, geod=None):\n    \"\"\"Calculate the nominal deltas along axes of a latitude/longitude grid.\"\"\"\n    if geod is None:\n        geod = CRS('+proj=latlon').get_geod()\n\n    # This allows working with coordinates that have been manually broadcast\n    longitude = longitude.squeeze()\n    latitude = latitude.squeeze()\n\n    if longitude.ndim != 1 or latitude.ndim != 1:\n        raise ValueError(\n            'Cannot calculate nominal grid spacing from longitude and latitude arguments '\n            'that are not one dimensional.'\n        )\n\n    dx = units.Quantity(geod.a * np.diff(longitude).m_as('radian'), 'meter')\n    lat = latitude.m_as('degree')\n    lon_meridian_diff = np.zeros(len(lat) - 1, dtype=lat.dtype)\n    forward_az, _, dy = geod.inv(lon_meridian_diff, lat[:-1], lon_meridian_diff, lat[1:],\n                                 radians=False)\n    dy[(forward_az < -90.) | (forward_az > 90.)] *= -1\n    dy = units.Quantity(dy, 'meter')\n\n    return dx, dy",
  "def azimuth_range_to_lat_lon(azimuths, ranges, center_lon, center_lat, geod=None):\n    \"\"\"Convert azimuth and range locations in a polar coordinate system to lat/lon coordinates.\n\n    Pole refers to the origin of the coordinate system.\n\n    Parameters\n    ----------\n    azimuths : array-like\n        array of azimuths defining the grid. If not a `pint.Quantity`,\n        assumed to be in degrees.\n    ranges : array-like\n        array of range distances from the pole. Typically in meters.\n    center_lat : float\n        The latitude of the pole in decimal degrees\n    center_lon : float\n        The longitude of the pole in decimal degrees\n    geod : `pyproj.Geod` or ``None``\n        PyProj Geod to use for forward azimuth and distance calculations. If ``None``, use a\n        default spherical ellipsoid.\n\n    Returns\n    -------\n    lon, lat : 2D arrays of longitudes and latitudes corresponding to original locations\n\n    Notes\n    -----\n    Credit to Brian Blaylock for the original implementation.\n\n    \"\"\"\n    g = Geod(ellps='sphere') if geod is None else geod\n    try:  # convert range units to meters\n        ranges = ranges.m_as('meters')\n    except AttributeError:  # no units associated\n        _warnings.warn('Range values are not a Pint-Quantity, assuming values are in meters.')\n    try:  # convert azimuth units to degrees\n        azimuths = azimuths.m_as('degrees')\n    except AttributeError:  # no units associated\n        _warnings.warn(\n            'Azimuth values are not a Pint-Quantity, assuming values are in degrees.'\n        )\n    rng2d, az2d = np.meshgrid(ranges, azimuths)\n    lats = np.full(az2d.shape, center_lat)\n    lons = np.full(az2d.shape, center_lon)\n    lon, lat, _ = g.fwd(lons, lats, az2d, rng2d)\n\n    return lon, lat",
  "def xarray_derivative_wrap(func):\n    \"\"\"Decorate the derivative functions to make them work nicely with DataArrays.\n\n    This will automatically determine if the coordinates can be pulled directly from the\n    DataArray, or if a call to lat_lon_grid_deltas is needed.\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(f, **kwargs):\n        if 'x' in kwargs or 'delta' in kwargs:\n            # Use the usual DataArray to pint.Quantity preprocessing wrapper\n            return preprocess_and_wrap()(func)(f, **kwargs)\n        elif isinstance(f, xr.DataArray):\n            # Get axis argument, defaulting to first dimension\n            axis = f.metpy.find_axis_name(kwargs.get('axis', 0))\n\n            # Initialize new kwargs with the axis number\n            new_kwargs = {'axis': f.get_axis_num(axis)}\n\n            if check_axis(f[axis], 'time'):\n                # Time coordinate, need to get time deltas\n                new_kwargs['delta'] = f[axis].metpy.time_deltas\n            elif check_axis(f[axis], 'longitude'):\n                # Longitude coordinate, need to get grid deltas\n                new_kwargs['delta'], _ = grid_deltas_from_dataarray(f)\n            elif check_axis(f[axis], 'latitude'):\n                # Latitude coordinate, need to get grid deltas\n                _, new_kwargs['delta'] = grid_deltas_from_dataarray(f)\n            else:\n                # General coordinate, use as is\n                new_kwargs['x'] = f[axis].metpy.unit_array\n\n            # Calculate and return result as a DataArray\n            result = func(f.metpy.unit_array, **new_kwargs)\n            return xr.DataArray(result, coords=f.coords, dims=f.dims)\n        else:\n            # Error\n            raise ValueError('Must specify either \"x\" or \"delta\" for value positions when \"f\" '\n                             'is not a DataArray.')\n    return wrapper",
  "def _add_grid_params_to_docstring(docstring: str, orig_includes: dict) -> str:\n    \"\"\"Add documentation for some dynamically added grid parameters to the docstring.\"\"\"\n    other_params = docstring.find('Other Parameters')\n    blank = docstring.find('\\n\\n', other_params)\n\n    entries = {\n        'longitude': \"\"\"\n    longitude : `pint.Quantity`, optional\n        Longitude of data. Optional if `xarray.DataArray` with latitude/longitude coordinates\n        used as input. Also optional if parallel_scale and meridional_scale are given. If\n        otherwise omitted, calculation will be carried out on a Cartesian, rather than\n        geospatial, grid. Keyword-only argument.\"\"\",\n        'latitude': \"\"\"\n    latitude : `pint.Quantity`, optional\n        Latitude of data. Optional if `xarray.DataArray` with latitude/longitude coordinates\n        used as input. Also optional if parallel_scale and meridional_scale are given. If\n        otherwise omitted, calculation will be carried out on a Cartesian, rather than\n        geospatial, grid. Keyword-only argument.\"\"\",\n        'crs': \"\"\"\n    crs : `pyproj.crs.CRS`, optional\n        Coordinate Reference System of data. Optional if `xarray.DataArray` with MetPy CRS\n        used as input. Also optional if parallel_scale and meridional_scale are given. If\n        otherwise omitted, calculation will be carried out on a Cartesian, rather than\n        geospatial, grid. Keyword-only argument.\"\"\"\n    }\n\n    return ''.join([docstring[:blank],\n                    *(entries[p] for p, included in orig_includes.items() if not included),\n                    docstring[blank:]])",
  "def parse_grid_arguments(func):\n    \"\"\"Parse arguments to functions involving derivatives on a grid.\"\"\"\n    from ..xarray import dataarray_arguments\n\n    # Dynamically add new parameters for lat, lon, and crs to the function signature\n    # which is used to handle arguments inside the wrapper--but only if they're not in the\n    # original signature\n    sig = signature(func)\n    orig_func_uses = {param: param in sig.parameters\n                      for param in ('latitude', 'longitude', 'crs')}\n    newsig = sig.replace(parameters=[*sig.parameters.values(),\n                                     *(Parameter(name, Parameter.KEYWORD_ONLY, default=None)\n                                       for name, needed in orig_func_uses.items()\n                                       if not needed)])\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        bound_args = newsig.bind(*args, **kwargs)\n        bound_args.apply_defaults()\n        scale_lat = latitude = bound_args.arguments.pop('latitude')\n        scale_lon = longitude = bound_args.arguments.pop('longitude')\n        crs = bound_args.arguments.pop('crs')\n\n        # Choose the first DataArray argument to act as grid prototype\n        grid_prototype = next(dataarray_arguments(bound_args), None)\n\n        # Fill in x_dim/y_dim\n        if (\n            grid_prototype is not None\n            and 'x_dim' in bound_args.arguments\n            and 'y_dim' in bound_args.arguments\n        ):\n            try:\n                bound_args.arguments['x_dim'] = grid_prototype.metpy.find_axis_number('x')\n                bound_args.arguments['y_dim'] = grid_prototype.metpy.find_axis_number('y')\n            except AttributeError:\n                # If axis number not found, fall back to default but warn.\n                _warnings.warn('Horizontal dimension numbers not found. Defaulting to '\n                               '(..., Y, X) order.')\n\n        # Fill in vertical_dim\n        if (\n            grid_prototype is not None\n            and 'vertical_dim' in bound_args.arguments\n        ):\n            try:\n                bound_args.arguments['vertical_dim'] = (\n                    grid_prototype.metpy.find_axis_number('vertical')\n                )\n            except AttributeError:\n                # If axis number not found, fall back to default but warn.\n                _warnings.warn(\n                    'Vertical dimension number not found. Defaulting to (..., Z, Y, X) order.'\n                )\n\n        # Fill in dz\n        if (\n            grid_prototype is not None\n            and 'dz' in bound_args.arguments\n        ):\n            if bound_args.arguments['dz'] is None:\n                try:\n                    vertical_coord = grid_prototype.metpy.vertical\n                    bound_args.arguments['dz'] = np.diff(vertical_coord.metpy.unit_array)\n                except (AttributeError, ValueError):\n                    # Skip, since this only comes up in advection, where dz is optional\n                    # (may not need vertical at all)\n                    pass\n            if (\n                func.__name__.endswith('advection')\n                and bound_args.arguments['u'] is None\n                and bound_args.arguments['v'] is None\n            ):\n                return func(*bound_args.args, **bound_args.kwargs)\n\n        # Fill in dx and dy\n        if (\n            'dx' in bound_args.arguments and bound_args.arguments['dx'] is None\n            and 'dy' in bound_args.arguments and bound_args.arguments['dy'] is None\n        ):\n            if grid_prototype is not None:\n                grid_deltas = grid_prototype.metpy.grid_deltas\n                bound_args.arguments['dx'] = grid_deltas['dx']\n                bound_args.arguments['dy'] = grid_deltas['dy']\n            elif longitude is not None and latitude is not None and crs is not None:\n                # TODO: de-duplicate .metpy.grid_deltas code\n                geod = None if crs is None else crs.get_geod()\n                bound_args.arguments['dx'], bound_args.arguments['dy'] = (\n                    nominal_lat_lon_grid_deltas(longitude, latitude, geod)\n                )\n            elif 'dz' in bound_args.arguments:\n                # Handle advection case, allowing dx/dy to be None but dz to not be None\n                if bound_args.arguments['dz'] is None:\n                    raise ValueError(\n                        'Must provide dx, dy, and/or dz arguments or input DataArray with '\n                        'interpretable dimension coordinates.'\n                    )\n            else:\n                raise ValueError(\n                    'Must provide dx/dy arguments, input DataArray with interpretable '\n                    'dimension coordinates, or 1D longitude/latitude arguments with an '\n                    'optional PyProj CRS.'\n                )\n\n        # Fill in parallel_scale and meridional_scale\n        if (\n            'parallel_scale' in bound_args.arguments\n            and bound_args.arguments['parallel_scale'] is None\n            and 'meridional_scale' in bound_args.arguments\n            and bound_args.arguments['meridional_scale'] is None\n        ):\n            proj = None\n            if grid_prototype is not None:\n                # Fall back to basic cartesian calculation if we don't have a CRS or we\n                # are unable to get the coordinates needed for map factor calculation\n                # (either existing lat/lon or lat/lon computed from y/x)\n                with contextlib.suppress(AttributeError):\n                    latitude, longitude = grid_prototype.metpy.coordinates('latitude',\n                                                                           'longitude')\n                    scale_lat = latitude.metpy.unit_array\n                    scale_lon = longitude.metpy.unit_array\n                    if hasattr(grid_prototype.metpy, 'pyproj_proj'):\n                        proj = grid_prototype.metpy.pyproj_proj\n                    elif latitude.squeeze().ndim == 1 and longitude.squeeze().ndim == 1:\n                        proj = Proj(CRS('+proj=latlon'))\n            elif latitude is not None and longitude is not None:\n                try:\n                    proj = Proj(crs)\n                except Exception as e:\n                    # Whoops, intended to use\n                    raise ValueError(\n                        'Latitude and longitude arguments provided so as to make '\n                        'calculation projection-correct, however, projection CRS is '\n                        'missing or invalid.'\n                    ) from e\n\n            # Do we have everything we need to sensibly calculate the scale arrays?\n            if proj is not None:\n                scale_lat = scale_lat.squeeze().m_as('degrees')\n                scale_lon = scale_lon.squeeze().m_as('degrees')\n                if scale_lat.ndim == 1 and scale_lon.ndim == 1:\n                    scale_lon, scale_lat = np.meshgrid(scale_lon, scale_lat)\n                elif scale_lat.ndim != 2 or scale_lon.ndim != 2:\n                    raise ValueError('Latitude and longitude must be either 1D or 2D.')\n                factors = proj.get_factors(scale_lon, scale_lat)\n                p_scale = factors.parallel_scale\n                m_scale = factors.meridional_scale\n\n                if grid_prototype is not None:\n                    # Set the dims and coords using the original from the input lat/lon.\n                    # This particular implementation relies on them being 1D/2D for the dims.\n                    xr_kwargs = {'coords': {**latitude.coords, **longitude.coords},\n                                 'dims': (latitude.dims[0], longitude.dims[-1])}\n                    p_scale = xr.DataArray(p_scale, **xr_kwargs)\n                    m_scale = xr.DataArray(m_scale, **xr_kwargs)\n\n                bound_args.arguments['parallel_scale'] = p_scale\n                bound_args.arguments['meridional_scale'] = m_scale\n\n        # If the original function uses any of the arguments that are otherwise dynamically\n        # added, be sure to pass them to the original function.\n        local_namespace = vars()\n        bound_args.arguments.update({param: local_namespace[param]\n                                     for param, uses in orig_func_uses.items() if uses})\n\n        return func(*bound_args.args, **bound_args.kwargs)\n\n    # Override the wrapper function's signature with a better signature. Also add docstrings\n    # for our added parameters.\n    wrapper.__signature__ = newsig\n    if getattr(wrapper, '__doc__', None) is not None:\n        wrapper.__doc__ = _add_grid_params_to_docstring(wrapper.__doc__, orig_func_uses)\n\n    return wrapper",
  "def first_derivative(f, axis=None, x=None, delta=None):\n    \"\"\"Calculate the first derivative of a grid of values.\n\n    Works for both regularly-spaced data and grids with varying spacing.\n\n    Either `x` or `delta` must be specified, or `f` must be given as an `xarray.DataArray` with\n    attached coordinate and projection information. If `f` is an `xarray.DataArray`, and `x` or\n    `delta` are given, `f` will be converted to a `pint.Quantity` and the derivative returned\n    as a `pint.Quantity`, otherwise, if neither `x` nor `delta` are given, the attached\n    coordinate information belonging to `axis` will be used and the derivative will be returned\n    as an `xarray.DataArray`.\n\n    This uses 3 points to calculate the derivative, using forward or backward at the edges of\n    the grid as appropriate, and centered elsewhere. The irregular spacing is handled\n    explicitly, using the formulation as specified by [Bowen2005]_.\n\n    Parameters\n    ----------\n    f : array-like\n        Array of values of which to calculate the derivative\n    axis : int or str, optional\n        The array axis along which to take the derivative. If `f` is ndarray-like, must be an\n        integer. If `f` is a `DataArray`, can be a string (referring to either the coordinate\n        dimension name or the axis type) or integer (referring to axis number), unless using\n        implicit conversion to `pint.Quantity`, in which case it must be an integer. Defaults\n        to 0. For reference, the current standard axis types are 'time', 'vertical', 'y', and\n        'x'.\n    x : array-like, optional\n        The coordinate values corresponding to the grid points in `f`\n    delta : array-like, optional\n        Spacing between the grid points in `f`. Should be one item less than the size\n        of `f` along `axis`.\n\n    Returns\n    -------\n    array-like\n        The first derivative calculated along the selected axis\n\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(f, **kwargs)``\n\n    See Also\n    --------\n    second_derivative\n\n    \"\"\"\n    n, axis, delta = _process_deriv_args(f, axis, x, delta)\n    take = make_take(n, axis)\n\n    # First handle centered case\n    slice0 = take(slice(None, -2))\n    slice1 = take(slice(1, -1))\n    slice2 = take(slice(2, None))\n    delta_slice0 = take(slice(None, -1))\n    delta_slice1 = take(slice(1, None))\n\n    combined_delta = delta[delta_slice0] + delta[delta_slice1]\n    delta_diff = delta[delta_slice1] - delta[delta_slice0]\n    center = (- delta[delta_slice1] / (combined_delta * delta[delta_slice0]) * f[slice0]\n              + delta_diff / (delta[delta_slice0] * delta[delta_slice1]) * f[slice1]\n              + delta[delta_slice0] / (combined_delta * delta[delta_slice1]) * f[slice2])\n\n    # Fill in \"left\" edge with forward difference\n    slice0 = take(slice(None, 1))\n    slice1 = take(slice(1, 2))\n    slice2 = take(slice(2, 3))\n    delta_slice0 = take(slice(None, 1))\n    delta_slice1 = take(slice(1, 2))\n\n    combined_delta = delta[delta_slice0] + delta[delta_slice1]\n    big_delta = combined_delta + delta[delta_slice0]\n    left = (- big_delta / (combined_delta * delta[delta_slice0]) * f[slice0]\n            + combined_delta / (delta[delta_slice0] * delta[delta_slice1]) * f[slice1]\n            - delta[delta_slice0] / (combined_delta * delta[delta_slice1]) * f[slice2])\n\n    # Now the \"right\" edge with backward difference\n    slice0 = take(slice(-3, -2))\n    slice1 = take(slice(-2, -1))\n    slice2 = take(slice(-1, None))\n    delta_slice0 = take(slice(-2, -1))\n    delta_slice1 = take(slice(-1, None))\n\n    combined_delta = delta[delta_slice0] + delta[delta_slice1]\n    big_delta = combined_delta + delta[delta_slice1]\n    right = (delta[delta_slice1] / (combined_delta * delta[delta_slice0]) * f[slice0]\n             - combined_delta / (delta[delta_slice0] * delta[delta_slice1]) * f[slice1]\n             + big_delta / (combined_delta * delta[delta_slice1]) * f[slice2])\n\n    return concatenate((left, center, right), axis=axis)",
  "def second_derivative(f, axis=None, x=None, delta=None):\n    \"\"\"Calculate the second derivative of a grid of values.\n\n    Works for both regularly-spaced data and grids with varying spacing.\n\n    Either `x` or `delta` must be specified, or `f` must be given as an `xarray.DataArray` with\n    attached coordinate and projection information. If `f` is an `xarray.DataArray`, and `x` or\n    `delta` are given, `f` will be converted to a `pint.Quantity` and the derivative returned\n    as a `pint.Quantity`, otherwise, if neither `x` nor `delta` are given, the attached\n    coordinate information belonging to `axis` will be used and the derivative will be returned\n    as an `xarray.DataArray`\n\n    This uses 3 points to calculate the derivative, using forward or backward at the edges of\n    the grid as appropriate, and centered elsewhere. The irregular spacing is handled\n    explicitly, using the formulation as specified by [Bowen2005]_.\n\n    Parameters\n    ----------\n    f : array-like\n        Array of values of which to calculate the derivative\n    axis : int or str, optional\n        The array axis along which to take the derivative. If `f` is ndarray-like, must be an\n        integer. If `f` is a `DataArray`, can be a string (referring to either the coordinate\n        dimension name or the axis type) or integer (referring to axis number), unless using\n        implicit conversion to `pint.Quantity`, in which case it must be an integer. Defaults\n        to 0. For reference, the current standard axis types are 'time', 'vertical', 'y', and\n        'x'.\n    x : array-like, optional\n        The coordinate values corresponding to the grid points in `f`\n    delta : array-like, optional\n        Spacing between the grid points in `f`. There should be one item less than the size\n        of `f` along `axis`.\n\n    Returns\n    -------\n    array-like\n        The second derivative calculated along the selected axis\n\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(f, **kwargs)``\n\n    See Also\n    --------\n    first_derivative\n\n    \"\"\"\n    n, axis, delta = _process_deriv_args(f, axis, x, delta)\n    take = make_take(n, axis)\n\n    # First handle centered case\n    slice0 = take(slice(None, -2))\n    slice1 = take(slice(1, -1))\n    slice2 = take(slice(2, None))\n    delta_slice0 = take(slice(None, -1))\n    delta_slice1 = take(slice(1, None))\n\n    combined_delta = delta[delta_slice0] + delta[delta_slice1]\n    center = 2 * (f[slice0] / (combined_delta * delta[delta_slice0])\n                  - f[slice1] / (delta[delta_slice0] * delta[delta_slice1])\n                  + f[slice2] / (combined_delta * delta[delta_slice1]))\n\n    # Fill in \"left\" edge\n    slice0 = take(slice(None, 1))\n    slice1 = take(slice(1, 2))\n    slice2 = take(slice(2, 3))\n    delta_slice0 = take(slice(None, 1))\n    delta_slice1 = take(slice(1, 2))\n\n    combined_delta = delta[delta_slice0] + delta[delta_slice1]\n    left = 2 * (f[slice0] / (combined_delta * delta[delta_slice0])\n                - f[slice1] / (delta[delta_slice0] * delta[delta_slice1])\n                + f[slice2] / (combined_delta * delta[delta_slice1]))\n\n    # Now the \"right\" edge\n    slice0 = take(slice(-3, -2))\n    slice1 = take(slice(-2, -1))\n    slice2 = take(slice(-1, None))\n    delta_slice0 = take(slice(-2, -1))\n    delta_slice1 = take(slice(-1, None))\n\n    combined_delta = delta[delta_slice0] + delta[delta_slice1]\n    right = 2 * (f[slice0] / (combined_delta * delta[delta_slice0])\n                 - f[slice1] / (delta[delta_slice0] * delta[delta_slice1])\n                 + f[slice2] / (combined_delta * delta[delta_slice1]))\n\n    return concatenate((left, center, right), axis=axis)",
  "def gradient(f, axes=None, coordinates=None, deltas=None):\n    \"\"\"Calculate the gradient of a scalar quantity, assuming Cartesian coordinates.\n\n    Works for both regularly-spaced data, and grids with varying spacing.\n\n    Either `coordinates` or `deltas` must be specified, or `f` must be given as an\n    `xarray.DataArray` with  attached coordinate and projection information. If `f` is an\n    `xarray.DataArray`, and `coordinates` or `deltas` are given, `f` will be converted to a\n    `pint.Quantity` and the gradient returned as a tuple of `pint.Quantity`, otherwise, if\n    neither `coordinates` nor `deltas` are given, the attached coordinate information belonging\n    to `axis` will be used and the gradient will be returned as a tuple of `xarray.DataArray`.\n\n    Parameters\n    ----------\n    f : array-like\n        Array of values of which to calculate the derivative\n    axes : Sequence[str] or Sequence[int], optional\n        Sequence of strings (if `f` is a `xarray.DataArray` and implicit conversion to\n        `pint.Quantity` is not used) or integers that specify the array axes along which to\n        take the derivatives. Defaults to all axes of `f`. If given, and used with\n        `coordinates` or `deltas`, its length must be less than or equal to that of the\n        `coordinates` or `deltas` given. In general, each axis can be an axis number\n        (integer), dimension coordinate name (string) or a standard axis type (string). The\n        current standard axis types are 'time', 'vertical', 'y', and 'x'.\n    coordinates : array-like, optional\n        Sequence of arrays containing the coordinate values corresponding to the\n        grid points in `f` in axis order.\n    deltas : array-like, optional\n        Sequence of arrays or scalars that specify the spacing between the grid points in `f`\n        in axis order. There should be one item less than the size of `f` along the applicable\n        axis.\n\n    Returns\n    -------\n    tuple of array-like\n        The first derivative calculated along each specified axis of the original array\n\n    See Also\n    --------\n    laplacian, first_derivative, vector_derivative, geospatial_gradient\n\n    Notes\n    -----\n    If this function is used without the `axes` parameter, the length of `coordinates` or\n    `deltas` (as applicable) should match the number of dimensions of `f`.\n\n    This will not give projection-correct results for horizontal geospatial fields. Instead,\n    for vector quantities, use `vector_derivative`, and for scalar quantities, use\n    `geospatial_gradient`.\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(f, **kwargs)``\n\n    \"\"\"\n    pos_kwarg, positions, axes = _process_gradient_args(f, axes, coordinates, deltas)\n    return tuple(first_derivative(f, axis=axis, **{pos_kwarg: positions[ind]})\n                 for ind, axis in enumerate(axes))",
  "def laplacian(f, axes=None, coordinates=None, deltas=None):\n    \"\"\"Calculate the laplacian of a grid of values.\n\n    Works for both regularly-spaced data, and grids with varying spacing.\n\n    Either `coordinates` or `deltas` must be specified, or `f` must be given as an\n    `xarray.DataArray` with  attached coordinate and projection information. If `f` is an\n    `xarray.DataArray`, and `coordinates` or `deltas` are given, `f` will be converted to a\n    `pint.Quantity` and the gradient returned as a tuple of `pint.Quantity`, otherwise, if\n    neither `coordinates` nor `deltas` are given, the attached coordinate information belonging\n    to `axis` will be used and the gradient will be returned as a tuple of `xarray.DataArray`.\n\n    Parameters\n    ----------\n    f : array-like\n        Array of values of which to calculate the derivative\n    axes : Sequence[str] or Sequence[int], optional\n        Sequence of strings (if `f` is a `xarray.DataArray` and implicit conversion to\n        `pint.Quantity` is not used) or integers that specify the array axes along which to\n        take the derivatives. Defaults to all axes of `f`. If given, and used with\n        `coordinates` or `deltas`, its length must be less than or equal to that of the\n        `coordinates` or `deltas` given. In general, each axis can be an axis number\n        (integer), dimension coordinate name (string) or a standard axis type (string). The\n        current standard axis types are 'time', 'vertical', 'y', and 'x'.\n    coordinates : array-like, optional\n        The coordinate values corresponding to the grid points in `f`\n    deltas : array-like, optional\n        Spacing between the grid points in `f`. There should be one item less than the size\n        of `f` along the applicable axis.\n\n    Returns\n    -------\n    array-like\n        The laplacian\n\n    See Also\n    --------\n    gradient, second_derivative\n\n    Notes\n    -----\n    If this function is used without the `axes` parameter, the length of `coordinates` or\n    `deltas` (as applicable) should match the number of dimensions of `f`.\n\n    .. versionchanged:: 1.0\n       Changed signature from ``(f, **kwargs)``\n\n    \"\"\"\n    pos_kwarg, positions, axes = _process_gradient_args(f, axes, coordinates, deltas)\n    derivs = [second_derivative(f, axis=axis, **{pos_kwarg: positions[ind]})\n              for ind, axis in enumerate(axes)]\n    return sum(derivs)",
  "def vector_derivative(u, v, *, dx=None, dy=None, x_dim=-1, y_dim=-2,\n                      parallel_scale=None, meridional_scale=None, return_only=None):\n    r\"\"\"Calculate the projection-correct derivative matrix of a 2D vector.\n\n    Parameters\n    ----------\n    u : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        x component of the vector\n    v : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        y component of the vector\n    return_only : str or Sequence[str], optional\n        Sequence of which components of the derivative matrix to compute and return. If none,\n        returns the full matrix as a tuple of tuples (('du/dx', 'du/dy'), ('dv/dx', 'dv/dy')).\n        Otherwise, matches the return pattern of the given strings. Only valid strings are\n        'du/dx', 'du/dy', 'dv/dx', and 'dv/dy'.\n\n    Returns\n    -------\n    `pint.Quantity`, tuple of `pint.Quantity`, or tuple of tuple of `pint.Quantity`\n        Component(s) of vector derivative\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input. Also optional if one-dimensional\n        longitude and latitude arguments are given for your data on a non-projected grid.\n        Keyword-only argument.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input. Also optional if one-dimensional\n        longitude and latitude arguments are given for your data on a non-projected grid.\n        Keyword-only argument.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`. Keyword-only argument.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`. Keyword-only argument.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n    See Also\n    --------\n    geospatial_gradient, geospatial_laplacian, first_derivative\n\n    \"\"\"\n    # Determine which derivatives to calculate\n    derivatives = {\n        component: None\n        for component in ('du/dx', 'du/dy', 'dv/dx', 'dv/dy')\n        if (return_only is None or component in return_only)\n    }\n    map_factor_correction = parallel_scale is not None and meridional_scale is not None\n\n    # Add in the map factor derivatives if needed\n    if map_factor_correction and ('du/dx' in derivatives or 'dv/dx' in derivatives):\n        derivatives['dp/dy'] = None\n    if map_factor_correction and ('du/dy' in derivatives or 'dv/dy' in derivatives):\n        derivatives['dm/dx'] = None\n\n    # Compute the Cartesian derivatives\n    for component in derivatives:\n        scalar = {\n            'du': u, 'dv': v, 'dp': parallel_scale, 'dm': meridional_scale\n        }[component[:2]]\n        delta, dim = (dx, x_dim) if component[-2:] == 'dx' else (dy, y_dim)\n        derivatives[component] = first_derivative(scalar, delta=delta, axis=dim)\n\n    # Apply map factor corrections\n    if map_factor_correction:\n        # Factor against opposite component\n        if 'dp/dy' in derivatives:\n            dx_correction = meridional_scale / parallel_scale * derivatives['dp/dy']\n        if 'dm/dx' in derivatives:\n            dy_correction = parallel_scale / meridional_scale * derivatives['dm/dx']\n\n        # Corrected terms\n        if 'du/dx' in derivatives:\n            derivatives['du/dx'] = parallel_scale * derivatives['du/dx'] - v * dx_correction\n        if 'du/dy' in derivatives:\n            derivatives['du/dy'] = meridional_scale * derivatives['du/dy'] + v * dy_correction\n        if 'dv/dx' in derivatives:\n            derivatives['dv/dx'] = parallel_scale * derivatives['dv/dx'] + u * dx_correction\n        if 'dv/dy' in derivatives:\n            derivatives['dv/dy'] = meridional_scale * derivatives['dv/dy'] - u * dy_correction\n\n    if return_only is None:\n        return (\n            derivatives['du/dx'], derivatives['du/dy'],\n            derivatives['dv/dx'], derivatives['dv/dy']\n        )\n    elif isinstance(return_only, str):\n        return derivatives[return_only]\n    else:\n        return tuple(derivatives[component] for component in return_only)",
  "def geospatial_gradient(f, *, dx=None, dy=None, x_dim=-1, y_dim=-2,\n                        parallel_scale=None, meridional_scale=None, return_only=None):\n    r\"\"\"Calculate the projection-correct gradient of a 2D scalar field.\n\n    Parameters\n    ----------\n    f : (..., M, N) `xarray.DataArray` or `pint.Quantity`\n        scalar field for which the horizontal gradient should be calculated\n    return_only : str or Sequence[str], optional\n        Sequence of which components of the gradient to compute and return. If none,\n        returns the gradient tuple ('df/dx', 'df/dy'). Otherwise, matches the return\n        pattern of the given strings. Only valid strings are 'df/dx', 'df/dy'.\n\n    Returns\n    -------\n    `pint.Quantity`, tuple of `pint.Quantity`, or tuple of pairs of `pint.Quantity`\n        Component(s) of vector derivative\n\n    Other Parameters\n    ----------------\n    dx : `pint.Quantity`, optional\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input. Also optional if one-dimensional\n        longitude and latitude arguments are given for your data on a non-projected grid.\n        Keyword-only argument.\n    dy : `pint.Quantity`, optional\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis. Optional if `xarray.DataArray` with\n        latitude/longitude coordinates used as input. Also optional if one-dimensional\n        longitude and latitude arguments are given for your data on a non-projected grid.\n        Keyword-only argument.\n    x_dim : int, optional\n        Axis number of x dimension. Defaults to -1 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`. Keyword-only argument.\n    y_dim : int, optional\n        Axis number of y dimension. Defaults to -2 (implying [..., Y, X] order). Automatically\n        parsed from input if using `xarray.DataArray`. Keyword-only argument.\n    parallel_scale : `pint.Quantity`, optional\n        Parallel scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n    meridional_scale : `pint.Quantity`, optional\n        Meridional scale of map projection at data coordinate. Optional if `xarray.DataArray`\n        with latitude/longitude coordinates and MetPy CRS used as input. Also optional if\n        longitude, latitude, and crs are given. If otherwise omitted, calculation will be\n        carried out on a Cartesian, rather than geospatial, grid. Keyword-only argument.\n\n    See Also\n    --------\n    vector_derivative, gradient, geospatial_laplacian\n\n    \"\"\"\n    derivatives = {component: None\n                   for component in ('df/dx', 'df/dy')\n                   if (return_only is None or component in return_only)}\n\n    scales = {'df/dx': parallel_scale, 'df/dy': meridional_scale}\n\n    map_factor_correction = parallel_scale is not None and meridional_scale is not None\n\n    for component in derivatives:\n        delta, dim = (dx, x_dim) if component[-2:] == 'dx' else (dy, y_dim)\n        derivatives[component] = first_derivative(f, delta=delta, axis=dim)\n\n        if map_factor_correction:\n            derivatives[component] *= scales[component]\n\n    # Build return collection\n    if return_only is None:\n        return derivatives['df/dx'], derivatives['df/dy']\n    elif isinstance(return_only, str):\n        return derivatives[return_only]\n    else:\n        return tuple(derivatives[component] for component in return_only)",
  "def _broadcast_to_axis(arr, axis, ndim):\n    \"\"\"Handle reshaping coordinate array to have proper dimensionality.\n\n    This puts the values along the specified axis.\n    \"\"\"\n    if arr.ndim == 1 and arr.ndim < ndim:\n        new_shape = [1] * ndim\n        new_shape[axis] = arr.size\n        arr = arr.reshape(*new_shape)\n    return arr",
  "def _process_gradient_args(f, axes, coordinates, deltas):\n    \"\"\"Handle common processing of arguments for gradient and gradient-like functions.\"\"\"\n    axes_given = axes is not None\n    axes = axes if axes_given else range(f.ndim)\n\n    def _check_length(positions):\n        if axes_given and len(positions) < len(axes):\n            raise ValueError('Length of \"coordinates\" or \"deltas\" cannot be less than that '\n                             'of \"axes\".')\n        elif not axes_given and len(positions) != len(axes):\n            raise ValueError('Length of \"coordinates\" or \"deltas\" must match the number of '\n                             'dimensions of \"f\" when \"axes\" is not given.')\n\n    if deltas is not None:\n        if coordinates is not None:\n            raise ValueError('Cannot specify both \"coordinates\" and \"deltas\".')\n        _check_length(deltas)\n        return 'delta', deltas, axes\n    elif coordinates is not None:\n        _check_length(coordinates)\n        return 'x', coordinates, axes\n    elif isinstance(f, xr.DataArray):\n        return 'pass', axes, axes  # only the axis argument matters\n    else:\n        raise ValueError('Must specify either \"coordinates\" or \"deltas\" for value positions '\n                         'when \"f\" is not a DataArray.')",
  "def _process_deriv_args(f, axis, x, delta):\n    \"\"\"Handle common processing of arguments for derivative functions.\"\"\"\n    n = f.ndim\n    axis = normalize_axis_index(axis if axis is not None else 0, n)\n\n    if f.shape[axis] < 3:\n        raise ValueError('f must have at least 3 point along the desired axis.')\n\n    if delta is not None:\n        if x is not None:\n            raise ValueError('Cannot specify both \"x\" and \"delta\".')\n\n        delta = np.atleast_1d(delta)\n        if delta.size == 1:\n            diff_size = list(f.shape)\n            diff_size[axis] -= 1\n            delta_units = getattr(delta, 'units', None)\n            delta = np.broadcast_to(delta, diff_size, subok=True)\n            if not hasattr(delta, 'units') and delta_units is not None:\n                delta = units.Quantity(delta, delta_units)\n        else:\n            delta = _broadcast_to_axis(delta, axis, n)\n    elif x is not None:\n        x = _broadcast_to_axis(x, axis, n)\n        delta = np.diff(x, axis=axis)\n    else:\n        raise ValueError('Must specify either \"x\" or \"delta\" for value positions.')\n\n    return n, axis, delta",
  "def parse_angle(input_dir):\n    \"\"\"Calculate the meteorological angle from directional text.\n\n    Works for abbreviations or whole words (E -> 90 | South -> 180)\n    and also is able to parse 22.5 degree angles such as ESE/East South East.\n\n    Parameters\n    ----------\n    input_dir : str or Sequence[str]\n        Directional text such as west, [south-west, ne], etc.\n\n    Returns\n    -------\n    `pint.Quantity`\n        The angle in degrees\n\n    \"\"\"\n    if isinstance(input_dir, str):\n        # abb_dirs = abbrieviated directions\n        abb_dirs = _clean_direction([_abbrieviate_direction(input_dir)])\n    elif hasattr(input_dir, '__len__'):  # handle np.array, pd.Series, list, and array-like\n        input_dir_str = ','.join(_clean_direction(input_dir, preprocess=True))\n        abb_dir_str = _abbrieviate_direction(input_dir_str)\n        abb_dirs = _clean_direction(abb_dir_str.split(','))\n    else:  # handle unrecognizable scalar\n        return np.nan\n\n    return itemgetter(*abb_dirs)(DIR_DICT)",
  "def _clean_direction(dir_list, preprocess=False):\n    \"\"\"Handle None if preprocess, else handles anything not in DIR_STRS.\"\"\"\n    if preprocess:  # primarily to remove None from list so ','.join works\n        return [UND if not isinstance(the_dir, str) else the_dir\n                for the_dir in dir_list]\n    else:  # remove extraneous abbreviated directions\n        return [UND if the_dir not in DIR_STRS else the_dir\n                for the_dir in dir_list]",
  "def _abbrieviate_direction(ext_dir_str):\n    \"\"\"Convert extended (non-abbreviated) directions to abbreviation.\"\"\"\n    return (ext_dir_str\n            .upper()\n            .replace('_', '')\n            .replace('-', '')\n            .replace(' ', '')\n            .replace('NORTH', 'N')\n            .replace('EAST', 'E')\n            .replace('SOUTH', 'S')\n            .replace('WEST', 'W')\n            )",
  "def angle_to_direction(input_angle, full=False, level=3):\n    \"\"\"Convert the meteorological angle to directional text.\n\n    Works for angles greater than or equal to 360 (360 -> N | 405 -> NE)\n    and rounds to the nearest angle (355 -> N | 404 -> NNE)\n\n    Parameters\n    ----------\n    input_angle : float or array-like\n        Angles such as 0, 25, 45, 360, 410, etc.\n    full : bool\n        True returns full text (South), False returns abbreviated text (S)\n    level : int\n        Level of detail (3 = N/NNE/NE/ENE/E... 2 = N/NE/E/SE... 1 = N/E/S/W)\n\n    Returns\n    -------\n    direction\n        The directional text\n\n    \"\"\"\n    try:  # strip units temporarily\n        origin_units = input_angle.units\n        input_angle = input_angle.m\n    except AttributeError:  # no units associated\n        origin_units = units.degree\n\n    if not hasattr(input_angle, '__len__') or isinstance(input_angle, str):\n        input_angle = [input_angle]\n        scalar = True\n    else:\n        scalar = False\n\n    # clean any numeric strings, negatives, and None does not handle strings with alphabet\n    input_angle = units.Quantity(np.array(input_angle).astype(float), origin_units)\n    input_angle[input_angle < 0] = units.Quantity(np.nan, origin_units)\n\n    # normalizer used for angles > 360 degree to normalize between 0 - 360\n    normalizer = np.array(input_angle.m / MAX_DEGREE_ANGLE.m, dtype=int)\n    norm_angles = abs(input_angle - MAX_DEGREE_ANGLE * normalizer)\n\n    if level == 3:\n        nskip = 1\n    elif level == 2:\n        nskip = 2\n    elif level == 1:\n        nskip = 4\n    else:\n        err_msg = 'Level of complexity cannot be less than 1 or greater than 3!'\n        raise ValueError(err_msg)\n\n    angle_dict = {i * BASE_DEGREE_MULTIPLIER.m * nskip: dir_str\n                  for i, dir_str in enumerate(DIR_STRS[::nskip])}\n    angle_dict[MAX_DEGREE_ANGLE.m] = 'N'  # handle edge case of 360.\n    angle_dict[UND_ANGLE] = UND\n\n    # round to the nearest angles for dict lookup\n    # 0.001 is subtracted so there's an equal number of dir_str from\n    # np.arange(0, 360, 22.5), or else some dir_str will be preferred\n\n    # without the 0.001, level=2 would yield:\n    # ['N', 'N', 'NE', 'E', 'E', 'E', 'SE', 'S', 'S',\n    #  'S', 'SW', 'W', 'W', 'W', 'NW', 'N']\n\n    # with the -0.001, level=2 would yield:\n    # ['N', 'N', 'NE', 'NE', 'E', 'E', 'SE', 'SE',\n    #  'S', 'S', 'SW', 'SW', 'W', 'W', 'NW', 'NW']\n\n    multiplier = np.round(\n        (norm_angles / BASE_DEGREE_MULTIPLIER / nskip) - 0.001).m\n    round_angles = (multiplier * BASE_DEGREE_MULTIPLIER.m * nskip)\n    round_angles[np.where(np.isnan(round_angles))] = UND_ANGLE\n\n    dir_str_arr = itemgetter(*round_angles)(angle_dict)  # for array\n    if not full:\n        return dir_str_arr\n\n    dir_str_arr = ','.join(dir_str_arr)\n    dir_str_arr = _unabbrieviate_direction(dir_str_arr)\n    return dir_str_arr.replace(',', ' ') if scalar else dir_str_arr.split(',')",
  "def _unabbrieviate_direction(abb_dir_str):\n    \"\"\"Convert abbrieviated directions to non-abbrieviated direction.\"\"\"\n    return (abb_dir_str\n            .upper()\n            .replace(UND, 'Undefined ')\n            .replace('N', 'North ')\n            .replace('E', 'East ')\n            .replace('S', 'South ')\n            .replace('W', 'West ')\n            .replace(' ,', ',')\n            ).strip()",
  "def _remove_nans(*variables):\n    \"\"\"Remove NaNs from arrays that cause issues with calculations.\n\n    Takes a variable number of arguments and returns masked arrays in the same\n    order as provided.\n    \"\"\"\n    mask = None\n    for v in variables:\n        if mask is None:\n            mask = np.isnan(v)\n        else:\n            mask |= np.isnan(v)\n\n    # Mask everyone with that joint mask\n    ret = []\n    for v in variables:\n        ret.append(v[~mask])\n    return ret",
  "def take(indexer):\n        return tuple(indexer if slice_dim % ndims == i else slice(None)  # noqa: S001\n                     for i in range(ndims))",
  "def wrapper(f, **kwargs):\n        if 'x' in kwargs or 'delta' in kwargs:\n            # Use the usual DataArray to pint.Quantity preprocessing wrapper\n            return preprocess_and_wrap()(func)(f, **kwargs)\n        elif isinstance(f, xr.DataArray):\n            # Get axis argument, defaulting to first dimension\n            axis = f.metpy.find_axis_name(kwargs.get('axis', 0))\n\n            # Initialize new kwargs with the axis number\n            new_kwargs = {'axis': f.get_axis_num(axis)}\n\n            if check_axis(f[axis], 'time'):\n                # Time coordinate, need to get time deltas\n                new_kwargs['delta'] = f[axis].metpy.time_deltas\n            elif check_axis(f[axis], 'longitude'):\n                # Longitude coordinate, need to get grid deltas\n                new_kwargs['delta'], _ = grid_deltas_from_dataarray(f)\n            elif check_axis(f[axis], 'latitude'):\n                # Latitude coordinate, need to get grid deltas\n                _, new_kwargs['delta'] = grid_deltas_from_dataarray(f)\n            else:\n                # General coordinate, use as is\n                new_kwargs['x'] = f[axis].metpy.unit_array\n\n            # Calculate and return result as a DataArray\n            result = func(f.metpy.unit_array, **new_kwargs)\n            return xr.DataArray(result, coords=f.coords, dims=f.dims)\n        else:\n            # Error\n            raise ValueError('Must specify either \"x\" or \"delta\" for value positions when \"f\" '\n                             'is not a DataArray.')",
  "def wrapper(*args, **kwargs):\n        bound_args = newsig.bind(*args, **kwargs)\n        bound_args.apply_defaults()\n        scale_lat = latitude = bound_args.arguments.pop('latitude')\n        scale_lon = longitude = bound_args.arguments.pop('longitude')\n        crs = bound_args.arguments.pop('crs')\n\n        # Choose the first DataArray argument to act as grid prototype\n        grid_prototype = next(dataarray_arguments(bound_args), None)\n\n        # Fill in x_dim/y_dim\n        if (\n            grid_prototype is not None\n            and 'x_dim' in bound_args.arguments\n            and 'y_dim' in bound_args.arguments\n        ):\n            try:\n                bound_args.arguments['x_dim'] = grid_prototype.metpy.find_axis_number('x')\n                bound_args.arguments['y_dim'] = grid_prototype.metpy.find_axis_number('y')\n            except AttributeError:\n                # If axis number not found, fall back to default but warn.\n                _warnings.warn('Horizontal dimension numbers not found. Defaulting to '\n                               '(..., Y, X) order.')\n\n        # Fill in vertical_dim\n        if (\n            grid_prototype is not None\n            and 'vertical_dim' in bound_args.arguments\n        ):\n            try:\n                bound_args.arguments['vertical_dim'] = (\n                    grid_prototype.metpy.find_axis_number('vertical')\n                )\n            except AttributeError:\n                # If axis number not found, fall back to default but warn.\n                _warnings.warn(\n                    'Vertical dimension number not found. Defaulting to (..., Z, Y, X) order.'\n                )\n\n        # Fill in dz\n        if (\n            grid_prototype is not None\n            and 'dz' in bound_args.arguments\n        ):\n            if bound_args.arguments['dz'] is None:\n                try:\n                    vertical_coord = grid_prototype.metpy.vertical\n                    bound_args.arguments['dz'] = np.diff(vertical_coord.metpy.unit_array)\n                except (AttributeError, ValueError):\n                    # Skip, since this only comes up in advection, where dz is optional\n                    # (may not need vertical at all)\n                    pass\n            if (\n                func.__name__.endswith('advection')\n                and bound_args.arguments['u'] is None\n                and bound_args.arguments['v'] is None\n            ):\n                return func(*bound_args.args, **bound_args.kwargs)\n\n        # Fill in dx and dy\n        if (\n            'dx' in bound_args.arguments and bound_args.arguments['dx'] is None\n            and 'dy' in bound_args.arguments and bound_args.arguments['dy'] is None\n        ):\n            if grid_prototype is not None:\n                grid_deltas = grid_prototype.metpy.grid_deltas\n                bound_args.arguments['dx'] = grid_deltas['dx']\n                bound_args.arguments['dy'] = grid_deltas['dy']\n            elif longitude is not None and latitude is not None and crs is not None:\n                # TODO: de-duplicate .metpy.grid_deltas code\n                geod = None if crs is None else crs.get_geod()\n                bound_args.arguments['dx'], bound_args.arguments['dy'] = (\n                    nominal_lat_lon_grid_deltas(longitude, latitude, geod)\n                )\n            elif 'dz' in bound_args.arguments:\n                # Handle advection case, allowing dx/dy to be None but dz to not be None\n                if bound_args.arguments['dz'] is None:\n                    raise ValueError(\n                        'Must provide dx, dy, and/or dz arguments or input DataArray with '\n                        'interpretable dimension coordinates.'\n                    )\n            else:\n                raise ValueError(\n                    'Must provide dx/dy arguments, input DataArray with interpretable '\n                    'dimension coordinates, or 1D longitude/latitude arguments with an '\n                    'optional PyProj CRS.'\n                )\n\n        # Fill in parallel_scale and meridional_scale\n        if (\n            'parallel_scale' in bound_args.arguments\n            and bound_args.arguments['parallel_scale'] is None\n            and 'meridional_scale' in bound_args.arguments\n            and bound_args.arguments['meridional_scale'] is None\n        ):\n            proj = None\n            if grid_prototype is not None:\n                # Fall back to basic cartesian calculation if we don't have a CRS or we\n                # are unable to get the coordinates needed for map factor calculation\n                # (either existing lat/lon or lat/lon computed from y/x)\n                with contextlib.suppress(AttributeError):\n                    latitude, longitude = grid_prototype.metpy.coordinates('latitude',\n                                                                           'longitude')\n                    scale_lat = latitude.metpy.unit_array\n                    scale_lon = longitude.metpy.unit_array\n                    if hasattr(grid_prototype.metpy, 'pyproj_proj'):\n                        proj = grid_prototype.metpy.pyproj_proj\n                    elif latitude.squeeze().ndim == 1 and longitude.squeeze().ndim == 1:\n                        proj = Proj(CRS('+proj=latlon'))\n            elif latitude is not None and longitude is not None:\n                try:\n                    proj = Proj(crs)\n                except Exception as e:\n                    # Whoops, intended to use\n                    raise ValueError(\n                        'Latitude and longitude arguments provided so as to make '\n                        'calculation projection-correct, however, projection CRS is '\n                        'missing or invalid.'\n                    ) from e\n\n            # Do we have everything we need to sensibly calculate the scale arrays?\n            if proj is not None:\n                scale_lat = scale_lat.squeeze().m_as('degrees')\n                scale_lon = scale_lon.squeeze().m_as('degrees')\n                if scale_lat.ndim == 1 and scale_lon.ndim == 1:\n                    scale_lon, scale_lat = np.meshgrid(scale_lon, scale_lat)\n                elif scale_lat.ndim != 2 or scale_lon.ndim != 2:\n                    raise ValueError('Latitude and longitude must be either 1D or 2D.')\n                factors = proj.get_factors(scale_lon, scale_lat)\n                p_scale = factors.parallel_scale\n                m_scale = factors.meridional_scale\n\n                if grid_prototype is not None:\n                    # Set the dims and coords using the original from the input lat/lon.\n                    # This particular implementation relies on them being 1D/2D for the dims.\n                    xr_kwargs = {'coords': {**latitude.coords, **longitude.coords},\n                                 'dims': (latitude.dims[0], longitude.dims[-1])}\n                    p_scale = xr.DataArray(p_scale, **xr_kwargs)\n                    m_scale = xr.DataArray(m_scale, **xr_kwargs)\n\n                bound_args.arguments['parallel_scale'] = p_scale\n                bound_args.arguments['meridional_scale'] = m_scale\n\n        # If the original function uses any of the arguments that are otherwise dynamically\n        # added, be sure to pass them to the original function.\n        local_namespace = vars()\n        bound_args.arguments.update({param: local_namespace[param]\n                                     for param, uses in orig_func_uses.items() if uses})\n\n        return func(*bound_args.args, **bound_args.kwargs)",
  "def _check_length(positions):\n        if axes_given and len(positions) < len(axes):\n            raise ValueError('Length of \"coordinates\" or \"deltas\" cannot be less than that '\n                             'of \"axes\".')\n        elif not axes_given and len(positions) != len(axes):\n            raise ValueError('Length of \"coordinates\" or \"deltas\" must match the number of '\n                             'dimensions of \"f\" when \"axes\" is not given.')",
  "def wind_speed(u, v):\n    r\"\"\"Compute the wind speed from u and v-components.\n\n    Parameters\n    ----------\n    u : `pint.Quantity`\n        Wind component in the X (East-West) direction\n    v : `pint.Quantity`\n        Wind component in the Y (North-South) direction\n\n    Returns\n    -------\n    wind speed: `pint.Quantity`\n        Speed of the wind\n\n    See Also\n    --------\n    wind_components\n\n    \"\"\"\n    return np.hypot(u, v)",
  "def wind_direction(u, v, convention='from'):\n    r\"\"\"Compute the wind direction from u and v-components.\n\n    Parameters\n    ----------\n    u : `pint.Quantity`\n        Wind component in the X (East-West) direction\n    v : `pint.Quantity`\n        Wind component in the Y (North-South) direction\n    convention : str\n        Convention to return direction; 'from' returns the direction the wind is coming from\n        (meteorological convention), 'to' returns the direction the wind is going towards\n        (oceanographic convention), default is 'from'.\n\n    Returns\n    -------\n    direction: `pint.Quantity`\n        The direction of the wind in intervals [0, 360] degrees, with 360 being North,\n        direction defined by the convention kwarg.\n\n    See Also\n    --------\n    wind_components\n\n    Notes\n    -----\n    In the case of calm winds (where `u` and `v` are zero), this function returns a direction\n    of 0.\n\n    \"\"\"\n    wdir = units.Quantity(90., 'deg') - np.arctan2(-v, -u)\n    origshape = wdir.shape\n    wdir = np.atleast_1d(wdir)\n\n    # Handle oceanographic convection\n    if convention == 'to':\n        wdir -= units.Quantity(180., 'deg')\n    elif convention not in ('to', 'from'):\n        raise ValueError('Invalid kwarg for \"convention\". Valid options are \"from\" or \"to\".')\n\n    mask = np.array(wdir <= 0)\n    if np.any(mask):\n        wdir[mask] += units.Quantity(360., 'deg')\n    # avoid unintended modification of `pint.Quantity` by direct use of magnitude\n    calm_mask = (np.asanyarray(u.magnitude) == 0.) & (np.asanyarray(v.magnitude) == 0.)\n\n    # np.any check required for legacy numpy which treats 0-d False boolean index as zero\n    if np.any(calm_mask):\n        wdir[calm_mask] = units.Quantity(0., 'deg')\n    return wdir.reshape(origshape).to('degrees')",
  "def wind_components(speed, wind_direction):\n    r\"\"\"Calculate the U, V wind vector components from the speed and direction.\n\n    Parameters\n    ----------\n    speed : `pint.Quantity`\n        Wind speed (magnitude)\n    wind_direction : `pint.Quantity`\n        Wind direction, specified as the direction from which the wind is\n        blowing (0-2 pi radians or 0-360 degrees), with 360 degrees being North.\n\n    Returns\n    -------\n    u, v : tuple of `pint.Quantity`\n        The wind components in the X (East-West) and Y (North-South)\n        directions, respectively.\n\n    See Also\n    --------\n    wind_speed\n    wind_direction\n\n    Examples\n    --------\n    >>> from metpy.calc import wind_components\n    >>> from metpy.units import units\n    >>> wind_components(10. * units('m/s'), 225. * units.deg)\n     (<Quantity(7.07106781, 'meter / second')>, <Quantity(7.07106781, 'meter / second')>)\n\n    .. versionchanged:: 1.0\n       Renamed ``wdir`` parameter to ``wind_direction``\n\n    \"\"\"\n    wind_direction = _check_radians(wind_direction, max_radians=4 * np.pi)\n    u = -speed * np.sin(wind_direction)\n    v = -speed * np.cos(wind_direction)\n    return u, v",
  "def windchill(temperature, speed, face_level_winds=False, mask_undefined=True):\n    r\"\"\"Calculate the Wind Chill Temperature Index (WCTI).\n\n    Calculates WCTI from the current temperature and wind speed using the formula\n    outlined by the FCM [FCMR192003]_.\n\n    Specifically, these formulas assume that wind speed is measured at\n    10m.  If, instead, the speeds are measured at face level, the winds\n    need to be multiplied by a factor of 1.5 (this can be done by specifying\n    `face_level_winds` as `True`).\n\n    Parameters\n    ----------\n    temperature : `pint.Quantity`\n        Air temperature\n    speed : `pint.Quantity`\n        Wind speed at 10m. If instead the winds are at face level,\n        `face_level_winds` should be set to `True` and the 1.5 multiplicative\n        correction will be applied automatically.\n    face_level_winds : bool, optional\n        A flag indicating whether the wind speeds were measured at facial\n        level instead of 10m, thus requiring a correction.  Defaults to\n        `False`.\n    mask_undefined : bool, optional\n        A flag indicating whether a masked array should be returned with\n        values where wind chill is undefined masked.  These are values where\n        the temperature > 50F or wind speed <= 3 miles per hour. Defaults\n        to `True`.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding Wind Chill Temperature Index value(s)\n\n    See Also\n    --------\n    heat_index, apparent_temperature\n\n    \"\"\"\n    # Correct for lower height measurement of winds if necessary\n    if face_level_winds:\n        # No in-place so that we copy\n        # noinspection PyAugmentAssignment\n        speed = speed * 1.5\n\n    temp_limit, speed_limit = units.Quantity(10., 'degC'), units.Quantity(3, 'mph')\n    speed_factor = speed.to('km/hr').magnitude ** 0.16\n    wcti = units.Quantity((0.6215 + 0.3965 * speed_factor) * temperature.to('degC').magnitude\n                          - 11.37 * speed_factor + 13.12, units.degC).to(temperature.units)\n\n    # See if we need to mask any undefined values\n    if mask_undefined:\n        mask = np.array((temperature > temp_limit) | (speed <= speed_limit))\n        if mask.any():\n            wcti = masked_array(wcti, mask=mask)\n\n    return wcti",
  "def heat_index(temperature, relative_humidity, mask_undefined=True):\n    r\"\"\"Calculate the Heat Index from the current temperature and relative humidity.\n\n    The implementation uses the formula outlined in [Rothfusz1990]_, which is a\n    multi-variable least-squares regression of the values obtained in [Steadman1979]_.\n    Additional conditional corrections are applied to match what the National\n    Weather Service operationally uses. See Figure 3 of [Anderson2013]_ for a\n    depiction of this algorithm and further discussion.\n\n    Parameters\n    ----------\n    temperature : `pint.Quantity`\n        Air temperature\n    relative_humidity : `pint.Quantity`\n        The relative humidity expressed as a unitless ratio in the range [0, 1].\n        Can also pass a percentage if proper units are attached.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding Heat Index value(s)\n\n    Other Parameters\n    ----------------\n    mask_undefined : bool, optional\n        A flag indicating whether a masked array should be returned with\n        values masked where the temperature < 80F. Defaults to `True`.\n\n\n    .. versionchanged:: 1.0\n       Renamed ``rh`` parameter to ``relative_humidity``\n\n    See Also\n    --------\n    windchill, apparent_temperature\n\n    \"\"\"\n    temperature = np.atleast_1d(temperature)\n    relative_humidity = np.atleast_1d(relative_humidity)\n    # assign units to relative_humidity if they currently are not present\n    if not hasattr(relative_humidity, 'units'):\n        relative_humidity = units.Quantity(relative_humidity, 'dimensionless')\n    delta = temperature.to(units.degF) - units.Quantity(0., 'degF')\n    rh2 = relative_humidity**2\n    delta2 = delta**2\n\n    # Simplified Heat Index -- constants converted for relative_humidity in [0, 1]\n    a = (units.Quantity(-10.3, 'degF') + 1.1 * delta\n         + units.Quantity(4.7, 'delta_degF') * relative_humidity)\n\n    # More refined Heat Index -- constants converted for relative_humidity in [0, 1]\n    b = (units.Quantity(-42.379, 'degF')\n         + 2.04901523 * delta\n         + units.Quantity(1014.333127, 'delta_degF') * relative_humidity\n         - 22.475541 * delta * relative_humidity\n         - units.Quantity(6.83783e-3, '1/delta_degF') * delta2\n         - units.Quantity(5.481717e2, 'delta_degF') * rh2\n         + units.Quantity(1.22874e-1, '1/delta_degF') * delta2 * relative_humidity\n         + 8.5282 * delta * rh2\n         - units.Quantity(1.99e-2, '1/delta_degF') * delta2 * rh2)\n\n    # Create return heat index\n    hi = units.Quantity(np.full(np.shape(temperature), np.nan), 'degF')\n    # Retain masked status of temperature with resulting heat index\n    if hasattr(temperature, 'mask'):\n        hi = masked_array(hi)\n\n    # If T <= 40F, Heat Index is T\n    sel = np.array(temperature <= units.Quantity(40., 'degF'))\n    if np.any(sel):\n        hi[sel] = temperature[sel].to(units.degF)\n\n    # If a < 79F and hi is unset, Heat Index is a\n    sel = np.array(a < units.Quantity(79., 'degF')) & np.isnan(hi)\n    if np.any(sel):\n        hi[sel] = a[sel]\n\n    # Use b now for anywhere hi has yet to be set\n    sel = np.isnan(hi)\n    if np.any(sel):\n        hi[sel] = b[sel]\n\n    # Adjustment for RH <= 13% and 80F <= T <= 112F\n    sel = np.array((relative_humidity <= units.Quantity(13., 'percent'))\n                   & (temperature >= units.Quantity(80., 'degF'))\n                   & (temperature <= units.Quantity(112., 'degF')))\n    if np.any(sel):\n        rh15adj = ((13. - relative_humidity[sel] * 100.) / 4.\n                   * np.sqrt((units.Quantity(17., 'delta_degF')\n                              - np.abs(delta[sel] - units.Quantity(95., 'delta_degF')))\n                             / units.Quantity(17., '1/delta_degF')))\n        hi[sel] = hi[sel] - rh15adj\n\n    # Adjustment for RH > 85% and 80F <= T <= 87F\n    sel = np.array((relative_humidity > units.Quantity(85., 'percent'))\n                   & (temperature >= units.Quantity(80., 'degF'))\n                   & (temperature <= units.Quantity(87., 'degF')))\n    if np.any(sel):\n        rh85adj = (0.02 * (relative_humidity[sel] * 100. - 85.)\n                   * (units.Quantity(87., 'delta_degF') - delta[sel]))\n        hi[sel] = hi[sel] + rh85adj\n\n    # See if we need to mask any undefined values\n    if mask_undefined:\n        mask = np.array(temperature < units.Quantity(80., 'degF'))\n        if mask.any():\n            hi = masked_array(hi, mask=mask)\n\n    return hi",
  "def apparent_temperature(temperature, relative_humidity, speed, face_level_winds=False,\n                         mask_undefined=True):\n    r\"\"\"Calculate the current apparent temperature.\n\n    Calculates the current apparent temperature based on the wind chill or heat index\n    as appropriate for the current conditions. Follows [NWS10201]_.\n\n    Parameters\n    ----------\n    temperature : `pint.Quantity`\n        Air temperature\n    relative_humidity : `pint.Quantity`\n        Relative humidity expressed as a unitless ratio in the range [0, 1].\n        Can also pass a percentage if proper units are attached.\n    speed : `pint.Quantity`\n        Wind speed at 10m.  If instead the winds are at face level,\n        `face_level_winds` should be set to `True` and the 1.5 multiplicative\n        correction will be applied automatically.\n    face_level_winds : bool, optional\n        A flag indicating whether the wind speeds were measured at facial\n        level instead of 10m, thus requiring a correction.  Defaults to\n        `False`.\n    mask_undefined : bool, optional\n        A flag indicating whether a masked array should be returned with\n        values where wind chill or heat_index is undefined masked. For wind\n        chill, these are values where the temperature > 50F or\n        wind speed <= 3 miles per hour. For heat index, these are values\n        where the temperature < 80F.\n        Defaults to `True`.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding apparent temperature value(s)\n\n\n    .. versionchanged:: 1.0\n       Renamed ``rh`` parameter to ``relative_humidity``\n\n    See Also\n    --------\n    heat_index, windchill\n\n    \"\"\"\n    is_not_scalar = hasattr(temperature, '__len__')\n\n    temperature = np.atleast_1d(temperature)\n    relative_humidity = np.atleast_1d(relative_humidity)\n    speed = np.atleast_1d(speed)\n\n    # NB: mask_defined=True is needed to know where computed values exist\n    wind_chill_temperature = windchill(temperature, speed, face_level_winds=face_level_winds,\n                                       mask_undefined=True).to(temperature.units)\n\n    heat_index_temperature = heat_index(temperature, relative_humidity,\n                                        mask_undefined=True).to(temperature.units)\n\n    # Combine the heat index and wind chill arrays (no point has a value in both)\n    # NB: older numpy.ma.where does not return a masked array\n    app_temperature = masked_array(\n        np.ma.where(masked_array(wind_chill_temperature).mask,\n                    heat_index_temperature.to(temperature.units),\n                    wind_chill_temperature.to(temperature.units)\n                    ), temperature.units)\n\n    # If mask_undefined is False, then set any masked values to the temperature\n    if not mask_undefined:\n        app_temperature[app_temperature.mask] = temperature[app_temperature.mask]\n\n    # If no values are masked and provided temperature does not have a mask\n    # we should return a non-masked array\n    if not np.any(app_temperature.mask) and not hasattr(temperature, 'mask'):\n        app_temperature = units.Quantity(np.array(app_temperature.m), temperature.units)\n\n    if is_not_scalar:\n        return app_temperature\n    else:\n        return np.atleast_1d(app_temperature)[0]",
  "def pressure_to_height_std(pressure):\n    r\"\"\"Convert pressure data to height using the U.S. standard atmosphere [NOAA1976]_.\n\n    The implementation uses the formula outlined in [Hobbs1977]_ pg.60-61.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding height value(s)\n\n    Notes\n    -----\n    .. math:: Z = \\frac{T_0}{\\Gamma}[1-\\frac{p}{p_0}^\\frac{R\\Gamma}{g}]\n\n    \"\"\"\n    return (t0 / gamma) * (1 - (pressure / p0).to('dimensionless')**(\n        mpconsts.Rd * gamma / mpconsts.g))",
  "def height_to_geopotential(height):\n    r\"\"\"Compute geopotential for a given height above sea level.\n\n    Calculates the geopotential from height above mean sea level using the following formula,\n    which is derived from the definition of geopotential as given in [Hobbs2006]_ Pg. 69 Eq\n    3.21, along with an approximation for variation of gravity with altitude:\n\n    .. math:: \\Phi = \\frac{g R_e z}{R_e + z}\n\n    (where :math:`\\Phi` is geopotential, :math:`z` is height, :math:`R_e` is average Earth\n    radius, and :math:`g` is standard gravity).\n\n\n    Parameters\n    ----------\n    height : `pint.Quantity`\n        Height above sea level\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding geopotential value(s)\n\n    Examples\n    --------\n    >>> import metpy.calc\n    >>> from metpy.units import units\n    >>> height = np.linspace(0, 10000, num=11) * units.m\n    >>> geopot = metpy.calc.height_to_geopotential(height)\n    >>> geopot\n    <Quantity([     0.           9805.11097983 19607.1448853  29406.10316465\n    39201.98726524 48994.79863351 58784.53871501 68571.20895435\n    78354.81079527 88135.34568058 97912.81505219], 'meter ** 2 / second ** 2')>\n\n    See Also\n    --------\n    geopotential_to_height\n\n    Notes\n    -----\n    This calculation approximates :math:`g(z)` as\n\n    .. math:: g(z) = g_0 \\left( \\frac{R_e}{R_e + z} \\right)^2\n\n    where :math:`g_0` is standard gravity. It thereby accounts for the average effects of\n    centrifugal force on apparent gravity, but neglects latitudinal variations due to\n    centrifugal force and Earth's eccentricity.\n\n    (Prior to MetPy v0.11, this formula instead calculated :math:`g(z)` from Newton's Law of\n    Gravitation assuming a spherical Earth and no centrifugal force effects).\n\n    \"\"\"\n    return (mpconsts.g * mpconsts.Re * height) / (mpconsts.Re + height)",
  "def geopotential_to_height(geopotential):\n    r\"\"\"Compute height above sea level from a given geopotential.\n\n    Calculates the height above mean sea level from geopotential using the following formula,\n    which is derived from the definition of geopotential as given in [Hobbs2006]_ Pg. 69 Eq\n    3.21, along with an approximation for variation of gravity with altitude:\n\n    .. math:: z = \\frac{\\Phi R_e}{gR_e - \\Phi}\n\n    (where :math:`\\Phi` is geopotential, :math:`z` is height, :math:`R_e` is average Earth\n    radius, and :math:`g` is standard gravity).\n\n\n    Parameters\n    ----------\n    geopotential : `pint.Quantity`\n        Geopotential\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding value(s) of height above sea level\n\n    Examples\n    --------\n    >>> import metpy.calc\n    >>> from metpy.units import units\n    >>> geopot = units.Quantity([0., 9805., 19607., 29406.], 'm^2/s^2')\n    >>> height = metpy.calc.geopotential_to_height(geopot)\n    >>> height\n    <Quantity([   0.          999.98867965 1999.98521653 2999.98947022], 'meter')>\n\n    See Also\n    --------\n    height_to_geopotential\n\n    Notes\n    -----\n    This calculation approximates :math:`g(z)` as\n\n    .. math:: g(z) = g_0 \\left( \\frac{R_e}{R_e + z} \\right)^2\n\n    where :math:`g_0` is standard gravity. It thereby accounts for the average effects of\n    centrifugal force on apparent gravity, but neglects latitudinal variations due to\n    centrifugal force and Earth's eccentricity.\n\n    (Prior to MetPy v0.11, this formula instead calculated :math:`g(z)` from Newton's Law of\n    Gravitation assuming a spherical Earth and no centrifugal force effects.)\n\n    .. versionchanged:: 1.0\n       Renamed ``geopot`` parameter to ``geopotential``\n\n    \"\"\"\n    return (geopotential * mpconsts.Re) / (mpconsts.g * mpconsts.Re - geopotential)",
  "def height_to_pressure_std(height):\n    r\"\"\"Convert height data to pressures using the U.S. standard atmosphere [NOAA1976]_.\n\n    The implementation inverts the formula outlined in [Hobbs1977]_ pg.60-61.\n\n    Parameters\n    ----------\n    height : `pint.Quantity`\n        Atmospheric height\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding pressure value(s)\n\n    Notes\n    -----\n    .. math:: p = p_0 e^{\\frac{g}{R \\Gamma} \\text{ln}(1-\\frac{Z \\Gamma}{T_0})}\n\n    \"\"\"\n    return p0 * (1 - (gamma / t0) * height) ** (mpconsts.g / (mpconsts.Rd * gamma))",
  "def coriolis_parameter(latitude):\n    r\"\"\"Calculate the Coriolis parameter at each point.\n\n    The implementation uses the formula outlined in [Hobbs1977]_ pg.370-371.\n\n    Parameters\n    ----------\n    latitude : array-like\n        Latitude at each point\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding Coriolis force at each point\n\n    \"\"\"\n    latitude = _check_radians(latitude, max_radians=np.pi / 2)\n    return (2. * mpconsts.omega * np.sin(latitude)).to('1/s')",
  "def add_height_to_pressure(pressure, height):\n    r\"\"\"Calculate the pressure at a certain height above another pressure level.\n\n    This assumes a standard atmosphere [NOAA1976]_.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressure level\n    height : `pint.Quantity`\n        Height above a pressure level\n\n    Examples\n    --------\n    >>> from metpy.calc import add_height_to_pressure\n    >>> from metpy.units import units\n    >>> add_height_to_pressure(1000 * units.hPa, 500 * units.meters)\n    <Quantity(941.953016, 'hectopascal')>\n\n    Returns\n    -------\n    `pint.Quantity`\n        Corresponding pressure value for the height above the pressure level\n\n    See Also\n    --------\n    pressure_to_height_std, height_to_pressure_std, add_pressure_to_height\n\n    \"\"\"\n    pressure_level_height = pressure_to_height_std(pressure)\n    return height_to_pressure_std(pressure_level_height + height)",
  "def add_pressure_to_height(height, pressure):\n    r\"\"\"Calculate the height at a certain pressure above another height.\n\n    This assumes a standard atmosphere [NOAA1976]_.\n\n    Parameters\n    ----------\n    height : `pint.Quantity`\n        Height level\n    pressure : `pint.Quantity`\n        Pressure above height level\n\n    Returns\n    -------\n    `pint.Quantity`\n        The corresponding height value for the pressure above the height level\n\n    Examples\n    --------\n    >>> from metpy.calc import add_pressure_to_height\n    >>> from metpy.units import units\n    >>> add_pressure_to_height(1000 * units.meters, 100 * units.hPa)\n    <Quantity(1.96117548, 'kilometer')>\n\n    See Also\n    --------\n    pressure_to_height_std, height_to_pressure_std, add_height_to_pressure\n\n    \"\"\"\n    pressure_at_height = height_to_pressure_std(height)\n    return pressure_to_height_std(pressure_at_height - pressure)",
  "def sigma_to_pressure(sigma, pressure_sfc, pressure_top):\n    r\"\"\"Calculate pressure from sigma values.\n\n    Parameters\n    ----------\n    sigma : numpy.ndarray\n        Sigma levels to be converted to pressure levels\n\n    pressure_sfc : `pint.Quantity`\n        Surface pressure value\n\n    pressure_top : `pint.Quantity`\n        Pressure value at the top of the model domain\n\n    Returns\n    -------\n    `pint.Quantity`\n        Pressure values at the given sigma levels\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from metpy.calc import sigma_to_pressure\n    >>> from metpy.units import units\n    >>> sigma_levs = np.linspace(0, 1, 10)\n    >>> sigma_to_pressure(sigma_levs, 1000 * units.hPa, 10 * units.hPa)\n    <Quantity([  10.  120.  230.  340.  450.  560.  670.  780.  890. 1000.], 'hectopascal')>\n\n    Notes\n    -----\n    Sigma definition adapted from [Philips1957]_:\n\n    .. math:: p = \\sigma * (p_{sfc} - p_{top}) + p_{top}\n\n    * :math:`p` is pressure at a given `\\sigma` level\n    * :math:`\\sigma` is non-dimensional, scaled pressure\n    * :math:`p_{sfc}` is pressure at the surface or model floor\n    * :math:`p_{top}` is pressure at the top of the model domain\n\n    .. versionchanged:: 1.0\n       Renamed ``psfc``, ``ptop`` parameters to ``pressure_sfc``, ``pressure_top``\n\n    \"\"\"\n    if np.any(sigma < 0) or np.any(sigma > 1):\n        raise ValueError('Sigma values should be bounded by 0 and 1')\n\n    if pressure_sfc.magnitude < 0 or pressure_top.magnitude < 0:\n        raise ValueError('Pressure input should be non-negative')\n\n    return sigma * (pressure_sfc - pressure_top) + pressure_top",
  "def smooth_gaussian(scalar_grid, n):\n    \"\"\"Filter with normal distribution of weights.\n\n    Parameters\n    ----------\n    scalar_grid : `pint.Quantity`\n        Some n-dimensional scalar grid. If more than two axes, smoothing\n        is only done across the last two.\n\n    n : int\n        Degree of filtering\n\n    Returns\n    -------\n    `pint.Quantity`\n        The filtered 2D scalar grid\n\n    Notes\n    -----\n    This function is a close replication of the GEMPAK function ``GWFS``,\n    but is not identical.  The following notes are incorporated from\n    the GEMPAK source code:\n\n    This function smooths a scalar grid using a moving average\n    low-pass filter whose weights are determined by the normal\n    (Gaussian) probability distribution function for two dimensions.\n    The weight given to any grid point within the area covered by the\n    moving average for a target grid point is proportional to:\n\n    .. math:: e^{-D^2}\n\n    where D is the distance from that point to the target point divided\n    by the standard deviation of the normal distribution.  The value of\n    the standard deviation is determined by the degree of filtering\n    requested.  The degree of filtering is specified by an integer.\n    This integer is the number of grid increments from crest to crest\n    of the wave for which the theoretical response is 1/e = .3679.  If\n    the grid increment is called delta_x, and the value of this integer\n    is represented by N, then the theoretical filter response function\n    value for the N * delta_x wave will be 1/e.  The actual response\n    function will be greater than the theoretical value.\n\n    The larger N is, the more severe the filtering will be, because the\n    response function for all wavelengths shorter than N * delta_x\n    will be less than 1/e.  Furthermore, as N is increased, the slope\n    of the filter response function becomes more shallow; so, the\n    response at all wavelengths decreases, but the amount of decrease\n    lessens with increasing wavelength.  (The theoretical response\n    function can be obtained easily--it is the Fourier transform of the\n    weight function described above.)\n\n    The area of the patch covered by the moving average varies with N.\n    As N gets bigger, the smoothing gets stronger, and weight values\n    farther from the target grid point are larger because the standard\n    deviation of the normal distribution is bigger.  Thus, increasing\n    N has the effect of expanding the moving average window as well as\n    changing the values of weights.  The patch is a square covering all\n    points whose weight values are within two standard deviations of the\n    mean of the two-dimensional normal distribution.\n\n    The key difference between GEMPAK's GWFS and this function is that,\n    in GEMPAK, the leftover weight values representing the fringe of the\n    distribution are applied to the target grid point.  In this\n    function, the leftover weights are not used.\n\n    When this function is invoked, the first argument is the grid to be\n    smoothed, the second is the value of N as described above:\n\n                        GWFS ( S, N )\n\n    where N > 1.  If N <= 1, N = 2 is assumed.  For example, if N = 4,\n    then the 4 delta x wave length is passed with approximate response\n    1/e.\n\n    \"\"\"\n    # Compute standard deviation in a manner consistent with GEMPAK\n    n = int(round(n))\n    n = max(n, 2)\n    sgma = n / (2 * np.pi)\n\n    # Construct sigma sequence so smoothing occurs only in horizontal direction\n    num_ax = len(scalar_grid.shape)\n    # Assume the last two axes represent the horizontal directions\n    sgma_seq = [sgma if i > num_ax - 3 else 0 for i in range(num_ax)]\n\n    filter_args = {'sigma': sgma_seq, 'truncate': 2 * np.sqrt(2)}\n    if hasattr(scalar_grid, 'mask'):\n        smoothed = gaussian_filter(scalar_grid.data, **filter_args)\n        return np.ma.array(smoothed, mask=scalar_grid.mask)\n    else:\n        return gaussian_filter(scalar_grid, **filter_args)",
  "def smooth_window(scalar_grid, window, passes=1, normalize_weights=True):\n    \"\"\"Filter with an arbitrary window smoother.\n\n    Parameters\n    ----------\n    scalar_grid : array-like\n        N-dimensional scalar grid to be smoothed\n\n    window : numpy.ndarray\n        Window to use in smoothing. Can have dimension less than or equal to N. If\n        dimension less than N, the scalar grid will be smoothed along its trailing dimensions.\n        Shape along each dimension must be odd.\n\n    passes : int\n        The number of times to apply the filter to the grid. Defaults to 1.\n\n    normalize_weights : bool\n        If true, divide the values in window by the sum of all values in the window to obtain\n        the normalized smoothing weights. If false, use supplied values directly as the\n        weights.\n\n    Returns\n    -------\n    array-like\n        The filtered scalar grid\n\n    See Also\n    --------\n    smooth_rectangular, smooth_circular, smooth_n_point, smooth_gaussian\n\n    Notes\n    -----\n    This function can be applied multiple times to create a more smoothed field and will only\n    smooth the interior points, leaving the end points with their original values (this\n    function will leave an unsmoothed edge of size `(n - 1) / 2` for each `n` in the shape of\n    `window` around the data). If a masked value or NaN values exists in the array, it will\n    propagate to any point that uses that particular grid point in the smoothing calculation.\n    Applying the smoothing function multiple times will propagate NaNs further throughout the\n    domain.\n\n    \"\"\"\n    def _pad(n):\n        # Return number of entries to pad given length along dimension.\n        return (n - 1) // 2\n\n    def _zero_to_none(x):\n        # Convert zero values to None, otherwise return what is given.\n        return x if x != 0 else None\n\n    def _offset(pad, k):\n        # Return padded slice offset by k entries\n        return slice(_zero_to_none(pad + k), _zero_to_none(-pad + k))\n\n    def _trailing_dims(indexer):\n        # Add ... to the front of an indexer, since we are working with trailing dimensions.\n        return (Ellipsis,) + tuple(indexer)\n\n    # Verify that shape in all dimensions is odd (need to have a neighborhood around a\n    # central point)\n    if any((size % 2 == 0) for size in window.shape):\n        raise ValueError('The shape of the smoothing window must be odd in all dimensions.')\n\n    # Optionally normalize the supplied weighting window\n    if normalize_weights:\n        weights = window / np.sum(window)\n    else:\n        weights = window\n\n    # Set indexes\n    # Inner index for the centered array elements that are affected by the smoothing\n    inner_full_index = _trailing_dims(_offset(_pad(n), 0) for n in weights.shape)\n    # Indexes to iterate over each weight\n    weight_indexes = tuple(product(*(range(n) for n in weights.shape)))\n\n    # Index for full array elements, offset by the weight index\n    def offset_full_index(weight_index):\n        return _trailing_dims(_offset(_pad(n), weight_index[i] - _pad(n))\n                              for i, n in enumerate(weights.shape))\n\n    # TODO: this is not lazy-loading/dask compatible, as it \"densifies\" the data\n    data = np.array(scalar_grid)\n    for _ in range(passes):\n        # Set values corresponding to smoothing weights by summing over each weight and\n        # applying offsets in needed dimensions\n        data[inner_full_index] = sum(weights[index] * data[offset_full_index(index)]\n                                     for index in weight_indexes)\n\n    return data",
  "def smooth_rectangular(scalar_grid, size, passes=1):\n    \"\"\"Filter with a rectangular window smoother.\n\n    Parameters\n    ----------\n    scalar_grid : array-like\n        N-dimensional scalar grid to be smoothed\n\n    size : int or Sequence[int]\n        Shape of rectangle along the trailing dimension(s) of the scalar grid\n\n    passes : int\n        The number of times to apply the filter to the grid. Defaults to 1.\n\n    Returns\n    -------\n    array-like\n        The filtered scalar grid\n\n    See Also\n    --------\n    smooth_window, smooth_circular, smooth_n_point, smooth_gaussian\n\n    Notes\n    -----\n    This function can be applied multiple times to create a more smoothed field and will only\n    smooth the interior points, leaving the end points with their original values (this\n    function will leave an unsmoothed edge of size `(n - 1) / 2` for each `n` in `size` around\n    the data). If a masked value or NaN values exists in the array, it will propagate to any\n    point that uses that particular grid point in the smoothing calculation. Applying the\n    smoothing function multiple times will propagate NaNs further throughout the domain.\n\n    \"\"\"\n    return smooth_window(scalar_grid, np.ones(size), passes=passes)",
  "def smooth_circular(scalar_grid, radius, passes=1):\n    \"\"\"Filter with a circular window smoother.\n\n    Parameters\n    ----------\n    scalar_grid : array-like\n        N-dimensional scalar grid to be smoothed. If more than two axes, smoothing is only\n        done along the last two.\n\n    radius : int\n        Radius of the circular smoothing window. The \"diameter\" of the circle (width of\n        smoothing window) is 2 * radius + 1 to provide a smoothing window with odd shape.\n\n    passes : int\n        The number of times to apply the filter to the grid. Defaults to 1.\n\n    Returns\n    -------\n    array-like\n        The filtered scalar grid\n\n    See Also\n    --------\n    smooth_window, smooth_rectangular, smooth_n_point, smooth_gaussian\n\n    Notes\n    -----\n    This function can be applied multiple times to create a more smoothed field and will only\n    smooth the interior points, leaving the end points with their original values (this\n    function will leave an unsmoothed edge of size `radius` around the data). If a masked\n    value or NaN values exists in the array, it will propagate to any point that uses that\n    particular grid point in the smoothing calculation. Applying the smoothing function\n    multiple times will propagate NaNs further throughout the domain.\n\n    \"\"\"\n    # Generate the circle\n    size = 2 * radius + 1\n    x, y = np.mgrid[:size, :size]\n    distance = np.sqrt((x - radius) ** 2 + (y - radius) ** 2)\n    circle = distance <= radius\n\n    # Apply smoother\n    return smooth_window(scalar_grid, circle, passes=passes)",
  "def smooth_n_point(scalar_grid, n=5, passes=1):\n    \"\"\"Filter with an n-point smoother.\n\n    Parameters\n    ----------\n    scalar_grid : array-like or `pint.Quantity`\n        N-dimensional scalar grid to be smoothed. If more than two axes, smoothing is only\n        done along the last two.\n\n    n: int\n        The number of points to use in smoothing, only valid inputs\n        are 5 and 9. Defaults to 5.\n\n    passes : int\n        The number of times to apply the filter to the grid. Defaults to 1.\n\n    Returns\n    -------\n    array-like or `pint.Quantity`\n        The filtered scalar grid\n\n    See Also\n    --------\n    smooth_window, smooth_rectangular, smooth_circular, smooth_gaussian\n\n    Notes\n    -----\n    This function is a close replication of the GEMPAK function SM5S and SM9S depending on the\n    choice of the number of points to use for smoothing. This function can be applied multiple\n    times to create a more smoothed field and will only smooth the interior points, leaving\n    the end points with their original values (this function will leave an unsmoothed edge of\n    size 1 around the data). If a masked value or NaN values exists in the array, it will\n    propagate to any point that uses that particular grid point in the smoothing calculation.\n    Applying the smoothing function multiple times will propagate NaNs further throughout the\n    domain.\n\n    \"\"\"\n    if n == 9:\n        weights = np.array([[0.0625, 0.125, 0.0625],\n                            [0.125, 0.25, 0.125],\n                            [0.0625, 0.125, 0.0625]])\n    elif n == 5:\n        weights = np.array([[0., 0.125, 0.],\n                            [0.125, 0.5, 0.125],\n                            [0., 0.125, 0.]])\n    else:\n        raise ValueError('The number of points to use in the smoothing '\n                         'calculation must be either 5 or 9.')\n\n    return smooth_window(scalar_grid, window=weights, passes=passes, normalize_weights=False)",
  "def zoom_xarray(input_field, zoom, output=None, order=3, mode='constant', cval=0.0,\n                prefilter=True):\n    \"\"\"Apply a spline interpolation to the data to effectively reduce the grid spacing.\n\n    This function applies `scipy.ndimage.zoom` to increase the number of grid points and\n    effectively reduce the grid spacing over the data provided.\n\n    Parameters\n    ----------\n    input_field  : `xarray.DataArray`\n        The 2D data array to be interpolated.\n\n    zoom : float or Sequence[float]\n        The zoom factor along the axes. If a float, zoom is the same for each axis. If a\n        sequence, zoom should contain one value for each axis.\n\n    order : int, optional\n        The order of the spline interpolation, default is 3. The order has to be in the\n        range 0-5.\n\n    mode : str, optional\n        One of {'reflect', 'grid-mirror', 'constant', 'grid-constant', 'nearest', 'mirror',\n        'grid-wrap', 'wrap'}. See `scipy.ndimage.zoom` documentation for details.\n\n    cval : float or int, optional\n        See `scipy.ndimage.zoom` documentation for details.\n\n    prefilter : bool, optional\n        See `scipy.ndimage.zoom` documentation for details. Defaults to `True`.\n\n    Returns\n    -------\n    zoomed_data: `xarray.DataArray`\n        The zoomed input with its associated coordinates and coordinate reference system, if\n        available.\n\n    \"\"\"\n    # Zoom data\n    zoomed_data = scipy_zoom(\n        input_field.data, zoom, output=output, order=order, mode=mode, cval=cval,\n        prefilter=prefilter\n    )\n\n    # Zoom dimension coordinates\n    if not np.iterable(zoom):\n        zoom = tuple(zoom for _ in input_field.dims)\n    zoomed_dim_coords = {}\n    for dim_name, dim_zoom in zip(input_field.dims, zoom):\n        if dim_name in input_field.coords:\n            zoomed_dim_coords[dim_name] = scipy_zoom(\n                input_field[dim_name].data, dim_zoom, order=order, mode=mode, cval=cval,\n                prefilter=prefilter\n            )\n    if hasattr(input_field, 'metpy_crs'):\n        zoomed_dim_coords['metpy_crs'] = input_field.metpy_crs\n    # Reconstruct (ignoring non-dimension coordinates)\n    return xr.DataArray(\n        zoomed_data, dims=input_field.dims, coords=zoomed_dim_coords, attrs=input_field.attrs\n    )",
  "def altimeter_to_station_pressure(altimeter_value, height):\n    r\"\"\"Convert the altimeter measurement to station pressure.\n\n    This function is useful for working with METARs since they do not provide\n    altimeter values, but not sea-level pressure or station pressure.\n    The following definitions of altimeter setting and station pressure\n    are taken from [Smithsonian1951]_ Altimeter setting is the\n    pressure value to which an aircraft altimeter scale is set so that it will\n    indicate the altitude above mean sea-level of an aircraft on the ground at the\n    location for which the value is determined. It assumes a standard atmosphere [NOAA1976]_.\n    Station pressure is the atmospheric pressure at the designated station elevation.\n    Finding the station pressure can be helpful for calculating sea-level pressure\n    or other parameters.\n\n    Parameters\n    ----------\n    altimeter_value : `pint.Quantity`\n        The altimeter setting value as defined by the METAR or other observation,\n        which can be measured in either inches of mercury (in. Hg) or millibars (mb)\n\n    height: `pint.Quantity`\n        Elevation of the station measuring pressure\n\n    Returns\n    -------\n    `pint.Quantity`\n        The station pressure in hPa or in. Hg. Can be used to calculate sea-level\n        pressure.\n\n    See Also\n    --------\n    altimeter_to_sea_level_pressure\n\n    Notes\n    -----\n    This function is implemented using the following equations from the\n    Smithsonian Handbook (1951) p. 269\n\n    Equation 1:\n     .. math:: A_{mb} = (p_{mb} - 0.3)F\n\n    Equation 3:\n     .. math::  F = \\left [1 + \\left(\\frac{p_{0}^n a}{T_{0}} \\right)\n                   \\frac{H_{b}}{p_{1}^n} \\right ] ^ \\frac{1}{n}\n\n    Where,\n\n    :math:`p_{0}` = standard sea-level pressure = 1013.25 mb\n\n    :math:`p_{1} = p_{mb} - 0.3` when :math:`p_{0} = 1013.25 mb`\n\n    gamma = lapse rate in [NOAA1976]_ standard atmosphere below the isothermal layer\n    :math:`6.5^{\\circ}C. km.^{-1}`\n\n    :math:`T_{0}` = standard sea-level temperature 288 K\n\n    :math:`H_{b} =` station elevation in meters (elevation for which station pressure is given)\n\n    :math:`n = \\frac{a R_{d}}{g} = 0.190284` where :math:`R_{d}` is the gas constant for dry\n    air\n\n    And solving for :math:`p_{mb}` results in the equation below, which is used to\n    calculate station pressure :math:`(p_{mb})`\n\n    .. math:: p_{mb} = \\left [A_{mb} ^ n - \\left (\\frac{p_{0} a H_{b}}{T_0}\n                       \\right) \\right] ^ \\frac{1}{n} + 0.3\n\n    \"\"\"\n    # N-Value\n    n = (mpconsts.Rd * gamma / mpconsts.g).to_base_units()\n\n    return ((altimeter_value ** n\n             - ((p0.to(altimeter_value.units) ** n * gamma * height) / t0)) ** (1 / n)\n            + units.Quantity(0.3, 'hPa'))",
  "def altimeter_to_sea_level_pressure(altimeter_value, height, temperature):\n    r\"\"\"Convert the altimeter setting to sea-level pressure.\n\n    This function is useful for working with METARs since most provide\n    altimeter values, but not sea-level pressure, which is often plotted\n    on surface maps. The following definitions of altimeter setting, station pressure, and\n    sea-level pressure are taken from [Smithsonian1951]_.\n    Altimeter setting is the pressure value to which an aircraft altimeter scale\n    is set so that it will indicate the altitude above mean sea-level of an aircraft\n    on the ground at the location for which the value is determined. It assumes a standard\n    atmosphere. Station pressure is the atmospheric pressure at the designated station\n    elevation. Sea-level pressure is a pressure value obtained by the theoretical reduction\n    of barometric pressure to sea level. It is assumed that atmosphere extends to sea level\n    below the station and that the properties of the atmosphere are related to conditions\n    observed at the station. This value is recorded by some surface observation stations,\n    but not all. If the value is recorded, it can be found in the remarks section. Finding\n    the sea-level pressure is helpful for plotting purposes and different calculations.\n\n    Parameters\n    ----------\n    altimeter_value : `pint.Quantity`\n        The altimeter setting value is defined by the METAR or other observation,\n        with units of inches of mercury (in Hg) or millibars (hPa).\n\n    height  : `pint.Quantity`\n        Elevation of the station measuring pressure. Often times measured in meters\n\n    temperature : `pint.Quantity`\n        Temperature at the station\n\n    Returns\n    -------\n    `pint.Quantity`\n        The sea-level pressure in hPa and makes pressure values easier to compare\n        between different stations.\n\n    See Also\n    --------\n    altimeter_to_station_pressure\n\n    Notes\n    -----\n    This function is implemented using the following equations from Wallace and Hobbs (1977).\n\n    Equation 2.29:\n     .. math::\n       \\Delta z = Z_{2} - Z_{1}\n       = \\frac{R_{d} \\bar T_{v}}{g_0}ln\\left(\\frac{p_{1}}{p_{2}}\\right)\n       = \\bar H ln \\left (\\frac {p_{1}}{p_{2}} \\right)\n\n    Equation 2.31:\n     .. math::\n       p_{0} = p_{g}exp \\left(\\frac{Z_{g}}{\\bar H} \\right)\n       = p_{g}exp \\left(\\frac{g_{0}Z_{g}}{R_{d}\\bar T_{v}} \\right)\n\n    Then by substituting :math:`\\Delta_{Z}` for :math:`Z_{g}` in Equation 2.31:\n     .. math:: p_{sealevel} = p_{station} exp\\left(\\frac{\\Delta z}{H}\\right)\n\n    where :math:`\\Delta_{Z}` is the elevation in meters and :math:`H = \\frac{R_{d}T}{g}`\n\n    \"\"\"\n    # Calculate the station pressure using function altimeter_to_station_pressure()\n    psfc = altimeter_to_station_pressure(altimeter_value, height)\n\n    # Calculate the scale height\n    h = mpconsts.Rd * temperature / mpconsts.g\n\n    return psfc * np.exp(height / h)",
  "def _check_radians(value, max_radians=2 * np.pi):\n    \"\"\"Input validation of values that could be in degrees instead of radians.\n\n    Parameters\n    ----------\n    value : `pint.Quantity`\n        Input value to check\n\n    max_radians : float\n        Maximum absolute value of radians before warning\n\n    Returns\n    -------\n    `pint.Quantity`\n        Input value\n\n    \"\"\"\n    with contextlib.suppress(AttributeError):\n        value = value.to('radians').m\n    if np.any(np.greater(np.abs(value), max_radians)):\n        warn(f'Input over {np.nanmax(max_radians)} radians. Ensure proper units are given.')\n    return value",
  "def _pad(n):\n        # Return number of entries to pad given length along dimension.\n        return (n - 1) // 2",
  "def _zero_to_none(x):\n        # Convert zero values to None, otherwise return what is given.\n        return x if x != 0 else None",
  "def _offset(pad, k):\n        # Return padded slice offset by k entries\n        return slice(_zero_to_none(pad + k), _zero_to_none(-pad + k))",
  "def _trailing_dims(indexer):\n        # Add ... to the front of an indexer, since we are working with trailing dimensions.\n        return (Ellipsis,) + tuple(indexer)",
  "def offset_full_index(weight_index):\n        return _trailing_dims(_offset(_pad(n), weight_index[i] - _pad(n))\n                              for i, n in enumerate(weights.shape))",
  "class Front(mpatheffects.AbstractPathEffect):\n    \"\"\"Provide base functionality for plotting fronts as a patheffect.\n\n    These are plotted as symbol markers tangent to the path.\n\n    \"\"\"\n\n    _symbol = mpath.Path([[0, 0], [0, 1], [1, 1], [1, 0], [0, 0]],\n                         [mpath.Path.MOVETO, mpath.Path.LINETO,\n                          mpath.Path.LINETO, mpath.Path.LINETO, mpath.Path.CLOSEPOLY])\n\n    def __init__(self, color, size=10, spacing=1, flip=False, filled=True):\n        \"\"\"Initialize the front path effect.\n\n        Parameters\n        ----------\n        color : str or tuple[float]\n            Color to use for the effect.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbol should be flipped to the other side of the path. Defaults\n            to `False`.\n        filled : bool\n            Whether the symbol should be filled with the color. Defaults to `True`.\n\n        \"\"\"\n        super().__init__()\n        self.size = size\n        self.spacing = spacing\n        self.color = mcolors.to_rgba(color)\n        self.flip = flip\n        self.filled = filled\n        self._symbol_width = None\n\n    @cached_property\n    def symbol_width(self):\n        \"\"\"Return the width of the symbol being plotted.\"\"\"\n        return self._symbol.get_extents().width\n\n    def _step_size(self, renderer):\n        \"\"\"Return the length of the step between markers in pixels.\"\"\"\n        return (self.symbol_width + self.spacing) * self._size_pixels(renderer)\n\n    def _size_pixels(self, renderer):\n        \"\"\"Return the size of the marker in pixels.\"\"\"\n        return renderer.points_to_pixels(self.size)\n\n    @staticmethod\n    def _process_path(path, path_trans):\n        \"\"\"Transform the main path into pixel coordinates; calculate the needed components.\"\"\"\n        path_points = path.transformed(path_trans).interpolated(500).vertices\n        deltas = (path_points[1:] - path_points[:-1]).T\n        pt_offsets = np.concatenate(([0], np.hypot(*deltas).cumsum()))\n        angles = np.arctan2(deltas[-1], deltas[0])\n        return path_points, pt_offsets, angles\n\n    def _get_marker_locations(self, segment_offsets, renderer):\n        # Calculate increment of path length occupied by each marker drawn\n        inc = self._step_size(renderer)\n\n        # Find out how many markers that will accommodate, as well as remainder space\n        num, leftover = divmod(segment_offsets[-1], inc)\n\n        # Find the offset for each marker along the path length. We center along\n        # the path by adding half of the remainder. The offset is also centered within\n        # the marker by adding half of the marker increment\n        marker_offsets = np.arange(num) * inc + (leftover + inc) / 2.\n\n        # Find the location of these offsets within the total offset within each\n        # path segment; subtracting 1 gives us the left point of the path rather\n        # than the last. We then need to adjust for any offsets that are <= the first\n        # point of the path (just set them to index 0).\n        inds = np.searchsorted(segment_offsets, marker_offsets) - 1\n        inds[inds < 0] = 0\n\n        # Return the indices to the proper segment and the offset within that segment\n        return inds, marker_offsets - segment_offsets[inds]\n\n    def _override_gc(self, renderer, gc, **kwargs):\n        ret = renderer.new_gc()\n        ret.copy_properties(gc)\n        ret.set_joinstyle('miter')\n        ret.set_capstyle('butt')\n        return self._update_gc(ret, kwargs)\n\n    def _get_symbol_transform(self, renderer, offset, line_shift, angle, start):\n        scalex = self._size_pixels(renderer)\n        scaley, line_shift = (-scalex, -line_shift) if self.flip else (scalex, line_shift)\n        return mtransforms.Affine2D().scale(scalex, scaley).translate(\n            offset - self.symbol_width * self._size_pixels(renderer) / 2,\n            line_shift).rotate(angle).translate(*start)\n\n    def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        # Set up a new graphics context for rendering the front effect; override the color\n        gc0 = self._override_gc(renderer, gc, foreground=self.color)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon and how\n        # far within that segment the markers will appear.\n        segment_indices, marker_offsets = self._get_marker_locations(offsets, renderer)\n\n        # Draw the original path\n        renderer.draw_path(gc0, path, affine, rgbFace)  # noqa: N803\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the markers to draw\n        for ind, marker_offset in zip(segment_indices, marker_offsets):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n            renderer.draw_path(gc0, self._symbol, sym_trans,\n                               self.color if self.filled else None)\n\n        gc0.restore()",
  "class Frontogenesis(Front):\n    \"\"\"Provide base functionality for plotting strengthening fronts as a patheffect.\n\n    These are plotted as symbol markers tangent to the path.\n\n    \"\"\"\n\n    def __init__(self, color, size=10, spacing=1, flip=False):\n        \"\"\"Initialize the frontogenesis path effect.\n\n        Parameters\n        ----------\n        color : str or tuple[float]\n            Color to use for the effect.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbol should be flipped to the other side of the path. Defaults\n            to `False`.\n\n        \"\"\"\n        super().__init__(color, size, spacing, flip)\n        self._padding = 4\n\n    def _step_size(self, renderer):\n        \"\"\"Return the length of the step between markers in pixels.\"\"\"\n        return (self.symbol_width + self.spacing + self._padding) * self._size_pixels(renderer)\n\n    def _get_path_locations(self, segment_offsets, renderer):\n        # Calculate increment of path length occupied by each marker drawn\n        inc = self._step_size(renderer)\n\n        # Find out how many markers that will accommodate, as well as remainder space\n        num, leftover = divmod(segment_offsets[-1], inc)\n\n        # Find the offset for each marker along the path length. We center along\n        # the path by adding half of the remainder. The offset is also centered within\n        # the marker by adding half of the marker increment\n        marker_offsets = np.arange(num) * inc + (leftover + inc) / 2.\n\n        # Do the same for path segments\n        start_offsets = marker_offsets - 0.33 * inc\n        end_offsets = marker_offsets + 0.33 * inc\n\n        # Find the location of these offsets within the total offset within each\n        # path segment; subtracting 1 gives us the left point of the path rather\n        # than the last. We then need to adjust for any offsets that are <= the first\n        # point of the path (just set them to index 0).\n        inds = np.searchsorted(segment_offsets, marker_offsets) - 1\n        inds[inds < 0] = 0\n\n        start_inds = np.searchsorted(segment_offsets, start_offsets) - 1\n        start_inds[start_inds < 0] = 0\n\n        end_inds = np.searchsorted(segment_offsets, end_offsets) - 1\n        end_inds[start_inds < 0] = 0\n\n        # Return the indices to the proper segment and the offset within that segment\n        return start_inds, end_inds, inds, marker_offsets - segment_offsets[inds]\n\n    def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        # Set up a new graphics context for rendering the front effect; override the color\n        gc0 = self._override_gc(renderer, gc, foreground=self.color)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon, how\n        # far within that segment the markers will appear, and the segment bounds.\n        (segment_starts, segment_ends,\n         segment_indices, marker_offsets) = self._get_path_locations(offsets, renderer)\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the segments to draw\n        for start_path, end_path in zip(segment_starts, segment_ends):\n            renderer.draw_path(gc0, mpath.Path(starts[start_path:end_path]),\n                               mtransforms.Affine2D(), None)\n\n        # Loop over all the markers to draw\n        for ind, marker_offset in zip(segment_indices, marker_offsets):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n\n            renderer.draw_path(gc0, self._symbol, sym_trans, self.color)\n\n        gc0.restore()",
  "class Frontolysis(Front):\n    \"\"\"Provide base functionality for plotting weakening fronts as a patheffect.\n\n    These are plotted as symbol markers tangent to the path.\n\n    \"\"\"\n\n    def __init__(self, color, size=10, spacing=1, flip=False):\n        \"\"\"Initialize the frontolysis path effect.\n\n        Parameters\n        ----------\n        color : str or tuple[float]\n            Color to use for the effect.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbol should be flipped to the other side of the path. Defaults\n            to `False`.\n\n        \"\"\"\n        super().__init__(color, size, spacing, flip)\n        self._padding = 4\n\n    def _step_size(self, renderer):\n        \"\"\"Return the length of the step between markers in pixels.\"\"\"\n        return (self.symbol_width + self.spacing + self._padding) * self._size_pixels(renderer)\n\n    def _get_path_locations(self, segment_offsets, renderer):\n        # Calculate increment of path length occupied by each marker drawn\n        inc = self._step_size(renderer)\n\n        # Find out how many markers that will accommodate, as well as remainder space\n        num, leftover = divmod(segment_offsets[-1], inc)\n\n        # Find the offset for each marker along the path length. We center along\n        # the path by adding half of the remainder. The offset is also centered within\n        # the marker by adding half of the marker increment\n        marker_offsets = np.arange(num) * inc + (leftover + inc) / 2.\n\n        # Do the same for path segments\n        start_offsets = marker_offsets - 0.33 * inc\n        end_offsets = marker_offsets + 0.33 * inc\n\n        # Find the location of these offsets within the total offset within each\n        # path segment; subtracting 1 gives us the left point of the path rather\n        # than the last. We then need to adjust for any offsets that are <= the first\n        # point of the path (just set them to index 0).\n        inds = np.searchsorted(segment_offsets, marker_offsets) - 1\n        inds[inds < 0] = 0\n\n        start_inds = np.searchsorted(segment_offsets, start_offsets) - 1\n        start_inds[start_inds < 0] = 0\n\n        end_inds = np.searchsorted(segment_offsets, end_offsets) - 1\n        end_inds[start_inds < 0] = 0\n\n        # Return the indices to the proper segment and the offset within that segment\n        return start_inds, end_inds, inds, marker_offsets - segment_offsets[inds]\n\n    def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        # Set up a new graphics context for rendering the front effect; override the color\n        gc0 = self._override_gc(renderer, gc, foreground=self.color)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon, how\n        # far within that segment the markers will appear, and the segment bounds.\n        (segment_starts, segment_ends,\n         segment_indices, marker_offsets) = self._get_path_locations(offsets, renderer)\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the segments to draw\n        for start_path, end_path in zip(segment_starts, segment_ends):\n            renderer.draw_path(gc0, mpath.Path(starts[start_path:end_path]),\n                               mtransforms.Affine2D(), None)\n\n        # Loop over all the markers to draw\n        for ind, marker_offset in zip(segment_indices[::2], marker_offsets[::2]):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n\n            renderer.draw_path(gc0, self._symbol, sym_trans, self.color)\n\n        gc0.restore()",
  "class ScallopedStroke(mpatheffects.AbstractPathEffect):\n    \"\"\"A line-based PathEffect which draws a path with a scalloped style.\n\n    The spacing, length, and side of the scallops can be controlled. This implementation is\n    based off of :class:`matplotlib.patheffects.TickedStroke`.\n    \"\"\"\n\n    def __init__(self, offset=(0, 0), spacing=10.0, side='left', length=1.15, **kwargs):\n        \"\"\"Create a scalloped path effect.\n\n        Parameters\n        ----------\n        offset : (float, float)\n            The (x, y) offset to apply to the path, in points. Defaults to no offset.\n        spacing : float\n            The spacing between ticks in points. Defaults to 10.0.\n        side : str\n            Side of the path scallops appear on from the reference of\n            walking along the curve. Options are left and right. Defaults to ``'left'``.\n        length : float\n            The length of the tick relative to spacing. Defaults to 1.414.\n        kwargs :\n            Extra keywords are stored and passed through to\n            `~matplotlib.renderer.GraphicsContextBase`.\n        \"\"\"\n        super().__init__(offset)\n\n        self._spacing = spacing\n        if side == 'left':\n            self._angle = 90\n        elif side == 'right':\n            self._angle = -90\n        else:\n            raise ValueError('Side must be left or right.')\n        self._length = length\n        self._gc = kwargs\n\n    def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the path with updated gc.\"\"\"\n        # Do not modify the input! Use copy instead.\n        gc0 = renderer.new_gc()\n        gc0.copy_properties(gc)\n\n        gc0 = self._update_gc(gc0, self._gc)\n        trans = affine + self._offset_transform(renderer)\n\n        theta = -np.radians(self._angle)\n        trans_matrix = np.array([[np.cos(theta), -np.sin(theta)],\n                                 [np.sin(theta), np.cos(theta)]])\n\n        # Convert spacing parameter to pixels.\n        spacing_px = renderer.points_to_pixels(self._spacing)\n\n        # Transform before evaluation because to_polygons works at resolution\n        # of one -- assuming it is working in pixel space.\n        transpath = affine.transform_path(path)\n\n        # Evaluate path to straight line segments that can be used to\n        # construct line scallops.\n        polys = transpath.to_polygons(closed_only=False)\n\n        for p in polys:\n            x = p[:, 0]\n            y = p[:, 1]\n\n            # Can not interpolate points or draw line if only one point in\n            # polyline.\n            if x.size < 2:\n                continue\n\n            # Find distance between points on the line\n            ds = np.hypot(x[1:] - x[:-1], y[1:] - y[:-1])\n\n            # Build parametric coordinate along curve\n            s = np.concatenate(([0.0], np.cumsum(ds)))\n            s_total = s[-1]\n\n            num = int(np.ceil(s_total / spacing_px)) - 1\n            # Pick parameter values for scallops.\n            s_tick = np.linspace(0, s_total, num)\n\n            # Find points along the parameterized curve\n            x_tick = np.interp(s_tick, s, x)\n            y_tick = np.interp(s_tick, s, y)\n\n            # Find unit vectors in local direction of curve\n            delta_s = self._spacing * .001\n            u = (np.interp(s_tick + delta_s, s, x) - x_tick) / delta_s\n            v = (np.interp(s_tick + delta_s, s, y) - y_tick) / delta_s\n\n            # Handle slope of end point\n            if (x_tick[-1], y_tick[-1]) == (x_tick[0], y_tick[0]):  # periodic\n                u[-1] = u[0]\n                v[-1] = v[0]\n            else:\n                u[-1] = u[-2]\n                v[-1] = v[-2]\n\n            # Normalize slope into unit slope vector.\n            n = np.hypot(u, v)\n            mask = n == 0\n            n[mask] = 1.0\n\n            uv = np.array([u / n, v / n]).T\n            uv[mask] = np.array([0, 0]).T\n\n            # Rotate and scale unit vector\n            dxy = np.dot(uv, trans_matrix) * self._length * spacing_px\n\n            # Build endpoints\n            x_end = x_tick + dxy[:, 0]\n            y_end = y_tick + dxy[:, 1]\n\n            # Interleave ticks to form Path vertices\n            xyt = np.empty((2 * num, 2), dtype=x_tick.dtype)\n            xyt[0::2, 0] = x_tick\n            xyt[1::2, 0] = x_end\n            xyt[0::2, 1] = y_tick\n            xyt[1::2, 1] = y_end\n\n            # Build path vertices that will define control points of the bezier curves\n            verts = []\n            i = 0\n            nverts = 0\n            while i < len(xyt) - 2:\n                verts.append(xyt[i, :])\n                verts.append(xyt[i + 1, :])\n                verts.append(xyt[i + 3, :])\n                verts.append(xyt[i + 2, :])\n                nverts += 1\n                i += 2\n\n            # Build up vector of Path codes\n            codes = np.tile([mpath.Path.LINETO, mpath.Path.CURVE4,\n                             mpath.Path.CURVE4, mpath.Path.CURVE4], nverts)\n            codes[0] = mpath.Path.MOVETO\n\n            # Construct and draw resulting path\n            h = mpath.Path(verts, codes)\n\n            # Transform back to data space during render\n            renderer.draw_path(gc0, h, affine.inverted() + trans, rgbFace)\n\n        gc0.restore()",
  "class ColdFront(Front):\n    \"\"\"Draw a path as a cold front, with (default blue) pips/triangles along the path.\"\"\"\n\n    _symbol = mpath.Path([[0, 0], [1, 1], [2, 0], [0, 0]],\n                         [mpath.Path.MOVETO, mpath.Path.LINETO, mpath.Path.LINETO,\n                          mpath.Path.CLOSEPOLY])\n\n    def __init__(self, color='blue', **kwargs):\n        super().__init__(color, **kwargs)",
  "class ColdFrontogenesis(Frontogenesis):\n    \"\"\"Draw a path as a strengthening cold.\"\"\"\n\n    _symbol = mpath.Path([[0, 0], [1, 1], [2, 0], [0, 0]],\n                         [mpath.Path.MOVETO, mpath.Path.LINETO, mpath.Path.LINETO,\n                          mpath.Path.CLOSEPOLY])\n\n    def __init__(self, color='blue', **kwargs):\n        super().__init__(color, **kwargs)",
  "class ColdFrontolysis(Frontolysis):\n    \"\"\"Draw a path as a weakening cold front.\"\"\"\n\n    _symbol = mpath.Path([[0, 0], [1, 1], [2, 0], [0, 0]],\n                         [mpath.Path.MOVETO, mpath.Path.LINETO, mpath.Path.LINETO,\n                          mpath.Path.CLOSEPOLY])\n\n    def __init__(self, color='blue', **kwargs):\n        super().__init__(color, **kwargs)",
  "class Dryline(Front):\n    \"\"\"Draw a path as a dryline with (default brown) scallops along the path.\"\"\"\n\n    _symbol = mpath.Path.wedge(0, 180).transformed(mtransforms.Affine2D().translate(1, 0))\n\n    def __init__(self, color='brown', spacing=0.144, filled=False, **kwargs):\n        super().__init__(color, spacing=spacing, filled=filled, **kwargs)",
  "class WarmFront(Front):\n    \"\"\"Draw a path as a warm front with (default red) scallops along the path.\"\"\"\n\n    _symbol = mpath.Path.wedge(0, 180).transformed(mtransforms.Affine2D().translate(1, 0))\n\n    def __init__(self, color='red', **kwargs):\n        super().__init__(color, **kwargs)",
  "class WarmFrontogenesis(Frontogenesis):\n    \"\"\"Draw a path as a strengthening warm front.\"\"\"\n\n    _symbol = mpath.Path.wedge(0, 180).transformed(mtransforms.Affine2D().translate(1, 0))\n\n    def __init__(self, color='red', **kwargs):\n        super().__init__(color, **kwargs)",
  "class WarmFrontolysis(Frontolysis):\n    \"\"\"Draw a path as a weakening warm front.\"\"\"\n\n    _symbol = mpath.Path.wedge(0, 180).transformed(mtransforms.Affine2D().translate(1, 0))\n\n    def __init__(self, color='red', **kwargs):\n        super().__init__(color, **kwargs)",
  "class OccludedFront(Front):\n    \"\"\"Draw an occluded front with (default purple) pips and scallops along the path.\"\"\"\n\n    def __init__(self, color='purple', **kwargs):\n        self._symbol_cycle = None\n        super().__init__(color, **kwargs)\n\n    def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        self._symbol_cycle = None\n        return super().draw_path(renderer, gc, path, affine, rgbFace)  # noqa: N803\n\n    @property\n    def _symbol(self):\n        \"\"\"Return the proper symbol to draw; alternatives between scallop and pip/triangle.\"\"\"\n        if self._symbol_cycle is None:\n            self._symbol_cycle = itertools.cycle([WarmFront._symbol, ColdFront._symbol])\n        return next(self._symbol_cycle)",
  "class OccludedFrontogenesis(Frontogenesis):\n    \"\"\"Draw a strengthening occluded front.\"\"\"\n\n    def __init__(self, color='purple', **kwargs):\n        self._symbol_cycle = None\n        super().__init__(color, **kwargs)\n\n    def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        self._symbol_cycle = None\n        return super().draw_path(renderer, gc, path, affine, rgbFace)  # noqa: N803\n\n    @property\n    def _symbol(self):\n        \"\"\"Return the proper symbol to draw; alternatives between scallop and pip/triangle.\"\"\"\n        if self._symbol_cycle is None:\n            self._symbol_cycle = itertools.cycle([WarmFrontogenesis._symbol,\n                                                  ColdFrontogenesis._symbol])\n        return next(self._symbol_cycle)",
  "class OccludedFrontolysis(Frontolysis):\n    \"\"\"Draw a weakening occluded front.\"\"\"\n\n    def __init__(self, color='purple', **kwargs):\n        self._symbol_cycle = None\n        super().__init__(color, **kwargs)\n\n    def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        self._symbol_cycle = None\n        return super().draw_path(renderer, gc, path, affine, rgbFace)  # noqa: N803\n\n    @property\n    def _symbol(self):\n        \"\"\"Return the proper symbol to draw; alternatives between scallop and pip/triangle.\"\"\"\n        if self._symbol_cycle is None:\n            self._symbol_cycle = itertools.cycle([WarmFrontolysis._symbol,\n                                                  ColdFrontolysis._symbol])\n        return next(self._symbol_cycle)",
  "class RidgeAxis(mpatheffects.AbstractPathEffect):\n    \"\"\"A line-based PathEffect which draws a path with a sawtooth-wave style.\n\n    This line style is frequently used to represent a ridge axis.\n    \"\"\"\n\n    def __init__(self, color='black', spacing=12.0, length=0.5):\n        \"\"\"Create ridge axis path effect.\n\n        Parameters\n        ----------\n        color : str\n            Color to use for the effect.\n        spacing : float\n            The spacing between ticks in points. Default is 12.\n        length : float\n            The length of the tick relative to spacing. Default is 0.5.\n\n        \"\"\"\n        self._spacing = spacing\n        self._angle = 90.0\n        self._length = length\n        self._color = color\n\n    def _override_gc(self, renderer, gc, **kwargs):\n        ret = renderer.new_gc()\n        ret.copy_properties(gc)\n        ret.set_joinstyle('miter')\n        ret.set_capstyle('butt')\n        return self._update_gc(ret, kwargs)\n\n    def draw_path(self, renderer, gc, tpath, affine, rgbFace):  # noqa: N803\n        \"\"\"Draw the path with updated gc.\"\"\"\n        # Do not modify the input! Use copy instead.\n        gc0 = self._override_gc(renderer, gc, foreground=self._color)\n\n        theta = -np.radians(self._angle)\n        trans_matrix = np.array([[np.cos(theta), -np.sin(theta)],\n                                 [np.sin(theta), np.cos(theta)]])\n\n        # Convert spacing parameter to pixels.\n        spacing_px = renderer.points_to_pixels(self._spacing)\n\n        # Transform before evaluation because to_polygons works at resolution\n        # of one -- assuming it is working in pixel space.\n        transpath = affine.transform_path(tpath)\n\n        # Evaluate path to straight line segments that can be used to\n        # construct line ticks.\n        polys = transpath.to_polygons(closed_only=False)\n\n        for p in polys:\n            x = p[:, 0]\n            y = p[:, 1]\n\n            # Can not interpolate points or draw line if only one point in\n            # polyline.\n            if x.size < 2:\n                continue\n\n            # Find distance between points on the line\n            ds = np.hypot(x[1:] - x[:-1], y[1:] - y[:-1])\n\n            # Build parametric coordinate along curve\n            s = np.concatenate(([0.0], np.cumsum(ds)))\n            s_total = s[-1]\n\n            num = int(np.ceil(s_total / spacing_px)) - 1\n            # Pick parameter values for ticks.\n            s_tick = np.linspace(spacing_px / 2, s_total - spacing_px / 2, num)\n\n            # Find points along the parameterized curve\n            x_tick = np.interp(s_tick, s, x)\n            y_tick = np.interp(s_tick, s, y)\n\n            # Find unit vectors in local direction of curve\n            delta_s = self._spacing * .001\n            u = (np.interp(s_tick + delta_s, s, x) - x_tick) / delta_s\n            v = (np.interp(s_tick + delta_s, s, y) - y_tick) / delta_s\n\n            # Normalize slope into unit slope vector.\n            n = np.hypot(u, v)\n            mask = n == 0\n            n[mask] = 1.0\n\n            uv = np.array([u / n, v / n]).T\n            uv[mask] = np.array([0, 0]).T\n\n            # Rotate and scale unit vector into tick vector\n            dxy1 = np.dot(uv[0::2], trans_matrix) * self._length * spacing_px\n            dxy2 = np.dot(uv[1::2], trans_matrix.T) * self._length * spacing_px\n\n            # Build tick endpoints\n            x_end = np.zeros(num)\n            y_end = np.zeros(num)\n            x_end[0::2] = x_tick[0::2] + dxy1[:, 0]\n            x_end[1::2] = x_tick[1::2] + dxy2[:, 0]\n            y_end[0::2] = y_tick[0::2] + dxy1[:, 1]\n            y_end[1::2] = y_tick[1::2] + dxy2[:, 1]\n\n            # Interleave ticks to form Path vertices\n            xyt = np.empty((num, 2), dtype=x_tick.dtype)\n            xyt[:, 0] = x_end\n            xyt[:, 1] = y_end\n\n            # Build up vector of Path codes\n            codes = np.concatenate([[mpath.Path.MOVETO], [mpath.Path.LINETO] * (len(xyt) - 1)])\n\n            # Construct and draw resulting path\n            h = mpath.Path(xyt, codes)\n\n            # Transform back to data space during render\n            renderer.draw_path(gc0, h, affine.inverted() + affine, rgbFace)  # noqa: N803\n\n        gc0.restore()",
  "class Squall(mpatheffects.AbstractPathEffect):\n    \"\"\"Squall line path effect.\"\"\"\n\n    symbol = mpath.Path.circle((0, 0), radius=4)\n\n    def __init__(self, color='black', spacing=75):\n        \"\"\"Initialize the squall line path effect.\n\n        Parameters\n        ----------\n        color : str\n            Color to use for the effect.\n        spacing : float\n            Spacing between symbols along path (in points).\n\n        \"\"\"\n        self.marker_margin = 10\n        self.spacing = spacing\n        self.color = mcolors.to_rgba(color)\n        self._symbol_width = None\n\n    @staticmethod\n    def _process_path(path, path_trans):\n        \"\"\"Transform the main path into pixel coordinates; calculate the needed components.\"\"\"\n        path_points = path.transformed(path_trans).interpolated(500).vertices\n        deltas = (path_points[1:] - path_points[:-1]).T\n        pt_offsets = np.concatenate(([0], np.hypot(*deltas).cumsum()))\n        return path_points, pt_offsets\n\n    def _override_gc(self, renderer, gc, **kwargs):\n        ret = renderer.new_gc()\n        ret.copy_properties(gc)\n        ret.set_joinstyle('miter')\n        ret.set_capstyle('butt')\n        return self._update_gc(ret, kwargs)\n\n    def _get_object_locations(self, segment_offsets, renderer):\n        # Calculate increment of path length\n        inc = renderer.points_to_pixels(self.spacing)\n        margin = renderer.points_to_pixels(self.marker_margin)\n\n        # Find out how many markers that will accommodate, as well as remainder space\n        num, leftover = divmod(segment_offsets[-1], inc)\n\n        # Find the offset for each marker along the path length. We center along\n        # the path by adding half of the remainder. The offset is also centered within\n        # the marker by adding half of the marker increment\n        first_marker = np.arange(num) * inc - 0.5 * margin + (leftover + inc) / 2.\n        second_marker = np.arange(num) * inc + 0.5 * margin + (leftover + inc) / 2.\n        marker_offsets = np.sort(np.concatenate([first_marker, second_marker]))\n\n        # Do the same for path segments\n        first = segment_offsets[0]\n        last = segment_offsets[-1]\n        path_offset_1 = np.arange(num) * inc - 1.5 * margin + (leftover + inc) / 2\n        path_offset_2 = np.arange(num) * inc + 1.5 * margin + (leftover + inc) / 2\n        path_offsets = np.sort(np.concatenate(\n            [[first], path_offset_1, path_offset_2, [last]]\n        ))\n\n        # Find the location of these offsets within the total offset within each\n        # path segment; subtracting 1 gives us the left point of the path rather\n        # than the last. We then need to adjust for any offsets that are <= the first\n        # point of the path (just set them to index 0).\n        marker_inds = np.searchsorted(segment_offsets, marker_offsets) - 1\n        marker_inds[marker_inds < 0] = 0\n\n        # Do the same for path segments\n        path_inds = np.searchsorted(segment_offsets, path_offsets) - 1\n        path_inds[path_inds < 0] = 0\n\n        # Return the indices to the proper segment and the offset within that segment\n        return marker_inds, path_inds\n\n    def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw path.\"\"\"\n        # Set up a new graphics context for rendering the front effect; override the color\n        gc0 = self._override_gc(renderer, gc, foreground=self.color)\n\n        # Get the information we need for drawing along the path\n        starts, offsets = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon and how\n        # far within that segment the markers will appear.\n        marker_indices, path_indices = self._get_object_locations(offsets, renderer)\n\n        base_trans = mtransforms.Affine2D()\n\n        # Loop over the segmented path\n        ipath = path.interpolated(500).vertices\n        for i in range(0, len(path_indices) - 1, 2):\n            start = path_indices[i]\n            stop = path_indices[i + 1]\n            n = stop - start\n            spath = mpath.Path(\n                ipath[start:stop],\n                [mpath.Path.MOVETO] + [mpath.Path.LINETO] * (n - 1)\n            )\n            renderer.draw_path(gc0, spath, affine, None)\n\n        # Loop over all the markers to draw\n        for ind in marker_indices:\n            sym_trans = base_trans.frozen().translate(*starts[ind])\n            renderer.draw_path(gc0, self.symbol, sym_trans, self.color)\n\n        gc0.restore()",
  "class StationaryFront(Front):\n    \"\"\"Draw a stationary front as alternating cold and warm front segments.\"\"\"\n\n    _symbol = WarmFront._symbol.transformed(mtransforms.Affine2D().scale(1, -1))\n    _symbol2 = ColdFront._symbol\n\n    def __init__(self, colors=('red', 'blue'), **kwargs):\n        \"\"\"Initialize a stationary front path effect.\n\n        This effect alternates between a warm front and cold front symbol.\n\n        Parameters\n        ----------\n        colors : Sequence[str] or Sequence[tuple[float]]\n            Matplotlib color identifiers to cycle between on the two different front styles.\n            Defaults to alternating red and blue.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbols should be flipped to the other side of the path. Defaults\n            to `False`.\n\n        \"\"\"\n        self._colors = list(map(mcolors.to_rgba, colors))\n        super().__init__(color=self._colors[0], **kwargs)\n\n    def _get_path_segment_ends(self, segment_offsets, renderer):\n        # Calculate increment of path length occupied by each marker drawn\n        inc = self._step_size(renderer)\n\n        # Find out how many markers that will accommodate, as well as remainder space\n        num, leftover = divmod(segment_offsets[-1], inc)\n\n        # Find the offset for each path segment end. We center along\n        # the entire path by adding half of the remainder.\n        path_offsets = np.arange(1, num + 1) * inc + leftover / 2.\n\n        # Find the location of these offsets within the total offset within each\n        # path segment; subtracting 1 gives us the left point of the path rather\n        # than the last.\n        return np.searchsorted(segment_offsets, path_offsets)\n\n    def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        gcs = [self._override_gc(renderer, gc, foreground=color) for color in self._colors]\n        self._gc_cycle = itertools.cycle(gcs)\n        self._symbol_cycle = itertools.cycle([self._symbol, self._symbol2])\n        self._color_cycle = itertools.cycle(self._colors)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon and how\n        # far within that segment the markers will appear.\n        segment_indices, marker_offsets = self._get_marker_locations(offsets, renderer)\n        end_path_inds = self._get_path_segment_ends(offsets, renderer)\n        start_path_inds = np.concatenate([[0], end_path_inds[:-1]])\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = -renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the markers to draw\n        for ind, start_path, end_path, marker_offset in zip(segment_indices, start_path_inds,\n                                                            end_path_inds, marker_offsets):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n            gc = next(self._gc_cycle)\n            color = next(self._color_cycle)\n            symbol = next(self._symbol_cycle)\n\n            renderer.draw_path(gc, symbol, sym_trans, color)\n            renderer.draw_path(gc, mpath.Path(starts[start_path:end_path]),\n                               mtransforms.Affine2D(), None)\n            line_shift *= -1\n\n        gcs[0].restore()",
  "class StationaryFrontogenesis(Frontogenesis):\n    \"\"\"Draw a strengthening stationary front.\"\"\"\n\n    _symbol = WarmFront._symbol\n    _symbol2 = ColdFront._symbol.transformed(mtransforms.Affine2D().scale(1, -1))\n\n    def __init__(self, colors=('red', 'blue'), **kwargs):\n        \"\"\"Initialize a strengthening stationary front path effect.\n\n        This effect alternates between a warm front and cold front symbol.\n\n        Parameters\n        ----------\n        colors : Sequence[str] or Sequence[tuple[float]]\n            Matplotlib color identifiers to cycle between on the two different front styles.\n            Defaults to alternating red and blue.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbols should be flipped to the other side of the path. Defaults\n            to `False`.\n\n        \"\"\"\n        self._colors = list(map(mcolors.to_rgba, colors))\n        super().__init__(color=self._colors[0], **kwargs)\n\n    def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        gcs = [self._override_gc(renderer, gc, foreground=color) for color in self._colors]\n        self._gc_cycle = itertools.cycle(gcs)\n        self._symbol_cycle = itertools.cycle([self._symbol, self._symbol2])\n        self._color_cycle = itertools.cycle(self._colors)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon, how\n        # far within that segment the markers will appear, and the segment bounds.\n        (segment_starts, segment_ends,\n         segment_indices, marker_offsets) = self._get_path_locations(offsets, renderer)\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the markers to draw\n        for ind, start_path, end_path, marker_offset in zip(segment_indices, segment_starts,\n                                                            segment_ends, marker_offsets):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n            gc = next(self._gc_cycle)\n            color = next(self._color_cycle)\n            symbol = next(self._symbol_cycle)\n\n            renderer.draw_path(gc, symbol, sym_trans, color)\n            renderer.draw_path(gc, mpath.Path(starts[start_path:end_path]),\n                               mtransforms.Affine2D(), None)\n            line_shift *= -1\n\n        gcs[0].restore()",
  "class StationaryFrontolysis(Frontolysis):\n    \"\"\"Draw a weakening stationary front..\"\"\"\n\n    _symbol = WarmFront._symbol\n    _symbol2 = ColdFront._symbol.transformed(mtransforms.Affine2D().scale(1, -1))\n\n    def __init__(self, colors=('red', 'blue'), **kwargs):\n        \"\"\"Initialize a weakening stationary front path effect.\n\n        This effect alternates between a warm front and cold front symbol.\n\n        Parameters\n        ----------\n        colors : Sequence[str] or Sequence[tuple[float]]\n            Matplotlib color identifiers to cycle between on the two different front styles.\n            Defaults to alternating red and blue.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbols should be flipped to the other side of the path. Defaults\n            to `False`.\n\n        \"\"\"\n        self._colors = list(map(mcolors.to_rgba, colors))\n        self._segment_colors = [\n            (self._colors[0], self._colors[0]),\n            (self._colors[0], self._colors[1]),\n            (self._colors[1], self._colors[1]),\n            (self._colors[1], self._colors[0])\n        ]\n        super().__init__(color=self._colors[0], **kwargs)\n\n    def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        gcs = [self._override_gc(renderer, gc, foreground=color) for color in self._colors]\n        self._gc_cycle = itertools.cycle(gcs)\n        self._symbol_cycle = itertools.cycle([self._symbol, self._symbol2])\n        self._color_cycle = itertools.cycle(self._colors)\n        self._segment_cycle = itertools.cycle(self._segment_colors)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon, how\n        # far within that segment the markers will appear, and the segment bounds.\n        (segment_starts, segment_ends,\n         segment_indices, marker_offsets) = self._get_path_locations(offsets, renderer)\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the markers to draw\n        for ind, marker_offset in zip(segment_indices[::2], marker_offsets[::2]):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n            gc = next(self._gc_cycle)\n            color = next(self._color_cycle)\n            symbol = next(self._symbol_cycle)\n\n            renderer.draw_path(gc, symbol, sym_trans, color)\n\n            line_shift *= -1\n\n        for start_path, mid_path, end_path in zip(segment_starts,\n                                                  segment_indices,\n                                                  segment_ends):\n            color1, color2 = next(self._segment_cycle)\n\n            gcx = self._override_gc(renderer, gc, foreground=mcolors.to_rgb(color1))\n            renderer.draw_path(gcx, mpath.Path(starts[start_path:mid_path]),\n                               mtransforms.Affine2D(), None)\n\n            gcx = self._override_gc(renderer, gc, foreground=mcolors.to_rgb(color2))\n            renderer.draw_path(gcx, mpath.Path(starts[mid_path:end_path]),\n                               mtransforms.Affine2D(), None)\n\n        gcs[0].restore()",
  "def __init__(self, color, size=10, spacing=1, flip=False, filled=True):\n        \"\"\"Initialize the front path effect.\n\n        Parameters\n        ----------\n        color : str or tuple[float]\n            Color to use for the effect.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbol should be flipped to the other side of the path. Defaults\n            to `False`.\n        filled : bool\n            Whether the symbol should be filled with the color. Defaults to `True`.\n\n        \"\"\"\n        super().__init__()\n        self.size = size\n        self.spacing = spacing\n        self.color = mcolors.to_rgba(color)\n        self.flip = flip\n        self.filled = filled\n        self._symbol_width = None",
  "def symbol_width(self):\n        \"\"\"Return the width of the symbol being plotted.\"\"\"\n        return self._symbol.get_extents().width",
  "def _step_size(self, renderer):\n        \"\"\"Return the length of the step between markers in pixels.\"\"\"\n        return (self.symbol_width + self.spacing) * self._size_pixels(renderer)",
  "def _size_pixels(self, renderer):\n        \"\"\"Return the size of the marker in pixels.\"\"\"\n        return renderer.points_to_pixels(self.size)",
  "def _process_path(path, path_trans):\n        \"\"\"Transform the main path into pixel coordinates; calculate the needed components.\"\"\"\n        path_points = path.transformed(path_trans).interpolated(500).vertices\n        deltas = (path_points[1:] - path_points[:-1]).T\n        pt_offsets = np.concatenate(([0], np.hypot(*deltas).cumsum()))\n        angles = np.arctan2(deltas[-1], deltas[0])\n        return path_points, pt_offsets, angles",
  "def _get_marker_locations(self, segment_offsets, renderer):\n        # Calculate increment of path length occupied by each marker drawn\n        inc = self._step_size(renderer)\n\n        # Find out how many markers that will accommodate, as well as remainder space\n        num, leftover = divmod(segment_offsets[-1], inc)\n\n        # Find the offset for each marker along the path length. We center along\n        # the path by adding half of the remainder. The offset is also centered within\n        # the marker by adding half of the marker increment\n        marker_offsets = np.arange(num) * inc + (leftover + inc) / 2.\n\n        # Find the location of these offsets within the total offset within each\n        # path segment; subtracting 1 gives us the left point of the path rather\n        # than the last. We then need to adjust for any offsets that are <= the first\n        # point of the path (just set them to index 0).\n        inds = np.searchsorted(segment_offsets, marker_offsets) - 1\n        inds[inds < 0] = 0\n\n        # Return the indices to the proper segment and the offset within that segment\n        return inds, marker_offsets - segment_offsets[inds]",
  "def _override_gc(self, renderer, gc, **kwargs):\n        ret = renderer.new_gc()\n        ret.copy_properties(gc)\n        ret.set_joinstyle('miter')\n        ret.set_capstyle('butt')\n        return self._update_gc(ret, kwargs)",
  "def _get_symbol_transform(self, renderer, offset, line_shift, angle, start):\n        scalex = self._size_pixels(renderer)\n        scaley, line_shift = (-scalex, -line_shift) if self.flip else (scalex, line_shift)\n        return mtransforms.Affine2D().scale(scalex, scaley).translate(\n            offset - self.symbol_width * self._size_pixels(renderer) / 2,\n            line_shift).rotate(angle).translate(*start)",
  "def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        # Set up a new graphics context for rendering the front effect; override the color\n        gc0 = self._override_gc(renderer, gc, foreground=self.color)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon and how\n        # far within that segment the markers will appear.\n        segment_indices, marker_offsets = self._get_marker_locations(offsets, renderer)\n\n        # Draw the original path\n        renderer.draw_path(gc0, path, affine, rgbFace)  # noqa: N803\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the markers to draw\n        for ind, marker_offset in zip(segment_indices, marker_offsets):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n            renderer.draw_path(gc0, self._symbol, sym_trans,\n                               self.color if self.filled else None)\n\n        gc0.restore()",
  "def __init__(self, color, size=10, spacing=1, flip=False):\n        \"\"\"Initialize the frontogenesis path effect.\n\n        Parameters\n        ----------\n        color : str or tuple[float]\n            Color to use for the effect.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbol should be flipped to the other side of the path. Defaults\n            to `False`.\n\n        \"\"\"\n        super().__init__(color, size, spacing, flip)\n        self._padding = 4",
  "def _step_size(self, renderer):\n        \"\"\"Return the length of the step between markers in pixels.\"\"\"\n        return (self.symbol_width + self.spacing + self._padding) * self._size_pixels(renderer)",
  "def _get_path_locations(self, segment_offsets, renderer):\n        # Calculate increment of path length occupied by each marker drawn\n        inc = self._step_size(renderer)\n\n        # Find out how many markers that will accommodate, as well as remainder space\n        num, leftover = divmod(segment_offsets[-1], inc)\n\n        # Find the offset for each marker along the path length. We center along\n        # the path by adding half of the remainder. The offset is also centered within\n        # the marker by adding half of the marker increment\n        marker_offsets = np.arange(num) * inc + (leftover + inc) / 2.\n\n        # Do the same for path segments\n        start_offsets = marker_offsets - 0.33 * inc\n        end_offsets = marker_offsets + 0.33 * inc\n\n        # Find the location of these offsets within the total offset within each\n        # path segment; subtracting 1 gives us the left point of the path rather\n        # than the last. We then need to adjust for any offsets that are <= the first\n        # point of the path (just set them to index 0).\n        inds = np.searchsorted(segment_offsets, marker_offsets) - 1\n        inds[inds < 0] = 0\n\n        start_inds = np.searchsorted(segment_offsets, start_offsets) - 1\n        start_inds[start_inds < 0] = 0\n\n        end_inds = np.searchsorted(segment_offsets, end_offsets) - 1\n        end_inds[start_inds < 0] = 0\n\n        # Return the indices to the proper segment and the offset within that segment\n        return start_inds, end_inds, inds, marker_offsets - segment_offsets[inds]",
  "def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        # Set up a new graphics context for rendering the front effect; override the color\n        gc0 = self._override_gc(renderer, gc, foreground=self.color)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon, how\n        # far within that segment the markers will appear, and the segment bounds.\n        (segment_starts, segment_ends,\n         segment_indices, marker_offsets) = self._get_path_locations(offsets, renderer)\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the segments to draw\n        for start_path, end_path in zip(segment_starts, segment_ends):\n            renderer.draw_path(gc0, mpath.Path(starts[start_path:end_path]),\n                               mtransforms.Affine2D(), None)\n\n        # Loop over all the markers to draw\n        for ind, marker_offset in zip(segment_indices, marker_offsets):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n\n            renderer.draw_path(gc0, self._symbol, sym_trans, self.color)\n\n        gc0.restore()",
  "def __init__(self, color, size=10, spacing=1, flip=False):\n        \"\"\"Initialize the frontolysis path effect.\n\n        Parameters\n        ----------\n        color : str or tuple[float]\n            Color to use for the effect.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbol should be flipped to the other side of the path. Defaults\n            to `False`.\n\n        \"\"\"\n        super().__init__(color, size, spacing, flip)\n        self._padding = 4",
  "def _step_size(self, renderer):\n        \"\"\"Return the length of the step between markers in pixels.\"\"\"\n        return (self.symbol_width + self.spacing + self._padding) * self._size_pixels(renderer)",
  "def _get_path_locations(self, segment_offsets, renderer):\n        # Calculate increment of path length occupied by each marker drawn\n        inc = self._step_size(renderer)\n\n        # Find out how many markers that will accommodate, as well as remainder space\n        num, leftover = divmod(segment_offsets[-1], inc)\n\n        # Find the offset for each marker along the path length. We center along\n        # the path by adding half of the remainder. The offset is also centered within\n        # the marker by adding half of the marker increment\n        marker_offsets = np.arange(num) * inc + (leftover + inc) / 2.\n\n        # Do the same for path segments\n        start_offsets = marker_offsets - 0.33 * inc\n        end_offsets = marker_offsets + 0.33 * inc\n\n        # Find the location of these offsets within the total offset within each\n        # path segment; subtracting 1 gives us the left point of the path rather\n        # than the last. We then need to adjust for any offsets that are <= the first\n        # point of the path (just set them to index 0).\n        inds = np.searchsorted(segment_offsets, marker_offsets) - 1\n        inds[inds < 0] = 0\n\n        start_inds = np.searchsorted(segment_offsets, start_offsets) - 1\n        start_inds[start_inds < 0] = 0\n\n        end_inds = np.searchsorted(segment_offsets, end_offsets) - 1\n        end_inds[start_inds < 0] = 0\n\n        # Return the indices to the proper segment and the offset within that segment\n        return start_inds, end_inds, inds, marker_offsets - segment_offsets[inds]",
  "def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        # Set up a new graphics context for rendering the front effect; override the color\n        gc0 = self._override_gc(renderer, gc, foreground=self.color)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon, how\n        # far within that segment the markers will appear, and the segment bounds.\n        (segment_starts, segment_ends,\n         segment_indices, marker_offsets) = self._get_path_locations(offsets, renderer)\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the segments to draw\n        for start_path, end_path in zip(segment_starts, segment_ends):\n            renderer.draw_path(gc0, mpath.Path(starts[start_path:end_path]),\n                               mtransforms.Affine2D(), None)\n\n        # Loop over all the markers to draw\n        for ind, marker_offset in zip(segment_indices[::2], marker_offsets[::2]):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n\n            renderer.draw_path(gc0, self._symbol, sym_trans, self.color)\n\n        gc0.restore()",
  "def __init__(self, offset=(0, 0), spacing=10.0, side='left', length=1.15, **kwargs):\n        \"\"\"Create a scalloped path effect.\n\n        Parameters\n        ----------\n        offset : (float, float)\n            The (x, y) offset to apply to the path, in points. Defaults to no offset.\n        spacing : float\n            The spacing between ticks in points. Defaults to 10.0.\n        side : str\n            Side of the path scallops appear on from the reference of\n            walking along the curve. Options are left and right. Defaults to ``'left'``.\n        length : float\n            The length of the tick relative to spacing. Defaults to 1.414.\n        kwargs :\n            Extra keywords are stored and passed through to\n            `~matplotlib.renderer.GraphicsContextBase`.\n        \"\"\"\n        super().__init__(offset)\n\n        self._spacing = spacing\n        if side == 'left':\n            self._angle = 90\n        elif side == 'right':\n            self._angle = -90\n        else:\n            raise ValueError('Side must be left or right.')\n        self._length = length\n        self._gc = kwargs",
  "def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the path with updated gc.\"\"\"\n        # Do not modify the input! Use copy instead.\n        gc0 = renderer.new_gc()\n        gc0.copy_properties(gc)\n\n        gc0 = self._update_gc(gc0, self._gc)\n        trans = affine + self._offset_transform(renderer)\n\n        theta = -np.radians(self._angle)\n        trans_matrix = np.array([[np.cos(theta), -np.sin(theta)],\n                                 [np.sin(theta), np.cos(theta)]])\n\n        # Convert spacing parameter to pixels.\n        spacing_px = renderer.points_to_pixels(self._spacing)\n\n        # Transform before evaluation because to_polygons works at resolution\n        # of one -- assuming it is working in pixel space.\n        transpath = affine.transform_path(path)\n\n        # Evaluate path to straight line segments that can be used to\n        # construct line scallops.\n        polys = transpath.to_polygons(closed_only=False)\n\n        for p in polys:\n            x = p[:, 0]\n            y = p[:, 1]\n\n            # Can not interpolate points or draw line if only one point in\n            # polyline.\n            if x.size < 2:\n                continue\n\n            # Find distance between points on the line\n            ds = np.hypot(x[1:] - x[:-1], y[1:] - y[:-1])\n\n            # Build parametric coordinate along curve\n            s = np.concatenate(([0.0], np.cumsum(ds)))\n            s_total = s[-1]\n\n            num = int(np.ceil(s_total / spacing_px)) - 1\n            # Pick parameter values for scallops.\n            s_tick = np.linspace(0, s_total, num)\n\n            # Find points along the parameterized curve\n            x_tick = np.interp(s_tick, s, x)\n            y_tick = np.interp(s_tick, s, y)\n\n            # Find unit vectors in local direction of curve\n            delta_s = self._spacing * .001\n            u = (np.interp(s_tick + delta_s, s, x) - x_tick) / delta_s\n            v = (np.interp(s_tick + delta_s, s, y) - y_tick) / delta_s\n\n            # Handle slope of end point\n            if (x_tick[-1], y_tick[-1]) == (x_tick[0], y_tick[0]):  # periodic\n                u[-1] = u[0]\n                v[-1] = v[0]\n            else:\n                u[-1] = u[-2]\n                v[-1] = v[-2]\n\n            # Normalize slope into unit slope vector.\n            n = np.hypot(u, v)\n            mask = n == 0\n            n[mask] = 1.0\n\n            uv = np.array([u / n, v / n]).T\n            uv[mask] = np.array([0, 0]).T\n\n            # Rotate and scale unit vector\n            dxy = np.dot(uv, trans_matrix) * self._length * spacing_px\n\n            # Build endpoints\n            x_end = x_tick + dxy[:, 0]\n            y_end = y_tick + dxy[:, 1]\n\n            # Interleave ticks to form Path vertices\n            xyt = np.empty((2 * num, 2), dtype=x_tick.dtype)\n            xyt[0::2, 0] = x_tick\n            xyt[1::2, 0] = x_end\n            xyt[0::2, 1] = y_tick\n            xyt[1::2, 1] = y_end\n\n            # Build path vertices that will define control points of the bezier curves\n            verts = []\n            i = 0\n            nverts = 0\n            while i < len(xyt) - 2:\n                verts.append(xyt[i, :])\n                verts.append(xyt[i + 1, :])\n                verts.append(xyt[i + 3, :])\n                verts.append(xyt[i + 2, :])\n                nverts += 1\n                i += 2\n\n            # Build up vector of Path codes\n            codes = np.tile([mpath.Path.LINETO, mpath.Path.CURVE4,\n                             mpath.Path.CURVE4, mpath.Path.CURVE4], nverts)\n            codes[0] = mpath.Path.MOVETO\n\n            # Construct and draw resulting path\n            h = mpath.Path(verts, codes)\n\n            # Transform back to data space during render\n            renderer.draw_path(gc0, h, affine.inverted() + trans, rgbFace)\n\n        gc0.restore()",
  "def __init__(self, color='blue', **kwargs):\n        super().__init__(color, **kwargs)",
  "def __init__(self, color='blue', **kwargs):\n        super().__init__(color, **kwargs)",
  "def __init__(self, color='blue', **kwargs):\n        super().__init__(color, **kwargs)",
  "def __init__(self, color='brown', spacing=0.144, filled=False, **kwargs):\n        super().__init__(color, spacing=spacing, filled=filled, **kwargs)",
  "def __init__(self, color='red', **kwargs):\n        super().__init__(color, **kwargs)",
  "def __init__(self, color='red', **kwargs):\n        super().__init__(color, **kwargs)",
  "def __init__(self, color='red', **kwargs):\n        super().__init__(color, **kwargs)",
  "def __init__(self, color='purple', **kwargs):\n        self._symbol_cycle = None\n        super().__init__(color, **kwargs)",
  "def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        self._symbol_cycle = None\n        return super().draw_path(renderer, gc, path, affine, rgbFace)",
  "def _symbol(self):\n        \"\"\"Return the proper symbol to draw; alternatives between scallop and pip/triangle.\"\"\"\n        if self._symbol_cycle is None:\n            self._symbol_cycle = itertools.cycle([WarmFront._symbol, ColdFront._symbol])\n        return next(self._symbol_cycle)",
  "def __init__(self, color='purple', **kwargs):\n        self._symbol_cycle = None\n        super().__init__(color, **kwargs)",
  "def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        self._symbol_cycle = None\n        return super().draw_path(renderer, gc, path, affine, rgbFace)",
  "def _symbol(self):\n        \"\"\"Return the proper symbol to draw; alternatives between scallop and pip/triangle.\"\"\"\n        if self._symbol_cycle is None:\n            self._symbol_cycle = itertools.cycle([WarmFrontogenesis._symbol,\n                                                  ColdFrontogenesis._symbol])\n        return next(self._symbol_cycle)",
  "def __init__(self, color='purple', **kwargs):\n        self._symbol_cycle = None\n        super().__init__(color, **kwargs)",
  "def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        self._symbol_cycle = None\n        return super().draw_path(renderer, gc, path, affine, rgbFace)",
  "def _symbol(self):\n        \"\"\"Return the proper symbol to draw; alternatives between scallop and pip/triangle.\"\"\"\n        if self._symbol_cycle is None:\n            self._symbol_cycle = itertools.cycle([WarmFrontolysis._symbol,\n                                                  ColdFrontolysis._symbol])\n        return next(self._symbol_cycle)",
  "def __init__(self, color='black', spacing=12.0, length=0.5):\n        \"\"\"Create ridge axis path effect.\n\n        Parameters\n        ----------\n        color : str\n            Color to use for the effect.\n        spacing : float\n            The spacing between ticks in points. Default is 12.\n        length : float\n            The length of the tick relative to spacing. Default is 0.5.\n\n        \"\"\"\n        self._spacing = spacing\n        self._angle = 90.0\n        self._length = length\n        self._color = color",
  "def _override_gc(self, renderer, gc, **kwargs):\n        ret = renderer.new_gc()\n        ret.copy_properties(gc)\n        ret.set_joinstyle('miter')\n        ret.set_capstyle('butt')\n        return self._update_gc(ret, kwargs)",
  "def draw_path(self, renderer, gc, tpath, affine, rgbFace):  # noqa: N803\n        \"\"\"Draw the path with updated gc.\"\"\"\n        # Do not modify the input! Use copy instead.\n        gc0 = self._override_gc(renderer, gc, foreground=self._color)\n\n        theta = -np.radians(self._angle)\n        trans_matrix = np.array([[np.cos(theta), -np.sin(theta)],\n                                 [np.sin(theta), np.cos(theta)]])\n\n        # Convert spacing parameter to pixels.\n        spacing_px = renderer.points_to_pixels(self._spacing)\n\n        # Transform before evaluation because to_polygons works at resolution\n        # of one -- assuming it is working in pixel space.\n        transpath = affine.transform_path(tpath)\n\n        # Evaluate path to straight line segments that can be used to\n        # construct line ticks.\n        polys = transpath.to_polygons(closed_only=False)\n\n        for p in polys:\n            x = p[:, 0]\n            y = p[:, 1]\n\n            # Can not interpolate points or draw line if only one point in\n            # polyline.\n            if x.size < 2:\n                continue\n\n            # Find distance between points on the line\n            ds = np.hypot(x[1:] - x[:-1], y[1:] - y[:-1])\n\n            # Build parametric coordinate along curve\n            s = np.concatenate(([0.0], np.cumsum(ds)))\n            s_total = s[-1]\n\n            num = int(np.ceil(s_total / spacing_px)) - 1\n            # Pick parameter values for ticks.\n            s_tick = np.linspace(spacing_px / 2, s_total - spacing_px / 2, num)\n\n            # Find points along the parameterized curve\n            x_tick = np.interp(s_tick, s, x)\n            y_tick = np.interp(s_tick, s, y)\n\n            # Find unit vectors in local direction of curve\n            delta_s = self._spacing * .001\n            u = (np.interp(s_tick + delta_s, s, x) - x_tick) / delta_s\n            v = (np.interp(s_tick + delta_s, s, y) - y_tick) / delta_s\n\n            # Normalize slope into unit slope vector.\n            n = np.hypot(u, v)\n            mask = n == 0\n            n[mask] = 1.0\n\n            uv = np.array([u / n, v / n]).T\n            uv[mask] = np.array([0, 0]).T\n\n            # Rotate and scale unit vector into tick vector\n            dxy1 = np.dot(uv[0::2], trans_matrix) * self._length * spacing_px\n            dxy2 = np.dot(uv[1::2], trans_matrix.T) * self._length * spacing_px\n\n            # Build tick endpoints\n            x_end = np.zeros(num)\n            y_end = np.zeros(num)\n            x_end[0::2] = x_tick[0::2] + dxy1[:, 0]\n            x_end[1::2] = x_tick[1::2] + dxy2[:, 0]\n            y_end[0::2] = y_tick[0::2] + dxy1[:, 1]\n            y_end[1::2] = y_tick[1::2] + dxy2[:, 1]\n\n            # Interleave ticks to form Path vertices\n            xyt = np.empty((num, 2), dtype=x_tick.dtype)\n            xyt[:, 0] = x_end\n            xyt[:, 1] = y_end\n\n            # Build up vector of Path codes\n            codes = np.concatenate([[mpath.Path.MOVETO], [mpath.Path.LINETO] * (len(xyt) - 1)])\n\n            # Construct and draw resulting path\n            h = mpath.Path(xyt, codes)\n\n            # Transform back to data space during render\n            renderer.draw_path(gc0, h, affine.inverted() + affine, rgbFace)  # noqa: N803\n\n        gc0.restore()",
  "def __init__(self, color='black', spacing=75):\n        \"\"\"Initialize the squall line path effect.\n\n        Parameters\n        ----------\n        color : str\n            Color to use for the effect.\n        spacing : float\n            Spacing between symbols along path (in points).\n\n        \"\"\"\n        self.marker_margin = 10\n        self.spacing = spacing\n        self.color = mcolors.to_rgba(color)\n        self._symbol_width = None",
  "def _process_path(path, path_trans):\n        \"\"\"Transform the main path into pixel coordinates; calculate the needed components.\"\"\"\n        path_points = path.transformed(path_trans).interpolated(500).vertices\n        deltas = (path_points[1:] - path_points[:-1]).T\n        pt_offsets = np.concatenate(([0], np.hypot(*deltas).cumsum()))\n        return path_points, pt_offsets",
  "def _override_gc(self, renderer, gc, **kwargs):\n        ret = renderer.new_gc()\n        ret.copy_properties(gc)\n        ret.set_joinstyle('miter')\n        ret.set_capstyle('butt')\n        return self._update_gc(ret, kwargs)",
  "def _get_object_locations(self, segment_offsets, renderer):\n        # Calculate increment of path length\n        inc = renderer.points_to_pixels(self.spacing)\n        margin = renderer.points_to_pixels(self.marker_margin)\n\n        # Find out how many markers that will accommodate, as well as remainder space\n        num, leftover = divmod(segment_offsets[-1], inc)\n\n        # Find the offset for each marker along the path length. We center along\n        # the path by adding half of the remainder. The offset is also centered within\n        # the marker by adding half of the marker increment\n        first_marker = np.arange(num) * inc - 0.5 * margin + (leftover + inc) / 2.\n        second_marker = np.arange(num) * inc + 0.5 * margin + (leftover + inc) / 2.\n        marker_offsets = np.sort(np.concatenate([first_marker, second_marker]))\n\n        # Do the same for path segments\n        first = segment_offsets[0]\n        last = segment_offsets[-1]\n        path_offset_1 = np.arange(num) * inc - 1.5 * margin + (leftover + inc) / 2\n        path_offset_2 = np.arange(num) * inc + 1.5 * margin + (leftover + inc) / 2\n        path_offsets = np.sort(np.concatenate(\n            [[first], path_offset_1, path_offset_2, [last]]\n        ))\n\n        # Find the location of these offsets within the total offset within each\n        # path segment; subtracting 1 gives us the left point of the path rather\n        # than the last. We then need to adjust for any offsets that are <= the first\n        # point of the path (just set them to index 0).\n        marker_inds = np.searchsorted(segment_offsets, marker_offsets) - 1\n        marker_inds[marker_inds < 0] = 0\n\n        # Do the same for path segments\n        path_inds = np.searchsorted(segment_offsets, path_offsets) - 1\n        path_inds[path_inds < 0] = 0\n\n        # Return the indices to the proper segment and the offset within that segment\n        return marker_inds, path_inds",
  "def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw path.\"\"\"\n        # Set up a new graphics context for rendering the front effect; override the color\n        gc0 = self._override_gc(renderer, gc, foreground=self.color)\n\n        # Get the information we need for drawing along the path\n        starts, offsets = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon and how\n        # far within that segment the markers will appear.\n        marker_indices, path_indices = self._get_object_locations(offsets, renderer)\n\n        base_trans = mtransforms.Affine2D()\n\n        # Loop over the segmented path\n        ipath = path.interpolated(500).vertices\n        for i in range(0, len(path_indices) - 1, 2):\n            start = path_indices[i]\n            stop = path_indices[i + 1]\n            n = stop - start\n            spath = mpath.Path(\n                ipath[start:stop],\n                [mpath.Path.MOVETO] + [mpath.Path.LINETO] * (n - 1)\n            )\n            renderer.draw_path(gc0, spath, affine, None)\n\n        # Loop over all the markers to draw\n        for ind in marker_indices:\n            sym_trans = base_trans.frozen().translate(*starts[ind])\n            renderer.draw_path(gc0, self.symbol, sym_trans, self.color)\n\n        gc0.restore()",
  "def __init__(self, colors=('red', 'blue'), **kwargs):\n        \"\"\"Initialize a stationary front path effect.\n\n        This effect alternates between a warm front and cold front symbol.\n\n        Parameters\n        ----------\n        colors : Sequence[str] or Sequence[tuple[float]]\n            Matplotlib color identifiers to cycle between on the two different front styles.\n            Defaults to alternating red and blue.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbols should be flipped to the other side of the path. Defaults\n            to `False`.\n\n        \"\"\"\n        self._colors = list(map(mcolors.to_rgba, colors))\n        super().__init__(color=self._colors[0], **kwargs)",
  "def _get_path_segment_ends(self, segment_offsets, renderer):\n        # Calculate increment of path length occupied by each marker drawn\n        inc = self._step_size(renderer)\n\n        # Find out how many markers that will accommodate, as well as remainder space\n        num, leftover = divmod(segment_offsets[-1], inc)\n\n        # Find the offset for each path segment end. We center along\n        # the entire path by adding half of the remainder.\n        path_offsets = np.arange(1, num + 1) * inc + leftover / 2.\n\n        # Find the location of these offsets within the total offset within each\n        # path segment; subtracting 1 gives us the left point of the path rather\n        # than the last.\n        return np.searchsorted(segment_offsets, path_offsets)",
  "def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        gcs = [self._override_gc(renderer, gc, foreground=color) for color in self._colors]\n        self._gc_cycle = itertools.cycle(gcs)\n        self._symbol_cycle = itertools.cycle([self._symbol, self._symbol2])\n        self._color_cycle = itertools.cycle(self._colors)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon and how\n        # far within that segment the markers will appear.\n        segment_indices, marker_offsets = self._get_marker_locations(offsets, renderer)\n        end_path_inds = self._get_path_segment_ends(offsets, renderer)\n        start_path_inds = np.concatenate([[0], end_path_inds[:-1]])\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = -renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the markers to draw\n        for ind, start_path, end_path, marker_offset in zip(segment_indices, start_path_inds,\n                                                            end_path_inds, marker_offsets):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n            gc = next(self._gc_cycle)\n            color = next(self._color_cycle)\n            symbol = next(self._symbol_cycle)\n\n            renderer.draw_path(gc, symbol, sym_trans, color)\n            renderer.draw_path(gc, mpath.Path(starts[start_path:end_path]),\n                               mtransforms.Affine2D(), None)\n            line_shift *= -1\n\n        gcs[0].restore()",
  "def __init__(self, colors=('red', 'blue'), **kwargs):\n        \"\"\"Initialize a strengthening stationary front path effect.\n\n        This effect alternates between a warm front and cold front symbol.\n\n        Parameters\n        ----------\n        colors : Sequence[str] or Sequence[tuple[float]]\n            Matplotlib color identifiers to cycle between on the two different front styles.\n            Defaults to alternating red and blue.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbols should be flipped to the other side of the path. Defaults\n            to `False`.\n\n        \"\"\"\n        self._colors = list(map(mcolors.to_rgba, colors))\n        super().__init__(color=self._colors[0], **kwargs)",
  "def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        gcs = [self._override_gc(renderer, gc, foreground=color) for color in self._colors]\n        self._gc_cycle = itertools.cycle(gcs)\n        self._symbol_cycle = itertools.cycle([self._symbol, self._symbol2])\n        self._color_cycle = itertools.cycle(self._colors)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon, how\n        # far within that segment the markers will appear, and the segment bounds.\n        (segment_starts, segment_ends,\n         segment_indices, marker_offsets) = self._get_path_locations(offsets, renderer)\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the markers to draw\n        for ind, start_path, end_path, marker_offset in zip(segment_indices, segment_starts,\n                                                            segment_ends, marker_offsets):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n            gc = next(self._gc_cycle)\n            color = next(self._color_cycle)\n            symbol = next(self._symbol_cycle)\n\n            renderer.draw_path(gc, symbol, sym_trans, color)\n            renderer.draw_path(gc, mpath.Path(starts[start_path:end_path]),\n                               mtransforms.Affine2D(), None)\n            line_shift *= -1\n\n        gcs[0].restore()",
  "def __init__(self, colors=('red', 'blue'), **kwargs):\n        \"\"\"Initialize a weakening stationary front path effect.\n\n        This effect alternates between a warm front and cold front symbol.\n\n        Parameters\n        ----------\n        colors : Sequence[str] or Sequence[tuple[float]]\n            Matplotlib color identifiers to cycle between on the two different front styles.\n            Defaults to alternating red and blue.\n        size : int or float\n            The size of the markers to plot in points. Defaults to 10.\n        spacing : int or float\n            The spacing between markers in normalized coordinates. Defaults to 1.\n        flip : bool\n            Whether the symbols should be flipped to the other side of the path. Defaults\n            to `False`.\n\n        \"\"\"\n        self._colors = list(map(mcolors.to_rgba, colors))\n        self._segment_colors = [\n            (self._colors[0], self._colors[0]),\n            (self._colors[0], self._colors[1]),\n            (self._colors[1], self._colors[1]),\n            (self._colors[1], self._colors[0])\n        ]\n        super().__init__(color=self._colors[0], **kwargs)",
  "def draw_path(self, renderer, gc, path, affine, rgbFace=None):  # noqa: N803\n        \"\"\"Draw the given path.\"\"\"\n        gcs = [self._override_gc(renderer, gc, foreground=color) for color in self._colors]\n        self._gc_cycle = itertools.cycle(gcs)\n        self._symbol_cycle = itertools.cycle([self._symbol, self._symbol2])\n        self._color_cycle = itertools.cycle(self._colors)\n        self._segment_cycle = itertools.cycle(self._segment_colors)\n\n        # Get the information we need for drawing along the path\n        starts, offsets, angles = self._process_path(path, affine)\n\n        # Figure out what segments the markers should be drawn upon, how\n        # far within that segment the markers will appear, and the segment bounds.\n        (segment_starts, segment_ends,\n         segment_indices, marker_offsets) = self._get_path_locations(offsets, renderer)\n\n        # Need to account for the line width in order to properly draw symbols at line edge\n        line_shift = renderer.points_to_pixels(gc.get_linewidth()) / 2\n\n        # Loop over all the markers to draw\n        for ind, marker_offset in zip(segment_indices[::2], marker_offsets[::2]):\n            sym_trans = self._get_symbol_transform(renderer, marker_offset, line_shift,\n                                                   angles[ind], starts[ind])\n            gc = next(self._gc_cycle)\n            color = next(self._color_cycle)\n            symbol = next(self._symbol_cycle)\n\n            renderer.draw_path(gc, symbol, sym_trans, color)\n\n            line_shift *= -1\n\n        for start_path, mid_path, end_path in zip(segment_starts,\n                                                  segment_indices,\n                                                  segment_ends):\n            color1, color2 = next(self._segment_cycle)\n\n            gcx = self._override_gc(renderer, gc, foreground=mcolors.to_rgb(color1))\n            renderer.draw_path(gcx, mpath.Path(starts[start_path:mid_path]),\n                               mtransforms.Affine2D(), None)\n\n            gcx = self._override_gc(renderer, gc, foreground=mcolors.to_rgb(color2))\n            renderer.draw_path(gcx, mpath.Path(starts[mid_path:end_path]),\n                               mtransforms.Affine2D(), None)\n\n        gcs[0].restore()",
  "class SkewTTransform(transforms.Affine2D):\n    \"\"\"Perform Skew transform for Skew-T plotting.\n\n    This works in pixel space, so is designed to be applied after the normal plotting\n    transformations.\n    \"\"\"\n\n    def __init__(self, bbox, rot):\n        \"\"\"Initialize skew transform.\n\n        This needs a reference to the parent bounding box to do the appropriate math and\n        to register it as a child so that the transform is invalidated and regenerated if\n        the bounding box changes.\n        \"\"\"\n        super().__init__()\n        self._bbox = bbox\n        self.set_children(bbox)\n        self.invalidate()\n\n        # We're not trying to support changing the rotation, so go ahead and convert to\n        # the right factor for skewing here and just save that.\n        self._rot_factor = np.tan(np.deg2rad(rot))\n\n    def get_matrix(self):\n        \"\"\"Return transformation matrix.\"\"\"\n        if self._invalid:\n            # The following matrix is equivalent to the following:\n            # x0, y0 = self._bbox.xmin, self._bbox.ymin\n            # self.translate(-x0, -y0).skew_deg(self._rot, 0).translate(x0, y0)\n            # Setting it this way is just more efficient.\n            self._mtx = np.array([[1.0, self._rot_factor, -self._rot_factor * self._bbox.ymin],\n                                  [0.0, 1.0, 0.0],\n                                  [0.0, 0.0, 1.0]])\n\n            # Need to clear both the invalid flag *and* reset the inverse, which is cached\n            # by the parent class.\n            self._invalid = 0\n            self._inverted = None\n        return self._mtx",
  "class SkewXTick(maxis.XTick):\n    r\"\"\"Make x-axis ticks for Skew-T plots.\n\n    This class adds to the standard :class:`matplotlib.axis.XTick` dynamic checking\n    for whether a top or bottom tick is actually within the data limits at that part\n    and draw as appropriate. It also performs similar checking for gridlines.\n    \"\"\"\n\n    # Taken from matplotlib's SkewT example to update for matplotlib 3.1's changes to\n    # state management for ticks. See matplotlib/matplotlib#10088\n    def draw(self, renderer):\n        \"\"\"Draw the tick.\"\"\"\n        # When adding the callbacks with `stack.callback`, we fetch the current\n        # visibility state of the artist with `get_visible`; the ExitStack will\n        # restore these states (`set_visible`) at the end of the block (after\n        # the draw).\n        with ExitStack() as stack:\n            for artist in [self.gridline, self.tick1line, self.tick2line,\n                           self.label1, self.label2]:\n                stack.callback(artist.set_visible, artist.get_visible())\n\n            self.tick1line.set_visible(self.tick1line.get_visible() and self.lower_in_bounds)\n            self.label1.set_visible(self.label1.get_visible() and self.lower_in_bounds)\n            self.tick2line.set_visible(self.tick2line.get_visible() and self.upper_in_bounds)\n            self.label2.set_visible(self.label2.get_visible() and self.upper_in_bounds)\n            self.gridline.set_visible(self.gridline.get_visible() and self.grid_in_bounds)\n            super().draw(renderer)\n\n    @property\n    def lower_in_bounds(self):\n        \"\"\"Whether the lower part of the tick is in bounds.\"\"\"\n        return transforms.interval_contains(self.axes.lower_xlim, self.get_loc())\n\n    @property\n    def upper_in_bounds(self):\n        \"\"\"Whether the upper part of the tick is in bounds.\"\"\"\n        return transforms.interval_contains(self.axes.upper_xlim, self.get_loc())\n\n    @property\n    def grid_in_bounds(self):\n        \"\"\"Whether any of the tick grid line is in bounds.\"\"\"\n        return transforms.interval_contains(self.axes.xaxis.get_view_interval(),\n                                            self.get_loc())",
  "class SkewXAxis(maxis.XAxis):\n    r\"\"\"Make an x-axis that works properly for Skew-T plots.\n\n    This class exists to force the use of our custom :class:`SkewXTick` as well\n    as provide a custom value for interval that combines the extents of the\n    upper and lower x-limits from the axes.\n    \"\"\"\n\n    def _get_tick(self, major):\n        return SkewXTick(self.axes, None, major=major)\n\n    # Needed to properly handle tight bbox\n    def _get_ticklabel_bboxes(self, ticks, renderer):\n        \"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\n        return ([tick.label1.get_window_extent(renderer)\n                 for tick in ticks if tick.label1.get_visible() and tick.lower_in_bounds],\n                [tick.label2.get_window_extent(renderer)\n                 for tick in ticks if tick.label2.get_visible() and tick.upper_in_bounds])\n\n    # Older name used on Matplotlib < 3.6\n    _get_tick_bboxes = _get_ticklabel_bboxes\n\n    def get_view_interval(self):\n        \"\"\"Get the view interval.\"\"\"\n        return self.axes.upper_xlim[0], self.axes.lower_xlim[1]",
  "class SkewSpine(mspines.Spine):\n    r\"\"\"Make an x-axis spine that works properly for Skew-T plots.\n\n    This class exists to use the separate x-limits from the axes to properly\n    locate the spine.\n    \"\"\"\n\n    def _adjust_location(self):\n        pts = self._path.vertices\n        if self.spine_type == 'top':\n            pts[:, 0] = self.axes.upper_xlim\n        else:\n            pts[:, 0] = self.axes.lower_xlim",
  "class SkewXAxes(Axes):\n    r\"\"\"Make a set of axes for Skew-T plots.\n\n    This class handles registration of the skew-xaxes as a projection as well as setting up\n    the appropriate transformations. It also makes sure we use our instances for spines\n    and x-axis: :class:`SkewSpine` and :class:`SkewXAxis`. It provides properties to\n    facilitate finding the x-limits for the bottom and top of the plot as well.\n    \"\"\"\n\n    # The projection must specify a name.  This will be used be the\n    # user to select the projection, i.e. ``subplot(111,\n    # projection='skewx')``.\n    name = 'skewx'\n\n    def __init__(self, *args, **kwargs):\n        r\"\"\"Initialize `SkewXAxes`.\n\n        Parameters\n        ----------\n        args : Arbitrary positional arguments\n            Passed to :class:`matplotlib.axes.Axes`\n\n        position: int, optional\n            The rotation of the x-axis against the y-axis, in degrees.\n\n        kwargs : Arbitrary keyword arguments\n            Passed to :class:`matplotlib.axes.Axes`\n\n        \"\"\"\n        # This needs to be popped and set before moving on\n        self.rot = kwargs.pop('rotation', 30)\n        super().__init__(*args, **kwargs)\n\n    def _init_axis(self):\n        # Taken from Axes and modified to use our modified X-axis\n        self.xaxis = SkewXAxis(self)\n        self.spines['top'].register_axis(self.xaxis)\n        self.spines['bottom'].register_axis(self.xaxis)\n        self.yaxis = maxis.YAxis(self)\n        self.spines['left'].register_axis(self.yaxis)\n        self.spines['right'].register_axis(self.yaxis)\n\n    def _gen_axes_spines(self, locations=None, offset=0.0, units='inches'):\n        # pylint: disable=unused-argument\n        return {'top': SkewSpine.linear_spine(self, 'top'),\n                'bottom': mspines.Spine.linear_spine(self, 'bottom'),\n                'left': mspines.Spine.linear_spine(self, 'left'),\n                'right': mspines.Spine.linear_spine(self, 'right')}\n\n    def _set_lim_and_transforms(self):\n        \"\"\"Set limits and transforms.\n\n        This is called once when the plot is created to set up all the\n        transforms for the data, text and grids.\n\n        \"\"\"\n        # Get the standard transform setup from the Axes base class\n        super()._set_lim_and_transforms()\n\n        # This transformation handles the skewing\n        skew_trans = SkewTTransform(self.bbox, self.rot)\n\n        # Create the full transform from Data to Pixels\n        self.transData += skew_trans\n\n        # Blended transforms like this need to have the skewing applied using\n        # both axes, in axes coords like before.\n        self._xaxis_transform += skew_trans\n\n    @property\n    def lower_xlim(self):\n        \"\"\"Get the data limits for the x-axis along the bottom of the axes.\"\"\"\n        return self.axes.viewLim.intervalx\n\n    @property\n    def upper_xlim(self):\n        \"\"\"Get the data limits for the x-axis along the top of the axes.\"\"\"\n        return self.transData.inverted().transform([[self.bbox.xmin, self.bbox.ymax],\n                                                    self.bbox.max])[:, 0]",
  "class SkewT:\n    r\"\"\"Make Skew-T log-P plots of data.\n\n    This class simplifies the process of creating Skew-T log-P plots in\n    using matplotlib. It handles requesting the appropriate skewed projection,\n    and provides simplified wrappers to make it easy to plot data, add wind\n    barbs, and add other lines to the plots (e.g. dry adiabats)\n\n    Attributes\n    ----------\n    ax : `matplotlib.axes.Axes`\n        The underlying Axes instance, which can be used for calling additional\n        plot functions (e.g. `axvline`)\n\n    \"\"\"\n\n    def __init__(self, fig=None, rotation=30, subplot=None, rect=None, aspect=80.5):\n        r\"\"\"Create SkewT - logP plots.\n\n        Parameters\n        ----------\n        fig : matplotlib.figure.Figure, optional\n            Source figure to use for plotting. If none is given, a new\n            :class:`matplotlib.figure.Figure` instance will be created.\n        rotation : float or int, optional\n            Controls the rotation of temperature relative to horizontal. Given\n            in degrees counterclockwise from x-axis. Defaults to 30 degrees.\n        subplot : tuple[int, int, int] or `matplotlib.gridspec.SubplotSpec` instance, optional\n            Controls the size/position of the created subplot. This allows creating\n            the skewT as part of a collection of subplots. If subplot is a tuple, it\n            should conform to the specification used for\n            :meth:`matplotlib.figure.Figure.add_subplot`. The\n            :class:`matplotlib.gridspec.SubplotSpec`\n            can be created by using :class:`matplotlib.gridspec.GridSpec`.\n        rect : tuple[float, float, float, float], optional\n            Rectangle (left, bottom, width, height) in which to place the axes. This\n            allows the user to place the axes at an arbitrary point on the figure.\n        aspect : float, int, or Literal['auto'], optional\n            Aspect ratio (i.e. ratio of y-scale to x-scale) to maintain in the plot.\n            Defaults to 80.5. Passing the string ``'auto'`` tells matplotlib to handle\n            the aspect ratio automatically (this is not recommended for SkewT).\n\n        \"\"\"\n        if fig is None:\n            import matplotlib.pyplot as plt\n            figsize = plt.rcParams.get('figure.figsize', (7, 7))\n            fig = plt.figure(figsize=figsize)\n        self._fig = fig\n\n        if rect and subplot:\n            raise ValueError(\"Specify only one of `rect' and `subplot', but not both\")\n\n        elif rect:\n            self.ax = fig.add_axes(rect, projection='skewx', rotation=rotation)\n\n        else:\n            if subplot is not None:\n                # Handle being passed a tuple for the subplot, or a GridSpec instance\n                try:\n                    len(subplot)\n                except TypeError:\n                    subplot = (subplot,)\n            else:\n                subplot = (1, 1, 1)\n\n            self.ax = fig.add_subplot(*subplot, projection='skewx', rotation=rotation)\n\n        # Set the yaxis as inverted with log scaling\n        self.ax.set_yscale('log')\n\n        # Override default ticking for log scaling\n        self.ax.yaxis.set_major_formatter(ScalarFormatter())\n        self.ax.yaxis.set_major_locator(MultipleLocator(100))\n        self.ax.yaxis.set_minor_formatter(NullFormatter())\n\n        # Needed to make sure matplotlib doesn't freak out and create a bunch of ticks\n        # Also takes care of inverting the y-axis\n        self.ax.set_ylim(1050, 100)\n        self.ax.yaxis.set_units(units.hPa)\n\n        # Try to make sane default temperature plotting ticks\n        self.ax.xaxis.set_major_locator(MultipleLocator(10))\n        self.ax.xaxis.set_units(units.degC)\n        self.ax.set_xlim(-40, 50)\n        self.ax.grid(True)\n\n        self.mixing_lines = None\n        self.dry_adiabats = None\n        self.moist_adiabats = None\n\n        # Maintain a reasonable ratio of data limits.\n        self.ax.set_aspect(aspect, adjustable='box')\n\n    def plot(self, pressure, t, *args, **kwargs):\n        r\"\"\"Plot data.\n\n        Simple wrapper around plot so that pressure is the first (independent)\n        input. This is essentially a wrapper around `plot`.\n\n        Parameters\n        ----------\n        pressure : array-like\n            pressure values\n        t : array-like\n            temperature values, can also be used for things like dew point\n        args\n            Other positional arguments to pass to :func:`~matplotlib.pyplot.plot`\n        kwargs\n            Other keyword arguments to pass to :func:`~matplotlib.pyplot.plot`\n\n        Returns\n        -------\n        list[matplotlib.lines.Line2D]\n            lines plotted\n\n        See Also\n        --------\n        :func:`matplotlib.pyplot.plot`\n\n        \"\"\"\n        # Skew-T logP plotting\n        t, pressure = _delete_masked_points(t, pressure)\n        return self.ax.plot(t, pressure, *args, **kwargs)\n\n    def plot_barbs(self, pressure, u, v, c=None, xloc=1.0, x_clip_radius=0.1,\n                   y_clip_radius=0.08, **kwargs):\n        r\"\"\"Plot wind barbs.\n\n        Adds wind barbs to the skew-T plot. This is a wrapper around the\n        `barbs` command that adds to appropriate transform to place the\n        barbs in a vertical line, located as a function of pressure.\n\n        Parameters\n        ----------\n        pressure : array-like\n            pressure values\n        u : array-like\n            U (East-West) component of wind\n        v : array-like\n            V (North-South) component of wind\n        c : array-like, optional\n            An optional array used to map colors to the barbs\n        xloc : float, optional\n            Position for the barbs, in normalized axes coordinates, where 0.0\n            denotes far left and 1.0 denotes far right. Defaults to far right.\n        x_clip_radius : float, optional\n            Space, in normalized axes coordinates, to leave before clipping\n            wind barbs in the x-direction. Defaults to 0.1.\n        y_clip_radius : float, optional\n            Space, in normalized axes coordinates, to leave above/below plot\n            before clipping wind barbs in the y-direction. Defaults to 0.08.\n        plot_units: `pint.Unit`\n            Units to plot in (performing conversion if necessary). Defaults to given units.\n        kwargs\n            Other keyword arguments to pass to :func:`~matplotlib.pyplot.barbs`\n\n        Returns\n        -------\n        matplotlib.quiver.Barbs\n            instance created\n\n        See Also\n        --------\n        :func:`matplotlib.pyplot.barbs`\n\n        \"\"\"\n        # If plot_units specified, convert the data to those units\n        plotting_units = kwargs.pop('plot_units', None)\n        if plotting_units:\n            if hasattr(u, 'units') and hasattr(v, 'units'):\n                u = u.to(plotting_units)\n                v = v.to(plotting_units)\n            else:\n                raise ValueError('To convert to plotting units, units must be attached to '\n                                 'u and v wind components.')\n\n        # Assemble array of x-locations in axes space\n        x = np.empty_like(pressure)\n        x.fill(xloc)\n\n        # Do barbs plot at this location\n        if c is not None:\n            b = self.ax.barbs(x, pressure, u, v, c,\n                              transform=self.ax.get_yaxis_transform(which='tick2'),\n                              clip_on=True, zorder=2, **kwargs)\n        else:\n            b = self.ax.barbs(x, pressure, u, v,\n                              transform=self.ax.get_yaxis_transform(which='tick2'),\n                              clip_on=True, zorder=2, **kwargs)\n\n        # Override the default clip box, which is the axes rectangle, so we can have\n        # barbs that extend outside.\n        ax_bbox = transforms.Bbox([[xloc - x_clip_radius, -y_clip_radius],\n                                   [xloc + x_clip_radius, 1.0 + y_clip_radius]])\n        b.set_clip_box(transforms.TransformedBbox(ax_bbox, self.ax.transAxes))\n        return b\n\n    def plot_dry_adiabats(self, t0=None, pressure=None, **kwargs):\n        r\"\"\"Plot dry adiabats.\n\n        Adds dry adiabats (lines of constant potential temperature) to the\n        plot. The default style of these lines is dashed red lines with an alpha\n        value of 0.5. These can be overridden using keyword arguments.\n\n        Parameters\n        ----------\n        t0 : array-like, optional\n            Starting temperature values in Kelvin. If none are given, they will be\n            generated using the current temperature range at the bottom of\n            the plot.\n        pressure : array-like, optional\n            Pressure values to be included in the dry adiabats. If not\n            specified, they will be linearly distributed across the current\n            plotted pressure range.\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.LineCollection`\n\n        Returns\n        -------\n        matplotlib.collections.LineCollection\n            instance created\n\n        See Also\n        --------\n        :func:`~metpy.calc.dry_lapse`\n        :meth:`plot_moist_adiabats`\n        :class:`matplotlib.collections.LineCollection`\n\n        \"\"\"\n        # Remove old lines\n        if self.dry_adiabats:\n            self.dry_adiabats.remove()\n\n        # Determine set of starting temps if necessary\n        if t0 is None:\n            xmin, xmax = self.ax.get_xlim()\n            t0 = units.Quantity(np.arange(xmin, xmax + 1, 10), 'degC')\n\n        # Get pressure levels based on ylims if necessary\n        if pressure is None:\n            pressure = units.Quantity(np.linspace(*self.ax.get_ylim()), 'mbar')\n\n        # Assemble into data for plotting\n        t = dry_lapse(pressure, t0[:, np.newaxis],\n                      units.Quantity(1000., 'mbar')).to(units.degC)\n        linedata = [np.vstack((ti.m, pressure.m)).T for ti in t]\n\n        # Add to plot\n        kwargs.setdefault('colors', 'r')\n        kwargs.setdefault('linestyles', 'dashed')\n        kwargs.setdefault('alpha', 0.5)\n        kwargs.setdefault('zorder', Line2D.zorder - 0.001)\n        self.dry_adiabats = self.ax.add_collection(LineCollection(linedata, **kwargs))\n        return self.dry_adiabats\n\n    def plot_moist_adiabats(self, t0=None, pressure=None, **kwargs):\n        r\"\"\"Plot moist adiabats.\n\n        Adds saturated pseudo-adiabats (lines of constant equivalent potential\n        temperature) to the plot. The default style of these lines is dashed\n        blue lines with an alpha value of 0.5. These can be overridden using\n        keyword arguments.\n\n        Parameters\n        ----------\n        t0 : array-like, optional\n            Starting temperature values in Kelvin. If none are given, they will be\n            generated using the current temperature range at the bottom of\n            the plot.\n        pressure : array-like, optional\n            Pressure values to be included in the moist adiabats. If not\n            specified, they will be linearly distributed across the current\n            plotted pressure range.\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.LineCollection`\n\n        Returns\n        -------\n        matplotlib.collections.LineCollection\n            instance created\n\n        See Also\n        --------\n        :func:`~metpy.calc.moist_lapse`\n        :meth:`plot_dry_adiabats`\n        :class:`matplotlib.collections.LineCollection`\n\n        \"\"\"\n        # Remove old lines\n        if self.moist_adiabats:\n            self.moist_adiabats.remove()\n\n        # Determine set of starting temps if necessary\n        if t0 is None:\n            xmin, xmax = self.ax.get_xlim()\n            t0 = units.Quantity(np.concatenate((np.arange(xmin, 0, 10),\n                                                np.arange(0, xmax + 1, 5))), 'degC')\n\n        # Get pressure levels based on ylims if necessary\n        if pressure is None:\n            pressure = units.Quantity(np.linspace(*self.ax.get_ylim()), 'mbar')\n\n        # Assemble into data for plotting\n        t = moist_lapse(pressure, t0, units.Quantity(1000., 'mbar')).to(units.degC)\n        linedata = [np.vstack((ti.m, pressure.m)).T for ti in t]\n\n        # Add to plot\n        kwargs.setdefault('colors', 'b')\n        kwargs.setdefault('linestyles', 'dashed')\n        kwargs.setdefault('alpha', 0.5)\n        kwargs.setdefault('zorder', Line2D.zorder - 0.001)\n        self.moist_adiabats = self.ax.add_collection(LineCollection(linedata, **kwargs))\n        return self.moist_adiabats\n\n    def plot_mixing_lines(self, mixing_ratio=None, pressure=None, **kwargs):\n        r\"\"\"Plot lines of constant mixing ratio.\n\n        Adds lines of constant mixing ratio (isohumes) to the\n        plot. The default style of these lines is dashed green lines with an\n        alpha value of 0.8. These can be overridden using keyword arguments.\n\n        Parameters\n        ----------\n        mixing_ratio : array-like, optional\n            Unitless mixing ratio values to plot. If none are given, default\n            values are used.\n        pressure : array-like, optional\n            Pressure values to be included in the isohumes. If not\n            specified, they will be linearly distributed across the current\n            plotted pressure range up to 600 mb.\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.LineCollection`\n\n        Returns\n        -------\n        matplotlib.collections.LineCollection\n            instance created\n\n        See Also\n        --------\n        :class:`matplotlib.collections.LineCollection`\n\n        \"\"\"\n        # Remove old lines\n        if self.mixing_lines:\n            self.mixing_lines.remove()\n\n        # Default mixing level values if necessary\n        if mixing_ratio is None:\n            mixing_ratio = np.array([0.0004, 0.001, 0.002, 0.004, 0.007, 0.01,\n                                     0.016, 0.024, 0.032])\n        mixing_ratio = mixing_ratio.reshape(-1, 1)\n\n        # Set pressure range if necessary\n        if pressure is None:\n            pressure = units.Quantity(np.linspace(600, max(self.ax.get_ylim())), 'mbar')\n\n        # Assemble data for plotting\n        td = dewpoint(vapor_pressure(pressure, mixing_ratio))\n        linedata = [np.vstack((t.m, pressure.m)).T for t in td]\n\n        # Add to plot\n        kwargs.setdefault('colors', 'g')\n        kwargs.setdefault('linestyles', 'dashed')\n        kwargs.setdefault('alpha', 0.8)\n        kwargs.setdefault('zorder', Line2D.zorder - 0.001)\n        self.mixing_lines = self.ax.add_collection(LineCollection(linedata, **kwargs))\n        return self.mixing_lines\n\n    def shade_area(self, y, x1, x2=0, which='both', **kwargs):\n        r\"\"\"Shade area between two curves.\n\n        Shades areas between curves. Area can be where one is greater or less than the other\n        or all areas shaded.\n\n        Parameters\n        ----------\n        y : array-like\n            1-dimensional array of numeric y-values\n        x1 : array-like\n            1-dimensional array of numeric x-values\n        x2 : array-like\n            1-dimensional array of numeric x-values\n        which : str\n            Specifies if `positive`, `negative`, or `both` areas are being shaded.\n            Will be overridden by where.\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.PolyCollection`\n\n        Returns\n        -------\n        :class:`matplotlib.collections.PolyCollection`\n\n        See Also\n        --------\n        :class:`matplotlib.collections.PolyCollection`\n        :meth:`matplotlib.axes.Axes.fill_betweenx`\n\n        \"\"\"\n        fill_properties = {'positive':\n                           {'facecolor': 'tab:red', 'alpha': 0.4, 'where': x1 > x2},\n                           'negative':\n                           {'facecolor': 'tab:blue', 'alpha': 0.4, 'where': x1 < x2},\n                           'both':\n                           {'facecolor': 'tab:green', 'alpha': 0.4, 'where': None}}\n\n        try:\n            fill_args = fill_properties[which]\n            fill_args.update(kwargs)\n        except KeyError:\n            raise ValueError(f'Unknown option for which: {which}') from None\n\n        arrs = y, x1, x2\n\n        if fill_args['where'] is not None:\n            arrs = arrs + (fill_args['where'],)\n            fill_args.pop('where', None)\n\n        fill_args['interpolate'] = True\n\n        arrs = _delete_masked_points(*arrs)\n\n        return self.ax.fill_betweenx(*arrs, **fill_args)\n\n    def shade_cape(self, pressure, t, t_parcel, **kwargs):\n        r\"\"\"Shade areas of Convective Available Potential Energy (CAPE).\n\n        Shades areas where the parcel is warmer than the environment (areas of positive\n        buoyancy.\n\n        Parameters\n        ----------\n        pressure : array-like\n            Pressure values\n        t : array-like\n            Temperature values\n        t_parcel : array-like\n            Parcel path temperature values\n        limit_shading : bool\n            Eliminate shading below the LCL or above the EL, default is True\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.PolyCollection`\n\n        Returns\n        -------\n        :class:`matplotlib.collections.PolyCollection`\n\n        See Also\n        --------\n        :class:`matplotlib.collections.PolyCollection`\n        :meth:`matplotlib.axes.Axes.fill_betweenx`\n\n        \"\"\"\n        return self.shade_area(pressure, t_parcel, t, which='positive', **kwargs)\n\n    def shade_cin(self, pressure, t, t_parcel, dewpoint=None, **kwargs):\n        r\"\"\"Shade areas of Convective INhibition (CIN).\n\n        Shades areas where the parcel is cooler than the environment (areas of negative\n        buoyancy). If `dewpoint` is passed in, negative area below the lifting condensation\n        level or above the equilibrium level is not shaded.\n\n        Parameters\n        ----------\n        pressure : array-like\n            Pressure values\n        t : array-like\n            Temperature values\n        t_parcel : array-like\n            Parcel path temperature values\n        dewpoint : array-like\n            Dew point values, optional\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.PolyCollection`\n\n        Returns\n        -------\n        :class:`matplotlib.collections.PolyCollection`\n\n        See Also\n        --------\n        :class:`matplotlib.collections.PolyCollection`\n        :meth:`matplotlib.axes.Axes.fill_betweenx`\n\n        \"\"\"\n        if dewpoint is not None:\n            lcl_p, _ = lcl(pressure[0], t[0], dewpoint[0])\n            el_p, _ = el(pressure, t, dewpoint, t_parcel)\n            idx = np.logical_and(pressure > el_p, pressure < lcl_p)\n        else:\n            idx = np.arange(0, len(pressure))\n        return self.shade_area(pressure[idx], t_parcel[idx], t[idx], which='negative',\n                               **kwargs)",
  "class Hodograph:\n    r\"\"\"Make a hodograph of wind data.\n\n    Plots the u and v components of the wind along the x and y axes, respectively.\n\n    This class simplifies the process of creating a hodograph using matplotlib.\n    It provides helpers for creating a circular grid and for plotting the wind as a line\n    colored by another value (such as wind speed).\n\n    Attributes\n    ----------\n    ax : `matplotlib.axes.Axes`\n        The underlying Axes instance used for all plotting\n\n    \"\"\"\n\n    def __init__(self, ax=None, component_range=80):\n        r\"\"\"Create a Hodograph instance.\n\n        Parameters\n        ----------\n        ax : `matplotlib.axes.Axes`, optional\n            The `Axes` instance used for plotting\n        component_range : int\n            The maximum range of the plot. Used to set plot bounds and control the maximum\n            number of grid rings needed.\n\n        \"\"\"\n        if ax is None:\n            import matplotlib.pyplot as plt\n            self.ax = plt.figure().add_subplot(1, 1, 1)\n        else:\n            self.ax = ax\n        self.ax.set_aspect('equal', 'box')\n        self.ax.set_xlim(-component_range, component_range)\n        self.ax.set_ylim(-component_range, component_range)\n\n        # == sqrt(2) * max_range, which is the distance at the corner\n        self.max_range = 1.4142135 * component_range\n\n    def add_grid(self, increment=10., **kwargs):\n        r\"\"\"Add grid lines to hodograph.\n\n        Creates lines for the x- and y-axes, as well as circles denoting wind speed values.\n\n        Parameters\n        ----------\n        increment : int, optional\n            The value increment between rings\n        kwargs\n            Other kwargs to control appearance of lines\n\n        See Also\n        --------\n        :class:`matplotlib.patches.Circle`\n        :meth:`matplotlib.axes.Axes.axhline`\n        :meth:`matplotlib.axes.Axes.axvline`\n\n        \"\"\"\n        # Some default arguments. Take those, and update with any\n        # arguments passed in\n        grid_args = {'color': 'grey', 'linestyle': 'dashed', 'zorder': Line2D.zorder - 0.001}\n        if kwargs:\n            grid_args.update(kwargs)\n\n        # Take those args and make appropriate for a Circle\n        circle_args = grid_args.copy()\n        color = circle_args.pop('color', None)\n        circle_args['edgecolor'] = color\n        circle_args['fill'] = False\n\n        self.rings = []\n        for r in np.arange(increment, self.max_range, increment):\n            c = Circle((0, 0), radius=r, **circle_args)\n            self.ax.add_patch(c)\n            self.rings.append(c)\n\n        # Add lines for x=0 and y=0\n        self.yaxis = self.ax.axvline(0, **grid_args)\n        self.xaxis = self.ax.axhline(0, **grid_args)\n\n    @staticmethod\n    def _form_line_args(kwargs):\n        \"\"\"Simplify taking the default line style and extending with kwargs.\"\"\"\n        def_args = {'linewidth': 3}\n        def_args.update(kwargs)\n        return def_args\n\n    def plot(self, u, v, **kwargs):\n        r\"\"\"Plot u, v data.\n\n        Plots the wind data on the hodograph.\n\n        Parameters\n        ----------\n        u : array-like\n            u-component of wind\n        v : array-like\n            v-component of wind\n        kwargs\n            Other keyword arguments to pass to :meth:`matplotlib.axes.Axes.plot`\n\n        Returns\n        -------\n        list[matplotlib.lines.Line2D]\n            lines plotted\n\n        See Also\n        --------\n        :meth:`Hodograph.plot_colormapped`\n\n        \"\"\"\n        line_args = self._form_line_args(kwargs)\n        u, v = _delete_masked_points(u, v)\n        return self.ax.plot(u, v, **line_args)\n\n    def wind_vectors(self, u, v, **kwargs):\n        r\"\"\"Plot u, v data as wind vectors.\n\n        Plot the wind data as vectors for each level, beginning at the origin.\n\n        Parameters\n        ----------\n        u : array-like\n            u-component of wind\n        v : array-like\n            v-component of wind\n        kwargs\n            Other keyword arguments to pass to :meth:`matplotlib.axes.Axes.quiver`\n\n        Returns\n        -------\n        matplotlib.quiver.Quiver\n            arrows plotted\n\n        \"\"\"\n        quiver_args = {'units': 'xy', 'scale': 1}\n        quiver_args.update(**kwargs)\n        center_position = np.zeros_like(u)\n        return self.ax.quiver(center_position, center_position,\n                              u, v, **quiver_args)\n\n    def plot_colormapped(self, u, v, c, intervals=None, colors=None, **kwargs):\n        r\"\"\"Plot u, v data, with line colored based on a third set of data.\n\n        Plots the wind data on the hodograph, but with a colormapped line. Takes a third\n        variable besides the winds (e.g. heights or pressure levels) and either a colormap to\n        color it with or a series of contour intervals and colors to create a colormap and\n        norm to control colormapping. The intervals must always be in increasing\n        order.\n\n        When c and intervals are height data (`pint.Quantity` objects with units of length,\n        such as 'm' or 'km'), the function will automatically interpolate to the contour\n        intervals from the height and wind data, as well as convert the input contour intervals\n        from height AGL to MSL to work with the provided heights.\n\n        Parameters\n        ----------\n        u : array-like\n            u-component of wind\n        v : array-like\n            v-component of wind\n        c : array-like\n            data to use for colormapping (e.g. heights, pressure, wind speed)\n        intervals: array-like, optional\n            Array of intervals for c to use in coloring the hodograph.\n        colors: list, optional\n            Array of strings representing colors for the hodograph segments.\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.LineCollection`\n\n        Returns\n        -------\n        matplotlib.collections.LineCollection\n            instance created\n\n        See Also\n        --------\n        :meth:`Hodograph.plot`\n\n        \"\"\"\n        u, v, c = _delete_masked_points(u, v, c)\n\n        # Plotting a color segmented hodograph\n        if colors:\n            cmap = mcolors.ListedColormap(colors)\n            # If we are segmenting by height (a length), interpolate the contour intervals\n            if is_quantity(intervals) and intervals.check('[length]'):\n\n                # Find any intervals not in the data and interpolate them\n                heights_min = np.nanmin(c)\n                heights_max = np.nanmax(c)\n                interpolation_heights = np.array([bound.m for bound in intervals\n                                                  if bound not in c\n                                                  and heights_min <= bound <= heights_max])\n                interpolation_heights = units.Quantity(np.sort(interpolation_heights),\n                                                       intervals.units)\n                interpolated_u, interpolated_v = interpolate_1d(interpolation_heights, c, u, v)\n\n                # Combine the interpolated data with the actual data\n                c = concatenate([c, interpolation_heights])\n                u = concatenate([u, interpolated_u])\n                v = concatenate([v, interpolated_v])\n                sort_inds = np.argsort(c)\n                c = c[sort_inds]\n                u = u[sort_inds]\n                v = v[sort_inds]\n\n                # Unit conversion required for coloring of bounds/data in dissimilar units\n                # to work properly.\n                c = c.to_base_units()  # TODO: This shouldn't be required!\n                intervals = intervals.to_base_units()\n\n            intervals_m = intervals.m if is_quantity(intervals) else intervals\n            norm = mcolors.BoundaryNorm(intervals_m, cmap.N)\n            cmap.set_over('none')\n            cmap.set_under('none')\n            kwargs['cmap'] = cmap\n            kwargs['norm'] = norm\n            line_args = self._form_line_args(kwargs)\n\n        # Plotting a continuously colored line\n        else:\n            line_args = self._form_line_args(kwargs)\n\n        # Do the plotting\n        lc = colored_line(u, v, c, **line_args)\n        self.ax.add_collection(lc)\n        return lc",
  "def __init__(self, bbox, rot):\n        \"\"\"Initialize skew transform.\n\n        This needs a reference to the parent bounding box to do the appropriate math and\n        to register it as a child so that the transform is invalidated and regenerated if\n        the bounding box changes.\n        \"\"\"\n        super().__init__()\n        self._bbox = bbox\n        self.set_children(bbox)\n        self.invalidate()\n\n        # We're not trying to support changing the rotation, so go ahead and convert to\n        # the right factor for skewing here and just save that.\n        self._rot_factor = np.tan(np.deg2rad(rot))",
  "def get_matrix(self):\n        \"\"\"Return transformation matrix.\"\"\"\n        if self._invalid:\n            # The following matrix is equivalent to the following:\n            # x0, y0 = self._bbox.xmin, self._bbox.ymin\n            # self.translate(-x0, -y0).skew_deg(self._rot, 0).translate(x0, y0)\n            # Setting it this way is just more efficient.\n            self._mtx = np.array([[1.0, self._rot_factor, -self._rot_factor * self._bbox.ymin],\n                                  [0.0, 1.0, 0.0],\n                                  [0.0, 0.0, 1.0]])\n\n            # Need to clear both the invalid flag *and* reset the inverse, which is cached\n            # by the parent class.\n            self._invalid = 0\n            self._inverted = None\n        return self._mtx",
  "def draw(self, renderer):\n        \"\"\"Draw the tick.\"\"\"\n        # When adding the callbacks with `stack.callback`, we fetch the current\n        # visibility state of the artist with `get_visible`; the ExitStack will\n        # restore these states (`set_visible`) at the end of the block (after\n        # the draw).\n        with ExitStack() as stack:\n            for artist in [self.gridline, self.tick1line, self.tick2line,\n                           self.label1, self.label2]:\n                stack.callback(artist.set_visible, artist.get_visible())\n\n            self.tick1line.set_visible(self.tick1line.get_visible() and self.lower_in_bounds)\n            self.label1.set_visible(self.label1.get_visible() and self.lower_in_bounds)\n            self.tick2line.set_visible(self.tick2line.get_visible() and self.upper_in_bounds)\n            self.label2.set_visible(self.label2.get_visible() and self.upper_in_bounds)\n            self.gridline.set_visible(self.gridline.get_visible() and self.grid_in_bounds)\n            super().draw(renderer)",
  "def lower_in_bounds(self):\n        \"\"\"Whether the lower part of the tick is in bounds.\"\"\"\n        return transforms.interval_contains(self.axes.lower_xlim, self.get_loc())",
  "def upper_in_bounds(self):\n        \"\"\"Whether the upper part of the tick is in bounds.\"\"\"\n        return transforms.interval_contains(self.axes.upper_xlim, self.get_loc())",
  "def grid_in_bounds(self):\n        \"\"\"Whether any of the tick grid line is in bounds.\"\"\"\n        return transforms.interval_contains(self.axes.xaxis.get_view_interval(),\n                                            self.get_loc())",
  "def _get_tick(self, major):\n        return SkewXTick(self.axes, None, major=major)",
  "def _get_ticklabel_bboxes(self, ticks, renderer):\n        \"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\n        return ([tick.label1.get_window_extent(renderer)\n                 for tick in ticks if tick.label1.get_visible() and tick.lower_in_bounds],\n                [tick.label2.get_window_extent(renderer)\n                 for tick in ticks if tick.label2.get_visible() and tick.upper_in_bounds])",
  "def get_view_interval(self):\n        \"\"\"Get the view interval.\"\"\"\n        return self.axes.upper_xlim[0], self.axes.lower_xlim[1]",
  "def _adjust_location(self):\n        pts = self._path.vertices\n        if self.spine_type == 'top':\n            pts[:, 0] = self.axes.upper_xlim\n        else:\n            pts[:, 0] = self.axes.lower_xlim",
  "def __init__(self, *args, **kwargs):\n        r\"\"\"Initialize `SkewXAxes`.\n\n        Parameters\n        ----------\n        args : Arbitrary positional arguments\n            Passed to :class:`matplotlib.axes.Axes`\n\n        position: int, optional\n            The rotation of the x-axis against the y-axis, in degrees.\n\n        kwargs : Arbitrary keyword arguments\n            Passed to :class:`matplotlib.axes.Axes`\n\n        \"\"\"\n        # This needs to be popped and set before moving on\n        self.rot = kwargs.pop('rotation', 30)\n        super().__init__(*args, **kwargs)",
  "def _init_axis(self):\n        # Taken from Axes and modified to use our modified X-axis\n        self.xaxis = SkewXAxis(self)\n        self.spines['top'].register_axis(self.xaxis)\n        self.spines['bottom'].register_axis(self.xaxis)\n        self.yaxis = maxis.YAxis(self)\n        self.spines['left'].register_axis(self.yaxis)\n        self.spines['right'].register_axis(self.yaxis)",
  "def _gen_axes_spines(self, locations=None, offset=0.0, units='inches'):\n        # pylint: disable=unused-argument\n        return {'top': SkewSpine.linear_spine(self, 'top'),\n                'bottom': mspines.Spine.linear_spine(self, 'bottom'),\n                'left': mspines.Spine.linear_spine(self, 'left'),\n                'right': mspines.Spine.linear_spine(self, 'right')}",
  "def _set_lim_and_transforms(self):\n        \"\"\"Set limits and transforms.\n\n        This is called once when the plot is created to set up all the\n        transforms for the data, text and grids.\n\n        \"\"\"\n        # Get the standard transform setup from the Axes base class\n        super()._set_lim_and_transforms()\n\n        # This transformation handles the skewing\n        skew_trans = SkewTTransform(self.bbox, self.rot)\n\n        # Create the full transform from Data to Pixels\n        self.transData += skew_trans\n\n        # Blended transforms like this need to have the skewing applied using\n        # both axes, in axes coords like before.\n        self._xaxis_transform += skew_trans",
  "def lower_xlim(self):\n        \"\"\"Get the data limits for the x-axis along the bottom of the axes.\"\"\"\n        return self.axes.viewLim.intervalx",
  "def upper_xlim(self):\n        \"\"\"Get the data limits for the x-axis along the top of the axes.\"\"\"\n        return self.transData.inverted().transform([[self.bbox.xmin, self.bbox.ymax],\n                                                    self.bbox.max])[:, 0]",
  "def __init__(self, fig=None, rotation=30, subplot=None, rect=None, aspect=80.5):\n        r\"\"\"Create SkewT - logP plots.\n\n        Parameters\n        ----------\n        fig : matplotlib.figure.Figure, optional\n            Source figure to use for plotting. If none is given, a new\n            :class:`matplotlib.figure.Figure` instance will be created.\n        rotation : float or int, optional\n            Controls the rotation of temperature relative to horizontal. Given\n            in degrees counterclockwise from x-axis. Defaults to 30 degrees.\n        subplot : tuple[int, int, int] or `matplotlib.gridspec.SubplotSpec` instance, optional\n            Controls the size/position of the created subplot. This allows creating\n            the skewT as part of a collection of subplots. If subplot is a tuple, it\n            should conform to the specification used for\n            :meth:`matplotlib.figure.Figure.add_subplot`. The\n            :class:`matplotlib.gridspec.SubplotSpec`\n            can be created by using :class:`matplotlib.gridspec.GridSpec`.\n        rect : tuple[float, float, float, float], optional\n            Rectangle (left, bottom, width, height) in which to place the axes. This\n            allows the user to place the axes at an arbitrary point on the figure.\n        aspect : float, int, or Literal['auto'], optional\n            Aspect ratio (i.e. ratio of y-scale to x-scale) to maintain in the plot.\n            Defaults to 80.5. Passing the string ``'auto'`` tells matplotlib to handle\n            the aspect ratio automatically (this is not recommended for SkewT).\n\n        \"\"\"\n        if fig is None:\n            import matplotlib.pyplot as plt\n            figsize = plt.rcParams.get('figure.figsize', (7, 7))\n            fig = plt.figure(figsize=figsize)\n        self._fig = fig\n\n        if rect and subplot:\n            raise ValueError(\"Specify only one of `rect' and `subplot', but not both\")\n\n        elif rect:\n            self.ax = fig.add_axes(rect, projection='skewx', rotation=rotation)\n\n        else:\n            if subplot is not None:\n                # Handle being passed a tuple for the subplot, or a GridSpec instance\n                try:\n                    len(subplot)\n                except TypeError:\n                    subplot = (subplot,)\n            else:\n                subplot = (1, 1, 1)\n\n            self.ax = fig.add_subplot(*subplot, projection='skewx', rotation=rotation)\n\n        # Set the yaxis as inverted with log scaling\n        self.ax.set_yscale('log')\n\n        # Override default ticking for log scaling\n        self.ax.yaxis.set_major_formatter(ScalarFormatter())\n        self.ax.yaxis.set_major_locator(MultipleLocator(100))\n        self.ax.yaxis.set_minor_formatter(NullFormatter())\n\n        # Needed to make sure matplotlib doesn't freak out and create a bunch of ticks\n        # Also takes care of inverting the y-axis\n        self.ax.set_ylim(1050, 100)\n        self.ax.yaxis.set_units(units.hPa)\n\n        # Try to make sane default temperature plotting ticks\n        self.ax.xaxis.set_major_locator(MultipleLocator(10))\n        self.ax.xaxis.set_units(units.degC)\n        self.ax.set_xlim(-40, 50)\n        self.ax.grid(True)\n\n        self.mixing_lines = None\n        self.dry_adiabats = None\n        self.moist_adiabats = None\n\n        # Maintain a reasonable ratio of data limits.\n        self.ax.set_aspect(aspect, adjustable='box')",
  "def plot(self, pressure, t, *args, **kwargs):\n        r\"\"\"Plot data.\n\n        Simple wrapper around plot so that pressure is the first (independent)\n        input. This is essentially a wrapper around `plot`.\n\n        Parameters\n        ----------\n        pressure : array-like\n            pressure values\n        t : array-like\n            temperature values, can also be used for things like dew point\n        args\n            Other positional arguments to pass to :func:`~matplotlib.pyplot.plot`\n        kwargs\n            Other keyword arguments to pass to :func:`~matplotlib.pyplot.plot`\n\n        Returns\n        -------\n        list[matplotlib.lines.Line2D]\n            lines plotted\n\n        See Also\n        --------\n        :func:`matplotlib.pyplot.plot`\n\n        \"\"\"\n        # Skew-T logP plotting\n        t, pressure = _delete_masked_points(t, pressure)\n        return self.ax.plot(t, pressure, *args, **kwargs)",
  "def plot_barbs(self, pressure, u, v, c=None, xloc=1.0, x_clip_radius=0.1,\n                   y_clip_radius=0.08, **kwargs):\n        r\"\"\"Plot wind barbs.\n\n        Adds wind barbs to the skew-T plot. This is a wrapper around the\n        `barbs` command that adds to appropriate transform to place the\n        barbs in a vertical line, located as a function of pressure.\n\n        Parameters\n        ----------\n        pressure : array-like\n            pressure values\n        u : array-like\n            U (East-West) component of wind\n        v : array-like\n            V (North-South) component of wind\n        c : array-like, optional\n            An optional array used to map colors to the barbs\n        xloc : float, optional\n            Position for the barbs, in normalized axes coordinates, where 0.0\n            denotes far left and 1.0 denotes far right. Defaults to far right.\n        x_clip_radius : float, optional\n            Space, in normalized axes coordinates, to leave before clipping\n            wind barbs in the x-direction. Defaults to 0.1.\n        y_clip_radius : float, optional\n            Space, in normalized axes coordinates, to leave above/below plot\n            before clipping wind barbs in the y-direction. Defaults to 0.08.\n        plot_units: `pint.Unit`\n            Units to plot in (performing conversion if necessary). Defaults to given units.\n        kwargs\n            Other keyword arguments to pass to :func:`~matplotlib.pyplot.barbs`\n\n        Returns\n        -------\n        matplotlib.quiver.Barbs\n            instance created\n\n        See Also\n        --------\n        :func:`matplotlib.pyplot.barbs`\n\n        \"\"\"\n        # If plot_units specified, convert the data to those units\n        plotting_units = kwargs.pop('plot_units', None)\n        if plotting_units:\n            if hasattr(u, 'units') and hasattr(v, 'units'):\n                u = u.to(plotting_units)\n                v = v.to(plotting_units)\n            else:\n                raise ValueError('To convert to plotting units, units must be attached to '\n                                 'u and v wind components.')\n\n        # Assemble array of x-locations in axes space\n        x = np.empty_like(pressure)\n        x.fill(xloc)\n\n        # Do barbs plot at this location\n        if c is not None:\n            b = self.ax.barbs(x, pressure, u, v, c,\n                              transform=self.ax.get_yaxis_transform(which='tick2'),\n                              clip_on=True, zorder=2, **kwargs)\n        else:\n            b = self.ax.barbs(x, pressure, u, v,\n                              transform=self.ax.get_yaxis_transform(which='tick2'),\n                              clip_on=True, zorder=2, **kwargs)\n\n        # Override the default clip box, which is the axes rectangle, so we can have\n        # barbs that extend outside.\n        ax_bbox = transforms.Bbox([[xloc - x_clip_radius, -y_clip_radius],\n                                   [xloc + x_clip_radius, 1.0 + y_clip_radius]])\n        b.set_clip_box(transforms.TransformedBbox(ax_bbox, self.ax.transAxes))\n        return b",
  "def plot_dry_adiabats(self, t0=None, pressure=None, **kwargs):\n        r\"\"\"Plot dry adiabats.\n\n        Adds dry adiabats (lines of constant potential temperature) to the\n        plot. The default style of these lines is dashed red lines with an alpha\n        value of 0.5. These can be overridden using keyword arguments.\n\n        Parameters\n        ----------\n        t0 : array-like, optional\n            Starting temperature values in Kelvin. If none are given, they will be\n            generated using the current temperature range at the bottom of\n            the plot.\n        pressure : array-like, optional\n            Pressure values to be included in the dry adiabats. If not\n            specified, they will be linearly distributed across the current\n            plotted pressure range.\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.LineCollection`\n\n        Returns\n        -------\n        matplotlib.collections.LineCollection\n            instance created\n\n        See Also\n        --------\n        :func:`~metpy.calc.dry_lapse`\n        :meth:`plot_moist_adiabats`\n        :class:`matplotlib.collections.LineCollection`\n\n        \"\"\"\n        # Remove old lines\n        if self.dry_adiabats:\n            self.dry_adiabats.remove()\n\n        # Determine set of starting temps if necessary\n        if t0 is None:\n            xmin, xmax = self.ax.get_xlim()\n            t0 = units.Quantity(np.arange(xmin, xmax + 1, 10), 'degC')\n\n        # Get pressure levels based on ylims if necessary\n        if pressure is None:\n            pressure = units.Quantity(np.linspace(*self.ax.get_ylim()), 'mbar')\n\n        # Assemble into data for plotting\n        t = dry_lapse(pressure, t0[:, np.newaxis],\n                      units.Quantity(1000., 'mbar')).to(units.degC)\n        linedata = [np.vstack((ti.m, pressure.m)).T for ti in t]\n\n        # Add to plot\n        kwargs.setdefault('colors', 'r')\n        kwargs.setdefault('linestyles', 'dashed')\n        kwargs.setdefault('alpha', 0.5)\n        kwargs.setdefault('zorder', Line2D.zorder - 0.001)\n        self.dry_adiabats = self.ax.add_collection(LineCollection(linedata, **kwargs))\n        return self.dry_adiabats",
  "def plot_moist_adiabats(self, t0=None, pressure=None, **kwargs):\n        r\"\"\"Plot moist adiabats.\n\n        Adds saturated pseudo-adiabats (lines of constant equivalent potential\n        temperature) to the plot. The default style of these lines is dashed\n        blue lines with an alpha value of 0.5. These can be overridden using\n        keyword arguments.\n\n        Parameters\n        ----------\n        t0 : array-like, optional\n            Starting temperature values in Kelvin. If none are given, they will be\n            generated using the current temperature range at the bottom of\n            the plot.\n        pressure : array-like, optional\n            Pressure values to be included in the moist adiabats. If not\n            specified, they will be linearly distributed across the current\n            plotted pressure range.\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.LineCollection`\n\n        Returns\n        -------\n        matplotlib.collections.LineCollection\n            instance created\n\n        See Also\n        --------\n        :func:`~metpy.calc.moist_lapse`\n        :meth:`plot_dry_adiabats`\n        :class:`matplotlib.collections.LineCollection`\n\n        \"\"\"\n        # Remove old lines\n        if self.moist_adiabats:\n            self.moist_adiabats.remove()\n\n        # Determine set of starting temps if necessary\n        if t0 is None:\n            xmin, xmax = self.ax.get_xlim()\n            t0 = units.Quantity(np.concatenate((np.arange(xmin, 0, 10),\n                                                np.arange(0, xmax + 1, 5))), 'degC')\n\n        # Get pressure levels based on ylims if necessary\n        if pressure is None:\n            pressure = units.Quantity(np.linspace(*self.ax.get_ylim()), 'mbar')\n\n        # Assemble into data for plotting\n        t = moist_lapse(pressure, t0, units.Quantity(1000., 'mbar')).to(units.degC)\n        linedata = [np.vstack((ti.m, pressure.m)).T for ti in t]\n\n        # Add to plot\n        kwargs.setdefault('colors', 'b')\n        kwargs.setdefault('linestyles', 'dashed')\n        kwargs.setdefault('alpha', 0.5)\n        kwargs.setdefault('zorder', Line2D.zorder - 0.001)\n        self.moist_adiabats = self.ax.add_collection(LineCollection(linedata, **kwargs))\n        return self.moist_adiabats",
  "def plot_mixing_lines(self, mixing_ratio=None, pressure=None, **kwargs):\n        r\"\"\"Plot lines of constant mixing ratio.\n\n        Adds lines of constant mixing ratio (isohumes) to the\n        plot. The default style of these lines is dashed green lines with an\n        alpha value of 0.8. These can be overridden using keyword arguments.\n\n        Parameters\n        ----------\n        mixing_ratio : array-like, optional\n            Unitless mixing ratio values to plot. If none are given, default\n            values are used.\n        pressure : array-like, optional\n            Pressure values to be included in the isohumes. If not\n            specified, they will be linearly distributed across the current\n            plotted pressure range up to 600 mb.\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.LineCollection`\n\n        Returns\n        -------\n        matplotlib.collections.LineCollection\n            instance created\n\n        See Also\n        --------\n        :class:`matplotlib.collections.LineCollection`\n\n        \"\"\"\n        # Remove old lines\n        if self.mixing_lines:\n            self.mixing_lines.remove()\n\n        # Default mixing level values if necessary\n        if mixing_ratio is None:\n            mixing_ratio = np.array([0.0004, 0.001, 0.002, 0.004, 0.007, 0.01,\n                                     0.016, 0.024, 0.032])\n        mixing_ratio = mixing_ratio.reshape(-1, 1)\n\n        # Set pressure range if necessary\n        if pressure is None:\n            pressure = units.Quantity(np.linspace(600, max(self.ax.get_ylim())), 'mbar')\n\n        # Assemble data for plotting\n        td = dewpoint(vapor_pressure(pressure, mixing_ratio))\n        linedata = [np.vstack((t.m, pressure.m)).T for t in td]\n\n        # Add to plot\n        kwargs.setdefault('colors', 'g')\n        kwargs.setdefault('linestyles', 'dashed')\n        kwargs.setdefault('alpha', 0.8)\n        kwargs.setdefault('zorder', Line2D.zorder - 0.001)\n        self.mixing_lines = self.ax.add_collection(LineCollection(linedata, **kwargs))\n        return self.mixing_lines",
  "def shade_area(self, y, x1, x2=0, which='both', **kwargs):\n        r\"\"\"Shade area between two curves.\n\n        Shades areas between curves. Area can be where one is greater or less than the other\n        or all areas shaded.\n\n        Parameters\n        ----------\n        y : array-like\n            1-dimensional array of numeric y-values\n        x1 : array-like\n            1-dimensional array of numeric x-values\n        x2 : array-like\n            1-dimensional array of numeric x-values\n        which : str\n            Specifies if `positive`, `negative`, or `both` areas are being shaded.\n            Will be overridden by where.\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.PolyCollection`\n\n        Returns\n        -------\n        :class:`matplotlib.collections.PolyCollection`\n\n        See Also\n        --------\n        :class:`matplotlib.collections.PolyCollection`\n        :meth:`matplotlib.axes.Axes.fill_betweenx`\n\n        \"\"\"\n        fill_properties = {'positive':\n                           {'facecolor': 'tab:red', 'alpha': 0.4, 'where': x1 > x2},\n                           'negative':\n                           {'facecolor': 'tab:blue', 'alpha': 0.4, 'where': x1 < x2},\n                           'both':\n                           {'facecolor': 'tab:green', 'alpha': 0.4, 'where': None}}\n\n        try:\n            fill_args = fill_properties[which]\n            fill_args.update(kwargs)\n        except KeyError:\n            raise ValueError(f'Unknown option for which: {which}') from None\n\n        arrs = y, x1, x2\n\n        if fill_args['where'] is not None:\n            arrs = arrs + (fill_args['where'],)\n            fill_args.pop('where', None)\n\n        fill_args['interpolate'] = True\n\n        arrs = _delete_masked_points(*arrs)\n\n        return self.ax.fill_betweenx(*arrs, **fill_args)",
  "def shade_cape(self, pressure, t, t_parcel, **kwargs):\n        r\"\"\"Shade areas of Convective Available Potential Energy (CAPE).\n\n        Shades areas where the parcel is warmer than the environment (areas of positive\n        buoyancy.\n\n        Parameters\n        ----------\n        pressure : array-like\n            Pressure values\n        t : array-like\n            Temperature values\n        t_parcel : array-like\n            Parcel path temperature values\n        limit_shading : bool\n            Eliminate shading below the LCL or above the EL, default is True\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.PolyCollection`\n\n        Returns\n        -------\n        :class:`matplotlib.collections.PolyCollection`\n\n        See Also\n        --------\n        :class:`matplotlib.collections.PolyCollection`\n        :meth:`matplotlib.axes.Axes.fill_betweenx`\n\n        \"\"\"\n        return self.shade_area(pressure, t_parcel, t, which='positive', **kwargs)",
  "def shade_cin(self, pressure, t, t_parcel, dewpoint=None, **kwargs):\n        r\"\"\"Shade areas of Convective INhibition (CIN).\n\n        Shades areas where the parcel is cooler than the environment (areas of negative\n        buoyancy). If `dewpoint` is passed in, negative area below the lifting condensation\n        level or above the equilibrium level is not shaded.\n\n        Parameters\n        ----------\n        pressure : array-like\n            Pressure values\n        t : array-like\n            Temperature values\n        t_parcel : array-like\n            Parcel path temperature values\n        dewpoint : array-like\n            Dew point values, optional\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.PolyCollection`\n\n        Returns\n        -------\n        :class:`matplotlib.collections.PolyCollection`\n\n        See Also\n        --------\n        :class:`matplotlib.collections.PolyCollection`\n        :meth:`matplotlib.axes.Axes.fill_betweenx`\n\n        \"\"\"\n        if dewpoint is not None:\n            lcl_p, _ = lcl(pressure[0], t[0], dewpoint[0])\n            el_p, _ = el(pressure, t, dewpoint, t_parcel)\n            idx = np.logical_and(pressure > el_p, pressure < lcl_p)\n        else:\n            idx = np.arange(0, len(pressure))\n        return self.shade_area(pressure[idx], t_parcel[idx], t[idx], which='negative',\n                               **kwargs)",
  "def __init__(self, ax=None, component_range=80):\n        r\"\"\"Create a Hodograph instance.\n\n        Parameters\n        ----------\n        ax : `matplotlib.axes.Axes`, optional\n            The `Axes` instance used for plotting\n        component_range : int\n            The maximum range of the plot. Used to set plot bounds and control the maximum\n            number of grid rings needed.\n\n        \"\"\"\n        if ax is None:\n            import matplotlib.pyplot as plt\n            self.ax = plt.figure().add_subplot(1, 1, 1)\n        else:\n            self.ax = ax\n        self.ax.set_aspect('equal', 'box')\n        self.ax.set_xlim(-component_range, component_range)\n        self.ax.set_ylim(-component_range, component_range)\n\n        # == sqrt(2) * max_range, which is the distance at the corner\n        self.max_range = 1.4142135 * component_range",
  "def add_grid(self, increment=10., **kwargs):\n        r\"\"\"Add grid lines to hodograph.\n\n        Creates lines for the x- and y-axes, as well as circles denoting wind speed values.\n\n        Parameters\n        ----------\n        increment : int, optional\n            The value increment between rings\n        kwargs\n            Other kwargs to control appearance of lines\n\n        See Also\n        --------\n        :class:`matplotlib.patches.Circle`\n        :meth:`matplotlib.axes.Axes.axhline`\n        :meth:`matplotlib.axes.Axes.axvline`\n\n        \"\"\"\n        # Some default arguments. Take those, and update with any\n        # arguments passed in\n        grid_args = {'color': 'grey', 'linestyle': 'dashed', 'zorder': Line2D.zorder - 0.001}\n        if kwargs:\n            grid_args.update(kwargs)\n\n        # Take those args and make appropriate for a Circle\n        circle_args = grid_args.copy()\n        color = circle_args.pop('color', None)\n        circle_args['edgecolor'] = color\n        circle_args['fill'] = False\n\n        self.rings = []\n        for r in np.arange(increment, self.max_range, increment):\n            c = Circle((0, 0), radius=r, **circle_args)\n            self.ax.add_patch(c)\n            self.rings.append(c)\n\n        # Add lines for x=0 and y=0\n        self.yaxis = self.ax.axvline(0, **grid_args)\n        self.xaxis = self.ax.axhline(0, **grid_args)",
  "def _form_line_args(kwargs):\n        \"\"\"Simplify taking the default line style and extending with kwargs.\"\"\"\n        def_args = {'linewidth': 3}\n        def_args.update(kwargs)\n        return def_args",
  "def plot(self, u, v, **kwargs):\n        r\"\"\"Plot u, v data.\n\n        Plots the wind data on the hodograph.\n\n        Parameters\n        ----------\n        u : array-like\n            u-component of wind\n        v : array-like\n            v-component of wind\n        kwargs\n            Other keyword arguments to pass to :meth:`matplotlib.axes.Axes.plot`\n\n        Returns\n        -------\n        list[matplotlib.lines.Line2D]\n            lines plotted\n\n        See Also\n        --------\n        :meth:`Hodograph.plot_colormapped`\n\n        \"\"\"\n        line_args = self._form_line_args(kwargs)\n        u, v = _delete_masked_points(u, v)\n        return self.ax.plot(u, v, **line_args)",
  "def wind_vectors(self, u, v, **kwargs):\n        r\"\"\"Plot u, v data as wind vectors.\n\n        Plot the wind data as vectors for each level, beginning at the origin.\n\n        Parameters\n        ----------\n        u : array-like\n            u-component of wind\n        v : array-like\n            v-component of wind\n        kwargs\n            Other keyword arguments to pass to :meth:`matplotlib.axes.Axes.quiver`\n\n        Returns\n        -------\n        matplotlib.quiver.Quiver\n            arrows plotted\n\n        \"\"\"\n        quiver_args = {'units': 'xy', 'scale': 1}\n        quiver_args.update(**kwargs)\n        center_position = np.zeros_like(u)\n        return self.ax.quiver(center_position, center_position,\n                              u, v, **quiver_args)",
  "def plot_colormapped(self, u, v, c, intervals=None, colors=None, **kwargs):\n        r\"\"\"Plot u, v data, with line colored based on a third set of data.\n\n        Plots the wind data on the hodograph, but with a colormapped line. Takes a third\n        variable besides the winds (e.g. heights or pressure levels) and either a colormap to\n        color it with or a series of contour intervals and colors to create a colormap and\n        norm to control colormapping. The intervals must always be in increasing\n        order.\n\n        When c and intervals are height data (`pint.Quantity` objects with units of length,\n        such as 'm' or 'km'), the function will automatically interpolate to the contour\n        intervals from the height and wind data, as well as convert the input contour intervals\n        from height AGL to MSL to work with the provided heights.\n\n        Parameters\n        ----------\n        u : array-like\n            u-component of wind\n        v : array-like\n            v-component of wind\n        c : array-like\n            data to use for colormapping (e.g. heights, pressure, wind speed)\n        intervals: array-like, optional\n            Array of intervals for c to use in coloring the hodograph.\n        colors: list, optional\n            Array of strings representing colors for the hodograph segments.\n        kwargs\n            Other keyword arguments to pass to :class:`matplotlib.collections.LineCollection`\n\n        Returns\n        -------\n        matplotlib.collections.LineCollection\n            instance created\n\n        See Also\n        --------\n        :meth:`Hodograph.plot`\n\n        \"\"\"\n        u, v, c = _delete_masked_points(u, v, c)\n\n        # Plotting a color segmented hodograph\n        if colors:\n            cmap = mcolors.ListedColormap(colors)\n            # If we are segmenting by height (a length), interpolate the contour intervals\n            if is_quantity(intervals) and intervals.check('[length]'):\n\n                # Find any intervals not in the data and interpolate them\n                heights_min = np.nanmin(c)\n                heights_max = np.nanmax(c)\n                interpolation_heights = np.array([bound.m for bound in intervals\n                                                  if bound not in c\n                                                  and heights_min <= bound <= heights_max])\n                interpolation_heights = units.Quantity(np.sort(interpolation_heights),\n                                                       intervals.units)\n                interpolated_u, interpolated_v = interpolate_1d(interpolation_heights, c, u, v)\n\n                # Combine the interpolated data with the actual data\n                c = concatenate([c, interpolation_heights])\n                u = concatenate([u, interpolated_u])\n                v = concatenate([v, interpolated_v])\n                sort_inds = np.argsort(c)\n                c = c[sort_inds]\n                u = u[sort_inds]\n                v = v[sort_inds]\n\n                # Unit conversion required for coloring of bounds/data in dissimilar units\n                # to work properly.\n                c = c.to_base_units()  # TODO: This shouldn't be required!\n                intervals = intervals.to_base_units()\n\n            intervals_m = intervals.m if is_quantity(intervals) else intervals\n            norm = mcolors.BoundaryNorm(intervals_m, cmap.N)\n            cmap.set_over('none')\n            cmap.set_under('none')\n            kwargs['cmap'] = cmap\n            kwargs['norm'] = norm\n            line_args = self._form_line_args(kwargs)\n\n        # Plotting a continuously colored line\n        else:\n            line_args = self._form_line_args(kwargs)\n\n        # Do the plotting\n        lc = colored_line(u, v, c, **line_args)\n        self.ax.add_collection(lc)\n        return lc",
  "def scattertext(self, x, y, texts, loc=(0, 0), **kw):\n        \"\"\"Add text to the axes.\n\n        Add text in string `s` to axis at location `x`, `y`, data\n        coordinates.\n\n        Parameters\n        ----------\n        x, y : array-like, shape (n, )\n            Input positions\n\n        texts : array-like, shape (n, )\n            Collection of text that will be plotted at each (x,y) location\n\n        loc : length-2 tuple\n            Offset (in screen coordinates) from x,y position. Allows\n            positioning text relative to original point.\n\n        Other Parameters\n        ----------------\n        kwargs : `~matplotlib.text.TextCollection` properties.\n            Other miscellaneous text parameters.\n\n        Examples\n        --------\n        Individual keyword arguments can be used to override any given\n        parameter::\n\n            >>> ax = plt.gca()\n            >>> ax.scattertext([0.25, 0.75], [0.25, 0.75], ['aa', 'bb'],\n            ... fontsize=12)  #doctest: +ELLIPSIS\n            TextCollection\n\n        The default setting to to center the text at the specified x, y\n        locations in data coordinates. The example below places the text\n        above and to the right by 10 pixels::\n\n            >>> ax = plt.gca()\n            >>> ax.scattertext([0.25, 0.75], [0.25, 0.75], ['aa', 'bb'],\n            ... loc=(10, 10))  #doctest: +ELLIPSIS\n            TextCollection\n\n        \"\"\"\n        # Start with default args and update from kw\n        new_kw = {\n            'verticalalignment': 'center',\n            'horizontalalignment': 'center',\n            'transform': self.transData,\n            'clip_on': False}\n        new_kw.update(kw)\n\n        # Handle masked arrays\n        x, y, texts = cbook.delete_masked_points(x, y, texts)\n\n        # If there is nothing left after deleting the masked points, return None\n        if x.size == 0:\n            return None\n\n        # Make the TextCollection object\n        text_obj = TextCollection(x, y, texts, offset=loc, **new_kw)\n\n        # The margin adjustment is a hack to deal with the fact that we don't\n        # want to transform all the symbols whose scales are in points\n        # to data coords to get the exact bounding box for efficiency\n        # reasons.  It can be done right if this is deemed important.\n        # Also, only bother with this padding if there is anything to draw.\n        if self._xmargin < 0.05:\n            self.set_xmargin(0.05)\n\n        if self._ymargin < 0.05:\n            self.set_ymargin(0.05)\n\n        # Add it to the axes and update range\n        self.add_artist(text_obj)\n\n        # Matplotlib at least up to 3.2.2 does not properly clip text with paths, so\n        # work-around by setting to the bounding box of the Axes\n        # TODO: Remove when fixed in our minimum supported version of matplotlib\n        text_obj.clipbox = self.bbox\n\n        self.update_datalim(text_obj.get_datalim(self.transData))\n        self.autoscale_view()\n        return text_obj",
  "class TextCollection(Text):\n        \"\"\"Handle plotting a collection of text.\n\n        Text Collection plots text with a collection of similar properties: font, color,\n        and an offset relative to the x,y data location.\n        \"\"\"\n\n        def __init__(self, x, y, text, offset=(0, 0), **kwargs):\n            \"\"\"Initialize an instance of `TextCollection`.\n\n            This class encompasses drawing a collection of text values at a variety\n            of locations.\n\n            Parameters\n            ----------\n            x : array-like\n                The x locations, in data coordinates, for the text\n\n            y : array-like\n                The y locations, in data coordinates, for the text\n\n            text : array-like of str\n                The string values to draw\n\n            offset : (int, int)\n                The offset x and y, in normalized coordinates, to draw the text relative\n                to the data locations.\n\n            kwargs : arbitrary keywords arguments\n\n            \"\"\"\n            Text.__init__(self, **kwargs)\n            self.x = x\n            self.y = y\n            self.text = text\n            self.offset = offset\n\n        def __str__(self):\n            \"\"\"Make a string representation of `TextCollection`.\"\"\"\n            return 'TextCollection'\n\n        __repr__ = __str__\n\n        def get_datalim(self, transData):  # noqa: N803\n            \"\"\"Return the limits of the data.\n\n            Parameters\n            ----------\n            transData : matplotlib.transforms.Transform\n\n            Returns\n            -------\n            matplotlib.transforms.Bbox\n                The bounding box of the data\n\n            \"\"\"\n            full_transform = self.get_transform() - transData\n            posx = self.convert_xunits(self.x)\n            posy = self.convert_yunits(self.y)\n            XY = full_transform.transform(np.vstack((posx, posy)).T)  # noqa: N806\n            bbox = transforms.Bbox.null()\n            bbox.update_from_data_xy(XY, ignore=True)\n            return bbox\n\n        @allow_rasterization\n        def draw(self, renderer):\n            \"\"\"Draw the :class:`TextCollection` object to the given *renderer*.\"\"\"\n            if renderer is not None:\n                self._renderer = renderer\n            if not self.get_visible():\n                return\n            if not any(self.text):\n                return\n\n            renderer.open_group('text', self.get_gid())\n\n            trans = self.get_transform()\n            if self.offset != (0, 0):\n                scale = self.axes.figure.dpi / 72\n                xoff, yoff = self.offset\n                trans += mtransforms.Affine2D().translate(scale * xoff,\n                                                          scale * yoff)\n\n            posx = self.convert_xunits(self.x)\n            posy = self.convert_yunits(self.y)\n            pts = np.vstack((posx, posy)).T\n            pts = trans.transform(pts)\n            canvasw, canvash = renderer.get_canvas_width_height()\n\n            gc = renderer.new_gc()\n            gc.set_foreground(self.get_color())\n            gc.set_alpha(self.get_alpha())\n            gc.set_url(self._url)\n            self._set_gc_clip(gc)\n\n            angle = self.get_rotation()\n\n            for (posx, posy), t in zip(pts, self.text):\n                # Skip empty strings--not only is this a performance gain, but it fixes\n                # rendering with path effects below.\n                if not t:\n                    continue\n\n                self._text = t  # hack to allow self._get_layout to work\n                bbox, info, descent = self._get_layout(renderer)\n                self._text = ''\n\n                for line, _, x, y in info:\n\n                    mtext = self if len(info) == 1 else None\n                    x = x + posx\n                    y = y + posy\n                    if renderer.flipy():\n                        y = canvash - y\n\n                    clean_line, ismath = self._preprocess_math(line)\n\n                    if self.get_path_effects():\n                        from matplotlib.patheffects import PathEffectRenderer\n                        textrenderer = PathEffectRenderer(\n                                            self.get_path_effects(), renderer)  # noqa: E126\n                    else:\n                        textrenderer = renderer\n\n                    if self.get_usetex():\n                        textrenderer.draw_tex(gc, x, y, clean_line,\n                                              self._fontproperties, angle,\n                                              mtext=mtext)\n                    else:\n                        textrenderer.draw_text(gc, x, y, clean_line,\n                                               self._fontproperties, angle,\n                                               ismath=ismath, mtext=mtext)\n\n            gc.restore()\n            renderer.close_group('text')\n\n        def set_usetex(self, usetex):\n            \"\"\"\n            Set this `Text` object to render using TeX (or not).\n\n            If `None` is given, the option will be reset to use the value of\n            `rcParams['text.usetex']`\n            \"\"\"\n            self._usetex = None if usetex is None else bool(usetex)\n            self.stale = True\n\n        def get_usetex(self):\n            \"\"\"\n            Return whether this `Text` object will render using TeX.\n\n            If the user has not manually set this value, it will default to\n            the value of `rcParams['text.usetex']`\n            \"\"\"\n            if self._usetex is None:\n                return rcParams['text.usetex']\n            else:\n                return self._usetex",
  "def __init__(self, x, y, text, offset=(0, 0), **kwargs):\n            \"\"\"Initialize an instance of `TextCollection`.\n\n            This class encompasses drawing a collection of text values at a variety\n            of locations.\n\n            Parameters\n            ----------\n            x : array-like\n                The x locations, in data coordinates, for the text\n\n            y : array-like\n                The y locations, in data coordinates, for the text\n\n            text : array-like of str\n                The string values to draw\n\n            offset : (int, int)\n                The offset x and y, in normalized coordinates, to draw the text relative\n                to the data locations.\n\n            kwargs : arbitrary keywords arguments\n\n            \"\"\"\n            Text.__init__(self, **kwargs)\n            self.x = x\n            self.y = y\n            self.text = text\n            self.offset = offset",
  "def __str__(self):\n            \"\"\"Make a string representation of `TextCollection`.\"\"\"\n            return 'TextCollection'",
  "def get_datalim(self, transData):  # noqa: N803\n            \"\"\"Return the limits of the data.\n\n            Parameters\n            ----------\n            transData : matplotlib.transforms.Transform\n\n            Returns\n            -------\n            matplotlib.transforms.Bbox\n                The bounding box of the data\n\n            \"\"\"\n            full_transform = self.get_transform() - transData\n            posx = self.convert_xunits(self.x)\n            posy = self.convert_yunits(self.y)\n            XY = full_transform.transform(np.vstack((posx, posy)).T)  # noqa: N806\n            bbox = transforms.Bbox.null()\n            bbox.update_from_data_xy(XY, ignore=True)\n            return bbox",
  "def draw(self, renderer):\n            \"\"\"Draw the :class:`TextCollection` object to the given *renderer*.\"\"\"\n            if renderer is not None:\n                self._renderer = renderer\n            if not self.get_visible():\n                return\n            if not any(self.text):\n                return\n\n            renderer.open_group('text', self.get_gid())\n\n            trans = self.get_transform()\n            if self.offset != (0, 0):\n                scale = self.axes.figure.dpi / 72\n                xoff, yoff = self.offset\n                trans += mtransforms.Affine2D().translate(scale * xoff,\n                                                          scale * yoff)\n\n            posx = self.convert_xunits(self.x)\n            posy = self.convert_yunits(self.y)\n            pts = np.vstack((posx, posy)).T\n            pts = trans.transform(pts)\n            canvasw, canvash = renderer.get_canvas_width_height()\n\n            gc = renderer.new_gc()\n            gc.set_foreground(self.get_color())\n            gc.set_alpha(self.get_alpha())\n            gc.set_url(self._url)\n            self._set_gc_clip(gc)\n\n            angle = self.get_rotation()\n\n            for (posx, posy), t in zip(pts, self.text):\n                # Skip empty strings--not only is this a performance gain, but it fixes\n                # rendering with path effects below.\n                if not t:\n                    continue\n\n                self._text = t  # hack to allow self._get_layout to work\n                bbox, info, descent = self._get_layout(renderer)\n                self._text = ''\n\n                for line, _, x, y in info:\n\n                    mtext = self if len(info) == 1 else None\n                    x = x + posx\n                    y = y + posy\n                    if renderer.flipy():\n                        y = canvash - y\n\n                    clean_line, ismath = self._preprocess_math(line)\n\n                    if self.get_path_effects():\n                        from matplotlib.patheffects import PathEffectRenderer\n                        textrenderer = PathEffectRenderer(\n                                            self.get_path_effects(), renderer)  # noqa: E126\n                    else:\n                        textrenderer = renderer\n\n                    if self.get_usetex():\n                        textrenderer.draw_tex(gc, x, y, clean_line,\n                                              self._fontproperties, angle,\n                                              mtext=mtext)\n                    else:\n                        textrenderer.draw_text(gc, x, y, clean_line,\n                                               self._fontproperties, angle,\n                                               ismath=ismath, mtext=mtext)\n\n            gc.restore()\n            renderer.close_group('text')",
  "def set_usetex(self, usetex):\n            \"\"\"\n            Set this `Text` object to render using TeX (or not).\n\n            If `None` is given, the option will be reset to use the value of\n            `rcParams['text.usetex']`\n            \"\"\"\n            self._usetex = None if usetex is None else bool(usetex)\n            self.stale = True",
  "def get_usetex(self):\n            \"\"\"\n            Return whether this `Text` object will render using TeX.\n\n            If the user has not manually set this value, it will default to\n            the value of `rcParams['text.usetex']`\n            \"\"\"\n            if self._usetex is None:\n                return rcParams['text.usetex']\n            else:\n                return self._usetex",
  "def lookup_projection(projection_code):\n    \"\"\"Get a Cartopy projection based on a short abbreviation.\"\"\"\n    import cartopy.crs as ccrs\n\n    projections = {'lcc': ccrs.LambertConformal(central_latitude=40, central_longitude=-100,\n                                                standard_parallels=[30, 60]),\n                   'ps': ccrs.NorthPolarStereo(central_longitude=-100),\n                   'mer': ccrs.Mercator()}\n    return projections[projection_code]",
  "def lookup_map_feature(feature_name):\n    \"\"\"Get a Cartopy map feature based on a name.\"\"\"\n    import cartopy.feature as cfeature\n\n    from . import cartopy_utils\n\n    name = feature_name.upper()\n    try:\n        feat = getattr(cfeature, name)\n        scaler = cfeature.AdaptiveScaler('110m', (('50m', 50), ('10m', 15)))\n    except AttributeError:\n        feat = getattr(cartopy_utils, name)\n        scaler = cfeature.AdaptiveScaler('20m', (('5m', 5), ('500k', 1)))\n    return feat.with_scale(scaler)",
  "def plot_kwargs(data):\n    \"\"\"Set the keyword arguments for MapPanel plotting.\"\"\"\n    if hasattr(data.metpy, 'cartopy_crs'):\n        # Conditionally add cartopy transform if we are on a map.\n        kwargs = {'transform': data.metpy.cartopy_crs}\n    else:\n        kwargs = {}\n    return kwargs",
  "class ValidationMixin:\n    \"\"\"Provides validation of attribute names when set by user.\"\"\"\n\n    def __setattr__(self, name, value):\n        \"\"\"Set only permitted attributes.\"\"\"\n        allowlist = ['ax',\n                     'data',\n                     'handle',\n                     'notify_change',\n                     'panel'\n                     ]\n\n        allowlist.extend(self.trait_names())\n        if name in allowlist or name.startswith('_'):\n            super().__setattr__(name, value)\n        else:\n            closest = get_close_matches(name, allowlist, n=1)\n            if closest:\n                alt = closest[0]\n                suggest = f\" Perhaps you meant '{alt}'?\"\n            else:\n                suggest = ''\n            obj = self.__class__\n            msg = f\"'{name}' is not a valid attribute for {obj}.\" + suggest\n            raise AttributeError(msg)",
  "class MetPyHasTraits(HasTraits):\n    \"\"\"Provides modification layer on HasTraits for declarative classes.\"\"\"\n\n    def __dir__(self):\n        \"\"\"Filter dir to be more helpful for tab-completion in Jupyter.\"\"\"\n        return filter(\n            lambda name: not (name in dir(HasTraits) or name.startswith('_')),\n            dir(type(self))\n        )",
  "class Panel(MetPyHasTraits):\n    \"\"\"Draw one or more plots.\"\"\"",
  "class PanelContainer(MetPyHasTraits, ValidationMixin):\n    \"\"\"Collects panels and set complete figure related settings (e.g., size).\"\"\"\n\n    size = Union([Tuple(Union([Int(), Float()]), Union([Int(), Float()])),\n                 Instance(type(None))], default_value=None)\n    size.__doc__ = \"\"\"This trait takes a tuple of (width, height) to set the size of the\n    figure.\n\n    This trait defaults to None and will assume the default `matplotlib.pyplot.figure` size.\n    \"\"\"\n\n    panels = List(Instance(Panel))\n    panels.__doc__ = \"\"\"A list of panels to plot on the figure.\n\n    This trait must contain at least one panel to plot on the figure.\"\"\"\n\n    @property\n    def panel(self):\n        \"\"\"Provide simple access for a single panel.\"\"\"\n        return self.panels[0]\n\n    @panel.setter\n    def panel(self, val):\n        self.panels = [val]\n\n    @observe('panels')\n    def _panels_changed(self, change):\n        for panel in change.new:\n            panel.parent = self\n            panel.observe(self.refresh, names=('_need_redraw'))\n\n    @property\n    def figure(self):\n        \"\"\"Provide access to the underlying figure object.\"\"\"\n        if not hasattr(self, '_fig'):\n            self._fig = plt.figure(figsize=self.size)\n        return self._fig\n\n    def refresh(self, _):\n        \"\"\"Refresh the rendering of all panels.\"\"\"\n        # First make sure everything is properly constructed\n        self.draw()\n\n        # Trigger the graphics refresh\n        self.figure.canvas.draw()\n\n        # Flush out interactive events--only ok on Agg for newer matplotlib\n        with contextlib.suppress(NotImplementedError):\n            self.figure.canvas.flush_events()\n\n    def draw(self):\n        \"\"\"Draw the collection of panels.\"\"\"\n        for panel in self.panels:\n            with panel.hold_trait_notifications():\n                panel.draw()\n\n    def save(self, *args, **kwargs):\n        \"\"\"Save the constructed graphic as an image file.\n\n        This method takes a string for saved file name. Additionally, the same arguments and\n        keyword arguments that `matplotlib.pyplot.savefig` does.\n        \"\"\"\n        self.draw()\n        self.figure.savefig(*args, **kwargs)\n\n    def show(self):\n        \"\"\"Show the constructed graphic on the screen.\"\"\"\n        self.draw()\n        plt.show()\n\n    def copy(self):\n        \"\"\"Return a copy of the panel container.\"\"\"\n        return copy.copy(self)",
  "class MapPanel(Panel, ValidationMixin):\n    \"\"\"Set figure related elements for an individual panel.\n\n    Parameters that need to be set include collecting all plotting types\n    (e.g., contours, wind barbs, etc.) that are desired to be in a given panel.\n    Additionally, traits can be set to plot map related features (e.g., coastlines, borders),\n    projection, graphics area, and title.\n    \"\"\"\n\n    parent = Instance(PanelContainer, allow_none=True)\n\n    layout = Tuple(Int(), Int(), Int(), default_value=(1, 1, 1))\n    layout.__doc__ = \"\"\"A tuple that contains the description (nrows, ncols, index) of the\n    panel position; default value is (1, 1, 1).\n\n    This trait is set to describe the panel position and the default is for a single panel. For\n    example, a four-panel plot will have two rows and two columns with the tuple setting for\n    the upper-left panel as (2, 2, 1), upper-right as (2, 2, 2), lower-left as (2, 2, 3), and\n    lower-right as (2, 2, 4). For more details see the documentation for\n    `matplotlib.figure.Figure.add_subplot`.\n    \"\"\"\n\n    plots = List(Any())\n    plots.__doc__ = \"\"\"A list of handles that represent the plots (e.g., `ContourPlot`,\n    `FilledContourPlot`, `ImagePlot`) to put on a given panel.\n\n    This trait collects the different plots, including contours and images, that are intended\n    for a given panel.\n    \"\"\"\n\n    _need_redraw = Bool(default_value=True)\n\n    area = Union([Unicode(), Tuple(Float(), Float(), Float(), Float())], allow_none=True,\n                 default_value=None)\n    area.__doc__ = \"\"\"A tuple or string value that indicates the graphical area of the plot.\n\n    The tuple value corresponds to longitude/latitude box based on the projection of the map\n    with the format (west-most longitude, east-most longitude, south-most latitude,\n    north-most latitude). This tuple defines a box from the lower-left to the upper-right\n    corner.\n\n    This trait can also be set with a string value associated with the named geographic regions\n    within MetPy. The tuples associated with the names are based on a PlatteCarree projection.\n    For a CONUS region, the following strings can be used: 'us', 'spcus', 'ncus', and 'afus'.\n    For regional plots, US postal state abbreviations can be used, such as 'co', 'ny', 'ca',\n    et cetera. Providing a '+' or '-' suffix to the string value will zoom in or out,\n    respectively. Providing multiple '+' or '-' characters will zoom in or out further.\n\n    \"\"\"\n\n    projection = Union([Unicode(), Instance('cartopy.crs.Projection')], default_value='data')\n    projection.__doc__ = \"\"\"A string for a pre-defined projection or a Cartopy projection\n    object.\n\n    There are three pre-defined projections that can be called with a short name:\n    Lambert conformal conic ('lcc'), Mercator ('mer'), or polar-stereographic ('ps'), or 'area'\n    to use a default projection based on the string area used. Additionally, this trait can be\n    set to a Cartopy projection object.\n    \"\"\"\n\n    layers = List(Union([Unicode(), Instance('cartopy.feature.Feature')]),\n                  default_value=['coastline'])\n    layers.__doc__ = \"\"\"A list of strings for a pre-defined feature layer or a Cartopy Feature\n    object.\n\n    Like the projection, there are a couple of pre-defined feature layers that can be called\n    using a short name. The pre-defined layers are: 'coastline', 'states', 'borders', 'lakes',\n    'land', 'ocean', 'rivers', 'usstates', and 'uscounties'. Additionally, this can accept\n    Cartopy Feature objects.\n    \"\"\"\n\n    layers_edgecolor = List(Unicode(allow_none=True), default_value=['black'])\n    layers_edgecolor.__doc__ = \"\"\"A list of strings for a pre-defined edgecolor for a layer.\n\n    An option to set a different color for the map layer edge colors. Length of list should\n    match that of layers if not using default value. Behavior is to repeat colors if not enough\n    provided by user. Use `None` value for 'ocean', 'lakes', 'rivers', and 'land'.\n    \"\"\"\n\n    layers_linewidth = List(Union([Int(), Float()], allow_none=True), default_value=[1])\n    layers_linewidth.__doc__ = \"\"\"A list of values defining the linewidth for a layer.\n\n    An option to set a different color for the map layer edge colors. Length of list should\n    match that of layers if not using default value. Behavior is to repeat colors if not enough\n    provided by user. Use `None` value for 'ocean', 'lakes', 'rivers', and 'land'.\n    \"\"\"\n\n    title = Unicode()\n    title.__doc__ = \"\"\"A string to set a title for the figure.\n\n    This trait sets a user-defined title that will plot at the top center of the figure.\n    \"\"\"\n\n    left_title = Unicode(allow_none=True, default_value=None)\n    left_title.__doc__ = \"\"\"A string to set a title for the figure with the location on the\n    top left of the figure.\n\n    This trait sets a user-defined title that will plot at the top left of the figure.\n    \"\"\"\n\n    right_title = Unicode(allow_none=True, default_value=None)\n    right_title.__doc__ = \"\"\"A string to set a title for the figure with the location on the\n    top right of the figure.\n\n    This trait sets a user-defined title that will plot at the top right of the figure.\n    \"\"\"\n\n    title_fontsize = Union([Int(), Float(), Unicode()], allow_none=True, default_value=None)\n    title_fontsize.__doc__ = \"\"\"An integer or string value for the font size of the title of\n    the figure.\n\n    This trait sets the font size for the title that will plot at the top center of the figure.\n    Accepts size in points or relative size. Allowed relative sizes are those of Matplotlib:\n    'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'.\n    \"\"\"\n\n    @validate('area')\n    def _valid_area(self, proposal):\n        \"\"\"Check that proposed string or tuple is valid and turn string into a tuple extent.\"\"\"\n        from .plot_areas import named_areas\n\n        area = proposal['value']\n\n        # Parse string, check that string is valid, and determine extent based on string\n        if isinstance(area, str):\n            match = re.match(r'(\\w+)([-+]*)$', area)\n            if match is None:\n                raise TraitError(f'\"{area}\" is not a valid area.')\n            region, modifier = match.groups()\n            region = region.lower()\n\n            if region == 'global':\n                extent = 'global'\n            elif region in named_areas:\n                self._area_proj = named_areas[region].projection\n                extent = named_areas[region].bounds\n                zoom = modifier.count('+') - modifier.count('-')\n                extent = self._zoom_extent(extent, zoom)\n            else:\n                raise TraitError(f'\"{area}\" is not a valid string area.')\n        # Otherwise, assume area is a tuple and check that latitudes/longitudes are valid\n        else:\n            west_lon, east_lon, south_lat, north_lat = area\n            valid_west = -180 <= west_lon <= 180\n            valid_east = -180 <= east_lon <= 180\n            valid_south = -90 <= south_lat <= 90\n            valid_north = -90 <= north_lat <= 90\n            if not (valid_west and valid_east and valid_south and valid_north):\n                raise TraitError(f'\"{area}\" is not a valid string area.')\n            extent = area\n\n        return extent\n\n    @observe('plots')\n    def _plots_changed(self, change):\n        \"\"\"Handle when our collection of plots changes.\"\"\"\n        for plot in change.new:\n            plot.parent = self\n            plot.observe(self.refresh, names=('_need_redraw'))\n        self._need_redraw = True\n\n    @observe('parent')\n    def _parent_changed(self, _):\n        \"\"\"Handle when the parent is changed.\"\"\"\n        self.ax = None\n\n    @property\n    def _proj_obj(self):\n        \"\"\"Return the projection as a Cartopy object.\n\n        Handles looking up a string for the projection, or if the projection\n        is set to ``'data'`` looks at the data for the projection.\n\n        \"\"\"\n        if isinstance(self.projection, str):\n            if self.projection == 'data':\n                if isinstance(self.plots[0].griddata, tuple):\n                    proj = self.plots[0].griddata[0].metpy.cartopy_crs\n                else:\n                    proj = self.plots[0].griddata.metpy.cartopy_crs\n            elif self.projection == 'area':\n                proj = self._area_proj\n            else:\n                proj = lookup_projection(self.projection)\n        else:\n            proj = self.projection\n        return proj\n\n    @property\n    def _layer_features(self):\n        \"\"\"Iterate over all map features and return as Cartopy objects.\n\n        Handle converting names of maps to auto-scaling map features.\n\n        \"\"\"\n        for item in self.layers:\n            feat = lookup_map_feature(item) if isinstance(item, str) else item\n            yield feat\n\n    @observe('area')\n    def _set_need_redraw(self, _):\n        \"\"\"Watch traits and set the need redraw flag as necessary.\"\"\"\n        self._need_redraw = True\n\n    @staticmethod\n    def _zoom_extent(extent, zoom):\n        \"\"\"Calculate new bounds for zooming in or out of a given extent.\n\n        ``extent`` is given as a tuple with four numeric values, in the same format as the\n        ``area`` trait.\n\n        If ``zoom`` = 0, the extent will not be changed from what was provided to the method\n        If ``zoom`` > 0, the returned extent will be smaller (zoomed in)\n        If ``zoom`` < 0, the returned extent will be larger (zoomed out)\n\n        \"\"\"\n        west_lon, east_lon, south_lat, north_lat = extent\n\n        # Turn number of pluses and minuses into a number than can scale the latitudes and\n        # longitudes of our extent\n        zoom_multiplier = (1 - 2**-zoom) / 2\n\n        # Calculate bounds for new, zoomed extent\n        new_north_lat = north_lat + (south_lat - north_lat) * zoom_multiplier\n        new_south_lat = south_lat - (south_lat - north_lat) * zoom_multiplier\n        new_east_lon = east_lon + (west_lon - east_lon) * zoom_multiplier\n        new_west_lon = west_lon - (west_lon - east_lon) * zoom_multiplier\n\n        return (new_west_lon, new_east_lon, new_south_lat, new_north_lat)\n\n    @property\n    def ax(self):\n        \"\"\"Get the :class:`matplotlib.axes.Axes` to draw on.\n\n        Creates a new instance if necessary.\n\n        \"\"\"\n        # If we haven't actually made an instance yet, make one with the right size and\n        # map projection.\n        if getattr(self, '_ax', None) is None:\n            self._ax = self.parent.figure.add_subplot(*self.layout, projection=self._proj_obj)\n\n        return self._ax\n\n    @ax.setter\n    def ax(self, val):\n        \"\"\"Set the :class:`matplotlib.axes.Axes` to draw on.\n\n        Clears existing state as necessary.\n\n        \"\"\"\n        if getattr(self, '_ax', None) is not None:\n            self._ax.cla()\n        self._ax = val\n\n    def refresh(self, changed):\n        \"\"\"Refresh the drawing if necessary.\"\"\"\n        self._need_redraw = changed.new\n\n    def draw(self):\n        \"\"\"Draw the panel.\"\"\"\n        # Only need to run if we've actually changed.\n        if self._need_redraw:\n\n            # Set the extent as appropriate based on the area. One special case for 'global'.\n            if self.area == 'global':\n                self.ax.set_global()\n            elif self.area is not None:\n                self.ax.set_extent(self.area, ccrs.PlateCarree())\n\n            # Draw all of the plots.\n            for p in self.plots:\n                with p.hold_trait_notifications():\n                    p.draw()\n\n            # Add all of the maps\n            if len(self.layers) > len(self.layers_edgecolor):\n                self.layers_edgecolor *= len(self.layers)\n            if len(self.layers) > len(self.layers_linewidth):\n                self.layers_linewidth *= len(self.layers)\n            for i, feat in enumerate(self._layer_features):\n                if self.layers[i] in ['', 'land', 'lake', 'river']:\n                    color = 'face'\n                else:\n                    color = self.layers_edgecolor[i]\n                width = self.layers_linewidth[i]\n                self.ax.add_feature(feat, edgecolor=color, linewidth=width)\n\n            # Use the set title or generate one.\n            if (self.right_title is None) and (self.left_title is None):\n                title = self.title or ',\\n'.join(plot.name for plot in self.plots)\n                self.ax.set_title(title, fontsize=self.title_fontsize)\n            else:\n                if self.title is not None:\n                    self.ax.set_title(self.title, fontsize=self.title_fontsize)\n                if self.right_title is not None:\n                    self.ax.set_title(self.right_title, fontsize=self.title_fontsize,\n                                      loc='right')\n                if self.left_title is not None:\n                    self.ax.set_title(self.left_title, fontsize=self.title_fontsize,\n                                      loc='left')\n            self._need_redraw = False\n\n    def __copy__(self):\n        \"\"\"Return a copy of this MapPanel.\"\"\"\n        # Create new, blank instance of MapPanel\n        cls = self.__class__\n        obj = cls.__new__(cls)\n\n        # Copy each attribute from current MapPanel to new MapPanel\n        for name in self.trait_names():\n            # The 'plots' attribute is a list.\n            # A copy must be made for each plot in the list.\n            if name == 'plots':\n                obj.plots = [copy.copy(plot) for plot in self.plots]\n            else:\n                setattr(obj, name, getattr(self, name))\n\n        return obj\n\n    def copy(self):\n        \"\"\"Return a copy of the panel.\"\"\"\n        return copy.copy(self)",
  "class SubsetTraits(MetPyHasTraits):\n    \"\"\"Represent common traits for subsetting data.\"\"\"\n\n    x = Union([Float(allow_none=True, default_value=None), Instance(units.Quantity)])\n    x.__doc__ = \"\"\"The x coordinate of the field to be plotted.\n\n    This is a value with units to choose a desired x coordinate. For example, selecting a\n    point or transect through the projection origin, set this parameter to\n    ``0 * units.meter``. Note that this requires your data to have an x dimension coordinate.\n    \"\"\"\n\n    longitude = Union([Float(allow_none=True, default_value=None), Instance(units.Quantity)])\n    longitude.__doc__ = \"\"\"The longitude coordinate of the field to be plotted.\n\n    This is a value with units to choose a desired longitude coordinate. For example,\n    selecting a point or transect through 95 degrees west, set this parameter to\n    ``-95 * units.degrees_east``. Note that this requires your data to have a longitude\n    dimension coordinate.\n    \"\"\"\n\n    y = Union([Float(allow_none=True, default_value=None), Instance(units.Quantity)])\n    y.__doc__ = \"\"\"The y coordinate of the field to be plotted.\n\n    This is a value with units to choose a desired x coordinate. For example, selecting a\n    point or transect through the projection origin, set this parameter to\n    ``0 * units.meter``. Note that this requires your data to have an y dimension coordinate.\n    \"\"\"\n\n    latitude = Union([Float(allow_none=True, default_value=None), Instance(units.Quantity)])\n    latitude.__doc__ = \"\"\"The latitude coordinate of the field to be plotted.\n\n    This is a value with units to choose a desired latitude coordinate. For example,\n    selecting a point or transect through 40 degrees north, set this parameter to\n    ``40 * units.degrees_north``. Note that this requires your data to have a latitude\n    dimension coordinate.\n    \"\"\"\n\n    level = Union([Int(allow_none=True, default_value=None), Instance(units.Quantity)])\n    level.__doc__ = \"\"\"The level of the field to be plotted.\n\n    This is a value with units to choose a desired plot level. For example, selecting the\n    850-hPa level, set this parameter to ``850 * units.hPa``. Note that this requires your\n    data to have a vertical dimension coordinate.\n    \"\"\"\n\n    time = Instance(datetime, allow_none=True)\n    time.__doc__ = \"\"\"Set the valid time to be plotted as a datetime object.\n\n    If a forecast hour is to be plotted the time should be set to the valid future time, which\n    can be done using the `~datetime.datetime` and `~datetime.timedelta` objects\n    from the Python standard library. Note that this requires your data to have a time\n    dimension coordinate.\n    \"\"\"",
  "class Plots2D(SubsetTraits):\n    \"\"\"The highest level class related to plotting 2D data.\n\n    This class collects all common methods no matter whether plotting a scalar variable or\n    vector. Primary settings common to all types of 2D plots include those for data subsets.\n    \"\"\"\n\n    parent = Instance(Panel)\n    _need_redraw = Bool(default_value=True)\n\n    plot_units = Unicode(allow_none=True, default_value=None)\n    plot_units.__doc__ = \"\"\"The desired units to plot the field in.\n\n    Setting this attribute will convert the units of the field variable to the given units for\n    plotting using the MetPy Units module.\n    \"\"\"\n\n    scale = Float(default_value=1e0)\n    scale.__doc__ = \"\"\"Scale the field to be plotted by the value given.\n\n    This attribute will scale the field by multiplying by the scale. For example, to\n    scale vorticity to be whole values for contouring you could set the scale to 1e5, such that\n    the data values will be multiplied by 10^5.\n    \"\"\"\n\n    @property\n    def _cmap_obj(self):\n        \"\"\"Return the colormap object.\n\n        Handle convert the name of the colormap to an object from matplotlib or metpy.\n\n        \"\"\"\n        try:\n            return ctables.registry.get_colortable(self.colormap)\n        except KeyError:\n            return plt.get_cmap(self.colormap)\n\n    @property\n    def _norm_obj(self):\n        \"\"\"Return the normalization object.\n\n        If `image_range` is a matplotlib normalization instance, returns it, otherwise converts\n        the tuple image range to a matplotlib normalization instance.\n\n        \"\"\"\n        if isinstance(self.image_range, plt.Normalize):\n            return self.image_range\n        return plt.Normalize(*self.image_range)\n\n    def clear(self):\n        \"\"\"Clear the plot.\n\n        Resets all internal state and sets need for redraw.\n\n        \"\"\"\n        if getattr(self, 'handle', None) is not None:\n            if getattr(self.handle, 'collections', None) is not None:\n                self.clear_collections()\n            else:\n                self.clear_handle()\n            self._need_redraw = True\n\n    def clear_handle(self):\n        \"\"\"Clear the handle to the plot instance.\"\"\"\n        self.handle.remove()\n        self.handle = None\n\n    def clear_collections(self):\n        \"\"\"Clear the handle collections to the plot instance.\"\"\"\n        for col in self.handle.collections:\n            col.remove()\n        self.handle = None\n\n    @observe('parent')\n    def _parent_changed(self, _):\n        \"\"\"Handle setting the parent object for the plot.\"\"\"\n        self.clear()\n\n    @observe('x', 'longitude', 'y', 'latitude', 'level', 'time')\n    def _update_data(self, _=None):\n        \"\"\"Handle updating the internal cache of data.\n\n        Responds to changes in various subsetting parameters.\n\n        \"\"\"\n        self._griddata = None\n        self.clear()\n\n    # Can't be a Traitlet because notifications don't work with arrays for traits\n    # notification never happens\n    @property\n    def data(self):\n        \"\"\"Xarray dataset that contains the field to be plotted.\"\"\"\n        return self._data\n\n    @data.setter\n    def data(self, val):\n        self._data = val\n        self._update_data()\n\n    @property\n    def name(self):\n        \"\"\"Generate a name for the plot.\"\"\"\n        if isinstance(self.field, tuple):\n            ret = ''\n            ret += ' and '.join(self.field)\n        else:\n            ret = self.field\n        if self.level is not None:\n            ret += f'@{self.level:d}'\n        return ret\n\n    def copy(self):\n        \"\"\"Return a copy of the plot.\"\"\"\n        return copy.copy(self)",
  "class PlotScalar(Plots2D):\n    \"\"\"Defines the common elements of 2D scalar plots for single scalar value fields.\n\n    Most of the other traits here are for one or more of the specific plots. Currently this\n    allows too many options for `ContourPlot` since it does not user image_range, for\n    example. Similar issues for `ImagePlot` and `FilledContourPlot`.\n    \"\"\"\n\n    field = Unicode()\n    field.__doc__ = \"\"\"Name of the field to be plotted.\n\n    This is the name of the variable from the dataset that is to be plotted. An example,\n    from a model grid file that uses the THREDDS convention for naming would be\n    `Geopotential_height_isobaric` or `Temperature_isobaric`. For GOES-16/17 satellite data it\n    might be `Sectorized_CMI`. To check for the variables available within a dataset, list the\n    variables with the following command assuming the dataset was read using xarray as `ds`,\n    `list(ds)`\n    \"\"\"\n\n    smooth_field = Int(allow_none=True, default_value=None)\n    smooth_field.__doc__ = \"\"\"Number of smoothing passes using 9-pt smoother.\n\n    By setting this parameter with an integer value it will call the MetPy 9-pt smoother and\n    provide a smoothed field for plotting. It is best to use this smoothing for data with\n    finer resolutions (e.g., smaller grid spacings with a lot of grid points).\n\n    See Also\n    --------\n    metpy.calc.smooth_n_point, smooth_contour\n    \"\"\"\n\n    smooth_contour = Union([Int(allow_none=True, default_value=None),\n                            Tuple(Int(allow_none=True, default_value=None),\n                                  Int(allow_none=True, default_value=None))])\n    smooth_contour.__doc__ = \"\"\"Spline interpolation to smooth contours.\n\n    This attribute requires settings for the `metpy.calc.zoom_xarray` function, which will\n    produce a spline interpolation given an integer zoom factor. Either a single integer\n    specifying the zoom factor (e.g., 4) or a tuple containing two integers for the zoom factor\n    and the spline interpolation order can be used. The default spline interpolation order is\n    3.\n\n    This is best used to smooth contours when contouring a sparse grid (e.g., when your data\n    has a large grid spacing).\n\n    See Also\n    --------\n    metpy.calc.zoom_xarray, smooth_field\n    \"\"\"\n\n    @observe('field')\n    def _update_data(self, _=None):\n        \"\"\"Handle updating the internal cache of data.\n\n        Responds to changes in various subsetting parameters.\n\n        \"\"\"\n        self._griddata = None\n        self.clear()\n\n    @property\n    def griddata(self):\n        \"\"\"Return the internal cached data.\"\"\"\n        if getattr(self, '_griddata', None) is None:\n\n            # Select our particular field of interest\n            if self.field:\n                data = self.data.metpy.parse_cf(self.field)\n            elif hasattr(self.data.metpy, 'parse_cf'):\n                # Handles the case where we have a dataset but no specified field\n                raise ValueError('field attribute has not been set.')\n            else:\n                data = self.data\n\n            # Subset to 2D using MetPy's fancy .sel\n            subset = {'method': 'nearest'}\n            for dim_coord in ('x', 'longitude', 'y', 'latitude', 'vertical', 'time'):\n                selector = self.level if dim_coord == 'vertical' else getattr(self, dim_coord)\n                if selector is not None:\n                    subset[dim_coord] = selector\n            data_subset = data.metpy.sel(**subset).squeeze()\n            if (data_subset.ndim != 2):\n                if data_subset.ndim == 3:\n                    if (data_subset.shape[-1] not in (3, 4)):\n                        raise ValueError(\n                            'Must provide a combination of subsetting values to give either 2D'\n                            ' data or 3D data subset for plotting with third dimension size 3'\n                            ' or 4'\n                        )\n                else:\n                    raise ValueError(\n                        'Must provide a combination of subsetting values to give 2D data '\n                        'subset for plotting'\n                    )\n            # Handle unit conversion (both direct unit specification and scaling)\n            if self.plot_units is not None:\n                data_subset = data_subset.metpy.convert_units(self.plot_units)\n\n            # Handle smoothing of data\n            if self.smooth_field is not None:\n                data_subset = smooth_n_point(data_subset, 9, self.smooth_field)\n            # Handle zoom interpolation\n            if self.smooth_contour is not None:\n                if isinstance(self.smooth_contour, tuple):\n                    zoom = self.smooth_contour[0]\n                    order = self.smooth_contour[1]\n                else:\n                    zoom = self.smooth_contour\n                    order = 3\n                data_subset = zoom_xarray(data_subset, zoom, order=order)\n\n            self._griddata = data_subset * self.scale\n\n        return self._griddata\n\n    @property\n    def plotdata(self):\n        \"\"\"Return the data for plotting.\n\n        The two dimension coordinates and the data array.\n\n        \"\"\"\n        try:\n            plot_x_dim = self.griddata.metpy.find_axis_number('x')\n            plot_y_dim = self.griddata.metpy.find_axis_number('y')\n        except ValueError:\n            plot_x_dim = 1\n            plot_y_dim = 0\n\n        return (\n            self.griddata[self.griddata.dims[plot_x_dim]],\n            self.griddata[self.griddata.dims[plot_y_dim]],\n            self.griddata\n        )\n\n    def draw(self):\n        \"\"\"Draw the plot.\"\"\"\n        if self._need_redraw:\n            if getattr(self, 'handle', None) is None:\n                self._build()\n            if getattr(self, 'colorbar', None) is not None:\n                cbar = self.parent.ax.figure.colorbar(\n                    self.handle, orientation=self.colorbar, pad=0, aspect=50)\n                cbar.ax.tick_params(labelsize=self.colorbar_fontsize)\n            self._need_redraw = False",
  "class ContourTraits(MetPyHasTraits):\n    \"\"\"Represents common contour traits.\"\"\"\n\n    contours = Union([List(Float()), Int(), Instance(range)], default_value=25)\n    contours.__doc__ = \"\"\"A list of values to contour or an integer number of contour levels.\n\n    This parameter sets contour or colorfill values for a plot. Values can be entered either\n    as a Python range instance, a list of values or as an integer with the number of contours\n    to be plotted (as per matplotlib documentation). A list can be generated by using square\n    brackets or creating a numpy 1D array and converting it to a list with the\n    `~numpy.ndarray.tolist` method.\n    \"\"\"\n\n    clabels = Bool(default_value=False)\n    clabels.__doc__ = \"\"\"A boolean (True/False) on whether to plot contour labels.\n\n    To plot contour labels set this trait to ``True``, the default value is ``False``.\n    \"\"\"\n\n    label_fontsize = Union([Int(), Float(), Unicode()], allow_none=True, default_value=None)\n    label_fontsize.__doc__ = \"\"\"An integer, float, or string value to set the font size of\n    labels for contours.\n\n    This trait sets the font size for labels that will plot along contour lines. Accepts\n    size in points or relative size. Allowed relative sizes are those of Matplotlib:\n    'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'.\n    \"\"\"",
  "class ColorfillTraits(MetPyHasTraits):\n    \"\"\"Represent common colorfill traits.\"\"\"\n\n    colormap = Unicode(allow_none=True, default_value=None)\n    colormap.__doc__ = \"\"\"The string name for a Matplolib or MetPy colormap.\n\n    For example, the Blue-Purple colormap from Matplotlib can be accessed using 'BuPu'.\n    \"\"\"\n\n    image_range = Union([Tuple(Float(allow_none=True), Float(allow_none=True)),\n                         Instance(plt.Normalize)], default_value=(None, None))\n    image_range.__doc__ = \"\"\"A tuple of min and max values that represent the range of values\n    to color the rasterized image.\n\n    The min and max values entered as a tuple will be converted to a\n    `matplotlib.colors.Normalize` instance for plotting.\n    \"\"\"\n\n    colorbar = Unicode(default_value=None, allow_none=True)\n    colorbar.__doc__ = \"\"\"A string (horizontal/vertical) on whether to add a colorbar to the\n    plot.\n\n    To add a colorbar associated with the plot, set the trait to ``horizontal`` or\n    ``vertical``,specifying the orientation of the produced colorbar. The default value is\n    ``None``.\n    \"\"\"\n\n    colorbar_fontsize = Union([Int(), Float(), Unicode()], allow_none=True, default_value=None)\n    colorbar_fontsize.__doc__ = \"\"\"An integer, float, or string value to set the font size of\n    labels for the colorbar.\n\n    This trait sets the font size of labels for the colorbar. Accepts size in points or\n    relative size. Allowed relative sizes are those of Matplotlib: 'xx-small', 'x-small',\n    'small', 'medium', 'large', 'x-large', 'xx-large'.\n    \"\"\"",
  "class ImagePlot(PlotScalar, ColorfillTraits, ValidationMixin):\n    \"\"\"Make raster image using `~matplotlib.pyplot.imshow` for satellite or colored image.\"\"\"\n\n    @observe('colormap', 'image_range')\n    def _set_need_redraw(self, _):\n        \"\"\"Handle changes to attributes that just need a simple redraw.\"\"\"\n        if hasattr(self, 'handle'):\n            self.handle.set_cmap(self._cmap_obj)\n            self.handle.set_norm(self._norm_obj)\n            self._need_redraw = True\n\n    @observe('colorbar')\n    def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of contour()\n        self.clear()\n\n    @property\n    def plotdata(self):\n        \"\"\"Return the data for plotting.\n\n        The two dimension coordinates and the data array\n\n        \"\"\"\n        x_like = self.griddata[self.griddata.dims[1]]\n\n        # At least currently imshow with cartopy does not like this\n        if 'degree' in x_like.units:\n            x_like = x_like.data\n            x_like[x_like > 180] -= 360\n\n        return x_like, self.griddata[self.griddata.dims[0]], self.griddata\n\n    def _build(self):\n        \"\"\"Build the plot by calling any plotting methods as necessary.\"\"\"\n        x_like, y_like, imdata = self.plotdata\n\n        kwargs = plot_kwargs(imdata)\n\n        # If we're on a map, we use min/max for y and manually figure out origin to try to\n        # avoid upside down images created by images where y[0] > y[-1], as well as\n        # specifying the transform\n        kwargs['extent'] = (x_like[0], x_like[-1], y_like.min(), y_like.max())\n        kwargs['origin'] = 'upper' if y_like[0] > y_like[-1] else 'lower'\n\n        self.handle = self.parent.ax.imshow(\n            imdata,\n            cmap=self._cmap_obj,\n            norm=self._norm_obj,\n            **kwargs\n        )",
  "class ContourPlot(PlotScalar, ContourTraits, ValidationMixin):\n    \"\"\"Make contour plots by defining specific traits.\"\"\"\n\n    linecolor = Unicode('black')\n    linecolor.__doc__ = \"\"\"A string value to set the color of plotted contours; default is\n    black.\n\n    This trait can be set to any Matplotlib color\n    (https://matplotlib.org/3.1.0/gallery/color/named_colors.html)\n    \"\"\"\n\n    linewidth = Int(2)\n    linewidth.__doc__ = \"\"\"An integer value to set the width of plotted contours; default value\n    is 2.\n\n    This trait changes the thickness of contour lines with a higher value plotting a thicker\n    line.\n    \"\"\"\n\n    linestyle = Unicode(None, allow_none=True)\n    linestyle.__doc__ = \"\"\"A string value to set the linestyle (e.g., dashed), or `None`;\n    default is `None`, which, when using monochrome line colors, uses solid lines for positive\n    values and dashed lines for negative values.\n\n    The valid string values are those of Matplotlib which are 'solid', 'dashed', 'dotted', and\n    'dashdot', as well as their short codes ('-', '--', '.', '-.'). The object `None`, as\n    described above, can also be used.\n    \"\"\"\n\n    @observe('contours', 'linecolor', 'linewidth', 'linestyle', 'clabels', 'label_fontsize')\n    def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of contour()\n        self.clear()\n\n    def _build(self):\n        \"\"\"Build the plot by calling any plotting methods as necessary.\"\"\"\n        x_like, y_like, imdata = self.plotdata\n\n        kwargs = plot_kwargs(imdata)\n\n        self.handle = self.parent.ax.contour(x_like, y_like, imdata, self.contours,\n                                             colors=self.linecolor, linewidths=self.linewidth,\n                                             linestyles=self.linestyle, **kwargs)\n        if self.clabels:\n            self.handle.clabel(inline=1, fmt='%.0f', inline_spacing=8,\n                               use_clabeltext=True, fontsize=self.label_fontsize)",
  "class FilledContourPlot(PlotScalar, ColorfillTraits, ContourTraits, ValidationMixin):\n    \"\"\"Make color-filled contours plots by defining appropriate traits.\"\"\"\n\n    @observe('contours', 'colorbar', 'colormap')\n    def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of contour()\n        self.clear()\n\n    def _build(self):\n        \"\"\"Build the plot by calling any plotting methods as necessary.\"\"\"\n        x_like, y_like, imdata = self.plotdata\n\n        kwargs = plot_kwargs(imdata)\n\n        self.handle = self.parent.ax.contourf(x_like, y_like, imdata, self.contours,\n                                              cmap=self._cmap_obj, norm=self._norm_obj,\n                                              **kwargs)",
  "class RasterPlot(PlotScalar, ColorfillTraits):\n    \"\"\"Make raster plots by defining relevant traits.\"\"\"\n\n    @observe('image_range', 'colorbar', 'colormap')\n    def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of pcolormesh()\n        self.clear()\n\n    def _build(self):\n        \"\"\"Build the raster plot by calling any plotting methods as necessary.\"\"\"\n        x_like, y_like, imdata = self.plotdata\n\n        kwargs = plot_kwargs(imdata)\n\n        self.handle = self.parent.ax.pcolormesh(x_like, y_like, imdata,\n                                                cmap=self._cmap_obj, norm=self._norm_obj,\n                                                **kwargs)",
  "class PlotVector(Plots2D):\n    \"\"\"Defines common elements for 2D vector plots.\n\n    This class collects common elements including the field trait, which is a tuple argument\n    accepting two strings, for plotting 2D vector fields.\n    \"\"\"\n\n    field = Tuple(Unicode(), Unicode())\n    field.__doc__ = \"\"\"A tuple containing the two components of the vector field from the\n    dataset in the form (east-west component, north-south component).\n\n    For a wind barb plot each component of the wind must be specified and should be of the form\n    (u-wind, v-wind).\n    \"\"\"\n\n    pivot = Unicode('middle')\n    pivot.__doc__ = \"\"\"A string setting the pivot point of the vector. Default value is\n    'middle'.\n\n    This trait takes the values of the keyword argument from `matplotlin.pyplot.barbs`:\n    'tip' or 'middle'.\n    \"\"\"\n\n    skip = Tuple(Int(), Int(), default_value=(1, 1))\n    skip.__doc__ = \"\"\"A tuple of integers to indicate the number of grid points to skip between\n    plotting vectors. Default is (1, 1).\n\n    This trait is to be used to reduce the number of vectors plotted in the (east-west,\n    north-south) components. The two values can be set to the same or different integer values\n    depending on what is desired.\n    \"\"\"\n\n    earth_relative = Bool(default_value=True)\n    earth_relative.__doc__ = \"\"\"A boolean value to indicate whether the vector to be plotted\n    is earth- or grid-relative. Default value is `True`, indicating that vectors are\n    earth-relative.\n\n    Common gridded meteorological datasets including GFS and NARR output contain wind\n    components that are earth-relative. The primary exception is NAM output with wind\n    components that are grid-relative. For any grid-relative vectors set this trait to\n    `False`. This value is ignored for 2D vector fields not in the plane of the plot (e.g.,\n    cross sections).\n    \"\"\"\n\n    color = Unicode(default_value='black')\n    color.__doc__ = \"\"\"A string value that controls the color of the vectors. Default value is\n    black.\n\n    This trait can be set to any named color from\n    `Matplotlibs Colors <https://matplotlib.org/3.1.0/gallery/color/named_colors.html>`\n    \"\"\"\n\n    @observe('field')\n    def _update_data(self, _=None):\n        \"\"\"Handle updating the internal cache of data.\n\n        Responds to changes in various subsetting parameters.\n\n        \"\"\"\n        self._griddata_u = None\n        self._griddata_v = None\n        self.clear()\n\n    @property\n    def griddata(self):\n        \"\"\"Return the internal cached data.\"\"\"\n        if getattr(self, '_griddata_u', None) is None:\n\n            if not self.field[0]:\n                raise ValueError('field attribute not set correctly')\n\n            u = self.data.metpy.parse_cf(self.field[0])\n            v = self.data.metpy.parse_cf(self.field[1])\n\n            # Subset to 2D using MetPy's fancy .sel\n            subset = {'method': 'nearest'}\n            for dim_coord in ('x', 'longitude', 'y', 'latitude', 'vertical', 'time'):\n                selector = self.level if dim_coord == 'vertical' else getattr(self, dim_coord)\n                if selector is not None:\n                    subset[dim_coord] = selector\n            data_subset_u = u.metpy.sel(**subset).squeeze()\n            data_subset_v = v.metpy.sel(**subset).squeeze()\n            if data_subset_u.ndim != 2 or data_subset_v.ndim != 2:\n                raise ValueError(\n                    'Must provide a combination of subsetting values to give 2D data subsets '\n                    'for plotting'\n                )\n\n            if self.plot_units is not None:\n                data_subset_u = data_subset_u.metpy.convert_units(self.plot_units)\n                data_subset_v = data_subset_v.metpy.convert_units(self.plot_units)\n            self._griddata_u = data_subset_u * self.scale\n            self._griddata_v = data_subset_v * self.scale\n\n        return (self._griddata_u, self._griddata_v)\n\n    @property\n    def plotdata(self):\n        \"\"\"Return the data for plotting.\n\n        The dimension coordinates and data arrays.\n\n        \"\"\"\n        check_earth_relative = False\n        try:\n            plot_x_dim = self.griddata[0].metpy.find_axis_number('x')\n            plot_y_dim = self.griddata[0].metpy.find_axis_number('y')\n            check_earth_relative = True\n        except ValueError:\n            plot_x_dim = 1\n            plot_y_dim = 0\n\n        x_like = self.griddata[0][self.griddata[0].dims[plot_x_dim]]\n        y_like = self.griddata[0][self.griddata[0].dims[plot_y_dim]]\n\n        if check_earth_relative:\n            # Conditionally apply earth v. grid relative adjustments if we are in the plane of\n            # the plot\n            # TODO: this seems like it could use a refactor to be more explicit about what\n            # coords are grid x and y vs latitude and longitude (both for code readability and\n            # error-proneness).\n            x, y = x_like, y_like\n            if self.earth_relative:\n                x, y, _ = ccrs.PlateCarree().transform_points(\n                    self.griddata[0].metpy.cartopy_crs,\n                    *np.meshgrid(x, y)\n                ).T\n                x_like = x.T\n                y_like = y.T\n            else:\n                if 'degree' in x.units:\n                    x, y, _ = self.griddata[0].metpy.cartopy_crs.transform_points(\n                        ccrs.PlateCarree(), *np.meshgrid(x, y)).T\n                    x_like = x.T\n                    y_like = y.T\n\n        if x_like.ndim == 1:\n            x_like, y_like = np.meshgrid(x_like, y_like)\n\n        return x_like, y_like, self.griddata[0], self.griddata[1]\n\n    def draw(self):\n        \"\"\"Draw the plot.\"\"\"\n        if self._need_redraw:\n            if getattr(self, 'handle', None) is None:\n                self._build()\n            self._need_redraw = False",
  "class BarbPlot(PlotVector, ValidationMixin):\n    \"\"\"Make plots of wind barbs on a map with traits to refine the look of plotted elements.\"\"\"\n\n    barblength = Float(default_value=7)\n    barblength.__doc__ = \"\"\"A float value that changes the length of the wind barbs. Default\n    value is 7.\n\n    This trait corresponds to the keyword length in `matplotlib.pyplot.barbs`.\n    \"\"\"\n\n    @observe('barblength', 'pivot', 'skip', 'earth_relative', 'color')\n    def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of contour()\n        self.clear()\n\n    def _build(self):\n        \"\"\"Build the plot by calling needed plotting methods as necessary.\"\"\"\n        x_like, y_like, u, v = self.plotdata\n\n        kwargs = plot_kwargs(u)\n\n        # Conditionally apply the proper transform\n        if 'transform' in kwargs and self.earth_relative:\n            kwargs['transform'] = ccrs.PlateCarree()\n\n        wind_slice = (slice(None, None, self.skip[0]), slice(None, None, self.skip[1]))\n\n        self.handle = self.parent.ax.barbs(\n            x_like[wind_slice], y_like[wind_slice],\n            u.values[wind_slice], v.values[wind_slice],\n            color=self.color, pivot=self.pivot, length=self.barblength, zorder=2, **kwargs)",
  "class ArrowPlot(PlotVector, ValidationMixin):\n    \"\"\"Make plots of wind barbs on a map with traits to refine the look of plotted elements.\"\"\"\n\n    arrowscale = Union([Int(), Float(), Unicode()], allow_none=True, default_value=None)\n    arrowscale.__doc__ = \"\"\"Number of data units per arrow length unit, e.g., m/s per plot\n    width; a smaller scale parameter makes the arrow longer. Default is `None`.\n\n    If `None`, a simple autoscaling algorithm is used, based on the average\n    vector length and the number of vectors. The arrow length unit is given by\n    the `key_length` attribute.\n\n    This trait corresponds to the keyword length in `matplotlib.pyplot.quiver`.\n    \"\"\"\n\n    arrowkey = Tuple(Float(allow_none=True), Float(allow_none=True), Float(allow_none=True),\n                     Unicode(allow_none=True), Unicode(allow_none=True), default_value=None,\n                     allow_none=True)\n    arrowkey.__doc__ = \"\"\"Set the characteristics of an arrow key using a tuple of values\n    representing (value, xloc, yloc, position, string).\n\n    Default is `None`.\n\n    If `None`, no vector key will be plotted.\n\n    value default is 100\n    xloc default is 0.85\n    yloc default is 1.02\n    position default is 'E' (options are 'N', 'S', 'E', 'W')\n    label default is an empty string\n\n    If you wish to change a characteristic of the arrowkey you'll need to have a tuple of five\n    elements, fill in the full tuple using `None` for those characteristics you wish to use the\n    default value and put in the new values for the other elements. This trait corresponds to\n    the keyword length in `matplotlib.pyplot.quiverkey`.\n    \"\"\"\n\n    @observe('arrowscale', 'pivot', 'skip', 'earth_relative', 'color', 'arrowkey')\n    def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of quiver()\n        self.clear()\n\n    def _build(self):\n        \"\"\"Build the plot by calling needed plotting methods as necessary.\"\"\"\n        x_like, y_like, u, v = self.plotdata\n\n        kwargs = plot_kwargs(u)\n\n        # Conditionally apply the proper transform\n        if 'transform' in kwargs and self.earth_relative:\n            kwargs['transform'] = ccrs.PlateCarree()\n\n        wind_slice = (slice(None, None, self.skip[0]), slice(None, None, self.skip[1]))\n\n        self.handle = self.parent.ax.quiver(\n            x_like[wind_slice], y_like[wind_slice],\n            u.values[wind_slice], v.values[wind_slice],\n            color=self.color, pivot=self.pivot, scale=self.arrowscale, **kwargs)\n\n        # The order here needs to match the order of the tuple\n        if self.arrowkey is not None:\n            key_kwargs = {'U': 100, 'X': 0.85, 'Y': 1.02, 'labelpos': 'E', 'label': ''}\n            for name, val in zip(key_kwargs, self.arrowkey):\n                if val is not None:\n                    key_kwargs[name] = val\n            self.parent.ax.quiverkey(self.handle, labelcolor=self.color, **key_kwargs)",
  "class PlotObs(MetPyHasTraits, ValidationMixin):\n    \"\"\"The highest level class related to plotting observed surface and upperair data.\n\n    This class collects all common methods no matter whether plotting a upper-level or\n    surface data using station plots.\n\n    List of Traits:\n      * level\n      * time\n      * fields\n      * locations (optional)\n      * time_window (optional)\n      * formats (optional)\n      * colors (optional)\n      * plot_units (optional)\n      * vector_field (optional)\n      * vector_field_color (optional)\n      * vector_field_length (optional)\n      * vector_plot_units (optional)\n      * reduce_points (optional)\n      * fontsize (optional)\n    \"\"\"\n\n    parent = Instance(Panel)\n    _need_redraw = Bool(default_value=True)\n\n    level = Union([Int(allow_none=True), Instance(units.Quantity)], default_value=None)\n    level.__doc__ = \"\"\"The level of the field to be plotted.\n\n    This is a value with units to choose the desired plot level. For example, selecting the\n    850-hPa level, set this parameter to ``850 * units.hPa``. For surface data, parameter\n    must be set to `None`.\n    \"\"\"\n\n    time = Instance(datetime, allow_none=True)\n    time.__doc__ = \"\"\"Set the valid time to be plotted as a datetime object.\n\n    If a forecast hour is to be plotted the time should be set to the valid future time, which\n    can be done using the `~datetime.datetime` and `~datetime.timedelta` objects\n    from the Python standard library.\n    \"\"\"\n\n    time_window = Instance(timedelta, default_value=timedelta(minutes=0), allow_none=True)\n    time_window.__doc__ = \"\"\"Set a range to look for data to plot as a timedelta object.\n\n    If this parameter is set, it will subset the data provided to be within the time and plus\n    or minus the range value given. If there is more than one observation from a given station\n    then it will keep only the most recent one for plotting purposes. Default value is to have\n    no range. (optional)\n    \"\"\"\n\n    fields = List(Unicode())\n    fields.__doc__ = \"\"\"Name of the scalar or symbol fields to be plotted.\n\n    List of parameters to be plotted around station plot (e.g., temperature, dewpoint, skyc).\n    \"\"\"\n\n    locations = List(default_value=['C'])\n    locations.__doc__ = \"\"\"List of strings for scalar or symbol field plotting locations.\n\n    List of parameters locations for plotting parameters around the station plot (e.g.,\n    NW, NE, SW, SE, W, C). (optional)\n    \"\"\"\n\n    formats = List(default_value=[None])\n    formats.__doc__ = \"\"\"List of the scalar, symbol, and text field data formats. (optional)\n\n    List of scalar parameters formatters or mapping values (if symbol) for plotting text and/or\n    symbols around the station plot (e.g., for pressure variable\n    ```lambda v: format(10 * v, '.0f')[-3:]```).\n\n    For symbol mapping the following options are available to be put in as a string:\n    current_weather, sky_cover, low_clouds, mid_clouds, high_clouds, and pressure_tendency.\n\n    For plotting text, use the format setting of 'text'.\n    \"\"\"\n\n    colors = List(Unicode(), default_value=['black'])\n    colors.__doc__ = \"\"\"List of the scalar and symbol field colors.\n\n    List of strings that represent the colors to be used for the variable being plotted.\n    (optional)\n    \"\"\"\n\n    vector_field = List(default_value=[None], allow_none=True)\n    vector_field.__doc__ = \"\"\"List of the vector field to be plotted.\n\n    List of vector components to combined and plotted from the center of the station plot\n    (e.g., wind components). (optional)\n    \"\"\"\n\n    vector_field_color = Unicode('black', allow_none=True)\n    vector_field_color.__doc__ = \"\"\"String color name to plot the vector. (optional)\"\"\"\n\n    vector_field_length = Int(default_value=None, allow_none=True)\n    vector_field_length.__doc__ = \"\"\"Integer value to set the length of the plotted vector.\n    (optional)\n    \"\"\"\n\n    reduce_points = Float(default_value=0)\n    reduce_points.__doc__ = \"\"\"Float to reduce number of points plotted. (optional)\"\"\"\n\n    plot_units = List(default_value=[None], allow_none=True)\n    plot_units.__doc__ = \"\"\"A list of the desired units to plot the fields in.\n\n    Setting this attribute will convert the units of the field variable to the given units for\n    plotting using the MetPy Units module, provided that units are attached to the DataFrame.\n    \"\"\"\n\n    vector_plot_units = Unicode(default_value=None, allow_none=True)\n    vector_plot_units.__doc__ = \"\"\"The desired units to plot the vector field in.\n\n    Setting this attribute will convert the units of the field variable to the given units for\n    plotting using the MetPy Units module, provided that units are attached to the DataFrame.\n    \"\"\"\n\n    fontsize = Int(10)\n    fontsize.__doc__ = \"\"\"An integer value to set the font size of station plots. Default\n    is 10 pt.\"\"\"\n\n    def clear(self):\n        \"\"\"Clear the plot.\n\n        Resets all internal state and sets need for redraw.\n\n        \"\"\"\n        if getattr(self, 'handle', None) is not None:\n            self.handle.ax.cla()\n            self.handle = None\n            self._need_redraw = True\n\n    @observe('parent')\n    def _parent_changed(self, _):\n        \"\"\"Handle setting the parent object for the plot.\"\"\"\n        self.clear()\n\n    @observe('fields', 'level', 'time', 'vector_field', 'time_window')\n    def _update_data(self, _=None):\n        \"\"\"Handle updating the internal cache of data.\n\n        Responds to changes in various subsetting parameters.\n\n        \"\"\"\n        self._obsdata = None\n        self.clear()\n\n    # Can't be a Traitlet because notifications don't work with arrays for traits\n    # notification never happens\n    @property\n    def data(self):\n        \"\"\"Pandas dataframe that contains the fields to be plotted.\"\"\"\n        return self._data\n\n    @data.setter\n    def data(self, val):\n        self._data = val\n        self._update_data()\n\n    @property\n    def name(self):\n        \"\"\"Generate a name for the plot.\"\"\"\n        ret = ''\n        ret += ' and '.join(self.fields)\n        if self.level is not None:\n            ret += f'@{self.level:d}'\n        return ret\n\n    @property\n    def obsdata(self):\n        \"\"\"Return the internal cached data.\"\"\"\n        if getattr(self, '_obsdata', None) is None:\n            # Use a copy of data so we retain all of the original data passed in unmodified\n            data = self.data\n\n            # Subset for a particular level if given\n            if self.level is not None:\n                mag = getattr(self.level, 'magnitude', self.level)\n                data = data[data.pressure == mag]\n\n            # Subset for our particular time\n            if self.time is not None:\n                # If data are not currently indexed by time, we need to do so choosing one of\n                # the columns we're looking for\n                if not isinstance(data.index, pd.DatetimeIndex):\n                    time_vars = ['valid', 'time', 'valid_time', 'date_time', 'date']\n                    dim_times = [time_var for time_var in time_vars if\n                                 time_var in list(self.data)]\n                    if not dim_times:\n                        raise AttributeError(\n                            'Time variable not found. Valid variable names are:'\n                            f'{time_vars}')\n\n                    data = data.set_index(dim_times[0])\n                    if not isinstance(data.index, pd.DatetimeIndex):\n                        # Convert our column of interest to a datetime\n                        data = data.reset_index()\n                        time_index = pd.to_datetime(data[dim_times[0]])\n                        data = data.set_index(time_index)\n\n                # Works around the fact that traitlets 4.3 insists on sending us None by\n                # default because timedelta(0) is Falsey.\n                window = timedelta(minutes=0) if self.time_window is None else self.time_window\n\n                # Indexes need to be properly sorted for the slicing below to work; the\n                # error you get if that's not the case really convoluted, which is why\n                # we don't rely on users doing it.\n                data = data.sort_index()\n                data = data[self.time - window:self.time + window]\n\n            # Look for the station column\n            stn_vars = ['station', 'stn', 'station_id', 'stid']\n            dim_stns = [stn_var for stn_var in stn_vars if stn_var in list(self.data)]\n            if not dim_stns:\n                raise AttributeError('Station variable not found. Valid variable names are: '\n                                     f'{stn_vars}')\n            else:\n                dim_stn = dim_stns[0]\n\n            # Make sure we only use one observation per station\n            self._obsdata = data.groupby(dim_stn).tail(1)\n\n        return self._obsdata\n\n    @property\n    def plotdata(self):\n        \"\"\"Return the data for plotting.\n\n        The data arrays, x coordinates, and y coordinates.\n\n        \"\"\"\n        plot_data = {}\n        for dim_name in list(self.obsdata):\n            if dim_name.find('lat') != -1:\n                lat = self.obsdata[dim_name]\n            elif dim_name.find('lon') != -1:\n                lon = self.obsdata[dim_name]\n            else:\n                plot_data[dim_name] = self.obsdata[dim_name]\n        return lon.values, lat.values, plot_data\n\n    def draw(self):\n        \"\"\"Draw the plot.\"\"\"\n        if self._need_redraw:\n            if getattr(self, 'handle', None) is None:\n                self._build()\n            self._need_redraw = False\n\n    @observe('colors', 'formats', 'locations', 'reduce_points', 'vector_field_color')\n    def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of contour()\n        self.clear()\n\n    def _build(self):\n        \"\"\"Build the plot by calling needed plotting methods as necessary.\"\"\"\n        lon, lat, data = self.plotdata\n\n        # Use the cartopy map projection to transform station locations to the map and\n        # then refine the number of stations plotted by setting a radius\n        scale = 1. if self.parent._proj_obj == ccrs.PlateCarree() else 100000.\n        point_locs = self.parent._proj_obj.transform_points(ccrs.PlateCarree(), lon, lat)\n        subset = reduce_point_density(point_locs, self.reduce_points * scale)\n\n        self.handle = StationPlot(self.parent.ax, lon[subset], lat[subset], clip_on=True,\n                                  transform=ccrs.PlateCarree(), fontsize=self.fontsize)\n\n        for i, ob_type in enumerate(self.fields):\n            field_kwargs = {}\n            location = self.locations[i] if len(self.locations) > 1 else self.locations[0]\n            if len(self.colors) > 1:\n                field_kwargs['color'] = self.colors[i]\n            else:\n                field_kwargs['color'] = self.colors[0]\n            if len(self.formats) > 1:\n                field_kwargs['formatter'] = self.formats[i]\n            else:\n                field_kwargs['formatter'] = self.formats[0]\n            if len(self.plot_units) > 1:\n                field_kwargs['plot_units'] = self.plot_units[i]\n            else:\n                field_kwargs['plot_units'] = self.plot_units[0]\n            if hasattr(self.data, 'units') and (field_kwargs['plot_units'] is not None):\n                parameter = units.Quantity(data[ob_type][subset].values,\n                                           self.data.units[ob_type])\n            else:\n                parameter = data[ob_type][subset]\n            if field_kwargs['formatter'] is not None:\n                mapper = getattr(wx_symbols, str(field_kwargs['formatter']), None)\n                if mapper is not None:\n                    field_kwargs.pop('formatter')\n                    self.handle.plot_symbol(location, parameter, mapper, **field_kwargs)\n                else:\n                    if self.formats[i] == 'text':\n                        self.handle.plot_text(location, parameter, color=field_kwargs['color'])\n                    else:\n                        self.handle.plot_parameter(location, parameter, **field_kwargs)\n            else:\n                field_kwargs.pop('formatter')\n                self.handle.plot_parameter(location, parameter, **field_kwargs)\n\n        if self.vector_field[0] is not None:\n            vector_kwargs = {\n                'color': self.vector_field_color,\n                'plot_units': self.vector_plot_units,\n            }\n\n            if hasattr(self.data, 'units') and (vector_kwargs['plot_units'] is not None):\n                u = units.Quantity(data[self.vector_field[0]][subset].values,\n                                   self.data.units[self.vector_field[0]])\n                v = units.Quantity(data[self.vector_field[1]][subset].values,\n                                   self.data.units[self.vector_field[1]])\n            else:\n                vector_kwargs.pop('plot_units')\n                u = data[self.vector_field[0]][subset]\n                v = data[self.vector_field[1]][subset]\n            if self.vector_field_length is not None:\n                vector_kwargs['length'] = self.vector_field_length\n            self.handle.plot_barb(u, v, **vector_kwargs)\n\n    def copy(self):\n        \"\"\"Return a copy of the plot.\"\"\"\n        return copy.copy(self)",
  "class PlotGeometry(MetPyHasTraits):\n    \"\"\"Plot collections of Shapely objects and customize their appearance.\"\"\"\n\n    parent = Instance(Panel)\n    _need_redraw = Bool(default_value=True)\n\n    geometry = Instance(collections.abc.Iterable, allow_none=False)\n    geometry.__doc__ = \"\"\"A collection of Shapely objects to plot.\n\n    A collection of Shapely objects, such as the 'geometry' column from a\n    ``geopandas.GeoDataFrame``. Acceptable Shapely objects are ``shapely.MultiPolygon``,\n    ``shapely.Polygon``, ``shapely.MultiLineString``, ``shapely.LineString``,\n    ``shapely.MultiPoint``, and ``shapely.Point``.\n    \"\"\"\n\n    fill = Union([Instance(collections.abc.Iterable), Unicode()], default_value=['lightgray'],\n                 allow_none=True)\n    fill.__doc__ = \"\"\"Fill color(s) for polygons and points.\n\n    A single string (color name or hex code) or collection of strings with which to fill\n    polygons and points. If a collection, the first color corresponds to the first Shapely\n    object in `geometry`, the second color corresponds to the second Shapely object, and so on.\n    If `fill` is shorter than `geometry`, `fill` cycles back to the beginning, repeating the\n    sequence of colors as needed. Default value is lightgray.\n    \"\"\"\n\n    stroke = Union([Instance(collections.abc.Iterable), Unicode()], default_value=['black'],\n                   allow_none=True)\n    stroke.__doc__ = \"\"\"Stroke color(s) for polygons and line color(s) for lines.\n\n    A single string (color name or hex code) or collection of strings with which to outline\n    polygons and color lines. If a collection, the first color corresponds to the first Shapely\n    object in `geometry`, the second color corresponds to the second Shapely object, and so on.\n    If `stroke` is shorter than `geometry`, `stroke` cycles back to the beginning, repeating\n    the sequence of colors as needed. Default value is black.\n    \"\"\"\n\n    marker = Unicode(default_value='.', allow_none=False)\n    marker.__doc__ = \"\"\"Symbol used to denote points.\n\n    Accepts any matplotlib marker. Default value is '.', which plots a dot at each point.\n    \"\"\"\n\n    labels = Instance(collections.abc.Iterable, allow_none=True)\n    labels.__doc__ = \"\"\"A collection of labels corresponding to plotted geometry.\n\n    A collection of strings to use as labels for geometry, such as a column from a\n    ``Geopandas.GeoDataFrame``. The first label corresponds to the first Shapely object in\n    `geometry`, the second label corresponds to the second Shapely object, and so on. The\n    length of `labels` must be equal to the length of `geometry`. Labels are positioned along\n    the edge of polygons, and below lines and points. No labels are plotted if this attribute\n    is left undefined, or set equal to `None`.\n    \"\"\"\n\n    label_fontsize = Union([Int(), Float(), Unicode()], default_value=None, allow_none=True)\n    label_fontsize.__doc__ = \"\"\"An integer or string value for the font size of labels.\n\n    Accepts size in points or relative size. Allowed relative sizes are those of Matplotlib:\n    'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'.\n    \"\"\"\n\n    label_facecolor = Union([Instance(collections.abc.Iterable), Unicode()], allow_none=True)\n    label_facecolor.__doc__ = \"\"\"Font color(s) for labels.\n\n    A single string (color name or hex code) or collection of strings for the font color of\n    labels. If a collection, the first color corresponds to the label of the first Shapely\n    object in `geometry`, the second color corresponds to the label of the second Shapely\n    object, and so on. Default value is `stroke`.\n    \"\"\"\n\n    label_edgecolor = Union([Instance(collections.abc.Iterable), Unicode()], allow_none=True)\n    label_edgecolor.__doc__ = \"\"\"Outline color(s) for labels.\n\n    A single string (color name or hex code) or collection of strings for the outline color of\n    labels. If a collection, the first color corresponds to the label of the first Shapely\n    object in `geometry`, the second color corresponds to the label of the second Shapely\n    object, and so on. Default value is `fill`.\n    \"\"\"\n\n    @staticmethod\n    @validate('geometry')\n    def _valid_geometry(_, proposal):\n        \"\"\"Cast `geometry` into a list once it is provided by user.\n\n        Users can provide any kind of collection, such as a ``GeoPandas.GeoSeries``, and this\n        turns them into a list.\n        \"\"\"\n        geometry = proposal['value']\n        return list(geometry)\n\n    @staticmethod\n    @validate('fill', 'stroke', 'label_facecolor', 'label_edgecolor')\n    def _valid_color_list(_, proposal):\n        \"\"\"Cast color-related attributes into a list once provided by user.\n\n        This is necessary because _build() expects to cycle through a list of colors when\n        assigning them to the geometry.\n        \"\"\"\n        color = proposal['value']\n\n        if isinstance(color, str):\n            color = [color]\n        # `color` must be a collection if it is not a string\n        else:\n            color = list(color)\n\n        return color\n\n    @staticmethod\n    @validate('labels')\n    def _valid_labels(_, proposal):\n        \"\"\"Cast `labels` into a list once provided by user.\"\"\"\n        labels = proposal['value']\n        return list(labels)\n\n    @observe('fill', 'stroke')\n    def _update_label_colors(self, change):\n        \"\"\"Set default text colors using `fill` and `stroke`.\n\n        If `label_facecolor` or `label_edgecolor` have not been specified, provide default\n        colors for those attributes using `fill` and `stroke`.\n        \"\"\"\n        if change['name'] == 'fill' and self.label_edgecolor is None:\n            self.label_edgecolor = self.fill\n        elif change['name'] == 'stroke' and self.label_facecolor is None:\n            self.label_facecolor = self.stroke\n\n    @property\n    def name(self):\n        \"\"\"Generate a name for the plot.\"\"\"\n        # Unlike Plots2D and PlotObs, there are no other attributes (such as 'fields' or\n        # 'levels') from which to name the plot. A generic name is returned here in case the\n        # user does not provide their own title, in which case MapPanel.draw() looks here.\n        return 'Geometry Plot'\n\n    @staticmethod\n    def _position_label(geo_obj, label):\n        \"\"\"Return a (lon, lat) where the label of a polygon/line/point can be placed.\"\"\"\n        from shapely.geometry import MultiLineString, MultiPoint, MultiPolygon, Polygon\n\n        # A hash of the label is used in choosing a point along the polygon or line that\n        # will be returned. This \"psuedo-randomizes\" the position of a label, in hopes of\n        # spatially dispersing the labels and lessening the chance that labels overlap.\n        label_hash = sum(map(ord, str(label)))\n\n        # If object is a MultiPolygon or MultiLineString, associate the label with the single\n        # largest Polygon or LineString from the collection. If MultiPoint, associate the label\n        # with one of the Points in the MultiPoint, chosen based on the label hash.\n        if isinstance(geo_obj, (MultiPolygon, MultiLineString)):\n            geo_obj = max(geo_obj.geoms, key=lambda x: x.length)\n        elif isinstance(geo_obj, MultiPoint):\n            geo_obj = geo_obj.geoms[label_hash % len(geo_obj.geoms)]\n\n        # Get the list of coordinates of the polygon/line/point\n        if isinstance(geo_obj, Polygon):\n            coords = geo_obj.exterior.coords\n        else:\n            coords = geo_obj.coords\n\n        return coords[label_hash % len(coords)]\n\n    def _draw_label(self, text, lon, lat, color='black', outline='white', offset=(0, 0)):\n        \"\"\"Draw a label to the plot.\n\n        Parameters\n        ----------\n        text : str\n            The label's text\n        lon : float\n            Longitude at which to position the label\n        lat : float\n            Latitude at which to position the label\n        color : str (default: 'black')\n            Name or hex code for the color of the text\n        outline : str (default: 'white')\n            Name or hex code of the color of the outline of the text\n        offset : tuple (default: (0, 0))\n            A tuple containing the x- and y-offset of the label, respectively\n        \"\"\"\n        path_effects = [patheffects.withStroke(linewidth=4, foreground=outline)]\n        self.parent.ax.add_artist(TextCollection([lon], [lat], [str(text)],\n                                                 va='center',\n                                                 ha='center',\n                                                 offset=offset,\n                                                 weight='demi',\n                                                 size=self.label_fontsize,\n                                                 color=color,\n                                                 path_effects=path_effects,\n                                                 transform=ccrs.PlateCarree()))\n\n    def draw(self):\n        \"\"\"Draw the plot.\"\"\"\n        if self._need_redraw:\n            if getattr(self, 'handles', None) is None:\n                self._build()\n            self._need_redraw = False\n\n    def copy(self):\n        \"\"\"Return a copy of the plot.\"\"\"\n        return copy.copy(self)\n\n    def _build(self):\n        \"\"\"Build the plot by calling needed plotting methods as necessary.\"\"\"\n        from shapely.geometry import (LineString, MultiLineString, MultiPoint, MultiPolygon,\n                                      Point, Polygon)\n\n        # Cast attributes to a list if None, since traitlets doesn't call validators (like\n        # `_valid_color_list()` and `_valid_labels()`) when the proposed value is None.\n        self.fill = ['none'] if self.fill is None else self.fill\n        self.stroke = ['none'] if self.stroke is None else self.stroke\n        self.labels = [''] if self.labels is None else self.labels\n        self.label_edgecolor = (['none'] if self.label_edgecolor is None\n                                else self.label_edgecolor)\n        self.label_facecolor = (['none'] if self.label_facecolor is None\n                                else self.label_facecolor)\n\n        # Each Shapely object is plotted separately with its corresponding colors and label\n        for geo_obj, stroke, fill, label, fontcolor, fontoutline in zip(\n                self.geometry, cycle(self.stroke), cycle(self.fill), cycle(self.labels),\n                cycle(self.label_facecolor), cycle(self.label_edgecolor)):\n            # Plot the Shapely object with the appropriate method and colors\n            if isinstance(geo_obj, (MultiPolygon, Polygon)):\n                self.parent.ax.add_geometries([geo_obj], edgecolor=stroke,\n                                              facecolor=fill, crs=ccrs.PlateCarree())\n            elif isinstance(geo_obj, (MultiLineString, LineString)):\n                self.parent.ax.add_geometries([geo_obj], edgecolor=stroke,\n                                              facecolor='none', crs=ccrs.PlateCarree())\n            elif isinstance(geo_obj, MultiPoint):\n                for point in geo_obj.geoms:\n                    lon, lat = point.coords[0]\n                    self.parent.ax.plot(lon, lat, color=fill, marker=self.marker,\n                                        transform=ccrs.PlateCarree())\n            elif isinstance(geo_obj, Point):\n                lon, lat = geo_obj.coords[0]\n                self.parent.ax.plot(lon, lat, color=fill, marker=self.marker,\n                                    transform=ccrs.PlateCarree())\n\n            # Plot labels if provided\n            if label:\n                # If fontcolor is None/'none', choose a font color\n                if fontcolor in [None, 'none'] and stroke not in [None, 'none']:\n                    fontcolor = stroke\n                elif fontcolor in [None, 'none']:\n                    fontcolor = 'black'\n\n                # If fontoutline is None/'none', choose a font outline\n                if fontoutline in [None, 'none'] and fill not in [None, 'none']:\n                    fontoutline = fill\n                elif fontoutline in [None, 'none']:\n                    fontoutline = 'white'\n\n                # Choose a point along the polygon/line/point to place label\n                lon, lat = self._position_label(geo_obj, label)\n\n                # If polygon, put label directly on edge of polygon. If line or point, put\n                # label slightly below line/point.\n                if isinstance(geo_obj, (MultiPolygon, Polygon)):\n                    offset = (0, 0)\n                else:\n                    offset = (0, -12)\n\n                # Finally, draw the label\n                self._draw_label(label, lon, lat, fontcolor, fontoutline, offset)",
  "def __setattr__(self, name, value):\n        \"\"\"Set only permitted attributes.\"\"\"\n        allowlist = ['ax',\n                     'data',\n                     'handle',\n                     'notify_change',\n                     'panel'\n                     ]\n\n        allowlist.extend(self.trait_names())\n        if name in allowlist or name.startswith('_'):\n            super().__setattr__(name, value)\n        else:\n            closest = get_close_matches(name, allowlist, n=1)\n            if closest:\n                alt = closest[0]\n                suggest = f\" Perhaps you meant '{alt}'?\"\n            else:\n                suggest = ''\n            obj = self.__class__\n            msg = f\"'{name}' is not a valid attribute for {obj}.\" + suggest\n            raise AttributeError(msg)",
  "def __dir__(self):\n        \"\"\"Filter dir to be more helpful for tab-completion in Jupyter.\"\"\"\n        return filter(\n            lambda name: not (name in dir(HasTraits) or name.startswith('_')),\n            dir(type(self))\n        )",
  "def panel(self):\n        \"\"\"Provide simple access for a single panel.\"\"\"\n        return self.panels[0]",
  "def panel(self, val):\n        self.panels = [val]",
  "def _panels_changed(self, change):\n        for panel in change.new:\n            panel.parent = self\n            panel.observe(self.refresh, names=('_need_redraw'))",
  "def figure(self):\n        \"\"\"Provide access to the underlying figure object.\"\"\"\n        if not hasattr(self, '_fig'):\n            self._fig = plt.figure(figsize=self.size)\n        return self._fig",
  "def refresh(self, _):\n        \"\"\"Refresh the rendering of all panels.\"\"\"\n        # First make sure everything is properly constructed\n        self.draw()\n\n        # Trigger the graphics refresh\n        self.figure.canvas.draw()\n\n        # Flush out interactive events--only ok on Agg for newer matplotlib\n        with contextlib.suppress(NotImplementedError):\n            self.figure.canvas.flush_events()",
  "def draw(self):\n        \"\"\"Draw the collection of panels.\"\"\"\n        for panel in self.panels:\n            with panel.hold_trait_notifications():\n                panel.draw()",
  "def save(self, *args, **kwargs):\n        \"\"\"Save the constructed graphic as an image file.\n\n        This method takes a string for saved file name. Additionally, the same arguments and\n        keyword arguments that `matplotlib.pyplot.savefig` does.\n        \"\"\"\n        self.draw()\n        self.figure.savefig(*args, **kwargs)",
  "def show(self):\n        \"\"\"Show the constructed graphic on the screen.\"\"\"\n        self.draw()\n        plt.show()",
  "def copy(self):\n        \"\"\"Return a copy of the panel container.\"\"\"\n        return copy.copy(self)",
  "def _valid_area(self, proposal):\n        \"\"\"Check that proposed string or tuple is valid and turn string into a tuple extent.\"\"\"\n        from .plot_areas import named_areas\n\n        area = proposal['value']\n\n        # Parse string, check that string is valid, and determine extent based on string\n        if isinstance(area, str):\n            match = re.match(r'(\\w+)([-+]*)$', area)\n            if match is None:\n                raise TraitError(f'\"{area}\" is not a valid area.')\n            region, modifier = match.groups()\n            region = region.lower()\n\n            if region == 'global':\n                extent = 'global'\n            elif region in named_areas:\n                self._area_proj = named_areas[region].projection\n                extent = named_areas[region].bounds\n                zoom = modifier.count('+') - modifier.count('-')\n                extent = self._zoom_extent(extent, zoom)\n            else:\n                raise TraitError(f'\"{area}\" is not a valid string area.')\n        # Otherwise, assume area is a tuple and check that latitudes/longitudes are valid\n        else:\n            west_lon, east_lon, south_lat, north_lat = area\n            valid_west = -180 <= west_lon <= 180\n            valid_east = -180 <= east_lon <= 180\n            valid_south = -90 <= south_lat <= 90\n            valid_north = -90 <= north_lat <= 90\n            if not (valid_west and valid_east and valid_south and valid_north):\n                raise TraitError(f'\"{area}\" is not a valid string area.')\n            extent = area\n\n        return extent",
  "def _plots_changed(self, change):\n        \"\"\"Handle when our collection of plots changes.\"\"\"\n        for plot in change.new:\n            plot.parent = self\n            plot.observe(self.refresh, names=('_need_redraw'))\n        self._need_redraw = True",
  "def _parent_changed(self, _):\n        \"\"\"Handle when the parent is changed.\"\"\"\n        self.ax = None",
  "def _proj_obj(self):\n        \"\"\"Return the projection as a Cartopy object.\n\n        Handles looking up a string for the projection, or if the projection\n        is set to ``'data'`` looks at the data for the projection.\n\n        \"\"\"\n        if isinstance(self.projection, str):\n            if self.projection == 'data':\n                if isinstance(self.plots[0].griddata, tuple):\n                    proj = self.plots[0].griddata[0].metpy.cartopy_crs\n                else:\n                    proj = self.plots[0].griddata.metpy.cartopy_crs\n            elif self.projection == 'area':\n                proj = self._area_proj\n            else:\n                proj = lookup_projection(self.projection)\n        else:\n            proj = self.projection\n        return proj",
  "def _layer_features(self):\n        \"\"\"Iterate over all map features and return as Cartopy objects.\n\n        Handle converting names of maps to auto-scaling map features.\n\n        \"\"\"\n        for item in self.layers:\n            feat = lookup_map_feature(item) if isinstance(item, str) else item\n            yield feat",
  "def _set_need_redraw(self, _):\n        \"\"\"Watch traits and set the need redraw flag as necessary.\"\"\"\n        self._need_redraw = True",
  "def _zoom_extent(extent, zoom):\n        \"\"\"Calculate new bounds for zooming in or out of a given extent.\n\n        ``extent`` is given as a tuple with four numeric values, in the same format as the\n        ``area`` trait.\n\n        If ``zoom`` = 0, the extent will not be changed from what was provided to the method\n        If ``zoom`` > 0, the returned extent will be smaller (zoomed in)\n        If ``zoom`` < 0, the returned extent will be larger (zoomed out)\n\n        \"\"\"\n        west_lon, east_lon, south_lat, north_lat = extent\n\n        # Turn number of pluses and minuses into a number than can scale the latitudes and\n        # longitudes of our extent\n        zoom_multiplier = (1 - 2**-zoom) / 2\n\n        # Calculate bounds for new, zoomed extent\n        new_north_lat = north_lat + (south_lat - north_lat) * zoom_multiplier\n        new_south_lat = south_lat - (south_lat - north_lat) * zoom_multiplier\n        new_east_lon = east_lon + (west_lon - east_lon) * zoom_multiplier\n        new_west_lon = west_lon - (west_lon - east_lon) * zoom_multiplier\n\n        return (new_west_lon, new_east_lon, new_south_lat, new_north_lat)",
  "def ax(self):\n        \"\"\"Get the :class:`matplotlib.axes.Axes` to draw on.\n\n        Creates a new instance if necessary.\n\n        \"\"\"\n        # If we haven't actually made an instance yet, make one with the right size and\n        # map projection.\n        if getattr(self, '_ax', None) is None:\n            self._ax = self.parent.figure.add_subplot(*self.layout, projection=self._proj_obj)\n\n        return self._ax",
  "def ax(self, val):\n        \"\"\"Set the :class:`matplotlib.axes.Axes` to draw on.\n\n        Clears existing state as necessary.\n\n        \"\"\"\n        if getattr(self, '_ax', None) is not None:\n            self._ax.cla()\n        self._ax = val",
  "def refresh(self, changed):\n        \"\"\"Refresh the drawing if necessary.\"\"\"\n        self._need_redraw = changed.new",
  "def draw(self):\n        \"\"\"Draw the panel.\"\"\"\n        # Only need to run if we've actually changed.\n        if self._need_redraw:\n\n            # Set the extent as appropriate based on the area. One special case for 'global'.\n            if self.area == 'global':\n                self.ax.set_global()\n            elif self.area is not None:\n                self.ax.set_extent(self.area, ccrs.PlateCarree())\n\n            # Draw all of the plots.\n            for p in self.plots:\n                with p.hold_trait_notifications():\n                    p.draw()\n\n            # Add all of the maps\n            if len(self.layers) > len(self.layers_edgecolor):\n                self.layers_edgecolor *= len(self.layers)\n            if len(self.layers) > len(self.layers_linewidth):\n                self.layers_linewidth *= len(self.layers)\n            for i, feat in enumerate(self._layer_features):\n                if self.layers[i] in ['', 'land', 'lake', 'river']:\n                    color = 'face'\n                else:\n                    color = self.layers_edgecolor[i]\n                width = self.layers_linewidth[i]\n                self.ax.add_feature(feat, edgecolor=color, linewidth=width)\n\n            # Use the set title or generate one.\n            if (self.right_title is None) and (self.left_title is None):\n                title = self.title or ',\\n'.join(plot.name for plot in self.plots)\n                self.ax.set_title(title, fontsize=self.title_fontsize)\n            else:\n                if self.title is not None:\n                    self.ax.set_title(self.title, fontsize=self.title_fontsize)\n                if self.right_title is not None:\n                    self.ax.set_title(self.right_title, fontsize=self.title_fontsize,\n                                      loc='right')\n                if self.left_title is not None:\n                    self.ax.set_title(self.left_title, fontsize=self.title_fontsize,\n                                      loc='left')\n            self._need_redraw = False",
  "def __copy__(self):\n        \"\"\"Return a copy of this MapPanel.\"\"\"\n        # Create new, blank instance of MapPanel\n        cls = self.__class__\n        obj = cls.__new__(cls)\n\n        # Copy each attribute from current MapPanel to new MapPanel\n        for name in self.trait_names():\n            # The 'plots' attribute is a list.\n            # A copy must be made for each plot in the list.\n            if name == 'plots':\n                obj.plots = [copy.copy(plot) for plot in self.plots]\n            else:\n                setattr(obj, name, getattr(self, name))\n\n        return obj",
  "def copy(self):\n        \"\"\"Return a copy of the panel.\"\"\"\n        return copy.copy(self)",
  "def _cmap_obj(self):\n        \"\"\"Return the colormap object.\n\n        Handle convert the name of the colormap to an object from matplotlib or metpy.\n\n        \"\"\"\n        try:\n            return ctables.registry.get_colortable(self.colormap)\n        except KeyError:\n            return plt.get_cmap(self.colormap)",
  "def _norm_obj(self):\n        \"\"\"Return the normalization object.\n\n        If `image_range` is a matplotlib normalization instance, returns it, otherwise converts\n        the tuple image range to a matplotlib normalization instance.\n\n        \"\"\"\n        if isinstance(self.image_range, plt.Normalize):\n            return self.image_range\n        return plt.Normalize(*self.image_range)",
  "def clear(self):\n        \"\"\"Clear the plot.\n\n        Resets all internal state and sets need for redraw.\n\n        \"\"\"\n        if getattr(self, 'handle', None) is not None:\n            if getattr(self.handle, 'collections', None) is not None:\n                self.clear_collections()\n            else:\n                self.clear_handle()\n            self._need_redraw = True",
  "def clear_handle(self):\n        \"\"\"Clear the handle to the plot instance.\"\"\"\n        self.handle.remove()\n        self.handle = None",
  "def clear_collections(self):\n        \"\"\"Clear the handle collections to the plot instance.\"\"\"\n        for col in self.handle.collections:\n            col.remove()\n        self.handle = None",
  "def _parent_changed(self, _):\n        \"\"\"Handle setting the parent object for the plot.\"\"\"\n        self.clear()",
  "def _update_data(self, _=None):\n        \"\"\"Handle updating the internal cache of data.\n\n        Responds to changes in various subsetting parameters.\n\n        \"\"\"\n        self._griddata = None\n        self.clear()",
  "def data(self):\n        \"\"\"Xarray dataset that contains the field to be plotted.\"\"\"\n        return self._data",
  "def data(self, val):\n        self._data = val\n        self._update_data()",
  "def name(self):\n        \"\"\"Generate a name for the plot.\"\"\"\n        if isinstance(self.field, tuple):\n            ret = ''\n            ret += ' and '.join(self.field)\n        else:\n            ret = self.field\n        if self.level is not None:\n            ret += f'@{self.level:d}'\n        return ret",
  "def copy(self):\n        \"\"\"Return a copy of the plot.\"\"\"\n        return copy.copy(self)",
  "def _update_data(self, _=None):\n        \"\"\"Handle updating the internal cache of data.\n\n        Responds to changes in various subsetting parameters.\n\n        \"\"\"\n        self._griddata = None\n        self.clear()",
  "def griddata(self):\n        \"\"\"Return the internal cached data.\"\"\"\n        if getattr(self, '_griddata', None) is None:\n\n            # Select our particular field of interest\n            if self.field:\n                data = self.data.metpy.parse_cf(self.field)\n            elif hasattr(self.data.metpy, 'parse_cf'):\n                # Handles the case where we have a dataset but no specified field\n                raise ValueError('field attribute has not been set.')\n            else:\n                data = self.data\n\n            # Subset to 2D using MetPy's fancy .sel\n            subset = {'method': 'nearest'}\n            for dim_coord in ('x', 'longitude', 'y', 'latitude', 'vertical', 'time'):\n                selector = self.level if dim_coord == 'vertical' else getattr(self, dim_coord)\n                if selector is not None:\n                    subset[dim_coord] = selector\n            data_subset = data.metpy.sel(**subset).squeeze()\n            if (data_subset.ndim != 2):\n                if data_subset.ndim == 3:\n                    if (data_subset.shape[-1] not in (3, 4)):\n                        raise ValueError(\n                            'Must provide a combination of subsetting values to give either 2D'\n                            ' data or 3D data subset for plotting with third dimension size 3'\n                            ' or 4'\n                        )\n                else:\n                    raise ValueError(\n                        'Must provide a combination of subsetting values to give 2D data '\n                        'subset for plotting'\n                    )\n            # Handle unit conversion (both direct unit specification and scaling)\n            if self.plot_units is not None:\n                data_subset = data_subset.metpy.convert_units(self.plot_units)\n\n            # Handle smoothing of data\n            if self.smooth_field is not None:\n                data_subset = smooth_n_point(data_subset, 9, self.smooth_field)\n            # Handle zoom interpolation\n            if self.smooth_contour is not None:\n                if isinstance(self.smooth_contour, tuple):\n                    zoom = self.smooth_contour[0]\n                    order = self.smooth_contour[1]\n                else:\n                    zoom = self.smooth_contour\n                    order = 3\n                data_subset = zoom_xarray(data_subset, zoom, order=order)\n\n            self._griddata = data_subset * self.scale\n\n        return self._griddata",
  "def plotdata(self):\n        \"\"\"Return the data for plotting.\n\n        The two dimension coordinates and the data array.\n\n        \"\"\"\n        try:\n            plot_x_dim = self.griddata.metpy.find_axis_number('x')\n            plot_y_dim = self.griddata.metpy.find_axis_number('y')\n        except ValueError:\n            plot_x_dim = 1\n            plot_y_dim = 0\n\n        return (\n            self.griddata[self.griddata.dims[plot_x_dim]],\n            self.griddata[self.griddata.dims[plot_y_dim]],\n            self.griddata\n        )",
  "def draw(self):\n        \"\"\"Draw the plot.\"\"\"\n        if self._need_redraw:\n            if getattr(self, 'handle', None) is None:\n                self._build()\n            if getattr(self, 'colorbar', None) is not None:\n                cbar = self.parent.ax.figure.colorbar(\n                    self.handle, orientation=self.colorbar, pad=0, aspect=50)\n                cbar.ax.tick_params(labelsize=self.colorbar_fontsize)\n            self._need_redraw = False",
  "def _set_need_redraw(self, _):\n        \"\"\"Handle changes to attributes that just need a simple redraw.\"\"\"\n        if hasattr(self, 'handle'):\n            self.handle.set_cmap(self._cmap_obj)\n            self.handle.set_norm(self._norm_obj)\n            self._need_redraw = True",
  "def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of contour()\n        self.clear()",
  "def plotdata(self):\n        \"\"\"Return the data for plotting.\n\n        The two dimension coordinates and the data array\n\n        \"\"\"\n        x_like = self.griddata[self.griddata.dims[1]]\n\n        # At least currently imshow with cartopy does not like this\n        if 'degree' in x_like.units:\n            x_like = x_like.data\n            x_like[x_like > 180] -= 360\n\n        return x_like, self.griddata[self.griddata.dims[0]], self.griddata",
  "def _build(self):\n        \"\"\"Build the plot by calling any plotting methods as necessary.\"\"\"\n        x_like, y_like, imdata = self.plotdata\n\n        kwargs = plot_kwargs(imdata)\n\n        # If we're on a map, we use min/max for y and manually figure out origin to try to\n        # avoid upside down images created by images where y[0] > y[-1], as well as\n        # specifying the transform\n        kwargs['extent'] = (x_like[0], x_like[-1], y_like.min(), y_like.max())\n        kwargs['origin'] = 'upper' if y_like[0] > y_like[-1] else 'lower'\n\n        self.handle = self.parent.ax.imshow(\n            imdata,\n            cmap=self._cmap_obj,\n            norm=self._norm_obj,\n            **kwargs\n        )",
  "def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of contour()\n        self.clear()",
  "def _build(self):\n        \"\"\"Build the plot by calling any plotting methods as necessary.\"\"\"\n        x_like, y_like, imdata = self.plotdata\n\n        kwargs = plot_kwargs(imdata)\n\n        self.handle = self.parent.ax.contour(x_like, y_like, imdata, self.contours,\n                                             colors=self.linecolor, linewidths=self.linewidth,\n                                             linestyles=self.linestyle, **kwargs)\n        if self.clabels:\n            self.handle.clabel(inline=1, fmt='%.0f', inline_spacing=8,\n                               use_clabeltext=True, fontsize=self.label_fontsize)",
  "def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of contour()\n        self.clear()",
  "def _build(self):\n        \"\"\"Build the plot by calling any plotting methods as necessary.\"\"\"\n        x_like, y_like, imdata = self.plotdata\n\n        kwargs = plot_kwargs(imdata)\n\n        self.handle = self.parent.ax.contourf(x_like, y_like, imdata, self.contours,\n                                              cmap=self._cmap_obj, norm=self._norm_obj,\n                                              **kwargs)",
  "def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of pcolormesh()\n        self.clear()",
  "def _build(self):\n        \"\"\"Build the raster plot by calling any plotting methods as necessary.\"\"\"\n        x_like, y_like, imdata = self.plotdata\n\n        kwargs = plot_kwargs(imdata)\n\n        self.handle = self.parent.ax.pcolormesh(x_like, y_like, imdata,\n                                                cmap=self._cmap_obj, norm=self._norm_obj,\n                                                **kwargs)",
  "def _update_data(self, _=None):\n        \"\"\"Handle updating the internal cache of data.\n\n        Responds to changes in various subsetting parameters.\n\n        \"\"\"\n        self._griddata_u = None\n        self._griddata_v = None\n        self.clear()",
  "def griddata(self):\n        \"\"\"Return the internal cached data.\"\"\"\n        if getattr(self, '_griddata_u', None) is None:\n\n            if not self.field[0]:\n                raise ValueError('field attribute not set correctly')\n\n            u = self.data.metpy.parse_cf(self.field[0])\n            v = self.data.metpy.parse_cf(self.field[1])\n\n            # Subset to 2D using MetPy's fancy .sel\n            subset = {'method': 'nearest'}\n            for dim_coord in ('x', 'longitude', 'y', 'latitude', 'vertical', 'time'):\n                selector = self.level if dim_coord == 'vertical' else getattr(self, dim_coord)\n                if selector is not None:\n                    subset[dim_coord] = selector\n            data_subset_u = u.metpy.sel(**subset).squeeze()\n            data_subset_v = v.metpy.sel(**subset).squeeze()\n            if data_subset_u.ndim != 2 or data_subset_v.ndim != 2:\n                raise ValueError(\n                    'Must provide a combination of subsetting values to give 2D data subsets '\n                    'for plotting'\n                )\n\n            if self.plot_units is not None:\n                data_subset_u = data_subset_u.metpy.convert_units(self.plot_units)\n                data_subset_v = data_subset_v.metpy.convert_units(self.plot_units)\n            self._griddata_u = data_subset_u * self.scale\n            self._griddata_v = data_subset_v * self.scale\n\n        return (self._griddata_u, self._griddata_v)",
  "def plotdata(self):\n        \"\"\"Return the data for plotting.\n\n        The dimension coordinates and data arrays.\n\n        \"\"\"\n        check_earth_relative = False\n        try:\n            plot_x_dim = self.griddata[0].metpy.find_axis_number('x')\n            plot_y_dim = self.griddata[0].metpy.find_axis_number('y')\n            check_earth_relative = True\n        except ValueError:\n            plot_x_dim = 1\n            plot_y_dim = 0\n\n        x_like = self.griddata[0][self.griddata[0].dims[plot_x_dim]]\n        y_like = self.griddata[0][self.griddata[0].dims[plot_y_dim]]\n\n        if check_earth_relative:\n            # Conditionally apply earth v. grid relative adjustments if we are in the plane of\n            # the plot\n            # TODO: this seems like it could use a refactor to be more explicit about what\n            # coords are grid x and y vs latitude and longitude (both for code readability and\n            # error-proneness).\n            x, y = x_like, y_like\n            if self.earth_relative:\n                x, y, _ = ccrs.PlateCarree().transform_points(\n                    self.griddata[0].metpy.cartopy_crs,\n                    *np.meshgrid(x, y)\n                ).T\n                x_like = x.T\n                y_like = y.T\n            else:\n                if 'degree' in x.units:\n                    x, y, _ = self.griddata[0].metpy.cartopy_crs.transform_points(\n                        ccrs.PlateCarree(), *np.meshgrid(x, y)).T\n                    x_like = x.T\n                    y_like = y.T\n\n        if x_like.ndim == 1:\n            x_like, y_like = np.meshgrid(x_like, y_like)\n\n        return x_like, y_like, self.griddata[0], self.griddata[1]",
  "def draw(self):\n        \"\"\"Draw the plot.\"\"\"\n        if self._need_redraw:\n            if getattr(self, 'handle', None) is None:\n                self._build()\n            self._need_redraw = False",
  "def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of contour()\n        self.clear()",
  "def _build(self):\n        \"\"\"Build the plot by calling needed plotting methods as necessary.\"\"\"\n        x_like, y_like, u, v = self.plotdata\n\n        kwargs = plot_kwargs(u)\n\n        # Conditionally apply the proper transform\n        if 'transform' in kwargs and self.earth_relative:\n            kwargs['transform'] = ccrs.PlateCarree()\n\n        wind_slice = (slice(None, None, self.skip[0]), slice(None, None, self.skip[1]))\n\n        self.handle = self.parent.ax.barbs(\n            x_like[wind_slice], y_like[wind_slice],\n            u.values[wind_slice], v.values[wind_slice],\n            color=self.color, pivot=self.pivot, length=self.barblength, zorder=2, **kwargs)",
  "def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of quiver()\n        self.clear()",
  "def _build(self):\n        \"\"\"Build the plot by calling needed plotting methods as necessary.\"\"\"\n        x_like, y_like, u, v = self.plotdata\n\n        kwargs = plot_kwargs(u)\n\n        # Conditionally apply the proper transform\n        if 'transform' in kwargs and self.earth_relative:\n            kwargs['transform'] = ccrs.PlateCarree()\n\n        wind_slice = (slice(None, None, self.skip[0]), slice(None, None, self.skip[1]))\n\n        self.handle = self.parent.ax.quiver(\n            x_like[wind_slice], y_like[wind_slice],\n            u.values[wind_slice], v.values[wind_slice],\n            color=self.color, pivot=self.pivot, scale=self.arrowscale, **kwargs)\n\n        # The order here needs to match the order of the tuple\n        if self.arrowkey is not None:\n            key_kwargs = {'U': 100, 'X': 0.85, 'Y': 1.02, 'labelpos': 'E', 'label': ''}\n            for name, val in zip(key_kwargs, self.arrowkey):\n                if val is not None:\n                    key_kwargs[name] = val\n            self.parent.ax.quiverkey(self.handle, labelcolor=self.color, **key_kwargs)",
  "def clear(self):\n        \"\"\"Clear the plot.\n\n        Resets all internal state and sets need for redraw.\n\n        \"\"\"\n        if getattr(self, 'handle', None) is not None:\n            self.handle.ax.cla()\n            self.handle = None\n            self._need_redraw = True",
  "def _parent_changed(self, _):\n        \"\"\"Handle setting the parent object for the plot.\"\"\"\n        self.clear()",
  "def _update_data(self, _=None):\n        \"\"\"Handle updating the internal cache of data.\n\n        Responds to changes in various subsetting parameters.\n\n        \"\"\"\n        self._obsdata = None\n        self.clear()",
  "def data(self):\n        \"\"\"Pandas dataframe that contains the fields to be plotted.\"\"\"\n        return self._data",
  "def data(self, val):\n        self._data = val\n        self._update_data()",
  "def name(self):\n        \"\"\"Generate a name for the plot.\"\"\"\n        ret = ''\n        ret += ' and '.join(self.fields)\n        if self.level is not None:\n            ret += f'@{self.level:d}'\n        return ret",
  "def obsdata(self):\n        \"\"\"Return the internal cached data.\"\"\"\n        if getattr(self, '_obsdata', None) is None:\n            # Use a copy of data so we retain all of the original data passed in unmodified\n            data = self.data\n\n            # Subset for a particular level if given\n            if self.level is not None:\n                mag = getattr(self.level, 'magnitude', self.level)\n                data = data[data.pressure == mag]\n\n            # Subset for our particular time\n            if self.time is not None:\n                # If data are not currently indexed by time, we need to do so choosing one of\n                # the columns we're looking for\n                if not isinstance(data.index, pd.DatetimeIndex):\n                    time_vars = ['valid', 'time', 'valid_time', 'date_time', 'date']\n                    dim_times = [time_var for time_var in time_vars if\n                                 time_var in list(self.data)]\n                    if not dim_times:\n                        raise AttributeError(\n                            'Time variable not found. Valid variable names are:'\n                            f'{time_vars}')\n\n                    data = data.set_index(dim_times[0])\n                    if not isinstance(data.index, pd.DatetimeIndex):\n                        # Convert our column of interest to a datetime\n                        data = data.reset_index()\n                        time_index = pd.to_datetime(data[dim_times[0]])\n                        data = data.set_index(time_index)\n\n                # Works around the fact that traitlets 4.3 insists on sending us None by\n                # default because timedelta(0) is Falsey.\n                window = timedelta(minutes=0) if self.time_window is None else self.time_window\n\n                # Indexes need to be properly sorted for the slicing below to work; the\n                # error you get if that's not the case really convoluted, which is why\n                # we don't rely on users doing it.\n                data = data.sort_index()\n                data = data[self.time - window:self.time + window]\n\n            # Look for the station column\n            stn_vars = ['station', 'stn', 'station_id', 'stid']\n            dim_stns = [stn_var for stn_var in stn_vars if stn_var in list(self.data)]\n            if not dim_stns:\n                raise AttributeError('Station variable not found. Valid variable names are: '\n                                     f'{stn_vars}')\n            else:\n                dim_stn = dim_stns[0]\n\n            # Make sure we only use one observation per station\n            self._obsdata = data.groupby(dim_stn).tail(1)\n\n        return self._obsdata",
  "def plotdata(self):\n        \"\"\"Return the data for plotting.\n\n        The data arrays, x coordinates, and y coordinates.\n\n        \"\"\"\n        plot_data = {}\n        for dim_name in list(self.obsdata):\n            if dim_name.find('lat') != -1:\n                lat = self.obsdata[dim_name]\n            elif dim_name.find('lon') != -1:\n                lon = self.obsdata[dim_name]\n            else:\n                plot_data[dim_name] = self.obsdata[dim_name]\n        return lon.values, lat.values, plot_data",
  "def draw(self):\n        \"\"\"Draw the plot.\"\"\"\n        if self._need_redraw:\n            if getattr(self, 'handle', None) is None:\n                self._build()\n            self._need_redraw = False",
  "def _set_need_rebuild(self, _):\n        \"\"\"Handle changes to attributes that need to regenerate everything.\"\"\"\n        # Because matplotlib doesn't let you just change these properties, we need\n        # to trigger a clear and re-call of contour()\n        self.clear()",
  "def _build(self):\n        \"\"\"Build the plot by calling needed plotting methods as necessary.\"\"\"\n        lon, lat, data = self.plotdata\n\n        # Use the cartopy map projection to transform station locations to the map and\n        # then refine the number of stations plotted by setting a radius\n        scale = 1. if self.parent._proj_obj == ccrs.PlateCarree() else 100000.\n        point_locs = self.parent._proj_obj.transform_points(ccrs.PlateCarree(), lon, lat)\n        subset = reduce_point_density(point_locs, self.reduce_points * scale)\n\n        self.handle = StationPlot(self.parent.ax, lon[subset], lat[subset], clip_on=True,\n                                  transform=ccrs.PlateCarree(), fontsize=self.fontsize)\n\n        for i, ob_type in enumerate(self.fields):\n            field_kwargs = {}\n            location = self.locations[i] if len(self.locations) > 1 else self.locations[0]\n            if len(self.colors) > 1:\n                field_kwargs['color'] = self.colors[i]\n            else:\n                field_kwargs['color'] = self.colors[0]\n            if len(self.formats) > 1:\n                field_kwargs['formatter'] = self.formats[i]\n            else:\n                field_kwargs['formatter'] = self.formats[0]\n            if len(self.plot_units) > 1:\n                field_kwargs['plot_units'] = self.plot_units[i]\n            else:\n                field_kwargs['plot_units'] = self.plot_units[0]\n            if hasattr(self.data, 'units') and (field_kwargs['plot_units'] is not None):\n                parameter = units.Quantity(data[ob_type][subset].values,\n                                           self.data.units[ob_type])\n            else:\n                parameter = data[ob_type][subset]\n            if field_kwargs['formatter'] is not None:\n                mapper = getattr(wx_symbols, str(field_kwargs['formatter']), None)\n                if mapper is not None:\n                    field_kwargs.pop('formatter')\n                    self.handle.plot_symbol(location, parameter, mapper, **field_kwargs)\n                else:\n                    if self.formats[i] == 'text':\n                        self.handle.plot_text(location, parameter, color=field_kwargs['color'])\n                    else:\n                        self.handle.plot_parameter(location, parameter, **field_kwargs)\n            else:\n                field_kwargs.pop('formatter')\n                self.handle.plot_parameter(location, parameter, **field_kwargs)\n\n        if self.vector_field[0] is not None:\n            vector_kwargs = {\n                'color': self.vector_field_color,\n                'plot_units': self.vector_plot_units,\n            }\n\n            if hasattr(self.data, 'units') and (vector_kwargs['plot_units'] is not None):\n                u = units.Quantity(data[self.vector_field[0]][subset].values,\n                                   self.data.units[self.vector_field[0]])\n                v = units.Quantity(data[self.vector_field[1]][subset].values,\n                                   self.data.units[self.vector_field[1]])\n            else:\n                vector_kwargs.pop('plot_units')\n                u = data[self.vector_field[0]][subset]\n                v = data[self.vector_field[1]][subset]\n            if self.vector_field_length is not None:\n                vector_kwargs['length'] = self.vector_field_length\n            self.handle.plot_barb(u, v, **vector_kwargs)",
  "def copy(self):\n        \"\"\"Return a copy of the plot.\"\"\"\n        return copy.copy(self)",
  "def _valid_geometry(_, proposal):\n        \"\"\"Cast `geometry` into a list once it is provided by user.\n\n        Users can provide any kind of collection, such as a ``GeoPandas.GeoSeries``, and this\n        turns them into a list.\n        \"\"\"\n        geometry = proposal['value']\n        return list(geometry)",
  "def _valid_color_list(_, proposal):\n        \"\"\"Cast color-related attributes into a list once provided by user.\n\n        This is necessary because _build() expects to cycle through a list of colors when\n        assigning them to the geometry.\n        \"\"\"\n        color = proposal['value']\n\n        if isinstance(color, str):\n            color = [color]\n        # `color` must be a collection if it is not a string\n        else:\n            color = list(color)\n\n        return color",
  "def _valid_labels(_, proposal):\n        \"\"\"Cast `labels` into a list once provided by user.\"\"\"\n        labels = proposal['value']\n        return list(labels)",
  "def _update_label_colors(self, change):\n        \"\"\"Set default text colors using `fill` and `stroke`.\n\n        If `label_facecolor` or `label_edgecolor` have not been specified, provide default\n        colors for those attributes using `fill` and `stroke`.\n        \"\"\"\n        if change['name'] == 'fill' and self.label_edgecolor is None:\n            self.label_edgecolor = self.fill\n        elif change['name'] == 'stroke' and self.label_facecolor is None:\n            self.label_facecolor = self.stroke",
  "def name(self):\n        \"\"\"Generate a name for the plot.\"\"\"\n        # Unlike Plots2D and PlotObs, there are no other attributes (such as 'fields' or\n        # 'levels') from which to name the plot. A generic name is returned here in case the\n        # user does not provide their own title, in which case MapPanel.draw() looks here.\n        return 'Geometry Plot'",
  "def _position_label(geo_obj, label):\n        \"\"\"Return a (lon, lat) where the label of a polygon/line/point can be placed.\"\"\"\n        from shapely.geometry import MultiLineString, MultiPoint, MultiPolygon, Polygon\n\n        # A hash of the label is used in choosing a point along the polygon or line that\n        # will be returned. This \"psuedo-randomizes\" the position of a label, in hopes of\n        # spatially dispersing the labels and lessening the chance that labels overlap.\n        label_hash = sum(map(ord, str(label)))\n\n        # If object is a MultiPolygon or MultiLineString, associate the label with the single\n        # largest Polygon or LineString from the collection. If MultiPoint, associate the label\n        # with one of the Points in the MultiPoint, chosen based on the label hash.\n        if isinstance(geo_obj, (MultiPolygon, MultiLineString)):\n            geo_obj = max(geo_obj.geoms, key=lambda x: x.length)\n        elif isinstance(geo_obj, MultiPoint):\n            geo_obj = geo_obj.geoms[label_hash % len(geo_obj.geoms)]\n\n        # Get the list of coordinates of the polygon/line/point\n        if isinstance(geo_obj, Polygon):\n            coords = geo_obj.exterior.coords\n        else:\n            coords = geo_obj.coords\n\n        return coords[label_hash % len(coords)]",
  "def _draw_label(self, text, lon, lat, color='black', outline='white', offset=(0, 0)):\n        \"\"\"Draw a label to the plot.\n\n        Parameters\n        ----------\n        text : str\n            The label's text\n        lon : float\n            Longitude at which to position the label\n        lat : float\n            Latitude at which to position the label\n        color : str (default: 'black')\n            Name or hex code for the color of the text\n        outline : str (default: 'white')\n            Name or hex code of the color of the outline of the text\n        offset : tuple (default: (0, 0))\n            A tuple containing the x- and y-offset of the label, respectively\n        \"\"\"\n        path_effects = [patheffects.withStroke(linewidth=4, foreground=outline)]\n        self.parent.ax.add_artist(TextCollection([lon], [lat], [str(text)],\n                                                 va='center',\n                                                 ha='center',\n                                                 offset=offset,\n                                                 weight='demi',\n                                                 size=self.label_fontsize,\n                                                 color=color,\n                                                 path_effects=path_effects,\n                                                 transform=ccrs.PlateCarree()))",
  "def draw(self):\n        \"\"\"Draw the plot.\"\"\"\n        if self._need_redraw:\n            if getattr(self, 'handles', None) is None:\n                self._build()\n            self._need_redraw = False",
  "def copy(self):\n        \"\"\"Return a copy of the plot.\"\"\"\n        return copy.copy(self)",
  "def _build(self):\n        \"\"\"Build the plot by calling needed plotting methods as necessary.\"\"\"\n        from shapely.geometry import (LineString, MultiLineString, MultiPoint, MultiPolygon,\n                                      Point, Polygon)\n\n        # Cast attributes to a list if None, since traitlets doesn't call validators (like\n        # `_valid_color_list()` and `_valid_labels()`) when the proposed value is None.\n        self.fill = ['none'] if self.fill is None else self.fill\n        self.stroke = ['none'] if self.stroke is None else self.stroke\n        self.labels = [''] if self.labels is None else self.labels\n        self.label_edgecolor = (['none'] if self.label_edgecolor is None\n                                else self.label_edgecolor)\n        self.label_facecolor = (['none'] if self.label_facecolor is None\n                                else self.label_facecolor)\n\n        # Each Shapely object is plotted separately with its corresponding colors and label\n        for geo_obj, stroke, fill, label, fontcolor, fontoutline in zip(\n                self.geometry, cycle(self.stroke), cycle(self.fill), cycle(self.labels),\n                cycle(self.label_facecolor), cycle(self.label_edgecolor)):\n            # Plot the Shapely object with the appropriate method and colors\n            if isinstance(geo_obj, (MultiPolygon, Polygon)):\n                self.parent.ax.add_geometries([geo_obj], edgecolor=stroke,\n                                              facecolor=fill, crs=ccrs.PlateCarree())\n            elif isinstance(geo_obj, (MultiLineString, LineString)):\n                self.parent.ax.add_geometries([geo_obj], edgecolor=stroke,\n                                              facecolor='none', crs=ccrs.PlateCarree())\n            elif isinstance(geo_obj, MultiPoint):\n                for point in geo_obj.geoms:\n                    lon, lat = point.coords[0]\n                    self.parent.ax.plot(lon, lat, color=fill, marker=self.marker,\n                                        transform=ccrs.PlateCarree())\n            elif isinstance(geo_obj, Point):\n                lon, lat = geo_obj.coords[0]\n                self.parent.ax.plot(lon, lat, color=fill, marker=self.marker,\n                                    transform=ccrs.PlateCarree())\n\n            # Plot labels if provided\n            if label:\n                # If fontcolor is None/'none', choose a font color\n                if fontcolor in [None, 'none'] and stroke not in [None, 'none']:\n                    fontcolor = stroke\n                elif fontcolor in [None, 'none']:\n                    fontcolor = 'black'\n\n                # If fontoutline is None/'none', choose a font outline\n                if fontoutline in [None, 'none'] and fill not in [None, 'none']:\n                    fontoutline = fill\n                elif fontoutline in [None, 'none']:\n                    fontoutline = 'white'\n\n                # Choose a point along the polygon/line/point to place label\n                lon, lat = self._position_label(geo_obj, label)\n\n                # If polygon, put label directly on edge of polygon. If line or point, put\n                # label slightly below line/point.\n                if isinstance(geo_obj, (MultiPolygon, Polygon)):\n                    offset = (0, 0)\n                else:\n                    offset = (0, -12)\n\n                # Finally, draw the label\n                self._draw_label(label, lon, lat, fontcolor, fontoutline, offset)",
  "def wx_code_to_numeric(codes):\n    \"\"\"Determine the numeric weather symbol value from METAR code text.\n\n    A robust method to identifies the numeric value for plotting the correct symbol from a\n    decoded METAR current weather group. The METAR codes should be strings with no missing\n    values or NaN strings (empty strings are okay).\n\n    For example, if from a Pandas Dataframe ``sfc_df.wxcodes.fillna('')``.\n\n    Parameters\n    ----------\n    codes : Sequence[str]\n        String values of METAR weather codes\n\n    Returns\n    -------\n    `numpy.ndarray`\n        numeric codes of current weather symbols from the wx_code_map for use in plotting.\n    \"\"\"\n    wx_sym_list = []\n    for s in codes:\n        wxcode = s.split()[0] if ' ' in s else s\n        try:\n            wx_sym_list.append(wx_code_map[wxcode])\n        except KeyError:\n            if wxcode[0].startswith(('-', '+')):\n                options = [slice(None, 7), slice(None, 5), slice(1, 5), slice(None, 3),\n                           slice(1, 3)]\n            else:\n                options = [slice(None, 6), slice(None, 4), slice(None, 2)]\n\n            for opt in options:\n                try:\n                    wx_sym_list.append(wx_code_map[wxcode[opt]])\n                    break\n                except KeyError:\n                    # That option didn't work--move on.\n                    pass\n            else:\n                wx_sym_list.append(0)\n\n    return np.array(wx_sym_list)",
  "class CodePointMapping:\n    \"\"\"Map integer values to font code points.\"\"\"\n\n    def __init__(self, num, font_start, font_jumps=None, char_jumps=None):\n        \"\"\"Initialize the instance.\n\n        Parameters\n        ----------\n        num : int\n            The number of values that will be mapped\n        font_start : int\n            The first code point in the font to use in the mapping\n        font_jumps : list[int, int], optional\n            Sequence of code point jumps in the font. These are places where the next\n            font code point does not correspond to a new input code. This is usually caused\n            by there being multiple symbols for a single code. Defaults to :data:`None`, which\n            indicates no jumps.\n        char_jumps : list[int, int], optional\n            Sequence of code jumps. These are places where the next code value does not\n            have a valid code point in the font. This usually comes from place in the WMO\n            table where codes have no symbol. Defaults to :data:`None`, which indicates no\n            jumps.\n\n        \"\"\"\n        next_font_jump = self._safe_pop(font_jumps)\n        next_char_jump = self._safe_pop(char_jumps)\n        font_point = font_start\n        self.chrs = []\n        code = 0\n        while code < num:\n            if next_char_jump and code >= next_char_jump[0]:\n                jump_len = next_char_jump[1]\n                code += jump_len\n                self.chrs.extend([''] * jump_len)\n                next_char_jump = self._safe_pop(char_jumps)\n            else:\n                self.chrs.append(chr(font_point))\n                if next_font_jump and code >= next_font_jump[0]:\n                    font_point += next_font_jump[1]\n                    next_font_jump = self._safe_pop(font_jumps)\n                code += 1\n                font_point += 1\n\n    @staticmethod\n    def _safe_pop(lst):\n        \"\"\"Safely pop from a list.\n\n        Returns None if list empty.\n\n        \"\"\"\n        return lst.pop(0) if lst else None\n\n    def __call__(self, code):\n        \"\"\"Return the Unicode code point corresponding to `code`.\n\n        If code >= 1000, then an alternate code point is returned, with the thousands\n        digit indicating which alternate.\n        \"\"\"\n        if code < 1000:\n            return self.chrs[code]\n        else:\n            alt = code // 1000\n            code %= 1000\n            return self.alt_char(code, alt)\n\n    def __len__(self):\n        \"\"\"Return the number of codes supported by this mapping.\"\"\"\n        return len(self.chrs)\n\n    def alt_char(self, code, alt):\n        \"\"\"Get one of the alternate code points for a given value.\n\n        In the WMO tables, some code have multiple symbols. This allows getting that\n        symbol rather than main one.\n\n        Parameters\n        ----------\n        code : int\n            The code for looking up the font code point\n        alt : int\n            The number of the alternate symbol\n\n        Returns\n        -------\n        int\n            The appropriate code point in the font\n\n        \"\"\"\n        return chr(ord(self(code)) + alt)",
  "def __init__(self, num, font_start, font_jumps=None, char_jumps=None):\n        \"\"\"Initialize the instance.\n\n        Parameters\n        ----------\n        num : int\n            The number of values that will be mapped\n        font_start : int\n            The first code point in the font to use in the mapping\n        font_jumps : list[int, int], optional\n            Sequence of code point jumps in the font. These are places where the next\n            font code point does not correspond to a new input code. This is usually caused\n            by there being multiple symbols for a single code. Defaults to :data:`None`, which\n            indicates no jumps.\n        char_jumps : list[int, int], optional\n            Sequence of code jumps. These are places where the next code value does not\n            have a valid code point in the font. This usually comes from place in the WMO\n            table where codes have no symbol. Defaults to :data:`None`, which indicates no\n            jumps.\n\n        \"\"\"\n        next_font_jump = self._safe_pop(font_jumps)\n        next_char_jump = self._safe_pop(char_jumps)\n        font_point = font_start\n        self.chrs = []\n        code = 0\n        while code < num:\n            if next_char_jump and code >= next_char_jump[0]:\n                jump_len = next_char_jump[1]\n                code += jump_len\n                self.chrs.extend([''] * jump_len)\n                next_char_jump = self._safe_pop(char_jumps)\n            else:\n                self.chrs.append(chr(font_point))\n                if next_font_jump and code >= next_font_jump[0]:\n                    font_point += next_font_jump[1]\n                    next_font_jump = self._safe_pop(font_jumps)\n                code += 1\n                font_point += 1",
  "def _safe_pop(lst):\n        \"\"\"Safely pop from a list.\n\n        Returns None if list empty.\n\n        \"\"\"\n        return lst.pop(0) if lst else None",
  "def __call__(self, code):\n        \"\"\"Return the Unicode code point corresponding to `code`.\n\n        If code >= 1000, then an alternate code point is returned, with the thousands\n        digit indicating which alternate.\n        \"\"\"\n        if code < 1000:\n            return self.chrs[code]\n        else:\n            alt = code // 1000\n            code %= 1000\n            return self.alt_char(code, alt)",
  "def __len__(self):\n        \"\"\"Return the number of codes supported by this mapping.\"\"\"\n        return len(self.chrs)",
  "def alt_char(self, code, alt):\n        \"\"\"Get one of the alternate code points for a given value.\n\n        In the WMO tables, some code have multiple symbols. This allows getting that\n        symbol rather than main one.\n\n        Parameters\n        ----------\n        code : int\n            The code for looking up the font code point\n        alt : int\n            The number of the alternate symbol\n\n        Returns\n        -------\n        int\n            The appropriate code point in the font\n\n        \"\"\"\n        return chr(ord(self(code)) + alt)",
  "def _parse(s):\n    if hasattr(s, 'decode'):\n        s = s.decode('ascii')\n\n    if not s.startswith('#'):\n        return ast.literal_eval(s)\n\n    return None",
  "def read_colortable(fobj):\n    r\"\"\"Read colortable information from a file.\n\n    Reads a colortable, which consists of one color per line of the file, where\n    a color can be one of: a tuple of 3 floats, a string with a HTML color name,\n    or a string with a HTML hex color.\n\n    Parameters\n    ----------\n    fobj : file-like object\n        A file-like object to read the colors from\n\n    Returns\n    -------\n    List of tuples\n        A list of the RGB color values, where each RGB color is a tuple of 3 floats in the\n        range of [0, 1].\n\n    \"\"\"\n    ret = []\n    try:\n        for line in fobj:\n            literal = _parse(line)\n            if literal:\n                ret.append(mcolors.colorConverter.to_rgb(literal))\n        return ret\n    except (SyntaxError, ValueError) as e:\n        raise RuntimeError(f'Malformed colortable (bad line: {line})') from e",
  "def convert_gempak_table(infile, outfile):\n    r\"\"\"Convert a GEMPAK color table to one MetPy can read.\n\n    Reads lines from a GEMPAK-style color table file, and writes them to another file in\n    a format that MetPy can parse.\n\n    Parameters\n    ----------\n    infile : file-like object\n        The file-like object to read from\n    outfile : file-like object\n        The file-like object to write to\n\n    \"\"\"\n    for line in infile:\n        if not line.startswith('!') and line.strip():\n            r, g, b = map(int, line.split())\n            outfile.write(f'({r / 255:f}, {g / 255:f}, {b / 255:f})\\n')",
  "class ColortableRegistry(dict):\n    r\"\"\"Manages the collection of color tables.\n\n    Provides access to color tables, read collections of files, and generates\n    matplotlib's Normalize instances to go with the colortable.\n    \"\"\"\n\n    def scan_resource(self, pkg, path):\n        r\"\"\"Scan a resource directory for colortable files and add them to the registry.\n\n        Parameters\n        ----------\n        pkg : str\n            The package containing the resource directory\n        path : str\n            The path to the directory with the color tables\n\n        \"\"\"\n        import importlib.resources\n\n        for entry in (importlib.resources.files(pkg) / path).iterdir():\n            if entry.suffix == TABLE_EXT:\n                with entry.open() as stream:\n                    self.add_colortable(stream, entry.with_suffix('').name)\n\n    def scan_dir(self, path):\n        r\"\"\"Scan a directory on disk for color table files and add them to the registry.\n\n        Parameters\n        ----------\n        path : str\n            The path to the directory with the color tables\n\n        \"\"\"\n        for entry in Path(path).glob('*' + TABLE_EXT):\n            if entry.is_file():\n                with entry.open() as fobj:\n                    try:\n                        self.add_colortable(fobj, entry.with_suffix('').name)\n                        log.debug('Added colortable from file: %s', entry)\n                    except RuntimeError:\n                        # If we get a file we can't handle, assume we weren't meant to.\n                        log.info('Skipping unparsable file: %s', entry)\n\n    def add_colortable(self, fobj, name):\n        r\"\"\"Add a color table from a file to the registry.\n\n        Parameters\n        ----------\n        fobj : file-like object\n            The file to read the color table from\n        name : str\n            The name under which the color table will be stored\n\n        \"\"\"\n        self[name] = read_colortable(fobj)\n        self[name + '_r'] = self[name][::-1]\n\n    def get_with_steps(self, name, start, step):\n        r\"\"\"Get a color table from the registry with a corresponding norm.\n\n        Builds a `matplotlib.colors.BoundaryNorm` using `start`, `step`, and\n        the number of colors, based on the color table obtained from `name`.\n\n        Parameters\n        ----------\n        name : str\n            The name under which the color table will be stored\n        start : float\n            The starting boundary\n        step : float\n            The step between boundaries\n\n        Returns\n        -------\n        `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`\n            The boundary norm based on `start` and `step` with the number of colors\n            from the number of entries matching the color table, and the color table itself.\n\n        \"\"\"\n        from numpy import arange\n\n        # Need one more boundary than color\n        num_steps = len(self[name]) + 1\n        boundaries = arange(start, start + step * num_steps, step)\n        return self.get_with_boundaries(name, boundaries)\n\n    def get_with_range(self, name, start, end):\n        r\"\"\"Get a color table from the registry with a corresponding norm.\n\n        Builds a `matplotlib.colors.BoundaryNorm` using `start`, `end`, and\n        the number of colors, based on the color table obtained from `name`.\n\n        Parameters\n        ----------\n        name : str\n            The name under which the color table will be stored\n        start : float\n            The starting boundary\n        end : float\n            The ending boundary\n\n        Returns\n        -------\n        `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`\n            The boundary norm based on `start` and `end` with the number of colors\n            from the number of entries matching the color table, and the color table itself.\n\n        \"\"\"\n        from numpy import linspace\n\n        # Need one more boundary than color\n        num_steps = len(self[name]) + 1\n        boundaries = linspace(start, end, num_steps)\n        return self.get_with_boundaries(name, boundaries)\n\n    def get_with_boundaries(self, name, boundaries):\n        r\"\"\"Get a color table from the registry with a corresponding norm.\n\n        Builds a `matplotlib.colors.BoundaryNorm` using `boundaries`.\n\n        Parameters\n        ----------\n        name : str\n            The name under which the color table will be stored\n        boundaries : array-like\n            The list of boundaries for the norm\n\n        Returns\n        -------\n        `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`\n            The boundary norm based on `boundaries`, and the color table itself.\n\n        \"\"\"\n        cmap = self.get_colortable(name)\n        return mcolors.BoundaryNorm(boundaries, cmap.N), cmap\n\n    def get_colortable(self, name):\n        r\"\"\"Get a color table from the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name under which the color table will be stored\n\n        Returns\n        -------\n        `matplotlib.colors.ListedColormap`\n            The color table corresponding to `name`\n\n        \"\"\"\n        return mcolors.ListedColormap(self[name], name=name)",
  "def scan_resource(self, pkg, path):\n        r\"\"\"Scan a resource directory for colortable files and add them to the registry.\n\n        Parameters\n        ----------\n        pkg : str\n            The package containing the resource directory\n        path : str\n            The path to the directory with the color tables\n\n        \"\"\"\n        import importlib.resources\n\n        for entry in (importlib.resources.files(pkg) / path).iterdir():\n            if entry.suffix == TABLE_EXT:\n                with entry.open() as stream:\n                    self.add_colortable(stream, entry.with_suffix('').name)",
  "def scan_dir(self, path):\n        r\"\"\"Scan a directory on disk for color table files and add them to the registry.\n\n        Parameters\n        ----------\n        path : str\n            The path to the directory with the color tables\n\n        \"\"\"\n        for entry in Path(path).glob('*' + TABLE_EXT):\n            if entry.is_file():\n                with entry.open() as fobj:\n                    try:\n                        self.add_colortable(fobj, entry.with_suffix('').name)\n                        log.debug('Added colortable from file: %s', entry)\n                    except RuntimeError:\n                        # If we get a file we can't handle, assume we weren't meant to.\n                        log.info('Skipping unparsable file: %s', entry)",
  "def add_colortable(self, fobj, name):\n        r\"\"\"Add a color table from a file to the registry.\n\n        Parameters\n        ----------\n        fobj : file-like object\n            The file to read the color table from\n        name : str\n            The name under which the color table will be stored\n\n        \"\"\"\n        self[name] = read_colortable(fobj)\n        self[name + '_r'] = self[name][::-1]",
  "def get_with_steps(self, name, start, step):\n        r\"\"\"Get a color table from the registry with a corresponding norm.\n\n        Builds a `matplotlib.colors.BoundaryNorm` using `start`, `step`, and\n        the number of colors, based on the color table obtained from `name`.\n\n        Parameters\n        ----------\n        name : str\n            The name under which the color table will be stored\n        start : float\n            The starting boundary\n        step : float\n            The step between boundaries\n\n        Returns\n        -------\n        `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`\n            The boundary norm based on `start` and `step` with the number of colors\n            from the number of entries matching the color table, and the color table itself.\n\n        \"\"\"\n        from numpy import arange\n\n        # Need one more boundary than color\n        num_steps = len(self[name]) + 1\n        boundaries = arange(start, start + step * num_steps, step)\n        return self.get_with_boundaries(name, boundaries)",
  "def get_with_range(self, name, start, end):\n        r\"\"\"Get a color table from the registry with a corresponding norm.\n\n        Builds a `matplotlib.colors.BoundaryNorm` using `start`, `end`, and\n        the number of colors, based on the color table obtained from `name`.\n\n        Parameters\n        ----------\n        name : str\n            The name under which the color table will be stored\n        start : float\n            The starting boundary\n        end : float\n            The ending boundary\n\n        Returns\n        -------\n        `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`\n            The boundary norm based on `start` and `end` with the number of colors\n            from the number of entries matching the color table, and the color table itself.\n\n        \"\"\"\n        from numpy import linspace\n\n        # Need one more boundary than color\n        num_steps = len(self[name]) + 1\n        boundaries = linspace(start, end, num_steps)\n        return self.get_with_boundaries(name, boundaries)",
  "def get_with_boundaries(self, name, boundaries):\n        r\"\"\"Get a color table from the registry with a corresponding norm.\n\n        Builds a `matplotlib.colors.BoundaryNorm` using `boundaries`.\n\n        Parameters\n        ----------\n        name : str\n            The name under which the color table will be stored\n        boundaries : array-like\n            The list of boundaries for the norm\n\n        Returns\n        -------\n        `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`\n            The boundary norm based on `boundaries`, and the color table itself.\n\n        \"\"\"\n        cmap = self.get_colortable(name)\n        return mcolors.BoundaryNorm(boundaries, cmap.N), cmap",
  "def get_colortable(self, name):\n        r\"\"\"Get a color table from the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name under which the color table will be stored\n\n        Returns\n        -------\n        `matplotlib.colors.ListedColormap`\n            The color table corresponding to `name`\n\n        \"\"\"\n        return mcolors.ListedColormap(self[name], name=name)",
  "class CFProjection:\n    \"\"\"Handle parsing CF projection metadata.\"\"\"\n\n    # mapping from Cartopy to CF vocabulary\n    _default_attr_mapping = [('false_easting', 'false_easting'),\n                             ('false_northing', 'false_northing'),\n                             ('central_latitude', 'latitude_of_projection_origin'),\n                             ('central_longitude', 'longitude_of_projection_origin')]\n\n    projection_registry = Registry()\n\n    def __init__(self, attrs):\n        \"\"\"Initialize the CF Projection handler with a set of metadata attributes.\"\"\"\n        self._attrs = attrs\n\n    @classmethod\n    def register(cls, name):\n        \"\"\"Register a new projection to handle.\"\"\"\n        return cls.projection_registry.register(name)\n\n    @classmethod\n    def build_projection_kwargs(cls, source, mapping):\n        \"\"\"Handle mapping a dictionary of metadata to keyword arguments.\"\"\"\n        return cls._map_arg_names(source, cls._default_attr_mapping + mapping)\n\n    @staticmethod\n    def _map_arg_names(source, mapping):\n        \"\"\"Map one set of keys to another.\"\"\"\n        return {cartopy_name: source[cf_name] for cartopy_name, cf_name in mapping\n                if cf_name in source}\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the projection.\"\"\"\n        return self._attrs.get('grid_mapping_name', 'unknown')\n\n    @property\n    def cartopy_globe(self):\n        \"\"\"Initialize a `cartopy.crs.Globe` from the metadata.\"\"\"\n        if 'earth_radius' in self._attrs:\n            kwargs = {'ellipse': 'sphere', 'semimajor_axis': self._attrs['earth_radius'],\n                      'semiminor_axis': self._attrs['earth_radius']}\n\n        else:\n            attr_mapping = [('semimajor_axis', 'semi_major_axis'),\n                            ('semiminor_axis', 'semi_minor_axis'),\n                            ('inverse_flattening', 'inverse_flattening')]\n            kwargs = self._map_arg_names(self._attrs, attr_mapping)\n\n            # Override CartoPy's default ellipse setting depending on whether\n            # we have any metadata to map about the spheroid.\n            kwargs['ellipse'] = None if kwargs else 'sphere'\n\n        # interpret the 0 inverse_flattening as a spherical datum\n        # and don't pass the value on.\n        if kwargs.get('inverse_flattening') == 0:\n            kwargs['ellipse'] = 'sphere'\n            kwargs.pop('inverse_flattening', None)\n\n        return ccrs.Globe(**kwargs)\n\n    @property\n    def cartopy_geodetic(self):\n        \"\"\"Make a `cartopy.crs.Geodetic` instance from the appropriate `cartopy.crs.Globe`.\"\"\"\n        return ccrs.Geodetic(self.cartopy_globe)\n\n    def to_cartopy(self):\n        \"\"\"Convert to a CartoPy projection.\"\"\"\n        globe = self.cartopy_globe\n        try:\n            proj_handler = self.projection_registry[self.name]\n        except KeyError:\n            raise ValueError(f'Unhandled projection: {self.name}') from None\n\n        return proj_handler(self._attrs, globe)\n\n    def to_pyproj(self):\n        \"\"\"Convert to a PyProj CRS.\"\"\"\n        import pyproj\n\n        return pyproj.CRS.from_cf(self._attrs)\n\n    def to_dict(self):\n        \"\"\"Get the dictionary of metadata attributes.\"\"\"\n        return self._attrs.copy()\n\n    def __str__(self):\n        \"\"\"Get a string representation of the projection.\"\"\"\n        return f'Projection: {self.name}'\n\n    def __getitem__(self, item):\n        \"\"\"Return a given attribute.\"\"\"\n        return self._attrs[item]\n\n    def __eq__(self, other):\n        \"\"\"Test equality (CFProjection with matching attrs).\"\"\"\n        return self.__class__ == other.__class__ and self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Test inequality (not equal to).\"\"\"\n        return not self.__eq__(other)",
  "def make_geo(attrs_dict, globe):\n    \"\"\"Handle geostationary projection.\"\"\"\n    attr_mapping = [('satellite_height', 'perspective_point_height'),\n                    ('sweep_axis', 'sweep_angle_axis')]\n    kwargs = CFProjection.build_projection_kwargs(attrs_dict, attr_mapping)\n\n    # CartoPy can't handle central latitude for Geostationary (nor should it)\n    # Just remove it if it's 0.\n    if not kwargs.get('central_latitude'):\n        kwargs.pop('central_latitude', None)\n\n    # If sweep_angle_axis is not present, we should look for fixed_angle_axis and adjust\n    if 'sweep_axis' not in kwargs:\n        kwargs['sweep_axis'] = 'x' if attrs_dict['fixed_angle_axis'] == 'y' else 'y'\n\n    return ccrs.Geostationary(globe=globe, **kwargs)",
  "def make_lcc(attrs_dict, globe):\n    \"\"\"Handle Lambert conformal conic projection.\"\"\"\n    attr_mapping = [('central_longitude', 'longitude_of_central_meridian'),\n                    ('standard_parallels', 'standard_parallel')]\n    kwargs = CFProjection.build_projection_kwargs(attrs_dict, attr_mapping)\n    if 'standard_parallels' in kwargs:\n        try:\n            len(kwargs['standard_parallels'])\n        except TypeError:\n            kwargs['standard_parallels'] = [kwargs['standard_parallels']]\n    return ccrs.LambertConformal(globe=globe, **kwargs)",
  "def make_aea(attrs_dict, globe):\n    \"\"\"Handle Albers Equal Area.\"\"\"\n    attr_mapping = [('central_longitude', 'longitude_of_central_meridian'),\n                    ('standard_parallels', 'standard_parallel')]\n    kwargs = CFProjection.build_projection_kwargs(attrs_dict, attr_mapping)\n    if 'standard_parallels' in kwargs:\n        try:\n            len(kwargs['standard_parallels'])\n        except TypeError:\n            kwargs['standard_parallels'] = [kwargs['standard_parallels']]\n    return ccrs.AlbersEqualArea(globe=globe, **kwargs)",
  "def make_latlon(attrs_dict, globe):\n    \"\"\"Handle plain latitude/longitude mapping.\"\"\"\n    # TODO: Really need to use Geodetic to pass the proper globe\n    return ccrs.PlateCarree()",
  "def make_mercator(attrs_dict, globe):\n    \"\"\"Handle Mercator projection.\"\"\"\n    attr_mapping = [('latitude_true_scale', 'standard_parallel'),\n                    ('scale_factor', 'scale_factor_at_projection_origin')]\n    kwargs = CFProjection.build_projection_kwargs(attrs_dict, attr_mapping)\n\n    # Work around the fact that in CartoPy <= 0.16 can't handle the easting/northing\n    # or central_latitude in Mercator\n    if not kwargs.get('false_easting'):\n        kwargs.pop('false_easting', None)\n    if not kwargs.get('false_northing'):\n        kwargs.pop('false_northing', None)\n    if not kwargs.get('central_latitude'):\n        kwargs.pop('central_latitude', None)\n\n    return ccrs.Mercator(globe=globe, **kwargs)",
  "def make_stereo(attrs_dict, globe):\n    \"\"\"Handle generic stereographic projection.\"\"\"\n    attr_mapping = [('scale_factor', 'scale_factor_at_projection_origin')]\n    kwargs = CFProjection.build_projection_kwargs(attrs_dict, attr_mapping)\n\n    return ccrs.Stereographic(globe=globe, **kwargs)",
  "def make_polar_stereo(attrs_dict, globe):\n    \"\"\"Handle polar stereographic projection.\"\"\"\n    attr_mapping = [('central_longitude', 'straight_vertical_longitude_from_pole'),\n                    ('true_scale_latitude', 'standard_parallel'),\n                    ('scale_factor', 'scale_factor_at_projection_origin')]\n    kwargs = CFProjection.build_projection_kwargs(attrs_dict, attr_mapping)\n\n    return ccrs.Stereographic(globe=globe, **kwargs)",
  "def make_rotated_latlon(attrs_dict, globe):\n    \"\"\"Handle rotated latitude/longitude projection.\"\"\"\n    attr_mapping = [('pole_longitude', 'grid_north_pole_longitude'),\n                    ('pole_latitude', 'grid_north_pole_latitude'),\n                    ('central_rotated_longitude', 'north_pole_grid_longitude')]\n    kwargs = CFProjection.build_projection_kwargs(attrs_dict, attr_mapping)\n\n    return ccrs.RotatedPole(globe=globe, **kwargs)",
  "def __init__(self, attrs):\n        \"\"\"Initialize the CF Projection handler with a set of metadata attributes.\"\"\"\n        self._attrs = attrs",
  "def register(cls, name):\n        \"\"\"Register a new projection to handle.\"\"\"\n        return cls.projection_registry.register(name)",
  "def build_projection_kwargs(cls, source, mapping):\n        \"\"\"Handle mapping a dictionary of metadata to keyword arguments.\"\"\"\n        return cls._map_arg_names(source, cls._default_attr_mapping + mapping)",
  "def _map_arg_names(source, mapping):\n        \"\"\"Map one set of keys to another.\"\"\"\n        return {cartopy_name: source[cf_name] for cartopy_name, cf_name in mapping\n                if cf_name in source}",
  "def name(self):\n        \"\"\"Return the name of the projection.\"\"\"\n        return self._attrs.get('grid_mapping_name', 'unknown')",
  "def cartopy_globe(self):\n        \"\"\"Initialize a `cartopy.crs.Globe` from the metadata.\"\"\"\n        if 'earth_radius' in self._attrs:\n            kwargs = {'ellipse': 'sphere', 'semimajor_axis': self._attrs['earth_radius'],\n                      'semiminor_axis': self._attrs['earth_radius']}\n\n        else:\n            attr_mapping = [('semimajor_axis', 'semi_major_axis'),\n                            ('semiminor_axis', 'semi_minor_axis'),\n                            ('inverse_flattening', 'inverse_flattening')]\n            kwargs = self._map_arg_names(self._attrs, attr_mapping)\n\n            # Override CartoPy's default ellipse setting depending on whether\n            # we have any metadata to map about the spheroid.\n            kwargs['ellipse'] = None if kwargs else 'sphere'\n\n        # interpret the 0 inverse_flattening as a spherical datum\n        # and don't pass the value on.\n        if kwargs.get('inverse_flattening') == 0:\n            kwargs['ellipse'] = 'sphere'\n            kwargs.pop('inverse_flattening', None)\n\n        return ccrs.Globe(**kwargs)",
  "def cartopy_geodetic(self):\n        \"\"\"Make a `cartopy.crs.Geodetic` instance from the appropriate `cartopy.crs.Globe`.\"\"\"\n        return ccrs.Geodetic(self.cartopy_globe)",
  "def to_cartopy(self):\n        \"\"\"Convert to a CartoPy projection.\"\"\"\n        globe = self.cartopy_globe\n        try:\n            proj_handler = self.projection_registry[self.name]\n        except KeyError:\n            raise ValueError(f'Unhandled projection: {self.name}') from None\n\n        return proj_handler(self._attrs, globe)",
  "def to_pyproj(self):\n        \"\"\"Convert to a PyProj CRS.\"\"\"\n        import pyproj\n\n        return pyproj.CRS.from_cf(self._attrs)",
  "def to_dict(self):\n        \"\"\"Get the dictionary of metadata attributes.\"\"\"\n        return self._attrs.copy()",
  "def __str__(self):\n        \"\"\"Get a string representation of the projection.\"\"\"\n        return f'Projection: {self.name}'",
  "def __getitem__(self, item):\n        \"\"\"Return a given attribute.\"\"\"\n        return self._attrs[item]",
  "def __eq__(self, other):\n        \"\"\"Test equality (CFProjection with matching attrs).\"\"\"\n        return self.__class__ == other.__class__ and self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Test inequality (not equal to).\"\"\"\n        return not self.__eq__(other)",
  "class StationPlot:\n    \"\"\"Make a standard meteorological station plot.\n\n    Plots values, symbols, or text spaced around a central location. Can also plot wind\n    barbs as the center of the location.\n    \"\"\"\n\n    location_names = {'C': (0, 0), 'N': (0, 1), 'NE': (1, 1), 'E': (1, 0), 'SE': (1, -1),\n                      'S': (0, -1), 'SW': (-1, -1), 'W': (-1, 0), 'NW': (-1, 1),\n                      'N2': (0, 2), 'NNE': (1, 2), 'ENE': (2, 1), 'E2': (2, 0),\n                      'ESE': (2, -1), 'SSE': (1, -2), 'S2': (0, -2), 'SSW': (-1, -2),\n                      'WSW': (-2, -1), 'W2': (-2, 0), 'WNW': (-2, 1), 'NNW': (-1, 2)}\n\n    def __init__(self, ax, x, y, fontsize=10, spacing=None, transform=None, **kwargs):\n        \"\"\"Initialize the StationPlot with items that do not change.\n\n        This sets up the axes and station locations. The `fontsize` and `spacing`\n        are also specified here to ensure that they are consistent between individual\n        station elements.\n\n        Parameters\n        ----------\n        ax : matplotlib.axes.Axes\n            The :class:`~matplotlib.axes.Axes` for plotting\n        x : array-like\n            The x location of the stations in the plot\n        y : array-like\n            The y location of the stations in the plot\n        fontsize : int\n            The fontsize to use for drawing text\n        spacing : int\n            The spacing, in points, that corresponds to a single increment between\n            station plot elements.\n        transform : matplotlib.transforms.Transform\n            The default transform to apply to the x and y positions when plotting. Works\n            with anything compatible with the ``Transform`` interface.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n            These will be passed to all the plotting methods, and thus need to be valid\n            for all plot types, such as `clip_on`.\n\n        \"\"\"\n        self.ax = ax\n        self.x = np.atleast_1d(x)\n        self.y = np.atleast_1d(y)\n        self.fontsize = fontsize\n        self.spacing = fontsize if spacing is None else spacing\n        self.transform = transform\n        self.items = {}\n        self.barbs = None\n        self.arrows = None\n        self.default_kwargs = kwargs\n\n    def plot_symbol(self, location, codes, symbol_mapper, **kwargs):\n        \"\"\"At the specified location in the station model plot a set of symbols.\n\n        This specifies that at the offset `location`, the data in `codes` should be\n        converted to unicode characters (for our\n        :data:`!metpy.plots.wx_symbols.wx_symbol_font`) using `symbol_mapper`, and plotted.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        If something has already been plotted at this location, it will be replaced.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this parameter. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions; increments\n            are multiplied by `spacing` to give offsets in x and y relative to the center.\n        codes : array-like\n            The numeric values that should be converted to unicode characters for plotting.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value\n            and return a single unicode\n            character. See :mod:`!metpy.plots.wx_symbols` for included mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n\n        .. plot::\n\n           import matplotlib.pyplot as plt\n           import numpy as np\n           from math import ceil\n\n           from metpy.plots import StationPlot\n           from metpy.plots.wx_symbols import current_weather, current_weather_auto\n           from metpy.plots.wx_symbols import low_clouds, mid_clouds, high_clouds\n           from metpy.plots.wx_symbols import sky_cover, pressure_tendency\n\n\n           def plot_symbols(mapper, name, nwrap=10, figsize=(10, 1.4)):\n\n               # Determine how many symbols there are and layout in rows of nwrap\n               # if there are more than nwrap symbols\n               num_symbols = len(mapper)\n               codes = np.arange(len(mapper))\n               ncols = nwrap\n               if num_symbols <= nwrap:\n                   nrows = 1\n                   x = np.linspace(0, 1, len(mapper))\n                   y = np.ones_like(x)\n                   ax_height = 0.8\n               else:\n                   nrows = int(ceil(num_symbols / ncols))\n                   x = np.tile(np.linspace(0, 1, ncols), nrows)[:num_symbols]\n                   y = np.repeat(np.arange(nrows, 0, -1), ncols)[:num_symbols]\n                   figsize = (10, 1 * nrows + 0.4)\n                   ax_height = 0.8 + 0.018 * nrows\n\n               fig = plt.figure(figsize=figsize,  dpi=300)\n               ax = fig.add_axes([0, 0, 1, ax_height])\n               ax.set_title(name, size=20)\n               ax.xaxis.set_ticks([])\n               ax.yaxis.set_ticks([])\n               ax.set_frame_on(False)\n\n               # Plot\n               sp = StationPlot(ax, x, y, fontsize=36)\n               sp.plot_symbol('C', codes, mapper)\n               sp.plot_parameter((0, -1), codes, fontsize=18)\n\n               ax.set_ylim(-0.05, nrows + 0.5)\n\n               plt.show()\n\n\n           plot_symbols(current_weather, \"Current Weather Symbols\")\n           plot_symbols(current_weather_auto, \"Current Weather Auto Reported Symbols\")\n           plot_symbols(low_clouds, \"Low Cloud Symbols\")\n           plot_symbols(mid_clouds, \"Mid Cloud Symbols\")\n           plot_symbols(high_clouds, \"High Cloud Symbols\")\n           plot_symbols(sky_cover, \"Sky Cover Symbols\", nwrap=12)\n           plot_symbols(pressure_tendency, \"Pressure Tendency Symbols\")\n\n        See Also\n        --------\n        plot_barb, plot_parameter, plot_text\n\n        \"\"\"\n        # Make sure we use our font for symbols\n        kwargs['fontproperties'] = wx_symbol_font.copy()\n        return self.plot_parameter(location, codes, symbol_mapper, **kwargs)\n\n    def plot_parameter(self, location, parameter, formatter='.0f', **kwargs):\n        \"\"\"At the specified location in the station model plot a set of values.\n\n        This specifies that at the offset `location`, the data in `parameter` should be\n        plotted. The conversion of the data values to a string is controlled by ``formatter``.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        If something has already been plotted at this location, it will be replaced.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this parameter. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions; increments\n            are multiplied by `spacing` to give offsets in x and y relative to the center.\n        parameter : array-like\n            The numeric values that should be plotted\n        formatter : str or Callable, optional\n            How to format the data as a string for plotting. If a string, it should be\n            compatible with the :func:`format` builtin. If a callable, this should take a\n            value and return a string. Defaults to '0.f'.\n        plot_units: `pint.unit`\n            Units to plot in (performing conversion if necessary). Defaults to given units.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n\n        See Also\n        --------\n        plot_barb, plot_symbol, plot_text\n\n        \"\"\"\n        # If plot_units specified, convert the data to those units\n        plotting_units = kwargs.pop('plot_units', None)\n        parameter = self._scalar_plotting_units(parameter, plotting_units)\n        if hasattr(parameter, 'units'):\n            parameter = parameter.magnitude\n        text = self._to_string_list(parameter, formatter)\n        return self.plot_text(location, text, **kwargs)\n\n    def plot_text(self, location, text, **kwargs):\n        \"\"\"At the specified location in the station model plot a collection of text.\n\n        This specifies that at the offset `location`, the strings in `text` should be\n        plotted.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        If something has already been plotted at this location, it will be replaced.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this parameter. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions; increments\n            are multiplied by `spacing` to give offsets in x and y relative to the center.\n        text : Sequence[str]\n            The strings that should be plotted\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        plot_barb, plot_parameter, plot_symbol\n\n        \"\"\"\n        location = self._handle_location(location)\n\n        kwargs = self._make_kwargs(kwargs)\n        text_collection = self.ax.scattertext(self.x, self.y, text, loc=location,\n                                              size=kwargs.pop('fontsize', self.fontsize),\n                                              **kwargs)\n        if location in self.items:\n            self.items[location].remove()\n        self.items[location] = text_collection\n        return text_collection\n\n    def plot_barb(self, u, v, **kwargs):\n        r\"\"\"At the center of the station model plot wind barbs.\n\n        Additional keyword arguments given will be passed onto matplotlib's\n        :meth:`~matplotlib.axes.Axes.barbs` function; this is useful for specifying things\n        like color or line width.\n\n        Parameters\n        ----------\n        u : array-like\n            The data to use for the u-component of the barbs.\n        v : array-like\n            The data to use for the v-component of the barbs.\n        plot_units: `pint.Unit`\n            Units to plot in (performing conversion if necessary). Defaults to given units.\n        kwargs\n            Additional keyword arguments to pass to matplotlib's\n            :meth:`~matplotlib.axes.Axes.barbs` function.\n\n        See Also\n        --------\n        plot_arrow, plot_parameter, plot_symbol, plot_text\n\n        \"\"\"\n        kwargs = self._make_kwargs(kwargs)\n\n        # If plot_units specified, convert the data to those units\n        plotting_units = kwargs.pop('plot_units', None)\n        u, v = self._vector_plotting_units(u, v, plotting_units)\n\n        # Empirically determined\n        pivot = 0.51 * np.sqrt(self.fontsize)\n        length = 1.95 * np.sqrt(self.fontsize)\n        defaults = {'sizes': {'spacing': .15, 'height': 0.5, 'emptybarb': 0.35},\n                    'length': length, 'pivot': pivot}\n        defaults.update(kwargs)\n\n        # Remove old barbs\n        if self.barbs:\n            self.barbs.remove()\n\n        self.barbs = self.ax.barbs(self.x, self.y, u, v, **defaults)\n\n    def plot_arrow(self, u, v, **kwargs):\n        r\"\"\"At the center of the station model plot wind arrows.\n\n        Additional keyword arguments given will be passed onto matplotlib's\n        :meth:`~matplotlib.axes.Axes.quiver` function; this is useful for specifying things\n        like color or line width.\n\n        Parameters\n        ----------\n        u : array-like\n            The data to use for the u-component of the arrows.\n        v : array-like\n            The data to use for the v-component of the arrows.\n        plot_units: `pint.unit`\n            Units to plot in (performing conversion if necessary). Defaults to given units.\n        kwargs\n            Additional keyword arguments to pass to matplotlib's\n            :meth:`~matplotlib.axes.Axes.barbs` function.\n\n        See Also\n        --------\n        plot_barb, plot_parameter, plot_symbol, plot_text\n\n        \"\"\"\n        kwargs = self._make_kwargs(kwargs)\n\n        # If plot_units specified, convert the data to those units\n        plotting_units = kwargs.pop('plot_units', None)\n        u, v = self._vector_plotting_units(u, v, plotting_units)\n\n        defaults = {'pivot': 'tail', 'scale': 20, 'scale_units': 'inches', 'width': 0.002}\n        defaults.update(kwargs)\n\n        # Remove old arrows\n        if self.arrows:\n            self.arrows.remove()\n\n        self.arrows = self.ax.quiver(self.x, self.y, u, v, **defaults)\n\n    @staticmethod\n    def _vector_plotting_units(u, v, plotting_units):\n        \"\"\"Handle conversion to plotting units for barbs and arrows.\"\"\"\n        if plotting_units:\n            if hasattr(u, 'units') and hasattr(v, 'units'):\n                u = u.to(plotting_units)\n                v = v.to(plotting_units)\n            else:\n                raise ValueError('To convert to plotting units, units must be attached to '\n                                 'u and v wind components.')\n\n        # Strip units, CartoPy transform doesn't like\n        u = np.array(u)\n        v = np.array(v)\n        return u, v\n\n    @staticmethod\n    def _scalar_plotting_units(scalar_value, plotting_units):\n        \"\"\"Handle conversion to plotting units for non-vector quantities.\"\"\"\n        if plotting_units:\n            if hasattr(scalar_value, 'units'):\n                scalar_value = scalar_value.to(plotting_units)\n            else:\n                raise ValueError('To convert to plotting units, units must be attached to '\n                                 'scalar value being converted.')\n        return scalar_value\n\n    def _make_kwargs(self, kwargs):\n        \"\"\"Assemble kwargs as necessary.\n\n        Inserts our defaults as well as ensures transform is present when appropriate.\n        \"\"\"\n        # Use default kwargs and update with additional ones\n        all_kw = self.default_kwargs.copy()\n        all_kw.update(kwargs)\n\n        # Pass transform if necessary\n        if 'transform' not in all_kw and self.transform:\n            all_kw['transform'] = self.transform\n\n        return all_kw\n\n    @staticmethod\n    def _to_string_list(vals, fmt):\n        \"\"\"Convert a sequence of values to a list of strings.\"\"\"\n        if not callable(fmt):\n            def formatter(s):\n                \"\"\"Turn a format string into a callable.\"\"\"\n                return format(s, fmt)\n        else:\n            formatter = fmt\n\n        return [formatter(v) if np.isfinite(v) else '' for v in vals]\n\n    def _handle_location(self, location):\n        \"\"\"Process locations to get a consistent set of tuples for location.\"\"\"\n        if isinstance(location, str):\n            location = self.location_names[location]\n        xoff, yoff = location\n        return xoff * self.spacing, yoff * self.spacing",
  "class StationPlotLayout(dict):\n    r\"\"\"Make a layout to encapsulate plotting using `StationPlot`.\n\n    This class keeps a collection of offsets, plot formats, etc. for a parameter based\n    on its name. This then allows a dictionary of data (or any object that allows looking\n    up of arrays based on a name) to be passed to `plot()` to plot the data all at once.\n\n    See Also\n    --------\n    StationPlot\n\n    \"\"\"\n\n    class PlotTypes(Enum):\n        r\"\"\"Different plotting types for the layout.\n\n        Controls how items are displayed (e.g. converting values to symbols).\n        \"\"\"\n\n        value = 1\n        symbol = 2\n        text = 3\n        barb = 4\n\n    def add_value(self, location, name, fmt='.0f', units=None, **kwargs):\n        r\"\"\"Add a numeric value to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. The conversion of the data values to\n        a string is controlled by `fmt`. The units required for plotting can also\n        be passed in using `units`, which will cause the data to be converted before\n        plotting.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        fmt : str or Callable, optional\n            How to format the data as a string for plotting. If a string, it should be\n            compatible with the :func:`format` builtin. If a callable, this should take a\n            value and return a string. Defaults to '0.f'.\n        units : pint.Unit, optional\n            The units to use for plotting. Data will be converted to this unit before\n            conversion to a string. If not specified, no conversion is done.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_symbol, add_text\n\n        \"\"\"\n        self[location] = (self.PlotTypes.value, name, (fmt, units, kwargs))\n\n    def add_symbol(self, location, name, symbol_mapper, **kwargs):\n        r\"\"\"Add a symbol to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. Data values will be converted to glyphs\n        appropriate for MetPy's symbol font using the callable `symbol_mapper`.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value and\n            return a single unicode character. See :mod:`!metpy.plots.wx_symbols` for included\n            mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_text, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.symbol, name, (symbol_mapper, kwargs))\n\n    def add_text(self, location, name, **kwargs):\n        r\"\"\"Add a text field to the  station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted directly as text with no conversion\n        applied.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple(float, float)\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_symbol, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.text, name, kwargs)\n\n    def add_barb(self, u_name, v_name, units=None, **kwargs):\n        r\"\"\"Add a wind barb to the center of the station layout.\n\n        This specifies that u- and v-component data should be pulled from the data\n        container using the keys `u_name` and `v_name`, respectively, and plotted as\n        a wind barb at the center of the station plot. If `units` are given, both\n        components will be converted to these units.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or line width.\n\n        Parameters\n        ----------\n        u_name : str\n            The name of the parameter for the u-component for `barbs`, which is used as\n            a key to pull data out of the data container passed to :meth:`plot`.\n        v_name : str\n            The name of the parameter for the v-component for `barbs`, which is used as\n            a key to pull data out of the data container passed to :meth:`plot`.\n        units : pint.Unit, optional\n            The units to use for plotting. Data will be converted to this unit before\n            conversion to a string. If not specified, no conversion is done.\n        kwargs\n            Additional keyword arguments to use for matplotlib's\n            :meth:`~matplotlib.axes.Axes.barbs` function.\n\n        See Also\n        --------\n        add_symbol, add_text, add_value\n\n        \"\"\"\n        # Not sure if putting the v_name as a plot-specific option is appropriate,\n        # but it seems simpler than making name code in plot handle tuples\n        self['barb'] = (self.PlotTypes.barb, (u_name, v_name), (units, kwargs))\n\n    def names(self):\n        \"\"\"Get the list of names used by the layout.\n\n        Returns\n        -------\n        list[str]\n            the list of names of variables used by the layout\n\n        \"\"\"\n        ret = []\n        for item in self.values():\n            if item[0] == self.PlotTypes.barb:\n                ret.extend(item[1])\n            else:\n                ret.append(item[1])\n        return ret\n\n    def plot(self, plotter, data_dict):\n        \"\"\"Plot a collection of data using this layout for a station plot.\n\n        This function iterates through the entire specified layout, pulling the fields named\n        in the layout from `data_dict` and plotting them using `plotter` as specified\n        in the layout. Fields present in the layout, but not in `data_dict`, are ignored.\n\n        Parameters\n        ----------\n        plotter : StationPlot\n            :class:`StationPlot` to use to plot the data. This controls the axes,\n            spacing, station locations, etc.\n        data_dict : dict[str, array-like]\n            Data container that maps a name to an array of data. Data from this object\n            will be used to fill out the station plot.\n\n        \"\"\"\n        def coerce_data(dat, u):\n            try:\n                return dat.to(u).magnitude\n            except AttributeError:\n                return dat\n\n        for loc, info in self.items():\n            typ, name, args = info\n            if typ == self.PlotTypes.barb:\n                # Try getting the data\n                u_name, v_name = name\n                u_data = data_dict.get(u_name)\n                v_data = data_dict.get(v_name)\n\n                # Plot if we have the data\n                if not (v_data is None or u_data is None):\n                    units, kwargs = args\n                    plotter.plot_barb(coerce_data(u_data, units), coerce_data(v_data, units),\n                                      **kwargs)\n            else:\n                # Check that we have the data for this location\n                data = data_dict.get(name)\n                if data is not None:\n                    # If we have it, hand it to the appropriate method\n                    if typ == self.PlotTypes.value:\n                        fmt, units, kwargs = args\n                        plotter.plot_parameter(loc, coerce_data(data, units), fmt, **kwargs)\n                    elif typ == self.PlotTypes.symbol:\n                        mapper, kwargs = args\n                        plotter.plot_symbol(loc, data, mapper, **kwargs)\n                    elif typ == self.PlotTypes.text:\n                        plotter.plot_text(loc, data, **args)\n\n    def __repr__(self):\n        \"\"\"Return string representation of layout.\"\"\"\n        return ('{'\n                + ', '.join(f'{loc}: ({info[0].name}, {info[1]}, ...)'\n                            for loc, info in sorted(self.items()))\n                + '}')",
  "def __init__(self, ax, x, y, fontsize=10, spacing=None, transform=None, **kwargs):\n        \"\"\"Initialize the StationPlot with items that do not change.\n\n        This sets up the axes and station locations. The `fontsize` and `spacing`\n        are also specified here to ensure that they are consistent between individual\n        station elements.\n\n        Parameters\n        ----------\n        ax : matplotlib.axes.Axes\n            The :class:`~matplotlib.axes.Axes` for plotting\n        x : array-like\n            The x location of the stations in the plot\n        y : array-like\n            The y location of the stations in the plot\n        fontsize : int\n            The fontsize to use for drawing text\n        spacing : int\n            The spacing, in points, that corresponds to a single increment between\n            station plot elements.\n        transform : matplotlib.transforms.Transform\n            The default transform to apply to the x and y positions when plotting. Works\n            with anything compatible with the ``Transform`` interface.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n            These will be passed to all the plotting methods, and thus need to be valid\n            for all plot types, such as `clip_on`.\n\n        \"\"\"\n        self.ax = ax\n        self.x = np.atleast_1d(x)\n        self.y = np.atleast_1d(y)\n        self.fontsize = fontsize\n        self.spacing = fontsize if spacing is None else spacing\n        self.transform = transform\n        self.items = {}\n        self.barbs = None\n        self.arrows = None\n        self.default_kwargs = kwargs",
  "def plot_symbol(self, location, codes, symbol_mapper, **kwargs):\n        \"\"\"At the specified location in the station model plot a set of symbols.\n\n        This specifies that at the offset `location`, the data in `codes` should be\n        converted to unicode characters (for our\n        :data:`!metpy.plots.wx_symbols.wx_symbol_font`) using `symbol_mapper`, and plotted.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        If something has already been plotted at this location, it will be replaced.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this parameter. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions; increments\n            are multiplied by `spacing` to give offsets in x and y relative to the center.\n        codes : array-like\n            The numeric values that should be converted to unicode characters for plotting.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value\n            and return a single unicode\n            character. See :mod:`!metpy.plots.wx_symbols` for included mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n\n        .. plot::\n\n           import matplotlib.pyplot as plt\n           import numpy as np\n           from math import ceil\n\n           from metpy.plots import StationPlot\n           from metpy.plots.wx_symbols import current_weather, current_weather_auto\n           from metpy.plots.wx_symbols import low_clouds, mid_clouds, high_clouds\n           from metpy.plots.wx_symbols import sky_cover, pressure_tendency\n\n\n           def plot_symbols(mapper, name, nwrap=10, figsize=(10, 1.4)):\n\n               # Determine how many symbols there are and layout in rows of nwrap\n               # if there are more than nwrap symbols\n               num_symbols = len(mapper)\n               codes = np.arange(len(mapper))\n               ncols = nwrap\n               if num_symbols <= nwrap:\n                   nrows = 1\n                   x = np.linspace(0, 1, len(mapper))\n                   y = np.ones_like(x)\n                   ax_height = 0.8\n               else:\n                   nrows = int(ceil(num_symbols / ncols))\n                   x = np.tile(np.linspace(0, 1, ncols), nrows)[:num_symbols]\n                   y = np.repeat(np.arange(nrows, 0, -1), ncols)[:num_symbols]\n                   figsize = (10, 1 * nrows + 0.4)\n                   ax_height = 0.8 + 0.018 * nrows\n\n               fig = plt.figure(figsize=figsize,  dpi=300)\n               ax = fig.add_axes([0, 0, 1, ax_height])\n               ax.set_title(name, size=20)\n               ax.xaxis.set_ticks([])\n               ax.yaxis.set_ticks([])\n               ax.set_frame_on(False)\n\n               # Plot\n               sp = StationPlot(ax, x, y, fontsize=36)\n               sp.plot_symbol('C', codes, mapper)\n               sp.plot_parameter((0, -1), codes, fontsize=18)\n\n               ax.set_ylim(-0.05, nrows + 0.5)\n\n               plt.show()\n\n\n           plot_symbols(current_weather, \"Current Weather Symbols\")\n           plot_symbols(current_weather_auto, \"Current Weather Auto Reported Symbols\")\n           plot_symbols(low_clouds, \"Low Cloud Symbols\")\n           plot_symbols(mid_clouds, \"Mid Cloud Symbols\")\n           plot_symbols(high_clouds, \"High Cloud Symbols\")\n           plot_symbols(sky_cover, \"Sky Cover Symbols\", nwrap=12)\n           plot_symbols(pressure_tendency, \"Pressure Tendency Symbols\")\n\n        See Also\n        --------\n        plot_barb, plot_parameter, plot_text\n\n        \"\"\"\n        # Make sure we use our font for symbols\n        kwargs['fontproperties'] = wx_symbol_font.copy()\n        return self.plot_parameter(location, codes, symbol_mapper, **kwargs)",
  "def plot_parameter(self, location, parameter, formatter='.0f', **kwargs):\n        \"\"\"At the specified location in the station model plot a set of values.\n\n        This specifies that at the offset `location`, the data in `parameter` should be\n        plotted. The conversion of the data values to a string is controlled by ``formatter``.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        If something has already been plotted at this location, it will be replaced.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this parameter. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions; increments\n            are multiplied by `spacing` to give offsets in x and y relative to the center.\n        parameter : array-like\n            The numeric values that should be plotted\n        formatter : str or Callable, optional\n            How to format the data as a string for plotting. If a string, it should be\n            compatible with the :func:`format` builtin. If a callable, this should take a\n            value and return a string. Defaults to '0.f'.\n        plot_units: `pint.unit`\n            Units to plot in (performing conversion if necessary). Defaults to given units.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n\n        See Also\n        --------\n        plot_barb, plot_symbol, plot_text\n\n        \"\"\"\n        # If plot_units specified, convert the data to those units\n        plotting_units = kwargs.pop('plot_units', None)\n        parameter = self._scalar_plotting_units(parameter, plotting_units)\n        if hasattr(parameter, 'units'):\n            parameter = parameter.magnitude\n        text = self._to_string_list(parameter, formatter)\n        return self.plot_text(location, text, **kwargs)",
  "def plot_text(self, location, text, **kwargs):\n        \"\"\"At the specified location in the station model plot a collection of text.\n\n        This specifies that at the offset `location`, the strings in `text` should be\n        plotted.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        If something has already been plotted at this location, it will be replaced.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this parameter. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions; increments\n            are multiplied by `spacing` to give offsets in x and y relative to the center.\n        text : Sequence[str]\n            The strings that should be plotted\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        plot_barb, plot_parameter, plot_symbol\n\n        \"\"\"\n        location = self._handle_location(location)\n\n        kwargs = self._make_kwargs(kwargs)\n        text_collection = self.ax.scattertext(self.x, self.y, text, loc=location,\n                                              size=kwargs.pop('fontsize', self.fontsize),\n                                              **kwargs)\n        if location in self.items:\n            self.items[location].remove()\n        self.items[location] = text_collection\n        return text_collection",
  "def plot_barb(self, u, v, **kwargs):\n        r\"\"\"At the center of the station model plot wind barbs.\n\n        Additional keyword arguments given will be passed onto matplotlib's\n        :meth:`~matplotlib.axes.Axes.barbs` function; this is useful for specifying things\n        like color or line width.\n\n        Parameters\n        ----------\n        u : array-like\n            The data to use for the u-component of the barbs.\n        v : array-like\n            The data to use for the v-component of the barbs.\n        plot_units: `pint.Unit`\n            Units to plot in (performing conversion if necessary). Defaults to given units.\n        kwargs\n            Additional keyword arguments to pass to matplotlib's\n            :meth:`~matplotlib.axes.Axes.barbs` function.\n\n        See Also\n        --------\n        plot_arrow, plot_parameter, plot_symbol, plot_text\n\n        \"\"\"\n        kwargs = self._make_kwargs(kwargs)\n\n        # If plot_units specified, convert the data to those units\n        plotting_units = kwargs.pop('plot_units', None)\n        u, v = self._vector_plotting_units(u, v, plotting_units)\n\n        # Empirically determined\n        pivot = 0.51 * np.sqrt(self.fontsize)\n        length = 1.95 * np.sqrt(self.fontsize)\n        defaults = {'sizes': {'spacing': .15, 'height': 0.5, 'emptybarb': 0.35},\n                    'length': length, 'pivot': pivot}\n        defaults.update(kwargs)\n\n        # Remove old barbs\n        if self.barbs:\n            self.barbs.remove()\n\n        self.barbs = self.ax.barbs(self.x, self.y, u, v, **defaults)",
  "def plot_arrow(self, u, v, **kwargs):\n        r\"\"\"At the center of the station model plot wind arrows.\n\n        Additional keyword arguments given will be passed onto matplotlib's\n        :meth:`~matplotlib.axes.Axes.quiver` function; this is useful for specifying things\n        like color or line width.\n\n        Parameters\n        ----------\n        u : array-like\n            The data to use for the u-component of the arrows.\n        v : array-like\n            The data to use for the v-component of the arrows.\n        plot_units: `pint.unit`\n            Units to plot in (performing conversion if necessary). Defaults to given units.\n        kwargs\n            Additional keyword arguments to pass to matplotlib's\n            :meth:`~matplotlib.axes.Axes.barbs` function.\n\n        See Also\n        --------\n        plot_barb, plot_parameter, plot_symbol, plot_text\n\n        \"\"\"\n        kwargs = self._make_kwargs(kwargs)\n\n        # If plot_units specified, convert the data to those units\n        plotting_units = kwargs.pop('plot_units', None)\n        u, v = self._vector_plotting_units(u, v, plotting_units)\n\n        defaults = {'pivot': 'tail', 'scale': 20, 'scale_units': 'inches', 'width': 0.002}\n        defaults.update(kwargs)\n\n        # Remove old arrows\n        if self.arrows:\n            self.arrows.remove()\n\n        self.arrows = self.ax.quiver(self.x, self.y, u, v, **defaults)",
  "def _vector_plotting_units(u, v, plotting_units):\n        \"\"\"Handle conversion to plotting units for barbs and arrows.\"\"\"\n        if plotting_units:\n            if hasattr(u, 'units') and hasattr(v, 'units'):\n                u = u.to(plotting_units)\n                v = v.to(plotting_units)\n            else:\n                raise ValueError('To convert to plotting units, units must be attached to '\n                                 'u and v wind components.')\n\n        # Strip units, CartoPy transform doesn't like\n        u = np.array(u)\n        v = np.array(v)\n        return u, v",
  "def _scalar_plotting_units(scalar_value, plotting_units):\n        \"\"\"Handle conversion to plotting units for non-vector quantities.\"\"\"\n        if plotting_units:\n            if hasattr(scalar_value, 'units'):\n                scalar_value = scalar_value.to(plotting_units)\n            else:\n                raise ValueError('To convert to plotting units, units must be attached to '\n                                 'scalar value being converted.')\n        return scalar_value",
  "def _make_kwargs(self, kwargs):\n        \"\"\"Assemble kwargs as necessary.\n\n        Inserts our defaults as well as ensures transform is present when appropriate.\n        \"\"\"\n        # Use default kwargs and update with additional ones\n        all_kw = self.default_kwargs.copy()\n        all_kw.update(kwargs)\n\n        # Pass transform if necessary\n        if 'transform' not in all_kw and self.transform:\n            all_kw['transform'] = self.transform\n\n        return all_kw",
  "def _to_string_list(vals, fmt):\n        \"\"\"Convert a sequence of values to a list of strings.\"\"\"\n        if not callable(fmt):\n            def formatter(s):\n                \"\"\"Turn a format string into a callable.\"\"\"\n                return format(s, fmt)\n        else:\n            formatter = fmt\n\n        return [formatter(v) if np.isfinite(v) else '' for v in vals]",
  "def _handle_location(self, location):\n        \"\"\"Process locations to get a consistent set of tuples for location.\"\"\"\n        if isinstance(location, str):\n            location = self.location_names[location]\n        xoff, yoff = location\n        return xoff * self.spacing, yoff * self.spacing",
  "class PlotTypes(Enum):\n        r\"\"\"Different plotting types for the layout.\n\n        Controls how items are displayed (e.g. converting values to symbols).\n        \"\"\"\n\n        value = 1\n        symbol = 2\n        text = 3\n        barb = 4",
  "def add_value(self, location, name, fmt='.0f', units=None, **kwargs):\n        r\"\"\"Add a numeric value to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. The conversion of the data values to\n        a string is controlled by `fmt`. The units required for plotting can also\n        be passed in using `units`, which will cause the data to be converted before\n        plotting.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        fmt : str or Callable, optional\n            How to format the data as a string for plotting. If a string, it should be\n            compatible with the :func:`format` builtin. If a callable, this should take a\n            value and return a string. Defaults to '0.f'.\n        units : pint.Unit, optional\n            The units to use for plotting. Data will be converted to this unit before\n            conversion to a string. If not specified, no conversion is done.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_symbol, add_text\n\n        \"\"\"\n        self[location] = (self.PlotTypes.value, name, (fmt, units, kwargs))",
  "def add_symbol(self, location, name, symbol_mapper, **kwargs):\n        r\"\"\"Add a symbol to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. Data values will be converted to glyphs\n        appropriate for MetPy's symbol font using the callable `symbol_mapper`.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value and\n            return a single unicode character. See :mod:`!metpy.plots.wx_symbols` for included\n            mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_text, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.symbol, name, (symbol_mapper, kwargs))",
  "def add_text(self, location, name, **kwargs):\n        r\"\"\"Add a text field to the  station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted directly as text with no conversion\n        applied.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple(float, float)\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_symbol, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.text, name, kwargs)",
  "def add_barb(self, u_name, v_name, units=None, **kwargs):\n        r\"\"\"Add a wind barb to the center of the station layout.\n\n        This specifies that u- and v-component data should be pulled from the data\n        container using the keys `u_name` and `v_name`, respectively, and plotted as\n        a wind barb at the center of the station plot. If `units` are given, both\n        components will be converted to these units.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or line width.\n\n        Parameters\n        ----------\n        u_name : str\n            The name of the parameter for the u-component for `barbs`, which is used as\n            a key to pull data out of the data container passed to :meth:`plot`.\n        v_name : str\n            The name of the parameter for the v-component for `barbs`, which is used as\n            a key to pull data out of the data container passed to :meth:`plot`.\n        units : pint.Unit, optional\n            The units to use for plotting. Data will be converted to this unit before\n            conversion to a string. If not specified, no conversion is done.\n        kwargs\n            Additional keyword arguments to use for matplotlib's\n            :meth:`~matplotlib.axes.Axes.barbs` function.\n\n        See Also\n        --------\n        add_symbol, add_text, add_value\n\n        \"\"\"\n        # Not sure if putting the v_name as a plot-specific option is appropriate,\n        # but it seems simpler than making name code in plot handle tuples\n        self['barb'] = (self.PlotTypes.barb, (u_name, v_name), (units, kwargs))",
  "def names(self):\n        \"\"\"Get the list of names used by the layout.\n\n        Returns\n        -------\n        list[str]\n            the list of names of variables used by the layout\n\n        \"\"\"\n        ret = []\n        for item in self.values():\n            if item[0] == self.PlotTypes.barb:\n                ret.extend(item[1])\n            else:\n                ret.append(item[1])\n        return ret",
  "def plot(self, plotter, data_dict):\n        \"\"\"Plot a collection of data using this layout for a station plot.\n\n        This function iterates through the entire specified layout, pulling the fields named\n        in the layout from `data_dict` and plotting them using `plotter` as specified\n        in the layout. Fields present in the layout, but not in `data_dict`, are ignored.\n\n        Parameters\n        ----------\n        plotter : StationPlot\n            :class:`StationPlot` to use to plot the data. This controls the axes,\n            spacing, station locations, etc.\n        data_dict : dict[str, array-like]\n            Data container that maps a name to an array of data. Data from this object\n            will be used to fill out the station plot.\n\n        \"\"\"\n        def coerce_data(dat, u):\n            try:\n                return dat.to(u).magnitude\n            except AttributeError:\n                return dat\n\n        for loc, info in self.items():\n            typ, name, args = info\n            if typ == self.PlotTypes.barb:\n                # Try getting the data\n                u_name, v_name = name\n                u_data = data_dict.get(u_name)\n                v_data = data_dict.get(v_name)\n\n                # Plot if we have the data\n                if not (v_data is None or u_data is None):\n                    units, kwargs = args\n                    plotter.plot_barb(coerce_data(u_data, units), coerce_data(v_data, units),\n                                      **kwargs)\n            else:\n                # Check that we have the data for this location\n                data = data_dict.get(name)\n                if data is not None:\n                    # If we have it, hand it to the appropriate method\n                    if typ == self.PlotTypes.value:\n                        fmt, units, kwargs = args\n                        plotter.plot_parameter(loc, coerce_data(data, units), fmt, **kwargs)\n                    elif typ == self.PlotTypes.symbol:\n                        mapper, kwargs = args\n                        plotter.plot_symbol(loc, data, mapper, **kwargs)\n                    elif typ == self.PlotTypes.text:\n                        plotter.plot_text(loc, data, **args)",
  "def __repr__(self):\n        \"\"\"Return string representation of layout.\"\"\"\n        return ('{'\n                + ', '.join(f'{loc}: ({info[0].name}, {info[1]}, ...)'\n                            for loc, info in sorted(self.items()))\n                + '}')",
  "def coerce_data(dat, u):\n            try:\n                return dat.to(u).magnitude\n            except AttributeError:\n                return dat",
  "def formatter(s):\n                \"\"\"Turn a format string into a callable.\"\"\"\n                return format(s, fmt)",
  "def __getattr__(name):\n    \"\"\"Raise a proper error if something needing Cartopy is not available.\"\"\"\n    for mod in [cartopy_utils, plot_areas]:\n        if name in mod.__all__:\n            try:\n                return getattr(mod, name)\n            except AttributeError:\n                raise AttributeError(f'Cannot use {name} without Cartopy installed.') from None\n\n    raise AttributeError(f'module {__name__!r} has no attribute {name!r}')",
  "def __dir__():\n    return __all__ + cartopy_utils.__all__ + plot_areas.__all__",
  "def add_timestamp(ax, time=None, x=0.99, y=-0.04, ha='right', high_contrast=False,\n                  pretext='Created: ', time_format='%Y-%m-%dT%H:%M:%SZ', **kwargs):\n    \"\"\"Add a timestamp to a plot.\n\n    Adds a timestamp to a plot, defaulting to the time of plot creation in ISO format.\n\n    Parameters\n    ----------\n    ax : `matplotlib.axes.Axes`\n        The `Axes` instance used for plotting\n    time : `datetime.datetime` (or any object with a compatible ``strftime`` method)\n        Specific time to be plotted - datetime.utcnow will be use if not specified\n    x : float\n        Relative x position on the axes of the timestamp\n    y : float\n        Relative y position on the axes of the timestamp\n    ha : str\n        Horizontal alignment of the time stamp string\n    high_contrast : bool\n        Outline text for increased contrast\n    pretext : str\n        Text to appear before the timestamp, optional. Defaults to 'Created: '\n    time_format : str\n        Display format of time, optional. Defaults to ISO format.\n\n    Returns\n    -------\n    `matplotlib.text.Text`\n        The `matplotlib.text.Text` instance created\n\n    \"\"\"\n    if high_contrast:\n        text_args = {'color': 'white',\n                     'path_effects':\n                         [mpatheffects.withStroke(linewidth=2, foreground='black')]}\n    else:\n        text_args = {}\n    text_args.update(**kwargs)\n    if not time:\n        time = datetime.utcnow()\n    timestr = time.strftime(time_format)\n    # If we don't have a time string after that, assume xarray/numpy and see if item\n    if not isinstance(timestr, str):\n        timestr = timestr.item()\n    return ax.text(x, y, pretext + timestr, ha=ha, transform=ax.transAxes, **text_args)",
  "def _add_logo(fig, x=10, y=25, zorder=100, which='metpy', size='small', **kwargs):\n    \"\"\"Add the MetPy or Unidata logo to a figure.\n\n    Adds an image to the figure.\n\n    Parameters\n    ----------\n    fig : `matplotlib.figure`\n       The `figure` instance used for plotting\n    x : int\n       x position padding in pixels\n    y : float\n       y position padding in pixels\n    zorder : int\n       The zorder of the logo\n    which : str\n       Which logo to plot 'metpy' or 'unidata'\n    size : str\n       Size of logo to be used. Can be 'small' for 75 px square or 'large' for\n       150 px square.\n\n    Returns\n    -------\n    `matplotlib.image.FigureImage`\n       The `matplotlib.image.FigureImage` instance created\n\n    \"\"\"\n    import importlib.resources\n\n    fname_suffix = {'small': '_75x75.png',\n                    'large': '_150x150.png'}\n    fname_prefix = {'unidata': 'unidata',\n                    'metpy': 'metpy'}\n    try:\n        fname = fname_prefix[which] + fname_suffix[size]\n    except KeyError:\n        raise ValueError('Unknown logo size or selection') from None\n\n    with (importlib.resources.files('metpy.plots') / '_static' / fname).open('rb') as fobj:\n        logo = imread(fobj)\n    return fig.figimage(logo, x, y, zorder=zorder, **kwargs)",
  "def add_metpy_logo(fig, x=10, y=25, zorder=100, size='small', **kwargs):\n    \"\"\"Add the MetPy logo to a figure.\n\n    Adds an image of the MetPy logo to the figure.\n\n    Parameters\n    ----------\n    fig : `matplotlib.figure`\n       The `figure` instance used for plotting\n    x : int\n       x position padding in pixels\n    y : float\n       y position padding in pixels\n    zorder : int\n       The zorder of the logo\n    size : str\n       Size of logo to be used. Can be 'small' for 75 px square or 'large' for\n       150 px square.\n\n    Returns\n    -------\n    `matplotlib.image.FigureImage`\n       The `matplotlib.image.FigureImage` instance created\n\n    \"\"\"\n    return _add_logo(fig, x=x, y=y, zorder=zorder, which='metpy', size=size, **kwargs)",
  "def add_unidata_logo(fig, x=10, y=25, zorder=100, size='small', **kwargs):\n    \"\"\"Add the Unidata logo to a figure.\n\n    Adds an image of the MetPy logo to the figure.\n\n    Parameters\n    ----------\n    fig : `matplotlib.figure`\n       The `figure` instance used for plotting\n    x : int\n       x position padding in pixels\n    y : float\n       y position padding in pixels\n    zorder : int\n       The zorder of the logo\n    size : str\n       Size of logo to be used. Can be 'small' for 75 px square or 'large' for\n       150 px square.\n\n    Returns\n    -------\n    `matplotlib.image.FigureImage`\n       The `matplotlib.image.FigureImage` instance created\n\n    \"\"\"\n    return _add_logo(fig, x=x, y=y, zorder=zorder, which='unidata', size=size, **kwargs)",
  "def colored_line(x, y, c, **kwargs):\n    \"\"\"Create a multi-colored line.\n\n    Takes a set of points and turns them into a collection of lines colored by another array.\n\n    Parameters\n    ----------\n    x : array-like\n        x-axis coordinates\n    y : array-like\n        y-axis coordinates\n    c : array-like\n        values used for color-mapping\n    kwargs : dict\n        Other keyword arguments passed to :class:`matplotlib.collections.LineCollection`\n\n    Returns\n    -------\n        The created :class:`matplotlib.collections.LineCollection` instance.\n\n    \"\"\"\n    # Mask out any NaN values\n    nan_mask = ~(np.isnan(x) | np.isnan(y) | np.isnan(c))\n    x = x[nan_mask]\n    y = y[nan_mask]\n    c = c[nan_mask]\n\n    # Paste values end to end\n    points = concatenate([x, y])\n\n    # Exploit numpy's strides to present a view of these points without copying.\n    # Dimensions are (segment, start/end, x/y). Since x and y are concatenated back to back,\n    # moving between segments only moves one item; moving start to end is only an item;\n    # The move between x any moves from one half of the array to the other\n    num_pts = points.size // 2\n    final_shape = (num_pts - 1, 2, 2)\n    final_strides = (points.itemsize, points.itemsize, num_pts * points.itemsize)\n    segments = np.lib.stride_tricks.as_strided(points.m, shape=final_shape,\n                                               strides=final_strides)\n\n    # Create a LineCollection from the segments and set it to colormap based on c\n    lc = LineCollection(segments, **kwargs)\n    lc.set_array(getattr(c, 'magnitude', c))\n    return lc",
  "def convert_gempak_color(c, style='psc'):\n    \"\"\"Convert GEMPAK color numbers into corresponding Matplotlib colors.\n\n    Takes a sequence of GEMPAK color numbers and turns them into\n    equivalent Matplotlib colors. Various GEMPAK quirks are respected,\n    such as treating negative values as equivalent to 0.\n\n    Parameters\n    ----------\n    c : int or Sequence[int]\n        GEMPAK color number(s)\n    style : str, optional\n        The GEMPAK 'device' to use to interpret color numbers. May be 'psc'\n        (the default; best for a white background) or 'xw' (best for a black background).\n\n    Returns\n    -------\n        List of strings of Matplotlib colors, or a single string if only one color requested.\n\n    \"\"\"\n    def normalize(x):\n        \"\"\"Transform input x to an int in range 0 to 31 consistent with GEMPAK color quirks.\"\"\"\n        x = int(x)\n        if x < 0 or x == 101:\n            x = 0\n        else:\n            x %= 32\n        return x\n\n    # Define GEMPAK colors (Matplotlib doesn't appear to like numbered variants)\n    cols = ['white',       # 0/32\n            'black',       # 1\n            'red',         # 2\n            'green',       # 3\n            'blue',        # 4\n            'yellow',      # 5\n            'cyan',        # 6\n            'magenta',     # 7\n            '#CD6839',     # 8 (sienna3)\n            '#FF8247',     # 9 (sienna1)\n            '#FFA54F',     # 10 (tan1)\n            '#FFAEB9',     # 11 (LightPink1)\n            '#FF6A6A',     # 12 (IndianRed1)\n            '#EE2C2C',     # 13 (firebrick2)\n            '#8B0000',     # 14 (red4)\n            '#CD0000',     # 15 (red3)\n            '#EE4000',     # 16 (OrangeRed2)\n            '#FF7F00',     # 17 (DarkOrange1)\n            '#CD8500',     # 18 (orange3)\n            'gold',        # 19\n            '#EEEE00',     # 20 (yellow2)\n            'chartreuse',  # 21\n            '#00CD00',     # 22 (green3)\n            '#008B00',     # 23 (green4)\n            '#104E8B',     # 24 (DodgerBlue4)\n            'DodgerBlue',  # 25\n            '#00B2EE',     # 26 (DeepSkyBlue2)\n            '#00EEEE',     # 27 (cyan2)\n            '#8968CD',     # 28 (MediumPurple3)\n            '#912CEE',     # 29 (purple2)\n            '#8B008B',     # 30 (magenta4)\n            'bisque']      # 31\n\n    if style != 'psc':\n        if style == 'xw':\n            cols[0] = 'black'\n            cols[1] = 'bisque'\n            cols[31] = 'white'\n        else:\n            raise ValueError('Unknown style parameter')\n\n    try:\n        c_list = list(c)\n        res = [cols[normalize(x)] for x in c_list]\n    except TypeError:\n        res = cols[normalize(c)]\n    return res",
  "def normalize(x):\n        \"\"\"Transform input x to an int in range 0 to 31 consistent with GEMPAK color quirks.\"\"\"\n        x = int(x)\n        if x < 0 or x == 101:\n            x = 0\n        else:\n            x %= 32\n        return x",
  "def import_cartopy():\n    \"\"\"Import CartoPy; return a stub if unable.\n\n    This allows code requiring CartoPy to fail at use time rather than import time.\n    \"\"\"\n    try:\n        import cartopy.crs as ccrs\n        return ccrs\n    except ImportError:\n        return CartopyStub()",
  "class CartopyStub:\n    \"\"\"Fail if a CartoPy attribute is accessed.\"\"\"\n\n    def __getattr__(self, name):\n        \"\"\"Raise an error on any attribute access.\"\"\"\n        raise AttributeError(f'Cannot use {name} without Cartopy installed.')",
  "class MetPyMapFeature(Feature):\n        \"\"\"A simple interface to MetPy-included shapefiles.\"\"\"\n\n        def __init__(self, name, scale, **kwargs):\n            \"\"\"Create MetPyMapFeature instance.\"\"\"\n            import cartopy.crs as ccrs\n            super().__init__(ccrs.PlateCarree(), **kwargs)\n            self.name = name\n\n            if isinstance(scale, str):\n                scale = Scaler(scale)\n            self.scaler = scale\n\n        def geometries(self):\n            \"\"\"Return an iterator of (shapely) geometries for this feature.\"\"\"\n            import cartopy.io.shapereader as shapereader\n\n            # Ensure that the associated files are in the cache\n            fname = f'{self.name}_{self.scaler.scale}'\n            for extension in ['.dbf', '.shx']:\n                get_test_data(fname + extension, as_file_obj=False)\n            path = get_test_data(fname + '.shp', as_file_obj=False)\n            return iter(tuple(shapereader.Reader(path).geometries()))\n\n        def intersecting_geometries(self, extent):\n            \"\"\"Return geometries that intersect the extent.\"\"\"\n            self.scaler.scale_from_extent(extent)\n            return super().intersecting_geometries(extent)\n\n        def with_scale(self, new_scale):\n            \"\"\"\n            Return a copy of the feature with a new scale.\n\n            Parameters\n            ----------\n            new_scale\n                The new dataset scale, i.e. one of '500k', '5m', or '20m'.\n                Corresponding to 1:500,000, 1:5,000,000, and 1:20,000,000\n                respectively.\n\n            \"\"\"\n            return MetPyMapFeature(self.name, new_scale, **self.kwargs)",
  "def __getattr__(self, name):\n        \"\"\"Raise an error on any attribute access.\"\"\"\n        raise AttributeError(f'Cannot use {name} without Cartopy installed.')",
  "def __init__(self, name, scale, **kwargs):\n            \"\"\"Create MetPyMapFeature instance.\"\"\"\n            import cartopy.crs as ccrs\n            super().__init__(ccrs.PlateCarree(), **kwargs)\n            self.name = name\n\n            if isinstance(scale, str):\n                scale = Scaler(scale)\n            self.scaler = scale",
  "def geometries(self):\n            \"\"\"Return an iterator of (shapely) geometries for this feature.\"\"\"\n            import cartopy.io.shapereader as shapereader\n\n            # Ensure that the associated files are in the cache\n            fname = f'{self.name}_{self.scaler.scale}'\n            for extension in ['.dbf', '.shx']:\n                get_test_data(fname + extension, as_file_obj=False)\n            path = get_test_data(fname + '.shp', as_file_obj=False)\n            return iter(tuple(shapereader.Reader(path).geometries()))",
  "def intersecting_geometries(self, extent):\n            \"\"\"Return geometries that intersect the extent.\"\"\"\n            self.scaler.scale_from_extent(extent)\n            return super().intersecting_geometries(extent)",
  "def with_scale(self, new_scale):\n            \"\"\"\n            Return a copy of the feature with a new scale.\n\n            Parameters\n            ----------\n            new_scale\n                The new dataset scale, i.e. one of '500k', '5m', or '20m'.\n                Corresponding to 1:500,000, 1:5,000,000, and 1:20,000,000\n                respectively.\n\n            \"\"\"\n            return MetPyMapFeature(self.name, new_scale, **self.kwargs)",
  "def interpolate_to_slice(data, points, interp_type='linear'):\n    r\"\"\"Obtain an interpolated slice through data using xarray.\n\n    Utilizing the interpolation functionality in xarray, this function\n    takes a slice of the given data (currently only regular grids are supported), which is\n    given as an `xarray.DataArray` so that we can utilize its coordinate metadata.\n\n    Parameters\n    ----------\n    data: `xarray.DataArray` or `xarray.Dataset`\n        Three- (or higher) dimensional field(s) to interpolate. The DataArray (or each\n        DataArray in the Dataset) must have been parsed by MetPy and include both an x and\n        y coordinate dimension.\n    points: (N, 2) array-like\n        A list of x, y points in the data projection at which to interpolate the data\n    interp_type: str, optional\n        The interpolation method, either 'linear' or 'nearest' (see\n        `xarray.DataArray.interp()` for details). Defaults to 'linear'.\n\n    Returns\n    -------\n    `xarray.DataArray` or `xarray.Dataset`\n        The interpolated slice of data, with new index dimension of size N.\n\n    See Also\n    --------\n    cross_section\n\n    \"\"\"\n    try:\n        x, y = data.metpy.coordinates('x', 'y')\n    except AttributeError:\n        raise ValueError('Required coordinate information not available. Verify that '\n                         'your data has been parsed by MetPy with proper x and y '\n                         'dimension coordinates.') from None\n\n    data_sliced = data.interp({\n        x.name: xr.DataArray(points[:, 0], dims='index', attrs=x.attrs),\n        y.name: xr.DataArray(points[:, 1], dims='index', attrs=y.attrs)\n    }, method=interp_type)\n    data_sliced.coords['index'] = range(len(points))\n\n    # Bug in xarray: interp strips units\n    if is_quantity(data.data) and not is_quantity(data_sliced.data):\n        data_sliced.data = units.Quantity(data_sliced.data, data.data.units)\n\n    return data_sliced",
  "def geodesic(crs, start, end, steps):\n    r\"\"\"Construct a geodesic path between two points.\n\n    This function acts as a wrapper for the geodesic construction available in ``pyproj``.\n\n    Parameters\n    ----------\n    crs: `pyproj.crs.CRS`\n        PyProj Coordinate Reference System to use for the output\n    start: (2, ) array-like\n        A latitude-longitude pair designating the start point of the geodesic (units are\n        degrees north and degrees east).\n    end: (2, ) array-like\n        A latitude-longitude pair designating the end point of the geodesic (units are degrees\n        north and degrees east).\n    steps: int, optional\n        The number of points along the geodesic between the start and the end point\n        (including the end points).\n\n    Returns\n    -------\n    `numpy.ndarray`\n        The list of x, y points in the given CRS of length `steps` along the geodesic.\n\n    See Also\n    --------\n    cross_section\n\n    \"\"\"\n    from pyproj import Proj\n\n    g = crs.get_geod()\n    p = Proj(crs)\n\n    # Geod.npts only gives points *in between* the start and end, and we want to include\n    # the endpoints.\n    geodesic = np.concatenate([\n        np.array(start[::-1])[None],\n        np.array(g.npts(start[1], start[0], end[1], end[0], steps - 2)),\n        np.array(end[::-1])[None]\n    ]).transpose()\n    return np.stack(p(geodesic[0], geodesic[1], inverse=False, radians=False), axis=-1)",
  "def cross_section(data, start, end, steps=100, interp_type='linear'):\n    r\"\"\"Obtain an interpolated cross-sectional slice through gridded data.\n\n    Utilizing the interpolation functionality in xarray, this function takes a vertical\n    cross-sectional slice along a geodesic through the given data on a regular grid, which is\n    given as an `xarray.DataArray` so that we can utilize its coordinate and projection\n    metadata.\n\n    Parameters\n    ----------\n    data: `xarray.DataArray` or `xarray.Dataset`\n        Three- (or higher) dimensional field(s) to interpolate. The DataArray (or each\n        DataArray in the Dataset) must have been parsed by MetPy and include both an x and\n        y coordinate dimension and the added `crs` coordinate.\n    start: (2, ) array-like\n        A latitude-longitude pair designating the start point of the cross section (units are\n        degrees north and degrees east).\n    end: (2, ) array-like\n        A latitude-longitude pair designating the end point of the cross section (units are\n        degrees north and degrees east).\n    steps: int, optional\n        The number of points along the geodesic between the start and the end point\n        (including the end points) to use in the cross section. Defaults to 100.\n    interp_type: str, optional\n        The interpolation method, either 'linear' or 'nearest' (see\n        `xarray.DataArray.interp()` for details). Defaults to 'linear'.\n\n    Returns\n    -------\n    `xarray.DataArray` or `xarray.Dataset`\n        The interpolated cross section, with new index dimension along the cross-section.\n\n    See Also\n    --------\n    interpolate_to_slice, geodesic\n\n    \"\"\"\n    if isinstance(data, xr.Dataset):\n        # Recursively apply to dataset\n        return data.map(cross_section, True, (start, end), steps=steps,\n                        interp_type=interp_type)\n    elif data.ndim == 0:\n        # This has no dimensions, so it is likely a projection variable. In any case, there\n        # are no data here to take the cross section with. Therefore, do nothing.\n        return data\n    else:\n\n        # Get the projection and coordinates\n        try:\n            crs_data = data.metpy.pyproj_crs\n            x = data.metpy.x\n        except AttributeError:\n            raise ValueError('Data missing required coordinate information. Verify that '\n                             'your data have been parsed by MetPy with proper x and y '\n                             'dimension coordinates and added crs coordinate of the '\n                             'correct projection for each variable.') from None\n\n        # Get the geodesic\n        points_cross = geodesic(crs_data, start, end, steps)\n\n        # Patch points_cross to match given longitude range, whether [0, 360) or (-180,  180]\n        if check_axis(x, 'longitude') and (x > 180).any():\n            points_cross[points_cross[:, 0] < 0, 0] += 360.\n\n        # Return the interpolated data\n        return interpolate_to_slice(data, points_cross, interp_type=interp_type)",
  "def interpolate_nans_1d(x, y, kind='linear'):\n    \"\"\"Interpolate NaN values in y.\n\n    Interpolate NaN values in the y dimension. Works with unsorted x values.\n\n    Parameters\n    ----------\n    x : array-like\n        1-dimensional array of numeric x-values\n    y : array-like\n        1-dimensional array of numeric y-values\n    kind : str\n        specifies the kind of interpolation x coordinate - 'linear' or 'log', optional.\n        Defaults to 'linear'.\n\n    Returns\n    -------\n        An array of the y coordinate data with NaN values interpolated.\n\n    \"\"\"\n    x_sort_args = np.argsort(x)\n    x = x[x_sort_args]\n    y = y[x_sort_args]\n    nans = np.isnan(y)\n    if kind == 'linear':\n        y[nans] = np.interp(x[nans], x[~nans], y[~nans])\n    elif kind == 'log':\n        y[nans] = np.interp(np.log(x[nans]), np.log(x[~nans]), y[~nans])\n    else:\n        raise ValueError(f'Unknown option for kind: {kind}')\n    return y[x_sort_args]",
  "def interpolate_1d(x, xp, *args, axis=0, fill_value=np.nan, return_list_always=False):\n    r\"\"\"Interpolates data with any shape over a specified axis.\n\n    Interpolation over a specified axis for arrays of any shape.\n\n    Parameters\n    ----------\n    x : array-like\n        1-D array of desired interpolated values.\n\n    xp : array-like\n        The x-coordinates of the data points.\n\n    args : array-like\n        The data to be interpolated. Can be multiple arguments, all must be the same shape as\n        xp.\n\n    axis : int, optional\n        The axis to interpolate over. Defaults to 0.\n\n    fill_value: float, optional\n        Specify handling of interpolation points out of data bounds. If None, will return\n        ValueError if points are out of bounds. Defaults to nan.\n\n    return_list_always: bool, optional\n        Whether to always return a list of interpolated arrays, even when only a single\n        array is passed to `args`. Defaults to ``False``.\n\n    Returns\n    -------\n    array-like\n        Interpolated values for each point with coordinates sorted in ascending order.\n\n    Examples\n    --------\n     >>> import metpy.interpolate\n     >>> x = np.array([1., 2., 3., 4.])\n     >>> y = np.array([1., 2., 3., 4.])\n     >>> x_interp = np.array([2.5, 3.5])\n     >>> metpy.interpolate.interpolate_1d(x_interp, x, y)\n     array([2.5, 3.5])\n\n    Notes\n    -----\n    xp and args must be the same shape.\n\n    \"\"\"\n    # Handle units\n    x, xp = _strip_matching_units(x, xp)\n\n    # Make x an array\n    x = np.asanyarray(x).reshape(-1)\n\n    # Sort input data\n    sort_args = np.argsort(xp, axis=axis)\n    sort_x = np.argsort(x)\n\n    # The shape after all arrays are broadcast to each other\n    final_shape = np.broadcast_shapes(xp.shape, *(a.shape for a in args))\n\n    # indices for sorting\n    sorter = broadcast_indices(sort_args, final_shape, axis)\n\n    # sort xp -- need to make sure it's been manually broadcast due to our use of indices\n    # along all axes.\n    xp = np.broadcast_to(xp, final_shape)\n    xp = xp[sorter]\n\n    # Ensure source arrays are also in sorted order\n    variables = [arr[sorter] for arr in args]\n\n    # Make x broadcast with xp\n    x_array = x[sort_x]\n    expand = [np.newaxis] * len(final_shape)\n    expand[axis] = slice(None)\n    x_array = x_array[tuple(expand)]\n\n    # Calculate value above interpolated value\n    minv = np.apply_along_axis(np.searchsorted, axis, xp, x[sort_x])\n    minv2 = np.copy(minv)\n\n    # If fill_value is none and data is out of bounds, raise value error\n    if ((np.max(minv) == xp.shape[axis]) or (np.min(minv) == 0)) and fill_value is None:\n        raise ValueError('Interpolation point out of data bounds encountered')\n\n    # Warn if interpolated values are outside data bounds, will make these the values\n    # at end of data range.\n    if np.max(minv) == xp.shape[axis]:\n        _warnings.warn('Interpolation point out of data bounds encountered')\n        minv2[minv == xp.shape[axis]] = xp.shape[axis] - 1\n    if np.min(minv) == 0:\n        minv2[minv == 0] = 1\n\n    # Get indices for broadcasting arrays\n    above = broadcast_indices(minv2, final_shape, axis)\n    below = broadcast_indices(minv2 - 1, final_shape, axis)\n\n    if np.any(x_array < xp[below]):\n        _warnings.warn('Interpolation point out of data bounds encountered')\n\n    # Create empty output list\n    ret = []\n\n    # Calculate interpolation for each variable\n    for var in variables:\n        # Var needs to be on the *left* of the multiply to ensure that if it's a pint\n        # Quantity, it gets to control the operation--at least until we make sure\n        # masked arrays and pint play together better. See https://github.com/hgrecco/pint#633\n        var_interp = var[below] + (var[above] - var[below]) * ((x_array - xp[below])\n                                                               / (xp[above] - xp[below]))\n\n        # Set points out of bounds to fill value.\n        var_interp[minv == xp.shape[axis]] = fill_value\n        var_interp[x_array < xp[below]] = fill_value\n\n        # Check for input points in decreasing order and return output to match.\n        if x[0] > x[-1]:\n            var_interp = np.swapaxes(np.swapaxes(var_interp, 0, axis)[::-1], 0, axis)\n        # Output to list\n        ret.append(var_interp)\n\n    if return_list_always or len(ret) > 1:\n        return ret\n    else:\n        return ret[0]",
  "def log_interpolate_1d(x, xp, *args, axis=0, fill_value=np.nan):\n    r\"\"\"Interpolates data with logarithmic x-scale over a specified axis.\n\n    Interpolation on a logarithmic x-scale for interpolation values in pressure coordinates.\n\n    Parameters\n    ----------\n    x : array-like\n        1-D array of desired interpolated values.\n\n    xp : array-like\n        The x-coordinates of the data points.\n\n    args : array-like\n        The data to be interpolated. Can be multiple arguments, all must be the same shape as\n        xp.\n\n    axis : int, optional\n        The axis to interpolate over. Defaults to 0.\n\n    fill_value: float, optional\n        Specify handling of interpolation points out of data bounds. If None, will return\n        ValueError if points are out of bounds. Defaults to nan.\n\n    Returns\n    -------\n    array-like\n        Interpolated values for each point with coordinates sorted in ascending order.\n\n    Examples\n    --------\n     >>> x_log = np.array([1e3, 1e4, 1e5, 1e6])\n     >>> y_log = np.log(x_log) * 2 + 3\n     >>> x_interp = np.array([5e3, 5e4, 5e5])\n     >>> metpy.interpolate.log_interpolate_1d(x_interp, x_log, y_log)\n     array([20.03438638, 24.63955657, 29.24472675])\n\n    Notes\n    -----\n    xp and args must be the same shape.\n\n    \"\"\"\n    # Handle units\n    x, xp = _strip_matching_units(x, xp)\n\n    # Log x and xp\n    log_x = np.log(x)\n    log_xp = np.log(xp)\n    return interpolate_1d(log_x, log_xp, *args, axis=axis, fill_value=fill_value)",
  "def _strip_matching_units(*args):\n    \"\"\"Ensure arguments have same units and return with units stripped.\n\n    Replaces `@units.wraps(None, ('=A', '=A'))`, which breaks with `*args` handling for\n    pint>=0.9.\n    \"\"\"\n    if all(hasattr(arr, 'units') for arr in args):\n        return [arr.to(args[0].units).magnitude for arr in args]\n    else:\n        # Handle the case where we get mixed 'dimensionless' and bare array. This happens e.g.\n        # when you pass in a DataArray with no units for one arg.\n        return [arr.m_as('dimensionless') if hasattr(arr, 'units') else arr for arr in args]",
  "def cressman_point(sq_dist, values, radius):\n    r\"\"\"Generate a Cressman interpolation value for a point.\n\n    The calculated value is based on the given distances and search radius.\n\n    Parameters\n    ----------\n    sq_dist: (N, ) numpy.ndarray\n        Squared distance between observations and grid point\n    values: (N, ) numpy.ndarray\n        Observation values in same order as sq_dist\n    radius: float\n        Maximum distance to search for observations to use for\n        interpolation.\n\n    Returns\n    -------\n    value: float\n        Interpolation value for grid point.\n\n    \"\"\"\n    weights = tools.cressman_weights(sq_dist, radius)\n    total_weights = np.sum(weights)\n\n    return sum(v * (w / total_weights) for (w, v) in zip(weights, values))",
  "def barnes_point(sq_dist, values, kappa, gamma=None):\n    r\"\"\"Generate a single pass Barnes interpolation value for a point.\n\n    The calculated value is based on the given distances, kappa and gamma values. This\n    is calculated as an inverse distance-weighted average of the points in the neighborhood,\n    with weights given as:\n\n    .. math:: w = e ^ \\frac{-r^2}{\\kappa}\n\n    * :math:`\\kappa` is a scaling parameter\n    * :math:`r` is the distance to a point.\n\n    For more information see [Barnes1964]_ or [Koch1983]_.\n\n    Parameters\n    ----------\n    sq_dist: (N, ) numpy.ndarray\n        Squared distance between observations and grid point\n    values: (N, ) numpy.ndarray\n        Observation values in same order as sq_dist\n    kappa: float\n        Response parameter for barnes interpolation.\n    gamma: float\n        Adjustable smoothing parameter for the barnes interpolation. Default 1.\n\n    Returns\n    -------\n    value: float\n        Interpolation value for grid point.\n\n    \"\"\"\n    if gamma is None:\n        gamma = 1\n    weights = tools.barnes_weights(sq_dist, kappa, gamma)\n    total_weights = np.sum(weights)\n\n    return sum(v * (w / total_weights) for (w, v) in zip(weights, values))",
  "def natural_neighbor_point(xp, yp, variable, grid_loc, tri, neighbors, circumcenters):\n    r\"\"\"Generate a natural neighbor interpolation of the observations to the given point.\n\n    This uses the Liang and Hale approach [Liang2010]_. The interpolation will fail if\n    the grid point has no natural neighbors.\n\n    Parameters\n    ----------\n    xp: (N, ) numpy.ndarray\n        x-coordinates of observations\n    yp: (N, ) numpy.ndarray\n        y-coordinates of observations\n    variable: (N, ) numpy.ndarray\n        observation values associated with (xp, yp) pairs.\n        IE, variable[i] is a unique observation at (xp[i], yp[i])\n    grid_loc: (float, float)\n        Coordinates of the grid point at which to calculate the\n        interpolation.\n    tri: `scipy.spatial.Delaunay`\n        Delaunay triangulation of the observations.\n    neighbors: (N, ) numpy.ndarray\n        Simplex codes of the grid point's natural neighbors. The codes\n        will correspond to codes in the triangulation.\n    circumcenters: list\n        Pre-calculated triangle circumcenters for quick look ups. Requires\n        indices for the list to match the simplices from the Delaunay triangulation.\n\n    Returns\n    -------\n    value: float\n       Interpolated value for the grid location\n\n    \"\"\"\n    edges = geometry.find_local_boundary(tri, neighbors)\n    edge_vertices = [segment[0] for segment in geometry.order_edges(edges)]\n    num_vertices = len(edge_vertices)\n\n    p1 = edge_vertices[0]\n    p2 = edge_vertices[1]\n\n    c1 = geometry.circumcenter(grid_loc, tri.points[p1], tri.points[p2])\n    polygon = [c1]\n\n    area_list = []\n    total_area = 0.0\n\n    for i in range(num_vertices):\n\n        p3 = edge_vertices[(i + 2) % num_vertices]\n\n        try:\n\n            c2 = geometry.circumcenter(grid_loc, tri.points[p3], tri.points[p2])\n            polygon.append(c2)\n\n            for check_tri in neighbors:\n                if p2 in tri.simplices[check_tri]:\n                    polygon.append(circumcenters[check_tri])\n\n            pts = [polygon[i] for i in ConvexHull(polygon).vertices]\n            value = variable[(tri.points[p2][0] == xp) & (tri.points[p2][1] == yp)]\n\n            cur_area = geometry.area(pts)\n\n            total_area += cur_area\n\n            area_list.append(cur_area * value[0])\n\n        except (ZeroDivisionError, qhull.QhullError) as e:\n            message = ('Error during processing of a grid. '\n                       'Interpolation will continue but be mindful '\n                       f'of errors in output. {e}')\n\n            log.warning(message)\n            return np.nan\n\n        polygon = [c2]\n\n        p2 = p3\n\n    return sum(x / total_area for x in area_list)",
  "def natural_neighbor_to_points(points, values, xi):\n    r\"\"\"Generate a natural neighbor interpolation to the given points.\n\n    This assigns values to the given interpolation points using the Liang and Hale\n    [Liang2010]_. approach.\n\n    Parameters\n    ----------\n    points: array-like, (N, 2)\n        Coordinates of the data points.\n    values: array-like, (N,)\n        Values of the data points.\n    xi: array-like, (M, 2)\n        Points to interpolate the data onto.\n\n    Returns\n    -------\n    img: numpy.ndarray, (M,)\n        Array representing the interpolated values for each input point in `xi`\n\n    See Also\n    --------\n    natural_neighbor_to_grid\n\n    \"\"\"\n    tri = Delaunay(points)\n    members, circumcenters = geometry.find_natural_neighbors(tri, xi)\n\n    if hasattr(values, 'units'):\n        org_units = values.units\n        values = values.magnitude\n    else:\n        org_units = None\n\n    img = np.asarray([natural_neighbor_point(*np.array(points).transpose(),\n                                             values, xi[grid], tri, neighbors, circumcenters)\n                      if len(neighbors) > 0 else np.nan\n                      for grid, neighbors in members.items()])\n\n    if org_units:\n        img = units.Quantity(img, org_units)\n\n    return img",
  "def inverse_distance_to_points(points, values, xi, r, gamma=None, kappa=None, min_neighbors=3,\n                               kind='cressman'):\n    r\"\"\"Generate an inverse distance weighting interpolation to the given points.\n\n    Values are assigned to the given interpolation points based on either [Cressman1959]_ or\n    [Barnes1964]_. The Barnes implementation used here is based on [Koch1983]_.\n\n    Parameters\n    ----------\n    points: array-like, (N, 2)\n        Coordinates of the data points.\n    values: array-like, (N,)\n        Values of the data points.\n    xi: array-like, (M, 2)\n        Points to interpolate the data onto.\n    r: float\n        Radius from grid center, within which observations are considered and weighted.\n    gamma: float\n        Adjustable smoothing parameter for the barnes interpolation. Default None.\n    kappa: float\n        Response parameter for barnes interpolation. Default None.\n    min_neighbors: int\n        Minimum number of neighbors needed to perform barnes or cressman interpolation\n        for a point. Default is 3.\n    kind: str\n        Specify what inverse distance weighting interpolation to use.\n        Options: 'cressman' or 'barnes'. Default 'cressman'\n\n    Returns\n    -------\n    img: numpy.ndarray, (M,)\n        Array representing the interpolated values for each input point in `xi`\n\n    See Also\n    --------\n    inverse_distance_to_grid\n\n    \"\"\"\n    if kind == 'cressman':\n        interp_func = functools.partial(cressman_point, radius=r)\n    elif kind == 'barnes':\n        interp_func = functools.partial(barnes_point, kappa=kappa, gamma=gamma)\n    else:\n        raise ValueError(f'{kind} interpolation not supported.')\n\n    obs_tree = cKDTree(points)\n    indices = obs_tree.query_ball_point(xi, r=r)\n\n    if hasattr(values, 'units'):\n        org_units = values.units\n        values = values.magnitude\n    else:\n        org_units = None\n\n    img = np.asarray([interp_func(geometry.dist_2(*grid, *obs_tree.data[matches].T),\n                                  values[matches]) if len(matches) >= min_neighbors else np.nan\n                      for matches, grid in zip(indices, xi)])\n\n    if org_units:\n        img = units.Quantity(img, org_units)\n\n    return img",
  "def interpolate_to_points(points, values, xi, interp_type='linear', minimum_neighbors=3,\n                          gamma=0.25, kappa_star=5.052, search_radius=None, rbf_func='linear',\n                          rbf_smooth=0):\n    r\"\"\"Interpolate unstructured point data to the given points.\n\n    This function interpolates the given `values` valid at ``points`` to the points `xi`.\n    This is modeled after `scipy.interpolate.griddata`, but acts as a generalization of it by\n    including the following types of interpolation:\n\n    - Linear\n    - Nearest Neighbor\n    - Cubic\n    - Radial Basis Function\n    - Natural Neighbor (2D Only)\n    - Barnes (2D Only)\n    - Cressman (2D Only)\n\n    Parameters\n    ----------\n    points: array-like, (N, P)\n        Coordinates of the data points.\n    values: array-like, (N,)\n        Values of the data points.\n    xi: array-like, (M, P)\n        Points to interpolate the data onto.\n    interp_type: str\n        What type of interpolation to use. Available options include:\n        1) \"linear\", \"nearest\", \"cubic\", or \"rbf\" from `scipy.interpolate`.\n        2) \"natural_neighbor\", \"barnes\", or \"cressman\" from `metpy.interpolate`.\n        Default \"linear\".\n    minimum_neighbors: int\n        Minimum number of neighbors needed to perform Barnes or Cressman interpolation for a\n        point. Default is 3.\n    gamma: float\n        Adjustable smoothing parameter for the barnes interpolation. Default 0.25.\n    kappa_star: float\n        Response parameter for barnes interpolation, specified non-dimensionally\n        in terms of the Nyquist. Default 5.052.\n    search_radius: float\n        A search radius to use for the Barnes and Cressman interpolation schemes.\n        If search_radius is not specified, it will default to 5 times the average spacing of\n        observations.\n    rbf_func: str\n        Specifies which function to use for Rbf interpolation.\n        Options include: 'multiquadric', 'inverse', 'gaussian', 'linear', 'cubic',\n        'quintic', and 'thin_plate'. Default 'linear'. See `scipy.interpolate.Rbf` for more\n        information.\n    rbf_smooth: float\n        Smoothing value applied to rbf interpolation.  Higher values result in more smoothing.\n\n    Returns\n    -------\n    values_interpolated: ndarray, (M,)\n        Array representing the interpolated values for each input point in `xi`.\n\n    See Also\n    --------\n    interpolate_to_grid\n\n    Notes\n    -----\n    This function primarily acts as a wrapper for the individual interpolation routines. The\n    individual functions are also available for direct use.\n\n    \"\"\"\n    # If this is a type that `griddata` handles, hand it along to `griddata`\n    if interp_type in ['linear', 'nearest', 'cubic']:\n        if hasattr(values, 'units'):\n            org_units = values.units\n            values = values.magnitude\n        else:\n            org_units = None\n\n        ret = griddata(points, values, xi, method=interp_type)\n\n        if org_units:\n            ret = units.Quantity(ret, org_units)\n\n        return ret\n\n    # If this is natural neighbor, hand it along to `natural_neighbor`\n    elif interp_type == 'natural_neighbor':\n        return natural_neighbor_to_points(points, values, xi)\n\n    # If this is Barnes/Cressman, determine search_radius and hand it along to\n    # `inverse_distance`\n    elif interp_type in ['cressman', 'barnes']:\n        ave_spacing = tools.average_spacing(points)\n        if search_radius is None:\n            search_radius = 5 * ave_spacing\n\n        if interp_type == 'cressman':\n            return inverse_distance_to_points(points, values, xi, search_radius,\n                                              min_neighbors=minimum_neighbors,\n                                              kind=interp_type)\n        else:\n            kappa = tools.calc_kappa(ave_spacing, kappa_star)\n            return inverse_distance_to_points(points, values, xi, search_radius, gamma, kappa,\n                                              min_neighbors=minimum_neighbors,\n                                              kind=interp_type)\n\n    # If this is radial basis function, make the interpolator and apply it\n    elif interp_type == 'rbf':\n        points_transposed = np.array(points).transpose()\n        xi_transposed = np.array(xi).transpose()\n\n        if hasattr(values, 'units'):\n            org_units = values.units\n            values = values.magnitude\n        else:\n            org_units = None\n\n        rbfi = Rbf(points_transposed[0], points_transposed[1], values, function=rbf_func,\n                   smooth=rbf_smooth)\n        ret = rbfi(xi_transposed[0], xi_transposed[1])\n\n        if org_units:\n            ret = units.Quantity(ret, org_units)\n\n        return ret\n\n    else:\n        raise ValueError(f'Interpolation option {interp_type} not available. '\n                         'Try: linear, nearest, cubic, natural_neighbor, '\n                         'barnes, cressman, rbf')",
  "def generate_grid(horiz_dim, bbox):\n    r\"\"\"Generate a meshgrid based on bounding box and x & y resolution.\n\n    Parameters\n    ----------\n    horiz_dim: int or float\n        Horizontal resolution\n    bbox: dict\n        Dictionary with keys 'east', 'west', 'north', 'south' with the box extents\n        in those directions.\n\n    Returns\n    -------\n    grid_x: (X, Y) numpy.ndarray\n        X dimension meshgrid defined by given bounding box\n    grid_y: (X, Y) numpy.ndarray\n        Y dimension meshgrid defined by given bounding box\n\n    \"\"\"\n    x_steps, y_steps = get_xy_steps(bbox, horiz_dim)\n\n    grid_x = np.linspace(bbox['west'], bbox['east'], x_steps)\n    grid_y = np.linspace(bbox['south'], bbox['north'], y_steps)\n\n    return np.meshgrid(grid_x, grid_y)",
  "def generate_grid_coords(gx, gy):\n    r\"\"\"Calculate x,y coordinates of each grid cell.\n\n    Parameters\n    ----------\n    gx: numeric\n        x coordinates in meshgrid\n    gy: numeric\n        y coordinates in meshgrid\n\n    Returns\n    -------\n    (X, Y) numpy.ndarray\n        List of coordinates in meshgrid\n\n    \"\"\"\n    return np.stack([gx.ravel(), gy.ravel()], axis=1)",
  "def get_xy_range(bbox):\n    r\"\"\"Return x and y ranges in meters based on bounding box.\n\n    bbox: dict\n        Dictionary with keys 'east', 'west', 'north', 'south' with the box extents\n        in those directions.\n\n    Returns\n    -------\n    x_range: float\n        Range in meters in x dimension.\n    y_range: float\n        Range in meters in y dimension.\n\n    \"\"\"\n    x_range = bbox['east'] - bbox['west']\n    y_range = bbox['north'] - bbox['south']\n\n    return x_range, y_range",
  "def get_xy_steps(bbox, h_dim):\n    r\"\"\"Return meshgrid spacing based on bounding box.\n\n    bbox: dict\n        Dictionary with keys 'east', 'west', 'north', 'south' with the box extents\n        in those directions.\n    h_dim: int or float\n        Horizontal resolution in meters.\n\n    Returns\n    -------\n    x_steps, (X, ) numpy.ndarray\n        Number of grids in x dimension.\n    y_steps: (Y, ) numpy.ndarray\n        Number of grids in y dimension.\n\n    \"\"\"\n    x_range, y_range = get_xy_range(bbox)\n\n    x_steps = np.ceil(x_range / h_dim) + 1\n    y_steps = np.ceil(y_range / h_dim) + 1\n\n    return int(x_steps), int(y_steps)",
  "def get_boundary_coords(x, y, spatial_pad=0):\n    r\"\"\"Return bounding box based on given x and y coordinates assuming northern hemisphere.\n\n    x: numeric\n        x coordinates.\n    y: numeric\n        y coordinates.\n    spatial_pad: int or float\n        Number of meters to add to the x and y dimensions to reduce edge effects.\n\n    Returns\n    -------\n    bbox: dict\n        Dictionary with keys 'east', 'west', 'north', 'south' with the box extents\n        in those directions.\n\n    \"\"\"\n    west = np.min(x) - spatial_pad\n    east = np.max(x) + spatial_pad\n    north = np.max(y) + spatial_pad\n    south = np.min(y) - spatial_pad\n\n    return {'west': west, 'south': south, 'east': east, 'north': north}",
  "def natural_neighbor_to_grid(xp, yp, variable, grid_x, grid_y):\n    r\"\"\"Generate a natural neighbor interpolation of the given points to a regular grid.\n\n    This assigns values to the given grid using the Liang and Hale [Liang2010]_.\n    approach.\n\n    Parameters\n    ----------\n    xp: (P, ) numpy.ndarray\n        x-coordinates of observations\n    yp: (P, ) numpy.ndarray\n        y-coordinates of observations\n    variable: (P, ) numpy.ndarray\n        observation values associated with (xp, yp) pairs.\n        IE, variable[i] is a unique observation at (xp[i], yp[i])\n    grid_x: (M, N) numpy.ndarray\n        Meshgrid associated with x dimension\n    grid_y: (M, N) numpy.ndarray\n        Meshgrid associated with y dimension\n\n    Returns\n    -------\n    img: (M, N) numpy.ndarray\n        Interpolated values on a 2-dimensional grid\n\n    See Also\n    --------\n    natural_neighbor_to_points\n\n    \"\"\"\n    # Handle grid-to-points conversion, and use function from `interpolation`\n    points_obs = list(zip(xp, yp))\n    points_grid = generate_grid_coords(grid_x, grid_y)\n    img = natural_neighbor_to_points(points_obs, variable, points_grid)\n    return img.reshape(grid_x.shape)",
  "def inverse_distance_to_grid(xp, yp, variable, grid_x, grid_y, r, gamma=None, kappa=None,\n                             min_neighbors=3, kind='cressman'):\n    r\"\"\"Generate an inverse distance interpolation of the given points to a regular grid.\n\n    Values are assigned to the given grid using inverse distance weighting based on either\n    [Cressman1959]_ or [Barnes1964]_. The Barnes implementation used here based on [Koch1983]_.\n\n    Parameters\n    ----------\n    xp: (N, ) numpy.ndarray\n        x-coordinates of observations.\n    yp: (N, ) numpy.ndarray\n        y-coordinates of observations.\n    variable: (N, ) numpy.ndarray\n        observation values associated with (xp, yp) pairs.\n        IE, variable[i] is a unique observation at (xp[i], yp[i]).\n    grid_x: (M, 2) numpy.ndarray\n        Meshgrid associated with x dimension.\n    grid_y: (M, 2) numpy.ndarray\n        Meshgrid associated with y dimension.\n    r: float\n        Radius from grid center, within which observations\n        are considered and weighted.\n    gamma: float\n        Adjustable smoothing parameter for the barnes interpolation. Default None.\n    kappa: float\n        Response parameter for barnes interpolation. Default None.\n    min_neighbors: int\n        Minimum number of neighbors needed to perform barnes or cressman interpolation\n        for a point. Default is 3.\n    kind: str\n        Specify what inverse distance weighting interpolation to use.\n        Options: 'cressman' or 'barnes'. Default 'cressman'\n\n    Returns\n    -------\n    img: (M, N) numpy.ndarray\n        Interpolated values on a 2-dimensional grid\n\n    See Also\n    --------\n    inverse_distance_to_points\n\n    \"\"\"\n    # Handle grid-to-points conversion, and use function from `interpolation`\n    points_obs = list(zip(xp, yp))\n    points_grid = generate_grid_coords(grid_x, grid_y)\n    img = inverse_distance_to_points(points_obs, variable, points_grid, r, gamma=gamma,\n                                     kappa=kappa, min_neighbors=min_neighbors, kind=kind)\n    return img.reshape(grid_x.shape)",
  "def interpolate_to_grid(x, y, z, interp_type='linear', hres=50000,\n                        minimum_neighbors=3, gamma=0.25, kappa_star=5.052,\n                        search_radius=None, rbf_func='linear', rbf_smooth=0,\n                        boundary_coords=None):\n    r\"\"\"Interpolate given (x,y), observation (z) pairs to a grid based on given parameters.\n\n    Parameters\n    ----------\n    x: array-like\n        x coordinate, can have units of linear distance or degrees\n    y: array-like\n        y coordinate, can have units of linear distance or degrees\n    z: array-like\n        observation value\n    interp_type: str\n        What type of interpolation to use. Available options include:\n        1) \"linear\", \"nearest\", \"cubic\", or \"rbf\" from `scipy.interpolate`.\n        2) \"natural_neighbor\", \"barnes\", or \"cressman\" from `metpy.interpolate`.\n        Default \"linear\".\n    hres: float\n        The horizontal resolution of the generated grid, given in the same units as the\n        x and y parameters. Default 50000.\n    minimum_neighbors: int\n        Minimum number of neighbors needed to perform Barnes or Cressman interpolation for a\n        point. Default is 3.\n    gamma: float\n        Adjustable smoothing parameter for the barnes interpolation. Default 0.25.\n    kappa_star: float\n        Response parameter for barnes interpolation, specified nondimensionally\n        in terms of the Nyquist. Default 5.052\n    search_radius: float\n        A search radius to use for the Barnes and Cressman interpolation schemes.\n        If search_radius is not specified, it will default to 5 times the average spacing of\n        observations.\n    rbf_func: str\n        Specifies which function to use for Rbf interpolation.\n        Options include: 'multiquadric', 'inverse', 'gaussian', 'linear', 'cubic',\n        'quintic', and 'thin_plate'. Default 'linear'. See `scipy.interpolate.Rbf` for more\n        information.\n    rbf_smooth: float\n        Smoothing value applied to rbf interpolation.  Higher values result in more smoothing.\n    boundary_coords: dict\n        Optional dictionary containing coordinates of the study area boundary. Dictionary\n        should be in format: {'west': west, 'south': south, 'east': east, 'north': north}\n\n    Returns\n    -------\n    grid_x: (N, 2) numpy.ndarray\n        Meshgrid for the resulting interpolation in the x dimension\n    grid_y: (N, 2) numpy.ndarray\n        Meshgrid for the resulting interpolation in the y dimension numpy.ndarray\n    img: (M, N) numpy.ndarray\n        2-dimensional array representing the interpolated values for each grid.\n\n    See Also\n    --------\n    interpolate_to_points\n\n    Notes\n    -----\n    This function acts as a wrapper for `interpolate_points` to allow it to generate a regular\n    grid.\n\n    This function interpolates points to a Cartesian plane, even if lat/lon coordinates\n    are provided.\n\n    \"\"\"\n    # Generate the grid\n    if boundary_coords is None:\n        boundary_coords = get_boundary_coords(x, y)\n    grid_x, grid_y = generate_grid(hres, boundary_coords)\n\n    # Handle grid-to-points conversion, and use function from `interpolation`\n    points_obs = np.array(list(zip(x, y)))\n    points_grid = generate_grid_coords(grid_x, grid_y)\n    img = interpolate_to_points(points_obs, z, points_grid, interp_type=interp_type,\n                                minimum_neighbors=minimum_neighbors, gamma=gamma,\n                                kappa_star=kappa_star, search_radius=search_radius,\n                                rbf_func=rbf_func, rbf_smooth=rbf_smooth)\n\n    return grid_x, grid_y, img.reshape(grid_x.shape)",
  "def interpolate_to_isosurface(level_var, interp_var, level, bottom_up_search=True):\n    r\"\"\"Linear interpolation of a variable to a given vertical level from given values.\n\n    This function assumes that highest vertical level (lowest pressure) is zeroth index.\n    A classic use of this function would be to compute the potential temperature on the\n    dynamic tropopause (2 PVU surface).\n\n    Parameters\n    ----------\n    level_var: array-like (P, M, N)\n        Level values in 3D grid on common vertical coordinate (e.g., PV values on\n        isobaric levels). Assumes height dimension is highest to lowest in atmosphere.\n    interp_var: array-like (P, M, N)\n        Variable on 3D grid with same vertical coordinate as level_var to interpolate to\n        given level (e.g., potential temperature on isobaric levels)\n    level: int or float\n        Desired interpolated level (e.g., 2 PVU surface)\n    bottom_up_search : bool, optional\n        Controls whether to search for levels bottom-up (starting at lower indices),\n        or top-down (starting at higher indices). Defaults to True, which is bottom-up search.\n\n    Returns\n    -------\n    interp_level: (M, N) numpy.ndarray\n        The interpolated variable (e.g., potential temperature) on the desired level (e.g.,\n        2 PVU surface)\n\n    Notes\n    -----\n    This function implements a linear interpolation to estimate values on a given surface.\n    The prototypical example is interpolation of potential temperature to the dynamic\n    tropopause (e.g., 2 PVU surface)\n\n    \"\"\"\n    from ..calc import find_bounding_indices\n\n    # Find index values above and below desired interpolated surface value\n    above, below, good = find_bounding_indices(level_var, [level], axis=0,\n                                               from_below=bottom_up_search)\n\n    # Linear interpolation of variable to interpolated surface value\n    interp_level = (((level - level_var[above]) / (level_var[below] - level_var[above]))\n                    * (interp_var[below] - interp_var[above])) + interp_var[above]\n\n    # Handle missing values and instances where no values for surface exist above and below\n    interp_level[~good] = np.nan\n    minvar = (np.min(level_var, axis=0) >= level)\n    maxvar = (np.max(level_var, axis=0) <= level)\n    interp_level[0][minvar] = interp_var[-1][minvar]\n    interp_level[0][maxvar] = interp_var[0][maxvar]\n    return interp_level.squeeze()",
  "def calc_kappa(spacing, kappa_star=5.052):\n    r\"\"\"Calculate the kappa parameter for barnes interpolation.\n\n    Parameters\n    ----------\n    spacing: float\n        Average spacing between observations\n    kappa_star: float\n        Non-dimensional response parameter. Default 5.052.\n\n    Returns\n    -------\n        kappa: float\n\n    \"\"\"\n    return kappa_star * (2.0 * spacing / np.pi)**2",
  "def average_spacing(points):\n    \"\"\"Calculate the average spacing to the nearest other point.\n\n    Parameters\n    ----------\n    points : (M, N) array-like\n         M points in N dimensional space\n\n    Returns\n    -------\n        The average distance to the nearest neighbor across all points\n\n    \"\"\"\n    dist_matrix = cdist(points, points)\n    diag = np.arange(len(dist_matrix))\n    dist_matrix[diag, diag] = np.nan\n    return np.nanmin(dist_matrix, axis=0).mean()",
  "def remove_observations_below_value(x, y, z, val=0):\n    r\"\"\"Remove all x, y, and z where z is less than val.\n\n    Will not destroy original values.\n\n    Parameters\n    ----------\n    x: array-like\n        x coordinate.\n    y: array-like\n        y coordinate.\n    z: array-like\n        Observation value.\n    val: float\n        Value at which to threshold z.\n\n    Returns\n    -------\n    x, y, z\n        List of coordinate observation pairs without\n        observation values less than val.\n\n    \"\"\"\n    x_ = x[z >= val]\n    y_ = y[z >= val]\n    z_ = z[z >= val]\n\n    return x_, y_, z_",
  "def remove_nan_observations(x, y, z):\n    r\"\"\"Remove all x, y, and z where z is nan.\n\n    Will not destroy original values.\n\n    Parameters\n    ----------\n    x: array-like\n        x coordinate\n    y: array-like\n        y coordinate\n    z: array-like\n        observation value\n\n    Returns\n    -------\n    x, y, z\n        List of coordinate observation pairs without\n        nan valued observations.\n\n    \"\"\"\n    x_ = x[~np.isnan(z)]\n    y_ = y[~np.isnan(z)]\n    z_ = z[~np.isnan(z)]\n\n    return x_, y_, z_",
  "def remove_repeat_coordinates(x, y, z):\n    r\"\"\"Remove all x, y, and z where (x,y) is repeated and keep the first occurrence only.\n\n    Will not destroy original values.\n\n    Parameters\n    ----------\n    x: array-like\n        x coordinate\n    y: array-like\n        y coordinate\n    z: array-like\n        observation value\n\n    Returns\n    -------\n    x, y, z\n        List of coordinate observation pairs without\n        repeated coordinates.\n\n    \"\"\"\n    coords = []\n    variable = []\n\n    for (x_, y_, t_) in zip(x, y, z):\n        if (x_, y_) not in coords:\n            coords.append((x_, y_))\n            variable.append(t_)\n\n    coords = np.array(coords)\n\n    x_ = coords[:, 0]\n    y_ = coords[:, 1]\n\n    z_ = np.array(variable)\n\n    return x_, y_, z_",
  "def barnes_weights(sq_dist, kappa, gamma):\n    r\"\"\"Calculate the Barnes weights from squared distance values.\n\n    Parameters\n    ----------\n    sq_dist: (N, ) numpy.ndarray\n        Squared distances from interpolation point\n        associated with each observation in meters.\n    kappa: float\n        Response parameter for barnes interpolation. Default None.\n    gamma: float\n        Adjustable smoothing parameter for the barnes interpolation. Default None.\n\n    Returns\n    -------\n    weights: (N, ) numpy.ndarray\n        Calculated weights for the given observations determined by their distance\n        to the interpolation point.\n\n    \"\"\"\n    return np.exp(-1.0 * sq_dist / (kappa * gamma))",
  "def cressman_weights(sq_dist, r):\n    r\"\"\"Calculate the Cressman weights from squared distance values.\n\n    Parameters\n    ----------\n    sq_dist: (N, ) numpy.ndarray\n        Squared distances from interpolation point associated with each observation in meters.\n    r: float\n        Maximum distance an observation can be from an interpolation point to be considered\n        in the interpolation calculation.\n\n    Returns\n    -------\n    weights: (N, ) numpy.ndarray\n        Calculated weights for the given observations determined by their distance\n        to the interpolation point.\n\n    \"\"\"\n    return (r**2 - sq_dist) / (r**2 + sq_dist)",
  "def get_points_within_r(center_points, target_points, r):\n    r\"\"\"Get all target_points within a specified radius of a center point.\n\n    All data must be in same coordinate system, or you will get undetermined results.\n\n    Parameters\n    ----------\n    center_points: (X, Y) numpy.ndarray\n        location from which to grab surrounding points within r\n    target_points: (X, Y) numpy.ndarray\n        points from which to return if they are within r of center_points\n    r: integer\n        search radius around center_points to grab target_points\n\n    Returns\n    -------\n    matches: (X, Y) numpy.ndarray\n        A list of points within r distance of, and in the same\n        order as, center_points\n\n    \"\"\"\n    tree = cKDTree(target_points)\n    indices = tree.query_ball_point(center_points, r)\n    return tree.data[indices].T",
  "def get_point_count_within_r(center_points, target_points, r):\n    r\"\"\"Get count of target points within a specified radius from center points.\n\n    All data must be in same coordinate system, or you will get undetermined results.\n\n    Parameters\n    ----------\n    center_points: (X, Y) numpy.ndarray\n        locations from which to grab surrounding points within r\n    target_points: (X, Y) numpy.ndarray\n        points from which to return if they are within r of center_points\n    r: integer\n        search radius around center_points to grab target_points\n\n    Returns\n    -------\n    matches: (N, ) numpy.ndarray\n        A list of point counts within r distance of, and in the same\n        order as, center_points\n\n    \"\"\"\n    tree = cKDTree(target_points)\n    indices = tree.query_ball_point(center_points, r)\n    return np.array([len(x) for x in indices])",
  "def triangle_area(pt1, pt2, pt3):\n    r\"\"\"Return the area of a triangle.\n\n    Parameters\n    ----------\n    pt1: (X,Y) numpy.ndarray\n        Starting vertex of a triangle\n    pt2: (X,Y) numpy.ndarray\n        Second vertex of a triangle\n    pt3: (X,Y) numpy.ndarray\n        Ending vertex of a triangle\n\n    Returns\n    -------\n    area: float\n        Area of the given triangle.\n\n    \"\"\"\n    a = 0.0\n\n    a += pt1[0] * pt2[1] - pt2[0] * pt1[1]\n    a += pt2[0] * pt3[1] - pt3[0] * pt2[1]\n    a += pt3[0] * pt1[1] - pt1[0] * pt3[1]\n\n    return abs(a) / 2",
  "def dist_2(x0, y0, x1, y1):\n    r\"\"\"Return the squared distance between two points.\n\n    This is faster than calculating distance but should\n    only be used with comparable ratios.\n\n    Parameters\n    ----------\n    x0: float\n        Starting x coordinate\n    y0: float\n        Starting y coordinate\n    x1: float\n        Ending x coordinate\n    y1: float\n        Ending y coordinate\n\n    Returns\n    -------\n    d2: float\n        squared distance\n\n    See Also\n    --------\n    distance\n\n    \"\"\"\n    d0 = x1 - x0\n    d1 = y1 - y0\n    return d0**2 + d1**2",
  "def distance(p0, p1):\n    r\"\"\"Return the distance between two points.\n\n    Parameters\n    ----------\n    p0: (X,Y) numpy.ndarray\n        Starting coordinate\n    p1: (X,Y) numpy.ndarray\n        Ending coordinate\n\n    Returns\n    -------\n    d: float\n        distance\n\n    See Also\n    --------\n    dist_2\n\n    \"\"\"\n    return math.sqrt(dist_2(p0[0], p0[1], p1[0], p1[1]))",
  "def circumcircle_radius(pt0, pt1, pt2):\n    r\"\"\"Calculate and return the radius of a given triangle's circumcircle.\n\n    Parameters\n    ----------\n    pt0: (x, y)\n        Starting vertex of triangle\n    pt1: (x, y)\n        Second vertex of triangle\n    pt2: (x, y)\n        Final vertex of a triangle\n\n    Returns\n    -------\n    r: float\n        circumcircle radius\n\n    See Also\n    --------\n    circumcenter\n\n    \"\"\"\n    a = distance(pt0, pt1)\n    b = distance(pt1, pt2)\n    c = distance(pt2, pt0)\n\n    t_area = triangle_area(pt0, pt1, pt2)\n\n    return (a * b * c) / (4 * t_area) if t_area > 0 else np.nan",
  "def circumcenter(pt0, pt1, pt2):\n    r\"\"\"Calculate and return the circumcenter of a circumcircle generated by a given triangle.\n\n    All three points must be unique or a division by zero error will be raised.\n\n    Parameters\n    ----------\n    pt0: (x, y)\n        Starting vertex of triangle\n    pt1: (x, y)\n        Second vertex of triangle\n    pt2: (x, y)\n        Final vertex of a triangle\n\n    Returns\n    -------\n    cc: (x, y)\n        circumcenter coordinates\n\n    See Also\n    --------\n    circumcenter\n\n    \"\"\"\n    a_x = pt0[0]\n    a_y = pt0[1]\n    b_x = pt1[0]\n    b_y = pt1[1]\n    c_x = pt2[0]\n    c_y = pt2[1]\n\n    bc_y_diff = b_y - c_y\n    ca_y_diff = c_y - a_y\n    ab_y_diff = a_y - b_y\n    cb_x_diff = c_x - b_x\n    ac_x_diff = a_x - c_x\n    ba_x_diff = b_x - a_x\n\n    d_div = (a_x * bc_y_diff + b_x * ca_y_diff + c_x * ab_y_diff)\n\n    if d_div == 0:\n        raise ZeroDivisionError\n\n    d_inv = 0.5 / d_div\n\n    a_mag = a_x**2 + a_y**2\n    b_mag = b_x**2 + b_y**2\n    c_mag = c_x**2 + c_y**2\n\n    cx = (a_mag * bc_y_diff + b_mag * ca_y_diff + c_mag * ab_y_diff) * d_inv\n    cy = (a_mag * cb_x_diff + b_mag * ac_x_diff + c_mag * ba_x_diff) * d_inv\n\n    return cx, cy",
  "def find_natural_neighbors(tri, grid_points):\n    r\"\"\"Return the natural neighbor triangles for each given grid cell.\n\n    These are determined by the properties of the given Delaunay triangulation.\n    A triangle is a natural neighbor of a grid cell if that triangle's circumcenter\n    is within the circumradius of the grid cell center.\n\n    Parameters\n    ----------\n    tri: `scipy.spatial.Delaunay`\n        A Delaunay Triangulation.\n    grid_points: (X, Y) numpy.ndarray\n        Locations of grids.\n\n    Returns\n    -------\n    members: dict\n        List of simplex codes for natural neighbor triangles in ``tri`` for each grid cell.\n    circumcenters: numpy.ndarray\n        Circumcenter for each triangle in ``tri``.\n\n    \"\"\"\n    # Used for fast identification of points with a radius of another point\n    tree = cKDTree(grid_points)\n\n    # Mask for points that are outside the triangulation\n    in_triangulation = tri.find_simplex(tree.data) >= 0\n\n    circumcenters = []\n    members = {key: [] for key in range(len(tree.data))}\n    for i, indices in enumerate(tri.simplices):\n        # Find the circumcircle (center and radius) for the triangle.\n        triangle = tri.points[indices]\n        cc = circumcenter(*triangle)\n        r = circumcircle_radius(*triangle)\n        circumcenters.append(cc)\n\n        # Find all grid points within the circumcircle.\n        for point in tree.query_ball_point(cc, r):\n            # If this point is within the triangulation, add this triangle to its list of\n            # natural neighbors\n            if in_triangulation[point]:\n                members[point].append(i)\n\n    return members, np.array(circumcenters)",
  "def find_nn_triangles_point(tri, cur_tri, point):\n    r\"\"\"Return the natural neighbors of a triangle containing a point.\n\n    This is based on the provided Delaunay Triangulation.\n\n    Parameters\n    ----------\n    tri: `scipy.spatial.Delaunay`\n        A Delaunay Triangulation\n    cur_tri: int\n        Simplex code for Delaunay Triangulation lookup of\n        a given triangle that contains 'position'.\n    point: (x, y)\n        Coordinates used to calculate distances to\n        simplexes in 'tri'.\n\n    Returns\n    -------\n    nn: (N, ) array\n        List of simplex codes for natural neighbor\n        triangles in 'tri'.\n\n    \"\"\"\n    nn = []\n\n    candidates = set(tri.neighbors[cur_tri])\n\n    # find the union of the two sets\n    candidates |= set(tri.neighbors[tri.neighbors[cur_tri]].flat)\n\n    # remove instances of the \"no neighbors\" code\n    candidates.discard(-1)\n\n    for neighbor in candidates:\n\n        triangle = tri.points[tri.simplices[neighbor]]\n        cur_x, cur_y = circumcenter(triangle[0], triangle[1], triangle[2])\n        r = circumcircle_radius(triangle[0], triangle[1], triangle[2])\n\n        if dist_2(point[0], point[1], cur_x, cur_y) < r**2:\n\n            nn.append(neighbor)\n\n    return nn",
  "def find_local_boundary(tri, triangles):\n    r\"\"\"Find and return the outside edges of a collection of natural neighbor triangles.\n\n    There is no guarantee that this boundary is convex, so ConvexHull is not\n    sufficient in some situations.\n\n    Parameters\n    ----------\n    tri: `scipy.spatial.Delaunay`\n        A Delaunay Triangulation\n    triangles: (N, ) array\n        List of natural neighbor triangles.\n\n    Returns\n    -------\n    edges: list\n        List of vertex codes that form outer edges of\n        a group of natural neighbor triangles.\n\n    \"\"\"\n    edges = []\n\n    for triangle in triangles:\n\n        for i in range(3):\n\n            pt1 = tri.simplices[triangle][i]\n            pt2 = tri.simplices[triangle][(i + 1) % 3]\n\n            if (pt1, pt2) in edges:\n                edges.remove((pt1, pt2))\n\n            elif (pt2, pt1) in edges:\n                edges.remove((pt2, pt1))\n\n            else:\n                edges.append((pt1, pt2))\n\n    return edges",
  "def area(poly):\n    r\"\"\"Find the area of a given polygon using the shoelace algorithm.\n\n    Parameters\n    ----------\n    poly: (2, N) numpy.ndarray\n        2-dimensional coordinates representing an ordered\n        traversal around the edge a polygon.\n\n    Returns\n    -------\n    area: float\n\n    \"\"\"\n    a = 0.0\n    n = len(poly)\n\n    for i in range(n):\n        a += poly[i][0] * poly[(i + 1) % n][1] - poly[(i + 1) % n][0] * poly[i][1]\n\n    return abs(a) / 2.0",
  "def order_edges(edges):\n    r\"\"\"Return an ordered traversal of the edges of a two-dimensional polygon.\n\n    Parameters\n    ----------\n    edges: (2, N) numpy.ndarray\n        List of unordered line segments, where each\n        line segment is represented by two unique\n        vertex codes.\n\n    Returns\n    -------\n    ordered_edges: (2, N) numpy.ndarray\n\n    \"\"\"\n    edge = edges[0]\n    edges = edges[1:]\n\n    ordered_edges = [edge]\n\n    num_max = len(edges)\n    while len(edges) > 0 and num_max > 0:\n\n        match = edge[1]\n\n        for search_edge in edges:\n            vertex = search_edge[0]\n            if match == vertex:\n                edge = search_edge\n                edges.remove(edge)\n                ordered_edges.append(search_edge)\n                break\n        num_max -= 1\n\n    return ordered_edges",
  "def expanded_indexer(key, ndim):\n    \"\"\"Expand an indexer to a tuple with length ndim.\n\n    Given a key for indexing an ndarray, return an equivalent key which is a\n    tuple with length equal to the number of dimensions.\n    The expansion is done by replacing all `Ellipsis` items with the right\n    number of full slices and then padding the key with full slices so that it\n    reaches the appropriate dimensionality.\n    \"\"\"\n    if not isinstance(key, tuple):\n        # numpy treats non-tuple keys equivalent to tuples of length 1\n        key = (key,)\n    new_key = []\n    # handling Ellipsis right is a little tricky, see:\n    # http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing\n    found_ellipsis = False\n    for k in key:\n        if k is Ellipsis:\n            if not found_ellipsis:\n                new_key.extend((ndim + 1 - len(key)) * [slice(None)])\n                found_ellipsis = True\n            else:\n                new_key.append(slice(None))\n        else:\n            new_key.append(k)\n    if len(new_key) > ndim:\n        raise IndexError('too many indices')\n    new_key.extend((ndim - len(new_key)) * [slice(None)])\n    return tuple(new_key)",
  "def is_dict_like(value):\n    \"\"\"Check if value is dict-like.\"\"\"\n    return hasattr(value, 'keys') and hasattr(value, '__getitem__')",
  "def either_dict_or_kwargs(pos_kwargs, kw_kwargs, func_name):\n    \"\"\"Ensure dict-like argument from either positional or keyword arguments.\"\"\"\n    if pos_kwargs is not None:\n        if not is_dict_like(pos_kwargs):\n            raise ValueError('the first argument to .{} must be a '\n                             'dictionary'.format(func_name))\n        if kw_kwargs:\n            raise ValueError('cannot specify both keyword and positional arguments to '\n                             '.{}'.format(func_name))\n        return pos_kwargs\n    else:\n        return kw_kwargs",
  "def to_dec_deg(dms):\n    \"\"\"Convert to decimal degrees.\"\"\"\n    if not dms:\n        return 0.\n    deg, minutes = dms.split()\n    side = minutes[-1]\n    minutes = minutes[:2]\n    float_deg = int(deg) + int(minutes) / 60.\n    return float_deg if side in ('N', 'E') else -float_deg",
  "def _read_station_table(input_file=None):\n    \"\"\"Read in the GEMPAK station table.\n\n    Yields tuple of station ID and `Station` for each entry.\n    \"\"\"\n    if input_file is None:\n        input_file = get_test_data('sfstns.tbl', as_file_obj=False)\n    with open(input_file) as station_file:\n        for line in station_file:\n            stid = line[:9].strip()\n            synop_id = int(line[9:16].strip())\n            name = line[16:49].strip()\n            state = line[49:52].strip()\n            country = line[52:55].strip()\n            lat = int(line[55:61].strip()) / 100.\n            lon = int(line[61:68].strip()) / 100.\n            alt = int(line[68:74].strip())\n            yield stid, Station(stid, synop_id=synop_id, name=name.title(), latitude=lat,\n                                longitude=lon, altitude=alt, country=country, state=state,\n                                source=input_file)",
  "def _read_master_text_file(input_file=None):\n    \"\"\"Read in the master text file.\n\n    Yields tuple of station ID and `Station` for each entry.\n    \"\"\"\n    if input_file is None:\n        input_file = get_test_data('master.txt', as_file_obj=False)\n    with open(input_file) as station_file:\n        station_file.readline()\n        for line in station_file:\n            state = line[:3].strip()\n            name = line[3:20].strip().replace('_', ' ')\n            stid = line[20:25].strip()\n            synop_id = line[32:38].strip()\n            lat = to_dec_deg(line[39:46].strip())\n            lon = to_dec_deg(line[47:55].strip())\n            alt_part = line[55:60].strip()\n            alt = int(alt_part or 0.)\n            if stid:\n                if stid[0] in ('P', 'K'):\n                    country = 'US'\n                else:\n                    country = state\n                    state = '--'\n            yield stid, Station(stid, synop_id=synop_id, name=name.title(), latitude=lat,\n                                longitude=lon, altitude=alt, country=country, state=state,\n                                source=input_file)",
  "def _read_station_text_file(input_file=None):\n    \"\"\"Read the station text file.\n\n    Yields tuple of station ID and `Station` for each entry.\n    \"\"\"\n    if input_file is None:\n        input_file = get_test_data('stations.txt', as_file_obj=False)\n    with open(input_file) as station_file:\n        for line in station_file:\n            if line[0] == '!':\n                continue\n            lat = line[39:45].strip()\n            if not lat or lat == 'LAT':\n                continue\n            lat = to_dec_deg(lat)\n            state = line[:3].strip()\n            name = line[3:20].strip().replace('_', ' ')\n            stid = line[20:25].strip()\n            synop_id = line[32:38].strip()\n            lon = to_dec_deg(line[47:55].strip())\n            alt = int(line[55:60].strip())\n            country = line[81:83].strip()\n            yield stid, Station(stid, synop_id=synop_id, name=name.title(), latitude=lat,\n                                longitude=lon, altitude=alt, country=country, state=state,\n                                source=input_file)",
  "def _read_airports_file(input_file=None):\n    \"\"\"Read the airports file.\"\"\"\n    if input_file is None:\n        input_file = get_test_data('airport-codes.csv', as_file_obj=False)\n    df = pd.read_csv(input_file)\n    return pd.DataFrame({'id': df.ident.values, 'synop_id': 99999,\n                         'latitude': df.latitude_deg.values,\n                         'longitude': df.longitude_deg.values,\n                         'altitude': units.Quantity(df.elevation_ft.values, 'ft').to('m').m,\n                         'country': df.iso_region.str.split('-', n=1, expand=True)[1].values,\n                         'source': input_file\n                         }).to_dict()",
  "class StationLookup(Mapping):\n    \"\"\"Look up station information from multiple sources.\n\n    This class follows the `Mapping` protocol with station ID as the key. This makes it\n    possible to e.g. iterate over all locations and get all of a certain criteria:\n\n    >>> import metpy.io\n    >>> conus_stations = [s for s in metpy.io.station_info if s.startswith('K')]\n    >>> conus_stations[:3]\n    ['KEET', 'K8A0', 'KALX']\n    \"\"\"\n\n    @cached_property\n    def tables(self):\n        \"\"\"Return an iterable mapping combining all the tables.\"\"\"\n        return ChainMap(dict(_read_station_table()),\n                        dict(_read_master_text_file()),\n                        dict(_read_station_text_file()),\n                        dict(_read_airports_file()))\n\n    def __len__(self):\n        \"\"\"Get the number of stations.\"\"\"\n        return len(self.tables)\n\n    def __iter__(self):\n        \"\"\"Allow iteration over the stations.\"\"\"\n        return iter(self.tables)\n\n    def __getitem__(self, stid):\n        \"\"\"Lookup station information from the ID.\"\"\"\n        try:\n            return self.tables[stid]\n        except KeyError:\n            raise KeyError(f'No station information for {stid}') from None",
  "def add_station_lat_lon(df, stn_var=None):\n    \"\"\"Lookup station information to add the station latitude and longitude to the DataFrame.\n\n    This function will add two columns to the DataFrame ('latitude' and 'longitude') after\n    looking up all unique station identifiers available in the DataFrame.\n\n    Parameters\n    ----------\n    df : `pandas.DataFrame`\n        The DataFrame that contains the station observations\n    stn_var : str, optional\n        The string of the variable name that represents the station in the DataFrame. If not\n        provided, 'station', 'stid', and 'station_id' are tried in that order.\n\n    Returns\n    -------\n    `pandas.DataFrame` that contains original Dataframe now with the latitude and longitude\n    values for each location found in :data:`!station_info`.\n    \"\"\"\n\n    def key_finder(df):\n        names_to_try = ('station', 'stid', 'station_id')\n        for id_name in names_to_try:\n            if id_name in df:\n                return id_name\n        raise KeyError('Second argument not provided to add_station_lat_lon, but none of '\n                       f'{names_to_try} were found.')\n\n    df['latitude'] = None\n    df['longitude'] = None\n    if stn_var is None:\n        stn_var = key_finder(df)\n    for stn in df[stn_var].unique():\n        try:\n            info = station_info[stn]\n            df.loc[df[stn_var] == stn, 'latitude'] = info.latitude\n            df.loc[df[stn_var] == stn, 'longitude'] = info.longitude\n        except KeyError:\n            df.loc[df[stn_var] == stn, 'latitude'] = np.nan\n            df.loc[df[stn_var] == stn, 'longitude'] = np.nan\n    return df",
  "def tables(self):\n        \"\"\"Return an iterable mapping combining all the tables.\"\"\"\n        return ChainMap(dict(_read_station_table()),\n                        dict(_read_master_text_file()),\n                        dict(_read_station_text_file()),\n                        dict(_read_airports_file()))",
  "def __len__(self):\n        \"\"\"Get the number of stations.\"\"\"\n        return len(self.tables)",
  "def __iter__(self):\n        \"\"\"Allow iteration over the stations.\"\"\"\n        return iter(self.tables)",
  "def __getitem__(self, stid):\n        \"\"\"Lookup station information from the ID.\"\"\"\n        try:\n            return self.tables[stid]\n        except KeyError:\n            raise KeyError(f'No station information for {stid}') from None",
  "def key_finder(df):\n        names_to_try = ('station', 'stid', 'station_id')\n        for id_name in names_to_try:\n            if id_name in df:\n                return id_name\n        raise KeyError('Second argument not provided to add_station_lat_lon, but none of '\n                       f'{names_to_try} were found.')",
  "def _tree_repr_(self):\n    \"\"\"Produce string representation of a TreeNodex object.\"\"\"\n    rep = self.__class__.__name__ + '('\n    args = []\n    for key, value in self.__dict__.items():\n        if key == 'elements':\n            continue\n        args.append(key + '=' + repr(value))\n        dict_str = ', '.join(args) + ')'\n    rep += dict_str\n    return rep",
  "def parse_metar_to_dataframe(metar_text, *, year=None, month=None):\n    \"\"\"Parse a single METAR report into a Pandas DataFrame.\n\n    Takes a METAR string in a text form, and creates a `pandas.DataFrame` including the\n    essential information (not including the remarks)\n\n    The parser follows the WMO format, allowing for missing data and assigning\n    nan values where necessary. The WMO code is also provided for current weather,\n    which can be utilized when plotting.\n\n    Parameters\n    ----------\n    metar_text : str\n        The METAR report\n    year : int, optional\n        Year in which observation was taken, defaults to current year. Keyword-only argument.\n    month : int, optional\n        Month in which observation was taken, defaults to current month. Keyword-only argument.\n\n    Returns\n    -------\n    `pandas.DataFrame`\n\n    \"\"\"\n    return _metars_to_dataframe([metar_text], year=year, month=month)",
  "def parse_metar(metar_text, year, month, station_metadata=station_info):\n    \"\"\"Parse a METAR report in text form into a list of named tuples.\n\n    Parameters\n    ----------\n    metar_text : str\n        The METAR report\n    station_metadata : dict\n        Mapping of station identifiers to station metadata\n    year : int\n        Reported year of observation for constructing 'date_time'\n    month : int\n        Reported month of observation for constructing 'date_time'\n\n    Returns\n    -------\n    metar : namedtuple\n        Named tuple of parsed METAR fields\n\n    Notes\n    -----\n    Returned data has named tuples with the following attributes:\n\n    * 'station_id': Station Identifier (ex. KLOT)\n    * 'latitude': Latitude of the observation, measured in degrees\n    * 'longitude': Longitude of the observation, measured in degrees\n    * 'elevation': Elevation of the observation above sea level, measured in meters\n    * 'date_time': Date and time of the observation, datetime object\n    * 'wind_direction': Direction the wind is coming from, measured in degrees\n    * 'wind_speed': Wind speed, measured in knots\n    * 'wind_gust': Wind gust, measured in knots\n    * 'current_wx1': Current weather (1 of 3)\n    * 'current_wx2': Current weather (2 of 3)\n    * 'current_wx3': Current weather (3 of 3)\n    * 'skyc1': Sky cover (ex. FEW)\n    * 'skylev1': Height of sky cover 1, measured in feet\n    * 'skyc2': Sky cover (ex. OVC)\n    * 'skylev2': Height of sky cover 2, measured in feet\n    * 'skyc3': Sky cover (ex. FEW)\n    * 'skylev3': Height of sky cover 3, measured in feet\n    * 'skyc4': Sky cover (ex. CLR)\n    * 'skylev4:': Height of sky cover 4, measured in feet\n    * 'cloudcover': Cloud coverage measured in oktas, taken from maximum of sky cover values\n    * 'temperature': Temperature, measured in degrees Celsius\n    * 'dewpoint': Dewpoint, measured in degrees Celsius\n    * 'altimeter': Altimeter value, measured in inches of mercury\n    * 'current_wx1_symbol': Current weather symbol (1 of 3), WMO integer code from [WMO306]_\n      Attachment IV\n    * 'current_wx2_symbol': Current weather symbol (2 of 3), WMO integer code from [WMO306]_\n      Attachment IV\n    * 'current_wx3_symbol': Current weather symbol (3 of 3), WMO integer code from [WMO306]_\n      Attachment IV\n    * 'visibility': Visibility distance, measured in meters\n    * 'remarks': Remarks (unparsed) in the report\n\n    \"\"\"\n    from ..plots.wx_symbols import wx_code_to_numeric\n\n    # Decode the data using the parser (built using Canopy) the parser utilizes a grammar\n    # file which follows the format structure dictated by the WMO Handbook, but has the\n    # flexibility to decode the METAR text when there are missing or incorrectly\n    # encoded values\n    tree = parse(metar_text)\n\n    # Station ID which is used to find the latitude, longitude, and elevation\n    station_id = tree.siteid.text.strip()\n\n    # Extract the latitude and longitude values from 'master' dictionary\n    try:\n        info = station_metadata[station_id]\n        lat = info.latitude\n        lon = info.longitude\n        elev = info.altitude\n    except KeyError:\n        lat = np.nan\n        lon = np.nan\n        elev = np.nan\n\n    # Set the datetime, day, and time_utc\n    try:\n        day_time_utc = tree.datetime.text.strip()\n        day = int(day_time_utc[0:2])\n        hour = int(day_time_utc[2:4])\n        minute = int(day_time_utc[4:6])\n        date_time = datetime(year, month, day, hour, minute)\n    except ValueError:\n        date_time = np.nan\n\n    # Set the wind values\n    wind_units = 'kts'\n    try:\n        # If there are missing wind values, set wind speed and wind direction to nan\n        if ('/' in tree.wind.text) or (tree.wind.text == 'KT') or (tree.wind.text == ''):\n            wind_dir = np.nan\n            wind_spd = np.nan\n        # If the wind direction is variable, set wind direction to nan but keep the wind speed\n        else:\n            wind_spd = float(tree.wind.wind_spd.text)\n            if 'MPS' in tree.wind.text:\n                wind_units = 'm/s'\n                wind_spd = units.Quantity(wind_spd, wind_units).m_as('knots')\n            if (tree.wind.wind_dir.text == 'VRB') or (tree.wind.wind_dir.text == 'VAR'):\n                wind_dir = np.nan\n            else:\n                wind_dir = int(tree.wind.wind_dir.text)\n    # If there are any errors, return nan\n    except ValueError:\n        wind_dir = np.nan\n        wind_spd = np.nan\n\n    # Parse out the wind gust field\n    if 'G' in tree.wind.text:\n        wind_gust = units.Quantity(float(tree.wind.gust.text.strip()[1:]),\n                                   wind_units).m_as('knots')\n    else:\n        wind_gust = np.nan\n\n    # Handle visibility\n    try:\n        if tree.vis.text.endswith('SM'):\n            visibility = 0\n            # Strip off the SM and any whitespace around the value and any leading 'M'\n            vis_str = tree.vis.text[:-2].strip().lstrip('M')\n\n            # Case of e.g. 1 1/4SM\n            if ' ' in vis_str:\n                whole, vis_str = vis_str.split(maxsplit=1)\n                visibility += int(whole)\n\n            # Handle fraction regardless\n            if '/' in vis_str:\n                num, denom = vis_str.split('/', maxsplit=1)\n                if int(denom) == 0:\n                    raise ValueError('Visibility denominator is 0.')\n                visibility += int(num) / int(denom)\n            else:  # Should be getting all cases of whole number without fraction\n                visibility += int(vis_str)\n            visibility = units.Quantity(visibility, 'miles').m_as('meter')\n        # CAVOK means vis is \"at least 10km\" and no significant clouds or weather\n        elif 'CAVOK' in tree.vis.text:\n            visibility = 10000\n        elif not tree.vis.text or tree.vis.text.strip() == '////':\n            visibility = np.nan\n        else:\n            # Only worry about the first 4 characters (digits) and ignore possible 'NDV'\n            visibility = int(tree.vis.text.strip()[:4])\n    # If there are any errors, return nan\n    except ValueError:\n        visibility = np.nan\n\n    # Set the weather symbols\n    # If the weather symbol is missing, set values to nan\n    current_wx = []\n    current_wx_symbol = []\n    if tree.curwx.text.strip() not in ('', '//', 'NSW'):\n        current_wx = tree.curwx.text.strip().split()\n\n        # Handle having e.g. '+' and 'TSRA' parsed into separate items\n        if current_wx[0] in ('-', '+') and current_wx[1]:\n            current_wx[0] += current_wx[1]\n            current_wx.pop(1)\n\n        current_wx_symbol = wx_code_to_numeric(current_wx).tolist()\n    while len(current_wx) < 3:\n        current_wx.append(np.nan)\n    while len(current_wx_symbol) < 3:\n        current_wx_symbol.append(0)\n\n    # Set the sky conditions\n    skyc = [np.nan] * 4\n    skylev = [np.nan] * 4\n    if tree.skyc.text[1:3] == 'VV':\n        skyc[0] = 'VV'\n        level = tree.skyc.text.strip()[2:5]\n        skylev[0] = np.nan if not level or '/' in level else 100 * int(level)\n    else:\n        for ind, part in enumerate(tree.skyc.text.strip().split(maxsplit=3)):\n            cover = part[:3]\n            level = part[3:6]  # Strips off any ending text like in FEW017CB\n            if '/' not in cover:\n                skyc[ind] = cover\n            if level and '/' not in level:\n                with contextlib.suppress(ValueError):\n                    skylev[ind] = float(level) * 100\n\n    # Set the cloud cover variable (measured in oktas)\n    if 'OVC' in tree.skyc.text or 'VV' in tree.skyc.text:\n        cloudcover = 8\n    elif 'BKN' in tree.skyc.text:\n        cloudcover = 6\n    elif 'SCT' in tree.skyc.text:\n        cloudcover = 4\n    elif 'FEW' in tree.skyc.text:\n        cloudcover = 2\n    elif ('SKC' in tree.skyc.text or 'NCD' in tree.skyc.text or 'NSC' in tree.skyc.text\n          or 'CLR' in tree.skyc.text or 'CAVOK' in tree.vis.text):\n        cloudcover = 0\n    else:\n        cloudcover = 10\n\n    # Set the temperature and dewpoint\n    temp = np.nan\n    dewp = np.nan\n    if tree.temp_dewp.text and tree.temp_dewp.text != ' MM/MM':\n        with contextlib.suppress(ValueError):\n            temp = float(tree.temp_dewp.temp.text[-2:])\n            if 'M' in tree.temp_dewp.temp.text:\n                temp *= -1\n\n        with contextlib.suppress(ValueError):\n            dewp = float(tree.temp_dewp.dewp.text[-2:])\n            if 'M' in tree.temp_dewp.dewp.text:\n                dewp *= -1\n\n    # Set the altimeter value and sea level pressure\n    if tree.altim.text:\n        val = float(tree.altim.text.strip()[1:5])\n        altim = val / 100 if val > 1100 else units.Quantity(val, 'hPa').m_as('inHg')\n    else:\n        altim = np.nan\n\n    # Strip off extraneous stuff off the remarks section\n    remarks = tree.remarks.text.lstrip().rstrip('= ')\n    if remarks.startswith('RMK'):\n        remarks = remarks[3:].strip()\n\n    # Returns a named tuple with all the relevant variables\n    return Metar(station_id, lat, lon, elev, date_time, wind_dir, wind_spd, wind_gust,\n                 visibility, current_wx[0], current_wx[1], current_wx[2], skyc[0], skylev[0],\n                 skyc[1], skylev[1], skyc[2], skylev[2], skyc[3], skylev[3], cloudcover, temp,\n                 dewp, altim, current_wx_symbol[0], current_wx_symbol[1], current_wx_symbol[2],\n                 remarks)",
  "def parse_metar_file(filename, *, year=None, month=None):\n    \"\"\"Parse a text file containing multiple METAR reports and/or text products.\n\n    Parameters\n    ----------\n    filename : str or file-like object\n        If str, the name of the file to be opened. If `filename` is a file-like object,\n        this will be read from directly.\n    year : int, optional\n        Year in which observation was taken, defaults to current year. Keyword-only argument.\n    month : int, optional\n        Month in which observation was taken, defaults to current month. Keyword-only argument.\n\n    Returns\n    -------\n    `pandas.DataFrame`\n\n    \"\"\"\n    # Function to merge METARs\n    def full_metars(x, prefix='     '):\n        tmp = []\n        for i in x:\n            # Skip any blank lines\n            if not i.strip():\n                continue\n            # No prefix signals a new report, so yield\n            if not i.startswith(prefix) and tmp:\n                yield ' '.join(tmp)\n                tmp = []\n            tmp.append(i.strip())\n\n        # Handle any leftovers\n        if tmp:\n            yield ' '.join(tmp)\n\n    # Open the file\n    with contextlib.closing(open_as_needed(filename, 'rt')) as myfile:\n        # Merge multi-line METARs into a single report--drop reports that are too short to\n        # be a METAR with a robust amount of data.\n        return _metars_to_dataframe(filter(lambda m: len(m) > 25, full_metars(myfile)),\n                                    year=year, month=month)",
  "def _metars_to_dataframe(metar_iter, *, year=None, month=None):\n    \"\"\"Turn an iterable of METAR reports into a DataFrame.\n\n    Notes\n    -----\n    The output has the following columns:\n\n    * 'station_id': Station Identifier (ex. KLOT)\n    * 'latitude': Latitude of the observation, measured in degrees\n    * 'longitude': Longitude of the observation, measured in degrees\n    * 'elevation': Elevation of the observation above sea level, measured in meters\n    * 'date_time': Date and time of the observation, datetime object\n    * 'wind_direction': Direction the wind is coming from, measured in degrees\n    * 'wind_speed': Wind speed, measured in knots\n    * 'wind_gust': Wind gust, measured in knots\n    * 'visibility': Visibility distance, measured in meters\n    * 'current_wx1': Current weather (1 of 3)\n    * 'current_wx2': Current weather (2 of 3)\n    * 'current_wx3': Current weather (3 of 3)\n    * 'low_cloud_type': Low-level sky cover (ex. FEW)\n    * 'low_cloud_level': Height of low-level sky cover, measured in feet\n    * 'medium_cloud_type': Medium-level sky cover (ex. OVC)\n    * 'medium_cloud_level': Height of medium-level sky cover, measured in feet\n    * 'high_cloud_type': High-level sky cover (ex. FEW)\n    * 'high_cloud_level': Height of high-level sky cover, measured in feet\n    * 'highest_cloud_type': Highest-level Sky cover (ex. CLR)\n    * 'highest_cloud_level:': Height of highest-level sky cover, measured in feet\n    * 'cloud_coverage': Cloud cover measured in oktas, taken from maximum of sky cover values\n    * 'air_temperature': Temperature, measured in degrees Celsius\n    * 'dew_point_temperature': Dew point, measured in degrees Celsius\n    * 'altimeter': Altimeter value, measured in inches of mercury\n    * 'remarks': Any remarks section in the report\n    * 'current_wx1_symbol': Current weather symbol (1 of 3), WMO integer code from [WMO306]_\n      Attachment IV\n    * 'current_wx2_symbol': Current weather symbol (2 of 3), WMO integer code from [WMO306]_\n      Attachment IV\n    * 'current_wx3_symbol': Current weather symbol (3 of 3), WMO integer code from [WMO306]_\n      Attachment IV\n    * 'air_pressure_at_sea_level': Sea level pressure, derived from temperature, elevation\n      and altimeter value\n    * 'eastward_wind': Eastward component (u-component) of the wind vector, measured in knots\n    * 'northward_wind': Northward component (v-component) of the wind vector, measured in knots\n\n    \"\"\"\n    from ..calc import altimeter_to_sea_level_pressure, wind_components\n\n    # Defaults year and/or month to present reported date if not provided\n    if year is None or month is None:\n        now = datetime.now(timezone.utc)\n        year = now.year if year is None else year\n        month = now.month if month is None else month\n\n    # Try to parse each METAR that is given\n    metars = []\n    for metar in metar_iter:\n        with contextlib.suppress(ParseError):\n            # Parse the string of text and assign to values within the named tuple\n            metars.append(parse_metar(metar, year=year, month=month))\n\n    # Take the list of Metar objects and turn it into a DataFrame with appropriate columns\n    df = pd.DataFrame(metars)\n    df.set_index('station_id', inplace=True, drop=False)\n    df.rename(columns={'skyc1': 'low_cloud_type', 'skylev1': 'low_cloud_level',\n                       'skyc2': 'medium_cloud_type', 'skylev2': 'medium_cloud_level',\n                       'skyc3': 'high_cloud_type', 'skylev3': 'high_cloud_level',\n                       'skyc4': 'highest_cloud_type', 'skylev4': 'highest_cloud_level',\n                       'cloudcover': 'cloud_coverage', 'temperature': 'air_temperature',\n                       'dewpoint': 'dew_point_temperature'}, inplace=True)\n\n    # Drop duplicate values\n    df.drop_duplicates(subset=['date_time', 'latitude', 'longitude'], keep='last',\n                       inplace=True)\n\n    # Calculate sea-level pressure from function in metpy.calc\n    df['air_pressure_at_sea_level'] = altimeter_to_sea_level_pressure(\n        units.Quantity(df.altimeter.values, col_units['altimeter']),\n        units.Quantity(df.elevation.values, col_units['elevation']),\n        units.Quantity(df.air_temperature.values, col_units['air_temperature'])).m_as('hPa')\n\n    # Use get wind components and assign them to eastward and northward winds\n    u, v = wind_components(\n        units.Quantity(df.wind_speed.values, col_units['wind_speed']),\n        units.Quantity(df.wind_direction.values, col_units['wind_direction']))\n    df['eastward_wind'] = u.m\n    df['northward_wind'] = v.m\n\n    # Round altimeter and sea-level pressure values\n    df['altimeter'] = df.altimeter.round(2)\n    df['air_pressure_at_sea_level'] = df.air_pressure_at_sea_level.round(2)\n\n    # Set the units for the dataframe--filter out warning from Pandas\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', UserWarning)\n        df.units = col_units\n\n    return df",
  "def full_metars(x, prefix='     '):\n        tmp = []\n        for i in x:\n            # Skip any blank lines\n            if not i.strip():\n                continue\n            # No prefix signals a new report, so yield\n            if not i.startswith(prefix) and tmp:\n                yield ' '.join(tmp)\n                tmp = []\n            tmp.append(i.strip())\n\n        # Handle any leftovers\n        if tmp:\n            yield ' '.join(tmp)",
  "def _make_datetime(s):\n    r\"\"\"Convert 7 bytes from a GINI file to a `datetime` instance.\"\"\"\n    year, month, day, hour, minute, second, cs = s\n    if year < 70:\n        year += 100\n    return datetime(1900 + year, month, day, hour, minute, second, 10000 * cs)",
  "def _scaled_int(s):\n    r\"\"\"Convert a 3 byte string to a signed integer value.\"\"\"\n    # Get leftmost bit (sign) as 1 (if 0) or -1 (if 1)\n    sign = 1 - ((s[0] & 0x80) >> 6)\n\n    # Combine remaining bits\n    int_val = (((s[0] & 0x7f) << 16) | (s[1] << 8) | s[2])\n    log.debug('Source: %s Int: %x Sign: %d', ' '.join(hex(c) for c in s), int_val, sign)\n\n    # Return scaled and with proper sign\n    return (sign * int_val) / 10000.",
  "def _name_lookup(names):\n    r\"\"\"Create an io helper to convert an integer to a named value.\"\"\"\n    mapper = dict(zip(range(len(names)), names))\n\n    def lookup(val):\n        return mapper.get(val, 'UnknownValue')\n    return lookup",
  "class GiniProjection(Enum):\n    r\"\"\"Represents projection values in GINI files.\"\"\"\n\n    mercator = 1\n    lambert_conformal = 3\n    polar_stereographic = 5",
  "class GiniFile(AbstractDataStore):\n    \"\"\"A class that handles reading the GINI format satellite images from the NWS.\n\n    This class attempts to decode every byte that is in a given GINI file.\n\n    Notes\n    -----\n    The internal data structures that things are decoded into are subject to change.\n\n    \"\"\"\n\n    missing = 255\n    wmo_finder = re.compile('(T\\\\w{3}\\\\d{2})[\\\\s\\\\w\\\\d]+\\\\w*(\\\\w{3})\\r\\r\\n')\n\n    crafts = ['Unknown', 'Unknown', 'Miscellaneous', 'JERS', 'ERS/QuikSCAT', 'POES/NPOESS',\n              'Composite', 'DMSP', 'GMS', 'METEOSAT', 'GOES-7', 'GOES-8', 'GOES-9',\n              'GOES-10', 'GOES-11', 'GOES-12', 'GOES-13', 'GOES-14', 'GOES-15', 'GOES-16']\n\n    sectors = ['NH Composite', 'East CONUS', 'West CONUS', 'Alaska Regional',\n               'Alaska National', 'Hawaii Regional', 'Hawaii National', 'Puerto Rico Regional',\n               'Puerto Rico National', 'Supernational', 'NH Composite', 'Central CONUS',\n               'East Floater', 'West Floater', 'Central Floater', 'Polar Floater']\n\n    channels = ['Unknown', 'Visible', 'IR (3.9 micron)', 'WV (6.5/6.7 micron)',\n                'IR (11 micron)', 'IR (12 micron)', 'IR (13 micron)', 'IR (1.3 micron)',\n                'Reserved', 'Reserved', 'Reserved', 'Reserved', 'Reserved', 'LI (Imager)',\n                'PW (Imager)', 'Surface Skin Temp (Imager)', 'LI (Sounder)', 'PW (Sounder)',\n                'Surface Skin Temp (Sounder)', 'CAPE', 'Land-sea Temp', 'WINDEX',\n                'Dry Microburst Potential Index', 'Microburst Day Potential Index',\n                'Convective Inhibition', 'Volcano Imagery', 'Scatterometer', 'Cloud Top',\n                'Cloud Amount', 'Rainfall Rate', 'Surface Wind Speed', 'Surface Wetness',\n                'Ice Concentration', 'Ice Type', 'Ice Edge', 'Cloud Water Content',\n                'Surface Type', 'Snow Indicator', 'Snow/Water Content', 'Volcano Imagery',\n                'Reserved', 'Sounder (14.71 micron)', 'Sounder (14.37 micron)',\n                'Sounder (14.06 micron)', 'Sounder (13.64 micron)', 'Sounder (13.37 micron)',\n                'Sounder (12.66 micron)', 'Sounder (12.02 micron)', 'Sounder (11.03 micron)',\n                'Sounder (9.71 micron)', 'Sounder (7.43 micron)', 'Sounder (7.02 micron)',\n                'Sounder (6.51 micron)', 'Sounder (4.57 micron)', 'Sounder (4.52 micron)',\n                'Sounder (4.45 micron)', 'Sounder (4.13 micron)', 'Sounder (3.98 micron)',\n                # Percent Normal TPW found empirically from Service Change Notice 20-03\n                'Sounder (3.74 micron)', 'Sounder (Visible)', 'Percent Normal TPW']\n\n    prod_desc_fmt = NamedStruct([('source', 'b'),\n                                 ('creating_entity', 'b', _name_lookup(crafts)),\n                                 ('sector_id', 'b', _name_lookup(sectors)),\n                                 ('channel', 'b', _name_lookup(channels)),\n                                 ('num_records', 'H'), ('record_len', 'H'),\n                                 ('datetime', '7s', _make_datetime),\n                                 ('projection', 'b', GiniProjection), ('nx', 'H'), ('ny', 'H'),\n                                 ('la1', '3s', _scaled_int), ('lo1', '3s', _scaled_int)\n                                 ], '>', 'ProdDescStart')\n\n    lc_ps_fmt = NamedStruct([('reserved', 'b'), ('lov', '3s', _scaled_int),\n                             ('dx', '3s', _scaled_int), ('dy', '3s', _scaled_int),\n                             ('proj_center', 'b')], '>', 'LambertOrPolarProjection')\n\n    mercator_fmt = NamedStruct([('resolution', 'b'), ('la2', '3s', _scaled_int),\n                                ('lo2', '3s', _scaled_int), ('di', 'H'), ('dj', 'H')\n                                ], '>', 'MercatorProjection')\n\n    prod_desc2_fmt = NamedStruct([('scanning_mode', 'b', Bits(3)),\n                                  ('lat_in', '3s', _scaled_int), ('resolution', 'b'),\n                                  ('compression', 'b'), ('version', 'b'), ('pdb_size', 'H'),\n                                  ('nav_cal', 'b')], '>', 'ProdDescEnd')\n\n    nav_fmt = NamedStruct([('sat_lat', '3s', _scaled_int), ('sat_lon', '3s', _scaled_int),\n                           ('sat_height', 'H'), ('ur_lat', '3s', _scaled_int),\n                           ('ur_lon', '3s', _scaled_int)], '>', 'Navigation')\n\n    def __init__(self, filename):\n        r\"\"\"Create an instance of `GiniFile`.\n\n        Parameters\n        ----------\n        filename : str or file-like object\n            If str, the name of the file to be opened. Gzip-ed files are\n            recognized with the extension ``'.gz'``, as are bzip2-ed files with\n            the extension ``'.bz2'`` If `filename` is a file-like object,\n            this will be read from directly.\n\n        \"\"\"\n        fobj = open_as_needed(filename)\n\n        # Just read in the entire set of data at once\n        with contextlib.closing(fobj):\n            self._buffer = IOBuffer.fromfile(fobj)\n\n        # Pop off the WMO header if we find it\n        self.wmo_code = ''\n        self._process_wmo_header()\n        log.debug('First wmo code: %s', self.wmo_code)\n\n        # Decompress the data if necessary, and if so, pop off new header\n        log.debug('Length before decompression: %s', len(self._buffer))\n        self._buffer = IOBuffer(self._buffer.read_func(zlib_decompress_all_frames))\n        log.debug('Length after decompression: %s', len(self._buffer))\n\n        # Process WMO header inside compressed data if necessary\n        self._process_wmo_header()\n        log.debug('2nd wmo code: %s', self.wmo_code)\n\n        # Read product description start\n        start = self._buffer.set_mark()\n\n        #: :desc: Decoded first section of product description block\n        #: :type: namedtuple\n        self.prod_desc = self._buffer.read_struct(self.prod_desc_fmt)\n        log.debug(self.prod_desc)\n\n        #: :desc: Decoded geographic projection information\n        #: :type: namedtuple\n        self.proj_info = None\n\n        # Handle projection-dependent parts\n        if self.prod_desc.projection in (GiniProjection.lambert_conformal,\n                                         GiniProjection.polar_stereographic):\n            self.proj_info = self._buffer.read_struct(self.lc_ps_fmt)\n        elif self.prod_desc.projection == GiniProjection.mercator:\n            self.proj_info = self._buffer.read_struct(self.mercator_fmt)\n        else:\n            log.warning('Unknown projection: %d', self.prod_desc.projection)\n        log.debug(self.proj_info)\n\n        # Read the rest of the guaranteed product description block (PDB)\n        #: :desc: Decoded second section of product description block\n        #: :type: namedtuple\n        self.prod_desc2 = self._buffer.read_struct(self.prod_desc2_fmt)\n        log.debug(self.prod_desc2)\n\n        if self.prod_desc2.nav_cal not in (0, -128):  # TODO: See how GEMPAK/MCIDAS parses\n            # Only warn if there actually seems to be useful navigation data\n            if self._buffer.get_next(self.nav_fmt.size) != b'\\x00' * self.nav_fmt.size:\n                log.warning('Navigation/Calibration unhandled: %d', self.prod_desc2.nav_cal)\n            if self.prod_desc2.nav_cal in (1, 2):\n                self.navigation = self._buffer.read_struct(self.nav_fmt)\n                log.debug(self.navigation)\n\n        # Catch bad PDB with size set to 0\n        if self.prod_desc2.pdb_size == 0:\n            log.warning('Adjusting bad PDB size from 0 to 512.')\n            self.prod_desc2 = self.prod_desc2._replace(pdb_size=512)\n\n        # Jump past the remaining empty bytes in the product description block\n        self._buffer.jump_to(start, self.prod_desc2.pdb_size)\n\n        # Read the actual raster--unless it's PNG compressed, in which case that happens later\n        blob = self._buffer.read(self.prod_desc.num_records * self.prod_desc.record_len)\n\n        # Check for end marker\n        end = self._buffer.read(self.prod_desc.record_len)\n        if end != b''.join(repeat(b'\\xff\\x00', self.prod_desc.record_len // 2)):\n            log.warning('End marker not as expected: %s', end)\n\n        # Check to ensure that we processed all of the data\n        if not self._buffer.at_end():\n            if not blob:\n                log.debug('No data read yet, trying to decompress remaining data as an image.')\n                from matplotlib.image import imread\n                blob = (imread(BytesIO(self._buffer.read())) * 255).astype('uint8')\n            else:\n                log.warning('Leftover unprocessed data beyond EOF marker: %s',\n                            self._buffer.get_next(10))\n\n        self.data = np.array(blob).reshape((self.prod_desc.ny,\n                                            self.prod_desc.nx))\n\n    def _process_wmo_header(self):\n        \"\"\"Read off the WMO header from the file, if necessary.\"\"\"\n        data = self._buffer.get_next(64).decode('utf-8', 'ignore')\n        match = self.wmo_finder.search(data)\n        if match:\n            self.wmo_code = match.groups()[0]\n            self.siteID = match.groups()[-1]\n            self._buffer.skip(match.end())\n\n    def __str__(self):\n        \"\"\"Return a string representation of the product.\"\"\"\n        parts = [self.__class__.__name__ + ': {0.creating_entity} {0.sector_id} {0.channel}',\n                 'Time: {0.datetime}', 'Size: {0.ny}x{0.nx}',\n                 'Projection: {0.projection.name}',\n                 'Lower Left Corner (Lon, Lat): ({0.lo1}, {0.la1})',\n                 'Resolution: {1.resolution}km']\n        return '\\n\\t'.join(parts).format(self.prod_desc, self.prod_desc2)\n\n    def _make_proj_var(self):\n        proj_info = self.proj_info\n        prod_desc2 = self.prod_desc2\n        attrs = {'earth_radius': 6371200.0}\n        if self.prod_desc.projection == GiniProjection.lambert_conformal:\n            attrs['grid_mapping_name'] = 'lambert_conformal_conic'\n            attrs['standard_parallel'] = prod_desc2.lat_in\n            attrs['longitude_of_central_meridian'] = proj_info.lov\n            attrs['latitude_of_projection_origin'] = prod_desc2.lat_in\n        elif self.prod_desc.projection == GiniProjection.polar_stereographic:\n            attrs['grid_mapping_name'] = 'polar_stereographic'\n            attrs['straight_vertical_longitude_from_pole'] = proj_info.lov\n            attrs['latitude_of_projection_origin'] = -90 if proj_info.proj_center else 90\n            attrs['standard_parallel'] = 60.0  # See Note 2 for Table 4.4A in ICD\n        elif self.prod_desc.projection == GiniProjection.mercator:\n            attrs['grid_mapping_name'] = 'mercator'\n            attrs['longitude_of_projection_origin'] = self.prod_desc.lo1\n            attrs['latitude_of_projection_origin'] = self.prod_desc.la1\n            attrs['standard_parallel'] = prod_desc2.lat_in\n        else:\n            raise NotImplementedError(\n                f'Unhandled GINI Projection: {self.prod_desc.projection}')\n\n        return 'projection', Variable((), 0, attrs)\n\n    def _make_time_var(self):\n        base_time = self.prod_desc.datetime.replace(hour=0, minute=0, second=0, microsecond=0)\n        offset = self.prod_desc.datetime - base_time\n        time_var = Variable((), data=offset.seconds + offset.microseconds / 1e6,\n                            attrs={'units': 'seconds since ' + base_time.isoformat()})\n\n        return 'time', time_var\n\n    def _get_proj_and_res(self):\n        import pyproj\n\n        proj_info = self.proj_info\n        prod_desc2 = self.prod_desc2\n\n        kwargs = {'a': 6371200.0, 'b': 6371200.0}\n        if self.prod_desc.projection == GiniProjection.lambert_conformal:\n            kwargs['proj'] = 'lcc'\n            kwargs['lat_0'] = prod_desc2.lat_in\n            kwargs['lon_0'] = proj_info.lov\n            kwargs['lat_1'] = prod_desc2.lat_in\n            kwargs['lat_2'] = prod_desc2.lat_in\n            dx, dy = proj_info.dx, proj_info.dy\n        elif self.prod_desc.projection == GiniProjection.polar_stereographic:\n            kwargs['proj'] = 'stere'\n            kwargs['lon_0'] = proj_info.lov\n            kwargs['lat_0'] = -90 if proj_info.proj_center else 90\n            kwargs['lat_ts'] = 60.0  # See Note 2 for Table 4.4A in ICD\n            kwargs['x_0'] = False  # Easting\n            kwargs['y_0'] = False  # Northing\n            dx, dy = proj_info.dx, proj_info.dy\n        elif self.prod_desc.projection == GiniProjection.mercator:\n            kwargs['proj'] = 'merc'\n            kwargs['lat_0'] = self.prod_desc.la1\n            kwargs['lon_0'] = self.prod_desc.lo1\n            kwargs['lat_ts'] = prod_desc2.lat_in\n            kwargs['x_0'] = False  # Easting\n            kwargs['y_0'] = False  # Northing\n            dx, dy = prod_desc2.resolution, prod_desc2.resolution\n\n        return pyproj.Proj(**kwargs), dx, dy\n\n    def _make_coord_vars(self):\n        proj, dx, dy = self._get_proj_and_res()\n\n        # Get projected location of lower left point\n        x0, y0 = proj(self.prod_desc.lo1, self.prod_desc.la1)\n\n        # Coordinate variable for x\n        xlocs = x0 + np.arange(self.prod_desc.nx) * (1000. * dx)\n        attrs = {'units': 'm', 'long_name': 'x coordinate of projection',\n                 'standard_name': 'projection_x_coordinate'}\n        x_var = Variable(('x',), xlocs, attrs)\n\n        # Now y--Need to flip y because we calculated from the lower left corner,\n        # but the raster data is stored with top row first.\n        ylocs = (y0 + np.arange(self.prod_desc.ny) * (1000. * dy))[::-1]\n        attrs = {'units': 'm', 'long_name': 'y coordinate of projection',\n                 'standard_name': 'projection_y_coordinate'}\n        y_var = Variable(('y',), ylocs, attrs)\n\n        # Get the two-D lon,lat grid as well\n        x, y = np.meshgrid(xlocs, ylocs)\n        lon, lat = proj(x, y, inverse=True)\n\n        lon_var = Variable(('y', 'x'), data=lon,\n                           attrs={'long_name': 'longitude', 'units': 'degrees_east'})\n        lat_var = Variable(('y', 'x'), data=lat,\n                           attrs={'long_name': 'latitude', 'units': 'degrees_north'})\n\n        return [('x', x_var), ('y', y_var), ('lon', lon_var), ('lat', lat_var)]\n\n    def _make_data_vars(self):\n        proj_var_name, proj_var = self._make_proj_var()\n        name = self.prod_desc.channel\n        if '(' in name:\n            name = name.split('(')[0].rstrip()\n\n        missing_val = self.missing\n        attrs = {'long_name': self.prod_desc.channel, 'missing_value': missing_val,\n                 'coordinates': 'lon lat time', 'grid_mapping': proj_var_name}\n        data_var = Variable(('y', 'x'), data=self.data, attrs=attrs)\n        return [(proj_var_name, proj_var), (name, data_var)]\n\n    def get_variables(self):\n        \"\"\"Get all variables in the file.\n\n        This is used by `xarray.open_dataset`.\n\n        \"\"\"\n        variables = [self._make_time_var()]\n        variables.extend(self._make_coord_vars())\n        variables.extend(self._make_data_vars())\n\n        return FrozenDict(variables)\n\n    def get_attrs(self):\n        \"\"\"Get the global attributes.\n\n        This is used by `xarray.open_dataset`.\n\n        \"\"\"\n        return FrozenDict(satellite=self.prod_desc.creating_entity,\n                          sector=self.prod_desc.sector_id)",
  "class GiniXarrayBackend(BackendEntrypoint):\n    \"\"\"Entry point for direct reading of GINI data into Xarray.\"\"\"\n\n    def open_dataset(self, filename_or_obj, *, drop_variables=None):\n        \"\"\"Open the GINI datafile as a Xarray dataset.\n\n        This is the main entrypoint for plugging into Xarray read support.\n\n        \"\"\"\n        # TODO: This can be structured much better when we're not still supporting both the\n        # old Xarray API as well as direct use of GiniFile itself. In MetPy 2.0 the only\n        # access should be as an xarray backend entrypoint.\n        gini = GiniFile(filename_or_obj)\n        gini_attrs = gini.get_attrs()\n        coords = dict(gini._make_coord_vars() + [gini._make_time_var()])\n        coords['time'] = CFDatetimeCoder().decode(coords['time'])\n        (proj_name, proj_var), (data_name, data_var) = gini._make_data_vars()\n        data_var.attrs.pop('coordinates')\n        decoded_data_var = CFMaskCoder().decode(data_var, data_name)\n        return Dataset({proj_name: proj_var, data_name: decoded_data_var}, coords, gini_attrs)\n\n    def guess_can_open(self, filename_or_obj):\n        \"\"\"Try to guess whether we can read this file.\n\n        This allows files ending in '.gini' to be automatically opened by xarray.\n\n        \"\"\"\n        with contextlib.suppress(TypeError):\n            return Path(filename_or_obj).suffix == '.gini'\n        return False",
  "def lookup(val):\n        return mapper.get(val, 'UnknownValue')",
  "def __init__(self, filename):\n        r\"\"\"Create an instance of `GiniFile`.\n\n        Parameters\n        ----------\n        filename : str or file-like object\n            If str, the name of the file to be opened. Gzip-ed files are\n            recognized with the extension ``'.gz'``, as are bzip2-ed files with\n            the extension ``'.bz2'`` If `filename` is a file-like object,\n            this will be read from directly.\n\n        \"\"\"\n        fobj = open_as_needed(filename)\n\n        # Just read in the entire set of data at once\n        with contextlib.closing(fobj):\n            self._buffer = IOBuffer.fromfile(fobj)\n\n        # Pop off the WMO header if we find it\n        self.wmo_code = ''\n        self._process_wmo_header()\n        log.debug('First wmo code: %s', self.wmo_code)\n\n        # Decompress the data if necessary, and if so, pop off new header\n        log.debug('Length before decompression: %s', len(self._buffer))\n        self._buffer = IOBuffer(self._buffer.read_func(zlib_decompress_all_frames))\n        log.debug('Length after decompression: %s', len(self._buffer))\n\n        # Process WMO header inside compressed data if necessary\n        self._process_wmo_header()\n        log.debug('2nd wmo code: %s', self.wmo_code)\n\n        # Read product description start\n        start = self._buffer.set_mark()\n\n        #: :desc: Decoded first section of product description block\n        #: :type: namedtuple\n        self.prod_desc = self._buffer.read_struct(self.prod_desc_fmt)\n        log.debug(self.prod_desc)\n\n        #: :desc: Decoded geographic projection information\n        #: :type: namedtuple\n        self.proj_info = None\n\n        # Handle projection-dependent parts\n        if self.prod_desc.projection in (GiniProjection.lambert_conformal,\n                                         GiniProjection.polar_stereographic):\n            self.proj_info = self._buffer.read_struct(self.lc_ps_fmt)\n        elif self.prod_desc.projection == GiniProjection.mercator:\n            self.proj_info = self._buffer.read_struct(self.mercator_fmt)\n        else:\n            log.warning('Unknown projection: %d', self.prod_desc.projection)\n        log.debug(self.proj_info)\n\n        # Read the rest of the guaranteed product description block (PDB)\n        #: :desc: Decoded second section of product description block\n        #: :type: namedtuple\n        self.prod_desc2 = self._buffer.read_struct(self.prod_desc2_fmt)\n        log.debug(self.prod_desc2)\n\n        if self.prod_desc2.nav_cal not in (0, -128):  # TODO: See how GEMPAK/MCIDAS parses\n            # Only warn if there actually seems to be useful navigation data\n            if self._buffer.get_next(self.nav_fmt.size) != b'\\x00' * self.nav_fmt.size:\n                log.warning('Navigation/Calibration unhandled: %d', self.prod_desc2.nav_cal)\n            if self.prod_desc2.nav_cal in (1, 2):\n                self.navigation = self._buffer.read_struct(self.nav_fmt)\n                log.debug(self.navigation)\n\n        # Catch bad PDB with size set to 0\n        if self.prod_desc2.pdb_size == 0:\n            log.warning('Adjusting bad PDB size from 0 to 512.')\n            self.prod_desc2 = self.prod_desc2._replace(pdb_size=512)\n\n        # Jump past the remaining empty bytes in the product description block\n        self._buffer.jump_to(start, self.prod_desc2.pdb_size)\n\n        # Read the actual raster--unless it's PNG compressed, in which case that happens later\n        blob = self._buffer.read(self.prod_desc.num_records * self.prod_desc.record_len)\n\n        # Check for end marker\n        end = self._buffer.read(self.prod_desc.record_len)\n        if end != b''.join(repeat(b'\\xff\\x00', self.prod_desc.record_len // 2)):\n            log.warning('End marker not as expected: %s', end)\n\n        # Check to ensure that we processed all of the data\n        if not self._buffer.at_end():\n            if not blob:\n                log.debug('No data read yet, trying to decompress remaining data as an image.')\n                from matplotlib.image import imread\n                blob = (imread(BytesIO(self._buffer.read())) * 255).astype('uint8')\n            else:\n                log.warning('Leftover unprocessed data beyond EOF marker: %s',\n                            self._buffer.get_next(10))\n\n        self.data = np.array(blob).reshape((self.prod_desc.ny,\n                                            self.prod_desc.nx))",
  "def _process_wmo_header(self):\n        \"\"\"Read off the WMO header from the file, if necessary.\"\"\"\n        data = self._buffer.get_next(64).decode('utf-8', 'ignore')\n        match = self.wmo_finder.search(data)\n        if match:\n            self.wmo_code = match.groups()[0]\n            self.siteID = match.groups()[-1]\n            self._buffer.skip(match.end())",
  "def __str__(self):\n        \"\"\"Return a string representation of the product.\"\"\"\n        parts = [self.__class__.__name__ + ': {0.creating_entity} {0.sector_id} {0.channel}',\n                 'Time: {0.datetime}', 'Size: {0.ny}x{0.nx}',\n                 'Projection: {0.projection.name}',\n                 'Lower Left Corner (Lon, Lat): ({0.lo1}, {0.la1})',\n                 'Resolution: {1.resolution}km']\n        return '\\n\\t'.join(parts).format(self.prod_desc, self.prod_desc2)",
  "def _make_proj_var(self):\n        proj_info = self.proj_info\n        prod_desc2 = self.prod_desc2\n        attrs = {'earth_radius': 6371200.0}\n        if self.prod_desc.projection == GiniProjection.lambert_conformal:\n            attrs['grid_mapping_name'] = 'lambert_conformal_conic'\n            attrs['standard_parallel'] = prod_desc2.lat_in\n            attrs['longitude_of_central_meridian'] = proj_info.lov\n            attrs['latitude_of_projection_origin'] = prod_desc2.lat_in\n        elif self.prod_desc.projection == GiniProjection.polar_stereographic:\n            attrs['grid_mapping_name'] = 'polar_stereographic'\n            attrs['straight_vertical_longitude_from_pole'] = proj_info.lov\n            attrs['latitude_of_projection_origin'] = -90 if proj_info.proj_center else 90\n            attrs['standard_parallel'] = 60.0  # See Note 2 for Table 4.4A in ICD\n        elif self.prod_desc.projection == GiniProjection.mercator:\n            attrs['grid_mapping_name'] = 'mercator'\n            attrs['longitude_of_projection_origin'] = self.prod_desc.lo1\n            attrs['latitude_of_projection_origin'] = self.prod_desc.la1\n            attrs['standard_parallel'] = prod_desc2.lat_in\n        else:\n            raise NotImplementedError(\n                f'Unhandled GINI Projection: {self.prod_desc.projection}')\n\n        return 'projection', Variable((), 0, attrs)",
  "def _make_time_var(self):\n        base_time = self.prod_desc.datetime.replace(hour=0, minute=0, second=0, microsecond=0)\n        offset = self.prod_desc.datetime - base_time\n        time_var = Variable((), data=offset.seconds + offset.microseconds / 1e6,\n                            attrs={'units': 'seconds since ' + base_time.isoformat()})\n\n        return 'time', time_var",
  "def _get_proj_and_res(self):\n        import pyproj\n\n        proj_info = self.proj_info\n        prod_desc2 = self.prod_desc2\n\n        kwargs = {'a': 6371200.0, 'b': 6371200.0}\n        if self.prod_desc.projection == GiniProjection.lambert_conformal:\n            kwargs['proj'] = 'lcc'\n            kwargs['lat_0'] = prod_desc2.lat_in\n            kwargs['lon_0'] = proj_info.lov\n            kwargs['lat_1'] = prod_desc2.lat_in\n            kwargs['lat_2'] = prod_desc2.lat_in\n            dx, dy = proj_info.dx, proj_info.dy\n        elif self.prod_desc.projection == GiniProjection.polar_stereographic:\n            kwargs['proj'] = 'stere'\n            kwargs['lon_0'] = proj_info.lov\n            kwargs['lat_0'] = -90 if proj_info.proj_center else 90\n            kwargs['lat_ts'] = 60.0  # See Note 2 for Table 4.4A in ICD\n            kwargs['x_0'] = False  # Easting\n            kwargs['y_0'] = False  # Northing\n            dx, dy = proj_info.dx, proj_info.dy\n        elif self.prod_desc.projection == GiniProjection.mercator:\n            kwargs['proj'] = 'merc'\n            kwargs['lat_0'] = self.prod_desc.la1\n            kwargs['lon_0'] = self.prod_desc.lo1\n            kwargs['lat_ts'] = prod_desc2.lat_in\n            kwargs['x_0'] = False  # Easting\n            kwargs['y_0'] = False  # Northing\n            dx, dy = prod_desc2.resolution, prod_desc2.resolution\n\n        return pyproj.Proj(**kwargs), dx, dy",
  "def _make_coord_vars(self):\n        proj, dx, dy = self._get_proj_and_res()\n\n        # Get projected location of lower left point\n        x0, y0 = proj(self.prod_desc.lo1, self.prod_desc.la1)\n\n        # Coordinate variable for x\n        xlocs = x0 + np.arange(self.prod_desc.nx) * (1000. * dx)\n        attrs = {'units': 'm', 'long_name': 'x coordinate of projection',\n                 'standard_name': 'projection_x_coordinate'}\n        x_var = Variable(('x',), xlocs, attrs)\n\n        # Now y--Need to flip y because we calculated from the lower left corner,\n        # but the raster data is stored with top row first.\n        ylocs = (y0 + np.arange(self.prod_desc.ny) * (1000. * dy))[::-1]\n        attrs = {'units': 'm', 'long_name': 'y coordinate of projection',\n                 'standard_name': 'projection_y_coordinate'}\n        y_var = Variable(('y',), ylocs, attrs)\n\n        # Get the two-D lon,lat grid as well\n        x, y = np.meshgrid(xlocs, ylocs)\n        lon, lat = proj(x, y, inverse=True)\n\n        lon_var = Variable(('y', 'x'), data=lon,\n                           attrs={'long_name': 'longitude', 'units': 'degrees_east'})\n        lat_var = Variable(('y', 'x'), data=lat,\n                           attrs={'long_name': 'latitude', 'units': 'degrees_north'})\n\n        return [('x', x_var), ('y', y_var), ('lon', lon_var), ('lat', lat_var)]",
  "def _make_data_vars(self):\n        proj_var_name, proj_var = self._make_proj_var()\n        name = self.prod_desc.channel\n        if '(' in name:\n            name = name.split('(')[0].rstrip()\n\n        missing_val = self.missing\n        attrs = {'long_name': self.prod_desc.channel, 'missing_value': missing_val,\n                 'coordinates': 'lon lat time', 'grid_mapping': proj_var_name}\n        data_var = Variable(('y', 'x'), data=self.data, attrs=attrs)\n        return [(proj_var_name, proj_var), (name, data_var)]",
  "def get_variables(self):\n        \"\"\"Get all variables in the file.\n\n        This is used by `xarray.open_dataset`.\n\n        \"\"\"\n        variables = [self._make_time_var()]\n        variables.extend(self._make_coord_vars())\n        variables.extend(self._make_data_vars())\n\n        return FrozenDict(variables)",
  "def get_attrs(self):\n        \"\"\"Get the global attributes.\n\n        This is used by `xarray.open_dataset`.\n\n        \"\"\"\n        return FrozenDict(satellite=self.prod_desc.creating_entity,\n                          sector=self.prod_desc.sector_id)",
  "def open_dataset(self, filename_or_obj, *, drop_variables=None):\n        \"\"\"Open the GINI datafile as a Xarray dataset.\n\n        This is the main entrypoint for plugging into Xarray read support.\n\n        \"\"\"\n        # TODO: This can be structured much better when we're not still supporting both the\n        # old Xarray API as well as direct use of GiniFile itself. In MetPy 2.0 the only\n        # access should be as an xarray backend entrypoint.\n        gini = GiniFile(filename_or_obj)\n        gini_attrs = gini.get_attrs()\n        coords = dict(gini._make_coord_vars() + [gini._make_time_var()])\n        coords['time'] = CFDatetimeCoder().decode(coords['time'])\n        (proj_name, proj_var), (data_name, data_var) = gini._make_data_vars()\n        data_var.attrs.pop('coordinates')\n        decoded_data_var = CFMaskCoder().decode(data_var, data_name)\n        return Dataset({proj_name: proj_var, data_name: decoded_data_var}, coords, gini_attrs)",
  "def guess_can_open(self, filename_or_obj):\n        \"\"\"Try to guess whether we can read this file.\n\n        This allows files ending in '.gini' to be automatically opened by xarray.\n\n        \"\"\"\n        with contextlib.suppress(TypeError):\n            return Path(filename_or_obj).suffix == '.gini'\n        return False",
  "def version(val):\n    \"\"\"Calculate a string version from an integer value.\"\"\"\n    ver = val / 100. if val > 2. * 100. else val / 10.\n    return f'{ver:.1f}'",
  "def scaler(scale):\n    \"\"\"Create a function that scales by a specific value.\"\"\"\n    def inner(val):\n        return val * scale\n    return inner",
  "def angle(val):\n    \"\"\"Convert an integer value to a floating point angle.\"\"\"\n    return val * 360. / 2**16",
  "def az_rate(val):\n    \"\"\"Convert an integer value to a floating point angular rate.\"\"\"\n    return val * 90. / 2**16",
  "def bzip_blocks_decompress_all(data):\n    \"\"\"Decompress all the bzip2-ed blocks.\n\n    Returns the decompressed data as a `bytearray`.\n    \"\"\"\n    frames = bytearray()\n    offset = 0\n    while offset < len(data):\n        block_cmp_bytes = abs(int.from_bytes(data[offset:offset + 4], 'big', signed=True))\n        offset += 4\n        try:\n            frames += bz2.decompress(data[offset:offset + block_cmp_bytes])\n            offset += block_cmp_bytes\n        except OSError as e:\n            # If we've decompressed any frames, this is an error mid-stream, so warn, stop\n            # trying to decompress and let processing proceed\n            if frames:\n                logging.warning('Error decompressing bz2 block stream at offset: %d',\n                                offset - 4)\n                break\n            # Otherwise, this isn't a bzip2 stream, so bail\n            raise ValueError('Not a bz2 stream.') from e\n    return frames",
  "def nexrad_to_datetime(julian_date, ms_midnight):\n    \"\"\"Convert NEXRAD date time format to python `datetime.datetime`.\"\"\"\n    # Subtracting one from julian_date is because epoch date is 1\n    return datetime.datetime.utcfromtimestamp((julian_date - 1) * day + ms_midnight * milli)",
  "def remap_status(val):\n    \"\"\"Convert status integer value to appropriate bitmask.\"\"\"\n    status = 0\n    bad = BAD_DATA if val & 0xF0 else 0\n    val &= 0x0F\n    if val == 0:\n        status = START_ELEVATION\n    elif val == 1:\n        status = 0\n    elif val == 2:\n        status = END_ELEVATION\n    elif val == 3:\n        status = START_ELEVATION | START_VOLUME\n    elif val == 4:\n        status = END_ELEVATION | END_VOLUME\n    elif val == 5:\n        status = START_ELEVATION | LAST_ELEVATION\n\n    return status | bad",
  "class Level2File:\n    r\"\"\"Handle reading the NEXRAD Level 2 data and its various messages.\n\n    This class attempts to decode every byte that is in a given data file.\n    It supports both external compression, as well as the internal BZ2\n    compression that is used.\n\n    Attributes\n    ----------\n    stid : str\n        The ID of the radar station\n    dt : `~datetime.datetime`\n        The date and time of the data\n    vol_hdr : `collections.namedtuple`\n        The unpacked volume header\n    sweeps : list[tuple]\n        Data for each of the sweeps found in the file\n    rda_status : `collections.namedtuple`, optional\n        Unpacked RDA status information, if found\n    maintenance_data : `collections.namedtuple`, optional\n        Unpacked maintenance data information, if found\n    maintenance_data_desc : dict, optional\n        Descriptions of maintenance data fields, if maintenance data present\n    vcp_info : `collections.namedtuple`, optional\n        Unpacked VCP information, if found\n    clutter_filter_bypass_map : dict, optional\n        Unpacked clutter filter bypass map, if present\n    rda : dict, optional\n        Unpacked RDA adaptation data, if present\n    rda_adaptation_desc : dict, optional\n        Descriptions of RDA adaptation data, if adaptation data present\n\n    Notes\n    -----\n    The internal data structure that things are decoded into is still to be\n    determined.\n\n    \"\"\"\n\n    # Number of bytes\n    AR2_BLOCKSIZE = 2432  # 12 (CTM) + 2416 (Msg hdr + data) + 4 (FCS)\n    CTM_HEADER_SIZE = 12\n\n    MISSING = float('nan')\n    RANGE_FOLD = float('nan')  # TODO: Need to separate from missing\n\n    def __init__(self, filename, *, has_volume_header=True):\n        r\"\"\"Create instance of `Level2File`.\n\n        Parameters\n        ----------\n        filename : str or file-like object\n            If str, the name of the file to be opened. Gzip-ed files are\n            recognized with the extension '.gz', as are bzip2-ed files with\n            the extension `.bz2` If `filename` is a file-like object,\n            this will be read from directly.\n\n        \"\"\"\n        fobj = open_as_needed(filename)\n\n        with contextlib.closing(fobj):\n            self._buffer = IOBuffer.fromfile(fobj)\n\n        # Try to read the volume header. If this fails, or we're told we don't have one\n        # then we fall back and try to just read messages, assuming we have e.g. one of\n        # the real-time chunks.\n        try:\n            if has_volume_header:\n                self._read_volume_header()\n        except (OSError, ValueError):\n            log.warning('Unable to read volume header. Attempting to read messages.')\n            self._buffer.reset()\n\n        # See if we need to apply bz2 decompression\n        start = self._buffer.set_mark()\n        try:\n            self._buffer = IOBuffer(self._buffer.read_func(bzip_blocks_decompress_all))\n        except ValueError:\n            self._buffer.jump_to(start)\n\n        # Now we're all initialized, we can proceed with reading in data\n        self._read_data()\n\n    vol_hdr_fmt = NamedStruct([('version', '9s'), ('vol_num', '3s'),\n                               ('date', 'L'), ('time_ms', 'L'), ('stid', '4s')], '>', 'VolHdr')\n\n    def _read_volume_header(self):\n        self.vol_hdr = self._buffer.read_struct(self.vol_hdr_fmt)\n        self.dt = nexrad_to_datetime(self.vol_hdr.date, self.vol_hdr.time_ms)\n        self.stid = self.vol_hdr.stid\n\n    msg_hdr_fmt = NamedStruct([('size_hw', 'H'),\n                               ('rda_channel', 'B', BitField('Redundant Channel 1',\n                                                             'Redundant Channel 2',\n                                                             None, 'ORDA')),\n                               ('msg_type', 'B'), ('seq_num', 'H'), ('date', 'H'),\n                               ('time_ms', 'I'), ('num_segments', 'H'), ('segment_num', 'H')],\n                              '>', 'MsgHdr')\n\n    def _read_data(self):\n        self._msg_buf = {}\n        self.sweeps = []\n        self.rda_status = []\n        while not self._buffer.at_end():\n            # Clear old file book marks and set the start of message for\n            # easy jumping to the end\n            self._buffer.clear_marks()\n            msg_start = self._buffer.set_mark()\n\n            # Skip CTM\n            self._buffer.skip(self.CTM_HEADER_SIZE)\n\n            # Read the message header\n            msg_hdr = self._buffer.read_struct(self.msg_hdr_fmt)\n            log.debug('Got message: %s (at offset %d)', str(msg_hdr), self._buffer._offset)\n\n            # The AR2_BLOCKSIZE accounts for the CTM header before the\n            # data, as well as the Frame Check Sequence (4 bytes) after\n            # the end of the data.\n            msg_bytes = self.AR2_BLOCKSIZE\n\n            # If the size is 0, this is just padding, which is for certain\n            # done in the metadata messages. Let the default block size handle rather\n            # than any specific heuristic to skip.\n            if msg_hdr.size_hw:\n                # For new packets, the message size isn't on the fixed size boundaries,\n                # so we use header to figure out. For these, we need to include the\n                # CTM header but not FCS, in addition to the size.\n\n                # As of 2620002P, this is a special value used to indicate that the segment\n                # number/count bytes are used to indicate total size in bytes.\n                if msg_hdr.size_hw == 65535:\n                    msg_bytes = (msg_hdr.num_segments << 16 | msg_hdr.segment_num\n                                 + self.CTM_HEADER_SIZE)\n                elif msg_hdr.msg_type in (29, 31):\n                    msg_bytes = self.CTM_HEADER_SIZE + 2 * msg_hdr.size_hw\n\n                log.debug('Total message size: %d', msg_bytes)\n\n                # Try to handle the message. If we don't handle it, skipping\n                # past it is handled at the end anyway.\n                decoder = f'_decode_msg{msg_hdr.msg_type:d}'\n                if hasattr(self, decoder):\n                    getattr(self, decoder)(msg_hdr)\n                else:\n                    log.warning('Unknown message: %d', msg_hdr.msg_type)\n\n            # Jump to the start of the next message. This depends on whether\n            # the message was legacy with fixed block size or not.\n            self._buffer.jump_to(msg_start, msg_bytes)\n\n        # Check if we have any message segments still in the buffer\n        if self._msg_buf:\n            log.warning('Remaining buffered message segments for message type(s): %s',\n                        ' '.join(f'{typ} ({len(rem)})' for typ, rem in self._msg_buf.items()))\n\n        del self._msg_buf\n\n    msg1_fmt = NamedStruct([('time_ms', 'L'), ('date', 'H'),\n                            ('unamb_range', 'H', scaler(0.1)), ('az_angle', 'H', angle),\n                            ('az_num', 'H'), ('rad_status', 'H', remap_status),\n                            ('el_angle', 'H', angle), ('el_num', 'H'),\n                            ('surv_first_gate', 'h', scaler(0.001)),\n                            ('doppler_first_gate', 'h', scaler(0.001)),\n                            ('surv_gate_width', 'H', scaler(0.001)),\n                            ('doppler_gate_width', 'H', scaler(0.001)),\n                            ('surv_num_gates', 'H'), ('doppler_num_gates', 'H'),\n                            ('cut_sector_num', 'H'), ('calib_dbz0', 'f'),\n                            ('ref_offset', 'H'), ('vel_offset', 'H'), ('sw_offset', 'H'),\n                            ('dop_res', 'H', BitField(None, 0.5, 1.0)), ('vcp', 'H'),\n                            (None, '14x'), ('nyq_vel', 'H', scaler(0.01)),\n                            ('atmos_atten', 'H', scaler(0.001)), ('tover', 'H', scaler(0.1)),\n                            ('spot_blanking', 'B', BitField('Radial', 'Elevation', 'Volume')),\n                            (None, '32x')], '>', 'Msg1Fmt')\n\n    msg1_data_hdr = namedtuple('Msg1DataHdr',\n                               'name first_gate gate_width num_gates scale offset')\n\n    def _decode_msg1(self, msg_hdr):\n        msg_start = self._buffer.set_mark()\n        hdr = self._buffer.read_struct(self.msg1_fmt)\n        data_dict = {}\n\n        # Process all data pointers:\n        read_info = []\n        if hdr.surv_num_gates and hdr.ref_offset:\n            read_info.append((hdr.ref_offset,\n                              self.msg1_data_hdr('REF', hdr.surv_first_gate,\n                                                 hdr.surv_gate_width,\n                                                 hdr.surv_num_gates, 2.0, 66.0)))\n\n        if hdr.doppler_num_gates and hdr.vel_offset:\n            read_info.append((hdr.vel_offset,\n                              self.msg1_data_hdr('VEL', hdr.doppler_first_gate,\n                                                 hdr.doppler_gate_width,\n                                                 hdr.doppler_num_gates,\n                                                 1. / hdr.dop_res, 129.0)))\n\n        if hdr.doppler_num_gates and hdr.sw_offset:\n            read_info.append((hdr.sw_offset,\n                              self.msg1_data_hdr('SW', hdr.doppler_first_gate,\n                                                 hdr.doppler_gate_width,\n                                                 hdr.doppler_num_gates, 2.0, 129.0)))\n\n        for ptr, data_hdr in read_info:\n            # Jump and read\n            self._buffer.jump_to(msg_start, ptr)\n            vals = self._buffer.read_array(data_hdr.num_gates, 'B')\n\n            # Scale and flag data\n            scaled_vals = (vals - data_hdr.offset) / data_hdr.scale\n            scaled_vals[vals == 0] = self.MISSING\n            scaled_vals[vals == 1] = self.RANGE_FOLD\n\n            # Store\n            data_dict[data_hdr.name] = (data_hdr, scaled_vals)\n\n        self._add_sweep(hdr)\n        self.sweeps[-1].append((hdr, data_dict))\n\n    msg2_fmt = NamedStruct([\n        ('rda_status', 'H', BitField('None', 'Start-Up', 'Standby', 'Restart',\n                                     'Operate', 'Spare', 'Off-line Operate')),\n        ('op_status', 'H', BitField('Disabled', 'On-Line',\n                                    'Maintenance Action Required',\n                                    'Maintenance Action Mandatory',\n                                    'Commanded Shut Down', 'Inoperable',\n                                    'Automatic Calibration')),\n        ('control_status', 'H', BitField('None', 'Local Only',\n                                         'RPG (Remote) Only', 'Either')),\n        ('aux_power_gen_state', 'H', BitField('Switch to Aux Power',\n                                              'Utility PWR Available',\n                                              'Generator On',\n                                              'Transfer Switch Manual',\n                                              'Commanded Switchover')),\n        ('avg_tx_pwr', 'H'), ('ref_calib_cor', 'h', scaler(0.01)),\n        ('data_transmission_enabled', 'H', BitField('None', 'None',\n                                                    'Reflectivity', 'Velocity', 'Width')),\n        ('vcp_num', 'h'), ('rda_control_auth', 'H', BitField('No Action',\n                                                             'Local Control Requested',\n                                                             'Remote Control Enabled')),\n        ('rda_build', 'H', version), ('op_mode', 'H', BitField('None', 'Test',\n                                                               'Operational', 'Maintenance')),\n        ('super_res_status', 'H', BitField('None', 'Enabled', 'Disabled')),\n        ('cmd_status', 'H', Bits(6)),\n        ('avset_status', 'H', BitField('None', 'Enabled', 'Disabled')),\n        ('rda_alarm_status', 'H', BitField('No Alarms', 'Tower/Utilities',\n                                           'Pedestal', 'Transmitter', 'Receiver',\n                                           'RDA Control', 'Communication',\n                                           'Signal Processor')),\n        ('command_acknowledge', 'H', BitField('Remote VCP Received',\n                                              'Clutter Bypass map received',\n                                              'Redundant Chan Ctrl Cmd received')),\n        ('channel_control_status', 'H'),\n        ('spot_blanking', 'H', BitField('Enabled', 'Disabled')),\n        ('bypass_map_gen_date', 'H'), ('bypass_map_gen_time', 'H'),\n        ('clutter_filter_map_gen_date', 'H'), ('clutter_filter_map_gen_time', 'H'),\n        ('refv_calib_cor', 'h', scaler(0.01)),\n        ('transition_pwr_src_state', 'H', BitField('Off', 'OK')),\n        ('RMS_control_status', 'H', BitField('RMS in control', 'RDA in control')),\n        # See Table IV-A for definition of alarms\n        (None, '2x'), ('alarms', '28s', Array('>14H'))], '>', 'Msg2Fmt')\n\n    msg2_additional_fmt = NamedStruct([\n        ('sig_proc_options', 'H', BitField('CMD RhoHV Test')),\n        (None, '36x'), ('status_version', 'H')], '>', 'Msg2AdditionalFmt')\n\n    def _decode_msg2(self, msg_hdr):\n        msg_start = self._buffer.set_mark()\n        self.rda_status.append(self._buffer.read_struct(self.msg2_fmt))\n\n        remaining = (msg_hdr.size_hw * 2 - self.msg_hdr_fmt.size\n                     - self._buffer.offset_from(msg_start))\n\n        # RDA Build 18.0 expanded the size\n        if remaining >= self.msg2_additional_fmt.size:\n            self.rda_status.append(self._buffer.read_struct(self.msg2_additional_fmt))\n            remaining -= self.msg2_additional_fmt.size\n\n        if remaining:\n            log.info('Padding detected in message 2. Length encoded as %d but offset when '\n                     'done is %d', 2 * msg_hdr.size_hw, self._buffer.offset_from(msg_start))\n\n    def _decode_msg3(self, msg_hdr):\n        from ._nexrad_msgs.msg3 import descriptions, fields\n        self.maintenance_data_desc = descriptions\n        msg_fmt = DictStruct(fields, '>')\n\n        # The only version we decode isn't very flexible, so just skip if we don't have the\n        # right length, which happens with older data.\n        if msg_hdr.size_hw * 2 - self.msg_hdr_fmt.size != msg_fmt.size:\n            log.info('Length of message 3 is %d instead of expected %d; this is likely the '\n                     'legacy format. Skipping...', 2 * msg_hdr.size_hw)\n            return\n\n        self.maintenance_data = self._buffer.read_struct(msg_fmt)\n\n    vcp_fmt = NamedStruct([('size_hw', 'H'), ('pattern_type', 'H'),\n                           ('num', 'H'), ('num_el_cuts', 'H'),\n                           ('vcp_version', 'B'), ('clutter_map_group', 'B'),\n                           ('dop_res', 'B', BitField(None, 0.5, 1.0)),\n                           ('pulse_width', 'B', BitField('None', 'Short', 'Long')),\n                           (None, '4x'), ('vcp_sequencing', 'H'),\n                           ('vcp_supplemental_info', 'H'), (None, '2x'),\n                           ('els', None)], '>', 'VCPFmt')\n\n    vcp_el_fmt = NamedStruct([('el_angle', 'H', angle),\n                              ('channel_config', 'B', Enum('Constant Phase', 'Random Phase',\n                                                           'SZ2 Phase')),\n                              ('waveform', 'B', Enum('None', 'Contiguous Surveillance',\n                                                     'Contig. Doppler with Ambiguity Res.',\n                                                     'Contig. Doppler without Ambiguity Res.',\n                                                     'Batch', 'Staggered Pulse Pair')),\n                              ('super_res', 'B', BitField('0.5 azimuth and 0.25km range res.',\n                                                          'Doppler to 300km',\n                                                          'Dual Polarization Control',\n                                                          'Dual Polarization to 300km')),\n                              ('surv_prf_num', 'B'), ('surv_pulse_count', 'H'),\n                              ('az_rate', 'h', az_rate),\n                              ('ref_thresh', 'h', scaler(0.125)),\n                              ('vel_thresh', 'h', scaler(0.125)),\n                              ('sw_thresh', 'h', scaler(0.125)),\n                              ('zdr_thresh', 'h', scaler(0.125)),\n                              ('phidp_thresh', 'h', scaler(0.125)),\n                              ('rhohv_thresh', 'h', scaler(0.125)),\n                              ('sector1_edge', 'H', angle),\n                              ('sector1_doppler_prf_num', 'H'),\n                              ('sector1_pulse_count', 'H'), ('supplemental_data', 'H'),\n                              ('sector2_edge', 'H', angle),\n                              ('sector2_doppler_prf_num', 'H'),\n                              ('sector2_pulse_count', 'H'), ('ebc_angle', 'H', angle),\n                              ('sector3_edge', 'H', angle),\n                              ('sector3_doppler_prf_num', 'H'),\n                              ('sector3_pulse_count', 'H'), (None, '2x')], '>', 'VCPEl')\n\n    def _decode_msg5(self, msg_hdr):\n        vcp_info = self._buffer.read_struct(self.vcp_fmt)\n        # Just skip the vcp info if it says size is 0:\n        if vcp_info.size_hw:\n            els = [self._buffer.read_struct(self.vcp_el_fmt)\n                   for _ in range(vcp_info.num_el_cuts)]\n            self.vcp_info = vcp_info._replace(els=els)\n            self._check_size(msg_hdr,\n                             self.vcp_fmt.size + vcp_info.num_el_cuts * self.vcp_el_fmt.size)\n\n    def _decode_msg13(self, msg_hdr):\n        data = self._buffer_segment(msg_hdr)\n        if data:\n            data = struct.Struct(f'>{len(data) // 2:d}h').unpack(data)\n            # Legacy format doesn't have date/time and has fewer azimuths\n            if data[0] <= 5:\n                num_el = data[0]\n                dt = None\n                num_az = 256\n                offset = 1\n            else:\n                date, time, num_el = data[:3]\n                # time is in \"minutes since midnight\", need to pass as ms since midnight\n                dt = nexrad_to_datetime(date, 60 * 1000 * time)\n                num_az = 360\n                offset = 3\n\n            self.clutter_filter_bypass_map = {'datetime': dt, 'data': []}\n            chunk_size = 32\n            bit_conv = Bits(16)\n            for e in range(num_el):\n                seg_num = data[offset]\n                if seg_num != (e + 1):\n                    log.warning('Message 13 segments out of sync -- read %d but on %d',\n                                seg_num, e + 1)\n\n                az_data = []\n                for _ in range(num_az):\n                    gates = []\n                    for i in range(1, chunk_size + 1):\n                        gates.extend(bit_conv(data[offset + i]))\n                    az_data.append(gates)\n                self.clutter_filter_bypass_map['data'].append(az_data)\n                offset += num_az * chunk_size + 1\n\n            if offset != len(data):\n                log.warning('Message 13 left data -- Used: %d Avail: %d', offset, len(data))\n\n    msg15_code_map = {0: 'Bypass Filter', 1: 'Bypass map in Control',\n                      2: 'Force Filter'}\n\n    def _decode_msg15(self, msg_hdr):\n        # buffer the segments until we have the whole thing. The data\n        # will be returned concatenated when this is the case\n        data = self._buffer_segment(msg_hdr)\n        if data:\n            date, time, num_el, *data = struct.Struct(f'>{len(data) // 2:d}h').unpack(data)\n\n            if not 0 < num_el <= 5:\n                log.info('Message 15 num_el is outside (0, 5]--likely legacy clutter filter '\n                         'notch width. Skipping...')\n                return\n\n            # time is in \"minutes since midnight\", need to pass as ms since midnight\n            self.clutter_filter_map = {'datetime': nexrad_to_datetime(date, 60 * 1000 * time),\n                                       'data': []}\n\n            offset = 0\n            for _ in range(num_el):\n                az_data = []\n                for _ in range(360):\n                    num_rng = data[offset]\n                    codes = data[offset + 1:offset + 1 + 2 * num_rng:2]\n                    ends = data[offset + 2:offset + 2 + 2 * num_rng:2]\n                    az_data.append(list(zip(ends, codes)))\n                    offset += 2 * num_rng + 1\n                self.clutter_filter_map['data'].append(az_data)\n\n            if offset != len(data):\n                log.warning('Message 15 left data -- Used: %d Avail: %d', offset, len(data))\n\n    def _decode_msg18(self, msg_hdr):\n        # buffer the segments until we have the whole thing. The data\n        # will be returned concatenated when this is the case\n        data = self._buffer_segment(msg_hdr)\n\n        # Legacy versions don't even document this:\n        if data and self.vol_hdr.version[:8] not in (b'ARCHIVE2', b'AR2V0001'):\n            from ._nexrad_msgs.msg18 import descriptions, fields\n            self.rda_adaptation_desc = descriptions\n\n            # Can't use NamedStruct because we have more than 255 items--this\n            # is a CPython limit for arguments.\n            msg_fmt = DictStruct(fields, '>')\n\n            # Be extra paranoid about passing too much data in case of legacy files\n            self.rda = msg_fmt.unpack(data[:msg_fmt.size])\n            for num in (11, 21, 31, 32, 300, 301):\n                attr = f'VCPAT{num}'\n                dat = self.rda[attr]\n                vcp_hdr = self.vcp_fmt.unpack_from(dat, 0)\n                off = self.vcp_fmt.size\n                els = []\n                for _ in range(vcp_hdr.num_el_cuts):\n                    els.append(self.vcp_el_fmt.unpack_from(dat, off))\n                    off += self.vcp_el_fmt.size\n                self.rda[attr] = vcp_hdr._replace(els=els)\n\n    msg31_data_hdr_fmt = NamedStruct([('stid', '4s'), ('time_ms', 'L'),\n                                      ('date', 'H'), ('az_num', 'H'),\n                                      ('az_angle', 'f'), ('compression', 'B'),\n                                      (None, 'x'), ('rad_length', 'H'),\n                                      ('az_spacing', 'B', Enum(0, 0.5, 1.0)),\n                                      ('rad_status', 'B', remap_status),\n                                      ('el_num', 'B'), ('sector_num', 'B'),\n                                      ('el_angle', 'f'),\n                                      ('spot_blanking', 'B', BitField('Radial', 'Elevation',\n                                                                      'Volume')),\n                                      ('az_index_mode', 'B', scaler(0.01)),\n                                      ('num_data_blks', 'H')], '>', 'Msg31DataHdr')\n\n    msg31_vol_const_fmt = NamedStruct([('type', 's'), ('name', '3s'),\n                                       ('size', 'H'), ('major', 'B'),\n                                       ('minor', 'B'), ('lat', 'f'), ('lon', 'f'),\n                                       ('site_amsl', 'h'), ('feedhorn_agl', 'H'),\n                                       ('calib_dbz', 'f'), ('txpower_h', 'f'),\n                                       ('txpower_v', 'f'), ('sys_zdr', 'f'),\n                                       ('phidp0', 'f'), ('vcp', 'H'),\n                                       ('processing_status', 'H', BitField('RxR Noise',\n                                                                           'CBT'))],\n                                      '>', 'VolConsts')\n\n    msg31_el_const_fmt = NamedStruct([('type', 's'), ('name', '3s'),\n                                      ('size', 'H'), ('atmos_atten', 'h', scaler(0.001)),\n                                      ('calib_dbz0', 'f')], '>', 'ElConsts')\n\n    rad_const_fmt_v1 = NamedStruct([('type', 's'), ('name', '3s'), ('size', 'H'),\n                                    ('unamb_range', 'H', scaler(0.1)),\n                                    ('noise_h', 'f'), ('noise_v', 'f'),\n                                    ('nyq_vel', 'H', scaler(0.01)),\n                                    (None, '2x')], '>', 'RadConstsV1')\n    rad_const_fmt_v2 = NamedStruct([('type', 's'), ('name', '3s'), ('size', 'H'),\n                                    ('unamb_range', 'H', scaler(0.1)),\n                                    ('noise_h', 'f'), ('noise_v', 'f'),\n                                    ('nyq_vel', 'H', scaler(0.01)),\n                                    (None, '2x'), ('calib_dbz0_h', 'f'),\n                                    ('calib_dbz0_v', 'f')], '>', 'RadConstsV2')\n\n    data_block_fmt = NamedStruct([('type', 's'), ('name', '3s'),\n                                  ('reserved', 'L'), ('num_gates', 'H'),\n                                  ('first_gate', 'H', scaler(0.001)),\n                                  ('gate_width', 'H', scaler(0.001)),\n                                  ('tover', 'H', scaler(0.1)),\n                                  ('snr_thresh', 'h', scaler(0.1)),\n                                  ('recombined', 'B', BitField('Azimuths', 'Gates')),\n                                  ('data_size', 'B'),\n                                  ('scale', 'f'), ('offset', 'f')], '>', 'DataBlockHdr')\n\n    Radial = namedtuple('Radial', 'header vol_consts elev_consts radial_consts moments')\n\n    def _decode_msg31(self, msg_hdr):\n        msg_start = self._buffer.set_mark()\n        data_hdr = self._buffer.read_struct(self.msg31_data_hdr_fmt)\n        if data_hdr.compression:\n            log.warning('Compressed message 31 not supported!')\n\n        # Read all the block pointers. While the ICD specifies that at least the vol, el, rad\n        # constant blocks as well as REF moment block are present, it says \"the pointers are\n        # not order or location dependent.\"\n        radial = self.Radial(data_hdr, None, None, None, {})\n        block_count = 0\n        for ptr in self._buffer.read_binary(data_hdr.num_data_blks, '>L'):\n            if ptr:\n                block_count += 1\n                self._buffer.jump_to(msg_start, ptr)\n                info = self._buffer.get_next(6)\n                if info.startswith(b'RVOL'):\n                    radial = radial._replace(\n                        vol_consts=self._buffer.read_struct(self.msg31_vol_const_fmt))\n                elif info.startswith(b'RELV'):\n                    radial = radial._replace(\n                        elev_consts=self._buffer.read_struct(self.msg31_el_const_fmt))\n                elif info.startswith(b'RRAD'):\n                    # Relies on the fact that the struct is small enough for its size\n                    # to fit in a single byte\n                    if int(info[-1]) == self.rad_const_fmt_v2.size:\n                        rad_consts = self._buffer.read_struct(self.rad_const_fmt_v2)\n                    else:\n                        rad_consts = self._buffer.read_struct(self.rad_const_fmt_v1)\n                    radial = radial._replace(radial_consts=rad_consts)\n                elif info.startswith(b'D'):\n                    hdr = self._buffer.read_struct(self.data_block_fmt)\n                    # TODO: The correctness of this code is not tested\n                    vals = self._buffer.read_array(count=hdr.num_gates,\n                                                   dtype=f'>u{hdr.data_size // 8}')\n                    scaled_vals = (vals - hdr.offset) / hdr.scale\n                    scaled_vals[vals == 0] = self.MISSING\n                    scaled_vals[vals == 1] = self.RANGE_FOLD\n                    radial.moments[hdr.name.strip()] = (hdr, scaled_vals)\n                else:\n                    log.warning('Unknown Message 31 block type: %s', str(info[:4]))\n\n        self._add_sweep(data_hdr)\n        self.sweeps[-1].append(radial)\n\n        if data_hdr.num_data_blks != block_count:\n            log.warning('Incorrect number of blocks detected -- Got %d'\n                        ' instead of %d', block_count, data_hdr.num_data_blks)\n\n        if data_hdr.rad_length != self._buffer.offset_from(msg_start):\n            log.info('Padding detected in message. Length encoded as %d but offset when '\n                     'done is %d', data_hdr.rad_length, self._buffer.offset_from(msg_start))\n\n    def _buffer_segment(self, msg_hdr):\n        # Add to the buffer\n        bufs = self._msg_buf.setdefault(msg_hdr.msg_type, {})\n        bufs[msg_hdr.segment_num] = self._buffer.read(2 * msg_hdr.size_hw\n                                                      - self.msg_hdr_fmt.size)\n\n        # Warn for badly formatted data\n        if len(bufs) != msg_hdr.segment_num:\n            log.warning('Segment out of order (Got: %d Count: %d) for message type %d.',\n                        msg_hdr.segment_num, len(bufs), msg_hdr.msg_type)\n\n        # If we're complete, return the full collection of data\n        if msg_hdr.num_segments == len(bufs):\n            self._msg_buf.pop(msg_hdr.msg_type)\n            return b''.join(bytes(item[1]) for item in sorted(bufs.items()))\n        else:\n            return None\n\n    def _add_sweep(self, hdr):\n        if not self.sweeps and not hdr.rad_status & START_VOLUME:\n            log.warning('Missed start of volume!')\n\n        if hdr.rad_status & START_ELEVATION:\n            self.sweeps.append([])\n\n        if len(self.sweeps) != hdr.el_num:\n            log.warning('Missed elevation -- Have %d but data on %d.'\n                        ' Compensating...', len(self.sweeps), hdr.el_num)\n            while len(self.sweeps) < hdr.el_num:\n                self.sweeps.append([])\n\n    def _check_size(self, msg_hdr, size):\n        hdr_size = msg_hdr.size_hw * 2 - self.msg_hdr_fmt.size\n        if size != hdr_size:\n            log.warning('Message type %d should be %d bytes but got %d',\n                        msg_hdr.msg_type, size, hdr_size)",
  "def reduce_lists(d):\n    \"\"\"Replace single item lists in a dictionary with the single item.\"\"\"\n    for field in d:\n        old_data = d[field]\n        if len(old_data) == 1:\n            d[field] = old_data[0]",
  "def two_comp16(val):\n    \"\"\"Return the two's-complement signed representation of a 16-bit unsigned integer.\"\"\"\n    if val >> 15:\n        val = -(~val & 0x7fff) - 1\n    return val",
  "def float16(val):\n    \"\"\"Convert a 16-bit floating point value to a standard Python float.\"\"\"\n    # Fraction is 10 LSB, Exponent middle 5, and Sign the MSB\n    frac = val & 0x03ff\n    exp = (val >> 10) & 0x1F\n    sign = val >> 15\n\n    if exp:\n        value = 2 ** (exp - 16) * (1 + float(frac) / 2**10)\n    else:\n        value = float(frac) / 2**9\n\n    if sign:\n        value *= -1\n\n    return value",
  "def float32(short1, short2):\n    \"\"\"Unpack a pair of 16-bit integers as a Python float.\"\"\"\n    # Masking below in python will properly convert signed values to unsigned\n    return struct.unpack('>f', struct.pack('>HH', short1 & 0xFFFF, short2 & 0xFFFF))[0]",
  "def date_elem(ind_days, ind_minutes):\n    \"\"\"Create a function to parse a datetime from the product-specific blocks.\"\"\"\n    def inner(seq):\n        return nexrad_to_datetime(seq[ind_days], seq[ind_minutes] * 60 * 1000)\n    return inner",
  "def scaled_elem(index, scale):\n    \"\"\"Create a function to scale a certain product-specific block.\"\"\"\n    def inner(seq):\n        return seq[index] * scale\n    return inner",
  "def combine_elem(ind1, ind2):\n    \"\"\"Create a function to combine two specified product-specific blocks into a single int.\"\"\"\n    def inner(seq):\n        shift = 2**16\n        if seq[ind1] < 0:\n            seq[ind1] += shift\n        if seq[ind2] < 0:\n            seq[ind2] += shift\n        return (seq[ind1] << 16) | seq[ind2]\n    return inner",
  "def float_elem(ind1, ind2):\n    \"\"\"Create a function to combine two specified product-specific blocks into a float.\"\"\"\n    return lambda seq: float32(seq[ind1], seq[ind2])",
  "def high_byte(ind):\n    \"\"\"Create a function to return the high-byte of a product-specific block.\"\"\"\n    def inner(seq):\n        return seq[ind] >> 8\n    return inner",
  "def low_byte(ind):\n    \"\"\"Create a function to return the low-byte of a product-specific block.\"\"\"\n    def inner(seq):\n        return seq[ind] & 0x00FF\n    return inner",
  "def delta_time(ind):\n    \"\"\"Create a function to return the delta time from a product-specific block.\"\"\"\n    def inner(seq):\n        return seq[ind] >> 5\n    return inner",
  "def supplemental_scan(ind):\n    \"\"\"Create a function to return the supplement scan type from a product-specific block.\"\"\"\n    def inner(seq):\n        # ICD says 1->SAILS, 2->MRLE, but testing on 2020-08-17 makes this seem inverted\n        # given what's being reported by sites in the GSM.\n        return {0: 'Non-supplemental scan',\n                2: 'SAILS scan', 1: 'MRLE scan'}.get(seq[ind] & 0x001F, 'Unknown')\n    return inner",
  "class DataMapper:\n    \"\"\"Convert packed integer data into physical units.\"\"\"\n\n    # Need to find way to handle range folded\n    # RANGE_FOLD = -9999\n    RANGE_FOLD = float('nan')\n    MISSING = float('nan')\n\n    def __init__(self, num=256):\n        self.lut = np.full(num, self.MISSING, dtype=float)\n\n    def __call__(self, data):\n        \"\"\"Convert the values.\"\"\"\n        return self.lut[data]",
  "class DigitalMapper(DataMapper):\n    \"\"\"Maps packed integers to floats using a scale and offset from the product.\"\"\"\n\n    _min_scale = 0.1\n    _inc_scale = 0.1\n    _min_data = 2\n    _max_data = 255\n    range_fold = False\n\n    def __init__(self, prod):\n        \"\"\"Initialize the mapper and the lookup table.\"\"\"\n        super().__init__()\n        min_val = two_comp16(prod.thresholds[0]) * self._min_scale\n        inc = prod.thresholds[1] * self._inc_scale\n        num_levels = prod.thresholds[2]\n\n        # Generate lookup table -- sanity check on num_levels handles\n        # the fact that DHR advertises 256 levels, which *includes*\n        # missing, differing from other products\n        num_levels = min(num_levels, self._max_data - self._min_data + 1)\n        for i in range(num_levels):\n            self.lut[i + self._min_data] = min_val + i * inc",
  "class DigitalRefMapper(DigitalMapper):\n    \"\"\"Mapper for digital reflectivity products.\"\"\"\n\n    units = 'dBZ'",
  "class DigitalVelMapper(DigitalMapper):\n    \"\"\"Mapper for digital velocity products.\"\"\"\n\n    units = 'm/s'\n    range_fold = True",
  "class DigitalSPWMapper(DigitalVelMapper):\n    \"\"\"Mapper for digital spectrum width products.\"\"\"\n\n    _min_data = 129\n    # ICD says up to 152, but also says max value is 19, which implies 129 + 19/0.5 -> 167\n    _max_data = 167",
  "class PrecipArrayMapper(DigitalMapper):\n    \"\"\"Mapper for precipitation array products.\"\"\"\n\n    _inc_scale = 0.001\n    _min_data = 1\n    _max_data = 254\n    units = 'dBA'",
  "class DigitalStormPrecipMapper(DigitalMapper):\n    \"\"\"Mapper for digital storm precipitation products.\"\"\"\n\n    units = 'inches'\n    _inc_scale = 0.01",
  "class DigitalVILMapper(DataMapper):\n    \"\"\"Mapper for digital VIL products.\"\"\"\n\n    def __init__(self, prod):\n        \"\"\"Initialize the VIL mapper.\"\"\"\n        super().__init__()\n        lin_scale = float16(prod.thresholds[0])\n        lin_offset = float16(prod.thresholds[1])\n        log_start = prod.thresholds[2]\n        log_scale = float16(prod.thresholds[3])\n        log_offset = float16(prod.thresholds[4])\n\n        # VIL is allowed to use 2 through 254 inclusive. 0 is thresholded,\n        # 1 is flagged, and 255 is reserved\n        ind = np.arange(255)\n        self.lut[2:log_start] = (ind[2:log_start] - lin_offset) / lin_scale\n        self.lut[log_start:-1] = np.exp((ind[log_start:] - log_offset) / log_scale)",
  "class DigitalEETMapper(DataMapper):\n    \"\"\"Mapper for digital echo tops products.\"\"\"\n\n    def __init__(self, prod):\n        \"\"\"Initialize the mapper.\"\"\"\n        super().__init__()\n        data_mask = prod.thresholds[0]\n        scale = prod.thresholds[1]\n        offset = prod.thresholds[2]\n        topped_mask = prod.thresholds[3]\n        self.topped_lut = [False] * 256\n        for i in range(2, 256):\n            self.lut[i] = ((i & data_mask) - offset) / scale\n            self.topped_lut[i] = bool(i & topped_mask)\n\n        self.topped_lut = np.array(self.topped_lut)\n\n    def __call__(self, data_vals):\n        \"\"\"Convert the data values.\"\"\"\n        return self.lut[data_vals], self.topped_lut[data_vals]",
  "class GenericDigitalMapper(DataMapper):\n    \"\"\"Maps packed integers to floats using a scale and offset from the product.\n\n    Also handles special data flags.\n    \"\"\"\n\n    def __init__(self, prod):\n        \"\"\"Initialize the mapper by pulling out all the information from the product.\"\"\"\n        # Need to treat this value as unsigned, so we can use the full 16-bit range. This\n        # is necessary at least for the DPR product, otherwise it has a value of -1.\n        max_data_val = prod.thresholds[5] & 0xFFFF\n\n        # Values will be [0, max] inclusive, so need to add 1 to max value to get proper size.\n        super().__init__(max_data_val + 1)\n\n        scale = float32(prod.thresholds[0], prod.thresholds[1])\n        offset = float32(prod.thresholds[2], prod.thresholds[3])\n        leading_flags = prod.thresholds[6]\n        trailing_flags = prod.thresholds[7]\n\n        if leading_flags > 1:\n            self.lut[1] = self.RANGE_FOLD\n\n        # Need to add 1 to the end of the range so that it's inclusive\n        for i in range(leading_flags, max_data_val - trailing_flags + 1):\n            self.lut[i] = (i - offset) / scale",
  "class DigitalHMCMapper(DataMapper):\n    \"\"\"Mapper for hydrometeor classification products.\n\n    Handles assigning string labels based on values.\n    \"\"\"\n\n    labels = ['ND', 'BI', 'GC', 'IC', 'DS', 'WS', 'RA', 'HR',\n              'BD', 'GR', 'HA', 'LH', 'GH', 'UK', 'RF']\n\n    def __init__(self, prod):\n        \"\"\"Initialize the mapper.\"\"\"\n        super().__init__()\n        for i in range(10, 256):\n            self.lut[i] = i // 10\n        self.lut[150] = self.RANGE_FOLD",
  "class EDRMapper(DataMapper):\n    \"\"\"Mapper for eddy dissipation rate products.\"\"\"\n\n    def __init__(self, prod):\n        \"\"\"Initialize the mapper based on the product.\"\"\"\n        data_levels = prod.thresholds[2]\n        super().__init__(data_levels)\n        scale = prod.thresholds[0] / 1000.\n        offset = prod.thresholds[1] / 1000.\n        leading_flags = prod.thresholds[3]\n        for i in range(leading_flags, data_levels):\n            self.lut[i] = scale * i + offset",
  "class LegacyMapper(DataMapper):\n    \"\"\"Mapper for legacy products.\"\"\"\n\n    lut_names = ['Blank', 'TH', 'ND', 'RF', 'BI', 'GC', 'IC', 'GR', 'WS',\n                 'DS', 'RA', 'HR', 'BD', 'HA', 'UK']\n\n    def __init__(self, prod):\n        \"\"\"Initialize the values and labels from the product.\"\"\"\n        # Don't worry about super() since we're using our own lut assembled sequentially\n        self.labels = []\n        self.lut = []\n        for t in prod.thresholds:\n            codes, val = t >> 8, t & 0xFF\n            label = ''\n            if codes >> 7:\n                label = self.lut_names[val]\n                if label in ('Blank', 'TH', 'ND'):\n                    val = self.MISSING\n                elif label == 'RF':\n                    val = self.RANGE_FOLD\n\n            elif codes >> 6:\n                val *= 0.01\n                label = f'{val:.2f}'\n            elif codes >> 5:\n                val *= 0.05\n                label = f'{val:.2f}'\n            elif codes >> 4:\n                val *= 0.1\n                label = f'{val:.1f}'\n\n            if codes & 0x1:\n                val *= -1\n                label = '-' + label\n            elif (codes >> 1) & 0x1:\n                label = '+' + label\n\n            if (codes >> 2) & 0x1:\n                label = '<' + label\n            elif (codes >> 3) & 0x1:\n                label = '>' + label\n\n            if not label:\n                label = str(val)\n\n            self.lut.append(val)\n            self.labels.append(label)\n        self.lut = np.array(self.lut)",
  "class Level3File:\n    r\"\"\"Handle reading the wide array of NEXRAD Level 3 (NIDS) product files.\n\n    This class attempts to decode every byte that is in a given product file.\n    It supports all the various compression formats that exist for these\n    products in the wild.\n\n    Attributes\n    ----------\n    metadata : dict\n        Various general metadata available from the product\n    header : `collections.namedtuple`\n        Decoded product header\n    prod_desc : `collections.namedtuple`\n        Decoded product description block\n    siteID : str\n        ID of the site found in the header, empty string if none found\n    lat : float\n        Radar site latitude\n    lon : float\n        Radar site longitude\n    height : float\n        Radar site height AMSL\n    product_name : str\n        Name of the product contained in file\n    max_range : float\n        Maximum kilometer range of the product, taken from the NIDS ICD\n    map_data : `DataMapper`\n        Class instance mapping data int values to proper floating point values\n    sym_block : list, optional\n        Any symbology block packets that were found\n    tab_pages : list, optional\n        Any tabular pages that were found\n    graph_pages : list, optional\n        Any graphical pages that were found\n\n    Notes\n    -----\n    The internal data structure that things are decoded into is still to be\n    determined.\n\n    \"\"\"\n\n    ij_to_km = 0.25\n    wmo_finder = re.compile('((?:NX|SD|NO)US)\\\\d{2}[\\\\s\\\\w\\\\d]+\\\\w*(\\\\w{3})\\r\\r\\n')\n    header_fmt = NamedStruct([('code', 'H'), ('date', 'H'), ('time', 'l'),\n                              ('msg_len', 'L'), ('src_id', 'h'), ('dest_id', 'h'),\n                              ('num_blks', 'H')], '>', 'MsgHdr')\n    # See figure 3-17 in 2620001 document for definition of status bit fields\n    gsm_fmt = NamedStruct([('divider', 'h'), ('block_len', 'H'),\n                           ('op_mode', 'h', BitField('Clear Air', 'Precip')),\n                           ('rda_op_status', 'h', BitField('Spare', 'Online',\n                                                           'Maintenance Required',\n                                                           'Maintenance Mandatory',\n                                                           'Commanded Shutdown', 'Inoperable',\n                                                           'Spare', 'Wideband Disconnect')),\n                           ('vcp', 'h'), ('num_el', 'h'),\n                           ('el1', 'h', scaler(0.1)), ('el2', 'h', scaler(0.1)),\n                           ('el3', 'h', scaler(0.1)), ('el4', 'h', scaler(0.1)),\n                           ('el5', 'h', scaler(0.1)), ('el6', 'h', scaler(0.1)),\n                           ('el7', 'h', scaler(0.1)), ('el8', 'h', scaler(0.1)),\n                           ('el9', 'h', scaler(0.1)), ('el10', 'h', scaler(0.1)),\n                           ('el11', 'h', scaler(0.1)), ('el12', 'h', scaler(0.1)),\n                           ('el13', 'h', scaler(0.1)), ('el14', 'h', scaler(0.1)),\n                           ('el15', 'h', scaler(0.1)), ('el16', 'h', scaler(0.1)),\n                           ('el17', 'h', scaler(0.1)), ('el18', 'h', scaler(0.1)),\n                           ('el19', 'h', scaler(0.1)), ('el20', 'h', scaler(0.1)),\n                           ('rda_status', 'h', BitField('Spare', 'Startup', 'Standby',\n                                                        'Restart', 'Operate',\n                                                        'Off-line Operate')),\n                           ('rda_alarms', 'h', BitField('Indeterminate', 'Tower/Utilities',\n                                                        'Pedestal', 'Transmitter', 'Receiver',\n                                                        'RDA Control', 'RDA Communications',\n                                                        'Signal Processor')),\n                           ('tranmission_enable', 'h', BitField('Spare', 'None',\n                                                                'Reflectivity',\n                                                                'Velocity', 'Spectrum Width',\n                                                                'Dual Pol')),\n                           ('rpg_op_status', 'h', BitField('Loadshed', 'Online',\n                                                           'Maintenance Required',\n                                                           'Maintenance Mandatory',\n                                                           'Commanded shutdown')),\n                           ('rpg_alarms', 'h', BitField('None', 'Node Connectivity',\n                                                        'Wideband Failure',\n                                                        'RPG Control Task Failure',\n                                                        'Data Base Failure', 'Spare',\n                                                        'RPG Input Buffer Loadshed',\n                                                        'Spare', 'Product Storage Loadshed'\n                                                        'Spare', 'Spare', 'Spare',\n                                                        'RPG/RPG Intercomputer Link Failure',\n                                                        'Redundant Channel Error',\n                                                        'Task Failure', 'Media Failure')),\n                           ('rpg_status', 'h', BitField('Restart', 'Operate', 'Standby')),\n                           ('rpg_narrowband_status', 'h', BitField('Commanded Disconnect',\n                                                                   'Narrowband Loadshed')),\n                           ('h_ref_calib', 'h', scaler(0.25)),\n                           ('prod_avail', 'h', BitField('Product Availability',\n                                                        'Degraded Availability',\n                                                        'Not Available')),\n                           ('super_res_cuts', 'h', Bits(16)),\n                           ('cmd_status', 'h', Bits(6)),\n                           ('v_ref_calib', 'h', scaler(0.25)),\n                           ('rda_build', 'h', version), ('rda_channel', 'h'),\n                           ('reserved', 'h'), ('reserved2', 'h'),\n                           ('build_version', 'h', version)], '>', 'GSM')\n    # Build 14.0 added more bytes to the GSM\n    additional_gsm_fmt = NamedStruct([('el21', 'h', scaler(0.1)),\n                                      ('el22', 'h', scaler(0.1)),\n                                      ('el23', 'h', scaler(0.1)),\n                                      ('el24', 'h', scaler(0.1)),\n                                      ('el25', 'h', scaler(0.1)),\n                                      ('vcp_supplemental', 'H',\n                                       BitField('AVSET', 'SAILS', 'Site VCP', 'RxR Noise',\n                                                'CBT', 'VCP Sequence', 'SPRT', 'MRLE',\n                                                'Base Tilt', 'MPDA')),\n                                      ('supplemental_cut_map', 'H', Bits(16)),\n                                      ('supplemental_cut_count', 'B'),\n                                      ('supplemental_cut_map2', 'B', Bits(8)),\n                                      ('spare', '80s')], '>', 'GSM')\n    prod_desc_fmt = NamedStruct([('divider', 'h'), ('lat', 'l'), ('lon', 'l'),\n                                 ('height', 'h'), ('prod_code', 'h'),\n                                 ('op_mode', 'h'), ('vcp', 'h'), ('seq_num', 'h'),\n                                 ('vol_num', 'h'), ('vol_date', 'h'),\n                                 ('vol_start_time', 'l'), ('prod_gen_date', 'h'),\n                                 ('prod_gen_time', 'l'), ('dep1', 'h'),\n                                 ('dep2', 'h'), ('el_num', 'h'), ('dep3', 'h'),\n                                 ('thr1', 'h'), ('thr2', 'h'), ('thr3', 'h'),\n                                 ('thr4', 'h'), ('thr5', 'h'), ('thr6', 'h'),\n                                 ('thr7', 'h'), ('thr8', 'h'), ('thr9', 'h'),\n                                 ('thr10', 'h'), ('thr11', 'h'), ('thr12', 'h'),\n                                 ('thr13', 'h'), ('thr14', 'h'), ('thr15', 'h'),\n                                 ('thr16', 'h'), ('dep4', 'h'), ('dep5', 'h'),\n                                 ('dep6', 'h'), ('dep7', 'h'), ('dep8', 'h'),\n                                 ('dep9', 'h'), ('dep10', 'h'), ('version', 'b'),\n                                 ('spot_blank', 'b'), ('sym_off', 'L'), ('graph_off', 'L'),\n                                 ('tab_off', 'L')], '>', 'ProdDesc')\n    sym_block_fmt = NamedStruct([('divider', 'h'), ('block_id', 'h'),\n                                 ('block_len', 'L'), ('nlayer', 'H')], '>', 'SymBlock')\n    tab_header_fmt = NamedStruct([('divider', 'h'), ('block_id', 'h'),\n                                  ('block_len', 'L')], '>', 'TabHeader')\n    tab_block_fmt = NamedStruct([('divider', 'h'), ('num_pages', 'h')], '>', 'TabBlock')\n    sym_layer_fmt = NamedStruct([('divider', 'h'), ('length', 'L')], '>',\n                                'SymLayer')\n    graph_block_fmt = NamedStruct([('divider', 'h'), ('block_id', 'h'),\n                                   ('block_len', 'L'), ('num_pages', 'H')], '>', 'GraphBlock')\n    standalone_tabular = [62, 73, 75, 82]\n    prod_spec_map = {16: ('Base Reflectivity', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     17: ('Base Reflectivity', 460., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     18: ('Base Reflectivity', 460., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     19: ('Base Reflectivity', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('delta_time', delta_time(6)),\n                           ('supplemental_scan', supplemental_scan(6)),\n                           ('calib_const', float_elem(7, 8)))),\n                     20: ('Base Reflectivity', 460., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('delta_time', delta_time(6)),\n                           ('supplemental_scan', supplemental_scan(6)),\n                           ('calib_const', float_elem(7, 8)))),\n                     21: ('Base Reflectivity', 460., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     22: ('Base Velocity', 60., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('min', 3), ('max', 4))),\n                     23: ('Base Velocity', 115., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('min', 3), ('max', 4))),\n                     24: ('Base Velocity', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('min', 3), ('max', 4))),\n                     25: ('Base Velocity', 60., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('min', 3), ('max', 4))),\n                     26: ('Base Velocity', 115., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('min', 3), ('max', 4))),\n                     27: ('Base Velocity', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('min', 3), ('max', 4),\n                           ('delta_time', delta_time(6)),\n                           ('supplemental_scan', supplemental_scan(6)))),\n                     28: ('Base Spectrum Width', 60., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3))),\n                     29: ('Base Spectrum Width', 115., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3))),\n                     30: ('Base Spectrum Width', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('delta_time', delta_time(6)),\n                           ('supplemental_scan', supplemental_scan(6)))),\n                     31: ('User Selectable Storm Total Precipitation', 230., LegacyMapper,\n                          (('end_hour', 0),\n                           ('hour_span', 1),\n                           ('null_product', 2),\n                           ('max_rainfall', scaled_elem(3, 0.1)),\n                           ('rainfall_begin', date_elem(4, 5)),\n                           ('rainfall_end', date_elem(6, 7)),\n                           ('bias', scaled_elem(8, 0.01)),\n                           ('gr_pairs', scaled_elem(5, 0.01)))),\n                     32: ('Digital Hybrid Scan Reflectivity', 230., DigitalRefMapper,\n                          (('max', 3),\n                           ('avg_time', date_elem(4, 5)),\n                           ('compression', 7),\n                           ('uncompressed_size', combine_elem(8, 9)))),\n                     33: ('Hybrid Scan Reflectivity', 230., LegacyMapper,\n                          (('max', 3), ('avg_time', date_elem(4, 5)))),\n                     34: ('Clutter Filter Control', 230., LegacyMapper,\n                          (('clutter_bitmap', 0),\n                           ('cmd_map', 1),\n                           ('bypass_map_date', date_elem(4, 5)),\n                           ('notchwidth_map_date', date_elem(6, 7)))),\n                     35: ('Composite Reflectivity', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     36: ('Composite Reflectivity', 460., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     37: ('Composite Reflectivity', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     38: ('Composite Reflectivity', 460., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     41: ('Echo Tops', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', scaled_elem(3, 1000)))),  # Max in ft\n                     48: ('VAD Wind Profile', None, LegacyMapper,\n                          (('max', 3),\n                           ('dir_max', 4),\n                           ('alt_max', scaled_elem(5, 10)))),  # Max in ft\n                     50: ('Cross Section Reflectivity', 230., LegacyMapper,\n                          (('azimuth1', scaled_elem(0, 0.1)),\n                           ('range1', scaled_elem(1, 0.1)),\n                           ('azimuth2', scaled_elem(2, 0.1)),\n                           ('range2', scaled_elem(3, 0.1)))),\n                     51: ('Cross Section Velocity', 230., LegacyMapper,\n                          (('azimuth1', scaled_elem(0, 0.1)),\n                           ('range1', scaled_elem(1, 0.1)),\n                           ('azimuth2', scaled_elem(2, 0.1)),\n                           ('range2', scaled_elem(3, 0.1)))),\n                     55: ('Storm Relative Mean Radial Velocity', 50., LegacyMapper,\n                          (('window_az', scaled_elem(0, 0.1)),\n                           ('window_range', scaled_elem(1, 0.1)),\n                           ('el_angle', scaled_elem(2, 0.1)),\n                           ('min', 3),\n                           ('max', 4),\n                           ('source', 5),\n                           ('height', 6),\n                           ('avg_speed', scaled_elem(7, 0.1)),\n                           ('avg_dir', scaled_elem(8, 0.1)),\n                           ('alert_category', 9))),\n                     56: ('Storm Relative Mean Radial Velocity', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('min', 3),\n                           ('max', 4),\n                           ('source', 5),\n                           ('avg_speed', scaled_elem(7, 0.1)),\n                           ('avg_dir', scaled_elem(8, 0.1)))),\n                     57: ('Vertically Integrated Liquid', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3))),  # Max in kg / m^2\n                     58: ('Storm Tracking Information', 460., LegacyMapper,\n                          (('num_storms', 3),)),\n                     59: ('Hail Index', 230., LegacyMapper, ()),\n                     61: ('Tornado Vortex Signature', 230., LegacyMapper,\n                          (('num_tvs', 3), ('num_etvs', 4))),\n                     62: ('Storm Structure', 460., LegacyMapper, ()),\n                     63: ('Layer Composite Reflectivity (Layer 1 Average)', 230., LegacyMapper,\n                          (('max', 3),\n                           ('layer_bottom', scaled_elem(4, 1000.)),\n                           ('layer_top', scaled_elem(5, 1000.)),\n                           ('calib_const', float_elem(7, 8)))),\n                     64: ('Layer Composite Reflectivity (Layer 2 Average)', 230., LegacyMapper,\n                          (('max', 3),\n                           ('layer_bottom', scaled_elem(4, 1000.)),\n                           ('layer_top', scaled_elem(5, 1000.)),\n                           ('calib_const', float_elem(7, 8)))),\n                     65: ('Layer Composite Reflectivity (Layer 1 Max)', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('layer_bottom', scaled_elem(4, 1000.)),\n                           ('layer_top', scaled_elem(5, 1000.)),\n                           ('calib_const', float_elem(7, 8)))),\n                     66: ('Layer Composite Reflectivity (Layer 2 Max)', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('layer_bottom', scaled_elem(4, 1000.)),\n                           ('layer_top', scaled_elem(5, 1000.)),\n                           ('calib_const', float_elem(7, 8)))),\n                     67: ('Layer Composite Reflectivity - AP Removed', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('layer_bottom', scaled_elem(4, 1000.)),\n                           ('layer_top', scaled_elem(5, 1000.)),\n                           ('calib_const', float_elem(7, 8)))),\n                     74: ('Radar Coded Message', 460., LegacyMapper, ()),\n                     78: ('Surface Rainfall Accumulation (1 hour)', 230., LegacyMapper,\n                          (('max_rainfall', scaled_elem(3, 0.1)),\n                           ('bias', scaled_elem(4, 0.01)),\n                           ('gr_pairs', scaled_elem(5, 0.01)),\n                           ('rainfall_end', date_elem(6, 7)))),\n                     79: ('Surface Rainfall Accumulation (3 hour)', 230., LegacyMapper,\n                          (('max_rainfall', scaled_elem(3, 0.1)),\n                           ('bias', scaled_elem(4, 0.01)),\n                           ('gr_pairs', scaled_elem(5, 0.01)),\n                           ('rainfall_end', date_elem(6, 7)))),\n                     80: ('Storm Total Rainfall Accumulation', 230., LegacyMapper,\n                          (('max_rainfall', scaled_elem(3, 0.1)),\n                           ('rainfall_begin', date_elem(4, 5)),\n                           ('rainfall_end', date_elem(6, 7)),\n                           ('bias', scaled_elem(8, 0.01)),\n                           ('gr_pairs', scaled_elem(9, 0.01)))),\n                     81: ('Hourly Digital Precipitation Array', 230., PrecipArrayMapper,\n                          (('max_rainfall', scaled_elem(3, 0.001)),\n                           ('bias', scaled_elem(4, 0.01)),\n                           ('gr_pairs', scaled_elem(5, 0.01)),\n                           ('rainfall_end', date_elem(6, 7)))),\n                     82: ('Supplemental Precipitation Data', None, LegacyMapper, ()),\n                     89: ('Layer Composite Reflectivity (Layer 3 Average)', 230., LegacyMapper,\n                          (('max', 3),\n                           ('layer_bottom', scaled_elem(4, 1000.)),\n                           ('layer_top', scaled_elem(5, 1000.)),\n                           ('calib_const', float_elem(7, 8)))),\n                     90: ('Layer Composite Reflectivity (Layer 3 Max)', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('layer_bottom', scaled_elem(4, 1000.)),\n                           ('layer_top', scaled_elem(5, 1000.)),\n                           ('calib_const', float_elem(7, 8)))),\n                     93: ('ITWS Digital Base Velocity', 115., DigitalVelMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('min', 3),\n                           ('max', 4), ('precision', 6))),\n                     94: ('Base Reflectivity Data Array', 460., DigitalRefMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('delta_time', delta_time(6)),\n                           ('supplemental_scan', supplemental_scan(6)),\n                           ('compression', 7),\n                           ('uncompressed_size', combine_elem(8, 9)))),\n                     95: ('Composite Reflectivity Edited for AP', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     96: ('Composite Reflectivity Edited for AP', 460., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     97: ('Composite Reflectivity Edited for AP', 230., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     98: ('Composite Reflectivity Edited for AP', 460., LegacyMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('max', 3),\n                           ('calib_const', float_elem(7, 8)))),\n                     99: ('Base Velocity Data Array', 300., DigitalVelMapper,\n                          (('el_angle', scaled_elem(2, 0.1)),\n                           ('min', 3),\n                           ('max', 4),\n                           ('delta_time', delta_time(6)),\n                           ('supplemental_scan', supplemental_scan(6)),\n                           ('compression', 7),\n                           ('uncompressed_size', combine_elem(8, 9)))),\n                     113: ('Power Removed Control', 300., LegacyMapper,\n                           (('rpg_cut_num', 0), ('cmd_generated', 1),\n                            ('el_angle', scaled_elem(2, 0.1)),\n                            ('clutter_filter_map_dt', date_elem(4, 3)),\n                            # While the 2620001Y ICD doesn't talk about using these\n                            # product-specific blocks for this product, they have data in them\n                            # and the compression info is necessary for proper decoding.\n                            ('compression', 7), ('uncompressed_size', combine_elem(8, 9)))),\n                     132: ('Clutter Likelihood Reflectivity', 230., LegacyMapper,\n                           (('el_angle', scaled_elem(2, 0.1)), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)),)),\n                     133: ('Clutter Likelihood Doppler', 230., LegacyMapper,\n                           (('el_angle', scaled_elem(2, 0.1)), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)),)),\n                     134: ('High Resolution VIL', 460., DigitalVILMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('max', 3),\n                            ('num_edited', 4),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     135: ('Enhanced Echo Tops', 345., DigitalEETMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('max', scaled_elem(3, 1000.)),  # Max in ft\n                            ('num_edited', 4),\n                            ('ref_thresh', 5),\n                            ('points_removed', 6),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     138: ('Digital Storm Total Precipitation', 230., DigitalStormPrecipMapper,\n                           (('rainfall_begin', date_elem(0, 1)),\n                            ('bias', scaled_elem(2, 0.01)),\n                            ('max', scaled_elem(3, 0.01)),\n                            ('rainfall_end', date_elem(4, 5)),\n                            ('gr_pairs', scaled_elem(6, 0.01)),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     141: ('Mesocyclone Detection', 230., LegacyMapper,\n                           (('min_ref_thresh', 0),\n                            ('overlap_display_filter', 1),\n                            ('min_strength_rank', 2))),\n                     152: ('Archive III Status Product', None, LegacyMapper,\n                           (('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     153: ('Super Resolution Reflectivity Data Array', 460., DigitalRefMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('max', 3), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)), ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     154: ('Super Resolution Velocity Data Array', 300., DigitalVelMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('min', 3), ('max', 4), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)), ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     155: ('Super Resolution Spectrum Width Data Array', 300.,\n                           DigitalSPWMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('max', 3), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)), ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     156: ('Turbulence Detection (Eddy Dissipation Rate)', 230., EDRMapper,\n                           (('el_start_time', 0),\n                            ('el_end_time', 1),\n                            ('el_angle', scaled_elem(2, 0.1)),\n                            ('min_el', scaled_elem(3, 0.01)),\n                            ('mean_el', scaled_elem(4, 0.01)),\n                            ('max_el', scaled_elem(5, 0.01)))),\n                     157: ('Turbulence Detection (Eddy Dissipation Rate Confidence)', 230.,\n                           EDRMapper,\n                           (('el_start_time', 0),\n                            ('el_end_time', 1),\n                            ('el_angle', scaled_elem(2, 0.1)),\n                            ('min_el', scaled_elem(3, 0.01)),\n                            ('mean_el', scaled_elem(4, 0.01)),\n                            ('max_el', scaled_elem(5, 0.01)))),\n                     158: ('Differential Reflectivity', 230., LegacyMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('min', scaled_elem(3, 0.1)),\n                            ('max', scaled_elem(4, 0.1)))),\n                     159: ('Digital Differential Reflectivity', 300., GenericDigitalMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('min', scaled_elem(3, 0.1)),\n                            ('max', scaled_elem(4, 0.1)), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)), ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     160: ('Correlation Coefficient', 230., LegacyMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('min', scaled_elem(3, 0.00333)),\n                            ('max', scaled_elem(4, 0.00333)))),\n                     161: ('Digital Correlation Coefficient', 300., GenericDigitalMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('min', scaled_elem(3, 0.00333)),\n                            ('max', scaled_elem(4, 0.00333)), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)), ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     162: ('Specific Differential Phase', 230., LegacyMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('min', scaled_elem(3, 0.05)),\n                            ('max', scaled_elem(4, 0.05)))),\n                     163: ('Digital Specific Differential Phase', 300., GenericDigitalMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('min', scaled_elem(3, 0.05)),\n                            ('max', scaled_elem(4, 0.05)), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)), ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     164: ('Hydrometeor Classification', 230., LegacyMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),)),\n                     165: ('Digital Hydrometeor Classification', 300., DigitalHMCMapper,\n                           (('el_angle', scaled_elem(2, 0.1)), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)), ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     166: ('Melting Layer', 230., LegacyMapper,\n                           (('el_angle', scaled_elem(2, 0.1)), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)),)),\n                     167: ('Super Res Digital Correlation Coefficient', 300.,\n                           GenericDigitalMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('min', scaled_elem(3, 0.00333)),\n                            ('max', scaled_elem(4, 0.00333)), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)), ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     168: ('Super Res Digital Phi', 300., GenericDigitalMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('min', 3), ('max', 4), ('delta_time', delta_time(6)),\n                            ('supplemental_scan', supplemental_scan(6)), ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     169: ('One Hour Accumulation', 230., LegacyMapper,\n                           (('null_product', low_byte(2)),\n                            ('max', scaled_elem(3, 0.1)),\n                            ('rainfall_end', date_elem(4, 5)),\n                            ('bias', scaled_elem(6, 0.01)),\n                            ('gr_pairs', scaled_elem(7, 0.01)))),\n                     170: ('Digital Accumulation Array', 230., GenericDigitalMapper,\n                           (('null_product', low_byte(2)),\n                            ('max', scaled_elem(3, 0.1)),\n                            ('rainfall_end', date_elem(4, 5)),\n                            ('bias', scaled_elem(6, 0.01)),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     171: ('Storm Total Accumulation', 230., LegacyMapper,\n                           (('rainfall_begin', date_elem(0, 1)),\n                            ('null_product', low_byte(2)),\n                            ('max', scaled_elem(3, 0.1)),\n                            ('rainfall_end', date_elem(4, 5)),\n                            ('bias', scaled_elem(6, 0.01)),\n                            ('gr_pairs', scaled_elem(7, 0.01)))),\n                     172: ('Digital Storm Total Accumulation', 230., GenericDigitalMapper,\n                           (('rainfall_begin', date_elem(0, 1)),\n                            ('null_product', low_byte(2)),\n                            ('max', scaled_elem(3, 0.1)),\n                            ('rainfall_end', date_elem(4, 5)),\n                            ('bias', scaled_elem(6, 0.01)),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     173: ('Digital User-Selectable Accumulation', 230., GenericDigitalMapper,\n                           (('period', 1),\n                            ('missing_period', high_byte(2)),\n                            ('null_product', low_byte(2)),\n                            ('max', scaled_elem(3, 0.1)),\n                            ('rainfall_end', date_elem(4, 0)),\n                            ('start_time', 5),\n                            ('bias', scaled_elem(6, 0.01)),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     174: ('Digital One-Hour Difference Accumulation', 230.,\n                           GenericDigitalMapper,\n                           (('max', scaled_elem(3, 0.1)),\n                            ('rainfall_end', date_elem(4, 5)),\n                            ('min', scaled_elem(6, 0.1)),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     175: ('Digital Storm Total Difference Accumulation', 230.,\n                           GenericDigitalMapper,\n                           (('rainfall_begin', date_elem(0, 1)),\n                            ('null_product', low_byte(2)),\n                            ('max', scaled_elem(3, 0.1)),\n                            ('rainfall_end', date_elem(4, 5)),\n                            ('min', scaled_elem(6, 0.1)),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     176: ('Digital Instantaneous Precipitation Rate', 230.,\n                           GenericDigitalMapper,\n                           (('rainfall_begin', date_elem(0, 1)),\n                            ('precip_detected', high_byte(2)),\n                            ('need_bias', low_byte(2)),\n                            ('max', 3),\n                            ('percent_filled', scaled_elem(4, 0.01)),\n                            ('max_elev', scaled_elem(5, 0.1)),\n                            ('bias', scaled_elem(6, 0.01)),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     177: ('Hybrid Hydrometeor Classification', 230., DigitalHMCMapper,\n                           (('mode_filter_size', 3),\n                            ('hybrid_percent_filled', 4),\n                            ('max_elev', scaled_elem(5, 0.1)),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     180: ('TDWR Base Reflectivity', 90., DigitalRefMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('max', 3),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     181: ('TDWR Base Reflectivity', 90., LegacyMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('max', 3))),\n                     182: ('TDWR Base Velocity', 90., DigitalVelMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('min', 3),\n                            ('max', 4),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     183: ('TDWR Base Velocity', 90., LegacyMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('min', 3),\n                            ('max', 4))),\n                     185: ('TDWR Base Spectrum Width', 90., LegacyMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('max', 3))),\n                     186: ('TDWR Long Range Base Reflectivity', 416., DigitalRefMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('max', 3),\n                            ('compression', 7),\n                            ('uncompressed_size', combine_elem(8, 9)))),\n                     187: ('TDWR Long Range Base Reflectivity', 416., LegacyMapper,\n                           (('el_angle', scaled_elem(2, 0.1)),\n                            ('max', 3)))}\n\n    def __init__(self, filename):\n        r\"\"\"Create instance of `Level3File`.\n\n        Parameters\n        ----------\n        filename : str or file-like object\n            If str, the name of the file to be opened. If file-like object,\n            this will be read from directly.\n\n        \"\"\"\n        fobj = open_as_needed(filename)\n        if isinstance(filename, str):\n            self.filename = filename\n        elif isinstance(filename, pathlib.Path):\n            self.filename = str(filename)\n        else:\n            self.filename = 'No File'\n\n        # Just read in the entire set of data at once\n        with contextlib.closing(fobj):\n            self._buffer = IOBuffer.fromfile(fobj)\n\n        # Pop off the WMO header if we find it\n        self._process_wmo_header()\n\n        # Pop off last 4 bytes if necessary\n        self._process_end_bytes()\n\n        # Set up places to store data and metadata\n        self.metadata = {}\n\n        # Handle free text message products that are pure text\n        if self.wmo_code == 'NOUS':\n            self.header = None\n            self.prod_desc = None\n            self.thresholds = None\n            self.depVals = None\n            self.product_name = 'Free Text Message'\n            self.text = ''.join(self._buffer.read_ascii())\n            return\n\n        # Decompress the data if necessary, and if so, pop off new header\n        self._buffer = IOBuffer(self._buffer.read_func(zlib_decompress_all_frames))\n        self._process_wmo_header()\n\n        # Check for empty product\n        if len(self._buffer) == 0:\n            log.warning('%s: Empty product!', self.filename)\n            return\n\n        # Unpack the message header and the product description block\n        msg_start = self._buffer.set_mark()\n        self.header = self._buffer.read_struct(self.header_fmt)\n        log.debug('Buffer size: %d (%d expected) Header: %s', len(self._buffer),\n                  self.header.msg_len, self.header)\n\n        if not self._buffer.check_remains(self.header.msg_len - self.header_fmt.size):\n            log.warning('Product contains an unexpected amount of data remaining--have: %d '\n                        'expected: %d. This product may not parse correctly.',\n                        len(self._buffer) - self._buffer._offset,\n                        self.header.msg_len - self.header_fmt.size)\n\n        # Handle GSM and jump out\n        if self.header.code == 2:\n            self.gsm = self._buffer.read_struct(self.gsm_fmt)\n            self.product_name = 'General Status Message'\n            assert self.gsm.divider == -1\n            if self.gsm.block_len > 82:\n                # Due to the way the structures read it in, one bit from the count needs\n                # to be popped off and added as the supplemental cut status for the 25th\n                # elevation cut.\n                more = self._buffer.read_struct(self.additional_gsm_fmt)\n                cut_count = more.supplemental_cut_count\n                more.supplemental_cut_map2.append(bool(cut_count & 0x1))\n                self.gsm_additional = more._replace(supplemental_cut_count=cut_count >> 1)\n                assert self.gsm.block_len == 178\n            else:\n                assert self.gsm.block_len == 82\n            return\n\n        self.prod_desc = self._buffer.read_struct(self.prod_desc_fmt)\n        log.debug('Product description block: %s', self.prod_desc)\n\n        # Convert thresholds and dependent values to lists of values\n        self.thresholds = [getattr(self.prod_desc, f'thr{i}') for i in range(1, 17)]\n        self.depVals = [getattr(self.prod_desc, f'dep{i}') for i in range(1, 11)]\n\n        # Set up some time/location metadata\n        self.metadata['msg_time'] = nexrad_to_datetime(self.header.date,\n                                                       self.header.time * 1000)\n        self.metadata['vol_time'] = nexrad_to_datetime(self.prod_desc.vol_date,\n                                                       self.prod_desc.vol_start_time * 1000)\n        self.metadata['prod_time'] = nexrad_to_datetime(self.prod_desc.prod_gen_date,\n                                                        self.prod_desc.prod_gen_time * 1000)\n        self.lat = self.prod_desc.lat * 0.001\n        self.lon = self.prod_desc.lon * 0.001\n        self.height = self.prod_desc.height\n\n        # Handle product-specific blocks. Default to compression and elevation angle\n        # Also get other product specific information, like name,\n        # maximum range, and how to map data bytes to values\n        default = ('Unknown Product', 230., LegacyMapper,\n                   (('el_angle', scaled_elem(2, 0.1)), ('compression', 7),\n                    ('uncompressed_size', combine_elem(8, 9)), ('defaultVals', 0)))\n        self.product_name, self.max_range, mapper, meta = self.prod_spec_map.get(\n            self.header.code, default)\n        log.debug('Product info--name: %s max_range: %f mapper: %s metadata: %s',\n                  self.product_name, self.max_range, mapper, meta)\n\n        for name, block in meta:\n            if callable(block):\n                self.metadata[name] = block(self.depVals)\n            else:\n                self.metadata[name] = self.depVals[block]\n\n        # Now that we have the header, we have everything needed to make tables\n        # Store as class that can be called\n        self.map_data = mapper(self)\n\n        # Process compression if indicated. We need to fail\n        # gracefully here since we default to it being on\n        if self.metadata.get('compression', False):\n            try:\n                comp_start = self._buffer.set_mark()\n                decomp_data = self._buffer.read_func(bz2.decompress)\n                self._buffer.splice(comp_start, decomp_data)\n                assert self._buffer.check_remains(self.metadata['uncompressed_size'])\n            except OSError:\n                # Compression didn't work, so we just assume it wasn't actually compressed.\n                pass\n\n        # Unpack the various blocks, if present. The factor of 2 converts from\n        # 'half-words' to bytes\n        # Check to see if this is one of the \"special\" products that uses\n        # header-free blocks and re-assigns the offsets\n        if self.header.code in self.standalone_tabular:\n            if self.prod_desc.sym_off:\n                # For standalone tabular alphanumeric, symbology offset is\n                # actually tabular\n                self._unpack_tabblock(msg_start, 2 * self.prod_desc.sym_off, False)\n            if self.prod_desc.graph_off:\n                # Offset seems to be off by 1 from where we're counting, but\n                # it's not clear why.\n                self._unpack_standalone_graphblock(msg_start,\n                                                   2 * (self.prod_desc.graph_off - 1))\n        # Need special handling for (old) radar coded message format\n        elif self.header.code == 74:\n            self._unpack_rcm(msg_start, 2 * self.prod_desc.sym_off)\n        else:\n            if self.prod_desc.sym_off:\n                self._unpack_symblock(msg_start, 2 * self.prod_desc.sym_off)\n            if self.prod_desc.graph_off:\n                self._unpack_graphblock(msg_start, 2 * self.prod_desc.graph_off)\n            if self.prod_desc.tab_off:\n                self._unpack_tabblock(msg_start, 2 * self.prod_desc.tab_off)\n\n        if 'defaultVals' in self.metadata:\n            log.warning('%s: Using default metadata for product %d',\n                        self.filename, self.header.code)\n\n    def _process_wmo_header(self):\n        # Read off the WMO header if necessary\n        data = self._buffer.get_next(64).decode('ascii', 'ignore')\n        match = self.wmo_finder.search(data)\n        log.debug('WMO Header: %s', match)\n        if match:\n            self.wmo_code = match.groups()[0]\n            self.siteID = match.groups()[-1]\n            self._buffer.skip(match.end())\n        else:\n            self.wmo_code = ''\n\n    def _process_end_bytes(self):\n        check_bytes = self._buffer[-4:-1]\n        log.debug('End Bytes: %s', check_bytes)\n        if check_bytes in (b'\\r\\r\\n', b'\\xff\\xff\\n'):\n            self._buffer.truncate(4)\n\n    @staticmethod\n    def _unpack_rle_data(data):\n        # Unpack Run-length encoded data\n        unpacked = []\n        for run in data:\n            num, val = run >> 4, run & 0x0F\n            unpacked.extend([val] * num)\n        return unpacked\n\n    @staticmethod\n    def pos_scale(is_sym_block):\n        \"\"\"Scale of the position information in km.\"\"\"\n        return 0.25 if is_sym_block else 1\n\n    def _unpack_rcm(self, start, offset):\n        self._buffer.jump_to(start, offset)\n        header = self._buffer.read_ascii(10)\n        assert header == '1234 ROBUU'\n        text_data = self._buffer.read_ascii()\n        end = 0\n        # Appendix B of ICD tells how to interpret this stuff, but that just\n        # doesn't seem worth it.\n        for marker, name in [('AA', 'ref'), ('BB', 'vad'), ('CC', 'remarks')]:\n            start = text_data.find('/NEXR' + marker, end)\n            # For part C the search for end fails, but returns -1, which works\n            end = text_data.find('/END' + marker, start)\n            setattr(self, 'rcm_' + name, text_data[start:end])\n\n    def _unpack_symblock(self, start, offset):\n        self._buffer.jump_to(start, offset)\n        blk = self._buffer.read_struct(self.sym_block_fmt)\n        log.debug('Symbology block info: %s', blk)\n\n        self.sym_block = []\n        assert blk.divider == -1, ('Bad divider for symbology block: {:d} should be -1'\n                                   .format(blk.divider))\n        assert blk.block_id == 1, ('Bad block ID for symbology block: {:d} should be 1'\n                                   .format(blk.block_id))\n        for _ in range(blk.nlayer):\n            layer_hdr = self._buffer.read_struct(self.sym_layer_fmt)\n            assert layer_hdr.divider == -1\n            layer = []\n            self.sym_block.append(layer)\n            layer_start = self._buffer.set_mark()\n            while self._buffer.offset_from(layer_start) < layer_hdr.length:\n                packet_code = self._buffer.read_int(2, 'big', signed=False)\n                log.debug('Symbology packet: %d', packet_code)\n                if packet_code in self.packet_map:\n                    layer.append(self.packet_map[packet_code](self, packet_code, True))\n                else:\n                    log.warning('%s: Unknown symbology packet type %d/%x.',\n                                self.filename, packet_code, packet_code)\n                    self._buffer.jump_to(layer_start, layer_hdr.length)\n            assert self._buffer.offset_from(layer_start) == layer_hdr.length\n\n    def _unpack_graphblock(self, start, offset):\n        self._buffer.jump_to(start, offset)\n        hdr = self._buffer.read_struct(self.graph_block_fmt)\n        assert hdr.divider == -1, ('Bad divider for graphical block: {:d} should be -1'\n                                   .format(hdr.divider))\n        assert hdr.block_id == 2, ('Bad block ID for graphical block: {:d} should be 1'\n                                   .format(hdr.block_id))\n        self.graph_pages = []\n        for page in range(hdr.num_pages):\n            page_num = self._buffer.read_int(2, 'big', signed=False)\n            assert page + 1 == page_num\n            page_size = self._buffer.read_int(2, 'big', signed=False)\n            page_start = self._buffer.set_mark()\n            packets = []\n            while self._buffer.offset_from(page_start) < page_size:\n                packet_code = self._buffer.read_int(2, 'big', signed=False)\n                if packet_code in self.packet_map:\n                    packets.append(self.packet_map[packet_code](self, packet_code, False))\n                else:\n                    log.warning('%s: Unknown graphical packet type %d/%x.',\n                                self.filename, packet_code, packet_code)\n                    self._buffer.skip(page_size)\n            self.graph_pages.append(packets)\n\n    def _unpack_standalone_graphblock(self, start, offset):\n        self._buffer.jump_to(start, offset)\n        packets = []\n        while not self._buffer.at_end():\n            packet_code = self._buffer.read_int(2, 'big', signed=False)\n            if packet_code in self.packet_map:\n                packets.append(self.packet_map[packet_code](self, packet_code, False))\n            else:\n                log.warning('%s: Unknown standalone graphical packet type %d/%x.',\n                            self.filename, packet_code, packet_code)\n                # Assume next 2 bytes is packet length and try skipping\n                num_bytes = self._buffer.read_int(2, 'big', signed=False)\n                self._buffer.skip(num_bytes)\n        self.graph_pages = [packets]\n\n    def _unpack_tabblock(self, start, offset, have_header=True):\n        self._buffer.jump_to(start, offset)\n        block_start = self._buffer.set_mark()\n\n        # Read the header and validate if needed\n        if have_header:\n            header = self._buffer.read_struct(self.tab_header_fmt)\n            assert header.divider == -1\n            assert header.block_id == 3\n\n            # Read off secondary message and product description blocks,\n            # but as far as I can tell, all we really need is the text that follows\n            self._buffer.read_struct(self.header_fmt)\n            self._buffer.read_struct(self.prod_desc_fmt)\n\n        # Get the start of the block with number of pages and divider\n        blk = self._buffer.read_struct(self.tab_block_fmt)\n        assert blk.divider == -1\n\n        # Read the pages line by line, break pages on a -1 character count\n        self.tab_pages = []\n        for _ in range(blk.num_pages):\n            lines = []\n            num_chars = self._buffer.read_int(2, 'big', signed=True)\n            while num_chars != -1:\n                lines.append(''.join(self._buffer.read_ascii(num_chars)))\n                num_chars = self._buffer.read_int(2, 'big', signed=True)\n            self.tab_pages.append('\\n'.join(lines))\n\n        if have_header:\n            assert self._buffer.offset_from(block_start) == header.block_len\n\n    def __repr__(self):\n        \"\"\"Return the string representation of the product.\"\"\"\n        attrs = ('product_name', 'header', 'prod_desc', 'thresholds', 'depVals', 'metadata',\n                 'gsm', 'gsm_additional', 'siteID')\n        blocks = [str(getattr(self, name)) for name in attrs if hasattr(self, name)]\n        return self.filename + ': ' + '\\n'.join(blocks)\n\n    def _unpack_packet_radial_data(self, code, in_sym_block):\n        hdr_fmt = NamedStruct([('ind_first_bin', 'H'), ('nbins', 'H'),\n                               ('i_center', 'h'), ('j_center', 'h'),\n                               ('scale_factor', 'h'), ('num_rad', 'H')],\n                              '>', 'RadialHeader')\n        rad_fmt = NamedStruct([('num_hwords', 'H'), ('start_angle', 'h'),\n                               ('angle_delta', 'h')], '>', 'RadialData')\n        hdr = self._buffer.read_struct(hdr_fmt)\n        rads = []\n        for _ in range(hdr.num_rad):\n            rad = self._buffer.read_struct(rad_fmt)\n            start_az = rad.start_angle * 0.1\n            end_az = start_az + rad.angle_delta * 0.1\n            rads.append((start_az, end_az,\n                         self._unpack_rle_data(\n                             self._buffer.read_binary(2 * rad.num_hwords))))\n        start, end, vals = zip(*rads)\n        return {'start_az': list(start), 'end_az': list(end), 'data': list(vals),\n                'center': (hdr.i_center * self.pos_scale(in_sym_block),\n                           hdr.j_center * self.pos_scale(in_sym_block)),\n                'gate_scale': hdr.scale_factor * 0.001, 'first': hdr.ind_first_bin}\n\n    digital_radial_hdr_fmt = NamedStruct([('ind_first_bin', 'H'), ('nbins', 'H'),\n                                          ('i_center', 'h'), ('j_center', 'h'),\n                                          ('scale_factor', 'h'), ('num_rad', 'H')],\n                                         '>', 'DigitalRadialHeader')\n    digital_radial_fmt = NamedStruct([('num_bytes', 'H'), ('start_angle', 'h'),\n                                      ('angle_delta', 'h')], '>', 'DigitalRadialData')\n\n    def _unpack_packet_digital_radial(self, code, in_sym_block):\n        hdr = self._buffer.read_struct(self.digital_radial_hdr_fmt)\n        rads = []\n        for _ in range(hdr.num_rad):\n            rad = self._buffer.read_struct(self.digital_radial_fmt)\n            start_az = rad.start_angle * 0.1\n            end_az = start_az + rad.angle_delta * 0.1\n            rads.append((start_az, end_az, self._buffer.read_binary(rad.num_bytes)))\n        start, end, vals = zip(*rads)\n        return {'start_az': list(start), 'end_az': list(end), 'data': list(vals),\n                'center': (hdr.i_center * self.pos_scale(in_sym_block),\n                           hdr.j_center * self.pos_scale(in_sym_block)),\n                'gate_scale': hdr.scale_factor * 0.001, 'first': hdr.ind_first_bin}\n\n    def _unpack_packet_raster_data(self, code, in_sym_block):\n        hdr_fmt = NamedStruct([('code', 'L'),\n                               ('i_start', 'h'), ('j_start', 'h'),  # start in km/4\n                               ('xscale_int', 'h'), ('xscale_frac', 'h'),\n                               ('yscale_int', 'h'), ('yscale_frac', 'h'),\n                               ('num_rows', 'h'), ('packing', 'h')], '>', 'RasterData')\n        hdr = self._buffer.read_struct(hdr_fmt)\n        assert hdr.code == 0x800000C0\n        assert hdr.packing == 2\n        rows = []\n        for _ in range(hdr.num_rows):\n            num_bytes = self._buffer.read_int(2, 'big', signed=False)\n            rows.append(self._unpack_rle_data(self._buffer.read_binary(num_bytes)))\n        return {'start_x': hdr.i_start * hdr.xscale_int,\n                'start_y': hdr.j_start * hdr.yscale_int, 'data': rows}\n\n    def _unpack_packet_uniform_text(self, code, in_sym_block):\n        # By not using a struct, we can handle multiple codes\n        num_bytes = self._buffer.read_int(2, 'big', signed=False)\n        if code == 8:\n            value = self._buffer.read_int(2, 'big', signed=False)\n            read_bytes = 6\n        else:\n            value = None\n            read_bytes = 4\n        i_start = self._buffer.read_int(2, 'big', signed=True)\n        j_start = self._buffer.read_int(2, 'big', signed=True)\n\n        # Text is what remains beyond what's been read, not including byte count\n        text = ''.join(self._buffer.read_ascii(num_bytes - read_bytes))\n        return {'x': i_start * self.pos_scale(in_sym_block),\n                'y': j_start * self.pos_scale(in_sym_block), 'color': value, 'text': text}\n\n    def _unpack_packet_special_text_symbol(self, code, in_sym_block):\n        d = self._unpack_packet_uniform_text(code, in_sym_block)\n\n        # Translate special characters to their meaning\n        ret = {}\n        symbol_map = {'!': 'past storm position', '\"': 'current storm position',\n                      '#': 'forecast storm position', '$': 'past MDA position',\n                      '%': 'forecast MDA position', ' ': None}\n\n        # Use this meaning as the key in the returned packet\n        for c in d['text']:\n            if c not in symbol_map:\n                log.warning('%s: Unknown special symbol %d/%x.', self.filename, c, ord(c))\n            else:\n                key = symbol_map[c]\n                if key:\n                    ret[key] = d['x'], d['y']\n        del d['text']\n\n        return ret\n\n    def _unpack_packet_special_graphic_symbol(self, code, in_sym_block):\n        type_map = {3: 'Mesocyclone', 11: '3D Correlated Shear', 12: 'TVS',\n                    26: 'ETVS', 13: 'Positive Hail', 14: 'Probable Hail',\n                    15: 'Storm ID', 19: 'HDA', 25: 'STI Circle'}\n        point_feature_map = {1: 'Mesocyclone (ext.)', 3: 'Mesocyclone',\n                             5: 'TVS (Ext.)', 6: 'ETVS (Ext.)', 7: 'TVS',\n                             8: 'ETVS', 9: 'MDA', 10: 'MDA (Elev.)', 11: 'MDA (Weak)'}\n\n        # Read the number of bytes and set a mark for sanity checking\n        num_bytes = self._buffer.read_int(2, 'big', signed=False)\n        packet_data_start = self._buffer.set_mark()\n\n        scale = self.pos_scale(in_sym_block)\n\n        # Loop over the bytes we have\n        ret = defaultdict(list)\n        while self._buffer.offset_from(packet_data_start) < num_bytes:\n            # Read position\n            ret['x'].append(self._buffer.read_int(2, 'big', signed=True) * scale)\n            ret['y'].append(self._buffer.read_int(2, 'big', signed=True) * scale)\n\n            # Handle any types that have additional info\n            if code in (3, 11, 25):\n                ret['radius'].append(self._buffer.read_int(2, 'big', signed=True) * scale)\n            elif code == 15:\n                ret['id'].append(''.join(self._buffer.read_ascii(2)))\n            elif code == 19:\n                ret['POH'].append(self._buffer.read_int(2, 'big', signed=True))\n                ret['POSH'].append(self._buffer.read_int(2, 'big', signed=True))\n                ret['Max Size'].append(self._buffer.read_int(2, 'big', signed=False))\n            elif code == 20:\n                kind = self._buffer.read_int(2, 'big', signed=False)\n                attr = self._buffer.read_int(2, 'big', signed=False)\n                if kind < 5 or kind > 8:\n                    ret['radius'].append(attr * scale)\n\n                if kind not in point_feature_map:\n                    log.warning('%s: Unknown graphic symbol point kind %d/%x.',\n                                self.filename, kind, kind)\n                    ret['type'].append(f'Unknown ({kind:d})')\n                else:\n                    ret['type'].append(point_feature_map[kind])\n\n        # Map the code to a name for this type of symbol\n        if code != 20:\n            if code not in type_map:\n                log.warning('%s: Unknown graphic symbol type %d/%x.',\n                            self.filename, code, code)\n                ret['type'] = 'Unknown'\n            else:\n                ret['type'] = type_map[code]\n\n        # Check and return\n        assert self._buffer.offset_from(packet_data_start) == num_bytes\n\n        # Reduce dimensions of lists if possible\n        reduce_lists(ret)\n\n        return ret\n\n    def _unpack_packet_scit(self, code, in_sym_block):\n        num_bytes = self._buffer.read_int(2, 'big', signed=False)\n        packet_data_start = self._buffer.set_mark()\n        ret = defaultdict(list)\n        while self._buffer.offset_from(packet_data_start) < num_bytes:\n            next_code = self._buffer.read_int(2, 'big', signed=False)\n            if next_code not in self.packet_map:\n                log.warning('%s: Unknown packet in SCIT %d/%x.',\n                            self.filename, next_code, next_code)\n                self._buffer.jump_to(packet_data_start, num_bytes)\n                return ret\n            else:\n                next_packet = self.packet_map[next_code](self, next_code, in_sym_block)\n                if next_code == 6:\n                    ret['track'].append(next_packet['vectors'])\n                elif next_code == 25:\n                    ret['STI Circle'].append(next_packet)\n                elif next_code == 2:\n                    ret['markers'].append(next_packet)\n                else:\n                    log.warning('%s: Unsupported packet in SCIT %d/%x.',\n                                self.filename, next_code, next_code)\n                    ret['data'].append(next_packet)\n        reduce_lists(ret)\n        return ret\n\n    def _unpack_packet_digital_precipitation(self, code, in_sym_block):\n        # Read off a couple of unused spares\n        self._buffer.read_int(2, 'big', signed=False)\n        self._buffer.read_int(2, 'big', signed=False)\n\n        # Get the size of the grid\n        lfm_boxes = self._buffer.read_int(2, 'big', signed=False)\n        num_rows = self._buffer.read_int(2, 'big', signed=False)\n        rows = []\n\n        # Read off each row and decode the RLE data\n        for _ in range(num_rows):\n            row_num_bytes = self._buffer.read_int(2, 'big', signed=False)\n            row_bytes = self._buffer.read_binary(row_num_bytes)\n            if code == 18:\n                row = self._unpack_rle_data(row_bytes)\n            else:\n                row = []\n                for run, level in zip(row_bytes[::2], row_bytes[1::2]):\n                    row.extend([level] * run)\n            assert len(row) == lfm_boxes\n            rows.append(row)\n\n        return {'data': rows}\n\n    def _unpack_packet_linked_vector(self, code, in_sym_block):\n        num_bytes = self._buffer.read_int(2, 'big', signed=True)\n        if code == 9:\n            value = self._buffer.read_int(2, 'big', signed=True)\n            num_bytes -= 2\n        else:\n            value = None\n        scale = self.pos_scale(in_sym_block)\n        pos = [b * scale for b in self._buffer.read_binary(num_bytes / 2, '>h')]\n        vectors = list(zip(pos[::2], pos[1::2]))\n        return {'vectors': vectors, 'color': value}\n\n    def _unpack_packet_vector(self, code, in_sym_block):\n        num_bytes = self._buffer.read_int(2, 'big', signed=True)\n        if code == 10:\n            value = self._buffer.read_int(2, 'big', signed=True)\n            num_bytes -= 2\n        else:\n            value = None\n        scale = self.pos_scale(in_sym_block)\n        pos = [p * scale for p in self._buffer.read_binary(num_bytes / 2, '>h')]\n        vectors = list(zip(pos[::4], pos[1::4], pos[2::4], pos[3::4]))\n        return {'vectors': vectors, 'color': value}\n\n    def _unpack_packet_contour_color(self, code, in_sym_block):\n        # Check for color value indicator\n        assert self._buffer.read_int(2, 'big', signed=False) == 0x0002\n\n        # Read and return value (level) of contour\n        return {'color': self._buffer.read_int(2, 'big', signed=False)}\n\n    def _unpack_packet_linked_contour(self, code, in_sym_block):\n        # Check for initial point indicator\n        assert self._buffer.read_int(2, 'big', signed=False) == 0x8000\n\n        scale = self.pos_scale(in_sym_block)\n        startx = self._buffer.read_int(2, 'big', signed=True) * scale\n        starty = self._buffer.read_int(2, 'big', signed=True) * scale\n        vectors = [(startx, starty)]\n        num_bytes = self._buffer.read_int(2, 'big', signed=False)\n        pos = [b * scale for b in self._buffer.read_binary(num_bytes / 2, '>h')]\n        vectors.extend(zip(pos[::2], pos[1::2]))\n        return {'vectors': vectors}\n\n    def _unpack_packet_wind_barbs(self, code, in_sym_block):\n        # Figure out how much to read\n        num_bytes = self._buffer.read_int(2, 'big', signed=True)\n        packet_data_start = self._buffer.set_mark()\n        ret = defaultdict(list)\n\n        # Read while we have data, then return\n        while self._buffer.offset_from(packet_data_start) < num_bytes:\n            ret['color'].append(self._buffer.read_int(2, 'big', signed=True))\n            ret['x'].append(self._buffer.read_int(2, 'big', signed=True)\n                            * self.pos_scale(in_sym_block))\n            ret['y'].append(self._buffer.read_int(2, 'big', signed=True)\n                            * self.pos_scale(in_sym_block))\n            ret['direc'].append(self._buffer.read_int(2, 'big', signed=True))\n            ret['speed'].append(self._buffer.read_int(2, 'big', signed=True))\n        return ret\n\n    def _unpack_packet_generic(self, code, in_sym_block):\n        # Reserved HW\n        assert self._buffer.read_int(2, 'big', signed=True) == 0\n\n        # Read number of bytes (2 HW) and return\n        num_bytes = self._buffer.read_int(4, 'big', signed=True)\n        hunk = self._buffer.read(num_bytes)\n        xdrparser = Level3XDRParser(hunk)\n        return xdrparser(code)\n\n    def _unpack_packet_trend_times(self, code, in_sym_block):\n        self._buffer.read_int(2, 'big', signed=True)  # number of bytes, not needed to process\n        return {'times': self._read_trends()}\n\n    def _unpack_packet_cell_trend(self, code, in_sym_block):\n        code_map = ['Cell Top', 'Cell Base', 'Max Reflectivity Height',\n                    'Probability of Hail', 'Probability of Severe Hail',\n                    'Cell-based VIL', 'Maximum Reflectivity',\n                    'Centroid Height']\n        code_scales = [100, 100, 100, 1, 1, 1, 1, 100]\n        num_bytes = self._buffer.read_int(2, 'big', signed=True)\n        packet_data_start = self._buffer.set_mark()\n        cell_id = ''.join(self._buffer.read_ascii(2))\n        x = self._buffer.read_int(2, 'big', signed=True) * self.pos_scale(in_sym_block)\n        y = self._buffer.read_int(2, 'big', signed=True) * self.pos_scale(in_sym_block)\n        ret = {'id': cell_id, 'x': x, 'y': y}\n        while self._buffer.offset_from(packet_data_start) < num_bytes:\n            code = self._buffer.read_int(2, 'big', signed=True)\n            try:\n                ind = code - 1\n                key = code_map[ind]\n                scale = code_scales[ind]\n            except IndexError:\n                log.warning('%s: Unsupported trend code %d/%x.', self.filename, code, code)\n                key = 'Unknown'\n                scale = 1\n            vals = self._read_trends()\n            if code in (1, 2):\n                ret[f'{key} Limited'] = [v > 700 for v in vals]\n                vals = [v - 1000 if v > 700 else v for v in vals]\n            ret[key] = [v * scale for v in vals]\n\n        return ret\n\n    def _read_trends(self):\n        num_vols, latest = self._buffer.read(2)\n        vals = [self._buffer.read_int(2, 'big', signed=True) for _ in range(num_vols)]\n\n        # Wrap the circular buffer so that latest is last\n        return vals[latest:] + vals[:latest]\n\n    packet_map = {1: _unpack_packet_uniform_text,\n                  2: _unpack_packet_special_text_symbol,\n                  3: _unpack_packet_special_graphic_symbol,\n                  4: _unpack_packet_wind_barbs,\n                  6: _unpack_packet_linked_vector,\n                  8: _unpack_packet_uniform_text,\n                  # 9: _unpack_packet_linked_vector,\n                  10: _unpack_packet_vector,\n                  11: _unpack_packet_special_graphic_symbol,\n                  12: _unpack_packet_special_graphic_symbol,\n                  13: _unpack_packet_special_graphic_symbol,\n                  14: _unpack_packet_special_graphic_symbol,\n                  15: _unpack_packet_special_graphic_symbol,\n                  16: _unpack_packet_digital_radial,\n                  17: _unpack_packet_digital_precipitation,\n                  18: _unpack_packet_digital_precipitation,\n                  19: _unpack_packet_special_graphic_symbol,\n                  20: _unpack_packet_special_graphic_symbol,\n                  21: _unpack_packet_cell_trend,\n                  22: _unpack_packet_trend_times,\n                  23: _unpack_packet_scit,\n                  24: _unpack_packet_scit,\n                  25: _unpack_packet_special_graphic_symbol,\n                  26: _unpack_packet_special_graphic_symbol,\n                  28: _unpack_packet_generic,\n                  29: _unpack_packet_generic,\n                  0x0802: _unpack_packet_contour_color,\n                  0x0E03: _unpack_packet_linked_contour,\n                  0xaf1f: _unpack_packet_radial_data,\n                  0xba07: _unpack_packet_raster_data}",
  "class Level3XDRParser(Unpacker):\n    \"\"\"Handle XDR-formatted Level 3 NEXRAD products.\"\"\"\n\n    def __call__(self, code):\n        \"\"\"Perform the actual unpacking.\"\"\"\n        xdr = OrderedDict()\n\n        if code == 28:\n            xdr.update(self._unpack_prod_desc())\n        else:\n            log.warning('XDR: code %d not implemented', code)\n\n        # Check that we got it all\n        self.done()\n        return xdr\n\n    def unpack_string(self):\n        \"\"\"Unpack the internal data as a string.\"\"\"\n        return Unpacker.unpack_string(self).decode('ascii')\n\n    def _unpack_prod_desc(self):\n        xdr = OrderedDict()\n\n        # NOTE: The ICD (262001U) incorrectly lists op-mode, vcp, el_num, and\n        # spare as int*2. Changing to int*4 makes things parse correctly.\n        xdr['name'] = self.unpack_string()\n        xdr['description'] = self.unpack_string()\n        xdr['code'] = self.unpack_int()\n        xdr['type'] = self.unpack_int()\n        xdr['prod_time'] = self.unpack_uint()\n        xdr['radar_name'] = self.unpack_string()\n        xdr['latitude'] = self.unpack_float()\n        xdr['longitude'] = self.unpack_float()\n        xdr['height'] = self.unpack_float()\n        xdr['vol_time'] = self.unpack_uint()\n        xdr['el_time'] = self.unpack_uint()\n        xdr['el_angle'] = self.unpack_float()\n        xdr['vol_num'] = self.unpack_int()\n        xdr['op_mode'] = self.unpack_int()\n        xdr['vcp_num'] = self.unpack_int()\n        xdr['el_num'] = self.unpack_int()\n        xdr['compression'] = self.unpack_int()\n        xdr['uncompressed_size'] = self.unpack_int()\n        xdr['parameters'] = self._unpack_parameters()\n        xdr['components'] = self._unpack_components()\n\n        return xdr\n\n    def _unpack_parameters(self):\n        num = self.unpack_int()\n\n        # ICD documents a \"pointer\" here, that seems to be garbage. Just read\n        # and use the number, starting the list immediately.\n        self.unpack_int()\n\n        if num == 0:\n            return None\n\n        ret = []\n        for i in range(num):\n            ret.append((self.unpack_string(), self.unpack_string()))\n            if i < num - 1:\n                self.unpack_int()  # Another pointer for the 'list' ?\n\n        if num == 1:\n            ret = ret[0]\n\n        return ret\n\n    def _unpack_components(self):\n        num = self.unpack_int()\n\n        # ICD documents a \"pointer\" here, that seems to be garbage. Just read\n        # and use the number, starting the list immediately.\n        self.unpack_int()\n\n        ret = []\n        for i in range(num):\n            try:\n                code = self.unpack_int()\n                ret.append(self._component_lookup[code](self))\n                if i < num - 1:\n                    self.unpack_int()  # Another pointer for the 'list' ?\n            except KeyError:\n                log.warning('Unknown XDR Component: %d', code)\n                break\n\n        if num == 1:\n            ret = ret[0]\n\n        return ret\n\n    radial_fmt = namedtuple('RadialComponent', ['description', 'gate_width',\n                                                'first_gate', 'parameters',\n                                                'radials'])\n    radial_data_fmt = namedtuple('RadialData', ['azimuth', 'elevation', 'width',\n                                                'num_bins', 'attributes',\n                                                'data'])\n\n    def _unpack_radial(self):\n        ret = self.radial_fmt(description=self.unpack_string(),\n                              gate_width=self.unpack_float(),\n                              first_gate=self.unpack_float(),\n                              parameters=self._unpack_parameters(),\n                              radials=None)\n        num_rads = self.unpack_int()\n        rads = []\n        for _ in range(num_rads):\n            # ICD is wrong, says num_bins is float, should be int\n            rads.append(self.radial_data_fmt(azimuth=self.unpack_float(),\n                                             elevation=self.unpack_float(),\n                                             width=self.unpack_float(),\n                                             num_bins=self.unpack_int(),\n                                             attributes=self.unpack_string(),\n                                             data=self.unpack_array(self.unpack_int)))\n        return ret._replace(radials=rads)\n\n    text_fmt = namedtuple('TextComponent', ['parameters', 'text'])\n\n    def _unpack_text(self):\n        return self.text_fmt(parameters=self._unpack_parameters(),\n                             text=self.unpack_string())\n\n    _component_lookup = {1: _unpack_radial, 4: _unpack_text}",
  "def is_precip_mode(vcp_num):\n    r\"\"\"Determine if the NEXRAD radar is operating in precipitation mode.\n\n    Parameters\n    ----------\n    vcp_num : int\n        The NEXRAD volume coverage pattern (VCP) number\n\n    Returns\n    -------\n    bool\n        True if the VCP corresponds to precipitation mode, False otherwise\n\n    \"\"\"\n    return vcp_num // 10 != 3",
  "def inner(val):\n        return val * scale",
  "def __init__(self, filename, *, has_volume_header=True):\n        r\"\"\"Create instance of `Level2File`.\n\n        Parameters\n        ----------\n        filename : str or file-like object\n            If str, the name of the file to be opened. Gzip-ed files are\n            recognized with the extension '.gz', as are bzip2-ed files with\n            the extension `.bz2` If `filename` is a file-like object,\n            this will be read from directly.\n\n        \"\"\"\n        fobj = open_as_needed(filename)\n\n        with contextlib.closing(fobj):\n            self._buffer = IOBuffer.fromfile(fobj)\n\n        # Try to read the volume header. If this fails, or we're told we don't have one\n        # then we fall back and try to just read messages, assuming we have e.g. one of\n        # the real-time chunks.\n        try:\n            if has_volume_header:\n                self._read_volume_header()\n        except (OSError, ValueError):\n            log.warning('Unable to read volume header. Attempting to read messages.')\n            self._buffer.reset()\n\n        # See if we need to apply bz2 decompression\n        start = self._buffer.set_mark()\n        try:\n            self._buffer = IOBuffer(self._buffer.read_func(bzip_blocks_decompress_all))\n        except ValueError:\n            self._buffer.jump_to(start)\n\n        # Now we're all initialized, we can proceed with reading in data\n        self._read_data()",
  "def _read_volume_header(self):\n        self.vol_hdr = self._buffer.read_struct(self.vol_hdr_fmt)\n        self.dt = nexrad_to_datetime(self.vol_hdr.date, self.vol_hdr.time_ms)\n        self.stid = self.vol_hdr.stid",
  "def _read_data(self):\n        self._msg_buf = {}\n        self.sweeps = []\n        self.rda_status = []\n        while not self._buffer.at_end():\n            # Clear old file book marks and set the start of message for\n            # easy jumping to the end\n            self._buffer.clear_marks()\n            msg_start = self._buffer.set_mark()\n\n            # Skip CTM\n            self._buffer.skip(self.CTM_HEADER_SIZE)\n\n            # Read the message header\n            msg_hdr = self._buffer.read_struct(self.msg_hdr_fmt)\n            log.debug('Got message: %s (at offset %d)', str(msg_hdr), self._buffer._offset)\n\n            # The AR2_BLOCKSIZE accounts for the CTM header before the\n            # data, as well as the Frame Check Sequence (4 bytes) after\n            # the end of the data.\n            msg_bytes = self.AR2_BLOCKSIZE\n\n            # If the size is 0, this is just padding, which is for certain\n            # done in the metadata messages. Let the default block size handle rather\n            # than any specific heuristic to skip.\n            if msg_hdr.size_hw:\n                # For new packets, the message size isn't on the fixed size boundaries,\n                # so we use header to figure out. For these, we need to include the\n                # CTM header but not FCS, in addition to the size.\n\n                # As of 2620002P, this is a special value used to indicate that the segment\n                # number/count bytes are used to indicate total size in bytes.\n                if msg_hdr.size_hw == 65535:\n                    msg_bytes = (msg_hdr.num_segments << 16 | msg_hdr.segment_num\n                                 + self.CTM_HEADER_SIZE)\n                elif msg_hdr.msg_type in (29, 31):\n                    msg_bytes = self.CTM_HEADER_SIZE + 2 * msg_hdr.size_hw\n\n                log.debug('Total message size: %d', msg_bytes)\n\n                # Try to handle the message. If we don't handle it, skipping\n                # past it is handled at the end anyway.\n                decoder = f'_decode_msg{msg_hdr.msg_type:d}'\n                if hasattr(self, decoder):\n                    getattr(self, decoder)(msg_hdr)\n                else:\n                    log.warning('Unknown message: %d', msg_hdr.msg_type)\n\n            # Jump to the start of the next message. This depends on whether\n            # the message was legacy with fixed block size or not.\n            self._buffer.jump_to(msg_start, msg_bytes)\n\n        # Check if we have any message segments still in the buffer\n        if self._msg_buf:\n            log.warning('Remaining buffered message segments for message type(s): %s',\n                        ' '.join(f'{typ} ({len(rem)})' for typ, rem in self._msg_buf.items()))\n\n        del self._msg_buf",
  "def _decode_msg1(self, msg_hdr):\n        msg_start = self._buffer.set_mark()\n        hdr = self._buffer.read_struct(self.msg1_fmt)\n        data_dict = {}\n\n        # Process all data pointers:\n        read_info = []\n        if hdr.surv_num_gates and hdr.ref_offset:\n            read_info.append((hdr.ref_offset,\n                              self.msg1_data_hdr('REF', hdr.surv_first_gate,\n                                                 hdr.surv_gate_width,\n                                                 hdr.surv_num_gates, 2.0, 66.0)))\n\n        if hdr.doppler_num_gates and hdr.vel_offset:\n            read_info.append((hdr.vel_offset,\n                              self.msg1_data_hdr('VEL', hdr.doppler_first_gate,\n                                                 hdr.doppler_gate_width,\n                                                 hdr.doppler_num_gates,\n                                                 1. / hdr.dop_res, 129.0)))\n\n        if hdr.doppler_num_gates and hdr.sw_offset:\n            read_info.append((hdr.sw_offset,\n                              self.msg1_data_hdr('SW', hdr.doppler_first_gate,\n                                                 hdr.doppler_gate_width,\n                                                 hdr.doppler_num_gates, 2.0, 129.0)))\n\n        for ptr, data_hdr in read_info:\n            # Jump and read\n            self._buffer.jump_to(msg_start, ptr)\n            vals = self._buffer.read_array(data_hdr.num_gates, 'B')\n\n            # Scale and flag data\n            scaled_vals = (vals - data_hdr.offset) / data_hdr.scale\n            scaled_vals[vals == 0] = self.MISSING\n            scaled_vals[vals == 1] = self.RANGE_FOLD\n\n            # Store\n            data_dict[data_hdr.name] = (data_hdr, scaled_vals)\n\n        self._add_sweep(hdr)\n        self.sweeps[-1].append((hdr, data_dict))",
  "def _decode_msg2(self, msg_hdr):\n        msg_start = self._buffer.set_mark()\n        self.rda_status.append(self._buffer.read_struct(self.msg2_fmt))\n\n        remaining = (msg_hdr.size_hw * 2 - self.msg_hdr_fmt.size\n                     - self._buffer.offset_from(msg_start))\n\n        # RDA Build 18.0 expanded the size\n        if remaining >= self.msg2_additional_fmt.size:\n            self.rda_status.append(self._buffer.read_struct(self.msg2_additional_fmt))\n            remaining -= self.msg2_additional_fmt.size\n\n        if remaining:\n            log.info('Padding detected in message 2. Length encoded as %d but offset when '\n                     'done is %d', 2 * msg_hdr.size_hw, self._buffer.offset_from(msg_start))",
  "def _decode_msg3(self, msg_hdr):\n        from ._nexrad_msgs.msg3 import descriptions, fields\n        self.maintenance_data_desc = descriptions\n        msg_fmt = DictStruct(fields, '>')\n\n        # The only version we decode isn't very flexible, so just skip if we don't have the\n        # right length, which happens with older data.\n        if msg_hdr.size_hw * 2 - self.msg_hdr_fmt.size != msg_fmt.size:\n            log.info('Length of message 3 is %d instead of expected %d; this is likely the '\n                     'legacy format. Skipping...', 2 * msg_hdr.size_hw)\n            return\n\n        self.maintenance_data = self._buffer.read_struct(msg_fmt)",
  "def _decode_msg5(self, msg_hdr):\n        vcp_info = self._buffer.read_struct(self.vcp_fmt)\n        # Just skip the vcp info if it says size is 0:\n        if vcp_info.size_hw:\n            els = [self._buffer.read_struct(self.vcp_el_fmt)\n                   for _ in range(vcp_info.num_el_cuts)]\n            self.vcp_info = vcp_info._replace(els=els)\n            self._check_size(msg_hdr,\n                             self.vcp_fmt.size + vcp_info.num_el_cuts * self.vcp_el_fmt.size)",
  "def _decode_msg13(self, msg_hdr):\n        data = self._buffer_segment(msg_hdr)\n        if data:\n            data = struct.Struct(f'>{len(data) // 2:d}h').unpack(data)\n            # Legacy format doesn't have date/time and has fewer azimuths\n            if data[0] <= 5:\n                num_el = data[0]\n                dt = None\n                num_az = 256\n                offset = 1\n            else:\n                date, time, num_el = data[:3]\n                # time is in \"minutes since midnight\", need to pass as ms since midnight\n                dt = nexrad_to_datetime(date, 60 * 1000 * time)\n                num_az = 360\n                offset = 3\n\n            self.clutter_filter_bypass_map = {'datetime': dt, 'data': []}\n            chunk_size = 32\n            bit_conv = Bits(16)\n            for e in range(num_el):\n                seg_num = data[offset]\n                if seg_num != (e + 1):\n                    log.warning('Message 13 segments out of sync -- read %d but on %d',\n                                seg_num, e + 1)\n\n                az_data = []\n                for _ in range(num_az):\n                    gates = []\n                    for i in range(1, chunk_size + 1):\n                        gates.extend(bit_conv(data[offset + i]))\n                    az_data.append(gates)\n                self.clutter_filter_bypass_map['data'].append(az_data)\n                offset += num_az * chunk_size + 1\n\n            if offset != len(data):\n                log.warning('Message 13 left data -- Used: %d Avail: %d', offset, len(data))",
  "def _decode_msg15(self, msg_hdr):\n        # buffer the segments until we have the whole thing. The data\n        # will be returned concatenated when this is the case\n        data = self._buffer_segment(msg_hdr)\n        if data:\n            date, time, num_el, *data = struct.Struct(f'>{len(data) // 2:d}h').unpack(data)\n\n            if not 0 < num_el <= 5:\n                log.info('Message 15 num_el is outside (0, 5]--likely legacy clutter filter '\n                         'notch width. Skipping...')\n                return\n\n            # time is in \"minutes since midnight\", need to pass as ms since midnight\n            self.clutter_filter_map = {'datetime': nexrad_to_datetime(date, 60 * 1000 * time),\n                                       'data': []}\n\n            offset = 0\n            for _ in range(num_el):\n                az_data = []\n                for _ in range(360):\n                    num_rng = data[offset]\n                    codes = data[offset + 1:offset + 1 + 2 * num_rng:2]\n                    ends = data[offset + 2:offset + 2 + 2 * num_rng:2]\n                    az_data.append(list(zip(ends, codes)))\n                    offset += 2 * num_rng + 1\n                self.clutter_filter_map['data'].append(az_data)\n\n            if offset != len(data):\n                log.warning('Message 15 left data -- Used: %d Avail: %d', offset, len(data))",
  "def _decode_msg18(self, msg_hdr):\n        # buffer the segments until we have the whole thing. The data\n        # will be returned concatenated when this is the case\n        data = self._buffer_segment(msg_hdr)\n\n        # Legacy versions don't even document this:\n        if data and self.vol_hdr.version[:8] not in (b'ARCHIVE2', b'AR2V0001'):\n            from ._nexrad_msgs.msg18 import descriptions, fields\n            self.rda_adaptation_desc = descriptions\n\n            # Can't use NamedStruct because we have more than 255 items--this\n            # is a CPython limit for arguments.\n            msg_fmt = DictStruct(fields, '>')\n\n            # Be extra paranoid about passing too much data in case of legacy files\n            self.rda = msg_fmt.unpack(data[:msg_fmt.size])\n            for num in (11, 21, 31, 32, 300, 301):\n                attr = f'VCPAT{num}'\n                dat = self.rda[attr]\n                vcp_hdr = self.vcp_fmt.unpack_from(dat, 0)\n                off = self.vcp_fmt.size\n                els = []\n                for _ in range(vcp_hdr.num_el_cuts):\n                    els.append(self.vcp_el_fmt.unpack_from(dat, off))\n                    off += self.vcp_el_fmt.size\n                self.rda[attr] = vcp_hdr._replace(els=els)",
  "def _decode_msg31(self, msg_hdr):\n        msg_start = self._buffer.set_mark()\n        data_hdr = self._buffer.read_struct(self.msg31_data_hdr_fmt)\n        if data_hdr.compression:\n            log.warning('Compressed message 31 not supported!')\n\n        # Read all the block pointers. While the ICD specifies that at least the vol, el, rad\n        # constant blocks as well as REF moment block are present, it says \"the pointers are\n        # not order or location dependent.\"\n        radial = self.Radial(data_hdr, None, None, None, {})\n        block_count = 0\n        for ptr in self._buffer.read_binary(data_hdr.num_data_blks, '>L'):\n            if ptr:\n                block_count += 1\n                self._buffer.jump_to(msg_start, ptr)\n                info = self._buffer.get_next(6)\n                if info.startswith(b'RVOL'):\n                    radial = radial._replace(\n                        vol_consts=self._buffer.read_struct(self.msg31_vol_const_fmt))\n                elif info.startswith(b'RELV'):\n                    radial = radial._replace(\n                        elev_consts=self._buffer.read_struct(self.msg31_el_const_fmt))\n                elif info.startswith(b'RRAD'):\n                    # Relies on the fact that the struct is small enough for its size\n                    # to fit in a single byte\n                    if int(info[-1]) == self.rad_const_fmt_v2.size:\n                        rad_consts = self._buffer.read_struct(self.rad_const_fmt_v2)\n                    else:\n                        rad_consts = self._buffer.read_struct(self.rad_const_fmt_v1)\n                    radial = radial._replace(radial_consts=rad_consts)\n                elif info.startswith(b'D'):\n                    hdr = self._buffer.read_struct(self.data_block_fmt)\n                    # TODO: The correctness of this code is not tested\n                    vals = self._buffer.read_array(count=hdr.num_gates,\n                                                   dtype=f'>u{hdr.data_size // 8}')\n                    scaled_vals = (vals - hdr.offset) / hdr.scale\n                    scaled_vals[vals == 0] = self.MISSING\n                    scaled_vals[vals == 1] = self.RANGE_FOLD\n                    radial.moments[hdr.name.strip()] = (hdr, scaled_vals)\n                else:\n                    log.warning('Unknown Message 31 block type: %s', str(info[:4]))\n\n        self._add_sweep(data_hdr)\n        self.sweeps[-1].append(radial)\n\n        if data_hdr.num_data_blks != block_count:\n            log.warning('Incorrect number of blocks detected -- Got %d'\n                        ' instead of %d', block_count, data_hdr.num_data_blks)\n\n        if data_hdr.rad_length != self._buffer.offset_from(msg_start):\n            log.info('Padding detected in message. Length encoded as %d but offset when '\n                     'done is %d', data_hdr.rad_length, self._buffer.offset_from(msg_start))",
  "def _buffer_segment(self, msg_hdr):\n        # Add to the buffer\n        bufs = self._msg_buf.setdefault(msg_hdr.msg_type, {})\n        bufs[msg_hdr.segment_num] = self._buffer.read(2 * msg_hdr.size_hw\n                                                      - self.msg_hdr_fmt.size)\n\n        # Warn for badly formatted data\n        if len(bufs) != msg_hdr.segment_num:\n            log.warning('Segment out of order (Got: %d Count: %d) for message type %d.',\n                        msg_hdr.segment_num, len(bufs), msg_hdr.msg_type)\n\n        # If we're complete, return the full collection of data\n        if msg_hdr.num_segments == len(bufs):\n            self._msg_buf.pop(msg_hdr.msg_type)\n            return b''.join(bytes(item[1]) for item in sorted(bufs.items()))\n        else:\n            return None",
  "def _add_sweep(self, hdr):\n        if not self.sweeps and not hdr.rad_status & START_VOLUME:\n            log.warning('Missed start of volume!')\n\n        if hdr.rad_status & START_ELEVATION:\n            self.sweeps.append([])\n\n        if len(self.sweeps) != hdr.el_num:\n            log.warning('Missed elevation -- Have %d but data on %d.'\n                        ' Compensating...', len(self.sweeps), hdr.el_num)\n            while len(self.sweeps) < hdr.el_num:\n                self.sweeps.append([])",
  "def _check_size(self, msg_hdr, size):\n        hdr_size = msg_hdr.size_hw * 2 - self.msg_hdr_fmt.size\n        if size != hdr_size:\n            log.warning('Message type %d should be %d bytes but got %d',\n                        msg_hdr.msg_type, size, hdr_size)",
  "def inner(seq):\n        return nexrad_to_datetime(seq[ind_days], seq[ind_minutes] * 60 * 1000)",
  "def inner(seq):\n        return seq[index] * scale",
  "def inner(seq):\n        shift = 2**16\n        if seq[ind1] < 0:\n            seq[ind1] += shift\n        if seq[ind2] < 0:\n            seq[ind2] += shift\n        return (seq[ind1] << 16) | seq[ind2]",
  "def inner(seq):\n        return seq[ind] >> 8",
  "def inner(seq):\n        return seq[ind] & 0x00FF",
  "def inner(seq):\n        return seq[ind] >> 5",
  "def inner(seq):\n        # ICD says 1->SAILS, 2->MRLE, but testing on 2020-08-17 makes this seem inverted\n        # given what's being reported by sites in the GSM.\n        return {0: 'Non-supplemental scan',\n                2: 'SAILS scan', 1: 'MRLE scan'}.get(seq[ind] & 0x001F, 'Unknown')",
  "def __init__(self, num=256):\n        self.lut = np.full(num, self.MISSING, dtype=float)",
  "def __call__(self, data):\n        \"\"\"Convert the values.\"\"\"\n        return self.lut[data]",
  "def __init__(self, prod):\n        \"\"\"Initialize the mapper and the lookup table.\"\"\"\n        super().__init__()\n        min_val = two_comp16(prod.thresholds[0]) * self._min_scale\n        inc = prod.thresholds[1] * self._inc_scale\n        num_levels = prod.thresholds[2]\n\n        # Generate lookup table -- sanity check on num_levels handles\n        # the fact that DHR advertises 256 levels, which *includes*\n        # missing, differing from other products\n        num_levels = min(num_levels, self._max_data - self._min_data + 1)\n        for i in range(num_levels):\n            self.lut[i + self._min_data] = min_val + i * inc",
  "def __init__(self, prod):\n        \"\"\"Initialize the VIL mapper.\"\"\"\n        super().__init__()\n        lin_scale = float16(prod.thresholds[0])\n        lin_offset = float16(prod.thresholds[1])\n        log_start = prod.thresholds[2]\n        log_scale = float16(prod.thresholds[3])\n        log_offset = float16(prod.thresholds[4])\n\n        # VIL is allowed to use 2 through 254 inclusive. 0 is thresholded,\n        # 1 is flagged, and 255 is reserved\n        ind = np.arange(255)\n        self.lut[2:log_start] = (ind[2:log_start] - lin_offset) / lin_scale\n        self.lut[log_start:-1] = np.exp((ind[log_start:] - log_offset) / log_scale)",
  "def __init__(self, prod):\n        \"\"\"Initialize the mapper.\"\"\"\n        super().__init__()\n        data_mask = prod.thresholds[0]\n        scale = prod.thresholds[1]\n        offset = prod.thresholds[2]\n        topped_mask = prod.thresholds[3]\n        self.topped_lut = [False] * 256\n        for i in range(2, 256):\n            self.lut[i] = ((i & data_mask) - offset) / scale\n            self.topped_lut[i] = bool(i & topped_mask)\n\n        self.topped_lut = np.array(self.topped_lut)",
  "def __call__(self, data_vals):\n        \"\"\"Convert the data values.\"\"\"\n        return self.lut[data_vals], self.topped_lut[data_vals]",
  "def __init__(self, prod):\n        \"\"\"Initialize the mapper by pulling out all the information from the product.\"\"\"\n        # Need to treat this value as unsigned, so we can use the full 16-bit range. This\n        # is necessary at least for the DPR product, otherwise it has a value of -1.\n        max_data_val = prod.thresholds[5] & 0xFFFF\n\n        # Values will be [0, max] inclusive, so need to add 1 to max value to get proper size.\n        super().__init__(max_data_val + 1)\n\n        scale = float32(prod.thresholds[0], prod.thresholds[1])\n        offset = float32(prod.thresholds[2], prod.thresholds[3])\n        leading_flags = prod.thresholds[6]\n        trailing_flags = prod.thresholds[7]\n\n        if leading_flags > 1:\n            self.lut[1] = self.RANGE_FOLD\n\n        # Need to add 1 to the end of the range so that it's inclusive\n        for i in range(leading_flags, max_data_val - trailing_flags + 1):\n            self.lut[i] = (i - offset) / scale",
  "def __init__(self, prod):\n        \"\"\"Initialize the mapper.\"\"\"\n        super().__init__()\n        for i in range(10, 256):\n            self.lut[i] = i // 10\n        self.lut[150] = self.RANGE_FOLD",
  "def __init__(self, prod):\n        \"\"\"Initialize the mapper based on the product.\"\"\"\n        data_levels = prod.thresholds[2]\n        super().__init__(data_levels)\n        scale = prod.thresholds[0] / 1000.\n        offset = prod.thresholds[1] / 1000.\n        leading_flags = prod.thresholds[3]\n        for i in range(leading_flags, data_levels):\n            self.lut[i] = scale * i + offset",
  "def __init__(self, prod):\n        \"\"\"Initialize the values and labels from the product.\"\"\"\n        # Don't worry about super() since we're using our own lut assembled sequentially\n        self.labels = []\n        self.lut = []\n        for t in prod.thresholds:\n            codes, val = t >> 8, t & 0xFF\n            label = ''\n            if codes >> 7:\n                label = self.lut_names[val]\n                if label in ('Blank', 'TH', 'ND'):\n                    val = self.MISSING\n                elif label == 'RF':\n                    val = self.RANGE_FOLD\n\n            elif codes >> 6:\n                val *= 0.01\n                label = f'{val:.2f}'\n            elif codes >> 5:\n                val *= 0.05\n                label = f'{val:.2f}'\n            elif codes >> 4:\n                val *= 0.1\n                label = f'{val:.1f}'\n\n            if codes & 0x1:\n                val *= -1\n                label = '-' + label\n            elif (codes >> 1) & 0x1:\n                label = '+' + label\n\n            if (codes >> 2) & 0x1:\n                label = '<' + label\n            elif (codes >> 3) & 0x1:\n                label = '>' + label\n\n            if not label:\n                label = str(val)\n\n            self.lut.append(val)\n            self.labels.append(label)\n        self.lut = np.array(self.lut)",
  "def __init__(self, filename):\n        r\"\"\"Create instance of `Level3File`.\n\n        Parameters\n        ----------\n        filename : str or file-like object\n            If str, the name of the file to be opened. If file-like object,\n            this will be read from directly.\n\n        \"\"\"\n        fobj = open_as_needed(filename)\n        if isinstance(filename, str):\n            self.filename = filename\n        elif isinstance(filename, pathlib.Path):\n            self.filename = str(filename)\n        else:\n            self.filename = 'No File'\n\n        # Just read in the entire set of data at once\n        with contextlib.closing(fobj):\n            self._buffer = IOBuffer.fromfile(fobj)\n\n        # Pop off the WMO header if we find it\n        self._process_wmo_header()\n\n        # Pop off last 4 bytes if necessary\n        self._process_end_bytes()\n\n        # Set up places to store data and metadata\n        self.metadata = {}\n\n        # Handle free text message products that are pure text\n        if self.wmo_code == 'NOUS':\n            self.header = None\n            self.prod_desc = None\n            self.thresholds = None\n            self.depVals = None\n            self.product_name = 'Free Text Message'\n            self.text = ''.join(self._buffer.read_ascii())\n            return\n\n        # Decompress the data if necessary, and if so, pop off new header\n        self._buffer = IOBuffer(self._buffer.read_func(zlib_decompress_all_frames))\n        self._process_wmo_header()\n\n        # Check for empty product\n        if len(self._buffer) == 0:\n            log.warning('%s: Empty product!', self.filename)\n            return\n\n        # Unpack the message header and the product description block\n        msg_start = self._buffer.set_mark()\n        self.header = self._buffer.read_struct(self.header_fmt)\n        log.debug('Buffer size: %d (%d expected) Header: %s', len(self._buffer),\n                  self.header.msg_len, self.header)\n\n        if not self._buffer.check_remains(self.header.msg_len - self.header_fmt.size):\n            log.warning('Product contains an unexpected amount of data remaining--have: %d '\n                        'expected: %d. This product may not parse correctly.',\n                        len(self._buffer) - self._buffer._offset,\n                        self.header.msg_len - self.header_fmt.size)\n\n        # Handle GSM and jump out\n        if self.header.code == 2:\n            self.gsm = self._buffer.read_struct(self.gsm_fmt)\n            self.product_name = 'General Status Message'\n            assert self.gsm.divider == -1\n            if self.gsm.block_len > 82:\n                # Due to the way the structures read it in, one bit from the count needs\n                # to be popped off and added as the supplemental cut status for the 25th\n                # elevation cut.\n                more = self._buffer.read_struct(self.additional_gsm_fmt)\n                cut_count = more.supplemental_cut_count\n                more.supplemental_cut_map2.append(bool(cut_count & 0x1))\n                self.gsm_additional = more._replace(supplemental_cut_count=cut_count >> 1)\n                assert self.gsm.block_len == 178\n            else:\n                assert self.gsm.block_len == 82\n            return\n\n        self.prod_desc = self._buffer.read_struct(self.prod_desc_fmt)\n        log.debug('Product description block: %s', self.prod_desc)\n\n        # Convert thresholds and dependent values to lists of values\n        self.thresholds = [getattr(self.prod_desc, f'thr{i}') for i in range(1, 17)]\n        self.depVals = [getattr(self.prod_desc, f'dep{i}') for i in range(1, 11)]\n\n        # Set up some time/location metadata\n        self.metadata['msg_time'] = nexrad_to_datetime(self.header.date,\n                                                       self.header.time * 1000)\n        self.metadata['vol_time'] = nexrad_to_datetime(self.prod_desc.vol_date,\n                                                       self.prod_desc.vol_start_time * 1000)\n        self.metadata['prod_time'] = nexrad_to_datetime(self.prod_desc.prod_gen_date,\n                                                        self.prod_desc.prod_gen_time * 1000)\n        self.lat = self.prod_desc.lat * 0.001\n        self.lon = self.prod_desc.lon * 0.001\n        self.height = self.prod_desc.height\n\n        # Handle product-specific blocks. Default to compression and elevation angle\n        # Also get other product specific information, like name,\n        # maximum range, and how to map data bytes to values\n        default = ('Unknown Product', 230., LegacyMapper,\n                   (('el_angle', scaled_elem(2, 0.1)), ('compression', 7),\n                    ('uncompressed_size', combine_elem(8, 9)), ('defaultVals', 0)))\n        self.product_name, self.max_range, mapper, meta = self.prod_spec_map.get(\n            self.header.code, default)\n        log.debug('Product info--name: %s max_range: %f mapper: %s metadata: %s',\n                  self.product_name, self.max_range, mapper, meta)\n\n        for name, block in meta:\n            if callable(block):\n                self.metadata[name] = block(self.depVals)\n            else:\n                self.metadata[name] = self.depVals[block]\n\n        # Now that we have the header, we have everything needed to make tables\n        # Store as class that can be called\n        self.map_data = mapper(self)\n\n        # Process compression if indicated. We need to fail\n        # gracefully here since we default to it being on\n        if self.metadata.get('compression', False):\n            try:\n                comp_start = self._buffer.set_mark()\n                decomp_data = self._buffer.read_func(bz2.decompress)\n                self._buffer.splice(comp_start, decomp_data)\n                assert self._buffer.check_remains(self.metadata['uncompressed_size'])\n            except OSError:\n                # Compression didn't work, so we just assume it wasn't actually compressed.\n                pass\n\n        # Unpack the various blocks, if present. The factor of 2 converts from\n        # 'half-words' to bytes\n        # Check to see if this is one of the \"special\" products that uses\n        # header-free blocks and re-assigns the offsets\n        if self.header.code in self.standalone_tabular:\n            if self.prod_desc.sym_off:\n                # For standalone tabular alphanumeric, symbology offset is\n                # actually tabular\n                self._unpack_tabblock(msg_start, 2 * self.prod_desc.sym_off, False)\n            if self.prod_desc.graph_off:\n                # Offset seems to be off by 1 from where we're counting, but\n                # it's not clear why.\n                self._unpack_standalone_graphblock(msg_start,\n                                                   2 * (self.prod_desc.graph_off - 1))\n        # Need special handling for (old) radar coded message format\n        elif self.header.code == 74:\n            self._unpack_rcm(msg_start, 2 * self.prod_desc.sym_off)\n        else:\n            if self.prod_desc.sym_off:\n                self._unpack_symblock(msg_start, 2 * self.prod_desc.sym_off)\n            if self.prod_desc.graph_off:\n                self._unpack_graphblock(msg_start, 2 * self.prod_desc.graph_off)\n            if self.prod_desc.tab_off:\n                self._unpack_tabblock(msg_start, 2 * self.prod_desc.tab_off)\n\n        if 'defaultVals' in self.metadata:\n            log.warning('%s: Using default metadata for product %d',\n                        self.filename, self.header.code)",
  "def _process_wmo_header(self):\n        # Read off the WMO header if necessary\n        data = self._buffer.get_next(64).decode('ascii', 'ignore')\n        match = self.wmo_finder.search(data)\n        log.debug('WMO Header: %s', match)\n        if match:\n            self.wmo_code = match.groups()[0]\n            self.siteID = match.groups()[-1]\n            self._buffer.skip(match.end())\n        else:\n            self.wmo_code = ''",
  "def _process_end_bytes(self):\n        check_bytes = self._buffer[-4:-1]\n        log.debug('End Bytes: %s', check_bytes)\n        if check_bytes in (b'\\r\\r\\n', b'\\xff\\xff\\n'):\n            self._buffer.truncate(4)",
  "def _unpack_rle_data(data):\n        # Unpack Run-length encoded data\n        unpacked = []\n        for run in data:\n            num, val = run >> 4, run & 0x0F\n            unpacked.extend([val] * num)\n        return unpacked",
  "def pos_scale(is_sym_block):\n        \"\"\"Scale of the position information in km.\"\"\"\n        return 0.25 if is_sym_block else 1",
  "def _unpack_rcm(self, start, offset):\n        self._buffer.jump_to(start, offset)\n        header = self._buffer.read_ascii(10)\n        assert header == '1234 ROBUU'\n        text_data = self._buffer.read_ascii()\n        end = 0\n        # Appendix B of ICD tells how to interpret this stuff, but that just\n        # doesn't seem worth it.\n        for marker, name in [('AA', 'ref'), ('BB', 'vad'), ('CC', 'remarks')]:\n            start = text_data.find('/NEXR' + marker, end)\n            # For part C the search for end fails, but returns -1, which works\n            end = text_data.find('/END' + marker, start)\n            setattr(self, 'rcm_' + name, text_data[start:end])",
  "def _unpack_symblock(self, start, offset):\n        self._buffer.jump_to(start, offset)\n        blk = self._buffer.read_struct(self.sym_block_fmt)\n        log.debug('Symbology block info: %s', blk)\n\n        self.sym_block = []\n        assert blk.divider == -1, ('Bad divider for symbology block: {:d} should be -1'\n                                   .format(blk.divider))\n        assert blk.block_id == 1, ('Bad block ID for symbology block: {:d} should be 1'\n                                   .format(blk.block_id))\n        for _ in range(blk.nlayer):\n            layer_hdr = self._buffer.read_struct(self.sym_layer_fmt)\n            assert layer_hdr.divider == -1\n            layer = []\n            self.sym_block.append(layer)\n            layer_start = self._buffer.set_mark()\n            while self._buffer.offset_from(layer_start) < layer_hdr.length:\n                packet_code = self._buffer.read_int(2, 'big', signed=False)\n                log.debug('Symbology packet: %d', packet_code)\n                if packet_code in self.packet_map:\n                    layer.append(self.packet_map[packet_code](self, packet_code, True))\n                else:\n                    log.warning('%s: Unknown symbology packet type %d/%x.',\n                                self.filename, packet_code, packet_code)\n                    self._buffer.jump_to(layer_start, layer_hdr.length)\n            assert self._buffer.offset_from(layer_start) == layer_hdr.length",
  "def _unpack_graphblock(self, start, offset):\n        self._buffer.jump_to(start, offset)\n        hdr = self._buffer.read_struct(self.graph_block_fmt)\n        assert hdr.divider == -1, ('Bad divider for graphical block: {:d} should be -1'\n                                   .format(hdr.divider))\n        assert hdr.block_id == 2, ('Bad block ID for graphical block: {:d} should be 1'\n                                   .format(hdr.block_id))\n        self.graph_pages = []\n        for page in range(hdr.num_pages):\n            page_num = self._buffer.read_int(2, 'big', signed=False)\n            assert page + 1 == page_num\n            page_size = self._buffer.read_int(2, 'big', signed=False)\n            page_start = self._buffer.set_mark()\n            packets = []\n            while self._buffer.offset_from(page_start) < page_size:\n                packet_code = self._buffer.read_int(2, 'big', signed=False)\n                if packet_code in self.packet_map:\n                    packets.append(self.packet_map[packet_code](self, packet_code, False))\n                else:\n                    log.warning('%s: Unknown graphical packet type %d/%x.',\n                                self.filename, packet_code, packet_code)\n                    self._buffer.skip(page_size)\n            self.graph_pages.append(packets)",
  "def _unpack_standalone_graphblock(self, start, offset):\n        self._buffer.jump_to(start, offset)\n        packets = []\n        while not self._buffer.at_end():\n            packet_code = self._buffer.read_int(2, 'big', signed=False)\n            if packet_code in self.packet_map:\n                packets.append(self.packet_map[packet_code](self, packet_code, False))\n            else:\n                log.warning('%s: Unknown standalone graphical packet type %d/%x.',\n                            self.filename, packet_code, packet_code)\n                # Assume next 2 bytes is packet length and try skipping\n                num_bytes = self._buffer.read_int(2, 'big', signed=False)\n                self._buffer.skip(num_bytes)\n        self.graph_pages = [packets]",
  "def _unpack_tabblock(self, start, offset, have_header=True):\n        self._buffer.jump_to(start, offset)\n        block_start = self._buffer.set_mark()\n\n        # Read the header and validate if needed\n        if have_header:\n            header = self._buffer.read_struct(self.tab_header_fmt)\n            assert header.divider == -1\n            assert header.block_id == 3\n\n            # Read off secondary message and product description blocks,\n            # but as far as I can tell, all we really need is the text that follows\n            self._buffer.read_struct(self.header_fmt)\n            self._buffer.read_struct(self.prod_desc_fmt)\n\n        # Get the start of the block with number of pages and divider\n        blk = self._buffer.read_struct(self.tab_block_fmt)\n        assert blk.divider == -1\n\n        # Read the pages line by line, break pages on a -1 character count\n        self.tab_pages = []\n        for _ in range(blk.num_pages):\n            lines = []\n            num_chars = self._buffer.read_int(2, 'big', signed=True)\n            while num_chars != -1:\n                lines.append(''.join(self._buffer.read_ascii(num_chars)))\n                num_chars = self._buffer.read_int(2, 'big', signed=True)\n            self.tab_pages.append('\\n'.join(lines))\n\n        if have_header:\n            assert self._buffer.offset_from(block_start) == header.block_len",
  "def __repr__(self):\n        \"\"\"Return the string representation of the product.\"\"\"\n        attrs = ('product_name', 'header', 'prod_desc', 'thresholds', 'depVals', 'metadata',\n                 'gsm', 'gsm_additional', 'siteID')\n        blocks = [str(getattr(self, name)) for name in attrs if hasattr(self, name)]\n        return self.filename + ': ' + '\\n'.join(blocks)",
  "def _unpack_packet_radial_data(self, code, in_sym_block):\n        hdr_fmt = NamedStruct([('ind_first_bin', 'H'), ('nbins', 'H'),\n                               ('i_center', 'h'), ('j_center', 'h'),\n                               ('scale_factor', 'h'), ('num_rad', 'H')],\n                              '>', 'RadialHeader')\n        rad_fmt = NamedStruct([('num_hwords', 'H'), ('start_angle', 'h'),\n                               ('angle_delta', 'h')], '>', 'RadialData')\n        hdr = self._buffer.read_struct(hdr_fmt)\n        rads = []\n        for _ in range(hdr.num_rad):\n            rad = self._buffer.read_struct(rad_fmt)\n            start_az = rad.start_angle * 0.1\n            end_az = start_az + rad.angle_delta * 0.1\n            rads.append((start_az, end_az,\n                         self._unpack_rle_data(\n                             self._buffer.read_binary(2 * rad.num_hwords))))\n        start, end, vals = zip(*rads)\n        return {'start_az': list(start), 'end_az': list(end), 'data': list(vals),\n                'center': (hdr.i_center * self.pos_scale(in_sym_block),\n                           hdr.j_center * self.pos_scale(in_sym_block)),\n                'gate_scale': hdr.scale_factor * 0.001, 'first': hdr.ind_first_bin}",
  "def _unpack_packet_digital_radial(self, code, in_sym_block):\n        hdr = self._buffer.read_struct(self.digital_radial_hdr_fmt)\n        rads = []\n        for _ in range(hdr.num_rad):\n            rad = self._buffer.read_struct(self.digital_radial_fmt)\n            start_az = rad.start_angle * 0.1\n            end_az = start_az + rad.angle_delta * 0.1\n            rads.append((start_az, end_az, self._buffer.read_binary(rad.num_bytes)))\n        start, end, vals = zip(*rads)\n        return {'start_az': list(start), 'end_az': list(end), 'data': list(vals),\n                'center': (hdr.i_center * self.pos_scale(in_sym_block),\n                           hdr.j_center * self.pos_scale(in_sym_block)),\n                'gate_scale': hdr.scale_factor * 0.001, 'first': hdr.ind_first_bin}",
  "def _unpack_packet_raster_data(self, code, in_sym_block):\n        hdr_fmt = NamedStruct([('code', 'L'),\n                               ('i_start', 'h'), ('j_start', 'h'),  # start in km/4\n                               ('xscale_int', 'h'), ('xscale_frac', 'h'),\n                               ('yscale_int', 'h'), ('yscale_frac', 'h'),\n                               ('num_rows', 'h'), ('packing', 'h')], '>', 'RasterData')\n        hdr = self._buffer.read_struct(hdr_fmt)\n        assert hdr.code == 0x800000C0\n        assert hdr.packing == 2\n        rows = []\n        for _ in range(hdr.num_rows):\n            num_bytes = self._buffer.read_int(2, 'big', signed=False)\n            rows.append(self._unpack_rle_data(self._buffer.read_binary(num_bytes)))\n        return {'start_x': hdr.i_start * hdr.xscale_int,\n                'start_y': hdr.j_start * hdr.yscale_int, 'data': rows}",
  "def _unpack_packet_uniform_text(self, code, in_sym_block):\n        # By not using a struct, we can handle multiple codes\n        num_bytes = self._buffer.read_int(2, 'big', signed=False)\n        if code == 8:\n            value = self._buffer.read_int(2, 'big', signed=False)\n            read_bytes = 6\n        else:\n            value = None\n            read_bytes = 4\n        i_start = self._buffer.read_int(2, 'big', signed=True)\n        j_start = self._buffer.read_int(2, 'big', signed=True)\n\n        # Text is what remains beyond what's been read, not including byte count\n        text = ''.join(self._buffer.read_ascii(num_bytes - read_bytes))\n        return {'x': i_start * self.pos_scale(in_sym_block),\n                'y': j_start * self.pos_scale(in_sym_block), 'color': value, 'text': text}",
  "def _unpack_packet_special_text_symbol(self, code, in_sym_block):\n        d = self._unpack_packet_uniform_text(code, in_sym_block)\n\n        # Translate special characters to their meaning\n        ret = {}\n        symbol_map = {'!': 'past storm position', '\"': 'current storm position',\n                      '#': 'forecast storm position', '$': 'past MDA position',\n                      '%': 'forecast MDA position', ' ': None}\n\n        # Use this meaning as the key in the returned packet\n        for c in d['text']:\n            if c not in symbol_map:\n                log.warning('%s: Unknown special symbol %d/%x.', self.filename, c, ord(c))\n            else:\n                key = symbol_map[c]\n                if key:\n                    ret[key] = d['x'], d['y']\n        del d['text']\n\n        return ret",
  "def _unpack_packet_special_graphic_symbol(self, code, in_sym_block):\n        type_map = {3: 'Mesocyclone', 11: '3D Correlated Shear', 12: 'TVS',\n                    26: 'ETVS', 13: 'Positive Hail', 14: 'Probable Hail',\n                    15: 'Storm ID', 19: 'HDA', 25: 'STI Circle'}\n        point_feature_map = {1: 'Mesocyclone (ext.)', 3: 'Mesocyclone',\n                             5: 'TVS (Ext.)', 6: 'ETVS (Ext.)', 7: 'TVS',\n                             8: 'ETVS', 9: 'MDA', 10: 'MDA (Elev.)', 11: 'MDA (Weak)'}\n\n        # Read the number of bytes and set a mark for sanity checking\n        num_bytes = self._buffer.read_int(2, 'big', signed=False)\n        packet_data_start = self._buffer.set_mark()\n\n        scale = self.pos_scale(in_sym_block)\n\n        # Loop over the bytes we have\n        ret = defaultdict(list)\n        while self._buffer.offset_from(packet_data_start) < num_bytes:\n            # Read position\n            ret['x'].append(self._buffer.read_int(2, 'big', signed=True) * scale)\n            ret['y'].append(self._buffer.read_int(2, 'big', signed=True) * scale)\n\n            # Handle any types that have additional info\n            if code in (3, 11, 25):\n                ret['radius'].append(self._buffer.read_int(2, 'big', signed=True) * scale)\n            elif code == 15:\n                ret['id'].append(''.join(self._buffer.read_ascii(2)))\n            elif code == 19:\n                ret['POH'].append(self._buffer.read_int(2, 'big', signed=True))\n                ret['POSH'].append(self._buffer.read_int(2, 'big', signed=True))\n                ret['Max Size'].append(self._buffer.read_int(2, 'big', signed=False))\n            elif code == 20:\n                kind = self._buffer.read_int(2, 'big', signed=False)\n                attr = self._buffer.read_int(2, 'big', signed=False)\n                if kind < 5 or kind > 8:\n                    ret['radius'].append(attr * scale)\n\n                if kind not in point_feature_map:\n                    log.warning('%s: Unknown graphic symbol point kind %d/%x.',\n                                self.filename, kind, kind)\n                    ret['type'].append(f'Unknown ({kind:d})')\n                else:\n                    ret['type'].append(point_feature_map[kind])\n\n        # Map the code to a name for this type of symbol\n        if code != 20:\n            if code not in type_map:\n                log.warning('%s: Unknown graphic symbol type %d/%x.',\n                            self.filename, code, code)\n                ret['type'] = 'Unknown'\n            else:\n                ret['type'] = type_map[code]\n\n        # Check and return\n        assert self._buffer.offset_from(packet_data_start) == num_bytes\n\n        # Reduce dimensions of lists if possible\n        reduce_lists(ret)\n\n        return ret",
  "def _unpack_packet_scit(self, code, in_sym_block):\n        num_bytes = self._buffer.read_int(2, 'big', signed=False)\n        packet_data_start = self._buffer.set_mark()\n        ret = defaultdict(list)\n        while self._buffer.offset_from(packet_data_start) < num_bytes:\n            next_code = self._buffer.read_int(2, 'big', signed=False)\n            if next_code not in self.packet_map:\n                log.warning('%s: Unknown packet in SCIT %d/%x.',\n                            self.filename, next_code, next_code)\n                self._buffer.jump_to(packet_data_start, num_bytes)\n                return ret\n            else:\n                next_packet = self.packet_map[next_code](self, next_code, in_sym_block)\n                if next_code == 6:\n                    ret['track'].append(next_packet['vectors'])\n                elif next_code == 25:\n                    ret['STI Circle'].append(next_packet)\n                elif next_code == 2:\n                    ret['markers'].append(next_packet)\n                else:\n                    log.warning('%s: Unsupported packet in SCIT %d/%x.',\n                                self.filename, next_code, next_code)\n                    ret['data'].append(next_packet)\n        reduce_lists(ret)\n        return ret",
  "def _unpack_packet_digital_precipitation(self, code, in_sym_block):\n        # Read off a couple of unused spares\n        self._buffer.read_int(2, 'big', signed=False)\n        self._buffer.read_int(2, 'big', signed=False)\n\n        # Get the size of the grid\n        lfm_boxes = self._buffer.read_int(2, 'big', signed=False)\n        num_rows = self._buffer.read_int(2, 'big', signed=False)\n        rows = []\n\n        # Read off each row and decode the RLE data\n        for _ in range(num_rows):\n            row_num_bytes = self._buffer.read_int(2, 'big', signed=False)\n            row_bytes = self._buffer.read_binary(row_num_bytes)\n            if code == 18:\n                row = self._unpack_rle_data(row_bytes)\n            else:\n                row = []\n                for run, level in zip(row_bytes[::2], row_bytes[1::2]):\n                    row.extend([level] * run)\n            assert len(row) == lfm_boxes\n            rows.append(row)\n\n        return {'data': rows}",
  "def _unpack_packet_linked_vector(self, code, in_sym_block):\n        num_bytes = self._buffer.read_int(2, 'big', signed=True)\n        if code == 9:\n            value = self._buffer.read_int(2, 'big', signed=True)\n            num_bytes -= 2\n        else:\n            value = None\n        scale = self.pos_scale(in_sym_block)\n        pos = [b * scale for b in self._buffer.read_binary(num_bytes / 2, '>h')]\n        vectors = list(zip(pos[::2], pos[1::2]))\n        return {'vectors': vectors, 'color': value}",
  "def _unpack_packet_vector(self, code, in_sym_block):\n        num_bytes = self._buffer.read_int(2, 'big', signed=True)\n        if code == 10:\n            value = self._buffer.read_int(2, 'big', signed=True)\n            num_bytes -= 2\n        else:\n            value = None\n        scale = self.pos_scale(in_sym_block)\n        pos = [p * scale for p in self._buffer.read_binary(num_bytes / 2, '>h')]\n        vectors = list(zip(pos[::4], pos[1::4], pos[2::4], pos[3::4]))\n        return {'vectors': vectors, 'color': value}",
  "def _unpack_packet_contour_color(self, code, in_sym_block):\n        # Check for color value indicator\n        assert self._buffer.read_int(2, 'big', signed=False) == 0x0002\n\n        # Read and return value (level) of contour\n        return {'color': self._buffer.read_int(2, 'big', signed=False)}",
  "def _unpack_packet_linked_contour(self, code, in_sym_block):\n        # Check for initial point indicator\n        assert self._buffer.read_int(2, 'big', signed=False) == 0x8000\n\n        scale = self.pos_scale(in_sym_block)\n        startx = self._buffer.read_int(2, 'big', signed=True) * scale\n        starty = self._buffer.read_int(2, 'big', signed=True) * scale\n        vectors = [(startx, starty)]\n        num_bytes = self._buffer.read_int(2, 'big', signed=False)\n        pos = [b * scale for b in self._buffer.read_binary(num_bytes / 2, '>h')]\n        vectors.extend(zip(pos[::2], pos[1::2]))\n        return {'vectors': vectors}",
  "def _unpack_packet_wind_barbs(self, code, in_sym_block):\n        # Figure out how much to read\n        num_bytes = self._buffer.read_int(2, 'big', signed=True)\n        packet_data_start = self._buffer.set_mark()\n        ret = defaultdict(list)\n\n        # Read while we have data, then return\n        while self._buffer.offset_from(packet_data_start) < num_bytes:\n            ret['color'].append(self._buffer.read_int(2, 'big', signed=True))\n            ret['x'].append(self._buffer.read_int(2, 'big', signed=True)\n                            * self.pos_scale(in_sym_block))\n            ret['y'].append(self._buffer.read_int(2, 'big', signed=True)\n                            * self.pos_scale(in_sym_block))\n            ret['direc'].append(self._buffer.read_int(2, 'big', signed=True))\n            ret['speed'].append(self._buffer.read_int(2, 'big', signed=True))\n        return ret",
  "def _unpack_packet_generic(self, code, in_sym_block):\n        # Reserved HW\n        assert self._buffer.read_int(2, 'big', signed=True) == 0\n\n        # Read number of bytes (2 HW) and return\n        num_bytes = self._buffer.read_int(4, 'big', signed=True)\n        hunk = self._buffer.read(num_bytes)\n        xdrparser = Level3XDRParser(hunk)\n        return xdrparser(code)",
  "def _unpack_packet_trend_times(self, code, in_sym_block):\n        self._buffer.read_int(2, 'big', signed=True)  # number of bytes, not needed to process\n        return {'times': self._read_trends()}",
  "def _unpack_packet_cell_trend(self, code, in_sym_block):\n        code_map = ['Cell Top', 'Cell Base', 'Max Reflectivity Height',\n                    'Probability of Hail', 'Probability of Severe Hail',\n                    'Cell-based VIL', 'Maximum Reflectivity',\n                    'Centroid Height']\n        code_scales = [100, 100, 100, 1, 1, 1, 1, 100]\n        num_bytes = self._buffer.read_int(2, 'big', signed=True)\n        packet_data_start = self._buffer.set_mark()\n        cell_id = ''.join(self._buffer.read_ascii(2))\n        x = self._buffer.read_int(2, 'big', signed=True) * self.pos_scale(in_sym_block)\n        y = self._buffer.read_int(2, 'big', signed=True) * self.pos_scale(in_sym_block)\n        ret = {'id': cell_id, 'x': x, 'y': y}\n        while self._buffer.offset_from(packet_data_start) < num_bytes:\n            code = self._buffer.read_int(2, 'big', signed=True)\n            try:\n                ind = code - 1\n                key = code_map[ind]\n                scale = code_scales[ind]\n            except IndexError:\n                log.warning('%s: Unsupported trend code %d/%x.', self.filename, code, code)\n                key = 'Unknown'\n                scale = 1\n            vals = self._read_trends()\n            if code in (1, 2):\n                ret[f'{key} Limited'] = [v > 700 for v in vals]\n                vals = [v - 1000 if v > 700 else v for v in vals]\n            ret[key] = [v * scale for v in vals]\n\n        return ret",
  "def _read_trends(self):\n        num_vols, latest = self._buffer.read(2)\n        vals = [self._buffer.read_int(2, 'big', signed=True) for _ in range(num_vols)]\n\n        # Wrap the circular buffer so that latest is last\n        return vals[latest:] + vals[:latest]",
  "def __call__(self, code):\n        \"\"\"Perform the actual unpacking.\"\"\"\n        xdr = OrderedDict()\n\n        if code == 28:\n            xdr.update(self._unpack_prod_desc())\n        else:\n            log.warning('XDR: code %d not implemented', code)\n\n        # Check that we got it all\n        self.done()\n        return xdr",
  "def unpack_string(self):\n        \"\"\"Unpack the internal data as a string.\"\"\"\n        return Unpacker.unpack_string(self).decode('ascii')",
  "def _unpack_prod_desc(self):\n        xdr = OrderedDict()\n\n        # NOTE: The ICD (262001U) incorrectly lists op-mode, vcp, el_num, and\n        # spare as int*2. Changing to int*4 makes things parse correctly.\n        xdr['name'] = self.unpack_string()\n        xdr['description'] = self.unpack_string()\n        xdr['code'] = self.unpack_int()\n        xdr['type'] = self.unpack_int()\n        xdr['prod_time'] = self.unpack_uint()\n        xdr['radar_name'] = self.unpack_string()\n        xdr['latitude'] = self.unpack_float()\n        xdr['longitude'] = self.unpack_float()\n        xdr['height'] = self.unpack_float()\n        xdr['vol_time'] = self.unpack_uint()\n        xdr['el_time'] = self.unpack_uint()\n        xdr['el_angle'] = self.unpack_float()\n        xdr['vol_num'] = self.unpack_int()\n        xdr['op_mode'] = self.unpack_int()\n        xdr['vcp_num'] = self.unpack_int()\n        xdr['el_num'] = self.unpack_int()\n        xdr['compression'] = self.unpack_int()\n        xdr['uncompressed_size'] = self.unpack_int()\n        xdr['parameters'] = self._unpack_parameters()\n        xdr['components'] = self._unpack_components()\n\n        return xdr",
  "def _unpack_parameters(self):\n        num = self.unpack_int()\n\n        # ICD documents a \"pointer\" here, that seems to be garbage. Just read\n        # and use the number, starting the list immediately.\n        self.unpack_int()\n\n        if num == 0:\n            return None\n\n        ret = []\n        for i in range(num):\n            ret.append((self.unpack_string(), self.unpack_string()))\n            if i < num - 1:\n                self.unpack_int()  # Another pointer for the 'list' ?\n\n        if num == 1:\n            ret = ret[0]\n\n        return ret",
  "def _unpack_components(self):\n        num = self.unpack_int()\n\n        # ICD documents a \"pointer\" here, that seems to be garbage. Just read\n        # and use the number, starting the list immediately.\n        self.unpack_int()\n\n        ret = []\n        for i in range(num):\n            try:\n                code = self.unpack_int()\n                ret.append(self._component_lookup[code](self))\n                if i < num - 1:\n                    self.unpack_int()  # Another pointer for the 'list' ?\n            except KeyError:\n                log.warning('Unknown XDR Component: %d', code)\n                break\n\n        if num == 1:\n            ret = ret[0]\n\n        return ret",
  "def _unpack_radial(self):\n        ret = self.radial_fmt(description=self.unpack_string(),\n                              gate_width=self.unpack_float(),\n                              first_gate=self.unpack_float(),\n                              parameters=self._unpack_parameters(),\n                              radials=None)\n        num_rads = self.unpack_int()\n        rads = []\n        for _ in range(num_rads):\n            # ICD is wrong, says num_bins is float, should be int\n            rads.append(self.radial_data_fmt(azimuth=self.unpack_float(),\n                                             elevation=self.unpack_float(),\n                                             width=self.unpack_float(),\n                                             num_bins=self.unpack_int(),\n                                             attributes=self.unpack_string(),\n                                             data=self.unpack_array(self.unpack_int)))\n        return ret._replace(radials=rads)",
  "def _unpack_text(self):\n        return self.text_fmt(parameters=self._unpack_parameters(),\n                             text=self.unpack_string())",
  "class FileTypes(Enum):\n    \"\"\"GEMPAK file type.\"\"\"\n\n    surface = 1\n    sounding = 2\n    grid = 3",
  "class DataTypes(Enum):\n    \"\"\"Data management library data types.\"\"\"\n\n    real = 1\n    integer = 2\n    character = 3\n    realpack = 4\n    grid = 5",
  "class VerticalCoordinates(Enum):\n    \"\"\"Vertical coordinates.\"\"\"\n\n    none = 0\n    pres = 1\n    thta = 2\n    hght = 3\n    sgma = 4\n    dpth = 5\n    hybd = 6\n    pvab = 7\n    pvbl = 8",
  "class PackingType(Enum):\n    \"\"\"GRIB packing type.\"\"\"\n\n    none = 0\n    grib = 1\n    nmc = 2\n    diff = 3\n    dec = 4\n    grib2 = 5",
  "class ForecastType(Enum):\n    \"\"\"Forecast type.\"\"\"\n\n    analysis = 0\n    forecast = 1\n    guess = 2\n    initial = 3",
  "class DataSource(Enum):\n    \"\"\"Data source.\"\"\"\n\n    model = 0\n    airway_surface = 1\n    metar = 2\n    ship = 3\n    raob_buoy = 4\n    synop_raob_vas = 5\n    grid = 6\n    watch_by_county = 7\n    unknown = 99\n    text = 100\n    metar2 = 102\n    ship2 = 103\n    raob_buoy2 = 104\n    synop_raob_vas2 = 105",
  "def _check_nan(value, missing=-9999):\n    \"\"\"Check for nan values and replace with missing.\"\"\"\n    return missing if math.isnan(value) else value",
  "def convert_degc_to_k(val, missing=-9999):\n    \"\"\"Convert scalar values from degC to K, handling missing values.\"\"\"\n    return val + constants.nounit.zero_degc if val != missing else val",
  "def _data_source(source):\n    \"\"\"Get data source from stored integer.\"\"\"\n    try:\n        return DataSource(source)\n    except ValueError:\n        log.warning('Could not interpret data source `%s`. '\n                    'Setting to `Unknown`.', source)\n        return DataSource(99)",
  "def _word_to_position(word, bytes_per_word=BYTES_PER_WORD):\n    \"\"\"Return beginning position of a word in bytes.\"\"\"\n    return (word * bytes_per_word) - bytes_per_word",
  "def _interp_logp_data(sounding, missing=-9999):\n    \"\"\"Interpolate missing sounding data.\n\n    This function is similar to the MR_MISS subroutine in GEMPAK.\n    \"\"\"\n    size = len(sounding['PRES'])\n    recipe = [('TEMP', 'DWPT'), ('DRCT', 'SPED'), ('DWPT', None)]\n\n    for var1, var2 in recipe:\n        iabove = 0\n        i = 1\n        more = True\n        while i < (size - 1) and more:\n            if sounding[var1][i] == missing:\n                if iabove <= i:\n                    iabove = i + 1\n                    found = False\n                    while not found:\n                        if sounding[var1][iabove] != missing:\n                            found = True\n                        else:\n                            iabove += 1\n                            if iabove >= size:\n                                found = True\n                                iabove = 0\n                                more = False\n\n                if (var2 is None and iabove != 0\n                   and sounding['PRES'][i - 1] > 100\n                   and sounding['PRES'][iabove] < 100):\n                    iabove = 0\n\n                if iabove != 0:\n                    adata = {}\n                    bdata = {}\n                    for param, val in sounding.items():\n                        if (param in ['PRES', 'TEMP', 'DWPT',\n                                      'DRCT', 'SPED', 'HGHT']):\n                            adata[param] = val[i - 1]\n                            bdata[param] = val[iabove]\n                    vlev = sounding['PRES'][i]\n                    outdata = _interp_parameters(vlev, adata, bdata, missing)\n                    sounding[var1][i] = outdata[var1]\n                    if var2 is not None:\n                        sounding[var2][i] = outdata[var2]\n            i += 1",
  "def _interp_logp_height(sounding, missing=-9999):\n    \"\"\"Interpolate height linearly with respect to log p.\n\n    This function mimics the functionality of the MR_INTZ\n    subroutine in GEMPAK.\n    \"\"\"\n    size = maxlev = len(sounding['HGHT'])\n    for item in reversed(sounding['HGHT']):\n        maxlev -= 1\n        if item != missing:\n            break\n\n    pbot = missing\n    for i in range(maxlev):\n        press = sounding['PRES'][i]\n        hght = sounding['HGHT'][i]\n\n        if press == missing:\n            continue\n        elif hght != missing:\n            pbot = press\n            zbot = hght\n            ptop = 2000\n        elif pbot == missing:\n            continue\n        else:\n            ilev = i + 1\n            while press <= ptop:\n                if sounding['HGHT'][ilev] != missing:\n                    ptop = sounding['PRES'][ilev]\n                    ztop = sounding['HGHT'][ilev]\n                else:\n                    ilev += 1\n            sounding['HGHT'][i] = (zbot + (ztop - zbot)\n                                   * (np.log(press / pbot) / np.log(ptop / pbot)))\n\n    if maxlev < size - 1:\n        if maxlev > -1:\n            pb = sounding['PRES'][maxlev] * 100  # hPa to Pa\n            zb = sounding['HGHT'][maxlev]  # m\n            tb = convert_degc_to_k(sounding['TEMP'][maxlev], missing)\n            tdb = convert_degc_to_k(sounding['DWPT'][maxlev], missing)\n        else:\n            pb, zb, tb, tdb = repeat(missing, 4)\n\n        for i in range(maxlev + 1, size):\n            if sounding['HGHT'][i] == missing:\n                tt = convert_degc_to_k(sounding['TEMP'][i], missing)\n                tdt = convert_degc_to_k(sounding['DWPT'][i], missing)\n                pt = sounding['PRES'][i] * 100  # hPa to Pa\n\n                pl = np.array([pb, pt])\n                tl = np.array([tb, tt])\n                tdl = np.array([tdb, tdt])\n\n                if missing in tdl:\n                    tvl = tl\n                else:\n                    ql = specific_humidity_from_dewpoint._nounit(pl, tdl)\n                    tvl = virtual_temperature._nounit(tl, ql)\n\n                if missing not in [*tvl, zb]:\n                    sounding['HGHT'][i] = (zb + thickness_hydrostatic._nounit(pl, tvl))\n                else:\n                    sounding['HGHT'][i] = missing",
  "def _interp_logp_pressure(sounding, missing=-9999):\n    \"\"\"Interpolate pressure from heights.\n\n    This function is similar to the MR_INTP subroutine from GEMPAK.\n    \"\"\"\n    i = 0\n    ilev = -1\n    klev = -1\n    size = len(sounding['PRES'])\n    pt = missing\n    zt = missing\n    pb = missing\n    zb = missing\n\n    while i < size:\n        p = sounding['PRES'][i]\n        z = sounding['HGHT'][i]\n\n        if p != missing and z != missing:\n            klev = i\n            pt = p\n            zt = z\n\n        if ilev != -1 and klev != -1:\n            for j in range(ilev + 1, klev):\n                z = sounding['HGHT'][j]\n                if missing not in [z, zb, pb]:\n                    sounding['PRES'][j] = (\n                        pb * np.exp((z - zb) * np.log(pt / pb) / (zt - zb))\n                    )\n        ilev = klev\n        pb = pt\n        zb = zt\n        i += 1",
  "def _interp_moist_height(sounding, missing=-9999):\n    \"\"\"Interpolate moist hydrostatic height.\n\n    This function mimics the functionality of the MR_SCMZ\n    subroutine in GEMPAK. This the default behavior when\n    merging observed sounding data.\n    \"\"\"\n    hlist = np.ones(len(sounding['PRES'])) * -9999\n\n    ilev = -1\n    top = False\n\n    found = False\n    while not found and not top:\n        ilev += 1\n        if ilev >= len(sounding['PRES']):\n            top = True\n        elif missing not in [\n            sounding['PRES'][ilev],\n            sounding['TEMP'][ilev],\n            sounding['HGHT'][ilev]\n        ]:\n            found = True\n\n    while not top:\n        plev = sounding['PRES'][ilev] * 100  # hPa to Pa\n        pb = sounding['PRES'][ilev] * 100  # hPa to Pa\n        tb = convert_degc_to_k(sounding['TEMP'][ilev], missing)\n        tdb = convert_degc_to_k(sounding['DWPT'][ilev], missing)\n        zb = sounding['HGHT'][ilev]  # m\n        zlev = sounding['HGHT'][ilev]  # m\n        jlev = ilev\n        klev = 0\n        mand = False\n\n        while not mand:\n            jlev += 1\n            if jlev >= len(sounding['PRES']):\n                mand = True\n                top = True\n            else:\n                pt = sounding['PRES'][jlev] * 100  # hPa to Pa\n                tt = convert_degc_to_k(sounding['TEMP'][jlev], missing)\n                tdt = convert_degc_to_k(sounding['DWPT'][jlev], missing)\n                zt = sounding['HGHT'][jlev]  # m\n                if (zt != missing and tt != missing):\n                    mand = True\n                    klev = jlev\n\n                pl = np.array([pb, pt])\n                tl = np.array([tb, tt])\n                tdl = np.array([tdb, tdt])\n\n                if missing in tdl:\n                    tvl = tl\n                else:\n                    ql = specific_humidity_from_dewpoint._nounit(pl, tdl)\n                    tvl = virtual_temperature._nounit(tl, ql)\n\n                if missing not in [*tl, zb]:\n                    scale_z = scale_height._nounit(*tvl)\n                    znew = zb + thickness_hydrostatic._nounit(pl, tvl)\n                    tb = tt\n                    tdb = tdt\n                    pb = pt\n                    zb = znew\n                else:\n                    scale_z, znew = repeat(missing, 2)\n                hlist[jlev] = scale_z\n\n        if klev != 0:\n            s = (zt - zlev) / (znew - zlev)\n            for h in range(ilev + 1, klev + 1):\n                hlist[h] *= s\n\n        hbb = zlev\n        pbb = plev\n        for ii in range(ilev + 1, jlev):\n            p = sounding['PRES'][ii] * 100  # hPa to Pa\n            scale_z = hlist[ii]\n            if missing not in [scale_z, hbb, pbb, p]:\n                th = (scale_z * constants.nounit.g) / constants.nounit.Rd\n                tbar = np.array([th, th])\n                pll = np.array([pbb, p])\n                z = hbb + thickness_hydrostatic._nounit(pll, tbar)\n            else:\n                z = missing\n            sounding['HGHT'][ii] = z\n            hbb = z\n            pbb = p\n\n        ilev = klev",
  "def _interp_parameters(vlev, adata, bdata, missing=-9999):\n    \"\"\"General interpolation with respect to log-p.\n\n    See the PC_INTP subroutine in GEMPAK.\n    \"\"\"\n    pres1 = adata['PRES']\n    pres2 = bdata['PRES']\n    between = (((pres1 < pres2) and (pres1 < vlev)\n               and (vlev < pres2))\n               or ((pres2 < pres1) and (pres2 < vlev)\n               and (vlev < pres1)))\n\n    if not between:\n        raise ValueError('Current pressure does not fall between levels.')\n    elif pres1 <= 0 or pres2 <= 0:\n        raise ValueError('Pressure cannot be negative.')\n\n    outdata = {}\n    rmult = np.log(vlev / pres1) / np.log(pres2 / pres1)\n    outdata['PRES'] = vlev\n    for param, aval in adata.items():\n        bval = bdata[param]\n        if param == 'DRCT':\n            ang1 = aval % 360\n            ang2 = bval % 360\n            if abs(ang1 - ang2) > 180:\n                if ang1 < ang2:\n                    ang1 += 360\n                else:\n                    ang2 += 360\n            ang = ang1 + (ang2 - ang1) * rmult\n            outdata[param] = ang % 360\n        else:\n            outdata[param] = aval + (bval - aval) * rmult\n\n        if missing in [aval, bval]:\n            outdata[param] = missing\n\n    return outdata",
  "def _wx_to_wnum(wx1, wx2, wx3, missing=-9999):\n    \"\"\"Convert METAR present weather code to GEMPAK weather number.\n\n    Notes\n    -----\n    See GEMAPK function PT_WNMT.\n    \"\"\"\n    metar_codes = [\n        'BR', 'DS', 'DU', 'DZ', 'FC', 'FG', 'FU', 'GR', 'GS',\n        'HZ', 'IC', 'PL', 'PO', 'RA', 'SA', 'SG', 'SN', 'SQ',\n        'SS', 'TS', 'UP', 'VA', '+DS', '-DZ', '+DZ', '+FC',\n        '-GS', '+GS', '-PL', '+PL', '-RA', '+RA', '-SG',\n        '+SG', '-SN', '+SN', '+SS', 'BCFG', 'BLDU', 'BLPY',\n        'BLSA', 'BLSN', 'DRDU', 'DRSA', 'DRSN', 'FZDZ', 'FZFG',\n        'FZRA', 'MIFG', 'PRFG', 'SHGR', 'SHGS', 'SHPL', 'SHRA',\n        'SHSN', 'TSRA', '+BLDU', '+BLSA', '+BLSN', '-FZDZ',\n        '+FZDZ', '+FZFG', '-FZRA', '+FZRA', '-SHGS', '+SHGS',\n        '-SHPL', '+SHPL', '-SHRA', '+SHRA', '-SHSN', '+SHSN',\n        '-TSRA', '+TSRA'\n    ]\n\n    gempak_wnum = [\n        9, 33, 8, 2, -2, 9, 7, 4, 25, 6, 36, 23, 40, 1, 35, 24, 3, 10,\n        35, 5, 41, 11, 68, 17, 18, -1, 61, 62, 57, 58, 13, 14, 59, 60, 20,\n        21, 69, 9, 33, 34, 35, 32, 33, 35, 32, 19, 30, 15, 31, 9, 27, 67,\n        63, 16, 22, 66, 68, 69, 70, 53, 54, 30, 49, 50, 67, 67, 75, 76, 51,\n        52, 55, 56, 77, 78\n    ]\n\n    if wx1 in metar_codes:\n        wn1 = gempak_wnum[metar_codes.index(wx1)]\n    else:\n        wn1 = 0\n\n    if wx2 in metar_codes:\n        wn2 = gempak_wnum[metar_codes.index(wx2)]\n    else:\n        wn2 = 0\n\n    if wx3 in metar_codes:\n        wn3 = gempak_wnum[metar_codes.index(wx3)]\n    else:\n        wn3 = 0\n\n    if all(w >= 0 for w in [wn1, wn2, wn3]):\n        wnum = wn3 * 80 * 80 + wn2 * 80 + wn1\n    else:\n        wnum = min([wn1, wn2, wn3])\n        if wnum == 0:\n            wnum = missing\n\n    return wnum",
  "def _convert_clouds(cover, height, missing=-9999):\n    \"\"\"Convert METAR cloud cover to GEMPAK code.\n\n    Notes\n    -----\n    See GEMPAK function BR_CMTN.\n    \"\"\"\n    cover_text = ['CLR', 'SCT', 'BKN', 'OVC', 'VV', 'FEW', 'SKC']\n    if not isinstance(cover, str):\n        return missing\n\n    code = 0\n    if cover in cover_text:\n        code = cover_text.index(cover) + 1\n\n    if code == 7:\n        code = 1\n\n    if not math.isnan(height):\n        code += height\n        if height == 0:\n            code *= -1\n\n    return code",
  "class GempakFile:\n    \"\"\"Base class for GEMPAK files.\n\n    Reads ubiquitous GEMPAK file headers (i.e., the data management portion of\n    each file).\n    \"\"\"\n\n    prod_desc_fmt = [('version', 'i'), ('file_headers', 'i'),\n                     ('file_keys_ptr', 'i'), ('rows', 'i'),\n                     ('row_keys', 'i'), ('row_keys_ptr', 'i'),\n                     ('row_headers_ptr', 'i'), ('columns', 'i'),\n                     ('column_keys', 'i'), ('column_keys_ptr', 'i'),\n                     ('column_headers_ptr', 'i'), ('parts', 'i'),\n                     ('parts_ptr', 'i'), ('data_mgmt_ptr', 'i'),\n                     ('data_mgmt_length', 'i'), ('data_block_ptr', 'i'),\n                     ('file_type', 'i', FileTypes),\n                     ('data_source', 'i', _data_source),\n                     ('machine_type', 'i'), ('missing_int', 'i'),\n                     (None, '12x'), ('missing_float', 'f')]\n\n    grid_nav_fmt = [('grid_definition_type', 'f'),\n                    ('projection', '3sx', bytes.decode),\n                    ('left_grid_number', 'f'), ('bottom_grid_number', 'f'),\n                    ('right_grid_number', 'f'), ('top_grid_number', 'f'),\n                    ('lower_left_lat', 'f'), ('lower_left_lon', 'f'),\n                    ('upper_right_lat', 'f'), ('upper_right_lon', 'f'),\n                    ('proj_angle1', 'f'), ('proj_angle2', 'f'),\n                    ('proj_angle3', 'f'), (None, '972x')]\n\n    grid_anl_fmt1 = [('analysis_type', 'f'), ('delta_n', 'f'),\n                     ('delta_x', 'f'), ('delta_y', 'f'),\n                     (None, '4x'), ('garea_llcr_lat', 'f'),\n                     ('garea_llcr_lon', 'f'), ('garea_urcr_lat', 'f'),\n                     ('garea_urcr_lon', 'f'), ('extarea_llcr_lat', 'f'),\n                     ('extarea_llcr_lon', 'f'), ('extarea_urcr_lat', 'f'),\n                     ('extarea_urcr_lon', 'f'), ('datarea_llcr_lat', 'f'),\n                     ('datarea_llcr_lon', 'f'), ('datarea_urcr_lat', 'f'),\n                     ('datarea_urcrn_lon', 'f'), (None, '444x')]\n\n    grid_anl_fmt2 = [('analysis_type', 'f'), ('delta_n', 'f'),\n                     ('grid_ext_left', 'f'), ('grid_ext_down', 'f'),\n                     ('grid_ext_right', 'f'), ('grid_ext_up', 'f'),\n                     ('garea_llcr_lat', 'f'), ('garea_llcr_lon', 'f'),\n                     ('garea_urcr_lat', 'f'), ('garea_urcr_lon', 'f'),\n                     ('extarea_llcr_lat', 'f'), ('extarea_llcr_lon', 'f'),\n                     ('extarea_urcr_lat', 'f'), ('extarea_urcr_lon', 'f'),\n                     ('datarea_llcr_lat', 'f'), ('datarea_llcr_lon', 'f'),\n                     ('datarea_urcr_lat', 'f'), ('datarea_urcrn_lon', 'f'),\n                     (None, '440x')]\n\n    data_management_fmt = ([('next_free_word', 'i'), ('max_free_pairs', 'i'),\n                           ('actual_free_pairs', 'i'), ('last_word', 'i')]\n                           + [(f'free_word{n:d}', 'i') for n in range(1, 29)])\n\n    def __init__(self, file):\n        \"\"\"Instantiate GempakFile object from file.\"\"\"\n        fobj = open_as_needed(file)\n\n        with contextlib.closing(fobj):\n            self._buffer = IOBuffer.fromfile(fobj)\n\n        # Save file start position as pointers use this as reference\n        self._start = self._buffer.set_mark()\n\n        # Process the main GEMPAK header to verify file format\n        self._process_gempak_header()\n        meta = self._buffer.set_mark()\n\n        # # Check for byte swapping\n        self._swap_bytes(bytes(self._buffer.read_binary(4)))\n        self._buffer.jump_to(meta)\n\n        # Process main metadata header\n        self.prod_desc = self._buffer.read_struct(NamedStruct(self.prod_desc_fmt,\n                                                              self.prefmt,\n                                                              'ProductDescription'))\n\n        # File Keys\n        # Surface and upper-air files will not have the file headers, so we need to check.\n        if self.prod_desc.file_headers > 0:\n            # This would grab any file headers, but NAVB and ANLB are the only ones used.\n            fkey_prod = product(['header_name', 'header_length', 'header_type'],\n                                range(1, self.prod_desc.file_headers + 1))\n            fkey_names = ['{}{}'.format(*x) for x in fkey_prod]\n            fkey_info = list(zip(fkey_names, np.repeat(('4s', 'i', 'i'),\n                                                       self.prod_desc.file_headers)))\n            self.file_keys_format = NamedStruct(fkey_info, self.prefmt, 'FileKeys')\n\n            self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.file_keys_ptr))\n            self.file_keys = self._buffer.read_struct(self.file_keys_format)\n\n            # file_key_blocks = self._buffer.set_mark()\n            # Navigation Block\n            navb_size = self._buffer.read_int(4, self.endian, False)\n\n            nav_stuct = NamedStruct(self.grid_nav_fmt,\n                                    self.prefmt,\n                                    'NavigationBlock')\n\n            if navb_size != nav_stuct.size // BYTES_PER_WORD:\n                raise ValueError('Navigation block size does not match GEMPAK specification')\n            else:\n                self.navigation_block = (\n                    self._buffer.read_struct(nav_stuct)\n                )\n            self.kx = int(self.navigation_block.right_grid_number)\n            self.ky = int(self.navigation_block.top_grid_number)\n\n            # Analysis Block\n            anlb_size = self._buffer.read_int(4, self.endian, False)\n            anlb_start = self._buffer.set_mark()\n            anlb1_struct = NamedStruct(self.grid_anl_fmt1,\n                                       self.prefmt,\n                                       'AnalysisBlock')\n            anlb2_struct = NamedStruct(self.grid_anl_fmt2,\n                                       self.prefmt,\n                                       'AnalysisBlock')\n\n            if anlb_size not in [anlb1_struct.size // BYTES_PER_WORD,\n                                 anlb2_struct.size // BYTES_PER_WORD]:\n                raise ValueError('Analysis block size does not match GEMPAK specification')\n            else:\n                anlb_type = self._buffer.read_struct(struct.Struct(self.prefmt + 'f'))[0]\n                self._buffer.jump_to(anlb_start)\n                if anlb_type == 1:\n                    self.analysis_block = (\n                        self._buffer.read_struct(anlb1_struct)\n                    )\n                elif anlb_type == 2:\n                    self.analysis_block = (\n                        self._buffer.read_struct(anlb2_struct)\n                    )\n                else:\n                    self.analysis_block = None\n        else:\n            self.analysis_block = None\n            self.navigation_block = None\n\n        # Data Management\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.data_mgmt_ptr))\n        self.data_management = self._buffer.read_struct(NamedStruct(self.data_management_fmt,\n                                                                    self.prefmt,\n                                                                    'DataManagement'))\n\n        # Row Keys\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.row_keys_ptr))\n        row_key_info = [(f'row_key{n:d}', '4s', self._decode_strip)\n                        for n in range(1, self.prod_desc.row_keys + 1)]\n        row_key_info.extend([(None, None)])\n        row_keys_fmt = NamedStruct(row_key_info, self.prefmt, 'RowKeys')\n        self.row_keys = self._buffer.read_struct(row_keys_fmt)\n\n        # Column Keys\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.column_keys_ptr))\n        column_key_info = [(f'column_key{n:d}', '4s', self._decode_strip)\n                           for n in range(1, self.prod_desc.column_keys + 1)]\n        column_key_info.extend([(None, None)])\n        column_keys_fmt = NamedStruct(column_key_info, self.prefmt, 'ColumnKeys')\n        self.column_keys = self._buffer.read_struct(column_keys_fmt)\n\n        # Parts\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.parts_ptr))\n        # parts = self._buffer.set_mark()\n        self.parts = []\n        parts_info = [('name', '4s', self._decode_strip),\n                      (None, f'{(self.prod_desc.parts - 1) * BYTES_PER_WORD:d}x'),\n                      ('header_length', 'i'),\n                      (None, f'{(self.prod_desc.parts - 1) * BYTES_PER_WORD:d}x'),\n                      ('data_type', 'i', DataTypes),\n                      (None, f'{(self.prod_desc.parts - 1) * BYTES_PER_WORD:d}x'),\n                      ('parameter_count', 'i')]\n        parts_info.extend([(None, None)])\n        parts_fmt = NamedStruct(parts_info, self.prefmt, 'Parts')\n        for n in range(1, self.prod_desc.parts + 1):\n            self.parts.append(self._buffer.read_struct(parts_fmt))\n            self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.parts_ptr + n))\n\n        # Parameters\n        # No need to jump to any position as this follows parts information\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.parts_ptr\n                                                            + self.prod_desc.parts * 4))\n        self.parameters = [{key: [] for key, _ in PARAM_ATTR}\n                           for n in range(self.prod_desc.parts)]\n        for attr, fmt in PARAM_ATTR:\n            fmt = (fmt[0], self.prefmt + fmt[1] if fmt[1] != 's' else fmt[1])\n            for n, part in enumerate(self.parts):\n                for _ in range(part.parameter_count):\n                    if 's' in fmt[1]:\n                        self.parameters[n][attr] += [\n                            self._decode_strip(self._buffer.read_binary(*fmt)[0])\n                        ]\n                    else:\n                        self.parameters[n][attr] += self._buffer.read_binary(*fmt)\n\n    def _swap_bytes(self, binary):\n        \"\"\"Swap between little and big endian.\"\"\"\n        self.swaped_bytes = (struct.pack('@i', 1) != binary)\n\n        if self.swaped_bytes:\n            if sys.byteorder == 'little':\n                self.prefmt = '>'\n                self.endian = 'big'\n            elif sys.byteorder == 'big':\n                self.prefmt = '<'\n                self.endian = 'little'\n        else:\n            self.prefmt = ''\n            self.endian = sys.byteorder\n\n    def _process_gempak_header(self):\n        \"\"\"Read the GEMPAK header from the file.\"\"\"\n        fmt = [('text', '28s', bytes.decode), (None, None)]\n\n        header = self._buffer.read_struct(NamedStruct(fmt, '', 'GempakHeader'))\n        if header.text != GEMPAK_HEADER:\n            raise TypeError('Unknown file format or invalid GEMPAK file')\n\n    @staticmethod\n    def _convert_dattim(dattim):\n        \"\"\"Convert GEMPAK DATTIM integer to datetime object.\"\"\"\n        if dattim:\n            if dattim < 100000000:\n                dt = datetime.strptime(f'{dattim:06d}', '%y%m%d')\n            else:\n                dt = datetime.strptime(f'{dattim:010d}', '%m%d%y%H%M')\n        else:\n            dt = None\n        return dt\n\n    @staticmethod\n    def _convert_ftime(ftime):\n        \"\"\"Convert GEMPAK forecast time and type integer.\"\"\"\n        if ftime >= 0:\n            iftype = ForecastType(ftime // 100000)\n            iftime = ftime - iftype.value * 100000\n            hours = iftime // 100\n            minutes = iftime - hours * 100\n            out = (iftype.name, timedelta(hours=hours, minutes=minutes))\n        else:\n            out = None\n        return out\n\n    @staticmethod\n    def _convert_level(level):\n        \"\"\"Convert levels.\"\"\"\n        if isinstance(level, (int, float)) and level >= 0:\n            return level\n        else:\n            return None\n\n    @staticmethod\n    def _convert_vertical_coord(coord):\n        \"\"\"Convert integer vertical coordinate to name.\"\"\"\n        if coord <= 8:\n            return VerticalCoordinates(coord).name.upper()\n        else:\n            return struct.pack('i', coord).decode()\n\n    @staticmethod\n    def _fortran_ishift(i, shift):\n        \"\"\"Python-friendly bit shifting.\"\"\"\n        if shift >= 0:\n            # Shift left and only keep low 32 bits\n            ret = (i << shift) & 0xffffffff\n\n            # If high bit, convert back to negative of two's complement\n            if ret > 0x7fffffff:\n                ret = -(~ret & 0x7fffffff) - 1\n            return ret\n        else:\n            # Shift right the low 32 bits\n            return (i & 0xffffffff) >> -shift\n\n    @staticmethod\n    def _decode_strip(b):\n        \"\"\"Decode bytes to string and strip whitespace.\"\"\"\n        return b.decode().strip()\n\n    @staticmethod\n    def _make_date(dattim):\n        \"\"\"Make a date object from GEMPAK DATTIM integer.\"\"\"\n        return GempakFile._convert_dattim(dattim).date()\n\n    @staticmethod\n    def _make_time(t):\n        \"\"\"Make a time object from GEMPAK FTIME integer.\"\"\"\n        string = f'{t:04d}'\n        return datetime.strptime(string, '%H%M').time()\n\n    def _unpack_real(self, buffer, parameters, length):\n        \"\"\"Unpack floating point data packed in integers.\n\n        Similar to DP_UNPK subroutine in GEMPAK.\n        \"\"\"\n        nparms = len(parameters['name'])\n        mskpat = 0xffffffff\n\n        pwords = (sum(parameters['bits']) - 1) // 32 + 1\n        npack = (length - 1) // pwords + 1\n        unpacked = np.ones(npack * nparms, dtype=np.float32) * self.prod_desc.missing_float\n        if npack * pwords != length:\n            raise ValueError('Unpacking length mismatch.')\n\n        ir = 0\n        ii = 0\n        for _i in range(npack):\n            pdat = buffer[ii:(ii + pwords)]\n            rdat = unpacked[ir:(ir + nparms)]\n            itotal = 0\n            for idata in range(nparms):\n                scale = 10**parameters['scale'][idata]\n                offset = parameters['offset'][idata]\n                bits = parameters['bits'][idata]\n                isbitc = (itotal % 32) + 1\n                iswrdc = (itotal // 32)\n                imissc = self._fortran_ishift(mskpat, bits - 32)\n\n                jbit = bits\n                jsbit = isbitc\n                jshift = 1 - jsbit\n                jsword = iswrdc\n                jword = pdat[jsword]\n                mask = self._fortran_ishift(mskpat, jbit - 32)\n                ifield = self._fortran_ishift(jword, jshift)\n                ifield &= mask\n\n                if (jsbit + jbit - 1) > 32:\n                    jword = pdat[jsword + 1]\n                    jshift += 32\n                    iword = self._fortran_ishift(jword, jshift)\n                    iword &= mask\n                    ifield |= iword\n\n                if ifield == imissc:\n                    rdat[idata] = self.prod_desc.missing_float\n                else:\n                    rdat[idata] = (ifield + offset) * scale\n                itotal += bits\n            unpacked[ir:(ir + nparms)] = rdat\n            ir += nparms\n            ii += pwords\n\n        return unpacked.tolist()",
  "class GempakGrid(GempakFile):\n    \"\"\"Subclass of GempakFile specific to GEMPAK gridded data.\"\"\"\n\n    def __init__(self, file, *args, **kwargs):\n        super().__init__(file)\n\n        datetime_names = ['GDT1', 'GDT2']\n        level_names = ['GLV1', 'GLV2']\n        ftime_names = ['GTM1', 'GTM2']\n        string_names = ['GPM1', 'GPM2', 'GPM3']\n\n        # Row Headers\n        # Based on GEMPAK source, row/col headers have a 0th element in their Fortran arrays.\n        # This appears to be a flag value to say a header is used or not. 9999\n        # means its in use, otherwise -9999. GEMPAK allows empty grids, etc., but\n        # no real need to keep track of that in Python.\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.row_headers_ptr))\n        self.row_headers = []\n        row_headers_info = [(key, 'i') for key in self.row_keys]\n        row_headers_info.extend([(None, None)])\n        row_headers_fmt = NamedStruct(row_headers_info, self.prefmt, 'RowHeaders')\n        for _ in range(1, self.prod_desc.rows + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.row_headers.append(self._buffer.read_struct(row_headers_fmt))\n\n        # Column Headers\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.column_headers_ptr))\n        self.column_headers = []\n        column_headers_info = [(key, 'i', self._convert_level) if key in level_names\n                               else (key, 'i', self._convert_vertical_coord) if key == 'GVCD'\n                               else (key, 'i', self._convert_dattim) if key in datetime_names\n                               else (key, 'i', self._convert_ftime) if key in ftime_names\n                               else (key, '4s', self._decode_strip) if key in string_names\n                               else (key, 'i')\n                               for key in self.column_keys]\n        column_headers_info.extend([(None, None)])\n        column_headers_fmt = NamedStruct(column_headers_info, self.prefmt, 'ColumnHeaders')\n        for _ in range(1, self.prod_desc.columns + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.column_headers.append(self._buffer.read_struct(column_headers_fmt))\n\n        self._gdinfo = []\n        for n, head in enumerate(self.column_headers):\n            self._gdinfo.append(\n                Grid(\n                    n,\n                    head.GTM1[0],\n                    head.GDT1 + head.GTM1[1],\n                    head.GDT2 + head.GTM2[1] if head.GDT2 and head.GTM2 else None,\n                    head.GPM1 + head.GPM2 + head.GPM3,\n                    head.GLV1,\n                    head.GLV2,\n                    head.GVCD,\n                )\n            )\n\n        # Coordinates\n        if self.navigation_block is not None:\n            self._get_crs()\n            self._set_coordinates()\n\n    def gdinfo(self):\n        \"\"\"Return grid information.\"\"\"\n        return self._gdinfo\n\n    def _get_crs(self):\n        \"\"\"Create CRS from GEMPAK navigation block.\"\"\"\n        gemproj = self.navigation_block.projection\n        proj, ptype = GEMPROJ_TO_PROJ[gemproj]\n        radius_sph = 6371200.0\n\n        if ptype == 'azm':\n            lat_0 = self.navigation_block.proj_angle1\n            lon_0 = self.navigation_block.proj_angle2\n            rot = self.navigation_block.proj_angle3\n            if rot != 0:\n                log.warning('Rotated projections currently '\n                            'not supported. Angle3 (%7.2f) ignored.', rot)\n            self.crs = pyproj.CRS.from_dict({'proj': proj,\n                                             'lat_0': lat_0,\n                                             'lon_0': lon_0,\n                                             'R': radius_sph})\n        elif ptype == 'cyl':\n            if gemproj != 'mcd':\n                lat_0 = self.navigation_block.proj_angle1\n                lon_0 = self.navigation_block.proj_angle2\n                rot = self.navigation_block.proj_angle3\n                if rot != 0:\n                    log.warning('Rotated projections currently '\n                                'not supported. Angle3 (%7.2f) ignored.', rot)\n                self.crs = pyproj.CRS.from_dict({'proj': proj,\n                                                 'lat_0': lat_0,\n                                                 'lon_0': lon_0,\n                                                 'R': radius_sph})\n            else:\n                avglat = (self.navigation_block.upper_right_lat\n                          + self.navigation_block.lower_left_lat) * 0.5\n                k_0 = (1 / math.cos(avglat)\n                       if self.navigation_block.proj_angle1 == 0\n                       else self.navigation_block.proj_angle1\n                       )\n                lon_0 = self.navigation_block.proj_angle2\n                self.crs = pyproj.CRS.from_dict({'proj': proj,\n                                                 'lat_0': avglat,\n                                                 'lon_0': lon_0,\n                                                 'k_0': k_0,\n                                                 'R': radius_sph})\n        elif ptype == 'con':\n            lat_1 = self.navigation_block.proj_angle1\n            lon_0 = self.navigation_block.proj_angle2\n            lat_2 = self.navigation_block.proj_angle3\n            self.crs = pyproj.CRS.from_dict({'proj': proj,\n                                             'lon_0': lon_0,\n                                             'lat_1': lat_1,\n                                             'lat_2': lat_2,\n                                             'R': radius_sph})\n\n    def _set_coordinates(self):\n        \"\"\"Use GEMPAK navigation block to define coordinates.\n\n        Defines geographic and projection coordinates for the object.\n        \"\"\"\n        transform = pyproj.Proj(self.crs)\n        llx, lly = transform(self.navigation_block.lower_left_lon,\n                             self.navigation_block.lower_left_lat)\n        urx, ury = transform(self.navigation_block.upper_right_lon,\n                             self.navigation_block.upper_right_lat)\n        self.x = np.linspace(llx, urx, self.kx, dtype=np.float32)\n        self.y = np.linspace(lly, ury, self.ky, dtype=np.float32)\n        xx, yy = np.meshgrid(self.x, self.y, copy=False)\n        self.lon, self.lat = transform(xx, yy, inverse=True)\n        self.lon = self.lon.astype(np.float32)\n        self.lat = self.lat.astype(np.float32)\n\n    def _unpack_grid(self, packing_type, part):\n        \"\"\"Read raw GEMPAK grid integers and unpack into floats.\"\"\"\n        if packing_type == PackingType.none:\n            lendat = self.data_header_length - part.header_length - 1\n\n            if lendat > 1:\n                buffer_fmt = f'{self.prefmt}{lendat}f'\n                buffer = self._buffer.read_struct(struct.Struct(buffer_fmt))\n                grid = np.zeros(self.ky * self.kx, dtype=np.float32)\n                grid[...] = buffer\n            else:\n                grid = None\n\n            return grid\n\n        elif packing_type == PackingType.nmc:\n            raise NotImplementedError('NMC unpacking not supported.')\n            # integer_meta_fmt = [('bits', 'i'), ('missing_flag', 'i'), ('kxky', 'i')]\n            # real_meta_fmt = [('reference', 'f'), ('scale', 'f')]\n            # self.grid_meta_int = self._buffer.read_struct(NamedStruct(integer_meta_fmt,\n            #                                                           self.prefmt,\n            #                                                           'GridMetaInt'))\n            # self.grid_meta_real = self._buffer.read_struct(NamedStruct(real_meta_fmt,\n            #                                                            self.prefmt,\n            #                                                            'GridMetaReal'))\n            # grid_start = self._buffer.set_mark()\n        elif packing_type == PackingType.diff:\n            integer_meta_fmt = [('bits', 'i'), ('missing_flag', 'i'),\n                                ('kxky', 'i'), ('kx', 'i')]\n            real_meta_fmt = [('reference', 'f'), ('scale', 'f'), ('diffmin', 'f')]\n            self.grid_meta_int = self._buffer.read_struct(NamedStruct(integer_meta_fmt,\n                                                                      self.prefmt,\n                                                                      'GridMetaInt'))\n            self.grid_meta_real = self._buffer.read_struct(NamedStruct(real_meta_fmt,\n                                                                       self.prefmt,\n                                                                       'GridMetaReal'))\n            # grid_start = self._buffer.set_mark()\n\n            imiss = 2**self.grid_meta_int.bits - 1\n            lendat = self.data_header_length - part.header_length - 8\n            packed_buffer_fmt = f'{self.prefmt}{lendat}i'\n            packed_buffer = self._buffer.read_struct(struct.Struct(packed_buffer_fmt))\n            grid = np.zeros((self.ky, self.kx), dtype=np.float32)\n\n            if lendat > 1:\n                iword = 0\n                ibit = 1\n                first = True\n                for j in range(self.ky):\n                    line = False\n                    for i in range(self.kx):\n                        jshft = self.grid_meta_int.bits + ibit - 33\n                        idat = self._fortran_ishift(packed_buffer[iword], jshft)\n                        idat &= imiss\n\n                        if jshft > 0:\n                            jshft -= 32\n                            idat2 = self._fortran_ishift(packed_buffer[iword + 1], jshft)\n                            idat |= idat2\n\n                        ibit += self.grid_meta_int.bits\n                        if ibit > 32:\n                            ibit -= 32\n                            iword += 1\n\n                        if (self.grid_meta_int.missing_flag and idat == imiss):\n                            grid[j, i] = self.prod_desc.missing_float\n                        else:\n                            if first:\n                                grid[j, i] = self.grid_meta_real.reference\n                                psav = self.grid_meta_real.reference\n                                plin = self.grid_meta_real.reference\n                                line = True\n                                first = False\n                            else:\n                                if not line:\n                                    grid[j, i] = plin + (self.grid_meta_real.diffmin\n                                                         + idat * self.grid_meta_real.scale)\n                                    line = True\n                                    plin = grid[j, i]\n                                else:\n                                    grid[j, i] = psav + (self.grid_meta_real.diffmin\n                                                         + idat * self.grid_meta_real.scale)\n                                psav = grid[j, i]\n            else:\n                grid = None\n\n            return grid\n\n        elif packing_type in [PackingType.grib, PackingType.dec]:\n            integer_meta_fmt = [('bits', 'i'), ('missing_flag', 'i'), ('kxky', 'i')]\n            real_meta_fmt = [('reference', 'f'), ('scale', 'f')]\n            self.grid_meta_int = self._buffer.read_struct(NamedStruct(integer_meta_fmt,\n                                                                      self.prefmt,\n                                                                      'GridMetaInt'))\n            self.grid_meta_real = self._buffer.read_struct(NamedStruct(real_meta_fmt,\n                                                                       self.prefmt,\n                                                                       'GridMetaReal'))\n            # grid_start = self._buffer.set_mark()\n\n            lendat = self.data_header_length - part.header_length - 6\n            packed_buffer_fmt = f'{self.prefmt}{lendat}i'\n\n            grid = np.zeros(self.grid_meta_int.kxky, dtype=np.float32)\n            packed_buffer = self._buffer.read_struct(struct.Struct(packed_buffer_fmt))\n            if lendat > 1:\n                imax = 2**self.grid_meta_int.bits - 1\n                ibit = 1\n                iword = 0\n                for cell in range(self.grid_meta_int.kxky):\n                    jshft = self.grid_meta_int.bits + ibit - 33\n                    idat = self._fortran_ishift(packed_buffer[iword], jshft)\n                    idat &= imax\n\n                    if jshft > 0:\n                        jshft -= 32\n                        idat2 = self._fortran_ishift(packed_buffer[iword + 1], jshft)\n                        idat |= idat2\n\n                    if (idat == imax) and self.grid_meta_int.missing_flag:\n                        grid[cell] = self.prod_desc.missing_float\n                    else:\n                        grid[cell] = (self.grid_meta_real.reference\n                                      + (idat * self.grid_meta_real.scale))\n\n                    ibit += self.grid_meta_int.bits\n                    if ibit > 32:\n                        ibit -= 32\n                        iword += 1\n            else:\n                grid = None\n\n            return grid\n        elif packing_type == PackingType.grib2:\n            raise NotImplementedError('GRIB2 unpacking not supported.')\n            # integer_meta_fmt = [('iuscal', 'i'), ('kx', 'i'),\n            #                     ('ky', 'i'), ('iscan_mode', 'i')]\n            # real_meta_fmt = [('rmsval', 'f')]\n            # self.grid_meta_int = self._buffer.read_struct(NamedStruct(integer_meta_fmt,\n            #                                                           self.prefmt,\n            #                                                           'GridMetaInt'))\n            # self.grid_meta_real = self._buffer.read_struct(NamedStruct(real_meta_fmt,\n            #                                                            self.prefmt,\n            #                                                            'GridMetaReal'))\n            # grid_start = self._buffer.set_mark()\n        else:\n            raise NotImplementedError(\n                f'No method for unknown grid packing {packing_type.name}'\n            )\n\n    def gdxarray(self, parameter=None, date_time=None, coordinate=None,\n                 level=None, date_time2=None, level2=None):\n        \"\"\"Select grids and output as list of xarray DataArrays.\n\n        Subset the data by parameter values. The default is to not\n        subset and return the entire dataset.\n\n        Parameters\n        ----------\n        parameter : str or Sequence[str]\n            Name of GEMPAK parameter.\n\n        date_time : `~datetime.datetime` or Sequence[datetime]\n            Valid datetime of the grid. Alternatively can be\n            a string with the format YYYYmmddHHMM.\n\n        coordinate : str or Sequence[str]\n            Vertical coordinate.\n\n        level : float or Sequence[float]\n            Vertical level.\n\n        date_time2 : `~datetime.datetime` or Sequence[datetime]\n            Secondary valid datetime of the grid. Alternatively can be\n            a string with the format YYYYmmddHHMM.\n\n        level2: float or Sequence[float]\n            Secondary vertical level. Typically used for layers.\n\n        Returns\n        -------\n        list\n            List of `xarray.DataArray` objects for each grid.\n        \"\"\"\n        if parameter is not None:\n            if (not isinstance(parameter, Iterable)\n               or isinstance(parameter, str)):\n                parameter = [parameter]\n            parameter = [p.upper() for p in parameter]\n\n        if date_time is not None:\n            if (not isinstance(date_time, Iterable)\n               or isinstance(date_time, str)):\n                date_time = [date_time]\n            for i, dt in enumerate(date_time):\n                if isinstance(dt, str):\n                    date_time[i] = datetime.strptime(dt, '%Y%m%d%H%M')\n\n        if coordinate is not None:\n            if (not isinstance(coordinate, Iterable)\n               or isinstance(coordinate, str)):\n                coordinate = [coordinate]\n            coordinate = [c.upper() for c in coordinate]\n\n        if level is not None and not isinstance(level, Iterable):\n            level = [level]\n\n        if date_time2 is not None:\n            if (not isinstance(date_time2, Iterable)\n               or isinstance(date_time2, str)):\n                date_time2 = [date_time2]\n            for i, dt in enumerate(date_time2):\n                if isinstance(dt, str):\n                    date_time2[i] = datetime.strptime(dt, '%Y%m%d%H%M')\n\n        if level2 is not None and not isinstance(level2, Iterable):\n            level2 = [level2]\n\n        # Figure out which columns to extract from the file\n        matched = self._gdinfo.copy()\n\n        if parameter is not None:\n            matched = filter(\n                lambda grid: grid if grid.PARM in parameter else False,\n                matched\n            )\n\n        if date_time is not None:\n            matched = filter(\n                lambda grid: grid if grid.DATTIM1 in date_time else False,\n                matched\n            )\n\n        if coordinate is not None:\n            matched = filter(\n                lambda grid: grid if grid.COORD in coordinate else False,\n                matched\n            )\n\n        if level is not None:\n            matched = filter(\n                lambda grid: grid if grid.LEVEL1 in level else False,\n                matched\n            )\n\n        if date_time2 is not None:\n            matched = filter(\n                lambda grid: grid if grid.DATTIM2 in date_time2 else False,\n                matched\n            )\n\n        if level2 is not None:\n            matched = filter(\n                lambda grid: grid if grid.LEVEL2 in level2 else False,\n                matched\n            )\n\n        matched = list(matched)\n\n        if len(matched) < 1:\n            raise KeyError('No grids were matched with given parameters.')\n\n        gridno = [g.GRIDNO for g in matched]\n\n        grids = []\n        irow = 0  # Only one row for grids\n        for icol, col_head in enumerate(self.column_headers):\n            if icol not in gridno:\n                continue\n            for iprt, part in enumerate(self.parts):\n                pointer = (self.prod_desc.data_block_ptr\n                           + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                           + (icol * self.prod_desc.parts + iprt))\n                self._buffer.jump_to(self._start, _word_to_position(pointer))\n                self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                data_header = self._buffer.set_mark()\n                self._buffer.jump_to(data_header,\n                                     _word_to_position(part.header_length + 1))\n                packing_type = PackingType(self._buffer.read_int(4, self.endian, False))\n\n                full_name = col_head.GPM1 + col_head.GPM2 + col_head.GPM3\n                ftype, ftime = col_head.GTM1\n                valid = col_head.GDT1 + ftime\n                gvcord = col_head.GVCD.lower() if col_head.GVCD is not None else 'none'\n                var = (GVCORD_TO_VAR[full_name]\n                       if full_name in GVCORD_TO_VAR\n                       else full_name.lower()\n                       )\n                data = self._unpack_grid(packing_type, part)\n                if data is not None:\n                    if data.ndim < 2:\n                        data = np.ma.array(data.reshape((self.ky, self.kx)),\n                                           mask=data == self.prod_desc.missing_float,\n                                           dtype=np.float32)\n                    else:\n                        data = np.ma.array(data, mask=data == self.prod_desc.missing_float,\n                                           dtype=np.float32)\n\n                    xrda = xr.DataArray(\n                        data=data[np.newaxis, np.newaxis, ...],\n                        coords={\n                            'time': [valid],\n                            gvcord: [col_head.GLV1],\n                            'x': self.x,\n                            'y': self.y,\n                            'metpy_crs': CFProjection(self.crs.to_cf())\n                        },\n                        dims=['time', gvcord, 'y', 'x'],\n                        name=var,\n                        attrs={\n                            'gempak_grid_type': ftype,\n                        }\n                    )\n                    xrda = xrda.metpy.assign_latitude_longitude()\n                    xrda['x'].attrs['units'] = 'meters'\n                    xrda['y'].attrs['units'] = 'meters'\n                    grids.append(xrda)\n\n                else:\n                    log.warning('Unable to read grid for %s', col_head.GPM1)\n        return grids",
  "class GempakSounding(GempakFile):\n    \"\"\"Subclass of GempakFile specific to GEMPAK sounding data.\"\"\"\n\n    def __init__(self, file, *args, **kwargs):\n        super().__init__(file)\n\n        # Row Headers\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.row_headers_ptr))\n        self.row_headers = []\n        row_headers_info = [(key, 'i', self._make_date) if key == 'DATE'\n                            else (key, 'i', self._make_time) if key == 'TIME'\n                            else (key, 'i')\n                            for key in self.row_keys]\n        row_headers_info.extend([(None, None)])\n        row_headers_fmt = NamedStruct(row_headers_info, self.prefmt, 'RowHeaders')\n        for _ in range(1, self.prod_desc.rows + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.row_headers.append(self._buffer.read_struct(row_headers_fmt))\n\n        # Column Headers\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.column_headers_ptr))\n        self.column_headers = []\n        column_headers_info = [(key, '4s', self._decode_strip) if key == 'STID'\n                               else (key, 'i') if key == 'STNM'\n                               else (key, 'i', lambda x: x / 100) if key == 'SLAT'\n                               else (key, 'i', lambda x: x / 100) if key == 'SLON'\n                               else (key, 'i') if key == 'SELV'\n                               else (key, '4s', self._decode_strip) if key == 'STAT'\n                               else (key, '4s', self._decode_strip) if key == 'COUN'\n                               else (key, '4s', self._decode_strip) if key == 'STD2'\n                               else (key, 'i')\n                               for key in self.column_keys]\n        column_headers_info.extend([(None, None)])\n        column_headers_fmt = NamedStruct(column_headers_info, self.prefmt, 'ColumnHeaders')\n        for _ in range(1, self.prod_desc.columns + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.column_headers.append(self._buffer.read_struct(column_headers_fmt))\n\n        self.merged = 'SNDT' in (part.name for part in self.parts)\n\n        self._sninfo = []\n        for irow, row_head in enumerate(self.row_headers):\n            for icol, col_head in enumerate(self.column_headers):\n                pointer = (self.prod_desc.data_block_ptr\n                           + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                           + (icol * self.prod_desc.parts))\n\n                self._buffer.jump_to(self._start, _word_to_position(pointer))\n                data_ptr = self._buffer.read_int(4, self.endian, False)\n\n                if data_ptr:\n                    self._sninfo.append(\n                        Sounding(\n                            irow,\n                            icol,\n                            datetime.combine(row_head.DATE, row_head.TIME),\n                            col_head.STID,\n                            col_head.STNM,\n                            col_head.SLAT,\n                            col_head.SLON,\n                            col_head.SELV,\n                            col_head.STAT,\n                            col_head.COUN,\n                        )\n                    )\n\n    def sninfo(self):\n        \"\"\"Return sounding information.\"\"\"\n        return self._sninfo\n\n    def _unpack_merged(self, sndno):\n        \"\"\"Unpack merged sounding data.\"\"\"\n        soundings = []\n        for irow, row_head in enumerate(self.row_headers):\n            for icol, col_head in enumerate(self.column_headers):\n                if (irow, icol) not in sndno:\n                    continue\n                sounding = {'STID': col_head.STID,\n                            'STNM': col_head.STNM,\n                            'SLAT': col_head.SLAT,\n                            'SLON': col_head.SLON,\n                            'SELV': col_head.SELV,\n                            'STAT': col_head.STAT,\n                            'COUN': col_head.COUN,\n                            'DATE': row_head.DATE,\n                            'TIME': row_head.TIME,\n                            }\n                for iprt, part in enumerate(self.parts):\n                    pointer = (self.prod_desc.data_block_ptr\n                               + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                               + (icol * self.prod_desc.parts + iprt))\n                    self._buffer.jump_to(self._start, _word_to_position(pointer))\n                    self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                    if not self.data_ptr:\n                        continue\n                    self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                    self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                    data_header = self._buffer.set_mark()\n                    self._buffer.jump_to(data_header,\n                                         _word_to_position(part.header_length + 1))\n                    lendat = self.data_header_length - part.header_length\n\n                    fmt_code = {\n                        DataTypes.real: 'f',\n                        DataTypes.realpack: 'i',\n                        DataTypes.character: 's',\n                    }.get(part.data_type)\n\n                    if fmt_code is None:\n                        raise NotImplementedError(f'No methods for data type {part.data_type}')\n\n                    if fmt_code == 's':\n                        lendat *= BYTES_PER_WORD\n\n                    packed_buffer = (\n                        self._buffer.read_struct(\n                            struct.Struct(f'{self.prefmt}{lendat}{fmt_code}')\n                        )\n                    )\n\n                    parameters = self.parameters[iprt]\n                    nparms = len(parameters['name'])\n\n                    if part.data_type == DataTypes.realpack:\n                        unpacked = self._unpack_real(packed_buffer, parameters, lendat)\n                        for iprm, param in enumerate(parameters['name']):\n                            sounding[param] = unpacked[iprm::nparms]\n                    else:\n                        for iprm, param in enumerate(parameters['name']):\n                            sounding[param] = np.array(\n                                packed_buffer[iprm::nparms], dtype=np.float32\n                            )\n\n                soundings.append(sounding)\n        return soundings\n\n    def _unpack_unmerged(self, sndno):\n        \"\"\"Unpack unmerged sounding data.\"\"\"\n        soundings = []\n        for irow, row_head in enumerate(self.row_headers):\n            for icol, col_head in enumerate(self.column_headers):\n                if (irow, icol) not in sndno:\n                    continue\n                sounding = {'STID': col_head.STID,\n                            'STNM': col_head.STNM,\n                            'SLAT': col_head.SLAT,\n                            'SLON': col_head.SLON,\n                            'SELV': col_head.SELV,\n                            'STAT': col_head.STAT,\n                            'COUN': col_head.COUN,\n                            'DATE': row_head.DATE,\n                            'TIME': row_head.TIME,\n                            }\n                for iprt, part in enumerate(self.parts):\n                    pointer = (self.prod_desc.data_block_ptr\n                               + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                               + (icol * self.prod_desc.parts + iprt))\n                    self._buffer.jump_to(self._start, _word_to_position(pointer))\n                    self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                    if not self.data_ptr:\n                        continue\n                    self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                    self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                    data_header = self._buffer.set_mark()\n                    self._buffer.jump_to(data_header,\n                                         _word_to_position(part.header_length + 1))\n                    lendat = self.data_header_length - part.header_length\n\n                    fmt_code = {\n                        DataTypes.real: 'f',\n                        DataTypes.realpack: 'i',\n                        DataTypes.character: 's',\n                    }.get(part.data_type)\n\n                    if fmt_code is None:\n                        raise NotImplementedError(f'No methods for data type {part.data_type}')\n\n                    if fmt_code == 's':\n                        lendat *= BYTES_PER_WORD\n\n                    packed_buffer = (\n                        self._buffer.read_struct(\n                            struct.Struct(f'{self.prefmt}{lendat}{fmt_code}')\n                        )\n                    )\n\n                    parameters = self.parameters[iprt]\n                    nparms = len(parameters['name'])\n                    sounding[part.name] = {}\n\n                    if part.data_type == DataTypes.realpack:\n                        unpacked = self._unpack_real(packed_buffer, parameters, lendat)\n                        for iprm, param in enumerate(parameters['name']):\n                            sounding[part.name][param] = unpacked[iprm::nparms]\n                    elif part.data_type == DataTypes.character:\n                        for iprm, param in enumerate(parameters['name']):\n                            sounding[part.name][param] = (\n                                self._decode_strip(packed_buffer[iprm])\n                            )\n                    else:\n                        for iprm, param in enumerate(parameters['name']):\n                            sounding[part.name][param] = (\n                                np.array(packed_buffer[iprm::nparms], dtype=np.float32)\n                            )\n\n                soundings.append(self._merge_sounding(sounding))\n        return soundings\n\n    def _merge_significant_temps(self, merged, parts, section, pbot):\n        \"\"\"Process and merge a significant temperature sections.\"\"\"\n        for isigt, press in enumerate(parts[section]['PRES']):\n            press = abs(press)\n            if self.prod_desc.missing_float not in [\n                press,\n                parts[section]['TEMP'][isigt]\n            ] and press != 0:\n                if press > pbot:\n                    continue\n                elif press in merged['PRES']:\n                    ploc = merged['PRES'].index(press)\n                    if merged['TEMP'][ploc] == self.prod_desc.missing_float:\n                        merged['TEMP'][ploc] = parts[section]['TEMP'][isigt]\n                        merged['DWPT'][ploc] = parts[section]['DWPT'][isigt]\n                else:\n                    size = len(merged['PRES'])\n                    loc = size - bisect.bisect_left(merged['PRES'][::-1], press)\n                    merged['PRES'].insert(loc, press)\n                    merged['TEMP'].insert(loc, parts[section]['TEMP'][isigt])\n                    merged['DWPT'].insert(loc, parts[section]['DWPT'][isigt])\n                    merged['DRCT'].insert(loc, self.prod_desc.missing_float)\n                    merged['SPED'].insert(loc, self.prod_desc.missing_float)\n                    merged['HGHT'].insert(loc, self.prod_desc.missing_float)\n            pbot = press\n\n        return pbot\n\n    def _merge_tropopause_data(self, merged, parts, section, pbot):\n        \"\"\"Process and merge tropopause sections.\"\"\"\n        for itrp, press in enumerate(parts[section]['PRES']):\n            press = abs(press)\n            if self.prod_desc.missing_float not in [\n                press,\n                parts[section]['TEMP'][itrp]\n            ] and press != 0:\n                if press > pbot:\n                    continue\n                elif press in merged['PRES']:\n                    ploc = merged['PRES'].index(press)\n                    if merged['TEMP'][ploc] == self.prod_desc.missing_float:\n                        merged['TEMP'][ploc] = parts[section]['TEMP'][itrp]\n                        merged['DWPT'][ploc] = parts[section]['DWPT'][itrp]\n                    if merged['DRCT'][ploc] == self.prod_desc.missing_float:\n                        merged['DRCT'][ploc] = parts[section]['DRCT'][itrp]\n                        merged['SPED'][ploc] = parts[section]['SPED'][itrp]\n                    merged['HGHT'][ploc] = self.prod_desc.missing_float\n                else:\n                    size = len(merged['PRES'])\n                    loc = size - bisect.bisect_left(merged['PRES'][::-1], press)\n                    merged['PRES'].insert(loc, press)\n                    merged['TEMP'].insert(loc, parts[section]['TEMP'][itrp])\n                    merged['DWPT'].insert(loc, parts[section]['DWPT'][itrp])\n                    merged['DRCT'].insert(loc, parts[section]['DRCT'][itrp])\n                    merged['SPED'].insert(loc, parts[section]['SPED'][itrp])\n                    merged['HGHT'].insert(loc, self.prod_desc.missing_float)\n            pbot = press\n\n        return pbot\n\n    def _merge_mandatory_temps(self, merged, parts, section, qcman, bgl, plast):\n        \"\"\"Process and merge mandatory temperature sections.\"\"\"\n        num_levels = len(parts[section]['PRES'])\n        start_level = {\n            'TTAA': 1,\n            'TTCC': 0,\n        }\n        for i in range(start_level[section], num_levels):\n            if (parts[section]['PRES'][i] < plast\n                and self.prod_desc.missing_float not in [\n                    parts[section]['PRES'][i],\n                    parts[section]['TEMP'][i],\n                    parts[section]['HGHT'][i]\n            ]):\n                for pname, pval in parts[section].items():\n                    merged[pname].append(pval[i])\n                plast = merged['PRES'][-1]\n            else:\n                if section == 'TTAA':\n                    if parts[section]['PRES'][i] > merged['PRES'][0]:\n                        bgl += 1\n                    else:\n                        # GEMPAK ignores MAN data with missing TEMP/HGHT and does not\n                        # interpolate for them.\n                        if parts[section]['PRES'][i] != self.prod_desc.missing_float:\n                            qcman.append(parts[section]['PRES'][i])\n\n        return bgl, plast\n\n    def _merge_mandatory_winds(self, merged, parts, section, qcman):\n        \"\"\"Process and merge manadatory wind sections.\"\"\"\n        for iwind, press in enumerate(parts[section]['PRES']):\n            if press in merged['PRES'][1:]:\n                loc = merged['PRES'].index(press)\n                if merged['DRCT'][loc] == self.prod_desc.missing_float:\n                    merged['DRCT'][loc] = parts[section]['DRCT'][iwind]\n                    merged['SPED'][loc] = parts[section]['SPED'][iwind]\n            else:\n                if press not in qcman:\n                    size = len(merged['PRES'])\n                    loc = size - bisect.bisect_left(merged['PRES'][1:][::-1], press)\n                    if loc >= size + 1:\n                        loc = -1\n                    merged['PRES'].insert(loc, press)\n                    merged['TEMP'].insert(loc, self.prod_desc.missing_float)\n                    merged['DWPT'].insert(loc, self.prod_desc.missing_float)\n                    merged['DRCT'].insert(loc, parts[section]['DRCT'][iwind])\n                    merged['SPED'].insert(loc, parts[section]['SPED'][iwind])\n                    merged['HGHT'].insert(loc, self.prod_desc.missing_float)\n\n    def _merge_winds_pressure(self, merged, parts, section, pbot):\n        \"\"\"Process and merge wind sections on pressure surfaces.\"\"\"\n        for ilevel, press in enumerate(parts[section]['PRES']):\n            press = abs(press)\n            if self.prod_desc.missing_float not in [\n                press,\n                parts[section]['DRCT'][ilevel],\n                parts[section]['SPED'][ilevel]\n            ] and press != 0:\n                if press > pbot:\n                    continue\n                elif press in merged['PRES']:\n                    ploc = merged['PRES'].index(press)\n                    if self.prod_desc.missing_float in [\n                        merged['DRCT'][ploc],\n                        merged['SPED'][ploc]\n                    ]:\n                        merged['DRCT'][ploc] = parts[section]['DRCT'][ilevel]\n                        merged['SPED'][ploc] = parts[section]['SPED'][ilevel]\n                else:\n                    size = len(merged['PRES'])\n                    loc = size - bisect.bisect_left(merged['PRES'][::-1], press)\n                    merged['PRES'].insert(loc, press)\n                    merged['DRCT'].insert(loc, parts[section]['DRCT'][ilevel])\n                    merged['SPED'].insert(loc, parts[section]['SPED'][ilevel])\n                    merged['TEMP'].insert(loc, self.prod_desc.missing_float)\n                    merged['DWPT'].insert(loc, self.prod_desc.missing_float)\n                    merged['HGHT'].insert(loc, self.prod_desc.missing_float)\n            pbot = press\n\n        return pbot\n\n    def _merge_winds_height(self, merged, parts, nsgw, nasw, istart):\n        \"\"\"Merge wind sections on height surfaces.\"\"\"\n        size = len(merged['HGHT'])\n        psfc = merged['PRES'][0]\n        zsfc = merged['HGHT'][0]\n\n        if self.prod_desc.missing_float not in [\n            psfc,\n            zsfc\n        ] and size >= 2:\n            more = True\n            zold = merged['HGHT'][0]\n            znxt = merged['HGHT'][1]\n            ilev = 1\n        elif size >= 3:\n            more = True\n            zold = merged['HGHT'][1]\n            znxt = merged['HGHT'][2]\n            ilev = 2\n        else:\n            zold = self.prod_desc.missing_float\n            znxt = self.prod_desc.missing_float\n\n        if self.prod_desc.missing_float in [\n            zold,\n            znxt\n        ]:\n            more = False\n\n        if istart <= nsgw:\n            above = False\n            i = istart\n            iend = nsgw\n        else:\n            above = True\n            i = 0\n            iend = nasw\n\n        while more and i < iend:\n            if not above:\n                hght = parts['PPBB']['HGHT'][i]\n                drct = parts['PPBB']['DRCT'][i]\n                sped = parts['PPBB']['SPED'][i]\n            else:\n                hght = parts['PPDD']['HGHT'][i]\n                drct = parts['PPDD']['DRCT'][i]\n                sped = parts['PPDD']['SPED'][i]\n            skip = False\n\n            if self.prod_desc.missing_float in [\n                hght,\n                drct,\n                sped\n            ] or hght <= zold:\n                skip = True\n            elif abs(zold - hght) < 1:\n                skip = True\n                if self.prod_desc.missing_float in [\n                    merged['DRCT'][ilev - 1],\n                    merged['SPED'][ilev - 1]\n                ]:\n                    merged['DRCT'][ilev - 1] = drct\n                    merged['SPED'][ilev - 1] = sped\n            elif hght >= znxt:\n                while more and hght > znxt:\n                    zold = znxt\n                    ilev += 1\n                    if ilev >= size:\n                        more = False\n                    else:\n                        znxt = merged['HGHT'][ilev]\n                        if znxt == self.prod_desc.missing_float:\n                            more = False\n\n            if more and not skip:\n                if abs(znxt - hght) < 1:\n                    if self.prod_desc.missing_float in [\n                        merged['DRCT'][ilev - 1],\n                        merged['SPED'][ilev - 1]\n                    ]:\n                        merged['DRCT'][ilev] = drct\n                        merged['SPED'][ilev] = sped\n                else:\n                    loc = bisect.bisect_left(merged['HGHT'], hght)\n                    merged['HGHT'].insert(loc, hght)\n                    merged['DRCT'].insert(loc, drct)\n                    merged['SPED'].insert(loc, sped)\n                    merged['PRES'].insert(loc, self.prod_desc.missing_float)\n                    merged['TEMP'].insert(loc, self.prod_desc.missing_float)\n                    merged['DWPT'].insert(loc, self.prod_desc.missing_float)\n                    size += 1\n                    ilev += 1\n                    zold = hght\n\n            if not above and i == nsgw - 1:\n                above = True\n                i = 0\n                iend = nasw\n            else:\n                i += 1\n\n    def _merge_sounding(self, parts):\n        \"\"\"Merge unmerged sounding data.\"\"\"\n        merged = {'STID': parts['STID'],\n                  'STNM': parts['STNM'],\n                  'SLAT': parts['SLAT'],\n                  'SLON': parts['SLON'],\n                  'SELV': parts['SELV'],\n                  'STAT': parts['STAT'],\n                  'COUN': parts['COUN'],\n                  'DATE': parts['DATE'],\n                  'TIME': parts['TIME'],\n                  'PRES': [],\n                  'HGHT': [],\n                  'TEMP': [],\n                  'DWPT': [],\n                  'DRCT': [],\n                  'SPED': [],\n                  }\n\n        # Number of parameter levels\n        num_man_levels = len(parts['TTAA']['PRES']) if 'TTAA' in parts else 0\n        num_man_wind_levels = len(parts['PPAA']['PRES']) if 'PPAA' in parts else 0\n        num_trop_levels = len(parts['TRPA']['PRES']) if 'TRPA' in parts else 0\n        num_max_wind_levels = len(parts['MXWA']['PRES']) if 'MXWA' in parts else 0\n        num_sigt_levels = len(parts['TTBB']['PRES']) if 'TTBB' in parts else 0\n        num_sigw_levels = len(parts['PPBB']['SPED']) if 'PPBB' in parts else 0\n        num_above_man_levels = len(parts['TTCC']['PRES']) if 'TTCC' in parts else 0\n        num_above_trop_levels = len(parts['TRPC']['PRES']) if 'TRPC' in parts else 0\n        num_above_max_wind_levels = len(parts['MXWC']['SPED']) if 'MXWC' in parts else 0\n        num_above_sigt_levels = len(parts['TTDD']['PRES']) if 'TTDD' in parts else 0\n        num_above_sigw_levels = len(parts['PPDD']['SPED']) if 'PPDD' in parts else 0\n        num_above_man_wind_levels = len(parts['PPCC']['SPED']) if 'PPCC' in parts else 0\n\n        total_data = (num_man_levels\n                      + num_man_wind_levels\n                      + num_trop_levels\n                      + num_max_wind_levels\n                      + num_sigt_levels\n                      + num_sigw_levels\n                      + num_above_man_levels\n                      + num_above_trop_levels\n                      + num_above_max_wind_levels\n                      + num_above_sigt_levels\n                      + num_above_sigw_levels\n                      + num_above_man_wind_levels\n                      )\n        if total_data == 0:\n            return None\n\n        # Check SIG wind vertical coordinate\n        # For some reason, the pressure data can get put into the\n        # height array. Perhaps this is just a artifact of Python,\n        # as GEMPAK itself just uses array indices without any\n        # names involved. Since the first valid pressure of the\n        # array will be negative in the case of pressure coordinates,\n        # we can check for it and place data in the appropriate array.\n        ppbb_is_z = True\n        if num_sigw_levels:\n            if 'PRES' in parts['PPBB']:\n                ppbb_is_z = False\n            else:\n                for z in parts['PPBB']['HGHT']:\n                    if z != self.prod_desc.missing_float and z < 0:\n                        ppbb_is_z = False\n                        parts['PPBB']['PRES'] = parts['PPBB']['HGHT']\n                        break\n\n        ppdd_is_z = True\n        if num_above_sigw_levels:\n            if 'PRES' in parts['PPDD']:\n                ppdd_is_z = False\n            else:\n                for z in parts['PPDD']['HGHT']:\n                    if z != self.prod_desc.missing_float and z < 0:\n                        ppdd_is_z = False\n                        parts['PPDD']['PRES'] = parts['PPDD']['HGHT']\n                        break\n\n        # Process surface data\n        if num_man_levels < 1:\n            merged['PRES'].append(self.prod_desc.missing_float)\n            merged['HGHT'].append(self.prod_desc.missing_float)\n            merged['TEMP'].append(self.prod_desc.missing_float)\n            merged['DWPT'].append(self.prod_desc.missing_float)\n            merged['DRCT'].append(self.prod_desc.missing_float)\n            merged['SPED'].append(self.prod_desc.missing_float)\n        else:\n            merged['PRES'].append(parts['TTAA']['PRES'][0])\n            merged['HGHT'].append(parts['TTAA']['HGHT'][0])\n            merged['TEMP'].append(parts['TTAA']['TEMP'][0])\n            merged['DWPT'].append(parts['TTAA']['DWPT'][0])\n            merged['DRCT'].append(parts['TTAA']['DRCT'][0])\n            merged['SPED'].append(parts['TTAA']['SPED'][0])\n\n        merged['HGHT'][0] = merged['SELV']\n\n        first_man_p = self.prod_desc.missing_float\n        if num_man_levels >= 1:\n            for mp, mt, mz in zip(parts['TTAA']['PRES'],\n                                  parts['TTAA']['TEMP'],\n                                  parts['TTAA']['HGHT']):\n                if self.prod_desc.missing_float not in [\n                    mp,\n                    mt,\n                    mz\n                ]:\n                    first_man_p = mp\n                    break\n\n        surface_p = merged['PRES'][0]\n        if surface_p > 1060:\n            surface_p = self.prod_desc.missing_float\n\n        if (surface_p == self.prod_desc.missing_float\n           or (surface_p < first_man_p\n               and surface_p != self.prod_desc.missing_float)):\n            merged['PRES'][0] = self.prod_desc.missing_float\n            merged['HGHT'][0] = self.prod_desc.missing_float\n            merged['TEMP'][0] = self.prod_desc.missing_float\n            merged['DWPT'][0] = self.prod_desc.missing_float\n            merged['DRCT'][0] = self.prod_desc.missing_float\n            merged['SPED'][0] = self.prod_desc.missing_float\n\n        if (num_sigt_levels >= 1\n           and self.prod_desc.missing_float not in [\n               parts['TTBB']['PRES'][0],\n               parts['TTBB']['TEMP'][0]\n           ]):\n            first_man_p = merged['PRES'][0]\n            first_sig_p = parts['TTBB']['PRES'][0]\n            if (first_man_p == self.prod_desc.missing_float\n               or np.isclose(first_man_p, first_sig_p)):\n                merged['PRES'][0] = parts['TTBB']['PRES'][0]\n                merged['DWPT'][0] = parts['TTBB']['DWPT'][0]\n                merged['TEMP'][0] = parts['TTBB']['TEMP'][0]\n\n        if num_sigw_levels >= 1:\n            if ppbb_is_z:\n                if (parts['PPBB']['HGHT'][0] == 0\n                   and parts['PPBB']['DRCT'][0] != self.prod_desc.missing_float):\n                    merged['DRCT'][0] = parts['PPBB']['DRCT'][0]\n                    merged['SPED'][0] = parts['PPBB']['SPED'][0]\n            else:\n                if self.prod_desc.missing_float not in [\n                    parts['PPBB']['PRES'][0],\n                    parts['PPBB']['DRCT'][0]\n                ]:\n                    first_man_p = merged['PRES'][0]\n                    first_sig_p = abs(parts['PPBB']['PRES'][0])\n                    if (first_man_p == self.prod_desc.missing_float\n                       or np.isclose(first_man_p, first_sig_p)):\n                        merged['PRES'][0] = abs(parts['PPBB']['PRES'][0])\n                        merged['DRCT'][0] = parts['PPBB']['DRCT'][0]\n                        merged['SPED'][0] = parts['PPBB']['SPED'][0]\n\n        # Merge MAN temperature\n        bgl = 0\n        qcman = []\n        if num_man_levels >= 2 or num_above_man_levels >= 1:\n            if merged['PRES'][0] == self.prod_desc.missing_float:\n                plast = 2000\n            else:\n                plast = merged['PRES'][0]\n\n        if num_man_levels >= 2:\n            bgl, plast = self._merge_mandatory_temps(merged, parts, 'TTAA',\n                                                     qcman, bgl, plast)\n\n        if num_above_man_levels >= 1:\n            bgl, plast = self._merge_mandatory_temps(merged, parts, 'TTCC',\n                                                     qcman, bgl, plast)\n\n        # Merge MAN wind\n        if num_man_wind_levels >= 1 and num_man_levels >= 1 and len(merged['PRES']) >= 2:\n            self._merge_mandatory_winds(merged, parts, 'PPAA', qcman)\n\n        if num_above_man_wind_levels >= 1 and num_man_levels >= 1 and len(merged['PRES']) >= 2:\n            self._merge_mandatory_winds(merged, parts, 'PPCC', qcman)\n\n        # Merge TROP\n        if num_trop_levels >= 1 or num_above_trop_levels >= 1:\n            if merged['PRES'][0] != self.prod_desc.missing_float:\n                pbot = merged['PRES'][0]\n            elif len(merged['PRES']) > 1:\n                pbot = merged['PRES'][1]\n                if pbot < parts['TRPA']['PRES'][1]:\n                    pbot = 1050\n            else:\n                pbot = 1050\n\n        if num_trop_levels >= 1:\n            pbot = self._merge_tropopause_data(merged, parts, 'TRPA', pbot)\n\n        if num_above_trop_levels >= 1:\n            pbot = self._merge_tropopause_data(merged, parts, 'TRPC', pbot)\n\n        # Merge SIG temperature\n        if num_sigt_levels >= 1 or num_above_sigt_levels >= 1:\n            if merged['PRES'][0] != self.prod_desc.missing_float:\n                pbot = merged['PRES'][0]\n            elif len(merged['PRES']) > 1:\n                pbot = merged['PRES'][1]\n                if pbot < parts['TTBB']['PRES'][1]:\n                    pbot = 1050\n            else:\n                pbot = 1050\n\n        if num_sigt_levels >= 1:\n            pbot = self._merge_significant_temps(merged, parts, 'TTBB', pbot)\n\n        if num_above_sigt_levels >= 1:\n            pbot = self._merge_significant_temps(merged, parts, 'TTDD', pbot)\n\n        # Interpolate heights\n        _interp_moist_height(merged, self.prod_desc.missing_float)\n\n        # Merge SIG winds on pressure surfaces\n        if not ppbb_is_z or not ppdd_is_z:\n            if num_sigw_levels >= 1 or num_above_sigw_levels >= 1:\n                if merged['PRES'][0] != self.prod_desc.missing_float:\n                    pbot = merged['PRES'][0]\n                elif len(merged['PRES']) > 1:\n                    pbot = merged['PRES'][1]\n                else:\n                    pbot = 0\n\n            if num_sigw_levels >= 1 and not ppbb_is_z:\n                pbot = self._merge_winds_pressure(merged, parts, 'PPBB', pbot)\n\n            if num_above_sigw_levels >= 1 and not ppdd_is_z:\n                pbot = self._merge_winds_pressure(merged, parts, 'PPDD', pbot)\n\n        # Merge max winds on pressure surfaces\n        if num_max_wind_levels >= 1 or num_above_max_wind_levels >= 1:\n            if merged['PRES'][0] != self.prod_desc.missing_float:\n                pbot = merged['PRES'][0]\n            elif len(merged['PRES']) > 1:\n                pbot = merged['PRES'][1]\n            else:\n                pbot = 0\n\n        if num_max_wind_levels >= 1:\n            pbot = self._merge_winds_pressure(merged, parts, 'MXWA', pbot)\n\n        if num_above_max_wind_levels >= 1:\n            _ = self._merge_winds_pressure(merged, parts, 'MXWC', pbot)\n\n        # Interpolate height for SIG/MAX winds\n        _interp_logp_height(merged, self.prod_desc.missing_float)\n\n        # Merge SIG winds on height surfaces\n        if ppbb_is_z or ppdd_is_z:\n            nsgw = num_sigw_levels if ppbb_is_z else 0\n            nasw = num_above_sigw_levels if ppdd_is_z else 0\n            if (nsgw >= 1 and (parts['PPBB']['HGHT'][0] == 0\n               or parts['PPBB']['HGHT'][0] == merged['HGHT'][0])):\n                istart = 1\n            else:\n                istart = 0\n\n            self._merge_winds_height(merged, parts, nsgw, nasw, istart)\n\n            # Interpolate missing pressure with height\n            _interp_logp_pressure(merged, self.prod_desc.missing_float)\n\n        # Interpolate missing data\n        _interp_logp_data(merged, self.prod_desc.missing_float)\n\n        # Add below ground MAN data\n        if merged['PRES'][0] != self.prod_desc.missing_float and bgl > 0:\n            size = len(merged['PRES'])\n            for ibgl in range(1, num_man_levels):\n                press = parts['TTAA']['PRES'][ibgl]\n                if press > merged['PRES'][0]:\n                    loc = size - bisect.bisect_left(merged['PRES'][1:][::-1], press)\n                    merged['PRES'].insert(loc, press)\n                    merged['TEMP'].insert(loc, parts['TTAA']['TEMP'][ibgl])\n                    merged['DWPT'].insert(loc, parts['TTAA']['DWPT'][ibgl])\n                    merged['DRCT'].insert(loc, parts['TTAA']['DRCT'][ibgl])\n                    merged['SPED'].insert(loc, parts['TTAA']['SPED'][ibgl])\n                    merged['HGHT'].insert(loc, parts['TTAA']['HGHT'][ibgl])\n                    size += 1\n\n        # Add text data, if it is included\n        if 'TXTA' in parts:\n            merged['TXTA'] = parts['TXTA']['TEXT']\n        if 'TXTB' in parts:\n            merged['TXTB'] = parts['TXTB']['TEXT']\n        if 'TXTC' in parts:\n            merged['TXTC'] = parts['TXTC']['TEXT']\n        if 'TXPB' in parts:\n            merged['TXPB'] = parts['TXPB']['TEXT']\n\n        return merged\n\n    def snxarray(self, station_id=None, station_number=None,\n                 date_time=None, state=None, country=None):\n        \"\"\"Select soundings and output as list of xarray Datasets.\n\n        Subset the data by parameter values. The default is to not\n        subset and return the entire dataset.\n\n        Parameters\n        ----------\n        station_id : str or Sequence[str]\n            Station ID of sounding site.\n\n        station_number : int or Sequence[int]\n            Station number of sounding site.\n\n        date_time : `~datetime.datetime` or Sequence[datetime]\n            Valid datetime of the grid. Alternatively can be\n            a string with the format YYYYmmddHHMM.\n\n        state : str or Sequence[str]\n            State where sounding site is located.\n\n        country : str or Sequence[str]\n            Country where sounding site is located.\n\n        Returns\n        -------\n        list[xarray.Dataset]\n            List of `xarray.Dataset` objects for each sounding.\n        \"\"\"\n        if station_id is not None:\n            if (not isinstance(station_id, Iterable)\n               or isinstance(station_id, str)):\n                station_id = [station_id]\n            station_id = [c.upper() for c in station_id]\n\n        if station_number is not None:\n            if not isinstance(station_number, Iterable):\n                station_number = [station_number]\n            station_number = [int(sn) for sn in station_number]\n\n        if date_time is not None:\n            if (not isinstance(date_time, Iterable)\n               or isinstance(date_time, str)):\n                date_time = [date_time]\n            for i, dt in enumerate(date_time):\n                if isinstance(dt, str):\n                    date_time[i] = datetime.strptime(dt, '%Y%m%d%H%M')\n\n        if (state is not None\n           and (not isinstance(state, Iterable)\n                or isinstance(state, str))):\n            state = [state]\n            state = [s.upper() for s in state]\n\n        if (country is not None\n           and (not isinstance(country, Iterable)\n                or isinstance(country, str))):\n            country = [country]\n            country = [c.upper() for c in country]\n\n        # Figure out which columns to extract from the file\n        matched = self._sninfo.copy()\n\n        if station_id is not None:\n            matched = filter(\n                lambda snd: snd if snd.ID in station_id else False,\n                matched\n            )\n\n        if station_number is not None:\n            matched = filter(\n                lambda snd: snd if snd.NUMBER in station_number else False,\n                matched\n            )\n\n        if date_time is not None:\n            matched = filter(\n                lambda snd: snd if snd.DATTIM in date_time else False,\n                matched\n            )\n\n        if state is not None:\n            matched = filter(\n                lambda snd: snd if snd.STATE in state else False,\n                matched\n            )\n\n        if country is not None:\n            matched = filter(\n                lambda snd: snd if snd.COUNTRY in country else False,\n                matched\n            )\n\n        matched = list(matched)\n\n        if len(matched) < 1:\n            raise KeyError('No stations were matched with given parameters.')\n\n        sndno = [(s.DTNO, s.SNDNO) for s in matched]\n\n        if self.merged:\n            data = self._unpack_merged(sndno)\n        else:\n            data = self._unpack_unmerged(sndno)\n\n        soundings = []\n        for snd in data:\n            if snd is None or 'PRES' not in snd:\n                continue\n            station_pressure = snd['PRES'][0]\n            wmo_text = {}\n            attrs = {\n                'station_id': snd.pop('STID'),\n                'station_number': snd.pop('STNM'),\n                'lat': snd.pop('SLAT'),\n                'lon': snd.pop('SLON'),\n                'elevation': snd.pop('SELV'),\n                'station_pressure': station_pressure,\n                'state': snd.pop('STAT'),\n                'country': snd.pop('COUN'),\n            }\n\n            if 'TXTA' in snd:\n                wmo_text['txta'] = snd.pop('TXTA')\n            if 'TXTB' in snd:\n                wmo_text['txtb'] = snd.pop('TXTB')\n            if 'TXTC' in snd:\n                wmo_text['txtc'] = snd.pop('TXTC')\n            if 'TXPB' in snd:\n                wmo_text['txpb'] = snd.pop('TXPB')\n            if wmo_text:\n                attrs['WMO_CODES'] = wmo_text\n\n            dt = datetime.combine(snd.pop('DATE'), snd.pop('TIME'))\n            press = np.array(snd.pop('PRES'))\n\n            var = {}\n            for param, values in snd.items():\n                values = np.array(values)[np.newaxis, ...]\n                maskval = np.ma.array(values, mask=values == self.prod_desc.missing_float,\n                                      dtype=np.float32)\n                var[param.lower()] = (['time', 'pressure'], maskval)\n\n            xrds = xr.Dataset(var,\n                              coords={'time': np.atleast_1d(dt), 'pressure': press},\n                              attrs=attrs)\n\n            # Sort to fix GEMPAK surface data at first level\n            xrds = xrds.sortby('pressure', ascending=False)\n\n            soundings.append(xrds)\n        return soundings",
  "class GempakSurface(GempakFile):\n    \"\"\"Subclass of GempakFile specific to GEMPAK surface data.\"\"\"\n\n    def __init__(self, file, *args, **kwargs):\n        super().__init__(file)\n\n        # Row Headers\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.row_headers_ptr))\n        self.row_headers = []\n        row_headers_info = self._key_types(self.row_keys)\n        row_headers_info.extend([(None, None)])\n        row_headers_fmt = NamedStruct(row_headers_info, self.prefmt, 'RowHeaders')\n        for _ in range(1, self.prod_desc.rows + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.row_headers.append(self._buffer.read_struct(row_headers_fmt))\n\n        # Column Headers\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.column_headers_ptr))\n        self.column_headers = []\n        column_headers_info = self._key_types(self.column_keys)\n        column_headers_info.extend([(None, None)])\n        column_headers_fmt = NamedStruct(column_headers_info, self.prefmt, 'ColumnHeaders')\n        for _ in range(1, self.prod_desc.columns + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.column_headers.append(self._buffer.read_struct(column_headers_fmt))\n\n        self._get_surface_type()\n\n        self._sfinfo = []\n        if self.surface_type == 'standard':\n            for irow, row_head in enumerate(self.row_headers):\n                for icol, col_head in enumerate(self.column_headers):\n                    for iprt in range(len(self.parts)):\n                        pointer = (self.prod_desc.data_block_ptr\n                                   + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                                   + (icol * self.prod_desc.parts + iprt))\n\n                        self._buffer.jump_to(self._start, _word_to_position(pointer))\n                        data_ptr = self._buffer.read_int(4, self.endian, False)\n\n                        if data_ptr:\n                            self._sfinfo.append(\n                                Surface(\n                                    irow,\n                                    icol,\n                                    datetime.combine(row_head.DATE, row_head.TIME),\n                                    col_head.STID + col_head.STD2,\n                                    col_head.STNM,\n                                    col_head.SLAT,\n                                    col_head.SLON,\n                                    col_head.SELV,\n                                    col_head.STAT,\n                                    col_head.COUN,\n                                )\n                            )\n        elif self.surface_type == 'ship':\n            irow = 0\n            for icol, col_head in enumerate(self.column_headers):\n                for iprt in range(len(self.parts)):\n                    pointer = (self.prod_desc.data_block_ptr\n                               + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                               + (icol * self.prod_desc.parts + iprt))\n\n                    self._buffer.jump_to(self._start, _word_to_position(pointer))\n                    data_ptr = self._buffer.read_int(4, self.endian, False)\n\n                    if data_ptr:\n                        self._sfinfo.append(\n                            Surface(\n                                irow,\n                                icol,\n                                datetime.combine(col_head.DATE, col_head.TIME),\n                                col_head.STID + col_head.STD2,\n                                col_head.STNM,\n                                col_head.SLAT,\n                                col_head.SLON,\n                                col_head.SELV,\n                                col_head.STAT,\n                                col_head.COUN,\n                            )\n                        )\n        elif self.surface_type == 'climate':\n            for icol, col_head in enumerate(self.column_headers):\n                for irow, row_head in enumerate(self.row_headers):\n                    for iprt in range(len(self.parts)):\n                        pointer = (self.prod_desc.data_block_ptr\n                                   + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                                   + (icol * self.prod_desc.parts + iprt))\n\n                        self._buffer.jump_to(self._start, _word_to_position(pointer))\n                        data_ptr = self._buffer.read_int(4, self.endian, False)\n\n                        if data_ptr:\n                            self._sfinfo.append(\n                                Surface(\n                                    irow,\n                                    icol,\n                                    datetime.combine(col_head.DATE, col_head.TIME),\n                                    row_head.STID + row_head.STD2,\n                                    row_head.STNM,\n                                    row_head.SLAT,\n                                    row_head.SLON,\n                                    row_head.SELV,\n                                    row_head.STAT,\n                                    row_head.COUN,\n                                )\n                            )\n        else:\n            raise TypeError(f'Unknown surface type {self.surface_type}')\n\n    def sfinfo(self):\n        \"\"\"Return station information.\"\"\"\n        return self._sfinfo\n\n    def _get_surface_type(self):\n        \"\"\"Determine type of surface file.\"\"\"\n        if len(self.row_headers) == 1:\n            self.surface_type = 'ship'\n        elif 'DATE' in self.row_keys:\n            self.surface_type = 'standard'\n        elif 'DATE' in self.column_keys:\n            self.surface_type = 'climate'\n        else:\n            raise TypeError('Unknown surface data type')\n\n    def _key_types(self, keys):\n        \"\"\"Determine header information from a set of keys.\"\"\"\n        return [(key, '4s', self._decode_strip) if key == 'STID'\n                else (key, 'i') if key == 'STNM'\n                else (key, 'i', lambda x: x / 100) if key == 'SLAT'\n                else (key, 'i', lambda x: x / 100) if key == 'SLON'\n                else (key, 'i') if key == 'SELV'\n                else (key, '4s', self._decode_strip) if key == 'STAT'\n                else (key, '4s', self._decode_strip) if key == 'COUN'\n                else (key, '4s', self._decode_strip) if key == 'STD2'\n                else (key, 'i', self._make_date) if key == 'DATE'\n                else (key, 'i', self._make_time) if key == 'TIME'\n                else (key, 'i')\n                for key in keys]\n\n    def _unpack_climate(self, sfcno):\n        \"\"\"Unpack a climate surface data file.\"\"\"\n        stations = []\n        for icol, col_head in enumerate(self.column_headers):\n            for irow, row_head in enumerate(self.row_headers):\n                if (irow, icol) not in sfcno:\n                    continue\n                station = {'STID': row_head.STID,\n                           'STNM': row_head.STNM,\n                           'SLAT': row_head.SLAT,\n                           'SLON': row_head.SLON,\n                           'SELV': row_head.SELV,\n                           'STAT': row_head.STAT,\n                           'COUN': row_head.COUN,\n                           'STD2': row_head.STD2,\n                           'SPRI': row_head.SPRI,\n                           'DATE': col_head.DATE,\n                           'TIME': col_head.TIME,\n                           }\n                for iprt, part in enumerate(self.parts):\n                    pointer = (self.prod_desc.data_block_ptr\n                               + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                               + (icol * self.prod_desc.parts + iprt))\n                    self._buffer.jump_to(self._start, _word_to_position(pointer))\n                    self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                    if not self.data_ptr:\n                        continue\n                    self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                    self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                    data_header = self._buffer.set_mark()\n                    self._buffer.jump_to(data_header,\n                                         _word_to_position(part.header_length + 1))\n                    lendat = self.data_header_length - part.header_length\n\n                    fmt_code = {\n                        DataTypes.real: 'f',\n                        DataTypes.realpack: 'i',\n                        DataTypes.character: 's',\n                    }.get(part.data_type)\n\n                    if fmt_code is None:\n                        raise NotImplementedError(f'No methods for data type {part.data_type}')\n\n                    if fmt_code == 's':\n                        lendat *= BYTES_PER_WORD\n\n                    packed_buffer = (\n                        self._buffer.read_struct(\n                            struct.Struct(f'{self.prefmt}{lendat}{fmt_code}')\n                        )\n                    )\n\n                    parameters = self.parameters[iprt]\n\n                    if part.data_type == DataTypes.realpack:\n                        unpacked = self._unpack_real(packed_buffer, parameters, lendat)\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = unpacked[iprm]\n                    elif part.data_type == DataTypes.character:\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = self._decode_strip(packed_buffer[iprm])\n                    else:\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = np.array(\n                                packed_buffer[iprm], dtype=np.float32\n                            )\n\n                stations.append(station)\n        return stations\n\n    def _unpack_ship(self, sfcno):\n        \"\"\"Unpack ship (moving observation) surface data file.\"\"\"\n        stations = []\n        irow = 0\n        for icol, col_head in enumerate(self.column_headers):\n            if (irow, icol) not in sfcno:\n                continue\n            station = {'STID': col_head.STID,\n                       'STNM': col_head.STNM,\n                       'SLAT': col_head.SLAT,\n                       'SLON': col_head.SLON,\n                       'SELV': col_head.SELV,\n                       'STAT': col_head.STAT,\n                       'COUN': col_head.COUN,\n                       'STD2': col_head.STD2,\n                       'SPRI': col_head.SPRI,\n                       'DATE': col_head.DATE,\n                       'TIME': col_head.TIME,\n                       }\n            for iprt, part in enumerate(self.parts):\n                pointer = (self.prod_desc.data_block_ptr\n                           + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                           + (icol * self.prod_desc.parts + iprt))\n                self._buffer.jump_to(self._start, _word_to_position(pointer))\n                self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                if not self.data_ptr:\n                    continue\n                self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                data_header = self._buffer.set_mark()\n                self._buffer.jump_to(data_header,\n                                     _word_to_position(part.header_length + 1))\n                lendat = self.data_header_length - part.header_length\n\n                fmt_code = {\n                    DataTypes.real: 'f',\n                    DataTypes.realpack: 'i',\n                    DataTypes.character: 's',\n                }.get(part.data_type)\n\n                if fmt_code is None:\n                    raise NotImplementedError(f'No methods for data type {part.data_type}')\n\n                if fmt_code == 's':\n                    lendat *= BYTES_PER_WORD\n\n                packed_buffer = (\n                    self._buffer.read_struct(\n                        struct.Struct(f'{self.prefmt}{lendat}{fmt_code}')\n                    )\n                )\n\n                parameters = self.parameters[iprt]\n\n                if part.data_type == DataTypes.realpack:\n                    unpacked = self._unpack_real(packed_buffer, parameters, lendat)\n                    for iprm, param in enumerate(parameters['name']):\n                        station[param] = unpacked[iprm]\n                elif part.data_type == DataTypes.character:\n                    for iprm, param in enumerate(parameters['name']):\n                        station[param] = self._decode_strip(packed_buffer[iprm])\n                else:\n                    for iprm, param in enumerate(parameters['name']):\n                        station[param] = np.array(\n                            packed_buffer[iprm], dtype=np.float32\n                        )\n\n            stations.append(station)\n        return stations\n\n    def _unpack_standard(self, sfcno):\n        \"\"\"Unpack a standard surface data file.\"\"\"\n        stations = []\n        for irow, row_head in enumerate(self.row_headers):\n            for icol, col_head in enumerate(self.column_headers):\n                if (irow, icol) not in sfcno:\n                    continue\n                station = {'STID': col_head.STID,\n                           'STNM': col_head.STNM,\n                           'SLAT': col_head.SLAT,\n                           'SLON': col_head.SLON,\n                           'SELV': col_head.SELV,\n                           'STAT': col_head.STAT,\n                           'COUN': col_head.COUN,\n                           'STD2': col_head.STD2,\n                           'SPRI': col_head.SPRI,\n                           'DATE': row_head.DATE,\n                           'TIME': row_head.TIME,\n                           }\n                for iprt, part in enumerate(self.parts):\n                    pointer = (self.prod_desc.data_block_ptr\n                               + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                               + (icol * self.prod_desc.parts + iprt))\n                    self._buffer.jump_to(self._start, _word_to_position(pointer))\n                    self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                    if not self.data_ptr:\n                        continue\n                    self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                    self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                    data_header = self._buffer.set_mark()\n                    self._buffer.jump_to(data_header,\n                                         _word_to_position(part.header_length + 1))\n                    lendat = self.data_header_length - part.header_length\n\n                    fmt_code = {\n                        DataTypes.real: 'f',\n                        DataTypes.realpack: 'i',\n                        DataTypes.character: 's',\n                    }.get(part.data_type)\n\n                    if fmt_code is None:\n                        raise NotImplementedError(f'No methods for data type {part.data_type}')\n\n                    if fmt_code == 's':\n                        lendat *= BYTES_PER_WORD\n\n                    packed_buffer = (\n                        self._buffer.read_struct(\n                            struct.Struct(f'{self.prefmt}{lendat}{fmt_code}')\n                        )\n                    )\n\n                    parameters = self.parameters[iprt]\n\n                    if part.data_type == DataTypes.realpack:\n                        unpacked = self._unpack_real(packed_buffer, parameters, lendat)\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = unpacked[iprm]\n                    elif part.data_type == DataTypes.character:\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = self._decode_strip(packed_buffer[iprm])\n                    else:\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = packed_buffer[iprm]\n\n                stations.append(station)\n        return stations\n\n    @staticmethod\n    def _decode_special_observation(station, missing=-9999):\n        \"\"\"Decode raw special obsrvation text.\"\"\"\n        text = station['SPCL']\n        dt = datetime.combine(station['DATE'], station['TIME'])\n        parsed = parse_metar(text, dt.year, dt.month)\n\n        station['TIME'] = parsed.date_time.time()\n        if math.nan in [parsed.altimeter, parsed.elevation, parsed.temperature]:\n            station['PMSL'] = missing\n        else:\n            station['PMSL'] = altimeter_to_sea_level_pressure(\n                units.Quantity(parsed.altimeter, 'inHg'),\n                units.Quantity(parsed.elevation, 'm'),\n                units.Quantity(parsed.temperature, 'degC')\n            ).to('hPa').m\n        station['ALTI'] = _check_nan(parsed.altimeter, missing)\n        station['TMPC'] = _check_nan(parsed.temperature, missing)\n        station['DWPC'] = _check_nan(parsed.dewpoint, missing)\n        station['SKNT'] = _check_nan(parsed.wind_speed, missing)\n        station['DRCT'] = _check_nan(float(parsed.wind_direction), missing)\n        station['GUST'] = _check_nan(parsed.wind_gust, missing)\n        station['WNUM'] = float(_wx_to_wnum(parsed.current_wx1, parsed.current_wx2,\n                                            parsed.current_wx3, missing))\n        station['CHC1'] = _convert_clouds(parsed.skyc1, parsed.skylev1, missing)\n        station['CHC2'] = _convert_clouds(parsed.skyc2, parsed.skylev2, missing)\n        station['CHC3'] = _convert_clouds(parsed.skyc3, parsed.skylev3, missing)\n        if math.isnan(parsed.visibility):\n            station['VSBY'] = missing\n        else:\n            station['VSBY'] = float(round(parsed.visibility / 1609.344))\n\n        return station\n\n    def nearest_time(self, date_time, station_id=None, station_number=None,\n                     include_special=False):\n        \"\"\"Get nearest observation to given time for selected stations.\n\n        Parameters\n        ----------\n        date_time : `~datetime.datetime` or Sequence[datetime]\n            Valid/observed datetime of the surface station. Alternatively\n            object or a string with the format YYYYmmddHHMM.\n\n        station_id : str or Sequence[str]\n            Station ID of the surface station(s).\n\n        station_number : int or Sequence[int]\n            Station number of the surface station.\n\n        include_special : bool\n            If True, parse special observations that are stored\n            as raw METAR text. Default is False.\n\n        Returns\n        -------\n        list\n            List of dicts/JSONs for each surface station.\n\n        Notes\n        -----\n        One of either station_id or station_number must be used. If both\n        are present, station_id will take precedence.\n        \"\"\"\n        if isinstance(date_time, str):\n            date_time = datetime.strptime(date_time, '%Y%m%d%H%M')\n\n        if station_id is None and station_number is None:\n            raise ValueError('Must have either station_id or station_number')\n\n        if station_id is not None and station_number is not None:\n            station_number = None\n\n        if (station_id is not None\n           and (not isinstance(station_id, Iterable)\n                or isinstance(station_id, str))):\n            station_id = [station_id]\n            station_id = [c.upper() for c in station_id]\n\n        if station_number is not None and not isinstance(station_number, Iterable):\n            station_number = [station_number]\n            station_number = [int(sn) for sn in station_number]\n\n        time_matched = []\n        if station_id:\n            for stn in station_id:\n                matched = self.sfjson(station_id=stn, include_special=include_special)\n\n                nearest = min(\n                    matched,\n                    key=lambda d: abs(d['properties']['date_time'] - date_time)\n                )\n\n                time_matched.append(nearest)\n\n        if station_number:\n            for stn in station_id:\n                matched = self.sfjson(station_number=stn, include_special=include_special)\n\n                nearest = min(\n                    matched,\n                    key=lambda d: abs(d['properties']['date_time'] - date_time)\n                )\n\n                time_matched.append(nearest)\n\n        return time_matched\n\n    def sfjson(self, station_id=None, station_number=None,\n               date_time=None, state=None, country=None,\n               include_special=False):\n        \"\"\"Select surface stations and output as list of JSON objects.\n\n        Subset the data by parameter values. The default is to not\n        subset and return the entire dataset.\n\n        Parameters\n        ----------\n        station_id : str or Sequence[str]\n            Station ID of the surface station.\n\n        station_number : int or Sequence[int]\n            Station number of the surface station.\n\n        date_time : `~datetime.datetime` or Sequence[datetime]\n            Valid datetime of the grid. Alternatively can be\n            a string with the format YYYYmmddHHMM.\n\n        state : str or Sequence[str]\n            State where surface station is located.\n\n        country : str or Sequence[str]\n            Country where surface station is located.\n\n        include_special : bool\n            If True, parse special observations that are stored\n            as raw METAR text. Default is False.\n\n        Returns\n        -------\n        list\n            List of dicts/JSONs for each surface station.\n        \"\"\"\n        if (station_id is not None\n           and (not isinstance(station_id, Iterable)\n                or isinstance(station_id, str))):\n            station_id = [station_id]\n            station_id = [c.upper() for c in station_id]\n\n        if station_number is not None and not isinstance(station_number, Iterable):\n            station_number = [station_number]\n            station_number = [int(sn) for sn in station_number]\n\n        if date_time is not None:\n            if (not isinstance(date_time, Iterable)\n               or isinstance(date_time, str)):\n                date_time = [date_time]\n            for i, dt in enumerate(date_time):\n                if isinstance(dt, str):\n                    date_time[i] = datetime.strptime(dt, '%Y%m%d%H%M')\n\n        if (state is not None\n           and (not isinstance(state, Iterable)\n                or isinstance(state, str))):\n            state = [state]\n            state = [s.upper() for s in state]\n\n        if (country is not None\n           and (not isinstance(country, Iterable)\n                or isinstance(country, str))):\n            country = [country]\n            country = [c.upper() for c in country]\n\n        # Figure out which columns to extract from the file\n        matched = self._sfinfo.copy()\n\n        if station_id is not None:\n            matched = filter(\n                lambda sfc: sfc if sfc.ID in station_id else False,\n                matched\n            )\n\n        if station_number is not None:\n            matched = filter(\n                lambda sfc: sfc if sfc.NUMBER in station_number else False,\n                matched\n            )\n\n        if date_time is not None:\n            matched = filter(\n                lambda sfc: sfc if sfc.DATTIM in date_time else False,\n                matched\n            )\n\n        if state is not None:\n            matched = filter(\n                lambda sfc: sfc if sfc.STATE in state else False,\n                matched\n            )\n\n        if country is not None:\n            matched = filter(\n                lambda sfc: sfc if sfc.COUNTRY in country else False,\n                matched\n            )\n\n        matched = list(matched)\n\n        if len(matched) < 1:\n            raise KeyError('No stations were matched with given parameters.')\n\n        sfcno = [(s.ROW, s.COL) for s in matched]\n\n        if self.surface_type == 'standard':\n            data = self._unpack_standard(sfcno)\n        elif self.surface_type == 'ship':\n            data = self._unpack_ship(sfcno)\n        elif self.surface_type == 'climate':\n            data = self._unpack_climate(sfcno)\n        else:\n            raise ValueError(f'Unknown surface data type: {self.surface_type}')\n\n        stnarr = []\n        for stn in data:\n            if 'SPCL' in stn and include_special:\n                stn = self._decode_special_observation(stn, self.prod_desc.missing_float)\n            props = {'date_time': datetime.combine(stn.pop('DATE'), stn.pop('TIME')),\n                     'station_id': stn.pop('STID') + stn.pop('STD2'),\n                     'station_number': stn.pop('STNM'),\n                     'longitude': stn.pop('SLON'),\n                     'latitude': stn.pop('SLAT'),\n                     'elevation': stn.pop('SELV'),\n                     'state': stn.pop('STAT'),\n                     'country': stn.pop('COUN'),\n                     'priority': stn.pop('SPRI')}\n            if stn:\n                vals = {name.lower(): ob for name, ob in stn.items()}\n                stnarr.append({'properties': props, 'values': vals})\n\n        return stnarr",
  "def __init__(self, file):\n        \"\"\"Instantiate GempakFile object from file.\"\"\"\n        fobj = open_as_needed(file)\n\n        with contextlib.closing(fobj):\n            self._buffer = IOBuffer.fromfile(fobj)\n\n        # Save file start position as pointers use this as reference\n        self._start = self._buffer.set_mark()\n\n        # Process the main GEMPAK header to verify file format\n        self._process_gempak_header()\n        meta = self._buffer.set_mark()\n\n        # # Check for byte swapping\n        self._swap_bytes(bytes(self._buffer.read_binary(4)))\n        self._buffer.jump_to(meta)\n\n        # Process main metadata header\n        self.prod_desc = self._buffer.read_struct(NamedStruct(self.prod_desc_fmt,\n                                                              self.prefmt,\n                                                              'ProductDescription'))\n\n        # File Keys\n        # Surface and upper-air files will not have the file headers, so we need to check.\n        if self.prod_desc.file_headers > 0:\n            # This would grab any file headers, but NAVB and ANLB are the only ones used.\n            fkey_prod = product(['header_name', 'header_length', 'header_type'],\n                                range(1, self.prod_desc.file_headers + 1))\n            fkey_names = ['{}{}'.format(*x) for x in fkey_prod]\n            fkey_info = list(zip(fkey_names, np.repeat(('4s', 'i', 'i'),\n                                                       self.prod_desc.file_headers)))\n            self.file_keys_format = NamedStruct(fkey_info, self.prefmt, 'FileKeys')\n\n            self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.file_keys_ptr))\n            self.file_keys = self._buffer.read_struct(self.file_keys_format)\n\n            # file_key_blocks = self._buffer.set_mark()\n            # Navigation Block\n            navb_size = self._buffer.read_int(4, self.endian, False)\n\n            nav_stuct = NamedStruct(self.grid_nav_fmt,\n                                    self.prefmt,\n                                    'NavigationBlock')\n\n            if navb_size != nav_stuct.size // BYTES_PER_WORD:\n                raise ValueError('Navigation block size does not match GEMPAK specification')\n            else:\n                self.navigation_block = (\n                    self._buffer.read_struct(nav_stuct)\n                )\n            self.kx = int(self.navigation_block.right_grid_number)\n            self.ky = int(self.navigation_block.top_grid_number)\n\n            # Analysis Block\n            anlb_size = self._buffer.read_int(4, self.endian, False)\n            anlb_start = self._buffer.set_mark()\n            anlb1_struct = NamedStruct(self.grid_anl_fmt1,\n                                       self.prefmt,\n                                       'AnalysisBlock')\n            anlb2_struct = NamedStruct(self.grid_anl_fmt2,\n                                       self.prefmt,\n                                       'AnalysisBlock')\n\n            if anlb_size not in [anlb1_struct.size // BYTES_PER_WORD,\n                                 anlb2_struct.size // BYTES_PER_WORD]:\n                raise ValueError('Analysis block size does not match GEMPAK specification')\n            else:\n                anlb_type = self._buffer.read_struct(struct.Struct(self.prefmt + 'f'))[0]\n                self._buffer.jump_to(anlb_start)\n                if anlb_type == 1:\n                    self.analysis_block = (\n                        self._buffer.read_struct(anlb1_struct)\n                    )\n                elif anlb_type == 2:\n                    self.analysis_block = (\n                        self._buffer.read_struct(anlb2_struct)\n                    )\n                else:\n                    self.analysis_block = None\n        else:\n            self.analysis_block = None\n            self.navigation_block = None\n\n        # Data Management\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.data_mgmt_ptr))\n        self.data_management = self._buffer.read_struct(NamedStruct(self.data_management_fmt,\n                                                                    self.prefmt,\n                                                                    'DataManagement'))\n\n        # Row Keys\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.row_keys_ptr))\n        row_key_info = [(f'row_key{n:d}', '4s', self._decode_strip)\n                        for n in range(1, self.prod_desc.row_keys + 1)]\n        row_key_info.extend([(None, None)])\n        row_keys_fmt = NamedStruct(row_key_info, self.prefmt, 'RowKeys')\n        self.row_keys = self._buffer.read_struct(row_keys_fmt)\n\n        # Column Keys\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.column_keys_ptr))\n        column_key_info = [(f'column_key{n:d}', '4s', self._decode_strip)\n                           for n in range(1, self.prod_desc.column_keys + 1)]\n        column_key_info.extend([(None, None)])\n        column_keys_fmt = NamedStruct(column_key_info, self.prefmt, 'ColumnKeys')\n        self.column_keys = self._buffer.read_struct(column_keys_fmt)\n\n        # Parts\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.parts_ptr))\n        # parts = self._buffer.set_mark()\n        self.parts = []\n        parts_info = [('name', '4s', self._decode_strip),\n                      (None, f'{(self.prod_desc.parts - 1) * BYTES_PER_WORD:d}x'),\n                      ('header_length', 'i'),\n                      (None, f'{(self.prod_desc.parts - 1) * BYTES_PER_WORD:d}x'),\n                      ('data_type', 'i', DataTypes),\n                      (None, f'{(self.prod_desc.parts - 1) * BYTES_PER_WORD:d}x'),\n                      ('parameter_count', 'i')]\n        parts_info.extend([(None, None)])\n        parts_fmt = NamedStruct(parts_info, self.prefmt, 'Parts')\n        for n in range(1, self.prod_desc.parts + 1):\n            self.parts.append(self._buffer.read_struct(parts_fmt))\n            self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.parts_ptr + n))\n\n        # Parameters\n        # No need to jump to any position as this follows parts information\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.parts_ptr\n                                                            + self.prod_desc.parts * 4))\n        self.parameters = [{key: [] for key, _ in PARAM_ATTR}\n                           for n in range(self.prod_desc.parts)]\n        for attr, fmt in PARAM_ATTR:\n            fmt = (fmt[0], self.prefmt + fmt[1] if fmt[1] != 's' else fmt[1])\n            for n, part in enumerate(self.parts):\n                for _ in range(part.parameter_count):\n                    if 's' in fmt[1]:\n                        self.parameters[n][attr] += [\n                            self._decode_strip(self._buffer.read_binary(*fmt)[0])\n                        ]\n                    else:\n                        self.parameters[n][attr] += self._buffer.read_binary(*fmt)",
  "def _swap_bytes(self, binary):\n        \"\"\"Swap between little and big endian.\"\"\"\n        self.swaped_bytes = (struct.pack('@i', 1) != binary)\n\n        if self.swaped_bytes:\n            if sys.byteorder == 'little':\n                self.prefmt = '>'\n                self.endian = 'big'\n            elif sys.byteorder == 'big':\n                self.prefmt = '<'\n                self.endian = 'little'\n        else:\n            self.prefmt = ''\n            self.endian = sys.byteorder",
  "def _process_gempak_header(self):\n        \"\"\"Read the GEMPAK header from the file.\"\"\"\n        fmt = [('text', '28s', bytes.decode), (None, None)]\n\n        header = self._buffer.read_struct(NamedStruct(fmt, '', 'GempakHeader'))\n        if header.text != GEMPAK_HEADER:\n            raise TypeError('Unknown file format or invalid GEMPAK file')",
  "def _convert_dattim(dattim):\n        \"\"\"Convert GEMPAK DATTIM integer to datetime object.\"\"\"\n        if dattim:\n            if dattim < 100000000:\n                dt = datetime.strptime(f'{dattim:06d}', '%y%m%d')\n            else:\n                dt = datetime.strptime(f'{dattim:010d}', '%m%d%y%H%M')\n        else:\n            dt = None\n        return dt",
  "def _convert_ftime(ftime):\n        \"\"\"Convert GEMPAK forecast time and type integer.\"\"\"\n        if ftime >= 0:\n            iftype = ForecastType(ftime // 100000)\n            iftime = ftime - iftype.value * 100000\n            hours = iftime // 100\n            minutes = iftime - hours * 100\n            out = (iftype.name, timedelta(hours=hours, minutes=minutes))\n        else:\n            out = None\n        return out",
  "def _convert_level(level):\n        \"\"\"Convert levels.\"\"\"\n        if isinstance(level, (int, float)) and level >= 0:\n            return level\n        else:\n            return None",
  "def _convert_vertical_coord(coord):\n        \"\"\"Convert integer vertical coordinate to name.\"\"\"\n        if coord <= 8:\n            return VerticalCoordinates(coord).name.upper()\n        else:\n            return struct.pack('i', coord).decode()",
  "def _fortran_ishift(i, shift):\n        \"\"\"Python-friendly bit shifting.\"\"\"\n        if shift >= 0:\n            # Shift left and only keep low 32 bits\n            ret = (i << shift) & 0xffffffff\n\n            # If high bit, convert back to negative of two's complement\n            if ret > 0x7fffffff:\n                ret = -(~ret & 0x7fffffff) - 1\n            return ret\n        else:\n            # Shift right the low 32 bits\n            return (i & 0xffffffff) >> -shift",
  "def _decode_strip(b):\n        \"\"\"Decode bytes to string and strip whitespace.\"\"\"\n        return b.decode().strip()",
  "def _make_date(dattim):\n        \"\"\"Make a date object from GEMPAK DATTIM integer.\"\"\"\n        return GempakFile._convert_dattim(dattim).date()",
  "def _make_time(t):\n        \"\"\"Make a time object from GEMPAK FTIME integer.\"\"\"\n        string = f'{t:04d}'\n        return datetime.strptime(string, '%H%M').time()",
  "def _unpack_real(self, buffer, parameters, length):\n        \"\"\"Unpack floating point data packed in integers.\n\n        Similar to DP_UNPK subroutine in GEMPAK.\n        \"\"\"\n        nparms = len(parameters['name'])\n        mskpat = 0xffffffff\n\n        pwords = (sum(parameters['bits']) - 1) // 32 + 1\n        npack = (length - 1) // pwords + 1\n        unpacked = np.ones(npack * nparms, dtype=np.float32) * self.prod_desc.missing_float\n        if npack * pwords != length:\n            raise ValueError('Unpacking length mismatch.')\n\n        ir = 0\n        ii = 0\n        for _i in range(npack):\n            pdat = buffer[ii:(ii + pwords)]\n            rdat = unpacked[ir:(ir + nparms)]\n            itotal = 0\n            for idata in range(nparms):\n                scale = 10**parameters['scale'][idata]\n                offset = parameters['offset'][idata]\n                bits = parameters['bits'][idata]\n                isbitc = (itotal % 32) + 1\n                iswrdc = (itotal // 32)\n                imissc = self._fortran_ishift(mskpat, bits - 32)\n\n                jbit = bits\n                jsbit = isbitc\n                jshift = 1 - jsbit\n                jsword = iswrdc\n                jword = pdat[jsword]\n                mask = self._fortran_ishift(mskpat, jbit - 32)\n                ifield = self._fortran_ishift(jword, jshift)\n                ifield &= mask\n\n                if (jsbit + jbit - 1) > 32:\n                    jword = pdat[jsword + 1]\n                    jshift += 32\n                    iword = self._fortran_ishift(jword, jshift)\n                    iword &= mask\n                    ifield |= iword\n\n                if ifield == imissc:\n                    rdat[idata] = self.prod_desc.missing_float\n                else:\n                    rdat[idata] = (ifield + offset) * scale\n                itotal += bits\n            unpacked[ir:(ir + nparms)] = rdat\n            ir += nparms\n            ii += pwords\n\n        return unpacked.tolist()",
  "def __init__(self, file, *args, **kwargs):\n        super().__init__(file)\n\n        datetime_names = ['GDT1', 'GDT2']\n        level_names = ['GLV1', 'GLV2']\n        ftime_names = ['GTM1', 'GTM2']\n        string_names = ['GPM1', 'GPM2', 'GPM3']\n\n        # Row Headers\n        # Based on GEMPAK source, row/col headers have a 0th element in their Fortran arrays.\n        # This appears to be a flag value to say a header is used or not. 9999\n        # means its in use, otherwise -9999. GEMPAK allows empty grids, etc., but\n        # no real need to keep track of that in Python.\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.row_headers_ptr))\n        self.row_headers = []\n        row_headers_info = [(key, 'i') for key in self.row_keys]\n        row_headers_info.extend([(None, None)])\n        row_headers_fmt = NamedStruct(row_headers_info, self.prefmt, 'RowHeaders')\n        for _ in range(1, self.prod_desc.rows + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.row_headers.append(self._buffer.read_struct(row_headers_fmt))\n\n        # Column Headers\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.column_headers_ptr))\n        self.column_headers = []\n        column_headers_info = [(key, 'i', self._convert_level) if key in level_names\n                               else (key, 'i', self._convert_vertical_coord) if key == 'GVCD'\n                               else (key, 'i', self._convert_dattim) if key in datetime_names\n                               else (key, 'i', self._convert_ftime) if key in ftime_names\n                               else (key, '4s', self._decode_strip) if key in string_names\n                               else (key, 'i')\n                               for key in self.column_keys]\n        column_headers_info.extend([(None, None)])\n        column_headers_fmt = NamedStruct(column_headers_info, self.prefmt, 'ColumnHeaders')\n        for _ in range(1, self.prod_desc.columns + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.column_headers.append(self._buffer.read_struct(column_headers_fmt))\n\n        self._gdinfo = []\n        for n, head in enumerate(self.column_headers):\n            self._gdinfo.append(\n                Grid(\n                    n,\n                    head.GTM1[0],\n                    head.GDT1 + head.GTM1[1],\n                    head.GDT2 + head.GTM2[1] if head.GDT2 and head.GTM2 else None,\n                    head.GPM1 + head.GPM2 + head.GPM3,\n                    head.GLV1,\n                    head.GLV2,\n                    head.GVCD,\n                )\n            )\n\n        # Coordinates\n        if self.navigation_block is not None:\n            self._get_crs()\n            self._set_coordinates()",
  "def gdinfo(self):\n        \"\"\"Return grid information.\"\"\"\n        return self._gdinfo",
  "def _get_crs(self):\n        \"\"\"Create CRS from GEMPAK navigation block.\"\"\"\n        gemproj = self.navigation_block.projection\n        proj, ptype = GEMPROJ_TO_PROJ[gemproj]\n        radius_sph = 6371200.0\n\n        if ptype == 'azm':\n            lat_0 = self.navigation_block.proj_angle1\n            lon_0 = self.navigation_block.proj_angle2\n            rot = self.navigation_block.proj_angle3\n            if rot != 0:\n                log.warning('Rotated projections currently '\n                            'not supported. Angle3 (%7.2f) ignored.', rot)\n            self.crs = pyproj.CRS.from_dict({'proj': proj,\n                                             'lat_0': lat_0,\n                                             'lon_0': lon_0,\n                                             'R': radius_sph})\n        elif ptype == 'cyl':\n            if gemproj != 'mcd':\n                lat_0 = self.navigation_block.proj_angle1\n                lon_0 = self.navigation_block.proj_angle2\n                rot = self.navigation_block.proj_angle3\n                if rot != 0:\n                    log.warning('Rotated projections currently '\n                                'not supported. Angle3 (%7.2f) ignored.', rot)\n                self.crs = pyproj.CRS.from_dict({'proj': proj,\n                                                 'lat_0': lat_0,\n                                                 'lon_0': lon_0,\n                                                 'R': radius_sph})\n            else:\n                avglat = (self.navigation_block.upper_right_lat\n                          + self.navigation_block.lower_left_lat) * 0.5\n                k_0 = (1 / math.cos(avglat)\n                       if self.navigation_block.proj_angle1 == 0\n                       else self.navigation_block.proj_angle1\n                       )\n                lon_0 = self.navigation_block.proj_angle2\n                self.crs = pyproj.CRS.from_dict({'proj': proj,\n                                                 'lat_0': avglat,\n                                                 'lon_0': lon_0,\n                                                 'k_0': k_0,\n                                                 'R': radius_sph})\n        elif ptype == 'con':\n            lat_1 = self.navigation_block.proj_angle1\n            lon_0 = self.navigation_block.proj_angle2\n            lat_2 = self.navigation_block.proj_angle3\n            self.crs = pyproj.CRS.from_dict({'proj': proj,\n                                             'lon_0': lon_0,\n                                             'lat_1': lat_1,\n                                             'lat_2': lat_2,\n                                             'R': radius_sph})",
  "def _set_coordinates(self):\n        \"\"\"Use GEMPAK navigation block to define coordinates.\n\n        Defines geographic and projection coordinates for the object.\n        \"\"\"\n        transform = pyproj.Proj(self.crs)\n        llx, lly = transform(self.navigation_block.lower_left_lon,\n                             self.navigation_block.lower_left_lat)\n        urx, ury = transform(self.navigation_block.upper_right_lon,\n                             self.navigation_block.upper_right_lat)\n        self.x = np.linspace(llx, urx, self.kx, dtype=np.float32)\n        self.y = np.linspace(lly, ury, self.ky, dtype=np.float32)\n        xx, yy = np.meshgrid(self.x, self.y, copy=False)\n        self.lon, self.lat = transform(xx, yy, inverse=True)\n        self.lon = self.lon.astype(np.float32)\n        self.lat = self.lat.astype(np.float32)",
  "def _unpack_grid(self, packing_type, part):\n        \"\"\"Read raw GEMPAK grid integers and unpack into floats.\"\"\"\n        if packing_type == PackingType.none:\n            lendat = self.data_header_length - part.header_length - 1\n\n            if lendat > 1:\n                buffer_fmt = f'{self.prefmt}{lendat}f'\n                buffer = self._buffer.read_struct(struct.Struct(buffer_fmt))\n                grid = np.zeros(self.ky * self.kx, dtype=np.float32)\n                grid[...] = buffer\n            else:\n                grid = None\n\n            return grid\n\n        elif packing_type == PackingType.nmc:\n            raise NotImplementedError('NMC unpacking not supported.')\n            # integer_meta_fmt = [('bits', 'i'), ('missing_flag', 'i'), ('kxky', 'i')]\n            # real_meta_fmt = [('reference', 'f'), ('scale', 'f')]\n            # self.grid_meta_int = self._buffer.read_struct(NamedStruct(integer_meta_fmt,\n            #                                                           self.prefmt,\n            #                                                           'GridMetaInt'))\n            # self.grid_meta_real = self._buffer.read_struct(NamedStruct(real_meta_fmt,\n            #                                                            self.prefmt,\n            #                                                            'GridMetaReal'))\n            # grid_start = self._buffer.set_mark()\n        elif packing_type == PackingType.diff:\n            integer_meta_fmt = [('bits', 'i'), ('missing_flag', 'i'),\n                                ('kxky', 'i'), ('kx', 'i')]\n            real_meta_fmt = [('reference', 'f'), ('scale', 'f'), ('diffmin', 'f')]\n            self.grid_meta_int = self._buffer.read_struct(NamedStruct(integer_meta_fmt,\n                                                                      self.prefmt,\n                                                                      'GridMetaInt'))\n            self.grid_meta_real = self._buffer.read_struct(NamedStruct(real_meta_fmt,\n                                                                       self.prefmt,\n                                                                       'GridMetaReal'))\n            # grid_start = self._buffer.set_mark()\n\n            imiss = 2**self.grid_meta_int.bits - 1\n            lendat = self.data_header_length - part.header_length - 8\n            packed_buffer_fmt = f'{self.prefmt}{lendat}i'\n            packed_buffer = self._buffer.read_struct(struct.Struct(packed_buffer_fmt))\n            grid = np.zeros((self.ky, self.kx), dtype=np.float32)\n\n            if lendat > 1:\n                iword = 0\n                ibit = 1\n                first = True\n                for j in range(self.ky):\n                    line = False\n                    for i in range(self.kx):\n                        jshft = self.grid_meta_int.bits + ibit - 33\n                        idat = self._fortran_ishift(packed_buffer[iword], jshft)\n                        idat &= imiss\n\n                        if jshft > 0:\n                            jshft -= 32\n                            idat2 = self._fortran_ishift(packed_buffer[iword + 1], jshft)\n                            idat |= idat2\n\n                        ibit += self.grid_meta_int.bits\n                        if ibit > 32:\n                            ibit -= 32\n                            iword += 1\n\n                        if (self.grid_meta_int.missing_flag and idat == imiss):\n                            grid[j, i] = self.prod_desc.missing_float\n                        else:\n                            if first:\n                                grid[j, i] = self.grid_meta_real.reference\n                                psav = self.grid_meta_real.reference\n                                plin = self.grid_meta_real.reference\n                                line = True\n                                first = False\n                            else:\n                                if not line:\n                                    grid[j, i] = plin + (self.grid_meta_real.diffmin\n                                                         + idat * self.grid_meta_real.scale)\n                                    line = True\n                                    plin = grid[j, i]\n                                else:\n                                    grid[j, i] = psav + (self.grid_meta_real.diffmin\n                                                         + idat * self.grid_meta_real.scale)\n                                psav = grid[j, i]\n            else:\n                grid = None\n\n            return grid\n\n        elif packing_type in [PackingType.grib, PackingType.dec]:\n            integer_meta_fmt = [('bits', 'i'), ('missing_flag', 'i'), ('kxky', 'i')]\n            real_meta_fmt = [('reference', 'f'), ('scale', 'f')]\n            self.grid_meta_int = self._buffer.read_struct(NamedStruct(integer_meta_fmt,\n                                                                      self.prefmt,\n                                                                      'GridMetaInt'))\n            self.grid_meta_real = self._buffer.read_struct(NamedStruct(real_meta_fmt,\n                                                                       self.prefmt,\n                                                                       'GridMetaReal'))\n            # grid_start = self._buffer.set_mark()\n\n            lendat = self.data_header_length - part.header_length - 6\n            packed_buffer_fmt = f'{self.prefmt}{lendat}i'\n\n            grid = np.zeros(self.grid_meta_int.kxky, dtype=np.float32)\n            packed_buffer = self._buffer.read_struct(struct.Struct(packed_buffer_fmt))\n            if lendat > 1:\n                imax = 2**self.grid_meta_int.bits - 1\n                ibit = 1\n                iword = 0\n                for cell in range(self.grid_meta_int.kxky):\n                    jshft = self.grid_meta_int.bits + ibit - 33\n                    idat = self._fortran_ishift(packed_buffer[iword], jshft)\n                    idat &= imax\n\n                    if jshft > 0:\n                        jshft -= 32\n                        idat2 = self._fortran_ishift(packed_buffer[iword + 1], jshft)\n                        idat |= idat2\n\n                    if (idat == imax) and self.grid_meta_int.missing_flag:\n                        grid[cell] = self.prod_desc.missing_float\n                    else:\n                        grid[cell] = (self.grid_meta_real.reference\n                                      + (idat * self.grid_meta_real.scale))\n\n                    ibit += self.grid_meta_int.bits\n                    if ibit > 32:\n                        ibit -= 32\n                        iword += 1\n            else:\n                grid = None\n\n            return grid\n        elif packing_type == PackingType.grib2:\n            raise NotImplementedError('GRIB2 unpacking not supported.')\n            # integer_meta_fmt = [('iuscal', 'i'), ('kx', 'i'),\n            #                     ('ky', 'i'), ('iscan_mode', 'i')]\n            # real_meta_fmt = [('rmsval', 'f')]\n            # self.grid_meta_int = self._buffer.read_struct(NamedStruct(integer_meta_fmt,\n            #                                                           self.prefmt,\n            #                                                           'GridMetaInt'))\n            # self.grid_meta_real = self._buffer.read_struct(NamedStruct(real_meta_fmt,\n            #                                                            self.prefmt,\n            #                                                            'GridMetaReal'))\n            # grid_start = self._buffer.set_mark()\n        else:\n            raise NotImplementedError(\n                f'No method for unknown grid packing {packing_type.name}'\n            )",
  "def gdxarray(self, parameter=None, date_time=None, coordinate=None,\n                 level=None, date_time2=None, level2=None):\n        \"\"\"Select grids and output as list of xarray DataArrays.\n\n        Subset the data by parameter values. The default is to not\n        subset and return the entire dataset.\n\n        Parameters\n        ----------\n        parameter : str or Sequence[str]\n            Name of GEMPAK parameter.\n\n        date_time : `~datetime.datetime` or Sequence[datetime]\n            Valid datetime of the grid. Alternatively can be\n            a string with the format YYYYmmddHHMM.\n\n        coordinate : str or Sequence[str]\n            Vertical coordinate.\n\n        level : float or Sequence[float]\n            Vertical level.\n\n        date_time2 : `~datetime.datetime` or Sequence[datetime]\n            Secondary valid datetime of the grid. Alternatively can be\n            a string with the format YYYYmmddHHMM.\n\n        level2: float or Sequence[float]\n            Secondary vertical level. Typically used for layers.\n\n        Returns\n        -------\n        list\n            List of `xarray.DataArray` objects for each grid.\n        \"\"\"\n        if parameter is not None:\n            if (not isinstance(parameter, Iterable)\n               or isinstance(parameter, str)):\n                parameter = [parameter]\n            parameter = [p.upper() for p in parameter]\n\n        if date_time is not None:\n            if (not isinstance(date_time, Iterable)\n               or isinstance(date_time, str)):\n                date_time = [date_time]\n            for i, dt in enumerate(date_time):\n                if isinstance(dt, str):\n                    date_time[i] = datetime.strptime(dt, '%Y%m%d%H%M')\n\n        if coordinate is not None:\n            if (not isinstance(coordinate, Iterable)\n               or isinstance(coordinate, str)):\n                coordinate = [coordinate]\n            coordinate = [c.upper() for c in coordinate]\n\n        if level is not None and not isinstance(level, Iterable):\n            level = [level]\n\n        if date_time2 is not None:\n            if (not isinstance(date_time2, Iterable)\n               or isinstance(date_time2, str)):\n                date_time2 = [date_time2]\n            for i, dt in enumerate(date_time2):\n                if isinstance(dt, str):\n                    date_time2[i] = datetime.strptime(dt, '%Y%m%d%H%M')\n\n        if level2 is not None and not isinstance(level2, Iterable):\n            level2 = [level2]\n\n        # Figure out which columns to extract from the file\n        matched = self._gdinfo.copy()\n\n        if parameter is not None:\n            matched = filter(\n                lambda grid: grid if grid.PARM in parameter else False,\n                matched\n            )\n\n        if date_time is not None:\n            matched = filter(\n                lambda grid: grid if grid.DATTIM1 in date_time else False,\n                matched\n            )\n\n        if coordinate is not None:\n            matched = filter(\n                lambda grid: grid if grid.COORD in coordinate else False,\n                matched\n            )\n\n        if level is not None:\n            matched = filter(\n                lambda grid: grid if grid.LEVEL1 in level else False,\n                matched\n            )\n\n        if date_time2 is not None:\n            matched = filter(\n                lambda grid: grid if grid.DATTIM2 in date_time2 else False,\n                matched\n            )\n\n        if level2 is not None:\n            matched = filter(\n                lambda grid: grid if grid.LEVEL2 in level2 else False,\n                matched\n            )\n\n        matched = list(matched)\n\n        if len(matched) < 1:\n            raise KeyError('No grids were matched with given parameters.')\n\n        gridno = [g.GRIDNO for g in matched]\n\n        grids = []\n        irow = 0  # Only one row for grids\n        for icol, col_head in enumerate(self.column_headers):\n            if icol not in gridno:\n                continue\n            for iprt, part in enumerate(self.parts):\n                pointer = (self.prod_desc.data_block_ptr\n                           + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                           + (icol * self.prod_desc.parts + iprt))\n                self._buffer.jump_to(self._start, _word_to_position(pointer))\n                self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                data_header = self._buffer.set_mark()\n                self._buffer.jump_to(data_header,\n                                     _word_to_position(part.header_length + 1))\n                packing_type = PackingType(self._buffer.read_int(4, self.endian, False))\n\n                full_name = col_head.GPM1 + col_head.GPM2 + col_head.GPM3\n                ftype, ftime = col_head.GTM1\n                valid = col_head.GDT1 + ftime\n                gvcord = col_head.GVCD.lower() if col_head.GVCD is not None else 'none'\n                var = (GVCORD_TO_VAR[full_name]\n                       if full_name in GVCORD_TO_VAR\n                       else full_name.lower()\n                       )\n                data = self._unpack_grid(packing_type, part)\n                if data is not None:\n                    if data.ndim < 2:\n                        data = np.ma.array(data.reshape((self.ky, self.kx)),\n                                           mask=data == self.prod_desc.missing_float,\n                                           dtype=np.float32)\n                    else:\n                        data = np.ma.array(data, mask=data == self.prod_desc.missing_float,\n                                           dtype=np.float32)\n\n                    xrda = xr.DataArray(\n                        data=data[np.newaxis, np.newaxis, ...],\n                        coords={\n                            'time': [valid],\n                            gvcord: [col_head.GLV1],\n                            'x': self.x,\n                            'y': self.y,\n                            'metpy_crs': CFProjection(self.crs.to_cf())\n                        },\n                        dims=['time', gvcord, 'y', 'x'],\n                        name=var,\n                        attrs={\n                            'gempak_grid_type': ftype,\n                        }\n                    )\n                    xrda = xrda.metpy.assign_latitude_longitude()\n                    xrda['x'].attrs['units'] = 'meters'\n                    xrda['y'].attrs['units'] = 'meters'\n                    grids.append(xrda)\n\n                else:\n                    log.warning('Unable to read grid for %s', col_head.GPM1)\n        return grids",
  "def __init__(self, file, *args, **kwargs):\n        super().__init__(file)\n\n        # Row Headers\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.row_headers_ptr))\n        self.row_headers = []\n        row_headers_info = [(key, 'i', self._make_date) if key == 'DATE'\n                            else (key, 'i', self._make_time) if key == 'TIME'\n                            else (key, 'i')\n                            for key in self.row_keys]\n        row_headers_info.extend([(None, None)])\n        row_headers_fmt = NamedStruct(row_headers_info, self.prefmt, 'RowHeaders')\n        for _ in range(1, self.prod_desc.rows + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.row_headers.append(self._buffer.read_struct(row_headers_fmt))\n\n        # Column Headers\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.column_headers_ptr))\n        self.column_headers = []\n        column_headers_info = [(key, '4s', self._decode_strip) if key == 'STID'\n                               else (key, 'i') if key == 'STNM'\n                               else (key, 'i', lambda x: x / 100) if key == 'SLAT'\n                               else (key, 'i', lambda x: x / 100) if key == 'SLON'\n                               else (key, 'i') if key == 'SELV'\n                               else (key, '4s', self._decode_strip) if key == 'STAT'\n                               else (key, '4s', self._decode_strip) if key == 'COUN'\n                               else (key, '4s', self._decode_strip) if key == 'STD2'\n                               else (key, 'i')\n                               for key in self.column_keys]\n        column_headers_info.extend([(None, None)])\n        column_headers_fmt = NamedStruct(column_headers_info, self.prefmt, 'ColumnHeaders')\n        for _ in range(1, self.prod_desc.columns + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.column_headers.append(self._buffer.read_struct(column_headers_fmt))\n\n        self.merged = 'SNDT' in (part.name for part in self.parts)\n\n        self._sninfo = []\n        for irow, row_head in enumerate(self.row_headers):\n            for icol, col_head in enumerate(self.column_headers):\n                pointer = (self.prod_desc.data_block_ptr\n                           + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                           + (icol * self.prod_desc.parts))\n\n                self._buffer.jump_to(self._start, _word_to_position(pointer))\n                data_ptr = self._buffer.read_int(4, self.endian, False)\n\n                if data_ptr:\n                    self._sninfo.append(\n                        Sounding(\n                            irow,\n                            icol,\n                            datetime.combine(row_head.DATE, row_head.TIME),\n                            col_head.STID,\n                            col_head.STNM,\n                            col_head.SLAT,\n                            col_head.SLON,\n                            col_head.SELV,\n                            col_head.STAT,\n                            col_head.COUN,\n                        )\n                    )",
  "def sninfo(self):\n        \"\"\"Return sounding information.\"\"\"\n        return self._sninfo",
  "def _unpack_merged(self, sndno):\n        \"\"\"Unpack merged sounding data.\"\"\"\n        soundings = []\n        for irow, row_head in enumerate(self.row_headers):\n            for icol, col_head in enumerate(self.column_headers):\n                if (irow, icol) not in sndno:\n                    continue\n                sounding = {'STID': col_head.STID,\n                            'STNM': col_head.STNM,\n                            'SLAT': col_head.SLAT,\n                            'SLON': col_head.SLON,\n                            'SELV': col_head.SELV,\n                            'STAT': col_head.STAT,\n                            'COUN': col_head.COUN,\n                            'DATE': row_head.DATE,\n                            'TIME': row_head.TIME,\n                            }\n                for iprt, part in enumerate(self.parts):\n                    pointer = (self.prod_desc.data_block_ptr\n                               + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                               + (icol * self.prod_desc.parts + iprt))\n                    self._buffer.jump_to(self._start, _word_to_position(pointer))\n                    self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                    if not self.data_ptr:\n                        continue\n                    self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                    self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                    data_header = self._buffer.set_mark()\n                    self._buffer.jump_to(data_header,\n                                         _word_to_position(part.header_length + 1))\n                    lendat = self.data_header_length - part.header_length\n\n                    fmt_code = {\n                        DataTypes.real: 'f',\n                        DataTypes.realpack: 'i',\n                        DataTypes.character: 's',\n                    }.get(part.data_type)\n\n                    if fmt_code is None:\n                        raise NotImplementedError(f'No methods for data type {part.data_type}')\n\n                    if fmt_code == 's':\n                        lendat *= BYTES_PER_WORD\n\n                    packed_buffer = (\n                        self._buffer.read_struct(\n                            struct.Struct(f'{self.prefmt}{lendat}{fmt_code}')\n                        )\n                    )\n\n                    parameters = self.parameters[iprt]\n                    nparms = len(parameters['name'])\n\n                    if part.data_type == DataTypes.realpack:\n                        unpacked = self._unpack_real(packed_buffer, parameters, lendat)\n                        for iprm, param in enumerate(parameters['name']):\n                            sounding[param] = unpacked[iprm::nparms]\n                    else:\n                        for iprm, param in enumerate(parameters['name']):\n                            sounding[param] = np.array(\n                                packed_buffer[iprm::nparms], dtype=np.float32\n                            )\n\n                soundings.append(sounding)\n        return soundings",
  "def _unpack_unmerged(self, sndno):\n        \"\"\"Unpack unmerged sounding data.\"\"\"\n        soundings = []\n        for irow, row_head in enumerate(self.row_headers):\n            for icol, col_head in enumerate(self.column_headers):\n                if (irow, icol) not in sndno:\n                    continue\n                sounding = {'STID': col_head.STID,\n                            'STNM': col_head.STNM,\n                            'SLAT': col_head.SLAT,\n                            'SLON': col_head.SLON,\n                            'SELV': col_head.SELV,\n                            'STAT': col_head.STAT,\n                            'COUN': col_head.COUN,\n                            'DATE': row_head.DATE,\n                            'TIME': row_head.TIME,\n                            }\n                for iprt, part in enumerate(self.parts):\n                    pointer = (self.prod_desc.data_block_ptr\n                               + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                               + (icol * self.prod_desc.parts + iprt))\n                    self._buffer.jump_to(self._start, _word_to_position(pointer))\n                    self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                    if not self.data_ptr:\n                        continue\n                    self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                    self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                    data_header = self._buffer.set_mark()\n                    self._buffer.jump_to(data_header,\n                                         _word_to_position(part.header_length + 1))\n                    lendat = self.data_header_length - part.header_length\n\n                    fmt_code = {\n                        DataTypes.real: 'f',\n                        DataTypes.realpack: 'i',\n                        DataTypes.character: 's',\n                    }.get(part.data_type)\n\n                    if fmt_code is None:\n                        raise NotImplementedError(f'No methods for data type {part.data_type}')\n\n                    if fmt_code == 's':\n                        lendat *= BYTES_PER_WORD\n\n                    packed_buffer = (\n                        self._buffer.read_struct(\n                            struct.Struct(f'{self.prefmt}{lendat}{fmt_code}')\n                        )\n                    )\n\n                    parameters = self.parameters[iprt]\n                    nparms = len(parameters['name'])\n                    sounding[part.name] = {}\n\n                    if part.data_type == DataTypes.realpack:\n                        unpacked = self._unpack_real(packed_buffer, parameters, lendat)\n                        for iprm, param in enumerate(parameters['name']):\n                            sounding[part.name][param] = unpacked[iprm::nparms]\n                    elif part.data_type == DataTypes.character:\n                        for iprm, param in enumerate(parameters['name']):\n                            sounding[part.name][param] = (\n                                self._decode_strip(packed_buffer[iprm])\n                            )\n                    else:\n                        for iprm, param in enumerate(parameters['name']):\n                            sounding[part.name][param] = (\n                                np.array(packed_buffer[iprm::nparms], dtype=np.float32)\n                            )\n\n                soundings.append(self._merge_sounding(sounding))\n        return soundings",
  "def _merge_significant_temps(self, merged, parts, section, pbot):\n        \"\"\"Process and merge a significant temperature sections.\"\"\"\n        for isigt, press in enumerate(parts[section]['PRES']):\n            press = abs(press)\n            if self.prod_desc.missing_float not in [\n                press,\n                parts[section]['TEMP'][isigt]\n            ] and press != 0:\n                if press > pbot:\n                    continue\n                elif press in merged['PRES']:\n                    ploc = merged['PRES'].index(press)\n                    if merged['TEMP'][ploc] == self.prod_desc.missing_float:\n                        merged['TEMP'][ploc] = parts[section]['TEMP'][isigt]\n                        merged['DWPT'][ploc] = parts[section]['DWPT'][isigt]\n                else:\n                    size = len(merged['PRES'])\n                    loc = size - bisect.bisect_left(merged['PRES'][::-1], press)\n                    merged['PRES'].insert(loc, press)\n                    merged['TEMP'].insert(loc, parts[section]['TEMP'][isigt])\n                    merged['DWPT'].insert(loc, parts[section]['DWPT'][isigt])\n                    merged['DRCT'].insert(loc, self.prod_desc.missing_float)\n                    merged['SPED'].insert(loc, self.prod_desc.missing_float)\n                    merged['HGHT'].insert(loc, self.prod_desc.missing_float)\n            pbot = press\n\n        return pbot",
  "def _merge_tropopause_data(self, merged, parts, section, pbot):\n        \"\"\"Process and merge tropopause sections.\"\"\"\n        for itrp, press in enumerate(parts[section]['PRES']):\n            press = abs(press)\n            if self.prod_desc.missing_float not in [\n                press,\n                parts[section]['TEMP'][itrp]\n            ] and press != 0:\n                if press > pbot:\n                    continue\n                elif press in merged['PRES']:\n                    ploc = merged['PRES'].index(press)\n                    if merged['TEMP'][ploc] == self.prod_desc.missing_float:\n                        merged['TEMP'][ploc] = parts[section]['TEMP'][itrp]\n                        merged['DWPT'][ploc] = parts[section]['DWPT'][itrp]\n                    if merged['DRCT'][ploc] == self.prod_desc.missing_float:\n                        merged['DRCT'][ploc] = parts[section]['DRCT'][itrp]\n                        merged['SPED'][ploc] = parts[section]['SPED'][itrp]\n                    merged['HGHT'][ploc] = self.prod_desc.missing_float\n                else:\n                    size = len(merged['PRES'])\n                    loc = size - bisect.bisect_left(merged['PRES'][::-1], press)\n                    merged['PRES'].insert(loc, press)\n                    merged['TEMP'].insert(loc, parts[section]['TEMP'][itrp])\n                    merged['DWPT'].insert(loc, parts[section]['DWPT'][itrp])\n                    merged['DRCT'].insert(loc, parts[section]['DRCT'][itrp])\n                    merged['SPED'].insert(loc, parts[section]['SPED'][itrp])\n                    merged['HGHT'].insert(loc, self.prod_desc.missing_float)\n            pbot = press\n\n        return pbot",
  "def _merge_mandatory_temps(self, merged, parts, section, qcman, bgl, plast):\n        \"\"\"Process and merge mandatory temperature sections.\"\"\"\n        num_levels = len(parts[section]['PRES'])\n        start_level = {\n            'TTAA': 1,\n            'TTCC': 0,\n        }\n        for i in range(start_level[section], num_levels):\n            if (parts[section]['PRES'][i] < plast\n                and self.prod_desc.missing_float not in [\n                    parts[section]['PRES'][i],\n                    parts[section]['TEMP'][i],\n                    parts[section]['HGHT'][i]\n            ]):\n                for pname, pval in parts[section].items():\n                    merged[pname].append(pval[i])\n                plast = merged['PRES'][-1]\n            else:\n                if section == 'TTAA':\n                    if parts[section]['PRES'][i] > merged['PRES'][0]:\n                        bgl += 1\n                    else:\n                        # GEMPAK ignores MAN data with missing TEMP/HGHT and does not\n                        # interpolate for them.\n                        if parts[section]['PRES'][i] != self.prod_desc.missing_float:\n                            qcman.append(parts[section]['PRES'][i])\n\n        return bgl, plast",
  "def _merge_mandatory_winds(self, merged, parts, section, qcman):\n        \"\"\"Process and merge manadatory wind sections.\"\"\"\n        for iwind, press in enumerate(parts[section]['PRES']):\n            if press in merged['PRES'][1:]:\n                loc = merged['PRES'].index(press)\n                if merged['DRCT'][loc] == self.prod_desc.missing_float:\n                    merged['DRCT'][loc] = parts[section]['DRCT'][iwind]\n                    merged['SPED'][loc] = parts[section]['SPED'][iwind]\n            else:\n                if press not in qcman:\n                    size = len(merged['PRES'])\n                    loc = size - bisect.bisect_left(merged['PRES'][1:][::-1], press)\n                    if loc >= size + 1:\n                        loc = -1\n                    merged['PRES'].insert(loc, press)\n                    merged['TEMP'].insert(loc, self.prod_desc.missing_float)\n                    merged['DWPT'].insert(loc, self.prod_desc.missing_float)\n                    merged['DRCT'].insert(loc, parts[section]['DRCT'][iwind])\n                    merged['SPED'].insert(loc, parts[section]['SPED'][iwind])\n                    merged['HGHT'].insert(loc, self.prod_desc.missing_float)",
  "def _merge_winds_pressure(self, merged, parts, section, pbot):\n        \"\"\"Process and merge wind sections on pressure surfaces.\"\"\"\n        for ilevel, press in enumerate(parts[section]['PRES']):\n            press = abs(press)\n            if self.prod_desc.missing_float not in [\n                press,\n                parts[section]['DRCT'][ilevel],\n                parts[section]['SPED'][ilevel]\n            ] and press != 0:\n                if press > pbot:\n                    continue\n                elif press in merged['PRES']:\n                    ploc = merged['PRES'].index(press)\n                    if self.prod_desc.missing_float in [\n                        merged['DRCT'][ploc],\n                        merged['SPED'][ploc]\n                    ]:\n                        merged['DRCT'][ploc] = parts[section]['DRCT'][ilevel]\n                        merged['SPED'][ploc] = parts[section]['SPED'][ilevel]\n                else:\n                    size = len(merged['PRES'])\n                    loc = size - bisect.bisect_left(merged['PRES'][::-1], press)\n                    merged['PRES'].insert(loc, press)\n                    merged['DRCT'].insert(loc, parts[section]['DRCT'][ilevel])\n                    merged['SPED'].insert(loc, parts[section]['SPED'][ilevel])\n                    merged['TEMP'].insert(loc, self.prod_desc.missing_float)\n                    merged['DWPT'].insert(loc, self.prod_desc.missing_float)\n                    merged['HGHT'].insert(loc, self.prod_desc.missing_float)\n            pbot = press\n\n        return pbot",
  "def _merge_winds_height(self, merged, parts, nsgw, nasw, istart):\n        \"\"\"Merge wind sections on height surfaces.\"\"\"\n        size = len(merged['HGHT'])\n        psfc = merged['PRES'][0]\n        zsfc = merged['HGHT'][0]\n\n        if self.prod_desc.missing_float not in [\n            psfc,\n            zsfc\n        ] and size >= 2:\n            more = True\n            zold = merged['HGHT'][0]\n            znxt = merged['HGHT'][1]\n            ilev = 1\n        elif size >= 3:\n            more = True\n            zold = merged['HGHT'][1]\n            znxt = merged['HGHT'][2]\n            ilev = 2\n        else:\n            zold = self.prod_desc.missing_float\n            znxt = self.prod_desc.missing_float\n\n        if self.prod_desc.missing_float in [\n            zold,\n            znxt\n        ]:\n            more = False\n\n        if istart <= nsgw:\n            above = False\n            i = istart\n            iend = nsgw\n        else:\n            above = True\n            i = 0\n            iend = nasw\n\n        while more and i < iend:\n            if not above:\n                hght = parts['PPBB']['HGHT'][i]\n                drct = parts['PPBB']['DRCT'][i]\n                sped = parts['PPBB']['SPED'][i]\n            else:\n                hght = parts['PPDD']['HGHT'][i]\n                drct = parts['PPDD']['DRCT'][i]\n                sped = parts['PPDD']['SPED'][i]\n            skip = False\n\n            if self.prod_desc.missing_float in [\n                hght,\n                drct,\n                sped\n            ] or hght <= zold:\n                skip = True\n            elif abs(zold - hght) < 1:\n                skip = True\n                if self.prod_desc.missing_float in [\n                    merged['DRCT'][ilev - 1],\n                    merged['SPED'][ilev - 1]\n                ]:\n                    merged['DRCT'][ilev - 1] = drct\n                    merged['SPED'][ilev - 1] = sped\n            elif hght >= znxt:\n                while more and hght > znxt:\n                    zold = znxt\n                    ilev += 1\n                    if ilev >= size:\n                        more = False\n                    else:\n                        znxt = merged['HGHT'][ilev]\n                        if znxt == self.prod_desc.missing_float:\n                            more = False\n\n            if more and not skip:\n                if abs(znxt - hght) < 1:\n                    if self.prod_desc.missing_float in [\n                        merged['DRCT'][ilev - 1],\n                        merged['SPED'][ilev - 1]\n                    ]:\n                        merged['DRCT'][ilev] = drct\n                        merged['SPED'][ilev] = sped\n                else:\n                    loc = bisect.bisect_left(merged['HGHT'], hght)\n                    merged['HGHT'].insert(loc, hght)\n                    merged['DRCT'].insert(loc, drct)\n                    merged['SPED'].insert(loc, sped)\n                    merged['PRES'].insert(loc, self.prod_desc.missing_float)\n                    merged['TEMP'].insert(loc, self.prod_desc.missing_float)\n                    merged['DWPT'].insert(loc, self.prod_desc.missing_float)\n                    size += 1\n                    ilev += 1\n                    zold = hght\n\n            if not above and i == nsgw - 1:\n                above = True\n                i = 0\n                iend = nasw\n            else:\n                i += 1",
  "def _merge_sounding(self, parts):\n        \"\"\"Merge unmerged sounding data.\"\"\"\n        merged = {'STID': parts['STID'],\n                  'STNM': parts['STNM'],\n                  'SLAT': parts['SLAT'],\n                  'SLON': parts['SLON'],\n                  'SELV': parts['SELV'],\n                  'STAT': parts['STAT'],\n                  'COUN': parts['COUN'],\n                  'DATE': parts['DATE'],\n                  'TIME': parts['TIME'],\n                  'PRES': [],\n                  'HGHT': [],\n                  'TEMP': [],\n                  'DWPT': [],\n                  'DRCT': [],\n                  'SPED': [],\n                  }\n\n        # Number of parameter levels\n        num_man_levels = len(parts['TTAA']['PRES']) if 'TTAA' in parts else 0\n        num_man_wind_levels = len(parts['PPAA']['PRES']) if 'PPAA' in parts else 0\n        num_trop_levels = len(parts['TRPA']['PRES']) if 'TRPA' in parts else 0\n        num_max_wind_levels = len(parts['MXWA']['PRES']) if 'MXWA' in parts else 0\n        num_sigt_levels = len(parts['TTBB']['PRES']) if 'TTBB' in parts else 0\n        num_sigw_levels = len(parts['PPBB']['SPED']) if 'PPBB' in parts else 0\n        num_above_man_levels = len(parts['TTCC']['PRES']) if 'TTCC' in parts else 0\n        num_above_trop_levels = len(parts['TRPC']['PRES']) if 'TRPC' in parts else 0\n        num_above_max_wind_levels = len(parts['MXWC']['SPED']) if 'MXWC' in parts else 0\n        num_above_sigt_levels = len(parts['TTDD']['PRES']) if 'TTDD' in parts else 0\n        num_above_sigw_levels = len(parts['PPDD']['SPED']) if 'PPDD' in parts else 0\n        num_above_man_wind_levels = len(parts['PPCC']['SPED']) if 'PPCC' in parts else 0\n\n        total_data = (num_man_levels\n                      + num_man_wind_levels\n                      + num_trop_levels\n                      + num_max_wind_levels\n                      + num_sigt_levels\n                      + num_sigw_levels\n                      + num_above_man_levels\n                      + num_above_trop_levels\n                      + num_above_max_wind_levels\n                      + num_above_sigt_levels\n                      + num_above_sigw_levels\n                      + num_above_man_wind_levels\n                      )\n        if total_data == 0:\n            return None\n\n        # Check SIG wind vertical coordinate\n        # For some reason, the pressure data can get put into the\n        # height array. Perhaps this is just a artifact of Python,\n        # as GEMPAK itself just uses array indices without any\n        # names involved. Since the first valid pressure of the\n        # array will be negative in the case of pressure coordinates,\n        # we can check for it and place data in the appropriate array.\n        ppbb_is_z = True\n        if num_sigw_levels:\n            if 'PRES' in parts['PPBB']:\n                ppbb_is_z = False\n            else:\n                for z in parts['PPBB']['HGHT']:\n                    if z != self.prod_desc.missing_float and z < 0:\n                        ppbb_is_z = False\n                        parts['PPBB']['PRES'] = parts['PPBB']['HGHT']\n                        break\n\n        ppdd_is_z = True\n        if num_above_sigw_levels:\n            if 'PRES' in parts['PPDD']:\n                ppdd_is_z = False\n            else:\n                for z in parts['PPDD']['HGHT']:\n                    if z != self.prod_desc.missing_float and z < 0:\n                        ppdd_is_z = False\n                        parts['PPDD']['PRES'] = parts['PPDD']['HGHT']\n                        break\n\n        # Process surface data\n        if num_man_levels < 1:\n            merged['PRES'].append(self.prod_desc.missing_float)\n            merged['HGHT'].append(self.prod_desc.missing_float)\n            merged['TEMP'].append(self.prod_desc.missing_float)\n            merged['DWPT'].append(self.prod_desc.missing_float)\n            merged['DRCT'].append(self.prod_desc.missing_float)\n            merged['SPED'].append(self.prod_desc.missing_float)\n        else:\n            merged['PRES'].append(parts['TTAA']['PRES'][0])\n            merged['HGHT'].append(parts['TTAA']['HGHT'][0])\n            merged['TEMP'].append(parts['TTAA']['TEMP'][0])\n            merged['DWPT'].append(parts['TTAA']['DWPT'][0])\n            merged['DRCT'].append(parts['TTAA']['DRCT'][0])\n            merged['SPED'].append(parts['TTAA']['SPED'][0])\n\n        merged['HGHT'][0] = merged['SELV']\n\n        first_man_p = self.prod_desc.missing_float\n        if num_man_levels >= 1:\n            for mp, mt, mz in zip(parts['TTAA']['PRES'],\n                                  parts['TTAA']['TEMP'],\n                                  parts['TTAA']['HGHT']):\n                if self.prod_desc.missing_float not in [\n                    mp,\n                    mt,\n                    mz\n                ]:\n                    first_man_p = mp\n                    break\n\n        surface_p = merged['PRES'][0]\n        if surface_p > 1060:\n            surface_p = self.prod_desc.missing_float\n\n        if (surface_p == self.prod_desc.missing_float\n           or (surface_p < first_man_p\n               and surface_p != self.prod_desc.missing_float)):\n            merged['PRES'][0] = self.prod_desc.missing_float\n            merged['HGHT'][0] = self.prod_desc.missing_float\n            merged['TEMP'][0] = self.prod_desc.missing_float\n            merged['DWPT'][0] = self.prod_desc.missing_float\n            merged['DRCT'][0] = self.prod_desc.missing_float\n            merged['SPED'][0] = self.prod_desc.missing_float\n\n        if (num_sigt_levels >= 1\n           and self.prod_desc.missing_float not in [\n               parts['TTBB']['PRES'][0],\n               parts['TTBB']['TEMP'][0]\n           ]):\n            first_man_p = merged['PRES'][0]\n            first_sig_p = parts['TTBB']['PRES'][0]\n            if (first_man_p == self.prod_desc.missing_float\n               or np.isclose(first_man_p, first_sig_p)):\n                merged['PRES'][0] = parts['TTBB']['PRES'][0]\n                merged['DWPT'][0] = parts['TTBB']['DWPT'][0]\n                merged['TEMP'][0] = parts['TTBB']['TEMP'][0]\n\n        if num_sigw_levels >= 1:\n            if ppbb_is_z:\n                if (parts['PPBB']['HGHT'][0] == 0\n                   and parts['PPBB']['DRCT'][0] != self.prod_desc.missing_float):\n                    merged['DRCT'][0] = parts['PPBB']['DRCT'][0]\n                    merged['SPED'][0] = parts['PPBB']['SPED'][0]\n            else:\n                if self.prod_desc.missing_float not in [\n                    parts['PPBB']['PRES'][0],\n                    parts['PPBB']['DRCT'][0]\n                ]:\n                    first_man_p = merged['PRES'][0]\n                    first_sig_p = abs(parts['PPBB']['PRES'][0])\n                    if (first_man_p == self.prod_desc.missing_float\n                       or np.isclose(first_man_p, first_sig_p)):\n                        merged['PRES'][0] = abs(parts['PPBB']['PRES'][0])\n                        merged['DRCT'][0] = parts['PPBB']['DRCT'][0]\n                        merged['SPED'][0] = parts['PPBB']['SPED'][0]\n\n        # Merge MAN temperature\n        bgl = 0\n        qcman = []\n        if num_man_levels >= 2 or num_above_man_levels >= 1:\n            if merged['PRES'][0] == self.prod_desc.missing_float:\n                plast = 2000\n            else:\n                plast = merged['PRES'][0]\n\n        if num_man_levels >= 2:\n            bgl, plast = self._merge_mandatory_temps(merged, parts, 'TTAA',\n                                                     qcman, bgl, plast)\n\n        if num_above_man_levels >= 1:\n            bgl, plast = self._merge_mandatory_temps(merged, parts, 'TTCC',\n                                                     qcman, bgl, plast)\n\n        # Merge MAN wind\n        if num_man_wind_levels >= 1 and num_man_levels >= 1 and len(merged['PRES']) >= 2:\n            self._merge_mandatory_winds(merged, parts, 'PPAA', qcman)\n\n        if num_above_man_wind_levels >= 1 and num_man_levels >= 1 and len(merged['PRES']) >= 2:\n            self._merge_mandatory_winds(merged, parts, 'PPCC', qcman)\n\n        # Merge TROP\n        if num_trop_levels >= 1 or num_above_trop_levels >= 1:\n            if merged['PRES'][0] != self.prod_desc.missing_float:\n                pbot = merged['PRES'][0]\n            elif len(merged['PRES']) > 1:\n                pbot = merged['PRES'][1]\n                if pbot < parts['TRPA']['PRES'][1]:\n                    pbot = 1050\n            else:\n                pbot = 1050\n\n        if num_trop_levels >= 1:\n            pbot = self._merge_tropopause_data(merged, parts, 'TRPA', pbot)\n\n        if num_above_trop_levels >= 1:\n            pbot = self._merge_tropopause_data(merged, parts, 'TRPC', pbot)\n\n        # Merge SIG temperature\n        if num_sigt_levels >= 1 or num_above_sigt_levels >= 1:\n            if merged['PRES'][0] != self.prod_desc.missing_float:\n                pbot = merged['PRES'][0]\n            elif len(merged['PRES']) > 1:\n                pbot = merged['PRES'][1]\n                if pbot < parts['TTBB']['PRES'][1]:\n                    pbot = 1050\n            else:\n                pbot = 1050\n\n        if num_sigt_levels >= 1:\n            pbot = self._merge_significant_temps(merged, parts, 'TTBB', pbot)\n\n        if num_above_sigt_levels >= 1:\n            pbot = self._merge_significant_temps(merged, parts, 'TTDD', pbot)\n\n        # Interpolate heights\n        _interp_moist_height(merged, self.prod_desc.missing_float)\n\n        # Merge SIG winds on pressure surfaces\n        if not ppbb_is_z or not ppdd_is_z:\n            if num_sigw_levels >= 1 or num_above_sigw_levels >= 1:\n                if merged['PRES'][0] != self.prod_desc.missing_float:\n                    pbot = merged['PRES'][0]\n                elif len(merged['PRES']) > 1:\n                    pbot = merged['PRES'][1]\n                else:\n                    pbot = 0\n\n            if num_sigw_levels >= 1 and not ppbb_is_z:\n                pbot = self._merge_winds_pressure(merged, parts, 'PPBB', pbot)\n\n            if num_above_sigw_levels >= 1 and not ppdd_is_z:\n                pbot = self._merge_winds_pressure(merged, parts, 'PPDD', pbot)\n\n        # Merge max winds on pressure surfaces\n        if num_max_wind_levels >= 1 or num_above_max_wind_levels >= 1:\n            if merged['PRES'][0] != self.prod_desc.missing_float:\n                pbot = merged['PRES'][0]\n            elif len(merged['PRES']) > 1:\n                pbot = merged['PRES'][1]\n            else:\n                pbot = 0\n\n        if num_max_wind_levels >= 1:\n            pbot = self._merge_winds_pressure(merged, parts, 'MXWA', pbot)\n\n        if num_above_max_wind_levels >= 1:\n            _ = self._merge_winds_pressure(merged, parts, 'MXWC', pbot)\n\n        # Interpolate height for SIG/MAX winds\n        _interp_logp_height(merged, self.prod_desc.missing_float)\n\n        # Merge SIG winds on height surfaces\n        if ppbb_is_z or ppdd_is_z:\n            nsgw = num_sigw_levels if ppbb_is_z else 0\n            nasw = num_above_sigw_levels if ppdd_is_z else 0\n            if (nsgw >= 1 and (parts['PPBB']['HGHT'][0] == 0\n               or parts['PPBB']['HGHT'][0] == merged['HGHT'][0])):\n                istart = 1\n            else:\n                istart = 0\n\n            self._merge_winds_height(merged, parts, nsgw, nasw, istart)\n\n            # Interpolate missing pressure with height\n            _interp_logp_pressure(merged, self.prod_desc.missing_float)\n\n        # Interpolate missing data\n        _interp_logp_data(merged, self.prod_desc.missing_float)\n\n        # Add below ground MAN data\n        if merged['PRES'][0] != self.prod_desc.missing_float and bgl > 0:\n            size = len(merged['PRES'])\n            for ibgl in range(1, num_man_levels):\n                press = parts['TTAA']['PRES'][ibgl]\n                if press > merged['PRES'][0]:\n                    loc = size - bisect.bisect_left(merged['PRES'][1:][::-1], press)\n                    merged['PRES'].insert(loc, press)\n                    merged['TEMP'].insert(loc, parts['TTAA']['TEMP'][ibgl])\n                    merged['DWPT'].insert(loc, parts['TTAA']['DWPT'][ibgl])\n                    merged['DRCT'].insert(loc, parts['TTAA']['DRCT'][ibgl])\n                    merged['SPED'].insert(loc, parts['TTAA']['SPED'][ibgl])\n                    merged['HGHT'].insert(loc, parts['TTAA']['HGHT'][ibgl])\n                    size += 1\n\n        # Add text data, if it is included\n        if 'TXTA' in parts:\n            merged['TXTA'] = parts['TXTA']['TEXT']\n        if 'TXTB' in parts:\n            merged['TXTB'] = parts['TXTB']['TEXT']\n        if 'TXTC' in parts:\n            merged['TXTC'] = parts['TXTC']['TEXT']\n        if 'TXPB' in parts:\n            merged['TXPB'] = parts['TXPB']['TEXT']\n\n        return merged",
  "def snxarray(self, station_id=None, station_number=None,\n                 date_time=None, state=None, country=None):\n        \"\"\"Select soundings and output as list of xarray Datasets.\n\n        Subset the data by parameter values. The default is to not\n        subset and return the entire dataset.\n\n        Parameters\n        ----------\n        station_id : str or Sequence[str]\n            Station ID of sounding site.\n\n        station_number : int or Sequence[int]\n            Station number of sounding site.\n\n        date_time : `~datetime.datetime` or Sequence[datetime]\n            Valid datetime of the grid. Alternatively can be\n            a string with the format YYYYmmddHHMM.\n\n        state : str or Sequence[str]\n            State where sounding site is located.\n\n        country : str or Sequence[str]\n            Country where sounding site is located.\n\n        Returns\n        -------\n        list[xarray.Dataset]\n            List of `xarray.Dataset` objects for each sounding.\n        \"\"\"\n        if station_id is not None:\n            if (not isinstance(station_id, Iterable)\n               or isinstance(station_id, str)):\n                station_id = [station_id]\n            station_id = [c.upper() for c in station_id]\n\n        if station_number is not None:\n            if not isinstance(station_number, Iterable):\n                station_number = [station_number]\n            station_number = [int(sn) for sn in station_number]\n\n        if date_time is not None:\n            if (not isinstance(date_time, Iterable)\n               or isinstance(date_time, str)):\n                date_time = [date_time]\n            for i, dt in enumerate(date_time):\n                if isinstance(dt, str):\n                    date_time[i] = datetime.strptime(dt, '%Y%m%d%H%M')\n\n        if (state is not None\n           and (not isinstance(state, Iterable)\n                or isinstance(state, str))):\n            state = [state]\n            state = [s.upper() for s in state]\n\n        if (country is not None\n           and (not isinstance(country, Iterable)\n                or isinstance(country, str))):\n            country = [country]\n            country = [c.upper() for c in country]\n\n        # Figure out which columns to extract from the file\n        matched = self._sninfo.copy()\n\n        if station_id is not None:\n            matched = filter(\n                lambda snd: snd if snd.ID in station_id else False,\n                matched\n            )\n\n        if station_number is not None:\n            matched = filter(\n                lambda snd: snd if snd.NUMBER in station_number else False,\n                matched\n            )\n\n        if date_time is not None:\n            matched = filter(\n                lambda snd: snd if snd.DATTIM in date_time else False,\n                matched\n            )\n\n        if state is not None:\n            matched = filter(\n                lambda snd: snd if snd.STATE in state else False,\n                matched\n            )\n\n        if country is not None:\n            matched = filter(\n                lambda snd: snd if snd.COUNTRY in country else False,\n                matched\n            )\n\n        matched = list(matched)\n\n        if len(matched) < 1:\n            raise KeyError('No stations were matched with given parameters.')\n\n        sndno = [(s.DTNO, s.SNDNO) for s in matched]\n\n        if self.merged:\n            data = self._unpack_merged(sndno)\n        else:\n            data = self._unpack_unmerged(sndno)\n\n        soundings = []\n        for snd in data:\n            if snd is None or 'PRES' not in snd:\n                continue\n            station_pressure = snd['PRES'][0]\n            wmo_text = {}\n            attrs = {\n                'station_id': snd.pop('STID'),\n                'station_number': snd.pop('STNM'),\n                'lat': snd.pop('SLAT'),\n                'lon': snd.pop('SLON'),\n                'elevation': snd.pop('SELV'),\n                'station_pressure': station_pressure,\n                'state': snd.pop('STAT'),\n                'country': snd.pop('COUN'),\n            }\n\n            if 'TXTA' in snd:\n                wmo_text['txta'] = snd.pop('TXTA')\n            if 'TXTB' in snd:\n                wmo_text['txtb'] = snd.pop('TXTB')\n            if 'TXTC' in snd:\n                wmo_text['txtc'] = snd.pop('TXTC')\n            if 'TXPB' in snd:\n                wmo_text['txpb'] = snd.pop('TXPB')\n            if wmo_text:\n                attrs['WMO_CODES'] = wmo_text\n\n            dt = datetime.combine(snd.pop('DATE'), snd.pop('TIME'))\n            press = np.array(snd.pop('PRES'))\n\n            var = {}\n            for param, values in snd.items():\n                values = np.array(values)[np.newaxis, ...]\n                maskval = np.ma.array(values, mask=values == self.prod_desc.missing_float,\n                                      dtype=np.float32)\n                var[param.lower()] = (['time', 'pressure'], maskval)\n\n            xrds = xr.Dataset(var,\n                              coords={'time': np.atleast_1d(dt), 'pressure': press},\n                              attrs=attrs)\n\n            # Sort to fix GEMPAK surface data at first level\n            xrds = xrds.sortby('pressure', ascending=False)\n\n            soundings.append(xrds)\n        return soundings",
  "def __init__(self, file, *args, **kwargs):\n        super().__init__(file)\n\n        # Row Headers\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.row_headers_ptr))\n        self.row_headers = []\n        row_headers_info = self._key_types(self.row_keys)\n        row_headers_info.extend([(None, None)])\n        row_headers_fmt = NamedStruct(row_headers_info, self.prefmt, 'RowHeaders')\n        for _ in range(1, self.prod_desc.rows + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.row_headers.append(self._buffer.read_struct(row_headers_fmt))\n\n        # Column Headers\n        self._buffer.jump_to(self._start, _word_to_position(self.prod_desc.column_headers_ptr))\n        self.column_headers = []\n        column_headers_info = self._key_types(self.column_keys)\n        column_headers_info.extend([(None, None)])\n        column_headers_fmt = NamedStruct(column_headers_info, self.prefmt, 'ColumnHeaders')\n        for _ in range(1, self.prod_desc.columns + 1):\n            if self._buffer.read_int(4, self.endian, False) == USED_FLAG:\n                self.column_headers.append(self._buffer.read_struct(column_headers_fmt))\n\n        self._get_surface_type()\n\n        self._sfinfo = []\n        if self.surface_type == 'standard':\n            for irow, row_head in enumerate(self.row_headers):\n                for icol, col_head in enumerate(self.column_headers):\n                    for iprt in range(len(self.parts)):\n                        pointer = (self.prod_desc.data_block_ptr\n                                   + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                                   + (icol * self.prod_desc.parts + iprt))\n\n                        self._buffer.jump_to(self._start, _word_to_position(pointer))\n                        data_ptr = self._buffer.read_int(4, self.endian, False)\n\n                        if data_ptr:\n                            self._sfinfo.append(\n                                Surface(\n                                    irow,\n                                    icol,\n                                    datetime.combine(row_head.DATE, row_head.TIME),\n                                    col_head.STID + col_head.STD2,\n                                    col_head.STNM,\n                                    col_head.SLAT,\n                                    col_head.SLON,\n                                    col_head.SELV,\n                                    col_head.STAT,\n                                    col_head.COUN,\n                                )\n                            )\n        elif self.surface_type == 'ship':\n            irow = 0\n            for icol, col_head in enumerate(self.column_headers):\n                for iprt in range(len(self.parts)):\n                    pointer = (self.prod_desc.data_block_ptr\n                               + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                               + (icol * self.prod_desc.parts + iprt))\n\n                    self._buffer.jump_to(self._start, _word_to_position(pointer))\n                    data_ptr = self._buffer.read_int(4, self.endian, False)\n\n                    if data_ptr:\n                        self._sfinfo.append(\n                            Surface(\n                                irow,\n                                icol,\n                                datetime.combine(col_head.DATE, col_head.TIME),\n                                col_head.STID + col_head.STD2,\n                                col_head.STNM,\n                                col_head.SLAT,\n                                col_head.SLON,\n                                col_head.SELV,\n                                col_head.STAT,\n                                col_head.COUN,\n                            )\n                        )\n        elif self.surface_type == 'climate':\n            for icol, col_head in enumerate(self.column_headers):\n                for irow, row_head in enumerate(self.row_headers):\n                    for iprt in range(len(self.parts)):\n                        pointer = (self.prod_desc.data_block_ptr\n                                   + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                                   + (icol * self.prod_desc.parts + iprt))\n\n                        self._buffer.jump_to(self._start, _word_to_position(pointer))\n                        data_ptr = self._buffer.read_int(4, self.endian, False)\n\n                        if data_ptr:\n                            self._sfinfo.append(\n                                Surface(\n                                    irow,\n                                    icol,\n                                    datetime.combine(col_head.DATE, col_head.TIME),\n                                    row_head.STID + row_head.STD2,\n                                    row_head.STNM,\n                                    row_head.SLAT,\n                                    row_head.SLON,\n                                    row_head.SELV,\n                                    row_head.STAT,\n                                    row_head.COUN,\n                                )\n                            )\n        else:\n            raise TypeError(f'Unknown surface type {self.surface_type}')",
  "def sfinfo(self):\n        \"\"\"Return station information.\"\"\"\n        return self._sfinfo",
  "def _get_surface_type(self):\n        \"\"\"Determine type of surface file.\"\"\"\n        if len(self.row_headers) == 1:\n            self.surface_type = 'ship'\n        elif 'DATE' in self.row_keys:\n            self.surface_type = 'standard'\n        elif 'DATE' in self.column_keys:\n            self.surface_type = 'climate'\n        else:\n            raise TypeError('Unknown surface data type')",
  "def _key_types(self, keys):\n        \"\"\"Determine header information from a set of keys.\"\"\"\n        return [(key, '4s', self._decode_strip) if key == 'STID'\n                else (key, 'i') if key == 'STNM'\n                else (key, 'i', lambda x: x / 100) if key == 'SLAT'\n                else (key, 'i', lambda x: x / 100) if key == 'SLON'\n                else (key, 'i') if key == 'SELV'\n                else (key, '4s', self._decode_strip) if key == 'STAT'\n                else (key, '4s', self._decode_strip) if key == 'COUN'\n                else (key, '4s', self._decode_strip) if key == 'STD2'\n                else (key, 'i', self._make_date) if key == 'DATE'\n                else (key, 'i', self._make_time) if key == 'TIME'\n                else (key, 'i')\n                for key in keys]",
  "def _unpack_climate(self, sfcno):\n        \"\"\"Unpack a climate surface data file.\"\"\"\n        stations = []\n        for icol, col_head in enumerate(self.column_headers):\n            for irow, row_head in enumerate(self.row_headers):\n                if (irow, icol) not in sfcno:\n                    continue\n                station = {'STID': row_head.STID,\n                           'STNM': row_head.STNM,\n                           'SLAT': row_head.SLAT,\n                           'SLON': row_head.SLON,\n                           'SELV': row_head.SELV,\n                           'STAT': row_head.STAT,\n                           'COUN': row_head.COUN,\n                           'STD2': row_head.STD2,\n                           'SPRI': row_head.SPRI,\n                           'DATE': col_head.DATE,\n                           'TIME': col_head.TIME,\n                           }\n                for iprt, part in enumerate(self.parts):\n                    pointer = (self.prod_desc.data_block_ptr\n                               + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                               + (icol * self.prod_desc.parts + iprt))\n                    self._buffer.jump_to(self._start, _word_to_position(pointer))\n                    self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                    if not self.data_ptr:\n                        continue\n                    self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                    self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                    data_header = self._buffer.set_mark()\n                    self._buffer.jump_to(data_header,\n                                         _word_to_position(part.header_length + 1))\n                    lendat = self.data_header_length - part.header_length\n\n                    fmt_code = {\n                        DataTypes.real: 'f',\n                        DataTypes.realpack: 'i',\n                        DataTypes.character: 's',\n                    }.get(part.data_type)\n\n                    if fmt_code is None:\n                        raise NotImplementedError(f'No methods for data type {part.data_type}')\n\n                    if fmt_code == 's':\n                        lendat *= BYTES_PER_WORD\n\n                    packed_buffer = (\n                        self._buffer.read_struct(\n                            struct.Struct(f'{self.prefmt}{lendat}{fmt_code}')\n                        )\n                    )\n\n                    parameters = self.parameters[iprt]\n\n                    if part.data_type == DataTypes.realpack:\n                        unpacked = self._unpack_real(packed_buffer, parameters, lendat)\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = unpacked[iprm]\n                    elif part.data_type == DataTypes.character:\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = self._decode_strip(packed_buffer[iprm])\n                    else:\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = np.array(\n                                packed_buffer[iprm], dtype=np.float32\n                            )\n\n                stations.append(station)\n        return stations",
  "def _unpack_ship(self, sfcno):\n        \"\"\"Unpack ship (moving observation) surface data file.\"\"\"\n        stations = []\n        irow = 0\n        for icol, col_head in enumerate(self.column_headers):\n            if (irow, icol) not in sfcno:\n                continue\n            station = {'STID': col_head.STID,\n                       'STNM': col_head.STNM,\n                       'SLAT': col_head.SLAT,\n                       'SLON': col_head.SLON,\n                       'SELV': col_head.SELV,\n                       'STAT': col_head.STAT,\n                       'COUN': col_head.COUN,\n                       'STD2': col_head.STD2,\n                       'SPRI': col_head.SPRI,\n                       'DATE': col_head.DATE,\n                       'TIME': col_head.TIME,\n                       }\n            for iprt, part in enumerate(self.parts):\n                pointer = (self.prod_desc.data_block_ptr\n                           + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                           + (icol * self.prod_desc.parts + iprt))\n                self._buffer.jump_to(self._start, _word_to_position(pointer))\n                self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                if not self.data_ptr:\n                    continue\n                self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                data_header = self._buffer.set_mark()\n                self._buffer.jump_to(data_header,\n                                     _word_to_position(part.header_length + 1))\n                lendat = self.data_header_length - part.header_length\n\n                fmt_code = {\n                    DataTypes.real: 'f',\n                    DataTypes.realpack: 'i',\n                    DataTypes.character: 's',\n                }.get(part.data_type)\n\n                if fmt_code is None:\n                    raise NotImplementedError(f'No methods for data type {part.data_type}')\n\n                if fmt_code == 's':\n                    lendat *= BYTES_PER_WORD\n\n                packed_buffer = (\n                    self._buffer.read_struct(\n                        struct.Struct(f'{self.prefmt}{lendat}{fmt_code}')\n                    )\n                )\n\n                parameters = self.parameters[iprt]\n\n                if part.data_type == DataTypes.realpack:\n                    unpacked = self._unpack_real(packed_buffer, parameters, lendat)\n                    for iprm, param in enumerate(parameters['name']):\n                        station[param] = unpacked[iprm]\n                elif part.data_type == DataTypes.character:\n                    for iprm, param in enumerate(parameters['name']):\n                        station[param] = self._decode_strip(packed_buffer[iprm])\n                else:\n                    for iprm, param in enumerate(parameters['name']):\n                        station[param] = np.array(\n                            packed_buffer[iprm], dtype=np.float32\n                        )\n\n            stations.append(station)\n        return stations",
  "def _unpack_standard(self, sfcno):\n        \"\"\"Unpack a standard surface data file.\"\"\"\n        stations = []\n        for irow, row_head in enumerate(self.row_headers):\n            for icol, col_head in enumerate(self.column_headers):\n                if (irow, icol) not in sfcno:\n                    continue\n                station = {'STID': col_head.STID,\n                           'STNM': col_head.STNM,\n                           'SLAT': col_head.SLAT,\n                           'SLON': col_head.SLON,\n                           'SELV': col_head.SELV,\n                           'STAT': col_head.STAT,\n                           'COUN': col_head.COUN,\n                           'STD2': col_head.STD2,\n                           'SPRI': col_head.SPRI,\n                           'DATE': row_head.DATE,\n                           'TIME': row_head.TIME,\n                           }\n                for iprt, part in enumerate(self.parts):\n                    pointer = (self.prod_desc.data_block_ptr\n                               + (irow * self.prod_desc.columns * self.prod_desc.parts)\n                               + (icol * self.prod_desc.parts + iprt))\n                    self._buffer.jump_to(self._start, _word_to_position(pointer))\n                    self.data_ptr = self._buffer.read_int(4, self.endian, False)\n                    if not self.data_ptr:\n                        continue\n                    self._buffer.jump_to(self._start, _word_to_position(self.data_ptr))\n                    self.data_header_length = self._buffer.read_int(4, self.endian, False)\n                    data_header = self._buffer.set_mark()\n                    self._buffer.jump_to(data_header,\n                                         _word_to_position(part.header_length + 1))\n                    lendat = self.data_header_length - part.header_length\n\n                    fmt_code = {\n                        DataTypes.real: 'f',\n                        DataTypes.realpack: 'i',\n                        DataTypes.character: 's',\n                    }.get(part.data_type)\n\n                    if fmt_code is None:\n                        raise NotImplementedError(f'No methods for data type {part.data_type}')\n\n                    if fmt_code == 's':\n                        lendat *= BYTES_PER_WORD\n\n                    packed_buffer = (\n                        self._buffer.read_struct(\n                            struct.Struct(f'{self.prefmt}{lendat}{fmt_code}')\n                        )\n                    )\n\n                    parameters = self.parameters[iprt]\n\n                    if part.data_type == DataTypes.realpack:\n                        unpacked = self._unpack_real(packed_buffer, parameters, lendat)\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = unpacked[iprm]\n                    elif part.data_type == DataTypes.character:\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = self._decode_strip(packed_buffer[iprm])\n                    else:\n                        for iprm, param in enumerate(parameters['name']):\n                            station[param] = packed_buffer[iprm]\n\n                stations.append(station)\n        return stations",
  "def _decode_special_observation(station, missing=-9999):\n        \"\"\"Decode raw special obsrvation text.\"\"\"\n        text = station['SPCL']\n        dt = datetime.combine(station['DATE'], station['TIME'])\n        parsed = parse_metar(text, dt.year, dt.month)\n\n        station['TIME'] = parsed.date_time.time()\n        if math.nan in [parsed.altimeter, parsed.elevation, parsed.temperature]:\n            station['PMSL'] = missing\n        else:\n            station['PMSL'] = altimeter_to_sea_level_pressure(\n                units.Quantity(parsed.altimeter, 'inHg'),\n                units.Quantity(parsed.elevation, 'm'),\n                units.Quantity(parsed.temperature, 'degC')\n            ).to('hPa').m\n        station['ALTI'] = _check_nan(parsed.altimeter, missing)\n        station['TMPC'] = _check_nan(parsed.temperature, missing)\n        station['DWPC'] = _check_nan(parsed.dewpoint, missing)\n        station['SKNT'] = _check_nan(parsed.wind_speed, missing)\n        station['DRCT'] = _check_nan(float(parsed.wind_direction), missing)\n        station['GUST'] = _check_nan(parsed.wind_gust, missing)\n        station['WNUM'] = float(_wx_to_wnum(parsed.current_wx1, parsed.current_wx2,\n                                            parsed.current_wx3, missing))\n        station['CHC1'] = _convert_clouds(parsed.skyc1, parsed.skylev1, missing)\n        station['CHC2'] = _convert_clouds(parsed.skyc2, parsed.skylev2, missing)\n        station['CHC3'] = _convert_clouds(parsed.skyc3, parsed.skylev3, missing)\n        if math.isnan(parsed.visibility):\n            station['VSBY'] = missing\n        else:\n            station['VSBY'] = float(round(parsed.visibility / 1609.344))\n\n        return station",
  "def nearest_time(self, date_time, station_id=None, station_number=None,\n                     include_special=False):\n        \"\"\"Get nearest observation to given time for selected stations.\n\n        Parameters\n        ----------\n        date_time : `~datetime.datetime` or Sequence[datetime]\n            Valid/observed datetime of the surface station. Alternatively\n            object or a string with the format YYYYmmddHHMM.\n\n        station_id : str or Sequence[str]\n            Station ID of the surface station(s).\n\n        station_number : int or Sequence[int]\n            Station number of the surface station.\n\n        include_special : bool\n            If True, parse special observations that are stored\n            as raw METAR text. Default is False.\n\n        Returns\n        -------\n        list\n            List of dicts/JSONs for each surface station.\n\n        Notes\n        -----\n        One of either station_id or station_number must be used. If both\n        are present, station_id will take precedence.\n        \"\"\"\n        if isinstance(date_time, str):\n            date_time = datetime.strptime(date_time, '%Y%m%d%H%M')\n\n        if station_id is None and station_number is None:\n            raise ValueError('Must have either station_id or station_number')\n\n        if station_id is not None and station_number is not None:\n            station_number = None\n\n        if (station_id is not None\n           and (not isinstance(station_id, Iterable)\n                or isinstance(station_id, str))):\n            station_id = [station_id]\n            station_id = [c.upper() for c in station_id]\n\n        if station_number is not None and not isinstance(station_number, Iterable):\n            station_number = [station_number]\n            station_number = [int(sn) for sn in station_number]\n\n        time_matched = []\n        if station_id:\n            for stn in station_id:\n                matched = self.sfjson(station_id=stn, include_special=include_special)\n\n                nearest = min(\n                    matched,\n                    key=lambda d: abs(d['properties']['date_time'] - date_time)\n                )\n\n                time_matched.append(nearest)\n\n        if station_number:\n            for stn in station_id:\n                matched = self.sfjson(station_number=stn, include_special=include_special)\n\n                nearest = min(\n                    matched,\n                    key=lambda d: abs(d['properties']['date_time'] - date_time)\n                )\n\n                time_matched.append(nearest)\n\n        return time_matched",
  "def sfjson(self, station_id=None, station_number=None,\n               date_time=None, state=None, country=None,\n               include_special=False):\n        \"\"\"Select surface stations and output as list of JSON objects.\n\n        Subset the data by parameter values. The default is to not\n        subset and return the entire dataset.\n\n        Parameters\n        ----------\n        station_id : str or Sequence[str]\n            Station ID of the surface station.\n\n        station_number : int or Sequence[int]\n            Station number of the surface station.\n\n        date_time : `~datetime.datetime` or Sequence[datetime]\n            Valid datetime of the grid. Alternatively can be\n            a string with the format YYYYmmddHHMM.\n\n        state : str or Sequence[str]\n            State where surface station is located.\n\n        country : str or Sequence[str]\n            Country where surface station is located.\n\n        include_special : bool\n            If True, parse special observations that are stored\n            as raw METAR text. Default is False.\n\n        Returns\n        -------\n        list\n            List of dicts/JSONs for each surface station.\n        \"\"\"\n        if (station_id is not None\n           and (not isinstance(station_id, Iterable)\n                or isinstance(station_id, str))):\n            station_id = [station_id]\n            station_id = [c.upper() for c in station_id]\n\n        if station_number is not None and not isinstance(station_number, Iterable):\n            station_number = [station_number]\n            station_number = [int(sn) for sn in station_number]\n\n        if date_time is not None:\n            if (not isinstance(date_time, Iterable)\n               or isinstance(date_time, str)):\n                date_time = [date_time]\n            for i, dt in enumerate(date_time):\n                if isinstance(dt, str):\n                    date_time[i] = datetime.strptime(dt, '%Y%m%d%H%M')\n\n        if (state is not None\n           and (not isinstance(state, Iterable)\n                or isinstance(state, str))):\n            state = [state]\n            state = [s.upper() for s in state]\n\n        if (country is not None\n           and (not isinstance(country, Iterable)\n                or isinstance(country, str))):\n            country = [country]\n            country = [c.upper() for c in country]\n\n        # Figure out which columns to extract from the file\n        matched = self._sfinfo.copy()\n\n        if station_id is not None:\n            matched = filter(\n                lambda sfc: sfc if sfc.ID in station_id else False,\n                matched\n            )\n\n        if station_number is not None:\n            matched = filter(\n                lambda sfc: sfc if sfc.NUMBER in station_number else False,\n                matched\n            )\n\n        if date_time is not None:\n            matched = filter(\n                lambda sfc: sfc if sfc.DATTIM in date_time else False,\n                matched\n            )\n\n        if state is not None:\n            matched = filter(\n                lambda sfc: sfc if sfc.STATE in state else False,\n                matched\n            )\n\n        if country is not None:\n            matched = filter(\n                lambda sfc: sfc if sfc.COUNTRY in country else False,\n                matched\n            )\n\n        matched = list(matched)\n\n        if len(matched) < 1:\n            raise KeyError('No stations were matched with given parameters.')\n\n        sfcno = [(s.ROW, s.COL) for s in matched]\n\n        if self.surface_type == 'standard':\n            data = self._unpack_standard(sfcno)\n        elif self.surface_type == 'ship':\n            data = self._unpack_ship(sfcno)\n        elif self.surface_type == 'climate':\n            data = self._unpack_climate(sfcno)\n        else:\n            raise ValueError(f'Unknown surface data type: {self.surface_type}')\n\n        stnarr = []\n        for stn in data:\n            if 'SPCL' in stn and include_special:\n                stn = self._decode_special_observation(stn, self.prod_desc.missing_float)\n            props = {'date_time': datetime.combine(stn.pop('DATE'), stn.pop('TIME')),\n                     'station_id': stn.pop('STID') + stn.pop('STD2'),\n                     'station_number': stn.pop('STNM'),\n                     'longitude': stn.pop('SLON'),\n                     'latitude': stn.pop('SLAT'),\n                     'elevation': stn.pop('SELV'),\n                     'state': stn.pop('STAT'),\n                     'country': stn.pop('COUN'),\n                     'priority': stn.pop('SPRI')}\n            if stn:\n                vals = {name.lower(): ob for name, ob in stn.items()}\n                stnarr.append({'properties': props, 'values': vals})\n\n        return stnarr",
  "def open_as_needed(filename, mode='rb'):\n    \"\"\"Return a file-object given either a filename or an object.\n\n    Handles opening with the right class based on the file extension.\n\n    \"\"\"\n    # Handle file-like objects\n    if hasattr(filename, 'read'):\n        # See if the file object is really gzipped or bzipped.\n        lead = filename.read(4)\n\n        # If we can seek, seek back to start, otherwise read all the data into an\n        # in-memory file-like object.\n        try:\n            filename.seek(0)\n        except (AttributeError, UnsupportedOperation):\n            filename = BytesIO(lead + filename.read())\n\n        # If the leading bytes match one of the signatures, pass into the appropriate class.\n        with contextlib.suppress(AttributeError):\n            lead = lead.encode('ascii')\n        if lead.startswith(b'\\x1f\\x8b'):\n            filename = gzip.GzipFile(fileobj=filename)\n        elif lead.startswith(b'BZh'):\n            filename = bz2.BZ2File(filename)\n\n        return filename\n\n    # This will convert pathlib.Path instances to strings\n    filename = str(filename)\n\n    if filename.endswith('.bz2'):\n        return bz2.BZ2File(filename, mode)\n    elif filename.endswith('.gz'):\n        return gzip.GzipFile(filename, mode)\n    else:\n        kwargs = {'errors': 'surrogateescape'} if mode != 'rb' else {}\n        return open(filename, mode, **kwargs)",
  "class NamedStruct(Struct):\n    \"\"\"Parse bytes using :class:`Struct` but provide named fields.\"\"\"\n\n    def __init__(self, info, prefmt='', tuple_name=None):\n        \"\"\"Initialize the NamedStruct.\"\"\"\n        if tuple_name is None:\n            tuple_name = 'NamedStruct'\n        names, fmts = zip(*info)\n        self.converters = {}\n        conv_off = 0\n        for ind, i in enumerate(info):\n            if len(i) > 2:\n                self.converters[ind - conv_off] = i[-1]\n            elif not i[0]:  # Skip items with no name\n                conv_off += 1\n        self._tuple = namedtuple(tuple_name, ' '.join(n for n in names if n))\n        super().__init__(prefmt + ''.join(f for f in fmts if f))\n\n    def _create(self, items):\n        if self.converters:\n            items = list(items)\n            for ind, conv in self.converters.items():\n                items[ind] = conv(items[ind])\n            if len(items) < len(self._tuple._fields):\n                items.extend([None] * (len(self._tuple._fields) - len(items)))\n        return self.make_tuple(*items)\n\n    def make_tuple(self, *args, **kwargs):\n        \"\"\"Construct the underlying tuple from values.\"\"\"\n        return self._tuple(*args, **kwargs)\n\n    def unpack(self, s):\n        \"\"\"Parse bytes and return a namedtuple.\"\"\"\n        return self._create(super().unpack(s))\n\n    def unpack_from(self, buff, offset=0):\n        \"\"\"Read bytes from a buffer and return as a namedtuple.\"\"\"\n        return self._create(super().unpack_from(buff, offset))\n\n    def unpack_file(self, fobj):\n        \"\"\"Unpack the next bytes from a file object.\"\"\"\n        return self.unpack(fobj.read(self.size))\n\n    def pack(self, **kwargs):\n        \"\"\"Pack the arguments into bytes using the structure.\"\"\"\n        t = self.make_tuple(**kwargs)\n        return super().pack(*t)",
  "class DictStruct(Struct):\n    \"\"\"Parse bytes using :class:`Struct` but provide named fields using dictionary access.\"\"\"\n\n    def __init__(self, info, prefmt=''):\n        \"\"\"Initialize the DictStruct.\"\"\"\n        names, formats = zip(*info)\n\n        # Remove empty names\n        self._names = [n for n in names if n]\n\n        super().__init__(prefmt + ''.join(f for f in formats if f))\n\n    def _create(self, items):\n        return dict(zip(self._names, items))\n\n    def unpack(self, s):\n        \"\"\"Parse bytes and return a dict.\"\"\"\n        return self._create(super().unpack(s))\n\n    def unpack_from(self, buff, offset=0):\n        \"\"\"Unpack the next bytes from a file object.\"\"\"\n        return self._create(super().unpack_from(buff, offset))",
  "class Enum:\n    \"\"\"Map values to specific strings.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the mapping.\"\"\"\n        # Assign values for args in order starting at 0\n        self.val_map = dict(enumerate(args))\n\n        # Invert the kwargs dict so that we can map from value to name\n        self.val_map.update(zip(kwargs.values(), kwargs.keys()))\n\n    def __call__(self, val):\n        \"\"\"Map an integer to the string representation.\"\"\"\n        return self.val_map.get(val, f'Unknown ({val})')",
  "class Bits:\n    \"\"\"Breaks an integer into a specified number of True/False bits.\"\"\"\n\n    def __init__(self, num_bits):\n        \"\"\"Initialize the number of bits.\"\"\"\n        self._bits = range(num_bits)\n\n    def __call__(self, val):\n        \"\"\"Convert the integer to the list of True/False values.\"\"\"\n        return [bool((val >> i) & 0x1) for i in self._bits]",
  "class BitField:\n    \"\"\"Convert an integer to a string for each bit.\"\"\"\n\n    def __init__(self, *names):\n        \"\"\"Initialize the list of named bits.\"\"\"\n        self._names = names\n\n    def __call__(self, val):\n        \"\"\"Return a list with a string for each True bit in the integer.\"\"\"\n        if not val:\n            return None\n\n        bits = []\n        for n in self._names:\n            if val & 0x1:\n                bits.append(n)\n            val >>= 1\n            if not val:\n                break\n\n        # Return whole list if empty or multiple items, otherwise just single item\n        return bits[0] if len(bits) == 1 else bits",
  "class Array:\n    \"\"\"Use a Struct as a callable to unpack a bunch of bytes as a list.\"\"\"\n\n    def __init__(self, fmt):\n        \"\"\"Initialize the Struct unpacker.\"\"\"\n        self._struct = Struct(fmt)\n\n    def __call__(self, buf):\n        \"\"\"Perform the actual unpacking.\"\"\"\n        return list(self._struct.unpack(buf))",
  "class IOBuffer:\n    \"\"\"Holds bytes from a buffer to simplify parsing and random access.\"\"\"\n\n    def __init__(self, source):\n        \"\"\"Initialize the IOBuffer with the source data.\"\"\"\n        self._data = bytearray(source)\n        self.reset()\n\n    @classmethod\n    def fromfile(cls, fobj):\n        \"\"\"Initialize the IOBuffer with the contents of the file object.\"\"\"\n        return cls(fobj.read())\n\n    def reset(self):\n        \"\"\"Reset buffer back to initial state.\"\"\"\n        self._offset = 0\n        self.clear_marks()\n\n    def set_mark(self):\n        \"\"\"Mark the current location and return its id so that the buffer can return later.\"\"\"\n        self._bookmarks.append(self._offset)\n        return len(self._bookmarks) - 1\n\n    def jump_to(self, mark, offset=0):\n        \"\"\"Jump to a previously set mark.\"\"\"\n        self._offset = self._bookmarks[mark] + offset\n\n    def offset_from(self, mark):\n        \"\"\"Calculate the current offset relative to a marked location.\"\"\"\n        return self._offset - self._bookmarks[mark]\n\n    def clear_marks(self):\n        \"\"\"Clear all marked locations.\"\"\"\n        self._bookmarks = []\n\n    def splice(self, mark, newdata):\n        \"\"\"Replace the data after the marked location with the specified data.\"\"\"\n        self.jump_to(mark)\n        self._data = self._data[:self._offset] + bytearray(newdata)\n\n    def read_struct(self, struct_class):\n        \"\"\"Parse and return a structure from the current buffer offset.\"\"\"\n        struct = struct_class.unpack_from(memoryview(self._data), self._offset)\n        self.skip(struct_class.size)\n        return struct\n\n    def read_func(self, func, num_bytes=None):\n        \"\"\"Parse data from the current buffer offset using a function.\"\"\"\n        # only advance if func succeeds\n        res = func(self.get_next(num_bytes))\n        self.skip(num_bytes)\n        return res\n\n    def read_ascii(self, num_bytes=None):\n        \"\"\"Return the specified bytes as ascii-formatted text.\"\"\"\n        return self.read(num_bytes).decode('ascii')\n\n    def read_binary(self, num, item_type='B'):\n        \"\"\"Parse the current buffer offset as the specified code.\"\"\"\n        if 'B' in item_type:\n            return self.read(num)\n\n        if item_type[0] in ('@', '=', '<', '>', '!'):\n            order = item_type[0]\n            item_type = item_type[1:]\n        else:\n            order = '@'\n\n        return list(self.read_struct(Struct(order + f'{int(num):d}' + item_type)))\n\n    def read_int(self, size, endian, signed):\n        \"\"\"Parse the current buffer offset as the specified integer code.\"\"\"\n        return int.from_bytes(self.read(size), endian, signed=signed)\n\n    def read_array(self, count, dtype):\n        \"\"\"Read an array of values from the buffer.\"\"\"\n        ret = np.frombuffer(self._data, offset=self._offset, dtype=dtype, count=count)\n        self.skip(ret.nbytes)\n        return ret\n\n    def read(self, num_bytes=None):\n        \"\"\"Read and return the specified bytes from the buffer.\"\"\"\n        res = self.get_next(num_bytes)\n        self.skip(len(res))\n        return res\n\n    def get_next(self, num_bytes=None):\n        \"\"\"Get the next bytes in the buffer without modifying the offset.\"\"\"\n        if num_bytes is None:\n            return self._data[self._offset:]\n        else:\n            return self._data[self._offset:self._offset + num_bytes]\n\n    def skip(self, num_bytes):\n        \"\"\"Jump the ahead the specified bytes in the buffer.\"\"\"\n        if num_bytes is None:\n            self._offset = len(self._data)\n        else:\n            self._offset += num_bytes\n\n    def check_remains(self, num_bytes):\n        \"\"\"Check that the number of bytes specified remains in the buffer.\"\"\"\n        return len(self._data[self._offset:]) == num_bytes\n\n    def truncate(self, num_bytes):\n        \"\"\"Remove the specified number of bytes from the end of the buffer.\"\"\"\n        self._data = self._data[:-num_bytes]\n\n    def at_end(self):\n        \"\"\"Return whether the buffer has reached the end of data.\"\"\"\n        return self._offset >= len(self._data)\n\n    def __getitem__(self, item):\n        \"\"\"Return the data at the specified location.\"\"\"\n        return self._data[item]\n\n    def __str__(self):\n        \"\"\"Return a string representation of the IOBuffer.\"\"\"\n        return f'Size: {len(self._data)} Offset: {self._offset}'\n\n    def __len__(self):\n        \"\"\"Return the amount of data in the buffer.\"\"\"\n        return len(self._data)",
  "def zlib_decompress_all_frames(data):\n    \"\"\"Decompress all frames of zlib-compressed bytes.\n\n    Repeatedly tries to decompress `data` until all data are decompressed, or decompression\n    fails. This will skip over bytes that are not compressed with zlib.\n\n    Parameters\n    ----------\n    data : bytearray or bytes\n        Binary data compressed using zlib.\n\n    Returns\n    -------\n        bytearray\n            All decompressed bytes\n\n    \"\"\"\n    frames = bytearray()\n    data = bytes(data)\n    while data:\n        decomp = zlib.decompressobj()\n        try:\n            frames += decomp.decompress(data)\n            data = decomp.unused_data\n            log.debug('Decompressed zlib frame (total %d bytes). %d bytes remain.',\n                      len(frames), len(data))\n        except zlib.error:\n            log.debug('Remaining %d bytes are not zlib compressed.', len(data))\n            frames.extend(data)\n            break\n    return frames",
  "def bits_to_code(val):\n    \"\"\"Convert the number of bits to the proper code for unpacking.\"\"\"\n    if val == 8:\n        return 'B'\n    elif val == 16:\n        return 'H'\n    else:\n        log.warning('Unsupported bit size: %s. Returning \"B\"', val)\n        return 'B'",
  "def __init__(self, info, prefmt='', tuple_name=None):\n        \"\"\"Initialize the NamedStruct.\"\"\"\n        if tuple_name is None:\n            tuple_name = 'NamedStruct'\n        names, fmts = zip(*info)\n        self.converters = {}\n        conv_off = 0\n        for ind, i in enumerate(info):\n            if len(i) > 2:\n                self.converters[ind - conv_off] = i[-1]\n            elif not i[0]:  # Skip items with no name\n                conv_off += 1\n        self._tuple = namedtuple(tuple_name, ' '.join(n for n in names if n))\n        super().__init__(prefmt + ''.join(f for f in fmts if f))",
  "def _create(self, items):\n        if self.converters:\n            items = list(items)\n            for ind, conv in self.converters.items():\n                items[ind] = conv(items[ind])\n            if len(items) < len(self._tuple._fields):\n                items.extend([None] * (len(self._tuple._fields) - len(items)))\n        return self.make_tuple(*items)",
  "def make_tuple(self, *args, **kwargs):\n        \"\"\"Construct the underlying tuple from values.\"\"\"\n        return self._tuple(*args, **kwargs)",
  "def unpack(self, s):\n        \"\"\"Parse bytes and return a namedtuple.\"\"\"\n        return self._create(super().unpack(s))",
  "def unpack_from(self, buff, offset=0):\n        \"\"\"Read bytes from a buffer and return as a namedtuple.\"\"\"\n        return self._create(super().unpack_from(buff, offset))",
  "def unpack_file(self, fobj):\n        \"\"\"Unpack the next bytes from a file object.\"\"\"\n        return self.unpack(fobj.read(self.size))",
  "def pack(self, **kwargs):\n        \"\"\"Pack the arguments into bytes using the structure.\"\"\"\n        t = self.make_tuple(**kwargs)\n        return super().pack(*t)",
  "def __init__(self, info, prefmt=''):\n        \"\"\"Initialize the DictStruct.\"\"\"\n        names, formats = zip(*info)\n\n        # Remove empty names\n        self._names = [n for n in names if n]\n\n        super().__init__(prefmt + ''.join(f for f in formats if f))",
  "def _create(self, items):\n        return dict(zip(self._names, items))",
  "def unpack(self, s):\n        \"\"\"Parse bytes and return a dict.\"\"\"\n        return self._create(super().unpack(s))",
  "def unpack_from(self, buff, offset=0):\n        \"\"\"Unpack the next bytes from a file object.\"\"\"\n        return self._create(super().unpack_from(buff, offset))",
  "def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the mapping.\"\"\"\n        # Assign values for args in order starting at 0\n        self.val_map = dict(enumerate(args))\n\n        # Invert the kwargs dict so that we can map from value to name\n        self.val_map.update(zip(kwargs.values(), kwargs.keys()))",
  "def __call__(self, val):\n        \"\"\"Map an integer to the string representation.\"\"\"\n        return self.val_map.get(val, f'Unknown ({val})')",
  "def __init__(self, num_bits):\n        \"\"\"Initialize the number of bits.\"\"\"\n        self._bits = range(num_bits)",
  "def __call__(self, val):\n        \"\"\"Convert the integer to the list of True/False values.\"\"\"\n        return [bool((val >> i) & 0x1) for i in self._bits]",
  "def __init__(self, *names):\n        \"\"\"Initialize the list of named bits.\"\"\"\n        self._names = names",
  "def __call__(self, val):\n        \"\"\"Return a list with a string for each True bit in the integer.\"\"\"\n        if not val:\n            return None\n\n        bits = []\n        for n in self._names:\n            if val & 0x1:\n                bits.append(n)\n            val >>= 1\n            if not val:\n                break\n\n        # Return whole list if empty or multiple items, otherwise just single item\n        return bits[0] if len(bits) == 1 else bits",
  "def __init__(self, fmt):\n        \"\"\"Initialize the Struct unpacker.\"\"\"\n        self._struct = Struct(fmt)",
  "def __call__(self, buf):\n        \"\"\"Perform the actual unpacking.\"\"\"\n        return list(self._struct.unpack(buf))",
  "def __init__(self, source):\n        \"\"\"Initialize the IOBuffer with the source data.\"\"\"\n        self._data = bytearray(source)\n        self.reset()",
  "def fromfile(cls, fobj):\n        \"\"\"Initialize the IOBuffer with the contents of the file object.\"\"\"\n        return cls(fobj.read())",
  "def reset(self):\n        \"\"\"Reset buffer back to initial state.\"\"\"\n        self._offset = 0\n        self.clear_marks()",
  "def set_mark(self):\n        \"\"\"Mark the current location and return its id so that the buffer can return later.\"\"\"\n        self._bookmarks.append(self._offset)\n        return len(self._bookmarks) - 1",
  "def jump_to(self, mark, offset=0):\n        \"\"\"Jump to a previously set mark.\"\"\"\n        self._offset = self._bookmarks[mark] + offset",
  "def offset_from(self, mark):\n        \"\"\"Calculate the current offset relative to a marked location.\"\"\"\n        return self._offset - self._bookmarks[mark]",
  "def clear_marks(self):\n        \"\"\"Clear all marked locations.\"\"\"\n        self._bookmarks = []",
  "def splice(self, mark, newdata):\n        \"\"\"Replace the data after the marked location with the specified data.\"\"\"\n        self.jump_to(mark)\n        self._data = self._data[:self._offset] + bytearray(newdata)",
  "def read_struct(self, struct_class):\n        \"\"\"Parse and return a structure from the current buffer offset.\"\"\"\n        struct = struct_class.unpack_from(memoryview(self._data), self._offset)\n        self.skip(struct_class.size)\n        return struct",
  "def read_func(self, func, num_bytes=None):\n        \"\"\"Parse data from the current buffer offset using a function.\"\"\"\n        # only advance if func succeeds\n        res = func(self.get_next(num_bytes))\n        self.skip(num_bytes)\n        return res",
  "def read_ascii(self, num_bytes=None):\n        \"\"\"Return the specified bytes as ascii-formatted text.\"\"\"\n        return self.read(num_bytes).decode('ascii')",
  "def read_binary(self, num, item_type='B'):\n        \"\"\"Parse the current buffer offset as the specified code.\"\"\"\n        if 'B' in item_type:\n            return self.read(num)\n\n        if item_type[0] in ('@', '=', '<', '>', '!'):\n            order = item_type[0]\n            item_type = item_type[1:]\n        else:\n            order = '@'\n\n        return list(self.read_struct(Struct(order + f'{int(num):d}' + item_type)))",
  "def read_int(self, size, endian, signed):\n        \"\"\"Parse the current buffer offset as the specified integer code.\"\"\"\n        return int.from_bytes(self.read(size), endian, signed=signed)",
  "def read_array(self, count, dtype):\n        \"\"\"Read an array of values from the buffer.\"\"\"\n        ret = np.frombuffer(self._data, offset=self._offset, dtype=dtype, count=count)\n        self.skip(ret.nbytes)\n        return ret",
  "def read(self, num_bytes=None):\n        \"\"\"Read and return the specified bytes from the buffer.\"\"\"\n        res = self.get_next(num_bytes)\n        self.skip(len(res))\n        return res",
  "def get_next(self, num_bytes=None):\n        \"\"\"Get the next bytes in the buffer without modifying the offset.\"\"\"\n        if num_bytes is None:\n            return self._data[self._offset:]\n        else:\n            return self._data[self._offset:self._offset + num_bytes]",
  "def skip(self, num_bytes):\n        \"\"\"Jump the ahead the specified bytes in the buffer.\"\"\"\n        if num_bytes is None:\n            self._offset = len(self._data)\n        else:\n            self._offset += num_bytes",
  "def check_remains(self, num_bytes):\n        \"\"\"Check that the number of bytes specified remains in the buffer.\"\"\"\n        return len(self._data[self._offset:]) == num_bytes",
  "def truncate(self, num_bytes):\n        \"\"\"Remove the specified number of bytes from the end of the buffer.\"\"\"\n        self._data = self._data[:-num_bytes]",
  "def at_end(self):\n        \"\"\"Return whether the buffer has reached the end of data.\"\"\"\n        return self._offset >= len(self._data)",
  "def __getitem__(self, item):\n        \"\"\"Return the data at the specified location.\"\"\"\n        return self._data[item]",
  "def __str__(self):\n        \"\"\"Return a string representation of the IOBuffer.\"\"\"\n        return f'Size: {len(self._data)} Offset: {self._offset}'",
  "def __len__(self):\n        \"\"\"Return the amount of data in the buffer.\"\"\"\n        return len(self._data)",
  "def _decode_coords(coordinates):\n    \"\"\"Turn a string of coordinates from WPC coded surface bulletin into a lon/lat tuple.\n\n    Parameters\n    ----------\n    coordinates : str\n        A string of numbers that can be converted into a lon/lat tuple\n\n    Returns\n    -------\n    (lon, lat) : tuple\n        Longitude and latitude parsed from `coordinates`\n\n    Notes\n    -----\n    In the WPC coded surface bulletin, latitude and longitude are given in degrees north and\n    degrees west, respectively. Therefore, this function always returns latitude as a positive\n    number and longitude as a negative number.\n\n    Examples\n    --------\n    >>> _decode_coords('4731193')\n    (-119.3, 47.3)\n\n    \"\"\"\n    # Based on the number of digits, find the correct place to split between lat and lon\n    # Hires bulletins provide 7 digits for coordinates; regular bulletins provide 4 or 5 digits\n    split_pos = int(len(coordinates) / 2)\n    lat, lon = coordinates[:split_pos], coordinates[split_pos:]\n\n    # Insert decimal point at the correct place and convert to float\n    lat = float(f'{lat[:2]}.{lat[2:]}')\n    lon = -float(f'{lon[:3]}.{lon[3:]}')\n\n    return lon, lat",
  "def _regroup_lines(iterable):\n    starting_num = re.compile('^[0-9]')\n    lines = list(iterable)[::-1]\n    while lines:\n        line = lines.pop()\n        if not line.strip():\n            continue\n        parts = line.split()\n        while lines and starting_num.match(lines[-1]):\n            parts.extend(lines.pop().split())\n        yield parts",
  "def parse_wpc_surface_bulletin(bulletin, year=None):\n    \"\"\"Parse a coded surface bulletin from NWS WPC into a Pandas DataFrame.\n\n    Parameters\n    ----------\n    bulletin : str or file-like object\n        If str, the name of the file to be opened. If `bulletin` is a file-like object,\n        this will be read from directly.\n\n    Returns\n    -------\n    dataframe : pandas.DataFrame\n        A `DataFrame` where each row represents a pressure center or front. The `DataFrame`\n        has four columns: 'valid', 'feature', 'strength', and 'geometry'.\n    year : int\n        Year to assume when parsing the timestamp from the bulletin. Defaults to `None`,\n        which results in the parser trying to find a year in the product header; if this\n        search fails, the current year is assumed.\n\n    \"\"\"\n    from shapely.geometry import LineString, Point\n\n    # Create list with lines of text from file\n    with contextlib.closing(open_as_needed(bulletin)) as file:\n        text = file.read().decode('utf-8')\n\n    parsed_text = []\n    valid_time = datetime.utcnow()\n    for parts in _regroup_lines(text.splitlines()):\n        # A single file may have multiple sets of data that are valid at different times. Set\n        # the valid_time string that will correspond to all the following lines parsed, until\n        # the next valid_time is found.\n        if parts[0] in ('VALID', 'SURFACE PROG VALID'):\n            dtstr = parts[-1]\n            valid_time = valid_time.replace(year=year or valid_time.year, month=int(dtstr[:2]),\n                                            day=int(dtstr[2:4]), hour=int(dtstr[4:6]),\n                                            minute=0, second=0, microsecond=0)\n        else:\n            feature, *info = parts\n            if feature in {'HIGHS', 'LOWS'}:\n                # For each pressure center, add its data as a new row\n                # While ideally these occur in pairs, some bulletins have had multiple\n                # locations for a single center strength value. So instead walk one at a time\n                # and keep track of the most recent strength.\n                strength = np.nan\n                for item in info:\n                    if len(item) <= 4 and item[0] in {'8', '9', '1'}:\n                        strength = int(item)\n                    else:\n                        parsed_text.append((valid_time, feature.rstrip('S'), strength,\n                                            Point(_decode_coords(item))))\n            elif feature in {'WARM', 'COLD', 'STNRY', 'OCFNT', 'TROF'}:\n                # Some bulletins include 'WK', 'MDT', or 'STG' to indicate the front's\n                # strength. If present, separate it from the rest of the info, which gives the\n                # position of the front.\n                if info[0][0] in string.ascii_letters:\n                    strength, *boundary = info\n                else:\n                    strength, boundary = np.nan, info\n\n                # Create a list of Points and create Line from points, if possible\n                boundary = [Point(_decode_coords(point)) for point in boundary]\n                boundary = LineString(boundary) if len(boundary) > 1 else boundary[0]\n\n                # Add new row in the data for each front\n                parsed_text.append((valid_time, feature, strength, boundary))\n            # Look for a year at the end of the line (from the product header)\n            elif (year is None and len(info) >= 2 and re.match(r'\\d{4}', info[-1])\n                  and re.match(r'\\d{2}', info[-2])):\n                with contextlib.suppress(ValueError):\n                    year = int(info[-1])\n\n    return pd.DataFrame(parsed_text, columns=['valid', 'feature', 'strength', 'geometry'])",
  "class TreeNode(object):\n    def __init__(self, text, offset, elements):\n        self.text = text\n        self.offset = offset\n        self.elements = elements\n\n    def __iter__(self):\n        for el in self.elements:\n            yield el",
  "class TreeNode1(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode1, self).__init__(text, offset, elements)\n        self.metar = elements[0]\n        self.siteid = elements[1]\n        self.datetime = elements[2]\n        self.auto = elements[3]\n        self.wind = elements[4]\n        self.vis = elements[5]\n        self.run = elements[6]\n        self.curwx = elements[7]\n        self.skyc = elements[8]\n        self.temp_dewp = elements[9]\n        self.altim = elements[10]\n        self.remarks = elements[11]\n        self.end = elements[12]",
  "class TreeNode2(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode2, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "class TreeNode3(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode3, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "class TreeNode4(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode4, self).__init__(text, offset, elements)\n        self.wind_dir = elements[1]\n        self.wind_spd = elements[2]\n        self.gust = elements[3]",
  "class TreeNode5(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode5, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "class TreeNode6(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode6, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "class TreeNode7(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode7, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "class TreeNode8(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode8, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "class TreeNode9(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode9, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "class TreeNode10(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode10, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "class TreeNode11(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode11, self).__init__(text, offset, elements)\n        self.sep = elements[0]\n        self.wx = elements[1]",
  "class TreeNode12(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode12, self).__init__(text, offset, elements)\n        self.sep = elements[0]\n        self.cover = elements[1]",
  "class TreeNode13(TreeNode):\n    def __init__(self, text, offset, elements):\n        super(TreeNode13, self).__init__(text, offset, elements)\n        self.sep = elements[0]\n        self.temp = elements[2]\n        self.dewp = elements[4]",
  "class Grammar(object):\n    REGEX_1 = re.compile('^[0-9A-Z]')\n    REGEX_2 = re.compile('^[0-9A-Z]')\n    REGEX_3 = re.compile('^[0-9A-Z]')\n    REGEX_4 = re.compile('^[0-9A-Z]')\n    REGEX_5 = re.compile('^[\\\\d]')\n    REGEX_6 = re.compile('^[\\\\d]')\n    REGEX_7 = re.compile('^[\\\\d]')\n    REGEX_8 = re.compile('^[\\\\d]')\n    REGEX_9 = re.compile('^[\\\\d]')\n    REGEX_10 = re.compile('^[\\\\d]')\n    REGEX_11 = re.compile('^[\\\\d]')\n    REGEX_12 = re.compile('^[\\\\d]')\n    REGEX_13 = re.compile('^[\\\\d]')\n    REGEX_14 = re.compile('^[\\\\d]')\n    REGEX_15 = re.compile('^[\\\\d]')\n    REGEX_16 = re.compile('^[\\\\d]')\n    REGEX_17 = re.compile('^[\\\\d]')\n    REGEX_18 = re.compile('^[\\\\d]')\n    REGEX_19 = re.compile('^[\\\\d]')\n    REGEX_20 = re.compile('^[\\\\d]')\n    REGEX_21 = re.compile('^[\\\\d]')\n    REGEX_22 = re.compile('^[\\\\d]')\n    REGEX_23 = re.compile('^[\\\\d]')\n    REGEX_24 = re.compile('^[\\\\d]')\n    REGEX_25 = re.compile('^[\\\\d]')\n    REGEX_26 = re.compile('^[\\\\d]')\n    REGEX_27 = re.compile('^[\\\\d]')\n    REGEX_28 = re.compile('^[\\\\d]')\n    REGEX_29 = re.compile('^[\\\\d]')\n    REGEX_30 = re.compile('^[\\\\d]')\n    REGEX_31 = re.compile('^[\\\\d]')\n    REGEX_32 = re.compile('^[\\\\d]')\n    REGEX_33 = re.compile('^[\\\\d]')\n    REGEX_34 = re.compile('^[NSEW]')\n    REGEX_35 = re.compile('^[NSEW]')\n    REGEX_36 = re.compile('^[LRC]')\n    REGEX_37 = re.compile('^[\\\\d]')\n    REGEX_38 = re.compile('^[\\\\d]')\n    REGEX_39 = re.compile('^[LRC]')\n    REGEX_40 = re.compile('^[\\\\d]')\n    REGEX_41 = re.compile('^[\\\\d]')\n    REGEX_42 = re.compile('^[\\\\d]')\n    REGEX_43 = re.compile('^[\\\\d]')\n    REGEX_44 = re.compile('^[\"M\" / \"P\"]')\n    REGEX_45 = re.compile('^[\\\\d]')\n    REGEX_46 = re.compile('^[\\\\d]')\n    REGEX_47 = re.compile('^[\\\\d]')\n    REGEX_48 = re.compile('^[\\\\d]')\n    REGEX_49 = re.compile('^[UDN]')\n    REGEX_50 = re.compile('^[-+]')\n    REGEX_51 = re.compile('^[\\\\d]')\n    REGEX_52 = re.compile('^[M]')\n    REGEX_53 = re.compile('^[\\\\d]')\n    REGEX_54 = re.compile('^[\\\\d]')\n    REGEX_55 = re.compile('^[M]')\n    REGEX_56 = re.compile('^[\\\\d]')\n    REGEX_57 = re.compile('^[\\\\d]')\n    REGEX_58 = re.compile('^[\"Q\" / \"A\"]')\n    REGEX_59 = re.compile('^[\\\\d]')\n    REGEX_60 = re.compile('^[\\\\d]')\n    REGEX_61 = re.compile('^[\\\\d]')\n    REGEX_62 = re.compile('^[\\\\d]')\n\n    def _read_ob(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['ob'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_metar()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            address2 = self._read_siteid()\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                address3 = self._read_datetime()\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    address4 = self._read_auto()\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        address5 = self._read_wind()\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            address6 = self._read_vis()\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                                address7 = FAILURE\n                                address7 = self._read_run()\n                                if address7 is not FAILURE:\n                                    elements0.append(address7)\n                                    address8 = FAILURE\n                                    address8 = self._read_curwx()\n                                    if address8 is not FAILURE:\n                                        elements0.append(address8)\n                                        address9 = FAILURE\n                                        address9 = self._read_skyc()\n                                        if address9 is not FAILURE:\n                                            elements0.append(address9)\n                                            address10 = FAILURE\n                                            address10 = self._read_temp_dewp()\n                                            if address10 is not FAILURE:\n                                                elements0.append(address10)\n                                                address11 = FAILURE\n                                                address11 = self._read_altim()\n                                                if address11 is not FAILURE:\n                                                    elements0.append(address11)\n                                                    address12 = FAILURE\n                                                    address12 = self._read_remarks()\n                                                    if address12 is not FAILURE:\n                                                        elements0.append(address12)\n                                                        address13 = FAILURE\n                                                        address13 = self._read_end()\n                                                        if address13 is not FAILURE:\n                                                            elements0.append(address13)\n                                                        else:\n                                                            elements0 = None\n                                                            self._offset = index1\n                                                    else:\n                                                        elements0 = None\n                                                        self._offset = index1\n                                                else:\n                                                    elements0 = None\n                                                    self._offset = index1\n                                            else:\n                                                elements0 = None\n                                                self._offset = index1\n                                        else:\n                                            elements0 = None\n                                            self._offset = index1\n                                    else:\n                                        elements0 = None\n                                        self._offset = index1\n                                else:\n                                    elements0 = None\n                                    self._offset = index1\n                            else:\n                                elements0 = None\n                                self._offset = index1\n                        else:\n                            elements0 = None\n                            self._offset = index1\n                    else:\n                        elements0 = None\n                        self._offset = index1\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode1(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['ob'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_metar(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['metar'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        index2 = self._offset\n        chunk0, max0 = None, self._offset + 4\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 == 'COR ':\n            address1 = TreeNode(self._input[self._offset:self._offset + 4], self._offset, [])\n            self._offset = self._offset + 4\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::metar', '\"COR \"'))\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index2:index2], index2, [])\n            self._offset = index2\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3 = self._offset\n            index4 = self._offset\n            chunk1, max1 = None, self._offset + 5\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 == 'METAR':\n                address2 = TreeNode(self._input[self._offset:self._offset + 5], self._offset, [])\n                self._offset = self._offset + 5\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::metar', '\"METAR\"'))\n            if address2 is FAILURE:\n                self._offset = index4\n                chunk2, max2 = None, self._offset + 5\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 == 'SPECI':\n                    address2 = TreeNode(self._input[self._offset:self._offset + 5], self._offset, [])\n                    self._offset = self._offset + 5\n                else:\n                    address2 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::metar', '\"SPECI\"'))\n                if address2 is FAILURE:\n                    self._offset = index4\n            if address2 is FAILURE:\n                address2 = TreeNode(self._input[index3:index3], index3, [])\n                self._offset = index3\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                index5 = self._offset\n                address3 = self._read_auto()\n                if address3 is FAILURE:\n                    address3 = TreeNode(self._input[index5:index5], index5, [])\n                    self._offset = index5\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['metar'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_sep(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['sep'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0, address1 = self._offset, [], None\n        while True:\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 == ' ':\n                address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address1 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::sep', '\" \"'))\n            if address1 is not FAILURE:\n                elements0.append(address1)\n            else:\n                break\n        if len(elements0) >= 1:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        else:\n            address0 = FAILURE\n        self._cache['sep'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_siteid(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['siteid'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        index2 = self._offset\n        address1 = self._read_sep()\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index2:index2], index2, [])\n            self._offset = index2\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 is not None and Grammar.REGEX_1.search(chunk0):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::siteid', '[0-9A-Z]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_2.search(chunk1):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::siteid', '[0-9A-Z]'))\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    chunk2, max2 = None, self._offset + 1\n                    if max2 <= self._input_size:\n                        chunk2 = self._input[self._offset:max2]\n                    if chunk2 is not None and Grammar.REGEX_3.search(chunk2):\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::siteid', '[0-9A-Z]'))\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        chunk3, max3 = None, self._offset + 1\n                        if max3 <= self._input_size:\n                            chunk3 = self._input[self._offset:max3]\n                        if chunk3 is not None and Grammar.REGEX_4.search(chunk3):\n                            address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::siteid', '[0-9A-Z]'))\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                        else:\n                            elements0 = None\n                            self._offset = index1\n                    else:\n                        elements0 = None\n                        self._offset = index1\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['siteid'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_datetime(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['datetime'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index2, elements1, address3 = self._offset, [], None\n            while True:\n                chunk0, max0 = None, self._offset + 1\n                if max0 <= self._input_size:\n                    chunk0 = self._input[self._offset:max0]\n                if chunk0 is not None and Grammar.REGEX_5.search(chunk0):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::datetime', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                else:\n                    break\n            if len(elements1) >= 1:\n                address2 = TreeNode(self._input[index2:self._offset], index2, elements1)\n                self._offset = self._offset\n            else:\n                address2 = FAILURE\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address4 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 == 'Z':\n                    address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address4 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::datetime', '\"Z\"'))\n                if address4 is not FAILURE:\n                    elements0.append(address4)\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode2(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['datetime'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_auto(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['auto'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0, address1 = self._offset, [], None\n        while True:\n            index3, elements1 = self._offset, []\n            address2 = FAILURE\n            address2 = self._read_sep()\n            if address2 is not FAILURE:\n                elements1.append(address2)\n                address3 = FAILURE\n                index4 = self._offset\n                chunk0, max0 = None, self._offset + 4\n                if max0 <= self._input_size:\n                    chunk0 = self._input[self._offset:max0]\n                if chunk0 == 'AUTO':\n                    address3 = TreeNode(self._input[self._offset:self._offset + 4], self._offset, [])\n                    self._offset = self._offset + 4\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::auto', '\"AUTO\"'))\n                if address3 is FAILURE:\n                    self._offset = index4\n                    chunk1, max1 = None, self._offset + 3\n                    if max1 <= self._input_size:\n                        chunk1 = self._input[self._offset:max1]\n                    if chunk1 == 'COR':\n                        address3 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                        self._offset = self._offset + 3\n                    else:\n                        address3 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::auto', '\"COR\"'))\n                    if address3 is FAILURE:\n                        self._offset = index4\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                else:\n                    elements1 = None\n                    self._offset = index3\n            else:\n                elements1 = None\n                self._offset = index3\n            if elements1 is None:\n                address1 = FAILURE\n            else:\n                address1 = TreeNode3(self._input[index3:self._offset], index3, elements1)\n                self._offset = self._offset\n            if address1 is not FAILURE:\n                elements0.append(address1)\n            else:\n                break\n        if len(elements0) >= 1:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        else:\n            address0 = FAILURE\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['auto'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_wind(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['wind'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        index3 = self._offset\n        address1 = self._read_sep()\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index3:index3], index3, [])\n            self._offset = index3\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            address2 = self._read_wind_dir()\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                address3 = self._read_wind_spd()\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    address4 = self._read_gust()\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        index4 = self._offset\n                        chunk0, max0 = None, self._offset + 2\n                        if max0 <= self._input_size:\n                            chunk0 = self._input[self._offset:max0]\n                        if chunk0 == 'KT':\n                            address5 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                            self._offset = self._offset + 2\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::wind', '\"KT\"'))\n                        if address5 is FAILURE:\n                            self._offset = index4\n                            chunk1, max1 = None, self._offset + 3\n                            if max1 <= self._input_size:\n                                chunk1 = self._input[self._offset:max1]\n                            if chunk1 == 'MPS':\n                                address5 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                                self._offset = self._offset + 3\n                            else:\n                                address5 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::wind', '\"MPS\"'))\n                            if address5 is FAILURE:\n                                self._offset = index4\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            index5 = self._offset\n                            address6 = self._read_varwind()\n                            if address6 is FAILURE:\n                                address6 = TreeNode(self._input[index5:index5], index5, [])\n                                self._offset = index5\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                            else:\n                                elements0 = None\n                                self._offset = index2\n                        else:\n                            elements0 = None\n                            self._offset = index2\n                    else:\n                        elements0 = None\n                        self._offset = index2\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode4(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['wind'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_wind_dir(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['wind_dir'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2 = self._offset\n        index3, elements0 = self._offset, []\n        address1 = FAILURE\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 is not None and Grammar.REGEX_6.search(chunk0):\n            address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::wind_dir', '[\\\\d]'))\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk1, max1 = None, self._offset + 1\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 is not None and Grammar.REGEX_7.search(chunk1):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wind_dir', '[\\\\d]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                chunk2, max2 = None, self._offset + 1\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 is not None and Grammar.REGEX_8.search(chunk2):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::wind_dir', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                else:\n                    elements0 = None\n                    self._offset = index3\n            else:\n                elements0 = None\n                self._offset = index3\n        else:\n            elements0 = None\n            self._offset = index3\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index3:self._offset], index3, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            self._offset = index2\n            chunk3, max3 = None, self._offset + 3\n            if max3 <= self._input_size:\n                chunk3 = self._input[self._offset:max3]\n            if chunk3 == 'VAR':\n                address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                self._offset = self._offset + 3\n            else:\n                address0 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wind_dir', '\\'VAR\\''))\n            if address0 is FAILURE:\n                self._offset = index2\n                chunk4, max4 = None, self._offset + 3\n                if max4 <= self._input_size:\n                    chunk4 = self._input[self._offset:max4]\n                if chunk4 == 'VRB':\n                    address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                    self._offset = self._offset + 3\n                else:\n                    address0 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::wind_dir', '\\'VRB\\''))\n                if address0 is FAILURE:\n                    self._offset = index2\n                    chunk5, max5 = None, self._offset + 3\n                    if max5 <= self._input_size:\n                        chunk5 = self._input[self._offset:max5]\n                    if chunk5 == '///':\n                        address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                        self._offset = self._offset + 3\n                    else:\n                        address0 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::wind_dir', '\"///\"'))\n                    if address0 is FAILURE:\n                        self._offset = index2\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['wind_dir'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_wind_spd(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['wind_spd'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2 = self._offset\n        index3, elements0 = self._offset, []\n        address1 = FAILURE\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 is not None and Grammar.REGEX_9.search(chunk0):\n            address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::wind_spd', '[\\\\d]'))\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk1, max1 = None, self._offset + 1\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 is not None and Grammar.REGEX_10.search(chunk1):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wind_spd', '[\\\\d]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                index4 = self._offset\n                chunk2, max2 = None, self._offset + 1\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 is not None and Grammar.REGEX_11.search(chunk2):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::wind_spd', '[\\\\d]'))\n                if address3 is FAILURE:\n                    address3 = TreeNode(self._input[index4:index4], index4, [])\n                    self._offset = index4\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                else:\n                    elements0 = None\n                    self._offset = index3\n            else:\n                elements0 = None\n                self._offset = index3\n        else:\n            elements0 = None\n            self._offset = index3\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index3:self._offset], index3, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            self._offset = index2\n            chunk3, max3 = None, self._offset + 2\n            if max3 <= self._input_size:\n                chunk3 = self._input[self._offset:max3]\n            if chunk3 == '//':\n                address0 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                self._offset = self._offset + 2\n            else:\n                address0 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wind_spd', '\"//\"'))\n            if address0 is FAILURE:\n                self._offset = index2\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['wind_spd'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_gust(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['gust'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 == 'G':\n            address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::gust', '\"G\"'))\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3, elements1, address3 = self._offset, [], None\n            while True:\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_12.search(chunk1):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::gust', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                else:\n                    break\n            if len(elements1) >= 1:\n                address2 = TreeNode(self._input[index3:self._offset], index3, elements1)\n                self._offset = self._offset\n            else:\n                address2 = FAILURE\n            if address2 is not FAILURE:\n                elements0.append(address2)\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['gust'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_varwind(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['varwind'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 is not None and Grammar.REGEX_13.search(chunk0):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::varwind', '[\\\\d]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_14.search(chunk1):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::varwind', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    chunk2, max2 = None, self._offset + 1\n                    if max2 <= self._input_size:\n                        chunk2 = self._input[self._offset:max2]\n                    if chunk2 is not None and Grammar.REGEX_15.search(chunk2):\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::varwind', '[\\\\d]'))\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        chunk3, max3 = None, self._offset + 1\n                        if max3 <= self._input_size:\n                            chunk3 = self._input[self._offset:max3]\n                        if chunk3 == 'V':\n                            address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::varwind', '\"V\"'))\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            chunk4, max4 = None, self._offset + 1\n                            if max4 <= self._input_size:\n                                chunk4 = self._input[self._offset:max4]\n                            if chunk4 is not None and Grammar.REGEX_16.search(chunk4):\n                                address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::varwind', '[\\\\d]'))\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                                address7 = FAILURE\n                                chunk5, max5 = None, self._offset + 1\n                                if max5 <= self._input_size:\n                                    chunk5 = self._input[self._offset:max5]\n                                if chunk5 is not None and Grammar.REGEX_17.search(chunk5):\n                                    address7 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address7 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::varwind', '[\\\\d]'))\n                                if address7 is not FAILURE:\n                                    elements0.append(address7)\n                                    address8 = FAILURE\n                                    chunk6, max6 = None, self._offset + 1\n                                    if max6 <= self._input_size:\n                                        chunk6 = self._input[self._offset:max6]\n                                    if chunk6 is not None and Grammar.REGEX_18.search(chunk6):\n                                        address8 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                        self._offset = self._offset + 1\n                                    else:\n                                        address8 = FAILURE\n                                        if self._offset > self._failure:\n                                            self._failure = self._offset\n                                            self._expected = []\n                                        if self._offset == self._failure:\n                                            self._expected.append(('METAR::varwind', '[\\\\d]'))\n                                    if address8 is not FAILURE:\n                                        elements0.append(address8)\n                                    else:\n                                        elements0 = None\n                                        self._offset = index1\n                                else:\n                                    elements0 = None\n                                    self._offset = index1\n                            else:\n                                elements0 = None\n                                self._offset = index1\n                        else:\n                            elements0 = None\n                            self._offset = index1\n                    else:\n                        elements0 = None\n                        self._offset = index1\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode5(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['varwind'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_vis(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['vis'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3 = self._offset\n            index4, elements1 = self._offset, []\n            address3 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 is not None and Grammar.REGEX_19.search(chunk0):\n                address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address3 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::vis', '[\\\\d]'))\n            if address3 is not FAILURE:\n                elements1.append(address3)\n                address4 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_20.search(chunk1):\n                    address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address4 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::vis', '[\\\\d]'))\n                if address4 is not FAILURE:\n                    elements1.append(address4)\n                    address5 = FAILURE\n                    chunk2, max2 = None, self._offset + 1\n                    if max2 <= self._input_size:\n                        chunk2 = self._input[self._offset:max2]\n                    if chunk2 is not None and Grammar.REGEX_21.search(chunk2):\n                        address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address5 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::vis', '[\\\\d]'))\n                    if address5 is not FAILURE:\n                        elements1.append(address5)\n                        address6 = FAILURE\n                        chunk3, max3 = None, self._offset + 1\n                        if max3 <= self._input_size:\n                            chunk3 = self._input[self._offset:max3]\n                        if chunk3 is not None and Grammar.REGEX_22.search(chunk3):\n                            address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address6 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::vis', '[\\\\d]'))\n                        if address6 is not FAILURE:\n                            elements1.append(address6)\n                            address7 = FAILURE\n                            index5 = self._offset\n                            chunk4, max4 = None, self._offset + 3\n                            if max4 <= self._input_size:\n                                chunk4 = self._input[self._offset:max4]\n                            if chunk4 == 'NDV':\n                                address7 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                                self._offset = self._offset + 3\n                            else:\n                                address7 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::vis', '\"NDV\"'))\n                            if address7 is FAILURE:\n                                address7 = TreeNode(self._input[index5:index5], index5, [])\n                                self._offset = index5\n                            if address7 is not FAILURE:\n                                elements1.append(address7)\n                            else:\n                                elements1 = None\n                                self._offset = index4\n                        else:\n                            elements1 = None\n                            self._offset = index4\n                    else:\n                        elements1 = None\n                        self._offset = index4\n                else:\n                    elements1 = None\n                    self._offset = index4\n            else:\n                elements1 = None\n                self._offset = index4\n            if elements1 is None:\n                address2 = FAILURE\n            else:\n                address2 = TreeNode(self._input[index4:self._offset], index4, elements1)\n                self._offset = self._offset\n            if address2 is FAILURE:\n                self._offset = index3\n                index6, elements2 = self._offset, []\n                address8 = FAILURE\n                chunk5, max5 = None, self._offset + 1\n                if max5 <= self._input_size:\n                    chunk5 = self._input[self._offset:max5]\n                if chunk5 is not None and Grammar.REGEX_23.search(chunk5):\n                    address8 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address8 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::vis', '[\\\\d]'))\n                if address8 is not FAILURE:\n                    elements2.append(address8)\n                    address9 = FAILURE\n                    index7 = self._offset\n                    index8 = self._offset\n                    chunk6, max6 = None, self._offset + 1\n                    if max6 <= self._input_size:\n                        chunk6 = self._input[self._offset:max6]\n                    if chunk6 is not None and Grammar.REGEX_24.search(chunk6):\n                        address9 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address9 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::vis', '[\\\\d]'))\n                    if address9 is FAILURE:\n                        self._offset = index8\n                        index9, elements3 = self._offset, []\n                        address10 = FAILURE\n                        index10 = self._offset\n                        index11, elements4 = self._offset, []\n                        address11 = FAILURE\n                        chunk7, max7 = None, self._offset + 1\n                        if max7 <= self._input_size:\n                            chunk7 = self._input[self._offset:max7]\n                        if chunk7 == ' ':\n                            address11 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address11 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::vis', '\" \"'))\n                        if address11 is not FAILURE:\n                            elements4.append(address11)\n                            address12 = FAILURE\n                            chunk8, max8 = None, self._offset + 1\n                            if max8 <= self._input_size:\n                                chunk8 = self._input[self._offset:max8]\n                            if chunk8 is not None and Grammar.REGEX_25.search(chunk8):\n                                address12 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address12 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::vis', '[\\\\d]'))\n                            if address12 is not FAILURE:\n                                elements4.append(address12)\n                            else:\n                                elements4 = None\n                                self._offset = index11\n                        else:\n                            elements4 = None\n                            self._offset = index11\n                        if elements4 is None:\n                            address10 = FAILURE\n                        else:\n                            address10 = TreeNode(self._input[index11:self._offset], index11, elements4)\n                            self._offset = self._offset\n                        if address10 is FAILURE:\n                            address10 = TreeNode(self._input[index10:index10], index10, [])\n                            self._offset = index10\n                        if address10 is not FAILURE:\n                            elements3.append(address10)\n                            address13 = FAILURE\n                            chunk9, max9 = None, self._offset + 1\n                            if max9 <= self._input_size:\n                                chunk9 = self._input[self._offset:max9]\n                            if chunk9 == '/':\n                                address13 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address13 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::vis', '\"/\"'))\n                            if address13 is not FAILURE:\n                                elements3.append(address13)\n                                address14 = FAILURE\n                                chunk10, max10 = None, self._offset + 1\n                                if max10 <= self._input_size:\n                                    chunk10 = self._input[self._offset:max10]\n                                if chunk10 is not None and Grammar.REGEX_26.search(chunk10):\n                                    address14 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address14 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::vis', '[\\\\d]'))\n                                if address14 is not FAILURE:\n                                    elements3.append(address14)\n                                    address15 = FAILURE\n                                    index12 = self._offset\n                                    chunk11, max11 = None, self._offset + 1\n                                    if max11 <= self._input_size:\n                                        chunk11 = self._input[self._offset:max11]\n                                    if chunk11 is not None and Grammar.REGEX_27.search(chunk11):\n                                        address15 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                        self._offset = self._offset + 1\n                                    else:\n                                        address15 = FAILURE\n                                        if self._offset > self._failure:\n                                            self._failure = self._offset\n                                            self._expected = []\n                                        if self._offset == self._failure:\n                                            self._expected.append(('METAR::vis', '[\\\\d]'))\n                                    if address15 is FAILURE:\n                                        address15 = TreeNode(self._input[index12:index12], index12, [])\n                                        self._offset = index12\n                                    if address15 is not FAILURE:\n                                        elements3.append(address15)\n                                    else:\n                                        elements3 = None\n                                        self._offset = index9\n                                else:\n                                    elements3 = None\n                                    self._offset = index9\n                            else:\n                                elements3 = None\n                                self._offset = index9\n                        else:\n                            elements3 = None\n                            self._offset = index9\n                        if elements3 is None:\n                            address9 = FAILURE\n                        else:\n                            address9 = TreeNode(self._input[index9:self._offset], index9, elements3)\n                            self._offset = self._offset\n                        if address9 is FAILURE:\n                            self._offset = index8\n                    if address9 is FAILURE:\n                        address9 = TreeNode(self._input[index7:index7], index7, [])\n                        self._offset = index7\n                    if address9 is not FAILURE:\n                        elements2.append(address9)\n                        address16 = FAILURE\n                        chunk12, max12 = None, self._offset + 2\n                        if max12 <= self._input_size:\n                            chunk12 = self._input[self._offset:max12]\n                        if chunk12 == 'SM':\n                            address16 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                            self._offset = self._offset + 2\n                        else:\n                            address16 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::vis', '\"SM\"'))\n                        if address16 is not FAILURE:\n                            elements2.append(address16)\n                        else:\n                            elements2 = None\n                            self._offset = index6\n                    else:\n                        elements2 = None\n                        self._offset = index6\n                else:\n                    elements2 = None\n                    self._offset = index6\n                if elements2 is None:\n                    address2 = FAILURE\n                else:\n                    address2 = TreeNode(self._input[index6:self._offset], index6, elements2)\n                    self._offset = self._offset\n                if address2 is FAILURE:\n                    self._offset = index3\n                    index13, elements5 = self._offset, []\n                    address17 = FAILURE\n                    chunk13, max13 = None, self._offset + 1\n                    if max13 <= self._input_size:\n                        chunk13 = self._input[self._offset:max13]\n                    if chunk13 == 'M':\n                        address17 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address17 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::vis', '\"M\"'))\n                    if address17 is not FAILURE:\n                        elements5.append(address17)\n                        address18 = FAILURE\n                        chunk14, max14 = None, self._offset + 1\n                        if max14 <= self._input_size:\n                            chunk14 = self._input[self._offset:max14]\n                        if chunk14 is not None and Grammar.REGEX_28.search(chunk14):\n                            address18 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address18 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::vis', '[\\\\d]'))\n                        if address18 is not FAILURE:\n                            elements5.append(address18)\n                            address19 = FAILURE\n                            chunk15, max15 = None, self._offset + 1\n                            if max15 <= self._input_size:\n                                chunk15 = self._input[self._offset:max15]\n                            if chunk15 == '/':\n                                address19 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address19 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::vis', '\"/\"'))\n                            if address19 is not FAILURE:\n                                elements5.append(address19)\n                                address20 = FAILURE\n                                chunk16, max16 = None, self._offset + 1\n                                if max16 <= self._input_size:\n                                    chunk16 = self._input[self._offset:max16]\n                                if chunk16 is not None and Grammar.REGEX_29.search(chunk16):\n                                    address20 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address20 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::vis', '[\\\\d]'))\n                                if address20 is not FAILURE:\n                                    elements5.append(address20)\n                                    address21 = FAILURE\n                                    chunk17, max17 = None, self._offset + 2\n                                    if max17 <= self._input_size:\n                                        chunk17 = self._input[self._offset:max17]\n                                    if chunk17 == 'SM':\n                                        address21 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                        self._offset = self._offset + 2\n                                    else:\n                                        address21 = FAILURE\n                                        if self._offset > self._failure:\n                                            self._failure = self._offset\n                                            self._expected = []\n                                        if self._offset == self._failure:\n                                            self._expected.append(('METAR::vis', '\"SM\"'))\n                                    if address21 is not FAILURE:\n                                        elements5.append(address21)\n                                    else:\n                                        elements5 = None\n                                        self._offset = index13\n                                else:\n                                    elements5 = None\n                                    self._offset = index13\n                            else:\n                                elements5 = None\n                                self._offset = index13\n                        else:\n                            elements5 = None\n                            self._offset = index13\n                    else:\n                        elements5 = None\n                        self._offset = index13\n                    if elements5 is None:\n                        address2 = FAILURE\n                    else:\n                        address2 = TreeNode(self._input[index13:self._offset], index13, elements5)\n                        self._offset = self._offset\n                    if address2 is FAILURE:\n                        self._offset = index3\n                        chunk18, max18 = None, self._offset + 5\n                        if max18 <= self._input_size:\n                            chunk18 = self._input[self._offset:max18]\n                        if chunk18 == 'CAVOK':\n                            address2 = TreeNode(self._input[self._offset:self._offset + 5], self._offset, [])\n                            self._offset = self._offset + 5\n                        else:\n                            address2 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::vis', '\"CAVOK\"'))\n                        if address2 is FAILURE:\n                            self._offset = index3\n                            chunk19, max19 = None, self._offset + 4\n                            if max19 <= self._input_size:\n                                chunk19 = self._input[self._offset:max19]\n                            if chunk19 == '////':\n                                address2 = TreeNode(self._input[self._offset:self._offset + 4], self._offset, [])\n                                self._offset = self._offset + 4\n                            else:\n                                address2 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::vis', '\"////\"'))\n                            if address2 is FAILURE:\n                                self._offset = index3\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address22 = FAILURE\n                index14 = self._offset\n                address22 = self._read_varvis()\n                if address22 is FAILURE:\n                    address22 = TreeNode(self._input[index14:index14], index14, [])\n                    self._offset = index14\n                if address22 is not FAILURE:\n                    elements0.append(address22)\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode6(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['vis'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_varvis(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['varvis'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 is not None and Grammar.REGEX_30.search(chunk0):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::varvis', '[\\\\d]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_31.search(chunk1):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::varvis', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    chunk2, max2 = None, self._offset + 1\n                    if max2 <= self._input_size:\n                        chunk2 = self._input[self._offset:max2]\n                    if chunk2 is not None and Grammar.REGEX_32.search(chunk2):\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::varvis', '[\\\\d]'))\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        chunk3, max3 = None, self._offset + 1\n                        if max3 <= self._input_size:\n                            chunk3 = self._input[self._offset:max3]\n                        if chunk3 is not None and Grammar.REGEX_33.search(chunk3):\n                            address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::varvis', '[\\\\d]'))\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            index2 = self._offset\n                            chunk4, max4 = None, self._offset + 1\n                            if max4 <= self._input_size:\n                                chunk4 = self._input[self._offset:max4]\n                            if chunk4 is not None and Grammar.REGEX_34.search(chunk4):\n                                address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::varvis', '[NSEW]'))\n                            if address6 is FAILURE:\n                                address6 = TreeNode(self._input[index2:index2], index2, [])\n                                self._offset = index2\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                                address7 = FAILURE\n                                index3 = self._offset\n                                chunk5, max5 = None, self._offset + 1\n                                if max5 <= self._input_size:\n                                    chunk5 = self._input[self._offset:max5]\n                                if chunk5 is not None and Grammar.REGEX_35.search(chunk5):\n                                    address7 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address7 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::varvis', '[NSEW]'))\n                                if address7 is FAILURE:\n                                    address7 = TreeNode(self._input[index3:index3], index3, [])\n                                    self._offset = index3\n                                if address7 is not FAILURE:\n                                    elements0.append(address7)\n                                else:\n                                    elements0 = None\n                                    self._offset = index1\n                            else:\n                                elements0 = None\n                                self._offset = index1\n                        else:\n                            elements0 = None\n                            self._offset = index1\n                    else:\n                        elements0 = None\n                        self._offset = index1\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode7(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['varvis'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_run(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['run'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0, address1 = self._offset, [], None\n        while True:\n            index2, elements1 = self._offset, []\n            address2 = FAILURE\n            address2 = self._read_sep()\n            if address2 is not FAILURE:\n                elements1.append(address2)\n                address3 = FAILURE\n                chunk0, max0 = None, self._offset + 1\n                if max0 <= self._input_size:\n                    chunk0 = self._input[self._offset:max0]\n                if chunk0 == 'R':\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::run', '\"R\"'))\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                    address4 = FAILURE\n                    index3 = self._offset\n                    chunk1, max1 = None, self._offset + 1\n                    if max1 <= self._input_size:\n                        chunk1 = self._input[self._offset:max1]\n                    if chunk1 is not None and Grammar.REGEX_36.search(chunk1):\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::run', '[LRC]'))\n                    if address4 is FAILURE:\n                        address4 = TreeNode(self._input[index3:index3], index3, [])\n                        self._offset = index3\n                    if address4 is not FAILURE:\n                        elements1.append(address4)\n                        address5 = FAILURE\n                        chunk2, max2 = None, self._offset + 1\n                        if max2 <= self._input_size:\n                            chunk2 = self._input[self._offset:max2]\n                        if chunk2 is not None and Grammar.REGEX_37.search(chunk2):\n                            address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::run', '[\\\\d]'))\n                        if address5 is not FAILURE:\n                            elements1.append(address5)\n                            address6 = FAILURE\n                            chunk3, max3 = None, self._offset + 1\n                            if max3 <= self._input_size:\n                                chunk3 = self._input[self._offset:max3]\n                            if chunk3 is not None and Grammar.REGEX_38.search(chunk3):\n                                address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::run', '[\\\\d]'))\n                            if address6 is not FAILURE:\n                                elements1.append(address6)\n                                address7 = FAILURE\n                                index4 = self._offset\n                                chunk4, max4 = None, self._offset + 1\n                                if max4 <= self._input_size:\n                                    chunk4 = self._input[self._offset:max4]\n                                if chunk4 is not None and Grammar.REGEX_39.search(chunk4):\n                                    address7 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address7 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::run', '[LRC]'))\n                                if address7 is FAILURE:\n                                    address7 = TreeNode(self._input[index4:index4], index4, [])\n                                    self._offset = index4\n                                if address7 is not FAILURE:\n                                    elements1.append(address7)\n                                    address8 = FAILURE\n                                    chunk5, max5 = None, self._offset + 1\n                                    if max5 <= self._input_size:\n                                        chunk5 = self._input[self._offset:max5]\n                                    if chunk5 == '/':\n                                        address8 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                        self._offset = self._offset + 1\n                                    else:\n                                        address8 = FAILURE\n                                        if self._offset > self._failure:\n                                            self._failure = self._offset\n                                            self._expected = []\n                                        if self._offset == self._failure:\n                                            self._expected.append(('METAR::run', '\"/\"'))\n                                    if address8 is not FAILURE:\n                                        elements1.append(address8)\n                                        address9 = FAILURE\n                                        index5 = self._offset\n                                        index6, elements2 = self._offset, []\n                                        address10 = FAILURE\n                                        chunk6, max6 = None, self._offset + 1\n                                        if max6 <= self._input_size:\n                                            chunk6 = self._input[self._offset:max6]\n                                        if chunk6 is not None and Grammar.REGEX_40.search(chunk6):\n                                            address10 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                            self._offset = self._offset + 1\n                                        else:\n                                            address10 = FAILURE\n                                            if self._offset > self._failure:\n                                                self._failure = self._offset\n                                                self._expected = []\n                                            if self._offset == self._failure:\n                                                self._expected.append(('METAR::run', '[\\\\d]'))\n                                        if address10 is not FAILURE:\n                                            elements2.append(address10)\n                                            address11 = FAILURE\n                                            chunk7, max7 = None, self._offset + 1\n                                            if max7 <= self._input_size:\n                                                chunk7 = self._input[self._offset:max7]\n                                            if chunk7 is not None and Grammar.REGEX_41.search(chunk7):\n                                                address11 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                self._offset = self._offset + 1\n                                            else:\n                                                address11 = FAILURE\n                                                if self._offset > self._failure:\n                                                    self._failure = self._offset\n                                                    self._expected = []\n                                                if self._offset == self._failure:\n                                                    self._expected.append(('METAR::run', '[\\\\d]'))\n                                            if address11 is not FAILURE:\n                                                elements2.append(address11)\n                                                address12 = FAILURE\n                                                chunk8, max8 = None, self._offset + 1\n                                                if max8 <= self._input_size:\n                                                    chunk8 = self._input[self._offset:max8]\n                                                if chunk8 is not None and Grammar.REGEX_42.search(chunk8):\n                                                    address12 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                    self._offset = self._offset + 1\n                                                else:\n                                                    address12 = FAILURE\n                                                    if self._offset > self._failure:\n                                                        self._failure = self._offset\n                                                        self._expected = []\n                                                    if self._offset == self._failure:\n                                                        self._expected.append(('METAR::run', '[\\\\d]'))\n                                                if address12 is not FAILURE:\n                                                    elements2.append(address12)\n                                                    address13 = FAILURE\n                                                    chunk9, max9 = None, self._offset + 1\n                                                    if max9 <= self._input_size:\n                                                        chunk9 = self._input[self._offset:max9]\n                                                    if chunk9 is not None and Grammar.REGEX_43.search(chunk9):\n                                                        address13 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                        self._offset = self._offset + 1\n                                                    else:\n                                                        address13 = FAILURE\n                                                        if self._offset > self._failure:\n                                                            self._failure = self._offset\n                                                            self._expected = []\n                                                        if self._offset == self._failure:\n                                                            self._expected.append(('METAR::run', '[\\\\d]'))\n                                                    if address13 is not FAILURE:\n                                                        elements2.append(address13)\n                                                        address14 = FAILURE\n                                                        chunk10, max10 = None, self._offset + 1\n                                                        if max10 <= self._input_size:\n                                                            chunk10 = self._input[self._offset:max10]\n                                                        if chunk10 == 'V':\n                                                            address14 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                            self._offset = self._offset + 1\n                                                        else:\n                                                            address14 = FAILURE\n                                                            if self._offset > self._failure:\n                                                                self._failure = self._offset\n                                                                self._expected = []\n                                                            if self._offset == self._failure:\n                                                                self._expected.append(('METAR::run', '\"V\"'))\n                                                        if address14 is not FAILURE:\n                                                            elements2.append(address14)\n                                                        else:\n                                                            elements2 = None\n                                                            self._offset = index6\n                                                    else:\n                                                        elements2 = None\n                                                        self._offset = index6\n                                                else:\n                                                    elements2 = None\n                                                    self._offset = index6\n                                            else:\n                                                elements2 = None\n                                                self._offset = index6\n                                        else:\n                                            elements2 = None\n                                            self._offset = index6\n                                        if elements2 is None:\n                                            address9 = FAILURE\n                                        else:\n                                            address9 = TreeNode(self._input[index6:self._offset], index6, elements2)\n                                            self._offset = self._offset\n                                        if address9 is FAILURE:\n                                            address9 = TreeNode(self._input[index5:index5], index5, [])\n                                            self._offset = index5\n                                        if address9 is not FAILURE:\n                                            elements1.append(address9)\n                                            address15 = FAILURE\n                                            index7 = self._offset\n                                            chunk11, max11 = None, self._offset + 1\n                                            if max11 <= self._input_size:\n                                                chunk11 = self._input[self._offset:max11]\n                                            if chunk11 is not None and Grammar.REGEX_44.search(chunk11):\n                                                address15 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                self._offset = self._offset + 1\n                                            else:\n                                                address15 = FAILURE\n                                                if self._offset > self._failure:\n                                                    self._failure = self._offset\n                                                    self._expected = []\n                                                if self._offset == self._failure:\n                                                    self._expected.append(('METAR::run', '[\"M\" / \"P\"]'))\n                                            if address15 is FAILURE:\n                                                address15 = TreeNode(self._input[index7:index7], index7, [])\n                                                self._offset = index7\n                                            if address15 is not FAILURE:\n                                                elements1.append(address15)\n                                                address16 = FAILURE\n                                                chunk12, max12 = None, self._offset + 1\n                                                if max12 <= self._input_size:\n                                                    chunk12 = self._input[self._offset:max12]\n                                                if chunk12 is not None and Grammar.REGEX_45.search(chunk12):\n                                                    address16 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                    self._offset = self._offset + 1\n                                                else:\n                                                    address16 = FAILURE\n                                                    if self._offset > self._failure:\n                                                        self._failure = self._offset\n                                                        self._expected = []\n                                                    if self._offset == self._failure:\n                                                        self._expected.append(('METAR::run', '[\\\\d]'))\n                                                if address16 is not FAILURE:\n                                                    elements1.append(address16)\n                                                    address17 = FAILURE\n                                                    chunk13, max13 = None, self._offset + 1\n                                                    if max13 <= self._input_size:\n                                                        chunk13 = self._input[self._offset:max13]\n                                                    if chunk13 is not None and Grammar.REGEX_46.search(chunk13):\n                                                        address17 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                        self._offset = self._offset + 1\n                                                    else:\n                                                        address17 = FAILURE\n                                                        if self._offset > self._failure:\n                                                            self._failure = self._offset\n                                                            self._expected = []\n                                                        if self._offset == self._failure:\n                                                            self._expected.append(('METAR::run', '[\\\\d]'))\n                                                    if address17 is not FAILURE:\n                                                        elements1.append(address17)\n                                                        address18 = FAILURE\n                                                        chunk14, max14 = None, self._offset + 1\n                                                        if max14 <= self._input_size:\n                                                            chunk14 = self._input[self._offset:max14]\n                                                        if chunk14 is not None and Grammar.REGEX_47.search(chunk14):\n                                                            address18 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                            self._offset = self._offset + 1\n                                                        else:\n                                                            address18 = FAILURE\n                                                            if self._offset > self._failure:\n                                                                self._failure = self._offset\n                                                                self._expected = []\n                                                            if self._offset == self._failure:\n                                                                self._expected.append(('METAR::run', '[\\\\d]'))\n                                                        if address18 is not FAILURE:\n                                                            elements1.append(address18)\n                                                            address19 = FAILURE\n                                                            chunk15, max15 = None, self._offset + 1\n                                                            if max15 <= self._input_size:\n                                                                chunk15 = self._input[self._offset:max15]\n                                                            if chunk15 is not None and Grammar.REGEX_48.search(chunk15):\n                                                                address19 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                                self._offset = self._offset + 1\n                                                            else:\n                                                                address19 = FAILURE\n                                                                if self._offset > self._failure:\n                                                                    self._failure = self._offset\n                                                                    self._expected = []\n                                                                if self._offset == self._failure:\n                                                                    self._expected.append(('METAR::run', '[\\\\d]'))\n                                                            if address19 is not FAILURE:\n                                                                elements1.append(address19)\n                                                                address20 = FAILURE\n                                                                index8 = self._offset\n                                                                chunk16, max16 = None, self._offset + 2\n                                                                if max16 <= self._input_size:\n                                                                    chunk16 = self._input[self._offset:max16]\n                                                                if chunk16 == 'FT':\n                                                                    address20 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                    self._offset = self._offset + 2\n                                                                else:\n                                                                    address20 = FAILURE\n                                                                    if self._offset > self._failure:\n                                                                        self._failure = self._offset\n                                                                        self._expected = []\n                                                                    if self._offset == self._failure:\n                                                                        self._expected.append(('METAR::run', '\"FT\"'))\n                                                                if address20 is FAILURE:\n                                                                    address20 = TreeNode(self._input[index8:index8], index8, [])\n                                                                    self._offset = index8\n                                                                if address20 is not FAILURE:\n                                                                    elements1.append(address20)\n                                                                    address21 = FAILURE\n                                                                    index9 = self._offset\n                                                                    index10, elements3 = self._offset, []\n                                                                    address22 = FAILURE\n                                                                    index11 = self._offset\n                                                                    chunk17, max17 = None, self._offset + 1\n                                                                    if max17 <= self._input_size:\n                                                                        chunk17 = self._input[self._offset:max17]\n                                                                    if chunk17 == '/':\n                                                                        address22 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                                        self._offset = self._offset + 1\n                                                                    else:\n                                                                        address22 = FAILURE\n                                                                        if self._offset > self._failure:\n                                                                            self._failure = self._offset\n                                                                            self._expected = []\n                                                                        if self._offset == self._failure:\n                                                                            self._expected.append(('METAR::run', '\"/\"'))\n                                                                    if address22 is FAILURE:\n                                                                        address22 = TreeNode(self._input[index11:index11], index11, [])\n                                                                        self._offset = index11\n                                                                    if address22 is not FAILURE:\n                                                                        elements3.append(address22)\n                                                                        address23 = FAILURE\n                                                                        chunk18, max18 = None, self._offset + 1\n                                                                        if max18 <= self._input_size:\n                                                                            chunk18 = self._input[self._offset:max18]\n                                                                        if chunk18 is not None and Grammar.REGEX_49.search(chunk18):\n                                                                            address23 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                                            self._offset = self._offset + 1\n                                                                        else:\n                                                                            address23 = FAILURE\n                                                                            if self._offset > self._failure:\n                                                                                self._failure = self._offset\n                                                                                self._expected = []\n                                                                            if self._offset == self._failure:\n                                                                                self._expected.append(('METAR::run', '[UDN]'))\n                                                                        if address23 is not FAILURE:\n                                                                            elements3.append(address23)\n                                                                        else:\n                                                                            elements3 = None\n                                                                            self._offset = index10\n                                                                    else:\n                                                                        elements3 = None\n                                                                        self._offset = index10\n                                                                    if elements3 is None:\n                                                                        address21 = FAILURE\n                                                                    else:\n                                                                        address21 = TreeNode(self._input[index10:self._offset], index10, elements3)\n                                                                        self._offset = self._offset\n                                                                    if address21 is FAILURE:\n                                                                        address21 = TreeNode(self._input[index9:index9], index9, [])\n                                                                        self._offset = index9\n                                                                    if address21 is not FAILURE:\n                                                                        elements1.append(address21)\n                                                                    else:\n                                                                        elements1 = None\n                                                                        self._offset = index2\n                                                                else:\n                                                                    elements1 = None\n                                                                    self._offset = index2\n                                                            else:\n                                                                elements1 = None\n                                                                self._offset = index2\n                                                        else:\n                                                            elements1 = None\n                                                            self._offset = index2\n                                                    else:\n                                                        elements1 = None\n                                                        self._offset = index2\n                                                else:\n                                                    elements1 = None\n                                                    self._offset = index2\n                                            else:\n                                                elements1 = None\n                                                self._offset = index2\n                                        else:\n                                            elements1 = None\n                                            self._offset = index2\n                                    else:\n                                        elements1 = None\n                                        self._offset = index2\n                                else:\n                                    elements1 = None\n                                    self._offset = index2\n                            else:\n                                elements1 = None\n                                self._offset = index2\n                        else:\n                            elements1 = None\n                            self._offset = index2\n                    else:\n                        elements1 = None\n                        self._offset = index2\n                else:\n                    elements1 = None\n                    self._offset = index2\n            else:\n                elements1 = None\n                self._offset = index2\n            if elements1 is None:\n                address1 = FAILURE\n            else:\n                address1 = TreeNode8(self._input[index2:self._offset], index2, elements1)\n                self._offset = self._offset\n            if address1 is not FAILURE:\n                elements0.append(address1)\n            else:\n                break\n        if len(elements0) >= 0:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        else:\n            address0 = FAILURE\n        self._cache['run'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_curwx(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['curwx'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2 = self._offset\n        index3, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 2\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 == '//':\n                address2 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                self._offset = self._offset + 2\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::curwx', '\"//\"'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n            else:\n                elements0 = None\n                self._offset = index3\n        else:\n            elements0 = None\n            self._offset = index3\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode9(self._input[index3:self._offset], index3, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            self._offset = index2\n            index4, elements1 = self._offset, []\n            address3 = FAILURE\n            address3 = self._read_sep()\n            if address3 is not FAILURE:\n                elements1.append(address3)\n                address4 = FAILURE\n                chunk1, max1 = None, self._offset + 3\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 == 'NSW':\n                    address4 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                    self._offset = self._offset + 3\n                else:\n                    address4 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::curwx', '\"NSW\"'))\n                if address4 is not FAILURE:\n                    elements1.append(address4)\n                else:\n                    elements1 = None\n                    self._offset = index4\n            else:\n                elements1 = None\n                self._offset = index4\n            if elements1 is None:\n                address0 = FAILURE\n            else:\n                address0 = TreeNode10(self._input[index4:self._offset], index4, elements1)\n                self._offset = self._offset\n            if address0 is FAILURE:\n                self._offset = index2\n                index5, elements2, address5 = self._offset, [], None\n                while True:\n                    index6, elements3 = self._offset, []\n                    address6 = FAILURE\n                    address6 = self._read_sep()\n                    if address6 is not FAILURE:\n                        elements3.append(address6)\n                        address7 = FAILURE\n                        address7 = self._read_wx()\n                        if address7 is not FAILURE:\n                            elements3.append(address7)\n                        else:\n                            elements3 = None\n                            self._offset = index6\n                    else:\n                        elements3 = None\n                        self._offset = index6\n                    if elements3 is None:\n                        address5 = FAILURE\n                    else:\n                        address5 = TreeNode11(self._input[index6:self._offset], index6, elements3)\n                        self._offset = self._offset\n                    if address5 is not FAILURE:\n                        elements2.append(address5)\n                    else:\n                        break\n                if len(elements2) >= 0:\n                    address0 = TreeNode(self._input[index5:self._offset], index5, elements2)\n                    self._offset = self._offset\n                else:\n                    address0 = FAILURE\n                if address0 is FAILURE:\n                    self._offset = index2\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['curwx'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_wx(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['wx'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        index2 = self._offset\n        index3, elements1 = self._offset, []\n        address2 = FAILURE\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 is not None and Grammar.REGEX_50.search(chunk0):\n            address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address2 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::wx', '[-+]'))\n        if address2 is not FAILURE:\n            elements1.append(address2)\n            address3 = FAILURE\n            index4 = self._offset\n            chunk1, max1 = None, self._offset + 1\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 == ' ':\n                address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address3 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wx', '\" \"'))\n            if address3 is FAILURE:\n                address3 = TreeNode(self._input[index4:index4], index4, [])\n                self._offset = index4\n            if address3 is not FAILURE:\n                elements1.append(address3)\n            else:\n                elements1 = None\n                self._offset = index3\n        else:\n            elements1 = None\n            self._offset = index3\n        if elements1 is None:\n            address1 = FAILURE\n        else:\n            address1 = TreeNode(self._input[index3:self._offset], index3, elements1)\n            self._offset = self._offset\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index2:index2], index2, [])\n            self._offset = index2\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address4 = FAILURE\n            index5 = self._offset\n            chunk2, max2 = None, self._offset + 2\n            if max2 <= self._input_size:\n                chunk2 = self._input[self._offset:max2]\n            if chunk2 == 'VC':\n                address4 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                self._offset = self._offset + 2\n            else:\n                address4 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wx', '\"VC\"'))\n            if address4 is FAILURE:\n                address4 = TreeNode(self._input[index5:index5], index5, [])\n                self._offset = index5\n            if address4 is not FAILURE:\n                elements0.append(address4)\n                address5 = FAILURE\n                index6, elements2, address6 = self._offset, [], None\n                while True:\n                    index7 = self._offset\n                    chunk3, max3 = None, self._offset + 2\n                    if max3 <= self._input_size:\n                        chunk3 = self._input[self._offset:max3]\n                    if chunk3 == 'MI':\n                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                        self._offset = self._offset + 2\n                    else:\n                        address6 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::wx', '\"MI\"'))\n                    if address6 is FAILURE:\n                        self._offset = index7\n                        chunk4, max4 = None, self._offset + 2\n                        if max4 <= self._input_size:\n                            chunk4 = self._input[self._offset:max4]\n                        if chunk4 == 'BC':\n                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                            self._offset = self._offset + 2\n                        else:\n                            address6 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::wx', '\"BC\"'))\n                        if address6 is FAILURE:\n                            self._offset = index7\n                            chunk5, max5 = None, self._offset + 2\n                            if max5 <= self._input_size:\n                                chunk5 = self._input[self._offset:max5]\n                            if chunk5 == 'PR':\n                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                self._offset = self._offset + 2\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::wx', '\"PR\"'))\n                            if address6 is FAILURE:\n                                self._offset = index7\n                                chunk6, max6 = None, self._offset + 2\n                                if max6 <= self._input_size:\n                                    chunk6 = self._input[self._offset:max6]\n                                if chunk6 == 'DR':\n                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                    self._offset = self._offset + 2\n                                else:\n                                    address6 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::wx', '\"DR\"'))\n                                if address6 is FAILURE:\n                                    self._offset = index7\n                                    chunk7, max7 = None, self._offset + 2\n                                    if max7 <= self._input_size:\n                                        chunk7 = self._input[self._offset:max7]\n                                    if chunk7 == 'BL':\n                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                        self._offset = self._offset + 2\n                                    else:\n                                        address6 = FAILURE\n                                        if self._offset > self._failure:\n                                            self._failure = self._offset\n                                            self._expected = []\n                                        if self._offset == self._failure:\n                                            self._expected.append(('METAR::wx', '\"BL\"'))\n                                    if address6 is FAILURE:\n                                        self._offset = index7\n                                        chunk8, max8 = None, self._offset + 2\n                                        if max8 <= self._input_size:\n                                            chunk8 = self._input[self._offset:max8]\n                                        if chunk8 == 'SH':\n                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                            self._offset = self._offset + 2\n                                        else:\n                                            address6 = FAILURE\n                                            if self._offset > self._failure:\n                                                self._failure = self._offset\n                                                self._expected = []\n                                            if self._offset == self._failure:\n                                                self._expected.append(('METAR::wx', '\"SH\"'))\n                                        if address6 is FAILURE:\n                                            self._offset = index7\n                                            chunk9, max9 = None, self._offset + 2\n                                            if max9 <= self._input_size:\n                                                chunk9 = self._input[self._offset:max9]\n                                            if chunk9 == 'TS':\n                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                self._offset = self._offset + 2\n                                            else:\n                                                address6 = FAILURE\n                                                if self._offset > self._failure:\n                                                    self._failure = self._offset\n                                                    self._expected = []\n                                                if self._offset == self._failure:\n                                                    self._expected.append(('METAR::wx', '\"TS\"'))\n                                            if address6 is FAILURE:\n                                                self._offset = index7\n                                                chunk10, max10 = None, self._offset + 2\n                                                if max10 <= self._input_size:\n                                                    chunk10 = self._input[self._offset:max10]\n                                                if chunk10 == 'FZ':\n                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                    self._offset = self._offset + 2\n                                                else:\n                                                    address6 = FAILURE\n                                                    if self._offset > self._failure:\n                                                        self._failure = self._offset\n                                                        self._expected = []\n                                                    if self._offset == self._failure:\n                                                        self._expected.append(('METAR::wx', '\"FZ\"'))\n                                                if address6 is FAILURE:\n                                                    self._offset = index7\n                                                    chunk11, max11 = None, self._offset + 2\n                                                    if max11 <= self._input_size:\n                                                        chunk11 = self._input[self._offset:max11]\n                                                    if chunk11 == 'DZ':\n                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                        self._offset = self._offset + 2\n                                                    else:\n                                                        address6 = FAILURE\n                                                        if self._offset > self._failure:\n                                                            self._failure = self._offset\n                                                            self._expected = []\n                                                        if self._offset == self._failure:\n                                                            self._expected.append(('METAR::wx', '\"DZ\"'))\n                                                    if address6 is FAILURE:\n                                                        self._offset = index7\n                                                        chunk12, max12 = None, self._offset + 2\n                                                        if max12 <= self._input_size:\n                                                            chunk12 = self._input[self._offset:max12]\n                                                        if chunk12 == 'RA':\n                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                            self._offset = self._offset + 2\n                                                        else:\n                                                            address6 = FAILURE\n                                                            if self._offset > self._failure:\n                                                                self._failure = self._offset\n                                                                self._expected = []\n                                                            if self._offset == self._failure:\n                                                                self._expected.append(('METAR::wx', '\"RA\"'))\n                                                        if address6 is FAILURE:\n                                                            self._offset = index7\n                                                            chunk13, max13 = None, self._offset + 2\n                                                            if max13 <= self._input_size:\n                                                                chunk13 = self._input[self._offset:max13]\n                                                            if chunk13 == 'SN':\n                                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                self._offset = self._offset + 2\n                                                            else:\n                                                                address6 = FAILURE\n                                                                if self._offset > self._failure:\n                                                                    self._failure = self._offset\n                                                                    self._expected = []\n                                                                if self._offset == self._failure:\n                                                                    self._expected.append(('METAR::wx', '\"SN\"'))\n                                                            if address6 is FAILURE:\n                                                                self._offset = index7\n                                                                chunk14, max14 = None, self._offset + 2\n                                                                if max14 <= self._input_size:\n                                                                    chunk14 = self._input[self._offset:max14]\n                                                                if chunk14 == 'SG':\n                                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                    self._offset = self._offset + 2\n                                                                else:\n                                                                    address6 = FAILURE\n                                                                    if self._offset > self._failure:\n                                                                        self._failure = self._offset\n                                                                        self._expected = []\n                                                                    if self._offset == self._failure:\n                                                                        self._expected.append(('METAR::wx', '\"SG\"'))\n                                                                if address6 is FAILURE:\n                                                                    self._offset = index7\n                                                                    chunk15, max15 = None, self._offset + 2\n                                                                    if max15 <= self._input_size:\n                                                                        chunk15 = self._input[self._offset:max15]\n                                                                    if chunk15 == 'PL':\n                                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                        self._offset = self._offset + 2\n                                                                    else:\n                                                                        address6 = FAILURE\n                                                                        if self._offset > self._failure:\n                                                                            self._failure = self._offset\n                                                                            self._expected = []\n                                                                        if self._offset == self._failure:\n                                                                            self._expected.append(('METAR::wx', '\"PL\"'))\n                                                                    if address6 is FAILURE:\n                                                                        self._offset = index7\n                                                                        chunk16, max16 = None, self._offset + 2\n                                                                        if max16 <= self._input_size:\n                                                                            chunk16 = self._input[self._offset:max16]\n                                                                        if chunk16 == 'GR':\n                                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                            self._offset = self._offset + 2\n                                                                        else:\n                                                                            address6 = FAILURE\n                                                                            if self._offset > self._failure:\n                                                                                self._failure = self._offset\n                                                                                self._expected = []\n                                                                            if self._offset == self._failure:\n                                                                                self._expected.append(('METAR::wx', '\"GR\"'))\n                                                                        if address6 is FAILURE:\n                                                                            self._offset = index7\n                                                                            chunk17, max17 = None, self._offset + 2\n                                                                            if max17 <= self._input_size:\n                                                                                chunk17 = self._input[self._offset:max17]\n                                                                            if chunk17 == 'GS':\n                                                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                self._offset = self._offset + 2\n                                                                            else:\n                                                                                address6 = FAILURE\n                                                                                if self._offset > self._failure:\n                                                                                    self._failure = self._offset\n                                                                                    self._expected = []\n                                                                                if self._offset == self._failure:\n                                                                                    self._expected.append(('METAR::wx', '\"GS\"'))\n                                                                            if address6 is FAILURE:\n                                                                                self._offset = index7\n                                                                                chunk18, max18 = None, self._offset + 2\n                                                                                if max18 <= self._input_size:\n                                                                                    chunk18 = self._input[self._offset:max18]\n                                                                                if chunk18 == 'UP':\n                                                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                    self._offset = self._offset + 2\n                                                                                else:\n                                                                                    address6 = FAILURE\n                                                                                    if self._offset > self._failure:\n                                                                                        self._failure = self._offset\n                                                                                        self._expected = []\n                                                                                    if self._offset == self._failure:\n                                                                                        self._expected.append(('METAR::wx', '\"UP\"'))\n                                                                                if address6 is FAILURE:\n                                                                                    self._offset = index7\n                                                                                    chunk19, max19 = None, self._offset + 2\n                                                                                    if max19 <= self._input_size:\n                                                                                        chunk19 = self._input[self._offset:max19]\n                                                                                    if chunk19 == 'BR':\n                                                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                        self._offset = self._offset + 2\n                                                                                    else:\n                                                                                        address6 = FAILURE\n                                                                                        if self._offset > self._failure:\n                                                                                            self._failure = self._offset\n                                                                                            self._expected = []\n                                                                                        if self._offset == self._failure:\n                                                                                            self._expected.append(('METAR::wx', '\"BR\"'))\n                                                                                    if address6 is FAILURE:\n                                                                                        self._offset = index7\n                                                                                        chunk20, max20 = None, self._offset + 2\n                                                                                        if max20 <= self._input_size:\n                                                                                            chunk20 = self._input[self._offset:max20]\n                                                                                        if chunk20 == 'FG':\n                                                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                            self._offset = self._offset + 2\n                                                                                        else:\n                                                                                            address6 = FAILURE\n                                                                                            if self._offset > self._failure:\n                                                                                                self._failure = self._offset\n                                                                                                self._expected = []\n                                                                                            if self._offset == self._failure:\n                                                                                                self._expected.append(('METAR::wx', '\"FG\"'))\n                                                                                        if address6 is FAILURE:\n                                                                                            self._offset = index7\n                                                                                            chunk21, max21 = None, self._offset + 2\n                                                                                            if max21 <= self._input_size:\n                                                                                                chunk21 = self._input[self._offset:max21]\n                                                                                            if chunk21 == 'FU':\n                                                                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                self._offset = self._offset + 2\n                                                                                            else:\n                                                                                                address6 = FAILURE\n                                                                                                if self._offset > self._failure:\n                                                                                                    self._failure = self._offset\n                                                                                                    self._expected = []\n                                                                                                if self._offset == self._failure:\n                                                                                                    self._expected.append(('METAR::wx', '\"FU\"'))\n                                                                                            if address6 is FAILURE:\n                                                                                                self._offset = index7\n                                                                                                chunk22, max22 = None, self._offset + 2\n                                                                                                if max22 <= self._input_size:\n                                                                                                    chunk22 = self._input[self._offset:max22]\n                                                                                                if chunk22 == 'VA':\n                                                                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                    self._offset = self._offset + 2\n                                                                                                else:\n                                                                                                    address6 = FAILURE\n                                                                                                    if self._offset > self._failure:\n                                                                                                        self._failure = self._offset\n                                                                                                        self._expected = []\n                                                                                                    if self._offset == self._failure:\n                                                                                                        self._expected.append(('METAR::wx', '\"VA\"'))\n                                                                                                if address6 is FAILURE:\n                                                                                                    self._offset = index7\n                                                                                                    chunk23, max23 = None, self._offset + 2\n                                                                                                    if max23 <= self._input_size:\n                                                                                                        chunk23 = self._input[self._offset:max23]\n                                                                                                    if chunk23 == 'DU':\n                                                                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                        self._offset = self._offset + 2\n                                                                                                    else:\n                                                                                                        address6 = FAILURE\n                                                                                                        if self._offset > self._failure:\n                                                                                                            self._failure = self._offset\n                                                                                                            self._expected = []\n                                                                                                        if self._offset == self._failure:\n                                                                                                            self._expected.append(('METAR::wx', '\"DU\"'))\n                                                                                                    if address6 is FAILURE:\n                                                                                                        self._offset = index7\n                                                                                                        chunk24, max24 = None, self._offset + 2\n                                                                                                        if max24 <= self._input_size:\n                                                                                                            chunk24 = self._input[self._offset:max24]\n                                                                                                        if chunk24 == 'SA':\n                                                                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                            self._offset = self._offset + 2\n                                                                                                        else:\n                                                                                                            address6 = FAILURE\n                                                                                                            if self._offset > self._failure:\n                                                                                                                self._failure = self._offset\n                                                                                                                self._expected = []\n                                                                                                            if self._offset == self._failure:\n                                                                                                                self._expected.append(('METAR::wx', '\"SA\"'))\n                                                                                                        if address6 is FAILURE:\n                                                                                                            self._offset = index7\n                                                                                                            chunk25, max25 = None, self._offset + 2\n                                                                                                            if max25 <= self._input_size:\n                                                                                                                chunk25 = self._input[self._offset:max25]\n                                                                                                            if chunk25 == 'HZ':\n                                                                                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                self._offset = self._offset + 2\n                                                                                                            else:\n                                                                                                                address6 = FAILURE\n                                                                                                                if self._offset > self._failure:\n                                                                                                                    self._failure = self._offset\n                                                                                                                    self._expected = []\n                                                                                                                if self._offset == self._failure:\n                                                                                                                    self._expected.append(('METAR::wx', '\"HZ\"'))\n                                                                                                            if address6 is FAILURE:\n                                                                                                                self._offset = index7\n                                                                                                                chunk26, max26 = None, self._offset + 2\n                                                                                                                if max26 <= self._input_size:\n                                                                                                                    chunk26 = self._input[self._offset:max26]\n                                                                                                                if chunk26 == 'PO':\n                                                                                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                    self._offset = self._offset + 2\n                                                                                                                else:\n                                                                                                                    address6 = FAILURE\n                                                                                                                    if self._offset > self._failure:\n                                                                                                                        self._failure = self._offset\n                                                                                                                        self._expected = []\n                                                                                                                    if self._offset == self._failure:\n                                                                                                                        self._expected.append(('METAR::wx', '\"PO\"'))\n                                                                                                                if address6 is FAILURE:\n                                                                                                                    self._offset = index7\n                                                                                                                    chunk27, max27 = None, self._offset + 2\n                                                                                                                    if max27 <= self._input_size:\n                                                                                                                        chunk27 = self._input[self._offset:max27]\n                                                                                                                    if chunk27 == 'SQ':\n                                                                                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                        self._offset = self._offset + 2\n                                                                                                                    else:\n                                                                                                                        address6 = FAILURE\n                                                                                                                        if self._offset > self._failure:\n                                                                                                                            self._failure = self._offset\n                                                                                                                            self._expected = []\n                                                                                                                        if self._offset == self._failure:\n                                                                                                                            self._expected.append(('METAR::wx', '\"SQ\"'))\n                                                                                                                    if address6 is FAILURE:\n                                                                                                                        self._offset = index7\n                                                                                                                        chunk28, max28 = None, self._offset + 2\n                                                                                                                        if max28 <= self._input_size:\n                                                                                                                            chunk28 = self._input[self._offset:max28]\n                                                                                                                        if chunk28 == 'FC':\n                                                                                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                            self._offset = self._offset + 2\n                                                                                                                        else:\n                                                                                                                            address6 = FAILURE\n                                                                                                                            if self._offset > self._failure:\n                                                                                                                                self._failure = self._offset\n                                                                                                                                self._expected = []\n                                                                                                                            if self._offset == self._failure:\n                                                                                                                                self._expected.append(('METAR::wx', '\"FC\"'))\n                                                                                                                        if address6 is FAILURE:\n                                                                                                                            self._offset = index7\n                                                                                                                            chunk29, max29 = None, self._offset + 2\n                                                                                                                            if max29 <= self._input_size:\n                                                                                                                                chunk29 = self._input[self._offset:max29]\n                                                                                                                            if chunk29 == 'SS':\n                                                                                                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                                self._offset = self._offset + 2\n                                                                                                                            else:\n                                                                                                                                address6 = FAILURE\n                                                                                                                                if self._offset > self._failure:\n                                                                                                                                    self._failure = self._offset\n                                                                                                                                    self._expected = []\n                                                                                                                                if self._offset == self._failure:\n                                                                                                                                    self._expected.append(('METAR::wx', '\"SS\"'))\n                                                                                                                            if address6 is FAILURE:\n                                                                                                                                self._offset = index7\n                                                                                                                                chunk30, max30 = None, self._offset + 2\n                                                                                                                                if max30 <= self._input_size:\n                                                                                                                                    chunk30 = self._input[self._offset:max30]\n                                                                                                                                if chunk30 == 'DS':\n                                                                                                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                                    self._offset = self._offset + 2\n                                                                                                                                else:\n                                                                                                                                    address6 = FAILURE\n                                                                                                                                    if self._offset > self._failure:\n                                                                                                                                        self._failure = self._offset\n                                                                                                                                        self._expected = []\n                                                                                                                                    if self._offset == self._failure:\n                                                                                                                                        self._expected.append(('METAR::wx', '\"DS\"'))\n                                                                                                                                if address6 is FAILURE:\n                                                                                                                                    self._offset = index7\n                                                                                                                                    chunk31, max31 = None, self._offset + 2\n                                                                                                                                    if max31 <= self._input_size:\n                                                                                                                                        chunk31 = self._input[self._offset:max31]\n                                                                                                                                    if chunk31 == 'IC':\n                                                                                                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                                        self._offset = self._offset + 2\n                                                                                                                                    else:\n                                                                                                                                        address6 = FAILURE\n                                                                                                                                        if self._offset > self._failure:\n                                                                                                                                            self._failure = self._offset\n                                                                                                                                            self._expected = []\n                                                                                                                                        if self._offset == self._failure:\n                                                                                                                                            self._expected.append(('METAR::wx', '\"IC\"'))\n                                                                                                                                    if address6 is FAILURE:\n                                                                                                                                        self._offset = index7\n                                                                                                                                        chunk32, max32 = None, self._offset + 2\n                                                                                                                                        if max32 <= self._input_size:\n                                                                                                                                            chunk32 = self._input[self._offset:max32]\n                                                                                                                                        if chunk32 == 'PY':\n                                                                                                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                                            self._offset = self._offset + 2\n                                                                                                                                        else:\n                                                                                                                                            address6 = FAILURE\n                                                                                                                                            if self._offset > self._failure:\n                                                                                                                                                self._failure = self._offset\n                                                                                                                                                self._expected = []\n                                                                                                                                            if self._offset == self._failure:\n                                                                                                                                                self._expected.append(('METAR::wx', '\"PY\"'))\n                                                                                                                                        if address6 is FAILURE:\n                                                                                                                                            self._offset = index7\n                    if address6 is not FAILURE:\n                        elements2.append(address6)\n                    else:\n                        break\n                if len(elements2) >= 1:\n                    address5 = TreeNode(self._input[index6:self._offset], index6, elements2)\n                    self._offset = self._offset\n                else:\n                    address5 = FAILURE\n                if address5 is not FAILURE:\n                    elements0.append(address5)\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['wx'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_skyc(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['skyc'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0, address1 = self._offset, [], None\n        while True:\n            index3, elements1 = self._offset, []\n            address2 = FAILURE\n            address2 = self._read_sep()\n            if address2 is not FAILURE:\n                elements1.append(address2)\n                address3 = FAILURE\n                address3 = self._read_cover()\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                else:\n                    elements1 = None\n                    self._offset = index3\n            else:\n                elements1 = None\n                self._offset = index3\n            if elements1 is None:\n                address1 = FAILURE\n            else:\n                address1 = TreeNode12(self._input[index3:self._offset], index3, elements1)\n                self._offset = self._offset\n            if address1 is not FAILURE:\n                elements0.append(address1)\n            else:\n                break\n        if len(elements0) >= 0:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        else:\n            address0 = FAILURE\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['skyc'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_cover(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['cover'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        index3 = self._offset\n        chunk0, max0 = None, self._offset + 3\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 == 'FEW':\n            address1 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n            self._offset = self._offset + 3\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::cover', '\"FEW\"'))\n        if address1 is FAILURE:\n            self._offset = index3\n            chunk1, max1 = None, self._offset + 3\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 == 'SCT':\n                address1 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                self._offset = self._offset + 3\n            else:\n                address1 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::cover', '\"SCT\"'))\n            if address1 is FAILURE:\n                self._offset = index3\n                chunk2, max2 = None, self._offset + 3\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 == 'BKN':\n                    address1 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                    self._offset = self._offset + 3\n                else:\n                    address1 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::cover', '\"BKN\"'))\n                if address1 is FAILURE:\n                    self._offset = index3\n                    chunk3, max3 = None, self._offset + 3\n                    if max3 <= self._input_size:\n                        chunk3 = self._input[self._offset:max3]\n                    if chunk3 == 'OVC':\n                        address1 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                        self._offset = self._offset + 3\n                    else:\n                        address1 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::cover', '\"OVC\"'))\n                    if address1 is FAILURE:\n                        self._offset = index3\n                        chunk4, max4 = None, self._offset + 2\n                        if max4 <= self._input_size:\n                            chunk4 = self._input[self._offset:max4]\n                        if chunk4 == 'VV':\n                            address1 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                            self._offset = self._offset + 2\n                        else:\n                            address1 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::cover', '\"VV\"'))\n                        if address1 is FAILURE:\n                            self._offset = index3\n                            chunk5, max5 = None, self._offset + 3\n                            if max5 <= self._input_size:\n                                chunk5 = self._input[self._offset:max5]\n                            if chunk5 == '///':\n                                address1 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                                self._offset = self._offset + 3\n                            else:\n                                address1 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::cover', '\"///\"'))\n                            if address1 is FAILURE:\n                                self._offset = index3\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index4 = self._offset\n            index5, elements1, address3 = self._offset, [], None\n            while True:\n                chunk6, max6 = None, self._offset + 1\n                if max6 <= self._input_size:\n                    chunk6 = self._input[self._offset:max6]\n                if chunk6 is not None and Grammar.REGEX_51.search(chunk6):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::cover', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                else:\n                    break\n            if len(elements1) >= 0:\n                address2 = TreeNode(self._input[index5:self._offset], index5, elements1)\n                self._offset = self._offset\n            else:\n                address2 = FAILURE\n            if address2 is FAILURE:\n                address2 = TreeNode(self._input[index4:index4], index4, [])\n                self._offset = index4\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address4 = FAILURE\n                index6 = self._offset\n                index7 = self._offset\n                chunk7, max7 = None, self._offset + 3\n                if max7 <= self._input_size:\n                    chunk7 = self._input[self._offset:max7]\n                if chunk7 == 'TCU':\n                    address4 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                    self._offset = self._offset + 3\n                else:\n                    address4 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::cover', '\"TCU\"'))\n                if address4 is FAILURE:\n                    self._offset = index7\n                    chunk8, max8 = None, self._offset + 2\n                    if max8 <= self._input_size:\n                        chunk8 = self._input[self._offset:max8]\n                    if chunk8 == 'CB':\n                        address4 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                        self._offset = self._offset + 2\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::cover', '\"CB\"'))\n                    if address4 is FAILURE:\n                        self._offset = index7\n                        index8, elements2 = self._offset, []\n                        address5 = FAILURE\n                        chunk9, max9 = None, self._offset + 2\n                        if max9 <= self._input_size:\n                            chunk9 = self._input[self._offset:max9]\n                        if chunk9 == '//':\n                            address5 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                            self._offset = self._offset + 2\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::cover', '\"//\"'))\n                        if address5 is not FAILURE:\n                            elements2.append(address5)\n                            address6 = FAILURE\n                            index9 = self._offset\n                            chunk10, max10 = None, self._offset + 1\n                            if max10 <= self._input_size:\n                                chunk10 = self._input[self._offset:max10]\n                            if chunk10 == '/':\n                                address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::cover', '\"/\"'))\n                            if address6 is FAILURE:\n                                address6 = TreeNode(self._input[index9:index9], index9, [])\n                                self._offset = index9\n                            if address6 is not FAILURE:\n                                elements2.append(address6)\n                            else:\n                                elements2 = None\n                                self._offset = index8\n                        else:\n                            elements2 = None\n                            self._offset = index8\n                        if elements2 is None:\n                            address4 = FAILURE\n                        else:\n                            address4 = TreeNode(self._input[index8:self._offset], index8, elements2)\n                            self._offset = self._offset\n                        if address4 is FAILURE:\n                            self._offset = index7\n                if address4 is FAILURE:\n                    address4 = TreeNode(self._input[index6:index6], index6, [])\n                    self._offset = index6\n                if address4 is not FAILURE:\n                    elements0.append(address4)\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            self._offset = index1\n            index10 = self._offset\n            chunk11, max11 = None, self._offset + 3\n            if max11 <= self._input_size:\n                chunk11 = self._input[self._offset:max11]\n            if chunk11 == 'CLR':\n                address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                self._offset = self._offset + 3\n            else:\n                address0 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::cover', '\"CLR\"'))\n            if address0 is FAILURE:\n                self._offset = index10\n                chunk12, max12 = None, self._offset + 3\n                if max12 <= self._input_size:\n                    chunk12 = self._input[self._offset:max12]\n                if chunk12 == 'SKC':\n                    address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                    self._offset = self._offset + 3\n                else:\n                    address0 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::cover', '\"SKC\"'))\n                if address0 is FAILURE:\n                    self._offset = index10\n                    chunk13, max13 = None, self._offset + 3\n                    if max13 <= self._input_size:\n                        chunk13 = self._input[self._offset:max13]\n                    if chunk13 == 'NSC':\n                        address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                        self._offset = self._offset + 3\n                    else:\n                        address0 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::cover', '\"NSC\"'))\n                    if address0 is FAILURE:\n                        self._offset = index10\n                        chunk14, max14 = None, self._offset + 3\n                        if max14 <= self._input_size:\n                            chunk14 = self._input[self._offset:max14]\n                        if chunk14 == 'NCD':\n                            address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                            self._offset = self._offset + 3\n                        else:\n                            address0 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::cover', '\"NCD\"'))\n                        if address0 is FAILURE:\n                            self._offset = index10\n            if address0 is FAILURE:\n                self._offset = index1\n                address0 = self._read_wx()\n                if address0 is FAILURE:\n                    self._offset = index1\n                    chunk15, max15 = None, self._offset + 2\n                    if max15 <= self._input_size:\n                        chunk15 = self._input[self._offset:max15]\n                    if chunk15 == '//':\n                        address0 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                        self._offset = self._offset + 2\n                    else:\n                        address0 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::cover', '\"//\"'))\n                    if address0 is FAILURE:\n                        self._offset = index1\n        self._cache['cover'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_temp_dewp(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['temp_dewp'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3 = self._offset\n            chunk0, max0 = None, self._offset + 2\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 == '//':\n                address2 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                self._offset = self._offset + 2\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::temp_dewp', '\"//\"'))\n            if address2 is FAILURE:\n                address2 = TreeNode(self._input[index3:index3], index3, [])\n                self._offset = index3\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                address3 = self._read_temp()\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    chunk1, max1 = None, self._offset + 1\n                    if max1 <= self._input_size:\n                        chunk1 = self._input[self._offset:max1]\n                    if chunk1 == '/':\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::temp_dewp', '\"/\"'))\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        address5 = self._read_dewp()\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            index4 = self._offset\n                            chunk2, max2 = None, self._offset + 2\n                            if max2 <= self._input_size:\n                                chunk2 = self._input[self._offset:max2]\n                            if chunk2 == '//':\n                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                self._offset = self._offset + 2\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::temp_dewp', '\"//\"'))\n                            if address6 is FAILURE:\n                                address6 = TreeNode(self._input[index4:index4], index4, [])\n                                self._offset = index4\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                            else:\n                                elements0 = None\n                                self._offset = index2\n                        else:\n                            elements0 = None\n                            self._offset = index2\n                    else:\n                        elements0 = None\n                        self._offset = index2\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode13(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['temp_dewp'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_temp(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['temp'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        index2 = self._offset\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 is not None and Grammar.REGEX_52.search(chunk0):\n            address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::temp', '[M]'))\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index2:index2], index2, [])\n            self._offset = index2\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3 = self._offset\n            chunk1, max1 = None, self._offset + 1\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 is not None and Grammar.REGEX_53.search(chunk1):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::temp', '[\\\\d]'))\n            if address2 is FAILURE:\n                address2 = TreeNode(self._input[index3:index3], index3, [])\n                self._offset = index3\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                index4 = self._offset\n                chunk2, max2 = None, self._offset + 1\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 is not None and Grammar.REGEX_54.search(chunk2):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::temp', '[\\\\d]'))\n                if address3 is FAILURE:\n                    address3 = TreeNode(self._input[index4:index4], index4, [])\n                    self._offset = index4\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['temp'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_dewp(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['dewp'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        index2 = self._offset\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 is not None and Grammar.REGEX_55.search(chunk0):\n            address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::dewp', '[M]'))\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index2:index2], index2, [])\n            self._offset = index2\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3 = self._offset\n            chunk1, max1 = None, self._offset + 1\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 is not None and Grammar.REGEX_56.search(chunk1):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::dewp', '[\\\\d]'))\n            if address2 is FAILURE:\n                address2 = TreeNode(self._input[index3:index3], index3, [])\n                self._offset = index3\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                index4 = self._offset\n                chunk2, max2 = None, self._offset + 1\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 is not None and Grammar.REGEX_57.search(chunk2):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::dewp', '[\\\\d]'))\n                if address3 is FAILURE:\n                    address3 = TreeNode(self._input[index4:index4], index4, [])\n                    self._offset = index4\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['dewp'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_altim(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['altim'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        index3 = self._offset\n        address1 = self._read_sep()\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index3:index3], index3, [])\n            self._offset = index3\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 is not None and Grammar.REGEX_58.search(chunk0):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::altim', '[\"Q\" / \"A\"]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_59.search(chunk1):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::altim', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    chunk2, max2 = None, self._offset + 1\n                    if max2 <= self._input_size:\n                        chunk2 = self._input[self._offset:max2]\n                    if chunk2 is not None and Grammar.REGEX_60.search(chunk2):\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::altim', '[\\\\d]'))\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        chunk3, max3 = None, self._offset + 1\n                        if max3 <= self._input_size:\n                            chunk3 = self._input[self._offset:max3]\n                        if chunk3 is not None and Grammar.REGEX_61.search(chunk3):\n                            address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::altim', '[\\\\d]'))\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            chunk4, max4 = None, self._offset + 1\n                            if max4 <= self._input_size:\n                                chunk4 = self._input[self._offset:max4]\n                            if chunk4 is not None and Grammar.REGEX_62.search(chunk4):\n                                address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::altim', '[\\\\d]'))\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                                address7 = FAILURE\n                                index4 = self._offset\n                                chunk5, max5 = None, self._offset + 1\n                                if max5 <= self._input_size:\n                                    chunk5 = self._input[self._offset:max5]\n                                if chunk5 == '=':\n                                    address7 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address7 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::altim', '\"=\"'))\n                                if address7 is FAILURE:\n                                    address7 = TreeNode(self._input[index4:index4], index4, [])\n                                    self._offset = index4\n                                if address7 is not FAILURE:\n                                    elements0.append(address7)\n                                else:\n                                    elements0 = None\n                                    self._offset = index2\n                            else:\n                                elements0 = None\n                                self._offset = index2\n                        else:\n                            elements0 = None\n                            self._offset = index2\n                    else:\n                        elements0 = None\n                        self._offset = index2\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['altim'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_remarks(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['remarks'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        index3 = self._offset\n        address1 = self._read_sep()\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index3:index3], index3, [])\n            self._offset = index3\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index4 = self._offset\n            chunk0, max0 = None, self._offset + 3\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 == 'RMK':\n                address2 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                self._offset = self._offset + 3\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::remarks', '\"RMK\"'))\n            if address2 is FAILURE:\n                self._offset = index4\n                index5, elements1, address3 = self._offset, [], None\n                while True:\n                    chunk1, max1 = None, self._offset + 5\n                    if max1 <= self._input_size:\n                        chunk1 = self._input[self._offset:max1]\n                    if chunk1 == 'NOSIG':\n                        address3 = TreeNode(self._input[self._offset:self._offset + 5], self._offset, [])\n                        self._offset = self._offset + 5\n                    else:\n                        address3 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::remarks', '\"NOSIG\"'))\n                    if address3 is not FAILURE:\n                        elements1.append(address3)\n                    else:\n                        break\n                if len(elements1) >= 0:\n                    address2 = TreeNode(self._input[index5:self._offset], index5, elements1)\n                    self._offset = self._offset\n                else:\n                    address2 = FAILURE\n                if address2 is FAILURE:\n                    self._offset = index4\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address4 = FAILURE\n                index6, elements2, address5 = self._offset, [], None\n                while True:\n                    if self._offset < self._input_size:\n                        address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address5 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::remarks', '<any char>'))\n                    if address5 is not FAILURE:\n                        elements2.append(address5)\n                    else:\n                        break\n                if len(elements2) >= 0:\n                    address4 = TreeNode(self._input[index6:self._offset], index6, elements2)\n                    self._offset = self._offset\n                else:\n                    address4 = FAILURE\n                if address4 is not FAILURE:\n                    elements0.append(address4)\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['remarks'][index0] = (address0, self._offset)\n        return address0\n\n    def _read_end(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['end'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        index3 = self._offset\n        address1 = self._read_sep()\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index3:index3], index3, [])\n            self._offset = index3\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 == '=':\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::end', '\"=\"'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['end'][index0] = (address0, self._offset)\n        return address0",
  "class Parser(Grammar):\n    def __init__(self, input, actions, types):\n        self._input = input\n        self._input_size = len(input)\n        self._actions = actions\n        self._types = types\n        self._offset = 0\n        self._cache = defaultdict(dict)\n        self._failure = 0\n        self._expected = []\n\n    def parse(self):\n        tree = self._read_ob()\n        if tree is not FAILURE and self._offset == self._input_size:\n            return tree\n        if not self._expected:\n            self._failure = self._offset\n            self._expected.append(('METAR', '<EOF>'))\n        raise ParseError(format_error(self._input, self._failure, self._expected))",
  "class ParseError(SyntaxError):\n    pass",
  "def parse(input, actions=None, types=None):\n    parser = Parser(input, actions, types)\n    return parser.parse()",
  "def format_error(input, offset, expected):\n    lines = input.split('\\n')\n    line_no, position = 0, 0\n\n    while position <= offset:\n        position += len(lines[line_no]) + 1\n        line_no += 1\n\n    line = lines[line_no - 1]\n    message = 'Line ' + str(line_no) + ': expected one of:\\n\\n'\n\n    for pair in expected:\n        message += '    - ' + pair[1] + ' from ' + pair[0] + '\\n'\n\n    number = str(line_no)\n    while len(number) < 6:\n        number = ' ' + number\n\n    message += '\\n' + number + ' | ' + line + '\\n'\n    message += ' ' * (len(line) + 10 + offset - position)\n    return message + '^'",
  "def __init__(self, text, offset, elements):\n        self.text = text\n        self.offset = offset\n        self.elements = elements",
  "def __iter__(self):\n        for el in self.elements:\n            yield el",
  "def __init__(self, text, offset, elements):\n        super(TreeNode1, self).__init__(text, offset, elements)\n        self.metar = elements[0]\n        self.siteid = elements[1]\n        self.datetime = elements[2]\n        self.auto = elements[3]\n        self.wind = elements[4]\n        self.vis = elements[5]\n        self.run = elements[6]\n        self.curwx = elements[7]\n        self.skyc = elements[8]\n        self.temp_dewp = elements[9]\n        self.altim = elements[10]\n        self.remarks = elements[11]\n        self.end = elements[12]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode2, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode3, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode4, self).__init__(text, offset, elements)\n        self.wind_dir = elements[1]\n        self.wind_spd = elements[2]\n        self.gust = elements[3]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode5, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode6, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode7, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode8, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode9, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode10, self).__init__(text, offset, elements)\n        self.sep = elements[0]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode11, self).__init__(text, offset, elements)\n        self.sep = elements[0]\n        self.wx = elements[1]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode12, self).__init__(text, offset, elements)\n        self.sep = elements[0]\n        self.cover = elements[1]",
  "def __init__(self, text, offset, elements):\n        super(TreeNode13, self).__init__(text, offset, elements)\n        self.sep = elements[0]\n        self.temp = elements[2]\n        self.dewp = elements[4]",
  "def _read_ob(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['ob'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_metar()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            address2 = self._read_siteid()\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                address3 = self._read_datetime()\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    address4 = self._read_auto()\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        address5 = self._read_wind()\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            address6 = self._read_vis()\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                                address7 = FAILURE\n                                address7 = self._read_run()\n                                if address7 is not FAILURE:\n                                    elements0.append(address7)\n                                    address8 = FAILURE\n                                    address8 = self._read_curwx()\n                                    if address8 is not FAILURE:\n                                        elements0.append(address8)\n                                        address9 = FAILURE\n                                        address9 = self._read_skyc()\n                                        if address9 is not FAILURE:\n                                            elements0.append(address9)\n                                            address10 = FAILURE\n                                            address10 = self._read_temp_dewp()\n                                            if address10 is not FAILURE:\n                                                elements0.append(address10)\n                                                address11 = FAILURE\n                                                address11 = self._read_altim()\n                                                if address11 is not FAILURE:\n                                                    elements0.append(address11)\n                                                    address12 = FAILURE\n                                                    address12 = self._read_remarks()\n                                                    if address12 is not FAILURE:\n                                                        elements0.append(address12)\n                                                        address13 = FAILURE\n                                                        address13 = self._read_end()\n                                                        if address13 is not FAILURE:\n                                                            elements0.append(address13)\n                                                        else:\n                                                            elements0 = None\n                                                            self._offset = index1\n                                                    else:\n                                                        elements0 = None\n                                                        self._offset = index1\n                                                else:\n                                                    elements0 = None\n                                                    self._offset = index1\n                                            else:\n                                                elements0 = None\n                                                self._offset = index1\n                                        else:\n                                            elements0 = None\n                                            self._offset = index1\n                                    else:\n                                        elements0 = None\n                                        self._offset = index1\n                                else:\n                                    elements0 = None\n                                    self._offset = index1\n                            else:\n                                elements0 = None\n                                self._offset = index1\n                        else:\n                            elements0 = None\n                            self._offset = index1\n                    else:\n                        elements0 = None\n                        self._offset = index1\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode1(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['ob'][index0] = (address0, self._offset)\n        return address0",
  "def _read_metar(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['metar'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        index2 = self._offset\n        chunk0, max0 = None, self._offset + 4\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 == 'COR ':\n            address1 = TreeNode(self._input[self._offset:self._offset + 4], self._offset, [])\n            self._offset = self._offset + 4\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::metar', '\"COR \"'))\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index2:index2], index2, [])\n            self._offset = index2\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3 = self._offset\n            index4 = self._offset\n            chunk1, max1 = None, self._offset + 5\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 == 'METAR':\n                address2 = TreeNode(self._input[self._offset:self._offset + 5], self._offset, [])\n                self._offset = self._offset + 5\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::metar', '\"METAR\"'))\n            if address2 is FAILURE:\n                self._offset = index4\n                chunk2, max2 = None, self._offset + 5\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 == 'SPECI':\n                    address2 = TreeNode(self._input[self._offset:self._offset + 5], self._offset, [])\n                    self._offset = self._offset + 5\n                else:\n                    address2 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::metar', '\"SPECI\"'))\n                if address2 is FAILURE:\n                    self._offset = index4\n            if address2 is FAILURE:\n                address2 = TreeNode(self._input[index3:index3], index3, [])\n                self._offset = index3\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                index5 = self._offset\n                address3 = self._read_auto()\n                if address3 is FAILURE:\n                    address3 = TreeNode(self._input[index5:index5], index5, [])\n                    self._offset = index5\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['metar'][index0] = (address0, self._offset)\n        return address0",
  "def _read_sep(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['sep'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0, address1 = self._offset, [], None\n        while True:\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 == ' ':\n                address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address1 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::sep', '\" \"'))\n            if address1 is not FAILURE:\n                elements0.append(address1)\n            else:\n                break\n        if len(elements0) >= 1:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        else:\n            address0 = FAILURE\n        self._cache['sep'][index0] = (address0, self._offset)\n        return address0",
  "def _read_siteid(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['siteid'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        index2 = self._offset\n        address1 = self._read_sep()\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index2:index2], index2, [])\n            self._offset = index2\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 is not None and Grammar.REGEX_1.search(chunk0):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::siteid', '[0-9A-Z]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_2.search(chunk1):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::siteid', '[0-9A-Z]'))\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    chunk2, max2 = None, self._offset + 1\n                    if max2 <= self._input_size:\n                        chunk2 = self._input[self._offset:max2]\n                    if chunk2 is not None and Grammar.REGEX_3.search(chunk2):\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::siteid', '[0-9A-Z]'))\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        chunk3, max3 = None, self._offset + 1\n                        if max3 <= self._input_size:\n                            chunk3 = self._input[self._offset:max3]\n                        if chunk3 is not None and Grammar.REGEX_4.search(chunk3):\n                            address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::siteid', '[0-9A-Z]'))\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                        else:\n                            elements0 = None\n                            self._offset = index1\n                    else:\n                        elements0 = None\n                        self._offset = index1\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['siteid'][index0] = (address0, self._offset)\n        return address0",
  "def _read_datetime(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['datetime'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index2, elements1, address3 = self._offset, [], None\n            while True:\n                chunk0, max0 = None, self._offset + 1\n                if max0 <= self._input_size:\n                    chunk0 = self._input[self._offset:max0]\n                if chunk0 is not None and Grammar.REGEX_5.search(chunk0):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::datetime', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                else:\n                    break\n            if len(elements1) >= 1:\n                address2 = TreeNode(self._input[index2:self._offset], index2, elements1)\n                self._offset = self._offset\n            else:\n                address2 = FAILURE\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address4 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 == 'Z':\n                    address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address4 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::datetime', '\"Z\"'))\n                if address4 is not FAILURE:\n                    elements0.append(address4)\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode2(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['datetime'][index0] = (address0, self._offset)\n        return address0",
  "def _read_auto(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['auto'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0, address1 = self._offset, [], None\n        while True:\n            index3, elements1 = self._offset, []\n            address2 = FAILURE\n            address2 = self._read_sep()\n            if address2 is not FAILURE:\n                elements1.append(address2)\n                address3 = FAILURE\n                index4 = self._offset\n                chunk0, max0 = None, self._offset + 4\n                if max0 <= self._input_size:\n                    chunk0 = self._input[self._offset:max0]\n                if chunk0 == 'AUTO':\n                    address3 = TreeNode(self._input[self._offset:self._offset + 4], self._offset, [])\n                    self._offset = self._offset + 4\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::auto', '\"AUTO\"'))\n                if address3 is FAILURE:\n                    self._offset = index4\n                    chunk1, max1 = None, self._offset + 3\n                    if max1 <= self._input_size:\n                        chunk1 = self._input[self._offset:max1]\n                    if chunk1 == 'COR':\n                        address3 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                        self._offset = self._offset + 3\n                    else:\n                        address3 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::auto', '\"COR\"'))\n                    if address3 is FAILURE:\n                        self._offset = index4\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                else:\n                    elements1 = None\n                    self._offset = index3\n            else:\n                elements1 = None\n                self._offset = index3\n            if elements1 is None:\n                address1 = FAILURE\n            else:\n                address1 = TreeNode3(self._input[index3:self._offset], index3, elements1)\n                self._offset = self._offset\n            if address1 is not FAILURE:\n                elements0.append(address1)\n            else:\n                break\n        if len(elements0) >= 1:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        else:\n            address0 = FAILURE\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['auto'][index0] = (address0, self._offset)\n        return address0",
  "def _read_wind(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['wind'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        index3 = self._offset\n        address1 = self._read_sep()\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index3:index3], index3, [])\n            self._offset = index3\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            address2 = self._read_wind_dir()\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                address3 = self._read_wind_spd()\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    address4 = self._read_gust()\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        index4 = self._offset\n                        chunk0, max0 = None, self._offset + 2\n                        if max0 <= self._input_size:\n                            chunk0 = self._input[self._offset:max0]\n                        if chunk0 == 'KT':\n                            address5 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                            self._offset = self._offset + 2\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::wind', '\"KT\"'))\n                        if address5 is FAILURE:\n                            self._offset = index4\n                            chunk1, max1 = None, self._offset + 3\n                            if max1 <= self._input_size:\n                                chunk1 = self._input[self._offset:max1]\n                            if chunk1 == 'MPS':\n                                address5 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                                self._offset = self._offset + 3\n                            else:\n                                address5 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::wind', '\"MPS\"'))\n                            if address5 is FAILURE:\n                                self._offset = index4\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            index5 = self._offset\n                            address6 = self._read_varwind()\n                            if address6 is FAILURE:\n                                address6 = TreeNode(self._input[index5:index5], index5, [])\n                                self._offset = index5\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                            else:\n                                elements0 = None\n                                self._offset = index2\n                        else:\n                            elements0 = None\n                            self._offset = index2\n                    else:\n                        elements0 = None\n                        self._offset = index2\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode4(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['wind'][index0] = (address0, self._offset)\n        return address0",
  "def _read_wind_dir(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['wind_dir'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2 = self._offset\n        index3, elements0 = self._offset, []\n        address1 = FAILURE\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 is not None and Grammar.REGEX_6.search(chunk0):\n            address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::wind_dir', '[\\\\d]'))\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk1, max1 = None, self._offset + 1\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 is not None and Grammar.REGEX_7.search(chunk1):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wind_dir', '[\\\\d]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                chunk2, max2 = None, self._offset + 1\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 is not None and Grammar.REGEX_8.search(chunk2):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::wind_dir', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                else:\n                    elements0 = None\n                    self._offset = index3\n            else:\n                elements0 = None\n                self._offset = index3\n        else:\n            elements0 = None\n            self._offset = index3\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index3:self._offset], index3, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            self._offset = index2\n            chunk3, max3 = None, self._offset + 3\n            if max3 <= self._input_size:\n                chunk3 = self._input[self._offset:max3]\n            if chunk3 == 'VAR':\n                address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                self._offset = self._offset + 3\n            else:\n                address0 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wind_dir', '\\'VAR\\''))\n            if address0 is FAILURE:\n                self._offset = index2\n                chunk4, max4 = None, self._offset + 3\n                if max4 <= self._input_size:\n                    chunk4 = self._input[self._offset:max4]\n                if chunk4 == 'VRB':\n                    address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                    self._offset = self._offset + 3\n                else:\n                    address0 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::wind_dir', '\\'VRB\\''))\n                if address0 is FAILURE:\n                    self._offset = index2\n                    chunk5, max5 = None, self._offset + 3\n                    if max5 <= self._input_size:\n                        chunk5 = self._input[self._offset:max5]\n                    if chunk5 == '///':\n                        address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                        self._offset = self._offset + 3\n                    else:\n                        address0 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::wind_dir', '\"///\"'))\n                    if address0 is FAILURE:\n                        self._offset = index2\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['wind_dir'][index0] = (address0, self._offset)\n        return address0",
  "def _read_wind_spd(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['wind_spd'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2 = self._offset\n        index3, elements0 = self._offset, []\n        address1 = FAILURE\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 is not None and Grammar.REGEX_9.search(chunk0):\n            address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::wind_spd', '[\\\\d]'))\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk1, max1 = None, self._offset + 1\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 is not None and Grammar.REGEX_10.search(chunk1):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wind_spd', '[\\\\d]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                index4 = self._offset\n                chunk2, max2 = None, self._offset + 1\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 is not None and Grammar.REGEX_11.search(chunk2):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::wind_spd', '[\\\\d]'))\n                if address3 is FAILURE:\n                    address3 = TreeNode(self._input[index4:index4], index4, [])\n                    self._offset = index4\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                else:\n                    elements0 = None\n                    self._offset = index3\n            else:\n                elements0 = None\n                self._offset = index3\n        else:\n            elements0 = None\n            self._offset = index3\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index3:self._offset], index3, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            self._offset = index2\n            chunk3, max3 = None, self._offset + 2\n            if max3 <= self._input_size:\n                chunk3 = self._input[self._offset:max3]\n            if chunk3 == '//':\n                address0 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                self._offset = self._offset + 2\n            else:\n                address0 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wind_spd', '\"//\"'))\n            if address0 is FAILURE:\n                self._offset = index2\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['wind_spd'][index0] = (address0, self._offset)\n        return address0",
  "def _read_gust(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['gust'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 == 'G':\n            address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::gust', '\"G\"'))\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3, elements1, address3 = self._offset, [], None\n            while True:\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_12.search(chunk1):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::gust', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                else:\n                    break\n            if len(elements1) >= 1:\n                address2 = TreeNode(self._input[index3:self._offset], index3, elements1)\n                self._offset = self._offset\n            else:\n                address2 = FAILURE\n            if address2 is not FAILURE:\n                elements0.append(address2)\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['gust'][index0] = (address0, self._offset)\n        return address0",
  "def _read_varwind(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['varwind'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 is not None and Grammar.REGEX_13.search(chunk0):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::varwind', '[\\\\d]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_14.search(chunk1):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::varwind', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    chunk2, max2 = None, self._offset + 1\n                    if max2 <= self._input_size:\n                        chunk2 = self._input[self._offset:max2]\n                    if chunk2 is not None and Grammar.REGEX_15.search(chunk2):\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::varwind', '[\\\\d]'))\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        chunk3, max3 = None, self._offset + 1\n                        if max3 <= self._input_size:\n                            chunk3 = self._input[self._offset:max3]\n                        if chunk3 == 'V':\n                            address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::varwind', '\"V\"'))\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            chunk4, max4 = None, self._offset + 1\n                            if max4 <= self._input_size:\n                                chunk4 = self._input[self._offset:max4]\n                            if chunk4 is not None and Grammar.REGEX_16.search(chunk4):\n                                address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::varwind', '[\\\\d]'))\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                                address7 = FAILURE\n                                chunk5, max5 = None, self._offset + 1\n                                if max5 <= self._input_size:\n                                    chunk5 = self._input[self._offset:max5]\n                                if chunk5 is not None and Grammar.REGEX_17.search(chunk5):\n                                    address7 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address7 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::varwind', '[\\\\d]'))\n                                if address7 is not FAILURE:\n                                    elements0.append(address7)\n                                    address8 = FAILURE\n                                    chunk6, max6 = None, self._offset + 1\n                                    if max6 <= self._input_size:\n                                        chunk6 = self._input[self._offset:max6]\n                                    if chunk6 is not None and Grammar.REGEX_18.search(chunk6):\n                                        address8 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                        self._offset = self._offset + 1\n                                    else:\n                                        address8 = FAILURE\n                                        if self._offset > self._failure:\n                                            self._failure = self._offset\n                                            self._expected = []\n                                        if self._offset == self._failure:\n                                            self._expected.append(('METAR::varwind', '[\\\\d]'))\n                                    if address8 is not FAILURE:\n                                        elements0.append(address8)\n                                    else:\n                                        elements0 = None\n                                        self._offset = index1\n                                else:\n                                    elements0 = None\n                                    self._offset = index1\n                            else:\n                                elements0 = None\n                                self._offset = index1\n                        else:\n                            elements0 = None\n                            self._offset = index1\n                    else:\n                        elements0 = None\n                        self._offset = index1\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode5(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['varwind'][index0] = (address0, self._offset)\n        return address0",
  "def _read_vis(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['vis'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3 = self._offset\n            index4, elements1 = self._offset, []\n            address3 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 is not None and Grammar.REGEX_19.search(chunk0):\n                address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address3 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::vis', '[\\\\d]'))\n            if address3 is not FAILURE:\n                elements1.append(address3)\n                address4 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_20.search(chunk1):\n                    address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address4 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::vis', '[\\\\d]'))\n                if address4 is not FAILURE:\n                    elements1.append(address4)\n                    address5 = FAILURE\n                    chunk2, max2 = None, self._offset + 1\n                    if max2 <= self._input_size:\n                        chunk2 = self._input[self._offset:max2]\n                    if chunk2 is not None and Grammar.REGEX_21.search(chunk2):\n                        address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address5 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::vis', '[\\\\d]'))\n                    if address5 is not FAILURE:\n                        elements1.append(address5)\n                        address6 = FAILURE\n                        chunk3, max3 = None, self._offset + 1\n                        if max3 <= self._input_size:\n                            chunk3 = self._input[self._offset:max3]\n                        if chunk3 is not None and Grammar.REGEX_22.search(chunk3):\n                            address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address6 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::vis', '[\\\\d]'))\n                        if address6 is not FAILURE:\n                            elements1.append(address6)\n                            address7 = FAILURE\n                            index5 = self._offset\n                            chunk4, max4 = None, self._offset + 3\n                            if max4 <= self._input_size:\n                                chunk4 = self._input[self._offset:max4]\n                            if chunk4 == 'NDV':\n                                address7 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                                self._offset = self._offset + 3\n                            else:\n                                address7 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::vis', '\"NDV\"'))\n                            if address7 is FAILURE:\n                                address7 = TreeNode(self._input[index5:index5], index5, [])\n                                self._offset = index5\n                            if address7 is not FAILURE:\n                                elements1.append(address7)\n                            else:\n                                elements1 = None\n                                self._offset = index4\n                        else:\n                            elements1 = None\n                            self._offset = index4\n                    else:\n                        elements1 = None\n                        self._offset = index4\n                else:\n                    elements1 = None\n                    self._offset = index4\n            else:\n                elements1 = None\n                self._offset = index4\n            if elements1 is None:\n                address2 = FAILURE\n            else:\n                address2 = TreeNode(self._input[index4:self._offset], index4, elements1)\n                self._offset = self._offset\n            if address2 is FAILURE:\n                self._offset = index3\n                index6, elements2 = self._offset, []\n                address8 = FAILURE\n                chunk5, max5 = None, self._offset + 1\n                if max5 <= self._input_size:\n                    chunk5 = self._input[self._offset:max5]\n                if chunk5 is not None and Grammar.REGEX_23.search(chunk5):\n                    address8 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address8 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::vis', '[\\\\d]'))\n                if address8 is not FAILURE:\n                    elements2.append(address8)\n                    address9 = FAILURE\n                    index7 = self._offset\n                    index8 = self._offset\n                    chunk6, max6 = None, self._offset + 1\n                    if max6 <= self._input_size:\n                        chunk6 = self._input[self._offset:max6]\n                    if chunk6 is not None and Grammar.REGEX_24.search(chunk6):\n                        address9 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address9 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::vis', '[\\\\d]'))\n                    if address9 is FAILURE:\n                        self._offset = index8\n                        index9, elements3 = self._offset, []\n                        address10 = FAILURE\n                        index10 = self._offset\n                        index11, elements4 = self._offset, []\n                        address11 = FAILURE\n                        chunk7, max7 = None, self._offset + 1\n                        if max7 <= self._input_size:\n                            chunk7 = self._input[self._offset:max7]\n                        if chunk7 == ' ':\n                            address11 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address11 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::vis', '\" \"'))\n                        if address11 is not FAILURE:\n                            elements4.append(address11)\n                            address12 = FAILURE\n                            chunk8, max8 = None, self._offset + 1\n                            if max8 <= self._input_size:\n                                chunk8 = self._input[self._offset:max8]\n                            if chunk8 is not None and Grammar.REGEX_25.search(chunk8):\n                                address12 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address12 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::vis', '[\\\\d]'))\n                            if address12 is not FAILURE:\n                                elements4.append(address12)\n                            else:\n                                elements4 = None\n                                self._offset = index11\n                        else:\n                            elements4 = None\n                            self._offset = index11\n                        if elements4 is None:\n                            address10 = FAILURE\n                        else:\n                            address10 = TreeNode(self._input[index11:self._offset], index11, elements4)\n                            self._offset = self._offset\n                        if address10 is FAILURE:\n                            address10 = TreeNode(self._input[index10:index10], index10, [])\n                            self._offset = index10\n                        if address10 is not FAILURE:\n                            elements3.append(address10)\n                            address13 = FAILURE\n                            chunk9, max9 = None, self._offset + 1\n                            if max9 <= self._input_size:\n                                chunk9 = self._input[self._offset:max9]\n                            if chunk9 == '/':\n                                address13 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address13 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::vis', '\"/\"'))\n                            if address13 is not FAILURE:\n                                elements3.append(address13)\n                                address14 = FAILURE\n                                chunk10, max10 = None, self._offset + 1\n                                if max10 <= self._input_size:\n                                    chunk10 = self._input[self._offset:max10]\n                                if chunk10 is not None and Grammar.REGEX_26.search(chunk10):\n                                    address14 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address14 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::vis', '[\\\\d]'))\n                                if address14 is not FAILURE:\n                                    elements3.append(address14)\n                                    address15 = FAILURE\n                                    index12 = self._offset\n                                    chunk11, max11 = None, self._offset + 1\n                                    if max11 <= self._input_size:\n                                        chunk11 = self._input[self._offset:max11]\n                                    if chunk11 is not None and Grammar.REGEX_27.search(chunk11):\n                                        address15 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                        self._offset = self._offset + 1\n                                    else:\n                                        address15 = FAILURE\n                                        if self._offset > self._failure:\n                                            self._failure = self._offset\n                                            self._expected = []\n                                        if self._offset == self._failure:\n                                            self._expected.append(('METAR::vis', '[\\\\d]'))\n                                    if address15 is FAILURE:\n                                        address15 = TreeNode(self._input[index12:index12], index12, [])\n                                        self._offset = index12\n                                    if address15 is not FAILURE:\n                                        elements3.append(address15)\n                                    else:\n                                        elements3 = None\n                                        self._offset = index9\n                                else:\n                                    elements3 = None\n                                    self._offset = index9\n                            else:\n                                elements3 = None\n                                self._offset = index9\n                        else:\n                            elements3 = None\n                            self._offset = index9\n                        if elements3 is None:\n                            address9 = FAILURE\n                        else:\n                            address9 = TreeNode(self._input[index9:self._offset], index9, elements3)\n                            self._offset = self._offset\n                        if address9 is FAILURE:\n                            self._offset = index8\n                    if address9 is FAILURE:\n                        address9 = TreeNode(self._input[index7:index7], index7, [])\n                        self._offset = index7\n                    if address9 is not FAILURE:\n                        elements2.append(address9)\n                        address16 = FAILURE\n                        chunk12, max12 = None, self._offset + 2\n                        if max12 <= self._input_size:\n                            chunk12 = self._input[self._offset:max12]\n                        if chunk12 == 'SM':\n                            address16 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                            self._offset = self._offset + 2\n                        else:\n                            address16 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::vis', '\"SM\"'))\n                        if address16 is not FAILURE:\n                            elements2.append(address16)\n                        else:\n                            elements2 = None\n                            self._offset = index6\n                    else:\n                        elements2 = None\n                        self._offset = index6\n                else:\n                    elements2 = None\n                    self._offset = index6\n                if elements2 is None:\n                    address2 = FAILURE\n                else:\n                    address2 = TreeNode(self._input[index6:self._offset], index6, elements2)\n                    self._offset = self._offset\n                if address2 is FAILURE:\n                    self._offset = index3\n                    index13, elements5 = self._offset, []\n                    address17 = FAILURE\n                    chunk13, max13 = None, self._offset + 1\n                    if max13 <= self._input_size:\n                        chunk13 = self._input[self._offset:max13]\n                    if chunk13 == 'M':\n                        address17 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address17 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::vis', '\"M\"'))\n                    if address17 is not FAILURE:\n                        elements5.append(address17)\n                        address18 = FAILURE\n                        chunk14, max14 = None, self._offset + 1\n                        if max14 <= self._input_size:\n                            chunk14 = self._input[self._offset:max14]\n                        if chunk14 is not None and Grammar.REGEX_28.search(chunk14):\n                            address18 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address18 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::vis', '[\\\\d]'))\n                        if address18 is not FAILURE:\n                            elements5.append(address18)\n                            address19 = FAILURE\n                            chunk15, max15 = None, self._offset + 1\n                            if max15 <= self._input_size:\n                                chunk15 = self._input[self._offset:max15]\n                            if chunk15 == '/':\n                                address19 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address19 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::vis', '\"/\"'))\n                            if address19 is not FAILURE:\n                                elements5.append(address19)\n                                address20 = FAILURE\n                                chunk16, max16 = None, self._offset + 1\n                                if max16 <= self._input_size:\n                                    chunk16 = self._input[self._offset:max16]\n                                if chunk16 is not None and Grammar.REGEX_29.search(chunk16):\n                                    address20 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address20 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::vis', '[\\\\d]'))\n                                if address20 is not FAILURE:\n                                    elements5.append(address20)\n                                    address21 = FAILURE\n                                    chunk17, max17 = None, self._offset + 2\n                                    if max17 <= self._input_size:\n                                        chunk17 = self._input[self._offset:max17]\n                                    if chunk17 == 'SM':\n                                        address21 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                        self._offset = self._offset + 2\n                                    else:\n                                        address21 = FAILURE\n                                        if self._offset > self._failure:\n                                            self._failure = self._offset\n                                            self._expected = []\n                                        if self._offset == self._failure:\n                                            self._expected.append(('METAR::vis', '\"SM\"'))\n                                    if address21 is not FAILURE:\n                                        elements5.append(address21)\n                                    else:\n                                        elements5 = None\n                                        self._offset = index13\n                                else:\n                                    elements5 = None\n                                    self._offset = index13\n                            else:\n                                elements5 = None\n                                self._offset = index13\n                        else:\n                            elements5 = None\n                            self._offset = index13\n                    else:\n                        elements5 = None\n                        self._offset = index13\n                    if elements5 is None:\n                        address2 = FAILURE\n                    else:\n                        address2 = TreeNode(self._input[index13:self._offset], index13, elements5)\n                        self._offset = self._offset\n                    if address2 is FAILURE:\n                        self._offset = index3\n                        chunk18, max18 = None, self._offset + 5\n                        if max18 <= self._input_size:\n                            chunk18 = self._input[self._offset:max18]\n                        if chunk18 == 'CAVOK':\n                            address2 = TreeNode(self._input[self._offset:self._offset + 5], self._offset, [])\n                            self._offset = self._offset + 5\n                        else:\n                            address2 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::vis', '\"CAVOK\"'))\n                        if address2 is FAILURE:\n                            self._offset = index3\n                            chunk19, max19 = None, self._offset + 4\n                            if max19 <= self._input_size:\n                                chunk19 = self._input[self._offset:max19]\n                            if chunk19 == '////':\n                                address2 = TreeNode(self._input[self._offset:self._offset + 4], self._offset, [])\n                                self._offset = self._offset + 4\n                            else:\n                                address2 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::vis', '\"////\"'))\n                            if address2 is FAILURE:\n                                self._offset = index3\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address22 = FAILURE\n                index14 = self._offset\n                address22 = self._read_varvis()\n                if address22 is FAILURE:\n                    address22 = TreeNode(self._input[index14:index14], index14, [])\n                    self._offset = index14\n                if address22 is not FAILURE:\n                    elements0.append(address22)\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode6(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['vis'][index0] = (address0, self._offset)\n        return address0",
  "def _read_varvis(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['varvis'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 is not None and Grammar.REGEX_30.search(chunk0):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::varvis', '[\\\\d]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_31.search(chunk1):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::varvis', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    chunk2, max2 = None, self._offset + 1\n                    if max2 <= self._input_size:\n                        chunk2 = self._input[self._offset:max2]\n                    if chunk2 is not None and Grammar.REGEX_32.search(chunk2):\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::varvis', '[\\\\d]'))\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        chunk3, max3 = None, self._offset + 1\n                        if max3 <= self._input_size:\n                            chunk3 = self._input[self._offset:max3]\n                        if chunk3 is not None and Grammar.REGEX_33.search(chunk3):\n                            address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::varvis', '[\\\\d]'))\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            index2 = self._offset\n                            chunk4, max4 = None, self._offset + 1\n                            if max4 <= self._input_size:\n                                chunk4 = self._input[self._offset:max4]\n                            if chunk4 is not None and Grammar.REGEX_34.search(chunk4):\n                                address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::varvis', '[NSEW]'))\n                            if address6 is FAILURE:\n                                address6 = TreeNode(self._input[index2:index2], index2, [])\n                                self._offset = index2\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                                address7 = FAILURE\n                                index3 = self._offset\n                                chunk5, max5 = None, self._offset + 1\n                                if max5 <= self._input_size:\n                                    chunk5 = self._input[self._offset:max5]\n                                if chunk5 is not None and Grammar.REGEX_35.search(chunk5):\n                                    address7 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address7 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::varvis', '[NSEW]'))\n                                if address7 is FAILURE:\n                                    address7 = TreeNode(self._input[index3:index3], index3, [])\n                                    self._offset = index3\n                                if address7 is not FAILURE:\n                                    elements0.append(address7)\n                                else:\n                                    elements0 = None\n                                    self._offset = index1\n                            else:\n                                elements0 = None\n                                self._offset = index1\n                        else:\n                            elements0 = None\n                            self._offset = index1\n                    else:\n                        elements0 = None\n                        self._offset = index1\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode7(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['varvis'][index0] = (address0, self._offset)\n        return address0",
  "def _read_run(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['run'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0, address1 = self._offset, [], None\n        while True:\n            index2, elements1 = self._offset, []\n            address2 = FAILURE\n            address2 = self._read_sep()\n            if address2 is not FAILURE:\n                elements1.append(address2)\n                address3 = FAILURE\n                chunk0, max0 = None, self._offset + 1\n                if max0 <= self._input_size:\n                    chunk0 = self._input[self._offset:max0]\n                if chunk0 == 'R':\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::run', '\"R\"'))\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                    address4 = FAILURE\n                    index3 = self._offset\n                    chunk1, max1 = None, self._offset + 1\n                    if max1 <= self._input_size:\n                        chunk1 = self._input[self._offset:max1]\n                    if chunk1 is not None and Grammar.REGEX_36.search(chunk1):\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::run', '[LRC]'))\n                    if address4 is FAILURE:\n                        address4 = TreeNode(self._input[index3:index3], index3, [])\n                        self._offset = index3\n                    if address4 is not FAILURE:\n                        elements1.append(address4)\n                        address5 = FAILURE\n                        chunk2, max2 = None, self._offset + 1\n                        if max2 <= self._input_size:\n                            chunk2 = self._input[self._offset:max2]\n                        if chunk2 is not None and Grammar.REGEX_37.search(chunk2):\n                            address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::run', '[\\\\d]'))\n                        if address5 is not FAILURE:\n                            elements1.append(address5)\n                            address6 = FAILURE\n                            chunk3, max3 = None, self._offset + 1\n                            if max3 <= self._input_size:\n                                chunk3 = self._input[self._offset:max3]\n                            if chunk3 is not None and Grammar.REGEX_38.search(chunk3):\n                                address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::run', '[\\\\d]'))\n                            if address6 is not FAILURE:\n                                elements1.append(address6)\n                                address7 = FAILURE\n                                index4 = self._offset\n                                chunk4, max4 = None, self._offset + 1\n                                if max4 <= self._input_size:\n                                    chunk4 = self._input[self._offset:max4]\n                                if chunk4 is not None and Grammar.REGEX_39.search(chunk4):\n                                    address7 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address7 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::run', '[LRC]'))\n                                if address7 is FAILURE:\n                                    address7 = TreeNode(self._input[index4:index4], index4, [])\n                                    self._offset = index4\n                                if address7 is not FAILURE:\n                                    elements1.append(address7)\n                                    address8 = FAILURE\n                                    chunk5, max5 = None, self._offset + 1\n                                    if max5 <= self._input_size:\n                                        chunk5 = self._input[self._offset:max5]\n                                    if chunk5 == '/':\n                                        address8 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                        self._offset = self._offset + 1\n                                    else:\n                                        address8 = FAILURE\n                                        if self._offset > self._failure:\n                                            self._failure = self._offset\n                                            self._expected = []\n                                        if self._offset == self._failure:\n                                            self._expected.append(('METAR::run', '\"/\"'))\n                                    if address8 is not FAILURE:\n                                        elements1.append(address8)\n                                        address9 = FAILURE\n                                        index5 = self._offset\n                                        index6, elements2 = self._offset, []\n                                        address10 = FAILURE\n                                        chunk6, max6 = None, self._offset + 1\n                                        if max6 <= self._input_size:\n                                            chunk6 = self._input[self._offset:max6]\n                                        if chunk6 is not None and Grammar.REGEX_40.search(chunk6):\n                                            address10 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                            self._offset = self._offset + 1\n                                        else:\n                                            address10 = FAILURE\n                                            if self._offset > self._failure:\n                                                self._failure = self._offset\n                                                self._expected = []\n                                            if self._offset == self._failure:\n                                                self._expected.append(('METAR::run', '[\\\\d]'))\n                                        if address10 is not FAILURE:\n                                            elements2.append(address10)\n                                            address11 = FAILURE\n                                            chunk7, max7 = None, self._offset + 1\n                                            if max7 <= self._input_size:\n                                                chunk7 = self._input[self._offset:max7]\n                                            if chunk7 is not None and Grammar.REGEX_41.search(chunk7):\n                                                address11 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                self._offset = self._offset + 1\n                                            else:\n                                                address11 = FAILURE\n                                                if self._offset > self._failure:\n                                                    self._failure = self._offset\n                                                    self._expected = []\n                                                if self._offset == self._failure:\n                                                    self._expected.append(('METAR::run', '[\\\\d]'))\n                                            if address11 is not FAILURE:\n                                                elements2.append(address11)\n                                                address12 = FAILURE\n                                                chunk8, max8 = None, self._offset + 1\n                                                if max8 <= self._input_size:\n                                                    chunk8 = self._input[self._offset:max8]\n                                                if chunk8 is not None and Grammar.REGEX_42.search(chunk8):\n                                                    address12 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                    self._offset = self._offset + 1\n                                                else:\n                                                    address12 = FAILURE\n                                                    if self._offset > self._failure:\n                                                        self._failure = self._offset\n                                                        self._expected = []\n                                                    if self._offset == self._failure:\n                                                        self._expected.append(('METAR::run', '[\\\\d]'))\n                                                if address12 is not FAILURE:\n                                                    elements2.append(address12)\n                                                    address13 = FAILURE\n                                                    chunk9, max9 = None, self._offset + 1\n                                                    if max9 <= self._input_size:\n                                                        chunk9 = self._input[self._offset:max9]\n                                                    if chunk9 is not None and Grammar.REGEX_43.search(chunk9):\n                                                        address13 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                        self._offset = self._offset + 1\n                                                    else:\n                                                        address13 = FAILURE\n                                                        if self._offset > self._failure:\n                                                            self._failure = self._offset\n                                                            self._expected = []\n                                                        if self._offset == self._failure:\n                                                            self._expected.append(('METAR::run', '[\\\\d]'))\n                                                    if address13 is not FAILURE:\n                                                        elements2.append(address13)\n                                                        address14 = FAILURE\n                                                        chunk10, max10 = None, self._offset + 1\n                                                        if max10 <= self._input_size:\n                                                            chunk10 = self._input[self._offset:max10]\n                                                        if chunk10 == 'V':\n                                                            address14 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                            self._offset = self._offset + 1\n                                                        else:\n                                                            address14 = FAILURE\n                                                            if self._offset > self._failure:\n                                                                self._failure = self._offset\n                                                                self._expected = []\n                                                            if self._offset == self._failure:\n                                                                self._expected.append(('METAR::run', '\"V\"'))\n                                                        if address14 is not FAILURE:\n                                                            elements2.append(address14)\n                                                        else:\n                                                            elements2 = None\n                                                            self._offset = index6\n                                                    else:\n                                                        elements2 = None\n                                                        self._offset = index6\n                                                else:\n                                                    elements2 = None\n                                                    self._offset = index6\n                                            else:\n                                                elements2 = None\n                                                self._offset = index6\n                                        else:\n                                            elements2 = None\n                                            self._offset = index6\n                                        if elements2 is None:\n                                            address9 = FAILURE\n                                        else:\n                                            address9 = TreeNode(self._input[index6:self._offset], index6, elements2)\n                                            self._offset = self._offset\n                                        if address9 is FAILURE:\n                                            address9 = TreeNode(self._input[index5:index5], index5, [])\n                                            self._offset = index5\n                                        if address9 is not FAILURE:\n                                            elements1.append(address9)\n                                            address15 = FAILURE\n                                            index7 = self._offset\n                                            chunk11, max11 = None, self._offset + 1\n                                            if max11 <= self._input_size:\n                                                chunk11 = self._input[self._offset:max11]\n                                            if chunk11 is not None and Grammar.REGEX_44.search(chunk11):\n                                                address15 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                self._offset = self._offset + 1\n                                            else:\n                                                address15 = FAILURE\n                                                if self._offset > self._failure:\n                                                    self._failure = self._offset\n                                                    self._expected = []\n                                                if self._offset == self._failure:\n                                                    self._expected.append(('METAR::run', '[\"M\" / \"P\"]'))\n                                            if address15 is FAILURE:\n                                                address15 = TreeNode(self._input[index7:index7], index7, [])\n                                                self._offset = index7\n                                            if address15 is not FAILURE:\n                                                elements1.append(address15)\n                                                address16 = FAILURE\n                                                chunk12, max12 = None, self._offset + 1\n                                                if max12 <= self._input_size:\n                                                    chunk12 = self._input[self._offset:max12]\n                                                if chunk12 is not None and Grammar.REGEX_45.search(chunk12):\n                                                    address16 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                    self._offset = self._offset + 1\n                                                else:\n                                                    address16 = FAILURE\n                                                    if self._offset > self._failure:\n                                                        self._failure = self._offset\n                                                        self._expected = []\n                                                    if self._offset == self._failure:\n                                                        self._expected.append(('METAR::run', '[\\\\d]'))\n                                                if address16 is not FAILURE:\n                                                    elements1.append(address16)\n                                                    address17 = FAILURE\n                                                    chunk13, max13 = None, self._offset + 1\n                                                    if max13 <= self._input_size:\n                                                        chunk13 = self._input[self._offset:max13]\n                                                    if chunk13 is not None and Grammar.REGEX_46.search(chunk13):\n                                                        address17 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                        self._offset = self._offset + 1\n                                                    else:\n                                                        address17 = FAILURE\n                                                        if self._offset > self._failure:\n                                                            self._failure = self._offset\n                                                            self._expected = []\n                                                        if self._offset == self._failure:\n                                                            self._expected.append(('METAR::run', '[\\\\d]'))\n                                                    if address17 is not FAILURE:\n                                                        elements1.append(address17)\n                                                        address18 = FAILURE\n                                                        chunk14, max14 = None, self._offset + 1\n                                                        if max14 <= self._input_size:\n                                                            chunk14 = self._input[self._offset:max14]\n                                                        if chunk14 is not None and Grammar.REGEX_47.search(chunk14):\n                                                            address18 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                            self._offset = self._offset + 1\n                                                        else:\n                                                            address18 = FAILURE\n                                                            if self._offset > self._failure:\n                                                                self._failure = self._offset\n                                                                self._expected = []\n                                                            if self._offset == self._failure:\n                                                                self._expected.append(('METAR::run', '[\\\\d]'))\n                                                        if address18 is not FAILURE:\n                                                            elements1.append(address18)\n                                                            address19 = FAILURE\n                                                            chunk15, max15 = None, self._offset + 1\n                                                            if max15 <= self._input_size:\n                                                                chunk15 = self._input[self._offset:max15]\n                                                            if chunk15 is not None and Grammar.REGEX_48.search(chunk15):\n                                                                address19 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                                self._offset = self._offset + 1\n                                                            else:\n                                                                address19 = FAILURE\n                                                                if self._offset > self._failure:\n                                                                    self._failure = self._offset\n                                                                    self._expected = []\n                                                                if self._offset == self._failure:\n                                                                    self._expected.append(('METAR::run', '[\\\\d]'))\n                                                            if address19 is not FAILURE:\n                                                                elements1.append(address19)\n                                                                address20 = FAILURE\n                                                                index8 = self._offset\n                                                                chunk16, max16 = None, self._offset + 2\n                                                                if max16 <= self._input_size:\n                                                                    chunk16 = self._input[self._offset:max16]\n                                                                if chunk16 == 'FT':\n                                                                    address20 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                    self._offset = self._offset + 2\n                                                                else:\n                                                                    address20 = FAILURE\n                                                                    if self._offset > self._failure:\n                                                                        self._failure = self._offset\n                                                                        self._expected = []\n                                                                    if self._offset == self._failure:\n                                                                        self._expected.append(('METAR::run', '\"FT\"'))\n                                                                if address20 is FAILURE:\n                                                                    address20 = TreeNode(self._input[index8:index8], index8, [])\n                                                                    self._offset = index8\n                                                                if address20 is not FAILURE:\n                                                                    elements1.append(address20)\n                                                                    address21 = FAILURE\n                                                                    index9 = self._offset\n                                                                    index10, elements3 = self._offset, []\n                                                                    address22 = FAILURE\n                                                                    index11 = self._offset\n                                                                    chunk17, max17 = None, self._offset + 1\n                                                                    if max17 <= self._input_size:\n                                                                        chunk17 = self._input[self._offset:max17]\n                                                                    if chunk17 == '/':\n                                                                        address22 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                                        self._offset = self._offset + 1\n                                                                    else:\n                                                                        address22 = FAILURE\n                                                                        if self._offset > self._failure:\n                                                                            self._failure = self._offset\n                                                                            self._expected = []\n                                                                        if self._offset == self._failure:\n                                                                            self._expected.append(('METAR::run', '\"/\"'))\n                                                                    if address22 is FAILURE:\n                                                                        address22 = TreeNode(self._input[index11:index11], index11, [])\n                                                                        self._offset = index11\n                                                                    if address22 is not FAILURE:\n                                                                        elements3.append(address22)\n                                                                        address23 = FAILURE\n                                                                        chunk18, max18 = None, self._offset + 1\n                                                                        if max18 <= self._input_size:\n                                                                            chunk18 = self._input[self._offset:max18]\n                                                                        if chunk18 is not None and Grammar.REGEX_49.search(chunk18):\n                                                                            address23 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                                                            self._offset = self._offset + 1\n                                                                        else:\n                                                                            address23 = FAILURE\n                                                                            if self._offset > self._failure:\n                                                                                self._failure = self._offset\n                                                                                self._expected = []\n                                                                            if self._offset == self._failure:\n                                                                                self._expected.append(('METAR::run', '[UDN]'))\n                                                                        if address23 is not FAILURE:\n                                                                            elements3.append(address23)\n                                                                        else:\n                                                                            elements3 = None\n                                                                            self._offset = index10\n                                                                    else:\n                                                                        elements3 = None\n                                                                        self._offset = index10\n                                                                    if elements3 is None:\n                                                                        address21 = FAILURE\n                                                                    else:\n                                                                        address21 = TreeNode(self._input[index10:self._offset], index10, elements3)\n                                                                        self._offset = self._offset\n                                                                    if address21 is FAILURE:\n                                                                        address21 = TreeNode(self._input[index9:index9], index9, [])\n                                                                        self._offset = index9\n                                                                    if address21 is not FAILURE:\n                                                                        elements1.append(address21)\n                                                                    else:\n                                                                        elements1 = None\n                                                                        self._offset = index2\n                                                                else:\n                                                                    elements1 = None\n                                                                    self._offset = index2\n                                                            else:\n                                                                elements1 = None\n                                                                self._offset = index2\n                                                        else:\n                                                            elements1 = None\n                                                            self._offset = index2\n                                                    else:\n                                                        elements1 = None\n                                                        self._offset = index2\n                                                else:\n                                                    elements1 = None\n                                                    self._offset = index2\n                                            else:\n                                                elements1 = None\n                                                self._offset = index2\n                                        else:\n                                            elements1 = None\n                                            self._offset = index2\n                                    else:\n                                        elements1 = None\n                                        self._offset = index2\n                                else:\n                                    elements1 = None\n                                    self._offset = index2\n                            else:\n                                elements1 = None\n                                self._offset = index2\n                        else:\n                            elements1 = None\n                            self._offset = index2\n                    else:\n                        elements1 = None\n                        self._offset = index2\n                else:\n                    elements1 = None\n                    self._offset = index2\n            else:\n                elements1 = None\n                self._offset = index2\n            if elements1 is None:\n                address1 = FAILURE\n            else:\n                address1 = TreeNode8(self._input[index2:self._offset], index2, elements1)\n                self._offset = self._offset\n            if address1 is not FAILURE:\n                elements0.append(address1)\n            else:\n                break\n        if len(elements0) >= 0:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        else:\n            address0 = FAILURE\n        self._cache['run'][index0] = (address0, self._offset)\n        return address0",
  "def _read_curwx(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['curwx'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2 = self._offset\n        index3, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 2\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 == '//':\n                address2 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                self._offset = self._offset + 2\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::curwx', '\"//\"'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n            else:\n                elements0 = None\n                self._offset = index3\n        else:\n            elements0 = None\n            self._offset = index3\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode9(self._input[index3:self._offset], index3, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            self._offset = index2\n            index4, elements1 = self._offset, []\n            address3 = FAILURE\n            address3 = self._read_sep()\n            if address3 is not FAILURE:\n                elements1.append(address3)\n                address4 = FAILURE\n                chunk1, max1 = None, self._offset + 3\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 == 'NSW':\n                    address4 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                    self._offset = self._offset + 3\n                else:\n                    address4 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::curwx', '\"NSW\"'))\n                if address4 is not FAILURE:\n                    elements1.append(address4)\n                else:\n                    elements1 = None\n                    self._offset = index4\n            else:\n                elements1 = None\n                self._offset = index4\n            if elements1 is None:\n                address0 = FAILURE\n            else:\n                address0 = TreeNode10(self._input[index4:self._offset], index4, elements1)\n                self._offset = self._offset\n            if address0 is FAILURE:\n                self._offset = index2\n                index5, elements2, address5 = self._offset, [], None\n                while True:\n                    index6, elements3 = self._offset, []\n                    address6 = FAILURE\n                    address6 = self._read_sep()\n                    if address6 is not FAILURE:\n                        elements3.append(address6)\n                        address7 = FAILURE\n                        address7 = self._read_wx()\n                        if address7 is not FAILURE:\n                            elements3.append(address7)\n                        else:\n                            elements3 = None\n                            self._offset = index6\n                    else:\n                        elements3 = None\n                        self._offset = index6\n                    if elements3 is None:\n                        address5 = FAILURE\n                    else:\n                        address5 = TreeNode11(self._input[index6:self._offset], index6, elements3)\n                        self._offset = self._offset\n                    if address5 is not FAILURE:\n                        elements2.append(address5)\n                    else:\n                        break\n                if len(elements2) >= 0:\n                    address0 = TreeNode(self._input[index5:self._offset], index5, elements2)\n                    self._offset = self._offset\n                else:\n                    address0 = FAILURE\n                if address0 is FAILURE:\n                    self._offset = index2\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['curwx'][index0] = (address0, self._offset)\n        return address0",
  "def _read_wx(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['wx'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        index2 = self._offset\n        index3, elements1 = self._offset, []\n        address2 = FAILURE\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 is not None and Grammar.REGEX_50.search(chunk0):\n            address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address2 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::wx', '[-+]'))\n        if address2 is not FAILURE:\n            elements1.append(address2)\n            address3 = FAILURE\n            index4 = self._offset\n            chunk1, max1 = None, self._offset + 1\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 == ' ':\n                address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address3 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wx', '\" \"'))\n            if address3 is FAILURE:\n                address3 = TreeNode(self._input[index4:index4], index4, [])\n                self._offset = index4\n            if address3 is not FAILURE:\n                elements1.append(address3)\n            else:\n                elements1 = None\n                self._offset = index3\n        else:\n            elements1 = None\n            self._offset = index3\n        if elements1 is None:\n            address1 = FAILURE\n        else:\n            address1 = TreeNode(self._input[index3:self._offset], index3, elements1)\n            self._offset = self._offset\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index2:index2], index2, [])\n            self._offset = index2\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address4 = FAILURE\n            index5 = self._offset\n            chunk2, max2 = None, self._offset + 2\n            if max2 <= self._input_size:\n                chunk2 = self._input[self._offset:max2]\n            if chunk2 == 'VC':\n                address4 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                self._offset = self._offset + 2\n            else:\n                address4 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::wx', '\"VC\"'))\n            if address4 is FAILURE:\n                address4 = TreeNode(self._input[index5:index5], index5, [])\n                self._offset = index5\n            if address4 is not FAILURE:\n                elements0.append(address4)\n                address5 = FAILURE\n                index6, elements2, address6 = self._offset, [], None\n                while True:\n                    index7 = self._offset\n                    chunk3, max3 = None, self._offset + 2\n                    if max3 <= self._input_size:\n                        chunk3 = self._input[self._offset:max3]\n                    if chunk3 == 'MI':\n                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                        self._offset = self._offset + 2\n                    else:\n                        address6 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::wx', '\"MI\"'))\n                    if address6 is FAILURE:\n                        self._offset = index7\n                        chunk4, max4 = None, self._offset + 2\n                        if max4 <= self._input_size:\n                            chunk4 = self._input[self._offset:max4]\n                        if chunk4 == 'BC':\n                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                            self._offset = self._offset + 2\n                        else:\n                            address6 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::wx', '\"BC\"'))\n                        if address6 is FAILURE:\n                            self._offset = index7\n                            chunk5, max5 = None, self._offset + 2\n                            if max5 <= self._input_size:\n                                chunk5 = self._input[self._offset:max5]\n                            if chunk5 == 'PR':\n                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                self._offset = self._offset + 2\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::wx', '\"PR\"'))\n                            if address6 is FAILURE:\n                                self._offset = index7\n                                chunk6, max6 = None, self._offset + 2\n                                if max6 <= self._input_size:\n                                    chunk6 = self._input[self._offset:max6]\n                                if chunk6 == 'DR':\n                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                    self._offset = self._offset + 2\n                                else:\n                                    address6 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::wx', '\"DR\"'))\n                                if address6 is FAILURE:\n                                    self._offset = index7\n                                    chunk7, max7 = None, self._offset + 2\n                                    if max7 <= self._input_size:\n                                        chunk7 = self._input[self._offset:max7]\n                                    if chunk7 == 'BL':\n                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                        self._offset = self._offset + 2\n                                    else:\n                                        address6 = FAILURE\n                                        if self._offset > self._failure:\n                                            self._failure = self._offset\n                                            self._expected = []\n                                        if self._offset == self._failure:\n                                            self._expected.append(('METAR::wx', '\"BL\"'))\n                                    if address6 is FAILURE:\n                                        self._offset = index7\n                                        chunk8, max8 = None, self._offset + 2\n                                        if max8 <= self._input_size:\n                                            chunk8 = self._input[self._offset:max8]\n                                        if chunk8 == 'SH':\n                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                            self._offset = self._offset + 2\n                                        else:\n                                            address6 = FAILURE\n                                            if self._offset > self._failure:\n                                                self._failure = self._offset\n                                                self._expected = []\n                                            if self._offset == self._failure:\n                                                self._expected.append(('METAR::wx', '\"SH\"'))\n                                        if address6 is FAILURE:\n                                            self._offset = index7\n                                            chunk9, max9 = None, self._offset + 2\n                                            if max9 <= self._input_size:\n                                                chunk9 = self._input[self._offset:max9]\n                                            if chunk9 == 'TS':\n                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                self._offset = self._offset + 2\n                                            else:\n                                                address6 = FAILURE\n                                                if self._offset > self._failure:\n                                                    self._failure = self._offset\n                                                    self._expected = []\n                                                if self._offset == self._failure:\n                                                    self._expected.append(('METAR::wx', '\"TS\"'))\n                                            if address6 is FAILURE:\n                                                self._offset = index7\n                                                chunk10, max10 = None, self._offset + 2\n                                                if max10 <= self._input_size:\n                                                    chunk10 = self._input[self._offset:max10]\n                                                if chunk10 == 'FZ':\n                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                    self._offset = self._offset + 2\n                                                else:\n                                                    address6 = FAILURE\n                                                    if self._offset > self._failure:\n                                                        self._failure = self._offset\n                                                        self._expected = []\n                                                    if self._offset == self._failure:\n                                                        self._expected.append(('METAR::wx', '\"FZ\"'))\n                                                if address6 is FAILURE:\n                                                    self._offset = index7\n                                                    chunk11, max11 = None, self._offset + 2\n                                                    if max11 <= self._input_size:\n                                                        chunk11 = self._input[self._offset:max11]\n                                                    if chunk11 == 'DZ':\n                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                        self._offset = self._offset + 2\n                                                    else:\n                                                        address6 = FAILURE\n                                                        if self._offset > self._failure:\n                                                            self._failure = self._offset\n                                                            self._expected = []\n                                                        if self._offset == self._failure:\n                                                            self._expected.append(('METAR::wx', '\"DZ\"'))\n                                                    if address6 is FAILURE:\n                                                        self._offset = index7\n                                                        chunk12, max12 = None, self._offset + 2\n                                                        if max12 <= self._input_size:\n                                                            chunk12 = self._input[self._offset:max12]\n                                                        if chunk12 == 'RA':\n                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                            self._offset = self._offset + 2\n                                                        else:\n                                                            address6 = FAILURE\n                                                            if self._offset > self._failure:\n                                                                self._failure = self._offset\n                                                                self._expected = []\n                                                            if self._offset == self._failure:\n                                                                self._expected.append(('METAR::wx', '\"RA\"'))\n                                                        if address6 is FAILURE:\n                                                            self._offset = index7\n                                                            chunk13, max13 = None, self._offset + 2\n                                                            if max13 <= self._input_size:\n                                                                chunk13 = self._input[self._offset:max13]\n                                                            if chunk13 == 'SN':\n                                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                self._offset = self._offset + 2\n                                                            else:\n                                                                address6 = FAILURE\n                                                                if self._offset > self._failure:\n                                                                    self._failure = self._offset\n                                                                    self._expected = []\n                                                                if self._offset == self._failure:\n                                                                    self._expected.append(('METAR::wx', '\"SN\"'))\n                                                            if address6 is FAILURE:\n                                                                self._offset = index7\n                                                                chunk14, max14 = None, self._offset + 2\n                                                                if max14 <= self._input_size:\n                                                                    chunk14 = self._input[self._offset:max14]\n                                                                if chunk14 == 'SG':\n                                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                    self._offset = self._offset + 2\n                                                                else:\n                                                                    address6 = FAILURE\n                                                                    if self._offset > self._failure:\n                                                                        self._failure = self._offset\n                                                                        self._expected = []\n                                                                    if self._offset == self._failure:\n                                                                        self._expected.append(('METAR::wx', '\"SG\"'))\n                                                                if address6 is FAILURE:\n                                                                    self._offset = index7\n                                                                    chunk15, max15 = None, self._offset + 2\n                                                                    if max15 <= self._input_size:\n                                                                        chunk15 = self._input[self._offset:max15]\n                                                                    if chunk15 == 'PL':\n                                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                        self._offset = self._offset + 2\n                                                                    else:\n                                                                        address6 = FAILURE\n                                                                        if self._offset > self._failure:\n                                                                            self._failure = self._offset\n                                                                            self._expected = []\n                                                                        if self._offset == self._failure:\n                                                                            self._expected.append(('METAR::wx', '\"PL\"'))\n                                                                    if address6 is FAILURE:\n                                                                        self._offset = index7\n                                                                        chunk16, max16 = None, self._offset + 2\n                                                                        if max16 <= self._input_size:\n                                                                            chunk16 = self._input[self._offset:max16]\n                                                                        if chunk16 == 'GR':\n                                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                            self._offset = self._offset + 2\n                                                                        else:\n                                                                            address6 = FAILURE\n                                                                            if self._offset > self._failure:\n                                                                                self._failure = self._offset\n                                                                                self._expected = []\n                                                                            if self._offset == self._failure:\n                                                                                self._expected.append(('METAR::wx', '\"GR\"'))\n                                                                        if address6 is FAILURE:\n                                                                            self._offset = index7\n                                                                            chunk17, max17 = None, self._offset + 2\n                                                                            if max17 <= self._input_size:\n                                                                                chunk17 = self._input[self._offset:max17]\n                                                                            if chunk17 == 'GS':\n                                                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                self._offset = self._offset + 2\n                                                                            else:\n                                                                                address6 = FAILURE\n                                                                                if self._offset > self._failure:\n                                                                                    self._failure = self._offset\n                                                                                    self._expected = []\n                                                                                if self._offset == self._failure:\n                                                                                    self._expected.append(('METAR::wx', '\"GS\"'))\n                                                                            if address6 is FAILURE:\n                                                                                self._offset = index7\n                                                                                chunk18, max18 = None, self._offset + 2\n                                                                                if max18 <= self._input_size:\n                                                                                    chunk18 = self._input[self._offset:max18]\n                                                                                if chunk18 == 'UP':\n                                                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                    self._offset = self._offset + 2\n                                                                                else:\n                                                                                    address6 = FAILURE\n                                                                                    if self._offset > self._failure:\n                                                                                        self._failure = self._offset\n                                                                                        self._expected = []\n                                                                                    if self._offset == self._failure:\n                                                                                        self._expected.append(('METAR::wx', '\"UP\"'))\n                                                                                if address6 is FAILURE:\n                                                                                    self._offset = index7\n                                                                                    chunk19, max19 = None, self._offset + 2\n                                                                                    if max19 <= self._input_size:\n                                                                                        chunk19 = self._input[self._offset:max19]\n                                                                                    if chunk19 == 'BR':\n                                                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                        self._offset = self._offset + 2\n                                                                                    else:\n                                                                                        address6 = FAILURE\n                                                                                        if self._offset > self._failure:\n                                                                                            self._failure = self._offset\n                                                                                            self._expected = []\n                                                                                        if self._offset == self._failure:\n                                                                                            self._expected.append(('METAR::wx', '\"BR\"'))\n                                                                                    if address6 is FAILURE:\n                                                                                        self._offset = index7\n                                                                                        chunk20, max20 = None, self._offset + 2\n                                                                                        if max20 <= self._input_size:\n                                                                                            chunk20 = self._input[self._offset:max20]\n                                                                                        if chunk20 == 'FG':\n                                                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                            self._offset = self._offset + 2\n                                                                                        else:\n                                                                                            address6 = FAILURE\n                                                                                            if self._offset > self._failure:\n                                                                                                self._failure = self._offset\n                                                                                                self._expected = []\n                                                                                            if self._offset == self._failure:\n                                                                                                self._expected.append(('METAR::wx', '\"FG\"'))\n                                                                                        if address6 is FAILURE:\n                                                                                            self._offset = index7\n                                                                                            chunk21, max21 = None, self._offset + 2\n                                                                                            if max21 <= self._input_size:\n                                                                                                chunk21 = self._input[self._offset:max21]\n                                                                                            if chunk21 == 'FU':\n                                                                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                self._offset = self._offset + 2\n                                                                                            else:\n                                                                                                address6 = FAILURE\n                                                                                                if self._offset > self._failure:\n                                                                                                    self._failure = self._offset\n                                                                                                    self._expected = []\n                                                                                                if self._offset == self._failure:\n                                                                                                    self._expected.append(('METAR::wx', '\"FU\"'))\n                                                                                            if address6 is FAILURE:\n                                                                                                self._offset = index7\n                                                                                                chunk22, max22 = None, self._offset + 2\n                                                                                                if max22 <= self._input_size:\n                                                                                                    chunk22 = self._input[self._offset:max22]\n                                                                                                if chunk22 == 'VA':\n                                                                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                    self._offset = self._offset + 2\n                                                                                                else:\n                                                                                                    address6 = FAILURE\n                                                                                                    if self._offset > self._failure:\n                                                                                                        self._failure = self._offset\n                                                                                                        self._expected = []\n                                                                                                    if self._offset == self._failure:\n                                                                                                        self._expected.append(('METAR::wx', '\"VA\"'))\n                                                                                                if address6 is FAILURE:\n                                                                                                    self._offset = index7\n                                                                                                    chunk23, max23 = None, self._offset + 2\n                                                                                                    if max23 <= self._input_size:\n                                                                                                        chunk23 = self._input[self._offset:max23]\n                                                                                                    if chunk23 == 'DU':\n                                                                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                        self._offset = self._offset + 2\n                                                                                                    else:\n                                                                                                        address6 = FAILURE\n                                                                                                        if self._offset > self._failure:\n                                                                                                            self._failure = self._offset\n                                                                                                            self._expected = []\n                                                                                                        if self._offset == self._failure:\n                                                                                                            self._expected.append(('METAR::wx', '\"DU\"'))\n                                                                                                    if address6 is FAILURE:\n                                                                                                        self._offset = index7\n                                                                                                        chunk24, max24 = None, self._offset + 2\n                                                                                                        if max24 <= self._input_size:\n                                                                                                            chunk24 = self._input[self._offset:max24]\n                                                                                                        if chunk24 == 'SA':\n                                                                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                            self._offset = self._offset + 2\n                                                                                                        else:\n                                                                                                            address6 = FAILURE\n                                                                                                            if self._offset > self._failure:\n                                                                                                                self._failure = self._offset\n                                                                                                                self._expected = []\n                                                                                                            if self._offset == self._failure:\n                                                                                                                self._expected.append(('METAR::wx', '\"SA\"'))\n                                                                                                        if address6 is FAILURE:\n                                                                                                            self._offset = index7\n                                                                                                            chunk25, max25 = None, self._offset + 2\n                                                                                                            if max25 <= self._input_size:\n                                                                                                                chunk25 = self._input[self._offset:max25]\n                                                                                                            if chunk25 == 'HZ':\n                                                                                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                self._offset = self._offset + 2\n                                                                                                            else:\n                                                                                                                address6 = FAILURE\n                                                                                                                if self._offset > self._failure:\n                                                                                                                    self._failure = self._offset\n                                                                                                                    self._expected = []\n                                                                                                                if self._offset == self._failure:\n                                                                                                                    self._expected.append(('METAR::wx', '\"HZ\"'))\n                                                                                                            if address6 is FAILURE:\n                                                                                                                self._offset = index7\n                                                                                                                chunk26, max26 = None, self._offset + 2\n                                                                                                                if max26 <= self._input_size:\n                                                                                                                    chunk26 = self._input[self._offset:max26]\n                                                                                                                if chunk26 == 'PO':\n                                                                                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                    self._offset = self._offset + 2\n                                                                                                                else:\n                                                                                                                    address6 = FAILURE\n                                                                                                                    if self._offset > self._failure:\n                                                                                                                        self._failure = self._offset\n                                                                                                                        self._expected = []\n                                                                                                                    if self._offset == self._failure:\n                                                                                                                        self._expected.append(('METAR::wx', '\"PO\"'))\n                                                                                                                if address6 is FAILURE:\n                                                                                                                    self._offset = index7\n                                                                                                                    chunk27, max27 = None, self._offset + 2\n                                                                                                                    if max27 <= self._input_size:\n                                                                                                                        chunk27 = self._input[self._offset:max27]\n                                                                                                                    if chunk27 == 'SQ':\n                                                                                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                        self._offset = self._offset + 2\n                                                                                                                    else:\n                                                                                                                        address6 = FAILURE\n                                                                                                                        if self._offset > self._failure:\n                                                                                                                            self._failure = self._offset\n                                                                                                                            self._expected = []\n                                                                                                                        if self._offset == self._failure:\n                                                                                                                            self._expected.append(('METAR::wx', '\"SQ\"'))\n                                                                                                                    if address6 is FAILURE:\n                                                                                                                        self._offset = index7\n                                                                                                                        chunk28, max28 = None, self._offset + 2\n                                                                                                                        if max28 <= self._input_size:\n                                                                                                                            chunk28 = self._input[self._offset:max28]\n                                                                                                                        if chunk28 == 'FC':\n                                                                                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                            self._offset = self._offset + 2\n                                                                                                                        else:\n                                                                                                                            address6 = FAILURE\n                                                                                                                            if self._offset > self._failure:\n                                                                                                                                self._failure = self._offset\n                                                                                                                                self._expected = []\n                                                                                                                            if self._offset == self._failure:\n                                                                                                                                self._expected.append(('METAR::wx', '\"FC\"'))\n                                                                                                                        if address6 is FAILURE:\n                                                                                                                            self._offset = index7\n                                                                                                                            chunk29, max29 = None, self._offset + 2\n                                                                                                                            if max29 <= self._input_size:\n                                                                                                                                chunk29 = self._input[self._offset:max29]\n                                                                                                                            if chunk29 == 'SS':\n                                                                                                                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                                self._offset = self._offset + 2\n                                                                                                                            else:\n                                                                                                                                address6 = FAILURE\n                                                                                                                                if self._offset > self._failure:\n                                                                                                                                    self._failure = self._offset\n                                                                                                                                    self._expected = []\n                                                                                                                                if self._offset == self._failure:\n                                                                                                                                    self._expected.append(('METAR::wx', '\"SS\"'))\n                                                                                                                            if address6 is FAILURE:\n                                                                                                                                self._offset = index7\n                                                                                                                                chunk30, max30 = None, self._offset + 2\n                                                                                                                                if max30 <= self._input_size:\n                                                                                                                                    chunk30 = self._input[self._offset:max30]\n                                                                                                                                if chunk30 == 'DS':\n                                                                                                                                    address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                                    self._offset = self._offset + 2\n                                                                                                                                else:\n                                                                                                                                    address6 = FAILURE\n                                                                                                                                    if self._offset > self._failure:\n                                                                                                                                        self._failure = self._offset\n                                                                                                                                        self._expected = []\n                                                                                                                                    if self._offset == self._failure:\n                                                                                                                                        self._expected.append(('METAR::wx', '\"DS\"'))\n                                                                                                                                if address6 is FAILURE:\n                                                                                                                                    self._offset = index7\n                                                                                                                                    chunk31, max31 = None, self._offset + 2\n                                                                                                                                    if max31 <= self._input_size:\n                                                                                                                                        chunk31 = self._input[self._offset:max31]\n                                                                                                                                    if chunk31 == 'IC':\n                                                                                                                                        address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                                        self._offset = self._offset + 2\n                                                                                                                                    else:\n                                                                                                                                        address6 = FAILURE\n                                                                                                                                        if self._offset > self._failure:\n                                                                                                                                            self._failure = self._offset\n                                                                                                                                            self._expected = []\n                                                                                                                                        if self._offset == self._failure:\n                                                                                                                                            self._expected.append(('METAR::wx', '\"IC\"'))\n                                                                                                                                    if address6 is FAILURE:\n                                                                                                                                        self._offset = index7\n                                                                                                                                        chunk32, max32 = None, self._offset + 2\n                                                                                                                                        if max32 <= self._input_size:\n                                                                                                                                            chunk32 = self._input[self._offset:max32]\n                                                                                                                                        if chunk32 == 'PY':\n                                                                                                                                            address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                                                                                                                            self._offset = self._offset + 2\n                                                                                                                                        else:\n                                                                                                                                            address6 = FAILURE\n                                                                                                                                            if self._offset > self._failure:\n                                                                                                                                                self._failure = self._offset\n                                                                                                                                                self._expected = []\n                                                                                                                                            if self._offset == self._failure:\n                                                                                                                                                self._expected.append(('METAR::wx', '\"PY\"'))\n                                                                                                                                        if address6 is FAILURE:\n                                                                                                                                            self._offset = index7\n                    if address6 is not FAILURE:\n                        elements2.append(address6)\n                    else:\n                        break\n                if len(elements2) >= 1:\n                    address5 = TreeNode(self._input[index6:self._offset], index6, elements2)\n                    self._offset = self._offset\n                else:\n                    address5 = FAILURE\n                if address5 is not FAILURE:\n                    elements0.append(address5)\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['wx'][index0] = (address0, self._offset)\n        return address0",
  "def _read_skyc(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['skyc'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0, address1 = self._offset, [], None\n        while True:\n            index3, elements1 = self._offset, []\n            address2 = FAILURE\n            address2 = self._read_sep()\n            if address2 is not FAILURE:\n                elements1.append(address2)\n                address3 = FAILURE\n                address3 = self._read_cover()\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                else:\n                    elements1 = None\n                    self._offset = index3\n            else:\n                elements1 = None\n                self._offset = index3\n            if elements1 is None:\n                address1 = FAILURE\n            else:\n                address1 = TreeNode12(self._input[index3:self._offset], index3, elements1)\n                self._offset = self._offset\n            if address1 is not FAILURE:\n                elements0.append(address1)\n            else:\n                break\n        if len(elements0) >= 0:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        else:\n            address0 = FAILURE\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['skyc'][index0] = (address0, self._offset)\n        return address0",
  "def _read_cover(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['cover'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        index3 = self._offset\n        chunk0, max0 = None, self._offset + 3\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 == 'FEW':\n            address1 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n            self._offset = self._offset + 3\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::cover', '\"FEW\"'))\n        if address1 is FAILURE:\n            self._offset = index3\n            chunk1, max1 = None, self._offset + 3\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 == 'SCT':\n                address1 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                self._offset = self._offset + 3\n            else:\n                address1 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::cover', '\"SCT\"'))\n            if address1 is FAILURE:\n                self._offset = index3\n                chunk2, max2 = None, self._offset + 3\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 == 'BKN':\n                    address1 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                    self._offset = self._offset + 3\n                else:\n                    address1 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::cover', '\"BKN\"'))\n                if address1 is FAILURE:\n                    self._offset = index3\n                    chunk3, max3 = None, self._offset + 3\n                    if max3 <= self._input_size:\n                        chunk3 = self._input[self._offset:max3]\n                    if chunk3 == 'OVC':\n                        address1 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                        self._offset = self._offset + 3\n                    else:\n                        address1 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::cover', '\"OVC\"'))\n                    if address1 is FAILURE:\n                        self._offset = index3\n                        chunk4, max4 = None, self._offset + 2\n                        if max4 <= self._input_size:\n                            chunk4 = self._input[self._offset:max4]\n                        if chunk4 == 'VV':\n                            address1 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                            self._offset = self._offset + 2\n                        else:\n                            address1 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::cover', '\"VV\"'))\n                        if address1 is FAILURE:\n                            self._offset = index3\n                            chunk5, max5 = None, self._offset + 3\n                            if max5 <= self._input_size:\n                                chunk5 = self._input[self._offset:max5]\n                            if chunk5 == '///':\n                                address1 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                                self._offset = self._offset + 3\n                            else:\n                                address1 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::cover', '\"///\"'))\n                            if address1 is FAILURE:\n                                self._offset = index3\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index4 = self._offset\n            index5, elements1, address3 = self._offset, [], None\n            while True:\n                chunk6, max6 = None, self._offset + 1\n                if max6 <= self._input_size:\n                    chunk6 = self._input[self._offset:max6]\n                if chunk6 is not None and Grammar.REGEX_51.search(chunk6):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::cover', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements1.append(address3)\n                else:\n                    break\n            if len(elements1) >= 0:\n                address2 = TreeNode(self._input[index5:self._offset], index5, elements1)\n                self._offset = self._offset\n            else:\n                address2 = FAILURE\n            if address2 is FAILURE:\n                address2 = TreeNode(self._input[index4:index4], index4, [])\n                self._offset = index4\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address4 = FAILURE\n                index6 = self._offset\n                index7 = self._offset\n                chunk7, max7 = None, self._offset + 3\n                if max7 <= self._input_size:\n                    chunk7 = self._input[self._offset:max7]\n                if chunk7 == 'TCU':\n                    address4 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                    self._offset = self._offset + 3\n                else:\n                    address4 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::cover', '\"TCU\"'))\n                if address4 is FAILURE:\n                    self._offset = index7\n                    chunk8, max8 = None, self._offset + 2\n                    if max8 <= self._input_size:\n                        chunk8 = self._input[self._offset:max8]\n                    if chunk8 == 'CB':\n                        address4 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                        self._offset = self._offset + 2\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::cover', '\"CB\"'))\n                    if address4 is FAILURE:\n                        self._offset = index7\n                        index8, elements2 = self._offset, []\n                        address5 = FAILURE\n                        chunk9, max9 = None, self._offset + 2\n                        if max9 <= self._input_size:\n                            chunk9 = self._input[self._offset:max9]\n                        if chunk9 == '//':\n                            address5 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                            self._offset = self._offset + 2\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::cover', '\"//\"'))\n                        if address5 is not FAILURE:\n                            elements2.append(address5)\n                            address6 = FAILURE\n                            index9 = self._offset\n                            chunk10, max10 = None, self._offset + 1\n                            if max10 <= self._input_size:\n                                chunk10 = self._input[self._offset:max10]\n                            if chunk10 == '/':\n                                address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::cover', '\"/\"'))\n                            if address6 is FAILURE:\n                                address6 = TreeNode(self._input[index9:index9], index9, [])\n                                self._offset = index9\n                            if address6 is not FAILURE:\n                                elements2.append(address6)\n                            else:\n                                elements2 = None\n                                self._offset = index8\n                        else:\n                            elements2 = None\n                            self._offset = index8\n                        if elements2 is None:\n                            address4 = FAILURE\n                        else:\n                            address4 = TreeNode(self._input[index8:self._offset], index8, elements2)\n                            self._offset = self._offset\n                        if address4 is FAILURE:\n                            self._offset = index7\n                if address4 is FAILURE:\n                    address4 = TreeNode(self._input[index6:index6], index6, [])\n                    self._offset = index6\n                if address4 is not FAILURE:\n                    elements0.append(address4)\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            self._offset = index1\n            index10 = self._offset\n            chunk11, max11 = None, self._offset + 3\n            if max11 <= self._input_size:\n                chunk11 = self._input[self._offset:max11]\n            if chunk11 == 'CLR':\n                address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                self._offset = self._offset + 3\n            else:\n                address0 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::cover', '\"CLR\"'))\n            if address0 is FAILURE:\n                self._offset = index10\n                chunk12, max12 = None, self._offset + 3\n                if max12 <= self._input_size:\n                    chunk12 = self._input[self._offset:max12]\n                if chunk12 == 'SKC':\n                    address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                    self._offset = self._offset + 3\n                else:\n                    address0 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::cover', '\"SKC\"'))\n                if address0 is FAILURE:\n                    self._offset = index10\n                    chunk13, max13 = None, self._offset + 3\n                    if max13 <= self._input_size:\n                        chunk13 = self._input[self._offset:max13]\n                    if chunk13 == 'NSC':\n                        address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                        self._offset = self._offset + 3\n                    else:\n                        address0 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::cover', '\"NSC\"'))\n                    if address0 is FAILURE:\n                        self._offset = index10\n                        chunk14, max14 = None, self._offset + 3\n                        if max14 <= self._input_size:\n                            chunk14 = self._input[self._offset:max14]\n                        if chunk14 == 'NCD':\n                            address0 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                            self._offset = self._offset + 3\n                        else:\n                            address0 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::cover', '\"NCD\"'))\n                        if address0 is FAILURE:\n                            self._offset = index10\n            if address0 is FAILURE:\n                self._offset = index1\n                address0 = self._read_wx()\n                if address0 is FAILURE:\n                    self._offset = index1\n                    chunk15, max15 = None, self._offset + 2\n                    if max15 <= self._input_size:\n                        chunk15 = self._input[self._offset:max15]\n                    if chunk15 == '//':\n                        address0 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                        self._offset = self._offset + 2\n                    else:\n                        address0 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::cover', '\"//\"'))\n                    if address0 is FAILURE:\n                        self._offset = index1\n        self._cache['cover'][index0] = (address0, self._offset)\n        return address0",
  "def _read_temp_dewp(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['temp_dewp'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        address1 = self._read_sep()\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3 = self._offset\n            chunk0, max0 = None, self._offset + 2\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 == '//':\n                address2 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                self._offset = self._offset + 2\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::temp_dewp', '\"//\"'))\n            if address2 is FAILURE:\n                address2 = TreeNode(self._input[index3:index3], index3, [])\n                self._offset = index3\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                address3 = self._read_temp()\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    chunk1, max1 = None, self._offset + 1\n                    if max1 <= self._input_size:\n                        chunk1 = self._input[self._offset:max1]\n                    if chunk1 == '/':\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::temp_dewp', '\"/\"'))\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        address5 = self._read_dewp()\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            index4 = self._offset\n                            chunk2, max2 = None, self._offset + 2\n                            if max2 <= self._input_size:\n                                chunk2 = self._input[self._offset:max2]\n                            if chunk2 == '//':\n                                address6 = TreeNode(self._input[self._offset:self._offset + 2], self._offset, [])\n                                self._offset = self._offset + 2\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::temp_dewp', '\"//\"'))\n                            if address6 is FAILURE:\n                                address6 = TreeNode(self._input[index4:index4], index4, [])\n                                self._offset = index4\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                            else:\n                                elements0 = None\n                                self._offset = index2\n                        else:\n                            elements0 = None\n                            self._offset = index2\n                    else:\n                        elements0 = None\n                        self._offset = index2\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode13(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['temp_dewp'][index0] = (address0, self._offset)\n        return address0",
  "def _read_temp(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['temp'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        index2 = self._offset\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 is not None and Grammar.REGEX_52.search(chunk0):\n            address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::temp', '[M]'))\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index2:index2], index2, [])\n            self._offset = index2\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3 = self._offset\n            chunk1, max1 = None, self._offset + 1\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 is not None and Grammar.REGEX_53.search(chunk1):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::temp', '[\\\\d]'))\n            if address2 is FAILURE:\n                address2 = TreeNode(self._input[index3:index3], index3, [])\n                self._offset = index3\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                index4 = self._offset\n                chunk2, max2 = None, self._offset + 1\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 is not None and Grammar.REGEX_54.search(chunk2):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::temp', '[\\\\d]'))\n                if address3 is FAILURE:\n                    address3 = TreeNode(self._input[index4:index4], index4, [])\n                    self._offset = index4\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['temp'][index0] = (address0, self._offset)\n        return address0",
  "def _read_dewp(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['dewp'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1, elements0 = self._offset, []\n        address1 = FAILURE\n        index2 = self._offset\n        chunk0, max0 = None, self._offset + 1\n        if max0 <= self._input_size:\n            chunk0 = self._input[self._offset:max0]\n        if chunk0 is not None and Grammar.REGEX_55.search(chunk0):\n            address1 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n            self._offset = self._offset + 1\n        else:\n            address1 = FAILURE\n            if self._offset > self._failure:\n                self._failure = self._offset\n                self._expected = []\n            if self._offset == self._failure:\n                self._expected.append(('METAR::dewp', '[M]'))\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index2:index2], index2, [])\n            self._offset = index2\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index3 = self._offset\n            chunk1, max1 = None, self._offset + 1\n            if max1 <= self._input_size:\n                chunk1 = self._input[self._offset:max1]\n            if chunk1 is not None and Grammar.REGEX_56.search(chunk1):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::dewp', '[\\\\d]'))\n            if address2 is FAILURE:\n                address2 = TreeNode(self._input[index3:index3], index3, [])\n                self._offset = index3\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                index4 = self._offset\n                chunk2, max2 = None, self._offset + 1\n                if max2 <= self._input_size:\n                    chunk2 = self._input[self._offset:max2]\n                if chunk2 is not None and Grammar.REGEX_57.search(chunk2):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::dewp', '[\\\\d]'))\n                if address3 is FAILURE:\n                    address3 = TreeNode(self._input[index4:index4], index4, [])\n                    self._offset = index4\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                else:\n                    elements0 = None\n                    self._offset = index1\n            else:\n                elements0 = None\n                self._offset = index1\n        else:\n            elements0 = None\n            self._offset = index1\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index1:self._offset], index1, elements0)\n            self._offset = self._offset\n        self._cache['dewp'][index0] = (address0, self._offset)\n        return address0",
  "def _read_altim(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['altim'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        index3 = self._offset\n        address1 = self._read_sep()\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index3:index3], index3, [])\n            self._offset = index3\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 is not None and Grammar.REGEX_58.search(chunk0):\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::altim', '[\"Q\" / \"A\"]'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address3 = FAILURE\n                chunk1, max1 = None, self._offset + 1\n                if max1 <= self._input_size:\n                    chunk1 = self._input[self._offset:max1]\n                if chunk1 is not None and Grammar.REGEX_59.search(chunk1):\n                    address3 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                    self._offset = self._offset + 1\n                else:\n                    address3 = FAILURE\n                    if self._offset > self._failure:\n                        self._failure = self._offset\n                        self._expected = []\n                    if self._offset == self._failure:\n                        self._expected.append(('METAR::altim', '[\\\\d]'))\n                if address3 is not FAILURE:\n                    elements0.append(address3)\n                    address4 = FAILURE\n                    chunk2, max2 = None, self._offset + 1\n                    if max2 <= self._input_size:\n                        chunk2 = self._input[self._offset:max2]\n                    if chunk2 is not None and Grammar.REGEX_60.search(chunk2):\n                        address4 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address4 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::altim', '[\\\\d]'))\n                    if address4 is not FAILURE:\n                        elements0.append(address4)\n                        address5 = FAILURE\n                        chunk3, max3 = None, self._offset + 1\n                        if max3 <= self._input_size:\n                            chunk3 = self._input[self._offset:max3]\n                        if chunk3 is not None and Grammar.REGEX_61.search(chunk3):\n                            address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                            self._offset = self._offset + 1\n                        else:\n                            address5 = FAILURE\n                            if self._offset > self._failure:\n                                self._failure = self._offset\n                                self._expected = []\n                            if self._offset == self._failure:\n                                self._expected.append(('METAR::altim', '[\\\\d]'))\n                        if address5 is not FAILURE:\n                            elements0.append(address5)\n                            address6 = FAILURE\n                            chunk4, max4 = None, self._offset + 1\n                            if max4 <= self._input_size:\n                                chunk4 = self._input[self._offset:max4]\n                            if chunk4 is not None and Grammar.REGEX_62.search(chunk4):\n                                address6 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                self._offset = self._offset + 1\n                            else:\n                                address6 = FAILURE\n                                if self._offset > self._failure:\n                                    self._failure = self._offset\n                                    self._expected = []\n                                if self._offset == self._failure:\n                                    self._expected.append(('METAR::altim', '[\\\\d]'))\n                            if address6 is not FAILURE:\n                                elements0.append(address6)\n                                address7 = FAILURE\n                                index4 = self._offset\n                                chunk5, max5 = None, self._offset + 1\n                                if max5 <= self._input_size:\n                                    chunk5 = self._input[self._offset:max5]\n                                if chunk5 == '=':\n                                    address7 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                                    self._offset = self._offset + 1\n                                else:\n                                    address7 = FAILURE\n                                    if self._offset > self._failure:\n                                        self._failure = self._offset\n                                        self._expected = []\n                                    if self._offset == self._failure:\n                                        self._expected.append(('METAR::altim', '\"=\"'))\n                                if address7 is FAILURE:\n                                    address7 = TreeNode(self._input[index4:index4], index4, [])\n                                    self._offset = index4\n                                if address7 is not FAILURE:\n                                    elements0.append(address7)\n                                else:\n                                    elements0 = None\n                                    self._offset = index2\n                            else:\n                                elements0 = None\n                                self._offset = index2\n                        else:\n                            elements0 = None\n                            self._offset = index2\n                    else:\n                        elements0 = None\n                        self._offset = index2\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['altim'][index0] = (address0, self._offset)\n        return address0",
  "def _read_remarks(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['remarks'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        index3 = self._offset\n        address1 = self._read_sep()\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index3:index3], index3, [])\n            self._offset = index3\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            index4 = self._offset\n            chunk0, max0 = None, self._offset + 3\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 == 'RMK':\n                address2 = TreeNode(self._input[self._offset:self._offset + 3], self._offset, [])\n                self._offset = self._offset + 3\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::remarks', '\"RMK\"'))\n            if address2 is FAILURE:\n                self._offset = index4\n                index5, elements1, address3 = self._offset, [], None\n                while True:\n                    chunk1, max1 = None, self._offset + 5\n                    if max1 <= self._input_size:\n                        chunk1 = self._input[self._offset:max1]\n                    if chunk1 == 'NOSIG':\n                        address3 = TreeNode(self._input[self._offset:self._offset + 5], self._offset, [])\n                        self._offset = self._offset + 5\n                    else:\n                        address3 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::remarks', '\"NOSIG\"'))\n                    if address3 is not FAILURE:\n                        elements1.append(address3)\n                    else:\n                        break\n                if len(elements1) >= 0:\n                    address2 = TreeNode(self._input[index5:self._offset], index5, elements1)\n                    self._offset = self._offset\n                else:\n                    address2 = FAILURE\n                if address2 is FAILURE:\n                    self._offset = index4\n            if address2 is not FAILURE:\n                elements0.append(address2)\n                address4 = FAILURE\n                index6, elements2, address5 = self._offset, [], None\n                while True:\n                    if self._offset < self._input_size:\n                        address5 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                        self._offset = self._offset + 1\n                    else:\n                        address5 = FAILURE\n                        if self._offset > self._failure:\n                            self._failure = self._offset\n                            self._expected = []\n                        if self._offset == self._failure:\n                            self._expected.append(('METAR::remarks', '<any char>'))\n                    if address5 is not FAILURE:\n                        elements2.append(address5)\n                    else:\n                        break\n                if len(elements2) >= 0:\n                    address4 = TreeNode(self._input[index6:self._offset], index6, elements2)\n                    self._offset = self._offset\n                else:\n                    address4 = FAILURE\n                if address4 is not FAILURE:\n                    elements0.append(address4)\n                else:\n                    elements0 = None\n                    self._offset = index2\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['remarks'][index0] = (address0, self._offset)\n        return address0",
  "def _read_end(self):\n        address0, index0 = FAILURE, self._offset\n        cached = self._cache['end'].get(index0)\n        if cached:\n            self._offset = cached[1]\n            return cached[0]\n        index1 = self._offset\n        index2, elements0 = self._offset, []\n        address1 = FAILURE\n        index3 = self._offset\n        address1 = self._read_sep()\n        if address1 is FAILURE:\n            address1 = TreeNode(self._input[index3:index3], index3, [])\n            self._offset = index3\n        if address1 is not FAILURE:\n            elements0.append(address1)\n            address2 = FAILURE\n            chunk0, max0 = None, self._offset + 1\n            if max0 <= self._input_size:\n                chunk0 = self._input[self._offset:max0]\n            if chunk0 == '=':\n                address2 = TreeNode(self._input[self._offset:self._offset + 1], self._offset, [])\n                self._offset = self._offset + 1\n            else:\n                address2 = FAILURE\n                if self._offset > self._failure:\n                    self._failure = self._offset\n                    self._expected = []\n                if self._offset == self._failure:\n                    self._expected.append(('METAR::end', '\"=\"'))\n            if address2 is not FAILURE:\n                elements0.append(address2)\n            else:\n                elements0 = None\n                self._offset = index2\n        else:\n            elements0 = None\n            self._offset = index2\n        if elements0 is None:\n            address0 = FAILURE\n        else:\n            address0 = TreeNode(self._input[index2:self._offset], index2, elements0)\n            self._offset = self._offset\n        if address0 is FAILURE:\n            address0 = TreeNode(self._input[index1:index1], index1, [])\n            self._offset = index1\n        self._cache['end'][index0] = (address0, self._offset)\n        return address0",
  "def __init__(self, input, actions, types):\n        self._input = input\n        self._input_size = len(input)\n        self._actions = actions\n        self._types = types\n        self._offset = 0\n        self._cache = defaultdict(dict)\n        self._failure = 0\n        self._expected = []",
  "def parse(self):\n        tree = self._read_ob()\n        if tree is not FAILURE and self._offset == self._input_size:\n            return tree\n        if not self._expected:\n            self._failure = self._offset\n            self._expected.append(('METAR', '<EOF>'))\n        raise ParseError(format_error(self._input, self._failure, self._expected))",
  "def register_processor(num):\n    \"\"\"Register functions to handle particular message numbers.\"\"\"\n    def inner(func):\n        \"\"\"Perform actual function registration.\"\"\"\n        processors[num] = func\n        return func\n    return inner",
  "def process_msg3(fname):\n    \"\"\"Handle information for message type 3.\"\"\"\n    with open(fname) as infile:\n        info = []\n        for lineno, line in enumerate(infile):\n            parts = line.split('  ')\n            try:\n                var_name, desc, typ, units = parts[:4]\n                size_hw = parts[-1]\n                if '-' in size_hw:\n                    start, end = map(int, size_hw.split('-'))\n                    size = (end - start + 1) * 2\n                else:\n                    size = 2\n\n                assert size >= 2\n                fmt = fix_type(typ, size)\n\n                var_name = fix_var_name(var_name)\n                full_desc = fix_desc(desc, units)\n\n                info.append({'name': var_name, 'desc': full_desc, 'fmt': fmt})\n\n                if ignored_item(info[-1]) and var_name != 'Spare':\n                    warnings.warn(f'{var_name} has type {typ}. Setting as Spare')\n\n            except (ValueError, AssertionError):\n                warnings.warn('{} > {}'.format(lineno + 1, ':'.join(parts)))\n                raise\n        return info",
  "def process_msg18(fname):\n    \"\"\"Handle information for message type 18.\"\"\"\n    with open(fname) as infile:\n        info = []\n        for lineno, line in enumerate(infile):\n            parts = line.split('  ')\n            try:\n                if len(parts) == 8:\n                    parts = parts[:6] + [parts[6] + parts[7]]\n\n                var_name, desc, typ, units, rng, prec, byte_range = parts\n                start, end = map(int, byte_range.split('-'))\n                size = end - start + 1\n                assert size >= 4\n                fmt = fix_type(typ, size,\n                               additional=[('See Note (5)', ('{size}s', 1172))])\n\n                if ' ' in var_name:\n                    warnings.warn(f'Space in {var_name}')\n                if not desc:\n                    warnings.warn(f'null description for {var_name}')\n\n                var_name = fix_var_name(var_name)\n                full_desc = fix_desc(desc, units)\n\n                info.append({'name': var_name, 'desc': full_desc, 'fmt': fmt})\n\n                if (ignored_item(info[-1]) and var_name != 'SPARE'\n                        and 'SPARE' not in full_desc):\n                    warnings.warn(f'{var_name} has type {typ}. Setting as SPARE')\n\n            except (ValueError, AssertionError):\n                warnings.warn('{} > {}'.format(lineno + 1, ':'.join(parts)))\n                raise\n        return info",
  "def fix_type(typ, size, additional=None):\n    \"\"\"Fix up creating the appropriate struct type based on the information in the column.\"\"\"\n    my_types = types + additional if additional is not None else types\n    for t, info in my_types:\n        matches = t(typ) if callable(t) else t == typ\n        if matches:\n            fmt_str, true_size = info(size) if callable(info) else info\n            assert size == true_size, (f'{typ}: Got size {size} instead of {true_size}')\n            return fmt_str.format(size=size)\n\n    raise ValueError(f'No type match! ({typ})')",
  "def fix_var_name(var_name):\n    \"\"\"Clean up and apply standard formatting to variable names.\"\"\"\n    name = var_name.strip()\n    for char in '(). /#,':\n        name = name.replace(char, '_')\n    name = name.replace('+', 'pos_')\n    name = name.replace('-', 'neg_')\n    if name.endswith('_'):\n        name = name[:-1]\n    return name",
  "def fix_desc(desc, units=None):\n    \"\"\"Clean up description column.\"\"\"\n    full_desc = desc.strip()\n    if units and units != 'N/A':\n        if full_desc:\n            full_desc += ' (' + units + ')'\n        else:\n            full_desc = units\n    return full_desc",
  "def ignored_item(item):\n    \"\"\"Determine whether this item should be ignored.\"\"\"\n    return item['name'].upper() == 'SPARE' or 'x' in item['fmt']",
  "def need_desc(item):\n    \"\"\"Determine whether we need a description for this item.\"\"\"\n    return item['desc'] and not ignored_item(item)",
  "def field_name(item):\n    \"\"\"Return the field name if appropriate.\"\"\"\n    return '\"{:s}\"'.format(item['name']) if not ignored_item(item) else None",
  "def field_fmt(item):\n    \"\"\"Return the field format if appropriate.\"\"\"\n    return '\"{:s}\"'.format(item['fmt']) if '\"' not in item['fmt'] else item['fmt']",
  "def write_file(fname, info):\n    \"\"\"Write out the generated Python code.\"\"\"\n    with open(fname, 'w') as outfile:\n        # File header\n        outfile.write('# Copyright (c) 2018 MetPy Developers.\\n')\n        outfile.write('# Distributed under the terms of the BSD 3-Clause License.\\n')\n        outfile.write('# SPDX-License-Identifier: BSD-3-Clause\\n\\n')\n        outfile.write('# flake8: noqa\\n')\n        outfile.write('# Generated file -- do not modify\\n')\n\n        # Variable descriptions\n        outfile.write('descriptions = {')\n        outdata = ',\\n                '.join('\"{name}\": \"{desc}\"'.format(\n            **i) for i in info if need_desc(i))\n        outfile.write(outdata)\n        outfile.write('}\\n\\n')\n\n        # Now the struct format\n        outfile.write('fields = [')\n        outdata = ',\\n          '.join('({fname}, \"{fmt}\")'.format(\n            fname=field_name(i), **i) for i in info)\n        outfile.write(outdata)\n        outfile.write(']\\n')",
  "def inner(func):\n        \"\"\"Perform actual function registration.\"\"\"\n        processors[num] = func\n        return func",
  "def test_plugin(source, errs):\n    \"\"\"Test that the flake8 checker works correctly.\"\"\"\n    checker = MetPyChecker(ast.parse(source))\n    assert len(list(checker.run())) == errs",
  "class MetPyVisitor(ast.NodeVisitor):\n    \"\"\"Visit nodes of the AST looking for violations.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the visitor.\"\"\"\n        self.errors = []\n\n    @staticmethod\n    def _is_unit(node):\n        \"\"\"Check whether a node should be considered to represent \"units\".\"\"\"\n        # Looking for a .units attribute, a units.myunit, or a call to units()\n        is_units_attr = isinstance(node, ast.Attribute) and node.attr == 'units'\n        is_reg_attr = (isinstance(node, ast.Attribute)\n                       and isinstance(node.value, ast.Name) and node.value.id == 'units')\n        is_reg_call = (isinstance(node, ast.Call)\n                       and isinstance(node.func, ast.Name) and node.func.id == 'units')\n        is_unit_alias = isinstance(node, ast.Name) and 'unit' in node.id\n\n        return is_units_attr or is_reg_attr or is_reg_call or is_unit_alias\n\n    def visit_BinOp(self, node):  # noqa: N802\n        \"\"\"Visit binary operations.\"\"\"\n        # Check whether this is multiplying or dividing by units\n        if (isinstance(node.op, (ast.Mult, ast.Div))\n                and (self._is_unit(node.right) or self._is_unit(node.left))):\n            self.error(node.lineno, node.col_offset, 1)\n\n        super().generic_visit(node)\n\n    def error(self, lineno, col, code):\n        \"\"\"Add an error to our output list.\"\"\"\n        self.errors.append(Error(lineno, col, code))",
  "class MetPyChecker:\n    \"\"\"Flake8 plugin class to check MetPy style/best practice.\"\"\"\n\n    name = __name__\n    version = '1.0'\n\n    def __init__(self, tree):\n        \"\"\"Initialize the plugin.\"\"\"\n        self.tree = tree\n\n    def run(self):\n        \"\"\"Run the plugin and yield errors.\"\"\"\n        visitor = MetPyVisitor()\n        visitor.visit(self.tree)\n        for err in visitor.errors:\n            yield self.error(err)\n\n    def error(self, err):\n        \"\"\"Format errors into Flake8's required format.\"\"\"\n        return (err.lineno, err.col,\n                f'MPY{err.code:03d}: Multiplying/dividing by units--use units.Quantity()',\n                type(self))",
  "def __init__(self):\n        \"\"\"Initialize the visitor.\"\"\"\n        self.errors = []",
  "def _is_unit(node):\n        \"\"\"Check whether a node should be considered to represent \"units\".\"\"\"\n        # Looking for a .units attribute, a units.myunit, or a call to units()\n        is_units_attr = isinstance(node, ast.Attribute) and node.attr == 'units'\n        is_reg_attr = (isinstance(node, ast.Attribute)\n                       and isinstance(node.value, ast.Name) and node.value.id == 'units')\n        is_reg_call = (isinstance(node, ast.Call)\n                       and isinstance(node.func, ast.Name) and node.func.id == 'units')\n        is_unit_alias = isinstance(node, ast.Name) and 'unit' in node.id\n\n        return is_units_attr or is_reg_attr or is_reg_call or is_unit_alias",
  "def visit_BinOp(self, node):  # noqa: N802\n        \"\"\"Visit binary operations.\"\"\"\n        # Check whether this is multiplying or dividing by units\n        if (isinstance(node.op, (ast.Mult, ast.Div))\n                and (self._is_unit(node.right) or self._is_unit(node.left))):\n            self.error(node.lineno, node.col_offset, 1)\n\n        super().generic_visit(node)",
  "def error(self, lineno, col, code):\n        \"\"\"Add an error to our output list.\"\"\"\n        self.errors.append(Error(lineno, col, code))",
  "def __init__(self, tree):\n        \"\"\"Initialize the plugin.\"\"\"\n        self.tree = tree",
  "def run(self):\n        \"\"\"Run the plugin and yield errors.\"\"\"\n        visitor = MetPyVisitor()\n        visitor.visit(self.tree)\n        for err in visitor.errors:\n            yield self.error(err)",
  "def error(self, err):\n        \"\"\"Format errors into Flake8's required format.\"\"\"\n        return (err.lineno, err.col,\n                f'MPY{err.code:03d}: Multiplying/dividing by units--use units.Quantity()',\n                type(self))",
  "def grab_ne(category, feature, res):\n    \"\"\"Download the correct Natural Earth feature using Cartopy.\"\"\"\n    download = Downloader.from_config(('shapefiles', 'natural_earth'))\n    download.path({'category': category, 'name': feature, 'resolution': res, 'config': config})",
  "def get_failing_links(fname):\n    \"\"\"Yield links with problematic statuses.\"\"\"\n    with open(fname) as linkfile:\n        links = json.loads('[' + ','.join(linkfile) + ']')\n        for link in links:\n            if link['status'] not in {'working', 'ignored', 'unchecked'}:\n                yield link",
  "def get_added():\n    \"\"\"Get all lines added in the most recent merge.\"\"\"\n    revs = subprocess.check_output(['git', 'rev-list', '--parents', '-n', '1', 'HEAD'])\n    merge_commit, target, _ = revs.decode('utf-8').split()\n    diff = subprocess.check_output(['git', 'diff', f'{target}...{merge_commit}'])\n    return '\\n'.join(line for line in diff.decode('utf-8').split('\\n')\n                     if line.startswith('+') and not line.startswith('+++'))"
]