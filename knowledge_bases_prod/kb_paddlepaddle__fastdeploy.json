[
  "def cd(path):\n    if not os.path.isabs(path):\n        raise RuntimeError('Can only cd to absolute path, got: {}'.format(path))\n    orig_path = os.getcwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(orig_path)",
  "class ONNXCommand(setuptools.Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass",
  "def get_all_files(dirname):\n    files = list()\n    for root, dirs, filenames in os.walk(dirname):\n        for f in filenames:\n            fullname = os.path.join(root, f)\n            files.append(fullname)\n    return files",
  "class create_version(ONNXCommand):\n    def run(self):\n        with open(os.path.join(PYTHON_SRC_DIR, 'code_version.py'), 'w') as f:\n            f.write(\n                dedent('''\\\n            # This file is generated by setup.py. DO NOT EDIT!\n            from __future__ import absolute_import\n            from __future__ import division\n            from __future__ import print_function\n            from __future__ import unicode_literals\n            version = '{version}'\n            git_version = '{git_version}'\n            '''.format(**dict(VersionInfo._asdict()))))",
  "class cmake_build(setuptools.Command):\n    \"\"\"\n    Compiles everything when `python setupmnm.py build` is run using cmake.\n    Custom args can be passed to cmake by specifying the `CMAKE_ARGS`\n    environment variable.\n    The number of CPUs used by `make` can be specified by passing `-j<ncpus>`\n    to `setup.py build`.  By default all CPUs are used.\n    \"\"\"\n    user_options = [(str('jobs='), str('j'),\n                     str('Specifies the number of jobs to use with make'))]\n\n    built = False\n\n    def initialize_options(self):\n        self.jobs = None\n\n    def finalize_options(self):\n        if sys.version_info[0] >= 3:\n            self.set_undefined_options('build', ('parallel', 'jobs'))\n        if self.jobs is None and os.getenv(\"MAX_JOBS\") is not None:\n            self.jobs = os.getenv(\"MAX_JOBS\")\n        self.jobs = multiprocessing.cpu_count() if self.jobs is None else int(\n            self.jobs)\n\n    def run(self):\n        if cmake_build.built:\n            return\n        cmake_build.built = True\n        if not os.path.exists(CMAKE_BUILD_DIR):\n            os.makedirs(CMAKE_BUILD_DIR)\n\n        with cd(CMAKE_BUILD_DIR):\n            build_type = 'Release'\n            # configure\n            cmake_args = [\n                CMAKE,\n                '-DPYTHON_INCLUDE_DIR={}'.format(sysconfig.get_python_inc()),\n                '-DPYTHON_EXECUTABLE={}'.format(sys.executable),\n                '-DBUILD_FDSTREAMER_PYTHON=ON',\n                '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON',\n                '-DPY_EXT_SUFFIX={}'.format(\n                    sysconfig.get_config_var('EXT_SUFFIX') or ''),\n                '-DFASTDEPLOY_INSTALL_DIR={}'.format(FASTDEPLOY_INSTALL_DIR),\n            ]\n            cmake_args.append('-DCMAKE_BUILD_TYPE=%s' % build_type)\n            for k, v in setup_configs.items():\n                cmake_args.append(\"-D{}={}\".format(k, v))\n            if 'CMAKE_ARGS' in os.environ:\n                extra_cmake_args = shlex.split(os.environ['CMAKE_ARGS'])\n                # prevent crossfire with downstream scripts\n                del os.environ['CMAKE_ARGS']\n                log.info('Extra cmake args: {}'.format(extra_cmake_args))\n                cmake_args.extend(extra_cmake_args)\n            cmake_args.append(TOP_DIR)\n            subprocess.check_call(cmake_args)\n\n            build_args = [CMAKE, '--build', os.curdir]\n            if WINDOWS:\n                build_args.extend(['--config', build_type])\n                build_args.extend(['--', '/maxcpucount:{}'.format(self.jobs)])\n            else:\n                build_args.extend(['--', '-j', str(self.jobs)])\n            subprocess.check_call(build_args)",
  "class build_py(setuptools.command.build_py.build_py):\n    def run(self):\n        self.run_command('create_version')\n        self.run_command('cmake_build')\n\n        generated_python_files = \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, PACKAGE_NAME, '*.py')) + \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, PACKAGE_NAME, '*.pyi'))\n\n        for src in generated_python_files:\n            dst = os.path.join(TOP_DIR, os.path.relpath(src, CMAKE_BUILD_DIR))\n            self.copy_file(src, dst)\n\n        return setuptools.command.build_py.build_py.run(self)",
  "class develop(setuptools.command.develop.develop):\n    def run(self):\n        self.run_command('build_py')\n        setuptools.command.develop.develop.run(self)",
  "class build_ext(setuptools.command.build_ext.build_ext):\n    def run(self):\n        self.run_command('cmake_build')\n        setuptools.command.build_ext.build_ext.run(self)\n\n    def build_extensions(self):\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = os.path.basename(self.get_ext_filename(fullname))\n\n            lib_path = CMAKE_BUILD_DIR\n            if os.name == 'nt':\n                debug_lib_dir = os.path.join(lib_path, \"Debug\")\n                release_lib_dir = os.path.join(lib_path, \"Release\")\n                if os.path.exists(debug_lib_dir):\n                    lib_path = debug_lib_dir\n                elif os.path.exists(release_lib_dir):\n                    lib_path = release_lib_dir\n            src = os.path.join(lib_path, filename)\n            dst = os.path.join(\n                os.path.realpath(self.build_lib), PACKAGE_NAME, filename)\n            self.copy_file(src, dst)",
  "class mypy_type_check(ONNXCommand):\n    description = 'Run MyPy type checker'\n\n    def run(self):\n        \"\"\"Run command.\"\"\"\n        onnx_script = os.path.realpath(\n            os.path.join(\n                os.path.dirname(os.path.abspath(__file__)),\n                \"tools/mypy-onnx.py\"))\n        returncode = subprocess.call([sys.executable, onnx_script])\n        sys.exit(returncode)",
  "def initialize_options(self):\n        pass",
  "def finalize_options(self):\n        pass",
  "def run(self):\n        with open(os.path.join(PYTHON_SRC_DIR, 'code_version.py'), 'w') as f:\n            f.write(\n                dedent('''\\\n            # This file is generated by setup.py. DO NOT EDIT!\n            from __future__ import absolute_import\n            from __future__ import division\n            from __future__ import print_function\n            from __future__ import unicode_literals\n            version = '{version}'\n            git_version = '{git_version}'\n            '''.format(**dict(VersionInfo._asdict()))))",
  "def initialize_options(self):\n        self.jobs = None",
  "def finalize_options(self):\n        if sys.version_info[0] >= 3:\n            self.set_undefined_options('build', ('parallel', 'jobs'))\n        if self.jobs is None and os.getenv(\"MAX_JOBS\") is not None:\n            self.jobs = os.getenv(\"MAX_JOBS\")\n        self.jobs = multiprocessing.cpu_count() if self.jobs is None else int(\n            self.jobs)",
  "def run(self):\n        if cmake_build.built:\n            return\n        cmake_build.built = True\n        if not os.path.exists(CMAKE_BUILD_DIR):\n            os.makedirs(CMAKE_BUILD_DIR)\n\n        with cd(CMAKE_BUILD_DIR):\n            build_type = 'Release'\n            # configure\n            cmake_args = [\n                CMAKE,\n                '-DPYTHON_INCLUDE_DIR={}'.format(sysconfig.get_python_inc()),\n                '-DPYTHON_EXECUTABLE={}'.format(sys.executable),\n                '-DBUILD_FDSTREAMER_PYTHON=ON',\n                '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON',\n                '-DPY_EXT_SUFFIX={}'.format(\n                    sysconfig.get_config_var('EXT_SUFFIX') or ''),\n                '-DFASTDEPLOY_INSTALL_DIR={}'.format(FASTDEPLOY_INSTALL_DIR),\n            ]\n            cmake_args.append('-DCMAKE_BUILD_TYPE=%s' % build_type)\n            for k, v in setup_configs.items():\n                cmake_args.append(\"-D{}={}\".format(k, v))\n            if 'CMAKE_ARGS' in os.environ:\n                extra_cmake_args = shlex.split(os.environ['CMAKE_ARGS'])\n                # prevent crossfire with downstream scripts\n                del os.environ['CMAKE_ARGS']\n                log.info('Extra cmake args: {}'.format(extra_cmake_args))\n                cmake_args.extend(extra_cmake_args)\n            cmake_args.append(TOP_DIR)\n            subprocess.check_call(cmake_args)\n\n            build_args = [CMAKE, '--build', os.curdir]\n            if WINDOWS:\n                build_args.extend(['--config', build_type])\n                build_args.extend(['--', '/maxcpucount:{}'.format(self.jobs)])\n            else:\n                build_args.extend(['--', '-j', str(self.jobs)])\n            subprocess.check_call(build_args)",
  "def run(self):\n        self.run_command('create_version')\n        self.run_command('cmake_build')\n\n        generated_python_files = \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, PACKAGE_NAME, '*.py')) + \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, PACKAGE_NAME, '*.pyi'))\n\n        for src in generated_python_files:\n            dst = os.path.join(TOP_DIR, os.path.relpath(src, CMAKE_BUILD_DIR))\n            self.copy_file(src, dst)\n\n        return setuptools.command.build_py.build_py.run(self)",
  "def run(self):\n        self.run_command('build_py')\n        setuptools.command.develop.develop.run(self)",
  "def run(self):\n        self.run_command('cmake_build')\n        setuptools.command.build_ext.build_ext.run(self)",
  "def build_extensions(self):\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = os.path.basename(self.get_ext_filename(fullname))\n\n            lib_path = CMAKE_BUILD_DIR\n            if os.name == 'nt':\n                debug_lib_dir = os.path.join(lib_path, \"Debug\")\n                release_lib_dir = os.path.join(lib_path, \"Release\")\n                if os.path.exists(debug_lib_dir):\n                    lib_path = debug_lib_dir\n                elif os.path.exists(release_lib_dir):\n                    lib_path = release_lib_dir\n            src = os.path.join(lib_path, filename)\n            dst = os.path.join(\n                os.path.realpath(self.build_lib), PACKAGE_NAME, filename)\n            self.copy_file(src, dst)",
  "def run(self):\n        \"\"\"Run command.\"\"\"\n        onnx_script = os.path.realpath(\n            os.path.join(\n                os.path.dirname(os.path.abspath(__file__)),\n                \"tools/mypy-onnx.py\"))\n        returncode = subprocess.call([sys.executable, onnx_script])\n        sys.exit(returncode)",
  "class FDStreamer:\n    def __init__(self, config_file):\n        self.streamer = C.FDStreamer()\n        self.streamer.Init(config_file)\n\n    def Run(self):\n        self.streamer.Run()\n\n    def RunAsync(self):\n        self.streamer.RunAsync()",
  "def __init__(self, config_file):\n        self.streamer = C.FDStreamer()\n        self.streamer.Init(config_file)",
  "def Run(self):\n        self.streamer.Run()",
  "def RunAsync(self):\n        self.streamer.RunAsync()",
  "def process_on_linux(current_dir):\n    rpaths = [\"$ORIGIN:$ORIGIN/libs\"]\n    fd_libs = list()\n    libs_path = os.path.join(current_dir, \"streamer\", \"libs\")\n    for f in os.listdir(libs_path):\n        filename = os.path.join(libs_path, f)\n        if not os.path.isfile(filename):\n            continue\n        if f.count(\"fastdeploy\") and f.count(\".so\") > 0:\n            fd_libs.append(filename)\n\n    cmake_build_dir = os.path.join(current_dir, \".setuptools-cmake-build\")\n\n    for lib in fd_libs:\n        command = \"{} --set-rpath '{}' {}\".format(patchelf_bin_path, \":\".join(rpaths), lib)\n        if platform.machine() != 'sw_64' and platform.machine() != 'mips64':\n            assert subprocess.Popen(\n                command,\n                shell=True) != 0, \"patchelf {} failed, the command: {}\".format(\n                    command, lib)",
  "def get_all_files(dirname):\n    files = list()\n    for root, dirs, filenames in os.walk(dirname):\n        for f in filenames:\n            fullname = os.path.join(root, f)\n            files.append(fullname)\n    return files",
  "def process_libraries(current_dir):\n    if platform.system().lower() == \"linux\":\n        process_on_linux(current_dir)\n    elif platform.system().lower() == \"darwin\":\n        process_on_mac(current_dir)\n    elif platform.system().lower() == \"windows\":\n        process_on_windows(current_dir)\n\n    all_files = get_all_files(os.path.join(current_dir, \"streamer\", \"libs\"))\n    package_data = list()\n\n    filters = [\".vcxproj\", \".png\", \".java\", \".h\", \".cc\", \".cpp\", \".hpp\"]\n    for f in all_files:\n        remain = True\n\n        if remain:\n            package_data.append(\n                os.path.relpath(f, os.path.join(current_dir, \"streamer\")))\n    return package_data",
  "def cd(path):\n    if not os.path.isabs(path):\n        raise RuntimeError('Can only cd to absolute path, got: {}'.format(\n            path))\n    orig_path = os.getcwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(orig_path)",
  "class ONNXCommand(setuptools.Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass",
  "def get_all_files(dirname):\n    files = list()\n    for root, dirs, filenames in os.walk(dirname):\n        for f in filenames:\n            fullname = os.path.join(root, f)\n            files.append(fullname)\n    return files",
  "class create_version(ONNXCommand):\n    def run(self):\n        with open(os.path.join(PYTHON_SRC_DIR, 'code_version.py'), 'w') as f:\n            f.write(\n                dedent('''\\\n            # This file is generated by setup.py. DO NOT EDIT!\n            from __future__ import absolute_import\n            from __future__ import division\n            from __future__ import print_function\n            from __future__ import unicode_literals\n            version = '{version}'\n            git_version = '{git_version}'\n            extra_version_info = '{extra_version_info}'\n            enable_trt_backend = '{enable_trt_backend}'\n            enable_paddle_backend = '{enable_paddle_backend}'\n            with_gpu = '{with_gpu}'\n            '''.format(**dict(VersionInfo._asdict()))))",
  "class cmake_build(setuptools.Command):\n    \"\"\"\n    Compiles everything when `python setupmnm.py build` is run using cmake.\n    Custom args can be passed to cmake by specifying the `CMAKE_ARGS`\n    environment variable.\n    The number of CPUs used by `make` can be specified by passing `-j<ncpus>`\n    to `setup.py build`.  By default all CPUs are used.\n    \"\"\"\n    user_options = [(str('jobs='), str('j'),\n                     str('Specifies the number of jobs to use with make'))]\n\n    built = False\n\n    def initialize_options(self):\n        self.jobs = None\n\n    def finalize_options(self):\n        if sys.version_info[0] >= 3:\n            self.set_undefined_options('build', ('parallel', 'jobs'))\n        if self.jobs is None and os.getenv(\"MAX_JOBS\") is not None:\n            self.jobs = os.getenv(\"MAX_JOBS\")\n        self.jobs = multiprocessing.cpu_count() if self.jobs is None else int(\n            self.jobs)\n\n    def run(self):\n        if cmake_build.built:\n            return\n        cmake_build.built = True\n        if not os.path.exists(CMAKE_BUILD_DIR):\n            os.makedirs(CMAKE_BUILD_DIR)\n\n        with cd(CMAKE_BUILD_DIR):\n            build_type = 'Release'\n            # configure\n            cmake_args = [\n                CMAKE,\n                '-DPYTHON_INCLUDE_DIR={}'.format(sysconfig.get_python_inc()),\n                '-DPYTHON_EXECUTABLE={}'.format(sys.executable),\n                '-DBUILD_FASTDEPLOY_PYTHON=ON',\n                '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON',\n                '-DONNX_NAMESPACE={}'.format(ONNX_NAMESPACE),\n                '-DPY_EXT_SUFFIX={}'.format(\n                    sysconfig.get_config_var('EXT_SUFFIX') or ''),\n            ]\n            cmake_args.append('-DCMAKE_BUILD_TYPE=%s' % build_type)\n            for k, v in setup_configs.items():\n                cmake_args.append(\"-D{}={}\".format(k, v))\n            if WINDOWS:\n                cmake_args.extend([\n                    # we need to link with libpython on windows, so\n                    # passing python version to window in order to\n                    # find python in cmake\n                    '-DPY_VERSION={}'.format('{0}.{1}'.format(* \\\n                                                              sys.version_info[:2])),\n                ])\n                if platform.architecture()[0] == '64bit':\n                    cmake_args.extend(['-A', 'x64', '-T', 'host=x64'])\n                else:\n                    cmake_args.extend(['-A', 'Win32', '-T', 'host=x86'])\n            if 'CMAKE_ARGS' in os.environ:\n                extra_cmake_args = shlex.split(os.environ['CMAKE_ARGS'])\n                # prevent crossfire with downstream scripts\n                del os.environ['CMAKE_ARGS']\n                log.info('Extra cmake args: {}'.format(extra_cmake_args))\n                cmake_args.extend(extra_cmake_args)\n            cmake_args.append(TOP_DIR)\n            subprocess.check_call(cmake_args)\n\n            build_args = [CMAKE, '--build', os.curdir]\n            if WINDOWS:\n                build_args.extend(['--config', build_type])\n                build_args.extend(['--', '/maxcpucount:{}'.format(self.jobs)])\n            else:\n                build_args.extend(['--', '-j', str(self.jobs)])\n            subprocess.check_call(build_args)",
  "class build_py(setuptools.command.build_py.build_py):\n    def run(self):\n        self.run_command('create_version')\n        self.run_command('cmake_build')\n\n        generated_python_files = \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, PACKAGE_NAME, '*.py')) + \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, PACKAGE_NAME, '*.pyi'))\n\n        for src in generated_python_files:\n            dst = os.path.join(TOP_DIR, os.path.relpath(src, CMAKE_BUILD_DIR))\n            self.copy_file(src, dst)\n\n        return setuptools.command.build_py.build_py.run(self)",
  "class develop(setuptools.command.develop.develop):\n    def run(self):\n        self.run_command('build_py')\n        setuptools.command.develop.develop.run(self)",
  "class build_ext(setuptools.command.build_ext.build_ext):\n    def run(self):\n        self.run_command('cmake_build')\n        setuptools.command.build_ext.build_ext.run(self)\n\n    def build_extensions(self):\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = os.path.basename(self.get_ext_filename(fullname))\n\n            lib_path = CMAKE_BUILD_DIR\n            if os.name == 'nt':\n                debug_lib_dir = os.path.join(lib_path, \"Debug\")\n                release_lib_dir = os.path.join(lib_path, \"Release\")\n                if os.path.exists(debug_lib_dir):\n                    lib_path = debug_lib_dir\n                elif os.path.exists(release_lib_dir):\n                    lib_path = release_lib_dir\n            src = os.path.join(lib_path, filename)\n            dst = os.path.join(\n                os.path.realpath(self.build_lib), PACKAGE_NAME, filename)\n            self.copy_file(src, dst)",
  "class mypy_type_check(ONNXCommand):\n    description = 'Run MyPy type checker'\n\n    def run(self):\n        \"\"\"Run command.\"\"\"\n        onnx_script = os.path.realpath(\n            os.path.join(\n                os.path.dirname(os.path.abspath(__file__)),\n                \"tools/mypy-onnx.py\"))\n        returncode = subprocess.call([sys.executable, onnx_script])\n        sys.exit(returncode)",
  "def initialize_options(self):\n        pass",
  "def finalize_options(self):\n        pass",
  "def run(self):\n        with open(os.path.join(PYTHON_SRC_DIR, 'code_version.py'), 'w') as f:\n            f.write(\n                dedent('''\\\n            # This file is generated by setup.py. DO NOT EDIT!\n            from __future__ import absolute_import\n            from __future__ import division\n            from __future__ import print_function\n            from __future__ import unicode_literals\n            version = '{version}'\n            git_version = '{git_version}'\n            extra_version_info = '{extra_version_info}'\n            enable_trt_backend = '{enable_trt_backend}'\n            enable_paddle_backend = '{enable_paddle_backend}'\n            with_gpu = '{with_gpu}'\n            '''.format(**dict(VersionInfo._asdict()))))",
  "def initialize_options(self):\n        self.jobs = None",
  "def finalize_options(self):\n        if sys.version_info[0] >= 3:\n            self.set_undefined_options('build', ('parallel', 'jobs'))\n        if self.jobs is None and os.getenv(\"MAX_JOBS\") is not None:\n            self.jobs = os.getenv(\"MAX_JOBS\")\n        self.jobs = multiprocessing.cpu_count() if self.jobs is None else int(\n            self.jobs)",
  "def run(self):\n        if cmake_build.built:\n            return\n        cmake_build.built = True\n        if not os.path.exists(CMAKE_BUILD_DIR):\n            os.makedirs(CMAKE_BUILD_DIR)\n\n        with cd(CMAKE_BUILD_DIR):\n            build_type = 'Release'\n            # configure\n            cmake_args = [\n                CMAKE,\n                '-DPYTHON_INCLUDE_DIR={}'.format(sysconfig.get_python_inc()),\n                '-DPYTHON_EXECUTABLE={}'.format(sys.executable),\n                '-DBUILD_FASTDEPLOY_PYTHON=ON',\n                '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON',\n                '-DONNX_NAMESPACE={}'.format(ONNX_NAMESPACE),\n                '-DPY_EXT_SUFFIX={}'.format(\n                    sysconfig.get_config_var('EXT_SUFFIX') or ''),\n            ]\n            cmake_args.append('-DCMAKE_BUILD_TYPE=%s' % build_type)\n            for k, v in setup_configs.items():\n                cmake_args.append(\"-D{}={}\".format(k, v))\n            if WINDOWS:\n                cmake_args.extend([\n                    # we need to link with libpython on windows, so\n                    # passing python version to window in order to\n                    # find python in cmake\n                    '-DPY_VERSION={}'.format('{0}.{1}'.format(* \\\n                                                              sys.version_info[:2])),\n                ])\n                if platform.architecture()[0] == '64bit':\n                    cmake_args.extend(['-A', 'x64', '-T', 'host=x64'])\n                else:\n                    cmake_args.extend(['-A', 'Win32', '-T', 'host=x86'])\n            if 'CMAKE_ARGS' in os.environ:\n                extra_cmake_args = shlex.split(os.environ['CMAKE_ARGS'])\n                # prevent crossfire with downstream scripts\n                del os.environ['CMAKE_ARGS']\n                log.info('Extra cmake args: {}'.format(extra_cmake_args))\n                cmake_args.extend(extra_cmake_args)\n            cmake_args.append(TOP_DIR)\n            subprocess.check_call(cmake_args)\n\n            build_args = [CMAKE, '--build', os.curdir]\n            if WINDOWS:\n                build_args.extend(['--config', build_type])\n                build_args.extend(['--', '/maxcpucount:{}'.format(self.jobs)])\n            else:\n                build_args.extend(['--', '-j', str(self.jobs)])\n            subprocess.check_call(build_args)",
  "def run(self):\n        self.run_command('create_version')\n        self.run_command('cmake_build')\n\n        generated_python_files = \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, PACKAGE_NAME, '*.py')) + \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, PACKAGE_NAME, '*.pyi'))\n\n        for src in generated_python_files:\n            dst = os.path.join(TOP_DIR, os.path.relpath(src, CMAKE_BUILD_DIR))\n            self.copy_file(src, dst)\n\n        return setuptools.command.build_py.build_py.run(self)",
  "def run(self):\n        self.run_command('build_py')\n        setuptools.command.develop.develop.run(self)",
  "def run(self):\n        self.run_command('cmake_build')\n        setuptools.command.build_ext.build_ext.run(self)",
  "def build_extensions(self):\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = os.path.basename(self.get_ext_filename(fullname))\n\n            lib_path = CMAKE_BUILD_DIR\n            if os.name == 'nt':\n                debug_lib_dir = os.path.join(lib_path, \"Debug\")\n                release_lib_dir = os.path.join(lib_path, \"Release\")\n                if os.path.exists(debug_lib_dir):\n                    lib_path = debug_lib_dir\n                elif os.path.exists(release_lib_dir):\n                    lib_path = release_lib_dir\n            src = os.path.join(lib_path, filename)\n            dst = os.path.join(\n                os.path.realpath(self.build_lib), PACKAGE_NAME, filename)\n            self.copy_file(src, dst)",
  "def run(self):\n        \"\"\"Run command.\"\"\"\n        onnx_script = os.path.realpath(\n            os.path.join(\n                os.path.dirname(os.path.abspath(__file__)),\n                \"tools/mypy-onnx.py\"))\n        returncode = subprocess.call([sys.executable, onnx_script])\n        sys.exit(returncode)",
  "def md5check(fullname, md5sum=None):\n    if md5sum is None:\n        return True\n\n    logging.info(\"File {} md5 checking...\".format(fullname))\n    md5 = hashlib.md5()\n    with open(fullname, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            md5.update(chunk)\n    calc_md5sum = md5.hexdigest()\n\n    if calc_md5sum != md5sum:\n        logging.info(\"File {} md5 check failed, {}(calc) != \"\n                     \"{}(base)\".format(fullname, calc_md5sum, md5sum))\n        return False\n    return True",
  "def move_and_merge_tree(src, dst):\n    \"\"\"\n    Move src directory to dst, if dst is already exists,\n    merge src to dst\n    \"\"\"\n    if not osp.exists(dst):\n        shutil.move(src, dst)\n    else:\n        if not osp.isdir(src):\n            shutil.move(src, dst)\n            return\n        for fp in os.listdir(src):\n            src_fp = osp.join(src, fp)\n            dst_fp = osp.join(dst, fp)\n            if osp.isdir(src_fp):\n                if osp.isdir(dst_fp):\n                    move_and_merge_tree(src_fp, dst_fp)\n                else:\n                    shutil.move(src_fp, dst_fp)\n            elif osp.isfile(src_fp) and \\\n                    not osp.isfile(dst_fp):\n                shutil.move(src_fp, dst_fp)",
  "def download(url, path, rename=None, md5sum=None, show_progress=False):\n    \"\"\"\n    Download from url, save to path.\n    url (str): download url\n    path (str): download to given path\n    \"\"\"\n    if not osp.exists(path):\n        os.makedirs(path)\n\n    fname = osp.split(url)[-1]\n    fullname = osp.join(path, fname)\n    if rename is not None:\n        fullname = osp.join(path, rename)\n    retry_cnt = 0\n    while not (osp.exists(fullname) and md5check(fullname, md5sum)):\n        if retry_cnt < DOWNLOAD_RETRY_LIMIT:\n            retry_cnt += 1\n        else:\n            logging.debug(\"{} download failed.\".format(fname))\n            raise RuntimeError(\"Download from {} failed. \"\n                               \"Retry limit reached\".format(url))\n\n        logging.info(\"Downloading {} from {}\".format(fname, url))\n\n        req = requests.get(url, stream=True)\n        if req.status_code != 200:\n            raise RuntimeError(\"Downloading from {} failed with code \"\n                               \"{}!\".format(url, req.status_code))\n\n        # For protecting download interupted, download to\n        # tmp_fullname firstly, move tmp_fullname to fullname\n        # after download finished\n        tmp_fullname = fullname + \"_tmp\"\n        total_size = req.headers.get('content-length')\n        with open(tmp_fullname, 'wb') as f:\n            if total_size and show_progress:\n                for chunk in tqdm.tqdm(\n                        req.iter_content(chunk_size=1024),\n                        total=(int(total_size) + 1023) // 1024,\n                        unit='KB'):\n                    f.write(chunk)\n            else:\n                for chunk in req.iter_content(chunk_size=1024):\n                    if chunk:\n                        f.write(chunk)\n        shutil.move(tmp_fullname, fullname)\n        logging.debug(\"{} download completed.\".format(fname))\n\n    return fullname",
  "def decompress(fname):\n    \"\"\"\n    Decompress for zip and tar file\n    \"\"\"\n    logging.info(\"Decompressing {}...\".format(fname))\n\n    # For protecting decompressing interupted,\n    # decompress to fpath_tmp directory firstly, if decompress\n    # successed, move decompress files to fpath and delete\n    # fpath_tmp and remove download compress file.\n    fpath = osp.split(fname)[0]\n    fpath_tmp = osp.join(fpath, 'tmp')\n    if osp.isdir(fpath_tmp):\n        shutil.rmtree(fpath_tmp)\n        os.makedirs(fpath_tmp)\n\n    if fname.find('.tar') >= 0 or fname.find('.tgz') >= 0:\n        with tarfile.open(fname) as tf:\n\n            def is_within_directory(directory, target):\n\n                abs_directory = os.path.abspath(directory)\n                abs_target = os.path.abspath(target)\n\n                prefix = os.path.commonprefix([abs_directory, abs_target])\n\n                return prefix == abs_directory\n\n            def safe_extract(tar,\n                             path=\".\",\n                             members=None,\n                             *,\n                             numeric_owner=False):\n\n                for member in tar.getmembers():\n                    member_path = os.path.join(path, member.name)\n                    if not is_within_directory(path, member_path):\n                        raise Exception(\"Attempted Path Traversal in Tar File\")\n\n                tar.extractall(path, members, numeric_owner=numeric_owner)\n\n            safe_extract(tf, path=fpath_tmp)\n    elif fname.find('.zip') >= 0:\n        with zipfile.ZipFile(fname) as zf:\n            zf.extractall(path=fpath_tmp)\n    else:\n        raise TypeError(\"Unsupport compress file type {}\".format(fname))\n\n    for f in os.listdir(fpath_tmp):\n        src_dir = osp.join(fpath_tmp, f)\n        dst_dir = osp.join(fpath, f)\n        move_and_merge_tree(src_dir, dst_dir)\n\n    shutil.rmtree(fpath_tmp)\n    logging.debug(\"{} decompressed.\".format(fname))\n    return dst_dir",
  "def url2dir(url, path, rename=None):\n    full_name = download(url, path, rename, show_progress=True)\n    print(\"File is donwloaded, now extracting...\")\n    if url.count(\".tgz\") > 0 or url.count(\".tar\") > 0 or url.count(\"zip\") > 0:\n        return decompress(full_name)",
  "def download_and_decompress(url, path='.', rename=None):\n    fname = osp.split(url)[-1]\n    fullname = osp.join(path, fname)\n    # if url.endswith(('tgz', 'tar.gz', 'tar', 'zip')):\n    #     fullname = osp.join(path, fname.split('.')[0])\n    nranks = 0\n    if nranks <= 1:\n        dst_dir = url2dir(url, path, rename)\n        if dst_dir is not None:\n            fullname = dst_dir\n    else:\n        lock_path = fullname + '.lock'\n        if not os.path.exists(fullname):\n            with open(lock_path, 'w'):\n                os.utime(lock_path, None)\n            if local_rank == 0:\n                dst_dir = url2dir(url, path, rename)\n                if dst_dir is not None:\n                    fullname = dst_dir\n                os.remove(lock_path)\n            else:\n                while os.path.exists(lock_path):\n                    time.sleep(1)\n    return",
  "def get_model_list(category: str=None):\n    '''\n    Get all pre-trained models information supported by fd.download_model.\n    Args:\n        category(str): model category, if None, list all models in all categories.\n    Returns:\n        results(dict): a dictionary, key is category, value is a list which contains models information.\n    '''\n    result = model_server.get_model_list()\n    if result['status'] != 0:\n        raise ValueError(\n            'Failed to get pretrained models information from hub model server.'\n        )\n    result = result['data']\n    if category is None:\n        return result\n    elif category in result:\n        return {category: result[category]}\n    else:\n        raise ValueError(\n            'No pretrained model in category {} can be downloaded now.'.format(\n                category))",
  "def download_model(name: str,\n                   path: str=None,\n                   format: str=None,\n                   version: str=None):\n    '''\n    Download pre-trained model for FastDeploy inference engine.\n    Args:\n        name: model name\n        path(str): local path for saving model. If not set, default is hubenv.MODEL_HOME\n        format(str): FastDeploy model format\n        version(str) : FastDeploy model version\n    '''\n    result = model_server.search_model(name, format, version)\n    if path is None:\n        path = hubenv.MODEL_HOME\n    if result:\n        url = result[0]['url']\n        format = result[0]['format']\n        version = result[0]['version']\n        fullpath = download(url, path, show_progress=True)\n        model_server.stat_model(name, format, version)\n        if format == 'paddle':\n            if url.count(\".tgz\") > 0 or url.count(\".tar\") > 0 or url.count(\n                    \"zip\") > 0:\n                archive_path = fullpath\n                fullpath = decompress(fullpath)\n                try:\n                    os.rename(fullpath,\n                              os.path.join(os.path.dirname(fullpath), name))\n                    fullpath = os.path.join(os.path.dirname(fullpath), name)\n                    os.remove(archive_path)\n                except FileExistsError:\n                    pass\n        print('Successfully download model at path: {}'.format(fullpath))\n        return fullpath\n    else:\n        print('ERROR: Could not find a model named {}'.format(name))",
  "def is_within_directory(directory, target):\n\n                abs_directory = os.path.abspath(directory)\n                abs_target = os.path.abspath(target)\n\n                prefix = os.path.commonprefix([abs_directory, abs_target])\n\n                return prefix == abs_directory",
  "def safe_extract(tar,\n                             path=\".\",\n                             members=None,\n                             *,\n                             numeric_owner=False):\n\n                for member in tar.getmembers():\n                    member_path = os.path.join(path, member.name)\n                    if not is_within_directory(path, member_path):\n                        raise Exception(\"Attempted Path Traversal in Tar File\")\n\n                tar.extractall(path, members, numeric_owner=numeric_owner)",
  "class FastDeployModel:\n    def __init__(self, option):\n        self._model = None\n        if option is None:\n            self._runtime_option = C.RuntimeOption()\n        else:\n            self._runtime_option = option._option\n\n    def model_name(self):\n        return self._model.model_name()\n\n    def num_inputs_of_runtime(self):\n        return self._model.num_inputs_of_runtime()\n\n    def num_outputs_of_runtime(self):\n        return self._model.num_outputs_of_runtime()\n\n    def input_info_of_runtime(self, index):\n        assert index < self.num_inputs_of_runtime(\n        ), \"The index:{} must be less than number of inputs:{}.\".format(\n            index, self.num_inputs_of_runtime())\n        return self._model.input_info_of_runtime(index)\n\n    def output_info_of_runtime(self, index):\n        assert index < self.num_outputs_of_runtime(\n        ), \"The index:{} must be less than number of outputs:{}.\".format(\n            index, self.num_outputs_of_runtime())\n        return self._model.output_info_of_runtime(index)\n\n    def enable_record_time_of_runtime(self):\n        self._model.enable_record_time_of_runtime()\n\n    def disable_record_time_of_runtime(self):\n        self._model.disable_record_time_of_runtime()\n\n    def print_statis_info_of_runtime(self):\n        return self._model.print_statis_info_of_runtime()\n\n    def get_profile_time(self):\n        \"\"\"Get profile time of Runtime after the profile process is done.\n        \"\"\"\n        return self._model.get_profile_time()    \n\n    @property\n    def runtime_option(self):\n        return self._model.runtime_option if self._model is not None else None\n\n    @property\n    def initialized(self):\n        if self._model is None:\n            return False\n        return self._model.initialized()",
  "def __init__(self, option):\n        self._model = None\n        if option is None:\n            self._runtime_option = C.RuntimeOption()\n        else:\n            self._runtime_option = option._option",
  "def model_name(self):\n        return self._model.model_name()",
  "def num_inputs_of_runtime(self):\n        return self._model.num_inputs_of_runtime()",
  "def num_outputs_of_runtime(self):\n        return self._model.num_outputs_of_runtime()",
  "def input_info_of_runtime(self, index):\n        assert index < self.num_inputs_of_runtime(\n        ), \"The index:{} must be less than number of inputs:{}.\".format(\n            index, self.num_inputs_of_runtime())\n        return self._model.input_info_of_runtime(index)",
  "def output_info_of_runtime(self, index):\n        assert index < self.num_outputs_of_runtime(\n        ), \"The index:{} must be less than number of outputs:{}.\".format(\n            index, self.num_outputs_of_runtime())\n        return self._model.output_info_of_runtime(index)",
  "def enable_record_time_of_runtime(self):\n        self._model.enable_record_time_of_runtime()",
  "def disable_record_time_of_runtime(self):\n        self._model.disable_record_time_of_runtime()",
  "def print_statis_info_of_runtime(self):\n        return self._model.print_statis_info_of_runtime()",
  "def get_profile_time(self):\n        \"\"\"Get profile time of Runtime after the profile process is done.\n        \"\"\"\n        return self._model.get_profile_time()",
  "def runtime_option(self):\n        return self._model.runtime_option if self._model is not None else None",
  "def initialized(self):\n        if self._model is None:\n            return False\n        return self._model.initialized()",
  "class Runtime:\n    \"\"\"FastDeploy Runtime object.\n    \"\"\"\n\n    def __init__(self, runtime_option):\n        \"\"\"Initialize a FastDeploy Runtime object.\n\n        :param runtime_option: (fastdeploy.RuntimeOption)Options for FastDeploy Runtime\n        \"\"\"\n\n        self._runtime = C.Runtime()\n        self.runtime_option = runtime_option\n        assert self._runtime.init(\n            self.runtime_option._option), \"Initialize Runtime Failed!\"\n\n    def forward(self, *inputs):\n        \"\"\"[Only for Poros backend] Inference with input data for poros\n\n        :param data: (list[str : numpy.ndarray])The input data list\n        :return list of numpy.ndarray\n        \"\"\"\n        if self.runtime_option._option.model_format != ModelFormat.TORCHSCRIPT:\n            raise Exception(\n                \"The forward function is only used for Poros backend, please call infer function\"\n            )\n        inputs_dict = dict()\n        for i in range(len(inputs)):\n            inputs_dict[\"x\" + str(i)] = inputs[i]\n        return self.infer(inputs_dict)\n\n    def infer(self, data):\n        \"\"\"Inference with input data.\n\n        :param data: (dict[str : numpy.ndarray])The input data dict, key value must keep same with the loaded model\n        :return list of numpy.ndarray\n        \"\"\"\n        assert isinstance(data, dict) or isinstance(\n            data, list), \"The input data should be type of dict or list.\"\n        if isinstance(data, dict):\n            for k, v in data.items():\n                if isinstance(v, np.ndarray) and not v.data.contiguous:\n                    data[k] = np.ascontiguousarray(data[k])\n\n        return self._runtime.infer(data)\n\n    def bind_input_tensor(self, name, fdtensor):\n        \"\"\"Bind FDTensor by name, no copy and share input memory\n\n        :param name: (str)The name of input data.\n        :param fdtensor: (fastdeploy.FDTensor)The input FDTensor.\n        \"\"\"\n        self._runtime.bind_input_tensor(name, fdtensor)\n\n    def bind_output_tensor(self, name, fdtensor):\n        \"\"\"Bind FDTensor by name, no copy and share output memory\n\n        :param name: (str)The name of output data.\n        :param fdtensor: (fastdeploy.FDTensor)The output FDTensor.\n        \"\"\"\n        self._runtime.bind_output_tensor(name, fdtensor)\n\n    def zero_copy_infer(self):\n        \"\"\"No params inference the model.\n\n        the input and output data need to pass through the bind_input_tensor and get_output_tensor interfaces.\n        \"\"\"\n        self._runtime.infer()\n\n    def get_output_tensor(self, name):\n        \"\"\"Get output FDTensor by name, no copy and share backend output memory\n\n        :param name: (str)The name of output data.\n        :return fastdeploy.FDTensor\n        \"\"\"\n        return self._runtime.get_output_tensor(name)\n\n    def compile(self, warm_datas):\n        \"\"\"[Only for Poros backend] compile with prewarm data for poros\n\n        :param data: (list[str : numpy.ndarray])The prewarm data list\n        :return TorchScript Model\n        \"\"\"\n        if self.runtime_option._option.model_format != ModelFormat.TORCHSCRIPT:\n            raise Exception(\n                \"The compile function is only used for Poros backend, please call infer function\"\n            )\n        assert isinstance(warm_datas,\n                          list), \"The prewarm data should be type of list.\"\n        for i in range(len(warm_datas)):\n            warm_data = warm_datas[i]\n            if isinstance(warm_data[0], np.ndarray):\n                warm_data = list(data for data in warm_data)\n            else:\n                warm_data = list(data.numpy() for data in warm_data)\n            warm_datas[i] = warm_data\n        return self._runtime.compile(warm_datas, self.runtime_option._option)\n\n    def num_inputs(self):\n        \"\"\"Get number of inputs of the loaded model.\n        \"\"\"\n        return self._runtime.num_inputs()\n\n    def num_outputs(self):\n        \"\"\"Get number of outputs of the loaded model.\n        \"\"\"\n        return self._runtime.num_outputs()\n\n    def get_input_info(self, index):\n        \"\"\"Get input information of the loaded model.\n\n        :param index: (int)Index of the input\n        :return fastdeploy.TensorInfo\n        \"\"\"\n        assert isinstance(\n            index, int), \"The input parameter index should be type of int.\"\n        assert index < self.num_inputs(\n        ), \"The input parameter index:{} should less than number of inputs:{}.\".format(\n            index, self.num_inputs)\n        return self._runtime.get_input_info(index)\n\n    def get_output_info(self, index):\n        \"\"\"Get output information of the loaded model.\n\n        :param index: (int)Index of the output\n        :return fastdeploy.TensorInfo\n        \"\"\"\n        assert isinstance(\n            index, int), \"The input parameter index should be type of int.\"\n        assert index < self.num_outputs(\n        ), \"The input parameter index:{} should less than number of outputs:{}.\".format(\n            index, self.num_outputs)\n        return self._runtime.get_output_info(index)\n\n    def get_profile_time(self):\n        \"\"\"Get profile time of Runtime after the profile process is done.\n        \"\"\"\n        return self._runtime.get_profile_time()",
  "class RuntimeOption:\n    \"\"\"Options for FastDeploy Runtime.\n    \"\"\"\n\n    __slots__ = [\"_option\"]\n\n    def __init__(self):\n        \"\"\"Initialize a FastDeploy RuntimeOption object.\n        \"\"\"\n\n        self._option = C.RuntimeOption()\n\n    def set_model_path(self,\n                       model_path,\n                       params_path=\"\",\n                       model_format=ModelFormat.PADDLE):\n        \"\"\"Set path of model file and parameters file\n\n        :param model_path: (str)Path of model file\n        :param params_path: (str)Path of parameters file\n        :param model_format: (ModelFormat)Format of model, support ModelFormat.PADDLE/ModelFormat.ONNX/ModelFormat.TORCHSCRIPT\n        \"\"\"\n        return self._option.set_model_path(model_path, params_path,\n                                           model_format)\n\n    def set_model_buffer(self,\n                         model_buffer,\n                         params_buffer=\"\",\n                         model_format=ModelFormat.PADDLE):\n        \"\"\"Specify the memory buffer of model and parameter. Used when model and params are loaded directly from memory\n        :param model_buffer: (bytes)The memory buffer of model\n        :param params_buffer: (bytes)The memory buffer of the parameters\n        :param model_format: (ModelFormat)Format of model, support ModelFormat.PADDLE/ModelFormat.ONNX/ModelFormat.TORCHSCRIPT\n        \"\"\"\n        return self._option.set_model_buffer(model_buffer, params_buffer,\n                                             model_format)\n\n    def set_encryption_key(self, encryption_key):\n        \"\"\"When loading encrypted model, encryption_key is required to decrypte model\n        :param encryption_key: (str)The key for decrypting model\n        \"\"\"\n        return self._option.set_encryption_key(encryption_key)\n\n    def use_gpu(self, device_id=0):\n        \"\"\"Inference with Nvidia GPU\n\n        :param device_id: (int)The index of GPU will be used for inference, default 0\n        \"\"\"\n        if not C.is_built_with_gpu():\n            logging.warning(\n                \"The installed fastdeploy-python package is not built with GPU, will force to use CPU. To use GPU, following the commands to install fastdeploy-gpu-python.\"\n            )\n            logging.warning(\n                \"    ================= Install GPU FastDeploy===============\")\n            logging.warning(\"    python -m pip uninstall fastdeploy-python\")\n            logging.warning(\n                \"    python -m pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\"\n            )\n            return\n        return self._option.use_gpu(device_id)\n\n    def use_kunlunxin(self,\n                      device_id=0,\n                      l3_workspace_size=16 * 1024 * 1024,\n                      locked=False,\n                      autotune=True,\n                      autotune_file=\"\",\n                      precision=\"int16\",\n                      adaptive_seqlen=False,\n                      enable_multi_stream=False,\n                      gm_default_size=0):\n        \"\"\"Inference with KunlunXin XPU\n\n        :param device_id: (int)The index of KunlunXin XPU will be used for inference, default 0\n        :param l3_workspace_size: (int)The size of the video memory allocated by the l3 cache, the maximum is 16M, default 16M\n        :param locked: (bool)Whether the allocated L3 cache can be locked. If false, it means that the L3 cache is not locked,\n                        and the allocated L3 cache can be shared by multiple models, and multiple models\n        :param autotune: (bool)Whether to autotune the conv operator in the model.\n                        If true, when the conv operator of a certain dimension is executed for the first time,\n                        it will automatically search for a better algorithm to improve the performance of subsequent conv operators of the same dimension.\n        :param autotune_file: (str)Specify the path of the autotune file. If autotune_file is specified,\n                        the algorithm specified in the file will be used and autotune will not be performed again.\n        :param precision: (str)Calculation accuracy of multi_encoder\n        :param adaptive_seqlen: (bool)adaptive_seqlen Is the input of multi_encoder variable length\n        :param enable_multi_stream: (bool)Whether to enable the multi stream of KunlunXin XPU.\n        :param gm_default_size The default size of context global memory of KunlunXin XPU.\n        \"\"\"\n        return self._option.use_kunlunxin(\n            device_id, l3_workspace_size, locked, autotune, autotune_file,\n            precision, adaptive_seqlen, enable_multi_stream, gm_default_size)\n\n    def use_cpu(self):\n        \"\"\"Inference with CPU\n        \"\"\"\n        return self._option.use_cpu()\n\n    def use_rknpu2(self,\n                   rknpu2_name=C.CpuName.RK356X,\n                   rknpu2_core=C.CoreMask.RKNN_NPU_CORE_AUTO):\n        return self._option.use_rknpu2(rknpu2_name, rknpu2_core)\n\n    def use_sophgo(self):\n        \"\"\"Inference with SOPHGO TPU\n        \"\"\"\n        return self._option.use_sophgo()\n\n    def use_ascend(self):\n        \"\"\"Inference with Huawei Ascend NPU\n        \"\"\"\n        return self._option.use_ascend()\n\n    def disable_valid_backend_check(self):\n        \"\"\" Disable checking validity of backend during inference\n        \"\"\"\n        return self._option.disable_valid_backend_check()\n\n    def enable_valid_backend_check(self):\n        \"\"\"Enable checking validity of backend during inference\n        \"\"\"\n        return self._option.enable_valid_backend_check()\n\n    def set_cpu_thread_num(self, thread_num=-1):\n        \"\"\"Set number of threads if inference with CPU\n\n        :param thread_num: (int)Number of threads, if not positive, means the number of threads is decided by the backend, default -1\n        \"\"\"\n        return self._option.set_cpu_thread_num(thread_num)\n\n    def set_ort_graph_opt_level(self, level=-1):\n        \"\"\"Set graph optimization level for ONNX Runtime backend\n\n        :param level: (int)Optimization level, -1 means the default setting\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_ort_graph_opt_level` will be deprecated in v1.2.0, please use `RuntimeOption.graph_optimize_level = 99` instead.\"\n        )\n        self._option.ort_option.graph_optimize_level = level\n\n    def use_paddle_backend(self):\n        \"\"\"Use Paddle Inference backend, support inference Paddle model on CPU/Nvidia GPU.\n        \"\"\"\n        return self._option.use_paddle_backend()\n\n    def use_paddle_infer_backend(self):\n        \"\"\"Wrapper function of use_paddle_backend(), use Paddle Inference backend, support inference Paddle model on CPU/Nvidia GPU.\n        \"\"\"\n        return self.use_paddle_backend()\n\n    def use_poros_backend(self):\n        \"\"\"Use Poros backend, support inference TorchScript model on CPU/Nvidia GPU.\n        \"\"\"\n        return self._option.use_poros_backend()\n\n    def use_ort_backend(self):\n        \"\"\"Use ONNX Runtime backend, support inference Paddle/ONNX model on CPU/Nvidia GPU.\n        \"\"\"\n        return self._option.use_ort_backend()\n\n    def use_tvm_backend(self):\n        \"\"\"Use TVM Runtime backend, support inference TVM model on CPU.\n        \"\"\"\n        return self._option.use_tvm_backend()\n\n    def use_trt_backend(self):\n        \"\"\"Use TensorRT backend, support inference Paddle/ONNX model on Nvidia GPU.\n        \"\"\"\n        return self._option.use_trt_backend()\n\n    def use_openvino_backend(self):\n        \"\"\"Use OpenVINO backend, support inference Paddle/ONNX model on CPU.\n        \"\"\"\n        return self._option.use_openvino_backend()\n\n    def use_lite_backend(self):\n        \"\"\"Use Paddle Lite backend, support inference Paddle model on ARM CPU.\n        \"\"\"\n        return self._option.use_lite_backend()\n\n    def use_paddle_lite_backend(self):\n        \"\"\"Wrapper function of use_lite_backend(), use Paddle Lite backend, support inference Paddle model on ARM CPU.\n        \"\"\"\n        return self.use_lite_backend()\n\n    def set_lite_context_properties(self, context_properties):\n        \"\"\"Set nnadapter context properties for Paddle Lite backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_context_properties` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_context_properties = ...` instead.\"\n        )\n        self._option.paddle_lite_option.nnadapter_context_properties = context_properties\n\n    def set_lite_model_cache_dir(self, model_cache_dir):\n        \"\"\"Set nnadapter model cache dir for Paddle Lite backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_model_cache_dir` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_model_cache_dir = ...` instead.\"\n        )\n\n        self._option.paddle_lite_option.nnadapter_model_cache_dir = model_cache_dir\n\n    def set_lite_dynamic_shape_info(self, dynamic_shape_info):\n        \"\"\" Set nnadapter dynamic shape info for Paddle Lite backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_dynamic_shape_info` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_dynamic_shape_info = ...` instead.\"\n        )\n        self._option.paddle_lite_option.nnadapter_dynamic_shape_info = dynamic_shape_info\n\n    def set_lite_subgraph_partition_path(self, subgraph_partition_path):\n        \"\"\" Set nnadapter subgraph partition path for Paddle Lite backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_subgraph_partition_path` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_subgraph_partition_config_path = ...` instead.\"\n        )\n        self._option.paddle_lite_option.nnadapter_subgraph_partition_config_path = subgraph_partition_path\n\n    def set_lite_subgraph_partition_config_buffer(self,\n                                                  subgraph_partition_buffer):\n        \"\"\" Set nnadapter subgraph partition buffer for Paddle Lite backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_subgraph_partition_buffer` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_subgraph_partition_config_buffer = ...` instead.\"\n        )\n        self._option.paddle_lite_option.nnadapter_subgraph_partition_config_buffer = subgraph_partition_buffer\n\n    def set_lite_mixed_precision_quantization_config_path(\n            self, mixed_precision_quantization_config_path):\n        \"\"\" Set nnadapter mixed precision quantization config path for Paddle Lite backend..\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_mixed_precision_quantization_config_path` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_mixed_precision_quantization_config_path = ...` instead.\"\n        )\n        self._option.paddle_lite_option.nnadapter_mixed_precision_quantization_config_path = mixed_precision_quantization_config_path\n\n    def set_paddle_mkldnn(self, use_mkldnn=True):\n        \"\"\"Enable/Disable MKLDNN while using Paddle Inference backend, mkldnn is enabled by default.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_paddle_mkldnn` will be derepcated in v1.2.0, please use `RuntimeOption.paddle_infer_option.enable_mkldnn = True` instead.\"\n        )\n        self._option.paddle_infer_option.enable_mkldnn = True\n\n    def set_openvino_device(self, name=\"CPU\"):\n        \"\"\"Set device name for OpenVINO, default 'CPU', can also be 'AUTO', 'GPU', 'GPU.1'....\n           This interface is deprecated, please use `RuntimeOption.openvino_option.set_device` instead.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_openvino_device` will be deprecated in v1.2.0, please use `RuntimeOption.openvino_option.set_device` instead.\"\n        )\n        self._option.openvino_option.set_device(name)\n\n    def set_openvino_shape_info(self, shape_info):\n        \"\"\"Set shape information of the models' inputs, used for GPU to fix the shape\n           This interface is deprecated, please use `RuntimeOption.openvino_option.set_shape_info` instead.\n\n        :param shape_info: (dict{str, list of int})Shape information of model's inputs, e.g {\"image\": [1, 3, 640, 640], \"scale_factor\": [1, 2]}\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_openvino_shape_info` will be deprecated in v1.2.0, please use `RuntimeOption.openvino_option.set_shape_info` instead.\"\n        )\n        self._option.openvino_option.set_shape_info(shape_info)\n\n    def set_openvino_cpu_operators(self, operators):\n        \"\"\"While using OpenVINO backend and intel GPU, this interface specifies unsupported operators to run on CPU\n           This interface is deprecated, please use `RuntimeOption.openvino_option.set_cpu_operators` instead.\n\n        :param operators: (list of string)list of operators' name, e.g [\"MulticlasNms\"]\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_openvino_cpu_operators` will be deprecated in v1.2.0, please use `RuntimeOption.openvino_option.set_cpu_operators` instead.\"\n        )\n        self._option.openvino_option.set_cpu_operators(operators)\n\n    def enable_paddle_log_info(self):\n        \"\"\"Enable print out the debug log information while using Paddle Inference backend, the log information is disabled by default.\n        \"\"\"\n        logging.warning(\n            \"RuntimeOption.enable_paddle_log_info` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.enable_log_info = True` instead.\"\n        )\n        self._option.paddle_infer_option.enable_log_info = True\n\n    def disable_paddle_log_info(self):\n        \"\"\"Disable print out the debug log information while using Paddle Inference backend, the log information is disabled by default.\n        \"\"\"\n        logging.warning(\n            \"RuntimeOption.disable_paddle_log_info` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.enable_log_info = False` instead.\"\n        )\n        self._option.paddle_infer_option.enable_log_info = False\n\n    def set_paddle_mkldnn_cache_size(self, cache_size):\n        \"\"\"Set size of shape cache while using Paddle Inference backend with MKLDNN enabled, default will cache all the dynamic shape.\n        \"\"\"\n        logging.warning(\n            \"RuntimeOption.set_paddle_mkldnn_cache_size` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.mkldnn_cache_size = {}` instead.\".\n            format(cache_size))\n        self._option.paddle_infer_option.mkldnn_cache_size = cache_size\n\n    def enable_lite_fp16(self):\n        \"\"\"Enable half precision inference while using Paddle Lite backend on ARM CPU, fp16 is disabled by default.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.enable_lite_fp16` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.enable_fp16 = True` instead.\"\n        )\n        self._option.paddle_lite_option.enable_fp16 = True\n\n    def disable_lite_fp16(self):\n        \"\"\"Disable half precision inference while using Paddle Lite backend on ARM CPU, fp16 is disabled by default.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.disable_lite_fp16` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.enable_fp16 = False` instead.\"\n        )\n        self._option.paddle_lite_option.enable_fp16 = False\n\n    def set_lite_power_mode(self, mode):\n        \"\"\"Set POWER mode while using Paddle Lite backend on ARM CPU.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_powermode` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.power_mode = {}` instead.\".\n            format(mode))\n        self._option.paddle_lite_option.power_mode = mode\n\n    def set_trt_input_shape(self,\n                            tensor_name,\n                            min_shape,\n                            opt_shape=None,\n                            max_shape=None):\n        \"\"\"Set shape range information while using TensorRT backend with loadding a model contains dynamic input shape. While inference with a new input shape out of the set shape range, the tensorrt engine will be rebuilt to expand the shape range information.\n\n        :param tensor_name: (str)Name of input which has dynamic shape\n        :param min_shape: (list of int)Minimum shape of the input, e.g [1, 3, 224, 224]\n        :param opt_shape: (list of int)Optimize shape of the input, this offten set as the most common input shape, if set to None, it will keep same with min_shape\n        :param max_shape: (list of int)Maximum shape of the input, e.g [8, 3, 224, 224], if set to None, it will keep same with the min_shape\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_trt_input_shape` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_shape()` instead.\"\n        )\n        if opt_shape is None and max_shape is None:\n            opt_shape = min_shape\n            max_shape = min_shape\n        else:\n            assert opt_shape is not None and max_shape is not None, \"Set min_shape only, or set min_shape, opt_shape, max_shape both.\"\n        return self._option.trt_option.set_shape(tensor_name, min_shape,\n                                                 opt_shape, max_shape)\n\n    def set_trt_input_data(self,\n                           tensor_name,\n                           min_input_data,\n                           opt_input_data=None,\n                           max_input_data=None):\n        \"\"\"Set input data while using TensorRT backend with loadding a model contains dynamic input shape.\n\n        :param tensor_name: (str)Name of input which has dynamic shape\n        :param min_input_data: (list of int)Input data for Minimum shape of the input.\n        :param opt_input_data: (list of int)Input data for Optimize shape of the input, if set to None, it will keep same with min_input_data\n        :param max_input_data: (list of int)Input data for Maximum shape of the input, if set to None, it will keep same with the min_input_data\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_trt_input_data` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_input_data()` instead.\"\n        )\n        if opt_input_data is None and max_input_data is None:\n            opt_input_data = min_input_data\n            opt_input_data = min_input_data\n        else:\n            assert opt_input_data is not None and max_input_data is not None, \"Set min_input_data only, or set min_input_data, opt_input_data, max_input_data both.\"\n        return self._option.trt_option.set_input_data(\n            tensor_name, min_input_data, opt_input_data, max_input_data)\n\n    def set_trt_cache_file(self, cache_file_path):\n        \"\"\"Set a cache file path while using TensorRT backend. While loading a Paddle/ONNX model with set_trt_cache_file(\"./tensorrt_cache/model.trt\"), if file `./tensorrt_cache/model.trt` exists, it will skip building tensorrt engine and load the cache file directly; if file `./tensorrt_cache/model.trt` doesn't exist, it will building tensorrt engine and save the engine as binary string to the cache file.\n\n        :param cache_file_path: (str)Path of tensorrt cache file\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_trt_cache_file` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.serialize_file = {}` instead.\".\n            format(cache_file_path))\n        self._option.trt_option.serialize_file = cache_file_path\n\n    def enable_trt_fp16(self):\n        \"\"\"Enable half precision inference while using TensorRT backend, notice that not all the Nvidia GPU support FP16, in those cases, will fallback to FP32 inference.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.enable_trt_fp16` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.enable_fp16 = True` instead.\"\n        )\n        self._option.trt_option.enable_fp16 = True\n\n    def disable_trt_fp16(self):\n        \"\"\"Disable half precision inference while suing TensorRT backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.disable_trt_fp16` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.enable_fp16 = False` instead.\"\n        )\n        self._option.trt_option.enable_fp16 = False\n\n    def enable_pinned_memory(self):\n        \"\"\"Enable pinned memory. Pinned memory can be utilized to speedup the data transfer between CPU and GPU. Currently it's only suppurted in TRT backend and Paddle Inference backend.\n        \"\"\"\n        return self._option.enable_pinned_memory()\n\n    def disable_pinned_memory(self):\n        \"\"\"Disable pinned memory.\n        \"\"\"\n        return self._option.disable_pinned_memory()\n\n    def enable_paddle_to_trt(self):\n        \"\"\"While using TensorRT backend, enable_paddle_to_trt() will change to use Paddle Inference backend, and use its integrated TensorRT instead.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.enable_paddle_to_trt` will be deprecated in v1.2.l0, if you want to run tensorrt with Paddle Inference backend, please use the following method, \"\n        )\n        logging.warning(\"    ==============================================\")\n        logging.warning(\"    import fastdeploy as fd\")\n        logging.warning(\"    option = fd.RuntimeOption()\")\n        logging.warning(\"    option.use_gpu(0)\")\n        logging.warning(\"    option.use_paddle_infer_backend()\")\n        logging.warning(\"    option.paddle_infer_option.enable_trt = True\")\n        logging.warning(\"    ==============================================\")\n        self._option.use_paddle_backend()\n        self._option.paddle_infer_option.enable_trt = True\n\n    def set_trt_max_workspace_size(self, trt_max_workspace_size):\n        \"\"\"Set max workspace size while using TensorRT backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_trt_max_workspace_size` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.max_workspace_size = {}` instead.\".\n            format(trt_max_workspace_size))\n        self._option.trt_option.max_workspace_size = trt_max_workspace_size\n\n    def set_trt_max_batch_size(self, trt_max_batch_size):\n        \"\"\"Set max batch size while using TensorRT backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_trt_max_batch_size` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.max_batch_size = {}` instead.\".\n            format(trt_max_batch_size))\n        self._option.trt_option.max_batch_size = trt_max_batch_size\n\n    def enable_paddle_trt_collect_shape(self):\n        \"\"\"Enable collect subgraph shape information while using Paddle Inference with TensorRT\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.enable_paddle_trt_collect_shape` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.collect_trt_shape = True` instead.\"\n        )\n        self._option.paddle_infer_option.collect_trt_shape = True\n\n    def disable_paddle_trt_collect_shape(self):\n        \"\"\"Disable collect subgraph shape information while using Paddle Inference with TensorRT\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.disable_paddle_trt_collect_shape` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.collect_trt_shape = False` instead.\"\n        )\n        self._option.paddle_infer_option.collect_trt_shape = False\n\n    def delete_paddle_backend_pass(self, pass_name):\n        \"\"\"Delete pass by name in paddle backend\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.delete_paddle_backend_pass` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.delete_pass` instead.\"\n        )\n        self._option.paddle_infer_option.delete_pass(pass_name)\n\n    def disable_paddle_trt_ops(self, ops):\n        \"\"\"Disable some ops in paddle trt backend\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.disable_paddle_trt_ops` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.disable_trt_ops()` instead.\"\n        )\n        self._option.disable_trt_ops(ops)\n\n    def use_ipu(self,\n                device_num=1,\n                micro_batch_size=1,\n                enable_pipelining=False,\n                batches_per_step=1):\n        return self._option.use_ipu(device_num, micro_batch_size,\n                                    enable_pipelining, batches_per_step)\n\n    def set_ipu_config(self,\n                       enable_fp16=False,\n                       replica_num=1,\n                       available_memory_proportion=1.0,\n                       enable_half_partial=False):\n        logging.warning(\n            \"`RuntimeOption.set_ipu_config` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.set_ipu_config()` instead.\"\n        )\n        self._option.paddle_infer_option.set_ipu_config(\n            enable_fp16, replica_num, available_memory_proportion,\n            enable_half_partial)\n\n    @property\n    def poros_option(self):\n        \"\"\"Get PorosBackendOption object to configure Poros backend\n\n        :return PorosBackendOption\n        \"\"\"\n        return self._option.poros_option\n\n    @property\n    def paddle_lite_option(self):\n        \"\"\"Get LiteBackendOption object to configure Paddle Lite backend\n\n        :return LiteBackendOption\n        \"\"\"\n        return self._option.paddle_lite_option\n\n    @property\n    def openvino_option(self):\n        \"\"\"Get OpenVINOOption object to configure OpenVINO backend\n\n        :return OpenVINOOption\n        \"\"\"\n        return self._option.openvino_option\n\n    @property\n    def ort_option(self):\n        \"\"\"Get OrtBackendOption object to configure ONNX Runtime backend\n\n        :return OrtBackendOption\n        \"\"\"\n        return self._option.ort_option\n\n    @property\n    def trt_option(self):\n        \"\"\"Get TrtBackendOption object to configure TensorRT backend\n\n        :return TrtBackendOption\n        \"\"\"\n        return self._option.trt_option\n\n    @property\n    def paddle_infer_option(self):\n        \"\"\"Get PaddleBackendOption object to configure Paddle Inference backend\n\n        :return PaddleBackendOption\n        \"\"\"\n        return self._option.paddle_infer_option\n\n    def enable_profiling(self, inclue_h2d_d2h=False, repeat=100, warmup=50):\n        \"\"\"Set the profile mode as 'true'.\n        :param inclue_h2d_d2h Whether to include time of H2D_D2H for time of runtime.\n        :param repeat Repeat times for runtime inference.\n        :param warmup Warmup times for runtime inference.\n        \"\"\"\n        return self._option.enable_profiling(inclue_h2d_d2h, repeat, warmup)\n\n    def disable_profiling(self):\n        \"\"\"Set the profile mode as 'false'.\n        \"\"\"\n        return self._option.disable_profiling()\n\n    def set_external_raw_stream(self, cuda_stream):\n        \"\"\"Set the external raw stream used by fastdeploy runtime.\n        \"\"\"\n        self._option.set_external_raw_stream(cuda_stream)\n\n    def __repr__(self):\n        attrs = dir(self._option)\n        message = \"RuntimeOption(\\n\"\n        for attr in attrs:\n            if attr.startswith(\"__\"):\n                continue\n            if hasattr(getattr(self._option, attr), \"__call__\"):\n                continue\n            message += \"  {} : {}\\t\\n\".format(attr,\n                                              getattr(self._option, attr))\n        message.strip(\"\\n\")\n        message += \")\"\n        return message",
  "def __init__(self, runtime_option):\n        \"\"\"Initialize a FastDeploy Runtime object.\n\n        :param runtime_option: (fastdeploy.RuntimeOption)Options for FastDeploy Runtime\n        \"\"\"\n\n        self._runtime = C.Runtime()\n        self.runtime_option = runtime_option\n        assert self._runtime.init(\n            self.runtime_option._option), \"Initialize Runtime Failed!\"",
  "def forward(self, *inputs):\n        \"\"\"[Only for Poros backend] Inference with input data for poros\n\n        :param data: (list[str : numpy.ndarray])The input data list\n        :return list of numpy.ndarray\n        \"\"\"\n        if self.runtime_option._option.model_format != ModelFormat.TORCHSCRIPT:\n            raise Exception(\n                \"The forward function is only used for Poros backend, please call infer function\"\n            )\n        inputs_dict = dict()\n        for i in range(len(inputs)):\n            inputs_dict[\"x\" + str(i)] = inputs[i]\n        return self.infer(inputs_dict)",
  "def infer(self, data):\n        \"\"\"Inference with input data.\n\n        :param data: (dict[str : numpy.ndarray])The input data dict, key value must keep same with the loaded model\n        :return list of numpy.ndarray\n        \"\"\"\n        assert isinstance(data, dict) or isinstance(\n            data, list), \"The input data should be type of dict or list.\"\n        if isinstance(data, dict):\n            for k, v in data.items():\n                if isinstance(v, np.ndarray) and not v.data.contiguous:\n                    data[k] = np.ascontiguousarray(data[k])\n\n        return self._runtime.infer(data)",
  "def bind_input_tensor(self, name, fdtensor):\n        \"\"\"Bind FDTensor by name, no copy and share input memory\n\n        :param name: (str)The name of input data.\n        :param fdtensor: (fastdeploy.FDTensor)The input FDTensor.\n        \"\"\"\n        self._runtime.bind_input_tensor(name, fdtensor)",
  "def bind_output_tensor(self, name, fdtensor):\n        \"\"\"Bind FDTensor by name, no copy and share output memory\n\n        :param name: (str)The name of output data.\n        :param fdtensor: (fastdeploy.FDTensor)The output FDTensor.\n        \"\"\"\n        self._runtime.bind_output_tensor(name, fdtensor)",
  "def zero_copy_infer(self):\n        \"\"\"No params inference the model.\n\n        the input and output data need to pass through the bind_input_tensor and get_output_tensor interfaces.\n        \"\"\"\n        self._runtime.infer()",
  "def get_output_tensor(self, name):\n        \"\"\"Get output FDTensor by name, no copy and share backend output memory\n\n        :param name: (str)The name of output data.\n        :return fastdeploy.FDTensor\n        \"\"\"\n        return self._runtime.get_output_tensor(name)",
  "def compile(self, warm_datas):\n        \"\"\"[Only for Poros backend] compile with prewarm data for poros\n\n        :param data: (list[str : numpy.ndarray])The prewarm data list\n        :return TorchScript Model\n        \"\"\"\n        if self.runtime_option._option.model_format != ModelFormat.TORCHSCRIPT:\n            raise Exception(\n                \"The compile function is only used for Poros backend, please call infer function\"\n            )\n        assert isinstance(warm_datas,\n                          list), \"The prewarm data should be type of list.\"\n        for i in range(len(warm_datas)):\n            warm_data = warm_datas[i]\n            if isinstance(warm_data[0], np.ndarray):\n                warm_data = list(data for data in warm_data)\n            else:\n                warm_data = list(data.numpy() for data in warm_data)\n            warm_datas[i] = warm_data\n        return self._runtime.compile(warm_datas, self.runtime_option._option)",
  "def num_inputs(self):\n        \"\"\"Get number of inputs of the loaded model.\n        \"\"\"\n        return self._runtime.num_inputs()",
  "def num_outputs(self):\n        \"\"\"Get number of outputs of the loaded model.\n        \"\"\"\n        return self._runtime.num_outputs()",
  "def get_input_info(self, index):\n        \"\"\"Get input information of the loaded model.\n\n        :param index: (int)Index of the input\n        :return fastdeploy.TensorInfo\n        \"\"\"\n        assert isinstance(\n            index, int), \"The input parameter index should be type of int.\"\n        assert index < self.num_inputs(\n        ), \"The input parameter index:{} should less than number of inputs:{}.\".format(\n            index, self.num_inputs)\n        return self._runtime.get_input_info(index)",
  "def get_output_info(self, index):\n        \"\"\"Get output information of the loaded model.\n\n        :param index: (int)Index of the output\n        :return fastdeploy.TensorInfo\n        \"\"\"\n        assert isinstance(\n            index, int), \"The input parameter index should be type of int.\"\n        assert index < self.num_outputs(\n        ), \"The input parameter index:{} should less than number of outputs:{}.\".format(\n            index, self.num_outputs)\n        return self._runtime.get_output_info(index)",
  "def get_profile_time(self):\n        \"\"\"Get profile time of Runtime after the profile process is done.\n        \"\"\"\n        return self._runtime.get_profile_time()",
  "def __init__(self):\n        \"\"\"Initialize a FastDeploy RuntimeOption object.\n        \"\"\"\n\n        self._option = C.RuntimeOption()",
  "def set_model_path(self,\n                       model_path,\n                       params_path=\"\",\n                       model_format=ModelFormat.PADDLE):\n        \"\"\"Set path of model file and parameters file\n\n        :param model_path: (str)Path of model file\n        :param params_path: (str)Path of parameters file\n        :param model_format: (ModelFormat)Format of model, support ModelFormat.PADDLE/ModelFormat.ONNX/ModelFormat.TORCHSCRIPT\n        \"\"\"\n        return self._option.set_model_path(model_path, params_path,\n                                           model_format)",
  "def set_model_buffer(self,\n                         model_buffer,\n                         params_buffer=\"\",\n                         model_format=ModelFormat.PADDLE):\n        \"\"\"Specify the memory buffer of model and parameter. Used when model and params are loaded directly from memory\n        :param model_buffer: (bytes)The memory buffer of model\n        :param params_buffer: (bytes)The memory buffer of the parameters\n        :param model_format: (ModelFormat)Format of model, support ModelFormat.PADDLE/ModelFormat.ONNX/ModelFormat.TORCHSCRIPT\n        \"\"\"\n        return self._option.set_model_buffer(model_buffer, params_buffer,\n                                             model_format)",
  "def set_encryption_key(self, encryption_key):\n        \"\"\"When loading encrypted model, encryption_key is required to decrypte model\n        :param encryption_key: (str)The key for decrypting model\n        \"\"\"\n        return self._option.set_encryption_key(encryption_key)",
  "def use_gpu(self, device_id=0):\n        \"\"\"Inference with Nvidia GPU\n\n        :param device_id: (int)The index of GPU will be used for inference, default 0\n        \"\"\"\n        if not C.is_built_with_gpu():\n            logging.warning(\n                \"The installed fastdeploy-python package is not built with GPU, will force to use CPU. To use GPU, following the commands to install fastdeploy-gpu-python.\"\n            )\n            logging.warning(\n                \"    ================= Install GPU FastDeploy===============\")\n            logging.warning(\"    python -m pip uninstall fastdeploy-python\")\n            logging.warning(\n                \"    python -m pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\"\n            )\n            return\n        return self._option.use_gpu(device_id)",
  "def use_kunlunxin(self,\n                      device_id=0,\n                      l3_workspace_size=16 * 1024 * 1024,\n                      locked=False,\n                      autotune=True,\n                      autotune_file=\"\",\n                      precision=\"int16\",\n                      adaptive_seqlen=False,\n                      enable_multi_stream=False,\n                      gm_default_size=0):\n        \"\"\"Inference with KunlunXin XPU\n\n        :param device_id: (int)The index of KunlunXin XPU will be used for inference, default 0\n        :param l3_workspace_size: (int)The size of the video memory allocated by the l3 cache, the maximum is 16M, default 16M\n        :param locked: (bool)Whether the allocated L3 cache can be locked. If false, it means that the L3 cache is not locked,\n                        and the allocated L3 cache can be shared by multiple models, and multiple models\n        :param autotune: (bool)Whether to autotune the conv operator in the model.\n                        If true, when the conv operator of a certain dimension is executed for the first time,\n                        it will automatically search for a better algorithm to improve the performance of subsequent conv operators of the same dimension.\n        :param autotune_file: (str)Specify the path of the autotune file. If autotune_file is specified,\n                        the algorithm specified in the file will be used and autotune will not be performed again.\n        :param precision: (str)Calculation accuracy of multi_encoder\n        :param adaptive_seqlen: (bool)adaptive_seqlen Is the input of multi_encoder variable length\n        :param enable_multi_stream: (bool)Whether to enable the multi stream of KunlunXin XPU.\n        :param gm_default_size The default size of context global memory of KunlunXin XPU.\n        \"\"\"\n        return self._option.use_kunlunxin(\n            device_id, l3_workspace_size, locked, autotune, autotune_file,\n            precision, adaptive_seqlen, enable_multi_stream, gm_default_size)",
  "def use_cpu(self):\n        \"\"\"Inference with CPU\n        \"\"\"\n        return self._option.use_cpu()",
  "def use_rknpu2(self,\n                   rknpu2_name=C.CpuName.RK356X,\n                   rknpu2_core=C.CoreMask.RKNN_NPU_CORE_AUTO):\n        return self._option.use_rknpu2(rknpu2_name, rknpu2_core)",
  "def use_sophgo(self):\n        \"\"\"Inference with SOPHGO TPU\n        \"\"\"\n        return self._option.use_sophgo()",
  "def use_ascend(self):\n        \"\"\"Inference with Huawei Ascend NPU\n        \"\"\"\n        return self._option.use_ascend()",
  "def disable_valid_backend_check(self):\n        \"\"\" Disable checking validity of backend during inference\n        \"\"\"\n        return self._option.disable_valid_backend_check()",
  "def enable_valid_backend_check(self):\n        \"\"\"Enable checking validity of backend during inference\n        \"\"\"\n        return self._option.enable_valid_backend_check()",
  "def set_cpu_thread_num(self, thread_num=-1):\n        \"\"\"Set number of threads if inference with CPU\n\n        :param thread_num: (int)Number of threads, if not positive, means the number of threads is decided by the backend, default -1\n        \"\"\"\n        return self._option.set_cpu_thread_num(thread_num)",
  "def set_ort_graph_opt_level(self, level=-1):\n        \"\"\"Set graph optimization level for ONNX Runtime backend\n\n        :param level: (int)Optimization level, -1 means the default setting\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_ort_graph_opt_level` will be deprecated in v1.2.0, please use `RuntimeOption.graph_optimize_level = 99` instead.\"\n        )\n        self._option.ort_option.graph_optimize_level = level",
  "def use_paddle_backend(self):\n        \"\"\"Use Paddle Inference backend, support inference Paddle model on CPU/Nvidia GPU.\n        \"\"\"\n        return self._option.use_paddle_backend()",
  "def use_paddle_infer_backend(self):\n        \"\"\"Wrapper function of use_paddle_backend(), use Paddle Inference backend, support inference Paddle model on CPU/Nvidia GPU.\n        \"\"\"\n        return self.use_paddle_backend()",
  "def use_poros_backend(self):\n        \"\"\"Use Poros backend, support inference TorchScript model on CPU/Nvidia GPU.\n        \"\"\"\n        return self._option.use_poros_backend()",
  "def use_ort_backend(self):\n        \"\"\"Use ONNX Runtime backend, support inference Paddle/ONNX model on CPU/Nvidia GPU.\n        \"\"\"\n        return self._option.use_ort_backend()",
  "def use_tvm_backend(self):\n        \"\"\"Use TVM Runtime backend, support inference TVM model on CPU.\n        \"\"\"\n        return self._option.use_tvm_backend()",
  "def use_trt_backend(self):\n        \"\"\"Use TensorRT backend, support inference Paddle/ONNX model on Nvidia GPU.\n        \"\"\"\n        return self._option.use_trt_backend()",
  "def use_openvino_backend(self):\n        \"\"\"Use OpenVINO backend, support inference Paddle/ONNX model on CPU.\n        \"\"\"\n        return self._option.use_openvino_backend()",
  "def use_lite_backend(self):\n        \"\"\"Use Paddle Lite backend, support inference Paddle model on ARM CPU.\n        \"\"\"\n        return self._option.use_lite_backend()",
  "def use_paddle_lite_backend(self):\n        \"\"\"Wrapper function of use_lite_backend(), use Paddle Lite backend, support inference Paddle model on ARM CPU.\n        \"\"\"\n        return self.use_lite_backend()",
  "def set_lite_context_properties(self, context_properties):\n        \"\"\"Set nnadapter context properties for Paddle Lite backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_context_properties` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_context_properties = ...` instead.\"\n        )\n        self._option.paddle_lite_option.nnadapter_context_properties = context_properties",
  "def set_lite_model_cache_dir(self, model_cache_dir):\n        \"\"\"Set nnadapter model cache dir for Paddle Lite backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_model_cache_dir` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_model_cache_dir = ...` instead.\"\n        )\n\n        self._option.paddle_lite_option.nnadapter_model_cache_dir = model_cache_dir",
  "def set_lite_dynamic_shape_info(self, dynamic_shape_info):\n        \"\"\" Set nnadapter dynamic shape info for Paddle Lite backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_dynamic_shape_info` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_dynamic_shape_info = ...` instead.\"\n        )\n        self._option.paddle_lite_option.nnadapter_dynamic_shape_info = dynamic_shape_info",
  "def set_lite_subgraph_partition_path(self, subgraph_partition_path):\n        \"\"\" Set nnadapter subgraph partition path for Paddle Lite backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_subgraph_partition_path` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_subgraph_partition_config_path = ...` instead.\"\n        )\n        self._option.paddle_lite_option.nnadapter_subgraph_partition_config_path = subgraph_partition_path",
  "def set_lite_subgraph_partition_config_buffer(self,\n                                                  subgraph_partition_buffer):\n        \"\"\" Set nnadapter subgraph partition buffer for Paddle Lite backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_subgraph_partition_buffer` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_subgraph_partition_config_buffer = ...` instead.\"\n        )\n        self._option.paddle_lite_option.nnadapter_subgraph_partition_config_buffer = subgraph_partition_buffer",
  "def set_lite_mixed_precision_quantization_config_path(\n            self, mixed_precision_quantization_config_path):\n        \"\"\" Set nnadapter mixed precision quantization config path for Paddle Lite backend..\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_mixed_precision_quantization_config_path` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.nnadapter_mixed_precision_quantization_config_path = ...` instead.\"\n        )\n        self._option.paddle_lite_option.nnadapter_mixed_precision_quantization_config_path = mixed_precision_quantization_config_path",
  "def set_paddle_mkldnn(self, use_mkldnn=True):\n        \"\"\"Enable/Disable MKLDNN while using Paddle Inference backend, mkldnn is enabled by default.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_paddle_mkldnn` will be derepcated in v1.2.0, please use `RuntimeOption.paddle_infer_option.enable_mkldnn = True` instead.\"\n        )\n        self._option.paddle_infer_option.enable_mkldnn = True",
  "def set_openvino_device(self, name=\"CPU\"):\n        \"\"\"Set device name for OpenVINO, default 'CPU', can also be 'AUTO', 'GPU', 'GPU.1'....\n           This interface is deprecated, please use `RuntimeOption.openvino_option.set_device` instead.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_openvino_device` will be deprecated in v1.2.0, please use `RuntimeOption.openvino_option.set_device` instead.\"\n        )\n        self._option.openvino_option.set_device(name)",
  "def set_openvino_shape_info(self, shape_info):\n        \"\"\"Set shape information of the models' inputs, used for GPU to fix the shape\n           This interface is deprecated, please use `RuntimeOption.openvino_option.set_shape_info` instead.\n\n        :param shape_info: (dict{str, list of int})Shape information of model's inputs, e.g {\"image\": [1, 3, 640, 640], \"scale_factor\": [1, 2]}\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_openvino_shape_info` will be deprecated in v1.2.0, please use `RuntimeOption.openvino_option.set_shape_info` instead.\"\n        )\n        self._option.openvino_option.set_shape_info(shape_info)",
  "def set_openvino_cpu_operators(self, operators):\n        \"\"\"While using OpenVINO backend and intel GPU, this interface specifies unsupported operators to run on CPU\n           This interface is deprecated, please use `RuntimeOption.openvino_option.set_cpu_operators` instead.\n\n        :param operators: (list of string)list of operators' name, e.g [\"MulticlasNms\"]\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_openvino_cpu_operators` will be deprecated in v1.2.0, please use `RuntimeOption.openvino_option.set_cpu_operators` instead.\"\n        )\n        self._option.openvino_option.set_cpu_operators(operators)",
  "def enable_paddle_log_info(self):\n        \"\"\"Enable print out the debug log information while using Paddle Inference backend, the log information is disabled by default.\n        \"\"\"\n        logging.warning(\n            \"RuntimeOption.enable_paddle_log_info` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.enable_log_info = True` instead.\"\n        )\n        self._option.paddle_infer_option.enable_log_info = True",
  "def disable_paddle_log_info(self):\n        \"\"\"Disable print out the debug log information while using Paddle Inference backend, the log information is disabled by default.\n        \"\"\"\n        logging.warning(\n            \"RuntimeOption.disable_paddle_log_info` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.enable_log_info = False` instead.\"\n        )\n        self._option.paddle_infer_option.enable_log_info = False",
  "def set_paddle_mkldnn_cache_size(self, cache_size):\n        \"\"\"Set size of shape cache while using Paddle Inference backend with MKLDNN enabled, default will cache all the dynamic shape.\n        \"\"\"\n        logging.warning(\n            \"RuntimeOption.set_paddle_mkldnn_cache_size` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.mkldnn_cache_size = {}` instead.\".\n            format(cache_size))\n        self._option.paddle_infer_option.mkldnn_cache_size = cache_size",
  "def enable_lite_fp16(self):\n        \"\"\"Enable half precision inference while using Paddle Lite backend on ARM CPU, fp16 is disabled by default.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.enable_lite_fp16` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.enable_fp16 = True` instead.\"\n        )\n        self._option.paddle_lite_option.enable_fp16 = True",
  "def disable_lite_fp16(self):\n        \"\"\"Disable half precision inference while using Paddle Lite backend on ARM CPU, fp16 is disabled by default.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.disable_lite_fp16` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.enable_fp16 = False` instead.\"\n        )\n        self._option.paddle_lite_option.enable_fp16 = False",
  "def set_lite_power_mode(self, mode):\n        \"\"\"Set POWER mode while using Paddle Lite backend on ARM CPU.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_lite_powermode` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_lite_option.power_mode = {}` instead.\".\n            format(mode))\n        self._option.paddle_lite_option.power_mode = mode",
  "def set_trt_input_shape(self,\n                            tensor_name,\n                            min_shape,\n                            opt_shape=None,\n                            max_shape=None):\n        \"\"\"Set shape range information while using TensorRT backend with loadding a model contains dynamic input shape. While inference with a new input shape out of the set shape range, the tensorrt engine will be rebuilt to expand the shape range information.\n\n        :param tensor_name: (str)Name of input which has dynamic shape\n        :param min_shape: (list of int)Minimum shape of the input, e.g [1, 3, 224, 224]\n        :param opt_shape: (list of int)Optimize shape of the input, this offten set as the most common input shape, if set to None, it will keep same with min_shape\n        :param max_shape: (list of int)Maximum shape of the input, e.g [8, 3, 224, 224], if set to None, it will keep same with the min_shape\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_trt_input_shape` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_shape()` instead.\"\n        )\n        if opt_shape is None and max_shape is None:\n            opt_shape = min_shape\n            max_shape = min_shape\n        else:\n            assert opt_shape is not None and max_shape is not None, \"Set min_shape only, or set min_shape, opt_shape, max_shape both.\"\n        return self._option.trt_option.set_shape(tensor_name, min_shape,\n                                                 opt_shape, max_shape)",
  "def set_trt_input_data(self,\n                           tensor_name,\n                           min_input_data,\n                           opt_input_data=None,\n                           max_input_data=None):\n        \"\"\"Set input data while using TensorRT backend with loadding a model contains dynamic input shape.\n\n        :param tensor_name: (str)Name of input which has dynamic shape\n        :param min_input_data: (list of int)Input data for Minimum shape of the input.\n        :param opt_input_data: (list of int)Input data for Optimize shape of the input, if set to None, it will keep same with min_input_data\n        :param max_input_data: (list of int)Input data for Maximum shape of the input, if set to None, it will keep same with the min_input_data\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_trt_input_data` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_input_data()` instead.\"\n        )\n        if opt_input_data is None and max_input_data is None:\n            opt_input_data = min_input_data\n            opt_input_data = min_input_data\n        else:\n            assert opt_input_data is not None and max_input_data is not None, \"Set min_input_data only, or set min_input_data, opt_input_data, max_input_data both.\"\n        return self._option.trt_option.set_input_data(\n            tensor_name, min_input_data, opt_input_data, max_input_data)",
  "def set_trt_cache_file(self, cache_file_path):\n        \"\"\"Set a cache file path while using TensorRT backend. While loading a Paddle/ONNX model with set_trt_cache_file(\"./tensorrt_cache/model.trt\"), if file `./tensorrt_cache/model.trt` exists, it will skip building tensorrt engine and load the cache file directly; if file `./tensorrt_cache/model.trt` doesn't exist, it will building tensorrt engine and save the engine as binary string to the cache file.\n\n        :param cache_file_path: (str)Path of tensorrt cache file\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_trt_cache_file` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.serialize_file = {}` instead.\".\n            format(cache_file_path))\n        self._option.trt_option.serialize_file = cache_file_path",
  "def enable_trt_fp16(self):\n        \"\"\"Enable half precision inference while using TensorRT backend, notice that not all the Nvidia GPU support FP16, in those cases, will fallback to FP32 inference.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.enable_trt_fp16` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.enable_fp16 = True` instead.\"\n        )\n        self._option.trt_option.enable_fp16 = True",
  "def disable_trt_fp16(self):\n        \"\"\"Disable half precision inference while suing TensorRT backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.disable_trt_fp16` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.enable_fp16 = False` instead.\"\n        )\n        self._option.trt_option.enable_fp16 = False",
  "def enable_pinned_memory(self):\n        \"\"\"Enable pinned memory. Pinned memory can be utilized to speedup the data transfer between CPU and GPU. Currently it's only suppurted in TRT backend and Paddle Inference backend.\n        \"\"\"\n        return self._option.enable_pinned_memory()",
  "def disable_pinned_memory(self):\n        \"\"\"Disable pinned memory.\n        \"\"\"\n        return self._option.disable_pinned_memory()",
  "def enable_paddle_to_trt(self):\n        \"\"\"While using TensorRT backend, enable_paddle_to_trt() will change to use Paddle Inference backend, and use its integrated TensorRT instead.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.enable_paddle_to_trt` will be deprecated in v1.2.l0, if you want to run tensorrt with Paddle Inference backend, please use the following method, \"\n        )\n        logging.warning(\"    ==============================================\")\n        logging.warning(\"    import fastdeploy as fd\")\n        logging.warning(\"    option = fd.RuntimeOption()\")\n        logging.warning(\"    option.use_gpu(0)\")\n        logging.warning(\"    option.use_paddle_infer_backend()\")\n        logging.warning(\"    option.paddle_infer_option.enable_trt = True\")\n        logging.warning(\"    ==============================================\")\n        self._option.use_paddle_backend()\n        self._option.paddle_infer_option.enable_trt = True",
  "def set_trt_max_workspace_size(self, trt_max_workspace_size):\n        \"\"\"Set max workspace size while using TensorRT backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_trt_max_workspace_size` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.max_workspace_size = {}` instead.\".\n            format(trt_max_workspace_size))\n        self._option.trt_option.max_workspace_size = trt_max_workspace_size",
  "def set_trt_max_batch_size(self, trt_max_batch_size):\n        \"\"\"Set max batch size while using TensorRT backend.\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.set_trt_max_batch_size` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.max_batch_size = {}` instead.\".\n            format(trt_max_batch_size))\n        self._option.trt_option.max_batch_size = trt_max_batch_size",
  "def enable_paddle_trt_collect_shape(self):\n        \"\"\"Enable collect subgraph shape information while using Paddle Inference with TensorRT\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.enable_paddle_trt_collect_shape` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.collect_trt_shape = True` instead.\"\n        )\n        self._option.paddle_infer_option.collect_trt_shape = True",
  "def disable_paddle_trt_collect_shape(self):\n        \"\"\"Disable collect subgraph shape information while using Paddle Inference with TensorRT\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.disable_paddle_trt_collect_shape` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.collect_trt_shape = False` instead.\"\n        )\n        self._option.paddle_infer_option.collect_trt_shape = False",
  "def delete_paddle_backend_pass(self, pass_name):\n        \"\"\"Delete pass by name in paddle backend\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.delete_paddle_backend_pass` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.delete_pass` instead.\"\n        )\n        self._option.paddle_infer_option.delete_pass(pass_name)",
  "def disable_paddle_trt_ops(self, ops):\n        \"\"\"Disable some ops in paddle trt backend\n        \"\"\"\n        logging.warning(\n            \"`RuntimeOption.disable_paddle_trt_ops` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.disable_trt_ops()` instead.\"\n        )\n        self._option.disable_trt_ops(ops)",
  "def use_ipu(self,\n                device_num=1,\n                micro_batch_size=1,\n                enable_pipelining=False,\n                batches_per_step=1):\n        return self._option.use_ipu(device_num, micro_batch_size,\n                                    enable_pipelining, batches_per_step)",
  "def set_ipu_config(self,\n                       enable_fp16=False,\n                       replica_num=1,\n                       available_memory_proportion=1.0,\n                       enable_half_partial=False):\n        logging.warning(\n            \"`RuntimeOption.set_ipu_config` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.set_ipu_config()` instead.\"\n        )\n        self._option.paddle_infer_option.set_ipu_config(\n            enable_fp16, replica_num, available_memory_proportion,\n            enable_half_partial)",
  "def poros_option(self):\n        \"\"\"Get PorosBackendOption object to configure Poros backend\n\n        :return PorosBackendOption\n        \"\"\"\n        return self._option.poros_option",
  "def paddle_lite_option(self):\n        \"\"\"Get LiteBackendOption object to configure Paddle Lite backend\n\n        :return LiteBackendOption\n        \"\"\"\n        return self._option.paddle_lite_option",
  "def openvino_option(self):\n        \"\"\"Get OpenVINOOption object to configure OpenVINO backend\n\n        :return OpenVINOOption\n        \"\"\"\n        return self._option.openvino_option",
  "def ort_option(self):\n        \"\"\"Get OrtBackendOption object to configure ONNX Runtime backend\n\n        :return OrtBackendOption\n        \"\"\"\n        return self._option.ort_option",
  "def trt_option(self):\n        \"\"\"Get TrtBackendOption object to configure TensorRT backend\n\n        :return TrtBackendOption\n        \"\"\"\n        return self._option.trt_option",
  "def paddle_infer_option(self):\n        \"\"\"Get PaddleBackendOption object to configure Paddle Inference backend\n\n        :return PaddleBackendOption\n        \"\"\"\n        return self._option.paddle_infer_option",
  "def enable_profiling(self, inclue_h2d_d2h=False, repeat=100, warmup=50):\n        \"\"\"Set the profile mode as 'true'.\n        :param inclue_h2d_d2h Whether to include time of H2D_D2H for time of runtime.\n        :param repeat Repeat times for runtime inference.\n        :param warmup Warmup times for runtime inference.\n        \"\"\"\n        return self._option.enable_profiling(inclue_h2d_d2h, repeat, warmup)",
  "def disable_profiling(self):\n        \"\"\"Set the profile mode as 'false'.\n        \"\"\"\n        return self._option.disable_profiling()",
  "def set_external_raw_stream(self, cuda_stream):\n        \"\"\"Set the external raw stream used by fastdeploy runtime.\n        \"\"\"\n        self._option.set_external_raw_stream(cuda_stream)",
  "def __repr__(self):\n        attrs = dir(self._option)\n        message = \"RuntimeOption(\\n\"\n        for attr in attrs:\n            if attr.startswith(\"__\"):\n                continue\n            if hasattr(getattr(self._option, attr), \"__call__\"):\n                continue\n            message += \"  {} : {}\\t\\n\".format(attr,\n                                              getattr(self._option, attr))\n        message.strip(\"\\n\")\n        message += \")\"\n        return message",
  "def get_paddle_version():\n    paddle_version = \"\"\n    try:\n        import pkg_resources\n        paddle_version = pkg_resources.require(\"paddlepaddle-gpu\")[\n            0].version.split(\".post\")[0]\n    except:\n        try:\n            paddle_version = pkg_resources.require(\"paddlepaddle\")[\n                0].version.split(\".post\")[0]\n        except:\n            pass\n    return paddle_version",
  "def should_import_paddle():\n    if (\"paddle2.4\" in extra_version_info) or (\"post24\" in extra_version_info):\n        paddle_version = get_paddle_version()\n        if paddle_version != \"\" and paddle_version <= '2.4.2' and paddle_version != \"0.0.0\":\n            return True\n    return False",
  "def should_set_tensorrt():\n    if with_gpu == 'ON' and enable_paddle_backend == 'ON' and enable_trt_backend == 'ON':\n        return True\n    return False",
  "def tensorrt_is_avaliable():\n    # Note(qiuyanjun): Only support linux now.\n    found_trt_lib = False\n    if ('linux' in sys_platform) and ('LD_LIBRARY_PATH' in os.environ.keys()):\n        for lib_path in os.environ['LD_LIBRARY_PATH'].split(':'):\n            if os.path.exists(os.path.join(lib_path, 'libnvinfer.so')):\n                found_trt_lib = True\n                break\n    return found_trt_lib",
  "def set_logger(enable_info=True, enable_warning=True):\n    \"\"\"Set behaviour of logger while using FastDeploy\n\n    :param enable_info: (boolean)Whether to print out log level of INFO\n    :param enable_warning: (boolean)Whether to print out log level of WARNING, recommend to set to True\n    \"\"\"\n    from .c_lib_wrap import set_logger\n    set_logger(enable_info, enable_warning)",
  "class SchemaLanguage(object):\n    ZH = 0\n    EN = 1",
  "class SchemaNode(object):\n    def __init__(self, name, children=[]):\n        schema_node_children = []\n        if isinstance(children, str):\n            children = [children]\n        for child in children:\n            if isinstance(child, str):\n                schema_node_children += [C.text.SchemaNode(child, [])]\n            elif isinstance(child, dict):\n                for key, val in child.items():\n                    schema_node_child = SchemaNode(key, val)\n                    schema_node_children += [schema_node_child._schema_node]\n            else:\n                assert \"The type of child of SchemaNode should be str or dict.\"\n        self._schema_node = C.text.SchemaNode(name, schema_node_children)\n        self._schema_node_children = schema_node_children",
  "class UIEModel(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 vocab_file,\n                 position_prob=0.5,\n                 max_length=128,\n                 schema=[],\n                 batch_size=64,\n                 runtime_option=RuntimeOption(),\n                 model_format=ModelFormat.PADDLE,\n                 schema_language=SchemaLanguage.ZH):\n        if isinstance(schema, list):\n            schema = SchemaNode(\"\", schema)._schema_node_children\n        elif isinstance(schema, dict):\n            schema_tmp = []\n            for key, val in schema.items():\n                schema_tmp += [SchemaNode(key, val)._schema_node]\n            schema = schema_tmp\n        else:\n            assert \"The type of schema should be list or dict.\"\n        schema_language = C.text.SchemaLanguage(schema_language)\n        self._model = C.text.UIEModel(model_file, params_file, vocab_file,\n                                      position_prob, max_length, schema,\n                                      batch_size, runtime_option._option,\n                                      model_format, schema_language)\n        assert self.initialized, \"UIEModel initialize failed.\"\n\n    def set_schema(self, schema):\n        if isinstance(schema, list):\n            schema = SchemaNode(\"\", schema)._schema_node_children\n        elif isinstance(schema, dict):\n            schema_tmp = []\n            for key, val in schema.items():\n                schema_tmp += [SchemaNode(key, val)._schema_node]\n            schema = schema_tmp\n        self._model.set_schema(schema)\n\n    def predict(self, texts, return_dict=False):\n        results = self._model.predict(texts)\n        if not return_dict:\n            return results\n        new_results = []\n        for result in results:\n            uie_result = dict()\n            for key, uie_results in result.items():\n                uie_result[key] = list()\n                for uie_res in uie_results:\n                    uie_result[key].append(uie_res.get_dict())\n            new_results += [uie_result]\n        return new_results",
  "def __init__(self, name, children=[]):\n        schema_node_children = []\n        if isinstance(children, str):\n            children = [children]\n        for child in children:\n            if isinstance(child, str):\n                schema_node_children += [C.text.SchemaNode(child, [])]\n            elif isinstance(child, dict):\n                for key, val in child.items():\n                    schema_node_child = SchemaNode(key, val)\n                    schema_node_children += [schema_node_child._schema_node]\n            else:\n                assert \"The type of child of SchemaNode should be str or dict.\"\n        self._schema_node = C.text.SchemaNode(name, schema_node_children)\n        self._schema_node_children = schema_node_children",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 vocab_file,\n                 position_prob=0.5,\n                 max_length=128,\n                 schema=[],\n                 batch_size=64,\n                 runtime_option=RuntimeOption(),\n                 model_format=ModelFormat.PADDLE,\n                 schema_language=SchemaLanguage.ZH):\n        if isinstance(schema, list):\n            schema = SchemaNode(\"\", schema)._schema_node_children\n        elif isinstance(schema, dict):\n            schema_tmp = []\n            for key, val in schema.items():\n                schema_tmp += [SchemaNode(key, val)._schema_node]\n            schema = schema_tmp\n        else:\n            assert \"The type of schema should be list or dict.\"\n        schema_language = C.text.SchemaLanguage(schema_language)\n        self._model = C.text.UIEModel(model_file, params_file, vocab_file,\n                                      position_prob, max_length, schema,\n                                      batch_size, runtime_option._option,\n                                      model_format, schema_language)\n        assert self.initialized, \"UIEModel initialize failed.\"",
  "def set_schema(self, schema):\n        if isinstance(schema, list):\n            schema = SchemaNode(\"\", schema)._schema_node_children\n        elif isinstance(schema, dict):\n            schema_tmp = []\n            for key, val in schema.items():\n                schema_tmp += [SchemaNode(key, val)._schema_node]\n            schema = schema_tmp\n        self._model.set_schema(schema)",
  "def predict(self, texts, return_dict=False):\n        results = self._model.predict(texts)\n        if not return_dict:\n            return results\n        new_results = []\n        for result in results:\n            uie_result = dict()\n            for key, uie_results in result.items():\n                uie_result[key] = list()\n                for uie_res in uie_results:\n                    uie_result[key].append(uie_res.get_dict())\n            new_results += [uie_result]\n        return new_results",
  "def mask_to_json(result):\n    r_json = {\n        \"data\": result.data,\n        \"shape\": result.shape,\n    }\n    return json.dumps(r_json)",
  "def detection_to_json(result):\n    masks = []\n    for mask in result.masks:\n        masks.append(mask_to_json(mask))\n    r_json = {\n        \"boxes\": result.boxes,\n        \"scores\": result.scores,\n        \"label_ids\": result.label_ids,\n        \"masks\": masks,\n        \"contain_masks\": result.contain_masks\n    }\n    return json.dumps(r_json)",
  "def perception_to_json(result):\n    r_json = {\n        \"scores\": result.scores,\n        \"label_ids\": result.label_ids,\n        \"boxes\": result.boxes,\n        \"center\": result.center,\n        \"observation_angle\": result.observation_angle,\n        \"yaw_angle\": result.yaw_angle,\n        \"velocity\": result.velocity\n    }\n    return json.dumps(r_json)",
  "def classify_to_json(result):\n    r_json = {\n        \"label_ids\": result.label_ids,\n        \"scores\": result.scores,\n    }\n    return json.dumps(r_json)",
  "def keypoint_to_json(result):\n    r_json = {\n        \"keypoints\": result.keypoints,\n        \"scores\": result.scores,\n        \"num_joints\": result.num_joints,\n    }\n    return json.dumps(r_json)",
  "def ocr_to_json(result):\n    r_json = {\n        \"boxes\": result.boxes,\n        \"text\": result.text,\n        \"rec_scores\": result.rec_scores,\n        \"cls_scores\": result.cls_scores,\n        \"cls_labels\": result.cls_labels,\n    }\n    return json.dumps(r_json)",
  "def mot_to_json(result):\n    r_json = {\n        \"boxes\": result.boxes,\n        \"ids\": result.ids,\n        \"scores\": result.scores,\n        \"class_ids\": result.class_ids,\n    }\n    return json.dumps(r_json)",
  "def face_detection_to_json(result):\n    r_json = {\n        \"boxes\": result.boxes,\n        \"landmarks\": result.landmarks,\n        \"scores\": result.scores,\n        \"landmarks_per_face\": result.landmarks_per_face,\n    }\n    return json.dumps(r_json)",
  "def face_alignment_to_json(result):\n    r_json = {\"landmarks\": result.landmarks, }\n    return json.dumps(r_json)",
  "def face_recognition_to_json(result):\n    r_json = {\"embedding\": result.embedding, }\n    return json.dumps(r_json)",
  "def segmentation_to_json(result):\n    r_json = {\n        \"label_map\": result.label_map,\n        \"score_map\": result.score_map,\n        \"shape\": result.shape,\n        \"contain_score_map\": result.contain_score_map,\n    }\n    return json.dumps(r_json)",
  "def matting_to_json(result):\n    r_json = {\n        \"alpha\": result.alpha,\n        \"foreground\": result.foreground,\n        \"shape\": result.shape,\n        \"contain_foreground\": result.contain_foreground,\n    }\n    return json.dumps(r_json)",
  "def head_pose_to_json(result):\n    r_json = {\"euler_angles\": result.euler_angles, }\n    return json.dumps(r_json)",
  "def fd_result_to_json(result):\n    if isinstance(result, list):\n        r_list = []\n        for r in result:\n            r_list.append(fd_result_to_json(r))\n        return r_list\n    elif isinstance(result, C.vision.DetectionResult):\n        return detection_to_json(result)\n    elif isinstance(result, C.vision.Mask):\n        return mask_to_json(result)\n    elif isinstance(result, C.vision.ClassifyResult):\n        return classify_to_json(result)\n    elif isinstance(result, C.vision.KeyPointDetectionResult):\n        return keypoint_to_json(result)\n    elif isinstance(result, C.vision.OCRResult):\n        return ocr_to_json(result)\n    elif isinstance(result, C.vision.MOTResult):\n        return mot_to_json(result)\n    elif isinstance(result, C.vision.FaceDetectionResult):\n        return face_detection_to_json(result)\n    elif isinstance(result, C.vision.FaceAlignmentResult):\n        return face_alignment_to_json(result)\n    elif isinstance(result, C.vision.FaceRecognitionResult):\n        return face_recognition_to_json(result)\n    elif isinstance(result, C.vision.SegmentationResult):\n        return segmentation_to_json(result)\n    elif isinstance(result, C.vision.MattingResult):\n        return matting_to_json(result)\n    elif isinstance(result, C.vision.HeadPoseResult):\n        return head_pose_to_json(result)\n    elif isinstance(result, C.vision.PerceptionResult):\n        return perception_to_json(result)\n    else:\n        assert False, \"{} Conversion to JSON format is not supported\".format(\n            type(result))\n    return {}",
  "def json_to_mask(result):\n    mask = C.vision.Mask()\n    mask.data = result['data']\n    mask.shape = result['shape']\n    return mask",
  "def json_to_detection(result):\n    masks = []\n    for mask in result['masks']:\n        masks.append(json_to_mask(json.loads(mask)))\n    det_result = C.vision.DetectionResult()\n    det_result.boxes = result['boxes']\n    det_result.scores = result['scores']\n    det_result.label_ids = result['label_ids']\n    det_result.masks = masks\n    det_result.contain_masks = result['contain_masks']\n    return det_result",
  "def json_to_perception(result):\n    perception_result = C.vision.PerceptionResult()\n    perception_result.scores = result['scores']\n    perception_result.label_ids = result['label_ids']\n    perception_result.boxes = result['boxes']\n    perception_result.center = result['center']\n    perception_result.observation_angle = result['observation_angle']\n    perception_result.yaw_angle = result['yaw_angle']\n    perception_result.velocity = result['velocity']\n    return perception_result",
  "def json_to_classify(result):\n    cls_result = C.vision.ClassifyResult()\n    cls_result.label_ids = result['label_ids']\n    cls_result.scores = result['scores']\n    return cls_result",
  "def json_to_keypoint(result):\n    kp_result = C.vision.KeyPointDetectionResult()\n    kp_result.keypoints = result['keypoints']\n    kp_result.scores = result['scores']\n    kp_result.num_joints = result['num_joints']\n    return kp_result",
  "def json_to_ocr(result):\n    ocr_result = C.vision.OCRResult()\n    ocr_result.boxes = result['boxes']\n    ocr_result.text = result['text']\n    ocr_result.rec_scores = result['rec_scores']\n    ocr_result.cls_scores = result['cls_scores']\n    ocr_result.cls_labels = result['cls_labels']\n    return ocr_result",
  "def json_to_mot(result):\n    mot_result = C.vision.MOTResult()\n    mot_result.boxes = result['boxes']\n    mot_result.ids = result['ids']\n    mot_result.scores = result['scores']\n    mot_result.class_ids = result['class_ids']\n    return mot_result",
  "def json_to_face_detection(result):\n    face_result = C.vision.FaceDetectionResult()\n    face_result.boxes = result['boxes']\n    face_result.landmarks = result['landmarks']\n    face_result.scores = result['scores']\n    face_result.landmarks_per_face = result['landmarks_per_face']\n    return face_result",
  "def json_to_face_alignment(result):\n    face_result = C.vision.FaceAlignmentResult()\n    face_result.landmarks = result['landmarks']\n    return face_result",
  "def json_to_face_recognition(result):\n    face_result = C.vision.FaceRecognitionResult()\n    face_result.embedding = result['embedding']\n    return face_result",
  "def json_to_segmentation(result):\n    seg_result = C.vision.SegmentationResult()\n    seg_result.label_map = result['label_map']\n    seg_result.score_map = result['score_map']\n    seg_result.shape = result['shape']\n    seg_result.contain_score_map = result['contain_score_map']\n    return seg_result",
  "def json_to_matting(result):\n    matting_result = C.vision.MattingResult()\n    matting_result.alpha = result['alpha']\n    matting_result.foreground = result['foreground']\n    matting_result.shape = result['shape']\n    matting_result.contain_foreground = result['contain_foreground']\n    return matting_result",
  "def json_to_head_pose(result):\n    hp_result = C.vision.HeadPoseResult()\n    hp_result.euler_angles = result['euler_angles']\n    return hp_result",
  "def enable_flycv():\n    return C.vision.enable_flycv()",
  "def disable_flycv():\n    return C.vision.disable_flycv()",
  "class AdaFacePreprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for AdaFace Model\n        \"\"\"\n        self._preprocessor = C.vision.faceid.AdaFacePreprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for AdaFace Model\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor, include image, scale_factor, im_shape\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "class AdaFacePostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for AdaFace Model\n\n        \"\"\"\n        self._postprocessor = C.vision.faceid.AdaFacePostprocessor()\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for PaddleClas Model\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of FaceRecognitionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)\n\n    @property\n    def l2_normalize(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.l2_normalize",
  "class AdaFace(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a AdaFace model exported by PaddleClas.\n\n        :param model_file: (str)Path of model file, e.g adaface/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g adaface/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(AdaFace, self).__init__(runtime_option)\n        self._model = C.vision.faceid.AdaFace(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"AdaFace model initialize failed.\"\n\n    def predict(self, im):\n        \"\"\"Detect an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: DetectionResult\n        \"\"\"\n\n        assert im is not None, \"The input image data is None.\"\n        return self._model.predict(im)\n\n    def batch_predict(self, images):\n        \"\"\"Detect a batch of input image list\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get AdaFacePreprocessor object of the loaded model\n\n        :return AdaFacePreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get AdaFacePostprocessor object of the loaded model\n\n        :return AdaFacePostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self):\n        \"\"\"Create a preprocessor for AdaFace Model\n        \"\"\"\n        self._preprocessor = C.vision.faceid.AdaFacePreprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for AdaFace Model\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor, include image, scale_factor, im_shape\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def __init__(self):\n        \"\"\"Create a postprocessor for AdaFace Model\n\n        \"\"\"\n        self._postprocessor = C.vision.faceid.AdaFacePostprocessor()",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for PaddleClas Model\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of FaceRecognitionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def l2_normalize(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.l2_normalize",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a AdaFace model exported by PaddleClas.\n\n        :param model_file: (str)Path of model file, e.g adaface/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g adaface/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(AdaFace, self).__init__(runtime_option)\n        self._model = C.vision.faceid.AdaFace(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"AdaFace model initialize failed.\"",
  "def predict(self, im):\n        \"\"\"Detect an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: DetectionResult\n        \"\"\"\n\n        assert im is not None, \"The input image data is None.\"\n        return self._model.predict(im)",
  "def batch_predict(self, images):\n        \"\"\"Detect a batch of input image list\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get AdaFacePreprocessor object of the loaded model\n\n        :return AdaFacePreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get AdaFacePostprocessor object of the loaded model\n\n        :return AdaFacePostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class InsightFaceRecognitionPreprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for InsightFaceRecognition Model\n        \"\"\"\n        self._preprocessor = C.vision.faceid.InsightFaceRecognitionPreprocessor(\n        )\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for InsightFaceRecognition Model\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor, include image, scale_factor, im_shape\n        \"\"\"\n        return self._preprocessor.run(input_ims)\n\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, tuple of (width, height),\n        decide the target size after resize, default (112, 112)\n        \"\"\"\n        return self._preprocessor.size\n\n    @property\n    def alpha(self):\n        \"\"\"\n        Argument for image preprocessing step, alpha values for normalization,\n        default alpha = {1.f / 127.5f, 1.f / 127.5f, 1.f / 127.5f};\n        \"\"\"\n        return self._preprocessor.alpha\n\n    @property\n    def beta(self):\n        \"\"\"\n        Argument for image preprocessing step, beta values for normalization,\n        default beta = {-1.f, -1.f, -1.f}\n        \"\"\"\n        return self._preprocessor.beta\n\n    def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._preprocessor.disable_normalize()\n\n    def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._preprocessor.disable_permute()",
  "class InsightFaceRecognitionPostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for InsightFaceRecognition Model\n        \"\"\"\n        self._postprocessor = C.vision.faceid.InsightFaceRecognitionPostprocessor(\n        )\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for PaddleClas Model\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of FaceRecognitionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)\n\n    @property\n    def l2_normalize(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.l2_normalize",
  "class InsightFaceRecognitionBase(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a InsightFaceRecognitionBase model exported by PaddleClas.\n\n        :param model_file: (str)Path of model file, e.g InsightFaceRecognitionBase/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g InsightFaceRecognitionBase/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(InsightFaceRecognitionBase, self).__init__(runtime_option)\n        self._model = C.vision.faceid.InsightFaceRecognitionBase(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"InsightFaceRecognitionBase model initialize failed.\"\n\n    def predict(self, im):\n        \"\"\"Detect an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: DetectionResult\n        \"\"\"\n\n        assert im is not None, \"The input image data is None.\"\n        return self._model.predict(im)\n\n    def batch_predict(self, images):\n        \"\"\"Detect a batch of input image list\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get InsightFaceRecognitionPreprocessor object of the loaded model\n\n        :return InsightFaceRecognitionPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get InsightFaceRecognitionPostprocessor object of the loaded model\n\n        :return InsightFaceRecognitionPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class ArcFace(InsightFaceRecognitionBase):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a ArcFace model exported by PaddleClas.\n        :param model_file: (str)Path of model file, e.g ArcFace/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ArcFace/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(InsightFaceRecognitionBase, self).__init__(runtime_option)\n\n        self._model = C.vision.faceid.ArcFace(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"ArcFace model initialize failed.\"",
  "class CosFace(InsightFaceRecognitionBase):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a CosFace model exported by PaddleClas.\n        :param model_file: (str)Path of model file, e.g CosFace/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g CosFace/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(InsightFaceRecognitionBase, self).__init__(runtime_option)\n\n        self._model = C.vision.faceid.CosFace(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"CosFace model initialize failed.\"",
  "class PartialFC(InsightFaceRecognitionBase):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a PartialFC model exported by PaddleClas.\n        :param model_file: (str)Path of model file, e.g PartialFC/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g PartialFC/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(InsightFaceRecognitionBase, self).__init__(runtime_option)\n\n        self._model = C.vision.faceid.PartialFC(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"PartialFC model initialize failed.\"",
  "class VPL(InsightFaceRecognitionBase):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a VPL model exported by PaddleClas.\n        :param model_file: (str)Path of model file, e.g VPL/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g VPL/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(InsightFaceRecognitionBase, self).__init__(runtime_option)\n\n        self._model = C.vision.faceid.VPL(model_file, params_file,\n                                          self._runtime_option, model_format)\n        assert self.initialized, \"VPL model initialize failed.\"",
  "def __init__(self):\n        \"\"\"Create a preprocessor for InsightFaceRecognition Model\n        \"\"\"\n        self._preprocessor = C.vision.faceid.InsightFaceRecognitionPreprocessor(\n        )",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for InsightFaceRecognition Model\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor, include image, scale_factor, im_shape\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, tuple of (width, height),\n        decide the target size after resize, default (112, 112)\n        \"\"\"\n        return self._preprocessor.size",
  "def alpha(self):\n        \"\"\"\n        Argument for image preprocessing step, alpha values for normalization,\n        default alpha = {1.f / 127.5f, 1.f / 127.5f, 1.f / 127.5f};\n        \"\"\"\n        return self._preprocessor.alpha",
  "def beta(self):\n        \"\"\"\n        Argument for image preprocessing step, beta values for normalization,\n        default beta = {-1.f, -1.f, -1.f}\n        \"\"\"\n        return self._preprocessor.beta",
  "def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._preprocessor.disable_normalize()",
  "def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._preprocessor.disable_permute()",
  "def __init__(self):\n        \"\"\"Create a postprocessor for InsightFaceRecognition Model\n        \"\"\"\n        self._postprocessor = C.vision.faceid.InsightFaceRecognitionPostprocessor(\n        )",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for PaddleClas Model\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of FaceRecognitionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def l2_normalize(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.l2_normalize",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a InsightFaceRecognitionBase model exported by PaddleClas.\n\n        :param model_file: (str)Path of model file, e.g InsightFaceRecognitionBase/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g InsightFaceRecognitionBase/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(InsightFaceRecognitionBase, self).__init__(runtime_option)\n        self._model = C.vision.faceid.InsightFaceRecognitionBase(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"InsightFaceRecognitionBase model initialize failed.\"",
  "def predict(self, im):\n        \"\"\"Detect an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: DetectionResult\n        \"\"\"\n\n        assert im is not None, \"The input image data is None.\"\n        return self._model.predict(im)",
  "def batch_predict(self, images):\n        \"\"\"Detect a batch of input image list\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get InsightFaceRecognitionPreprocessor object of the loaded model\n\n        :return InsightFaceRecognitionPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get InsightFaceRecognitionPostprocessor object of the loaded model\n\n        :return InsightFaceRecognitionPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a ArcFace model exported by PaddleClas.\n        :param model_file: (str)Path of model file, e.g ArcFace/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ArcFace/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(InsightFaceRecognitionBase, self).__init__(runtime_option)\n\n        self._model = C.vision.faceid.ArcFace(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"ArcFace model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a CosFace model exported by PaddleClas.\n        :param model_file: (str)Path of model file, e.g CosFace/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g CosFace/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(InsightFaceRecognitionBase, self).__init__(runtime_option)\n\n        self._model = C.vision.faceid.CosFace(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"CosFace model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a PartialFC model exported by PaddleClas.\n        :param model_file: (str)Path of model file, e.g PartialFC/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g PartialFC/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(InsightFaceRecognitionBase, self).__init__(runtime_option)\n\n        self._model = C.vision.faceid.PartialFC(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"PartialFC model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a VPL model exported by PaddleClas.\n        :param model_file: (str)Path of model file, e.g VPL/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g VPL/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(InsightFaceRecognitionBase, self).__init__(runtime_option)\n\n        self._model = C.vision.faceid.VPL(model_file, params_file,\n                                          self._runtime_option, model_format)\n        assert self.initialized, \"VPL model initialize failed.\"",
  "class YOLOv7End2EndTRT(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv7End2EndTRT model exported by YOLOv7.\n\n        :param model_file: (str)Path of model file, e.g ./yolov7end2end_trt.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv7End2EndTRT, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv7End2EndTRT(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv7End2EndTRT initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold)\n\n    # \u4e00\u4e9b\u8ddf\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [1280, 1280]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size\n\n    @property\n    def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value\n\n    @property\n    def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad\n\n    @property\n    def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad\n\n    @property\n    def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up\n\n    @property\n    def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value\n\n    @is_no_pad.setter\n    def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value\n\n    @is_mini_pad.setter\n    def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value\n\n    @stride.setter\n    def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv7End2EndTRT model exported by YOLOv7.\n\n        :param model_file: (str)Path of model file, e.g ./yolov7end2end_trt.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv7End2EndTRT, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv7End2EndTRT(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv7End2EndTRT initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size",
  "def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value",
  "def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad",
  "def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad",
  "def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up",
  "def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value",
  "def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value",
  "def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value",
  "def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value",
  "class YOLOv6(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv6 model exported by YOLOv6.\n\n        :param model_file: (str)Path of model file, e.g ./yolov6.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv6, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv6(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv6 initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)\n\n    # \u4e00\u4e9b\u8ddfYOLOv6\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [1280, 1280]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size\n\n    @property\n    def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value\n\n    @property\n    def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad\n\n    @property\n    def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad\n\n    @property\n    def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up\n\n    @property\n    def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride\n\n    @property\n    def max_wh(self):\n        # for offseting the boxes by classes when using NMS\n        return self._model.max_wh\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value\n\n    @is_no_pad.setter\n    def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value\n\n    @is_mini_pad.setter\n    def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value\n\n    @stride.setter\n    def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value\n\n    @max_wh.setter\n    def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv6 model exported by YOLOv6.\n\n        :param model_file: (str)Path of model file, e.g ./yolov6.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv6, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv6(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv6 initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size",
  "def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value",
  "def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad",
  "def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad",
  "def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up",
  "def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride",
  "def max_wh(self):\n        # for offseting the boxes by classes when using NMS\n        return self._model.max_wh",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value",
  "def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value",
  "def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value",
  "def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value",
  "def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value",
  "class ScaledYOLOv4(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a ScaledYOLOv4 model exported by ScaledYOLOv4.\n\n        :param model_file: (str)Path of model file, e.g ./scaled_yolov4.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(ScaledYOLOv4, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.ScaledYOLOv4(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"ScaledYOLOv4 initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)\n\n    # \u4e00\u4e9b\u8ddfScaledYOLOv4\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [1280, 1280]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n\n        \"\"\"\n        return self._model.size\n\n    @property\n    def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value\n\n    @property\n    def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad\n\n    @property\n    def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad\n\n    @property\n    def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up\n\n    @property\n    def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride\n\n    @property\n    def max_wh(self):\n        # for offseting the boxes by classes when using NMS\n        return self._model.max_wh\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value\n\n    @is_no_pad.setter\n    def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value\n\n    @is_mini_pad.setter\n    def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value\n\n    @stride.setter\n    def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value\n\n    @max_wh.setter\n    def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a ScaledYOLOv4 model exported by ScaledYOLOv4.\n\n        :param model_file: (str)Path of model file, e.g ./scaled_yolov4.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(ScaledYOLOv4, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.ScaledYOLOv4(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"ScaledYOLOv4 initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n\n        \"\"\"\n        return self._model.size",
  "def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value",
  "def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad",
  "def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad",
  "def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up",
  "def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride",
  "def max_wh(self):\n        # for offseting the boxes by classes when using NMS\n        return self._model.max_wh",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value",
  "def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value",
  "def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value",
  "def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value",
  "def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value",
  "class YOLOv5SegPreprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for YOLOv5Seg\n        \"\"\"\n        self._preprocessor = C.vision.detection.YOLOv5SegPreprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for YOLOv5Seg\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)\n\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size\n\n    @property\n    def padding_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_value\n\n    @property\n    def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up\n\n    @property\n    def is_mini_pad(self):\n        \"\"\"\n        is_mini_pad for preprocessing, pad to the minimum rectange which height and width is times of stride, default false\n        \"\"\"\n        return self._preprocessor.is_mini_pad\n\n    @property\n    def stride(self):\n        \"\"\"\n        stride for preprocessing, only for mini_pad mode, default 32\n        \"\"\"\n        return self._preprocessor.stride\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._preprocessor.padding_value = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value\n\n    @is_mini_pad.setter\n    def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._preprocessor.is_mini_pad = value\n\n    @stride.setter\n    def stride(self, value):\n        assert isinstance(\n            stride, int), \"The value to set `stride` must be type of int.\"\n        self._preprocessor.stride = value",
  "class YOLOv5SegPostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for YOLOv5Seg\n        \"\"\"\n        self._postprocessor = C.vision.detection.YOLOv5SegPostprocessor()\n\n    def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for YOLOv5Seg\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)\n\n    @property\n    def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.25\n        \"\"\"\n        return self._postprocessor.conf_threshold\n\n    @property\n    def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.nms_threshold\n\n    @property\n    def multi_label(self):\n        \"\"\"\n        multi_label for postprocessing, set true for eval, default is True\n        \"\"\"\n        return self._postprocessor.multi_label\n\n    @conf_threshold.setter\n    def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold\n\n    @nms_threshold.setter\n    def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold\n\n    @multi_label.setter\n    def multi_label(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `multi_label` must be type of bool.\"\n        self._postprocessor.multi_label = value",
  "class YOLOv5Seg(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv5Seg model exported by YOLOv5.\n\n        :param model_file: (str)Path of model file, e.g ./yolov5s-seg.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(YOLOv5Seg, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv5Seg(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"YOLOv5Seg initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get YOLOv5SegPreprocessor object of the loaded model\n\n        :return YOLOv5SegPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get YOLOv5SegPostprocessor object of the loaded model\n\n        :return YOLOv5SegPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self):\n        \"\"\"Create a preprocessor for YOLOv5Seg\n        \"\"\"\n        self._preprocessor = C.vision.detection.YOLOv5SegPreprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for YOLOv5Seg\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size",
  "def padding_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_value",
  "def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up",
  "def is_mini_pad(self):\n        \"\"\"\n        is_mini_pad for preprocessing, pad to the minimum rectange which height and width is times of stride, default false\n        \"\"\"\n        return self._preprocessor.is_mini_pad",
  "def stride(self):\n        \"\"\"\n        stride for preprocessing, only for mini_pad mode, default 32\n        \"\"\"\n        return self._preprocessor.stride",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._preprocessor.padding_value = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value",
  "def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._preprocessor.is_mini_pad = value",
  "def stride(self, value):\n        assert isinstance(\n            stride, int), \"The value to set `stride` must be type of int.\"\n        self._preprocessor.stride = value",
  "def __init__(self):\n        \"\"\"Create a postprocessor for YOLOv5Seg\n        \"\"\"\n        self._postprocessor = C.vision.detection.YOLOv5SegPostprocessor()",
  "def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for YOLOv5Seg\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)",
  "def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.25\n        \"\"\"\n        return self._postprocessor.conf_threshold",
  "def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.nms_threshold",
  "def multi_label(self):\n        \"\"\"\n        multi_label for postprocessing, set true for eval, default is True\n        \"\"\"\n        return self._postprocessor.multi_label",
  "def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold",
  "def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "def multi_label(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `multi_label` must be type of bool.\"\n        self._postprocessor.multi_label = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv5Seg model exported by YOLOv5.\n\n        :param model_file: (str)Path of model file, e.g ./yolov5s-seg.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(YOLOv5Seg, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv5Seg(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"YOLOv5Seg initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get YOLOv5SegPreprocessor object of the loaded model\n\n        :return YOLOv5SegPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get YOLOv5SegPostprocessor object of the loaded model\n\n        :return YOLOv5SegPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class YOLOv5Lite(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv5Lite model exported by YOLOv5Lite.\n\n        :param model_file: (str)Path of model file, e.g ./yolov5lite.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv5Lite, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv5Lite(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv5Lite initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)\n\n    # \u4e00\u4e9b\u8ddfYOLOv5Lite\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [1280, 1280]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size\n\n    @property\n    def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value\n\n    @property\n    def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad\n\n    @property\n    def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad\n\n    @property\n    def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up\n\n    @property\n    def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride\n\n    @property\n    def max_wh(self):\n        # for offseting the boxes by classes when using NMS\n        return self._model.max_wh\n\n    @property\n    def is_decode_exported(self):\n        \"\"\"\n        whether the model_file was exported with decode module.\n        The official YOLOv5Lite/export.py script will export ONNX file without decode module.\n        Please set it 'true' manually if the model file was exported with decode module.\n        False : ONNX files without decode module. True : ONNX file with decode module.\n        default False\n        \"\"\"\n        return self._model.is_decode_exported\n\n    @property\n    def anchor_config(self):\n        return self._model.anchor_config\n\n    @property\n    def downsample_strides(self):\n        \"\"\"\n        downsample strides for YOLOv5Lite to generate anchors, will take (8,16,32) as default values, might have stride=64.\n        \"\"\"\n        return self._model.downsample_strides\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value\n\n    @is_no_pad.setter\n    def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value\n\n    @is_mini_pad.setter\n    def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value\n\n    @stride.setter\n    def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value\n\n    @max_wh.setter\n    def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value\n\n    @is_decode_exported.setter\n    def is_decode_exported(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_decode_exported` must be type of bool.\"\n        self._model.is_decode_exported = value\n\n    @anchor_config.setter\n    def anchor_config(self, anchor_config_val):\n        assert isinstance(anchor_config_val, list),\\\n            \"The value to set `anchor_config` must be type of tuple or list.\"\n        assert isinstance(anchor_config_val[0], list),\\\n            \"The value to set `anchor_config` must be 2-dimensions tuple or list\"\n        self._model.anchor_config = anchor_config_val\n\n    @downsample_strides.setter\n    def downsample_strides(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `downsample_strides` must be type of list.\"\n        self._model.downsample_strides = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv5Lite model exported by YOLOv5Lite.\n\n        :param model_file: (str)Path of model file, e.g ./yolov5lite.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv5Lite, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv5Lite(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv5Lite initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size",
  "def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value",
  "def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad",
  "def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad",
  "def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up",
  "def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride",
  "def max_wh(self):\n        # for offseting the boxes by classes when using NMS\n        return self._model.max_wh",
  "def is_decode_exported(self):\n        \"\"\"\n        whether the model_file was exported with decode module.\n        The official YOLOv5Lite/export.py script will export ONNX file without decode module.\n        Please set it 'true' manually if the model file was exported with decode module.\n        False : ONNX files without decode module. True : ONNX file with decode module.\n        default False\n        \"\"\"\n        return self._model.is_decode_exported",
  "def anchor_config(self):\n        return self._model.anchor_config",
  "def downsample_strides(self):\n        \"\"\"\n        downsample strides for YOLOv5Lite to generate anchors, will take (8,16,32) as default values, might have stride=64.\n        \"\"\"\n        return self._model.downsample_strides",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value",
  "def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value",
  "def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value",
  "def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value",
  "def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value",
  "def is_decode_exported(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_decode_exported` must be type of bool.\"\n        self._model.is_decode_exported = value",
  "def anchor_config(self, anchor_config_val):\n        assert isinstance(anchor_config_val, list),\\\n            \"The value to set `anchor_config` must be type of tuple or list.\"\n        assert isinstance(anchor_config_val[0], list),\\\n            \"The value to set `anchor_config` must be 2-dimensions tuple or list\"\n        self._model.anchor_config = anchor_config_val",
  "def downsample_strides(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `downsample_strides` must be type of list.\"\n        self._model.downsample_strides = value",
  "class NanoDetPlus(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a NanoDetPlus model exported by NanoDet.\n\n        :param model_file: (str)Path of model file, e.g ./nanodet.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(NanoDetPlus, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.NanoDetPlus(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"NanoDetPlus initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)\n\n    # \u4e00\u4e9b\u8ddfNanoDetPlus\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [416, 416]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height),  default (320, 320)\n        \"\"\"\n        return self._model.size\n\n    @property\n    def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value\n\n    @property\n    def keep_ratio(self):\n        # keep aspect ratio or not when perform resize operation. This option is set as false by default in NanoDet-Plus\n        return self._model.keep_ratio\n\n    @property\n    def downsample_strides(self):\n        # downsample strides for NanoDet-Plus to generate anchors, will take (8, 16, 32, 64) as default values\n        return self._model.downsample_strides\n\n    @property\n    def max_wh(self):\n        # for offseting the boxes by classes when using NMS, default 4096\n        return self._model.max_wh\n\n    @property\n    def reg_max(self):\n        \"\"\"\n        reg_max for GFL regression, default 7\n        \"\"\"\n        return self._model.reg_max\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value\n\n    @keep_ratio.setter\n    def keep_ratio(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `keep_ratio` must be type of bool.\"\n        self._model.keep_ratio = value\n\n    @downsample_strides.setter\n    def downsample_strides(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `downsample_strides` must be type of list.\"\n        self._model.downsample_strides = value\n\n    @max_wh.setter\n    def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value\n\n    @reg_max.setter\n    def reg_max(self, value):\n        assert isinstance(\n            value, int), \"The value to set `reg_max` must be type of int.\"\n        self._model.reg_max = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a NanoDetPlus model exported by NanoDet.\n\n        :param model_file: (str)Path of model file, e.g ./nanodet.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(NanoDetPlus, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.NanoDetPlus(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"NanoDetPlus initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height),  default (320, 320)\n        \"\"\"\n        return self._model.size",
  "def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value",
  "def keep_ratio(self):\n        # keep aspect ratio or not when perform resize operation. This option is set as false by default in NanoDet-Plus\n        return self._model.keep_ratio",
  "def downsample_strides(self):\n        # downsample strides for NanoDet-Plus to generate anchors, will take (8, 16, 32, 64) as default values\n        return self._model.downsample_strides",
  "def max_wh(self):\n        # for offseting the boxes by classes when using NMS, default 4096\n        return self._model.max_wh",
  "def reg_max(self):\n        \"\"\"\n        reg_max for GFL regression, default 7\n        \"\"\"\n        return self._model.reg_max",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value",
  "def keep_ratio(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `keep_ratio` must be type of bool.\"\n        self._model.keep_ratio = value",
  "def downsample_strides(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `downsample_strides` must be type of list.\"\n        self._model.downsample_strides = value",
  "def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value",
  "def reg_max(self, value):\n        assert isinstance(\n            value, int), \"The value to set `reg_max` must be type of int.\"\n        self._model.reg_max = value",
  "class YOLOv8Preprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for YOLOv8\n        \"\"\"\n        self._preprocessor = C.vision.detection.YOLOv8Preprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for YOLOv8\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)\n\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size\n\n    @property\n    def padding_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_value\n\n    @property\n    def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up\n\n    @property\n    def is_mini_pad(self):\n        \"\"\"\n        is_mini_pad for preprocessing, pad to the minimum rectange which height and width is times of stride, default false\n        \"\"\"\n        return self._preprocessor.is_mini_pad\n\n    @property\n    def stride(self):\n        \"\"\"\n        stride for preprocessing, only for mini_pad mode, default 32\n        \"\"\"\n        return self._preprocessor.stride\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._preprocessor.padding_value = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value\n\n    @is_mini_pad.setter\n    def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._preprocessor.is_mini_pad = value\n\n    @stride.setter\n    def stride(self, value):\n        assert isinstance(\n            stride, int), \"The value to set `stride` must be type of int.\"\n        self._preprocessor.stride = value",
  "class YOLOv8Postprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for YOLOv8\n        \"\"\"\n        self._postprocessor = C.vision.detection.YOLOv8Postprocessor()\n\n    def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for YOLOv8\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)\n\n    @property\n    def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.25\n        \"\"\"\n        return self._postprocessor.conf_threshold\n\n    @property\n    def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.nms_threshold\n\n    @property\n    def multi_label(self):\n        \"\"\"\n        multi_label for postprocessing, set true for eval, default is True\n        \"\"\"\n        return self._postprocessor.multi_label\n\n    @conf_threshold.setter\n    def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold\n\n    @nms_threshold.setter\n    def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold\n\n    @multi_label.setter\n    def multi_label(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `multi_label` must be type of bool.\"\n        self._postprocessor.multi_label = value",
  "class YOLOv8(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv8 model exported by YOLOv8.\n\n        :param model_file: (str)Path of model file, e.g ./yolov8s.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(YOLOv8, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv8(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"YOLOv8 initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get YOLOv8Preprocessor object of the loaded model\n\n        :return YOLOv8Preprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get YOLOv8Postprocessor object of the loaded model\n\n        :return YOLOv8Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self):\n        \"\"\"Create a preprocessor for YOLOv8\n        \"\"\"\n        self._preprocessor = C.vision.detection.YOLOv8Preprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for YOLOv8\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size",
  "def padding_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_value",
  "def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up",
  "def is_mini_pad(self):\n        \"\"\"\n        is_mini_pad for preprocessing, pad to the minimum rectange which height and width is times of stride, default false\n        \"\"\"\n        return self._preprocessor.is_mini_pad",
  "def stride(self):\n        \"\"\"\n        stride for preprocessing, only for mini_pad mode, default 32\n        \"\"\"\n        return self._preprocessor.stride",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._preprocessor.padding_value = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value",
  "def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._preprocessor.is_mini_pad = value",
  "def stride(self, value):\n        assert isinstance(\n            stride, int), \"The value to set `stride` must be type of int.\"\n        self._preprocessor.stride = value",
  "def __init__(self):\n        \"\"\"Create a postprocessor for YOLOv8\n        \"\"\"\n        self._postprocessor = C.vision.detection.YOLOv8Postprocessor()",
  "def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for YOLOv8\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)",
  "def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.25\n        \"\"\"\n        return self._postprocessor.conf_threshold",
  "def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.nms_threshold",
  "def multi_label(self):\n        \"\"\"\n        multi_label for postprocessing, set true for eval, default is True\n        \"\"\"\n        return self._postprocessor.multi_label",
  "def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold",
  "def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "def multi_label(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `multi_label` must be type of bool.\"\n        self._postprocessor.multi_label = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv8 model exported by YOLOv8.\n\n        :param model_file: (str)Path of model file, e.g ./yolov8s.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(YOLOv8, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv8(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"YOLOv8 initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get YOLOv8Preprocessor object of the loaded model\n\n        :return YOLOv8Preprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get YOLOv8Postprocessor object of the loaded model\n\n        :return YOLOv8Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class YOLOv5Preprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for YOLOv5\n        \"\"\"\n        self._preprocessor = C.vision.detection.YOLOv5Preprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for YOLOv5\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)\n\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size\n\n    @property\n    def padding_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_value\n\n    @property\n    def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up\n\n    @property\n    def is_mini_pad(self):\n        \"\"\"\n        is_mini_pad for preprocessing, pad to the minimum rectange which height and width is times of stride, default false\n        \"\"\"\n        return self._preprocessor.is_mini_pad\n\n    @property\n    def stride(self):\n        \"\"\"\n        stride for preprocessing, only for mini_pad mode, default 32\n        \"\"\"\n        return self._preprocessor.stride\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._preprocessor.padding_value = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value\n\n    @is_mini_pad.setter\n    def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._preprocessor.is_mini_pad = value\n\n    @stride.setter\n    def stride(self, value):\n        assert isinstance(\n            stride, int), \"The value to set `stride` must be type of int.\"\n        self._preprocessor.stride = value",
  "class YOLOv5Postprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for YOLOv5\n        \"\"\"\n        self._postprocessor = C.vision.detection.YOLOv5Postprocessor()\n\n    def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for YOLOv5\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)\n\n    @property\n    def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.25\n        \"\"\"\n        return self._postprocessor.conf_threshold\n\n    @property\n    def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.nms_threshold\n\n    @property\n    def multi_label(self):\n        \"\"\"\n        multi_label for postprocessing, set true for eval, default is True\n        \"\"\"\n        return self._postprocessor.multi_label\n\n    @conf_threshold.setter\n    def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold\n\n    @nms_threshold.setter\n    def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold\n\n    @multi_label.setter\n    def multi_label(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `multi_label` must be type of bool.\"\n        self._postprocessor.multi_label = value",
  "class YOLOv5(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv5 model exported by YOLOv5.\n\n        :param model_file: (str)Path of model file, e.g ./yolov5.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv5, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv5(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv5 initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        self.postprocessor.conf_threshold = conf_threshold\n        self.postprocessor.nms_threshold = nms_iou_threshold\n        return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get YOLOv5Preprocessor object of the loaded model\n\n        :return YOLOv5Preprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get YOLOv5Postprocessor object of the loaded model\n\n        :return YOLOv5Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self):\n        \"\"\"Create a preprocessor for YOLOv5\n        \"\"\"\n        self._preprocessor = C.vision.detection.YOLOv5Preprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for YOLOv5\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size",
  "def padding_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_value",
  "def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up",
  "def is_mini_pad(self):\n        \"\"\"\n        is_mini_pad for preprocessing, pad to the minimum rectange which height and width is times of stride, default false\n        \"\"\"\n        return self._preprocessor.is_mini_pad",
  "def stride(self):\n        \"\"\"\n        stride for preprocessing, only for mini_pad mode, default 32\n        \"\"\"\n        return self._preprocessor.stride",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._preprocessor.padding_value = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value",
  "def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._preprocessor.is_mini_pad = value",
  "def stride(self, value):\n        assert isinstance(\n            stride, int), \"The value to set `stride` must be type of int.\"\n        self._preprocessor.stride = value",
  "def __init__(self):\n        \"\"\"Create a postprocessor for YOLOv5\n        \"\"\"\n        self._postprocessor = C.vision.detection.YOLOv5Postprocessor()",
  "def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for YOLOv5\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)",
  "def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.25\n        \"\"\"\n        return self._postprocessor.conf_threshold",
  "def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.nms_threshold",
  "def multi_label(self):\n        \"\"\"\n        multi_label for postprocessing, set true for eval, default is True\n        \"\"\"\n        return self._postprocessor.multi_label",
  "def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold",
  "def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "def multi_label(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `multi_label` must be type of bool.\"\n        self._postprocessor.multi_label = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv5 model exported by YOLOv5.\n\n        :param model_file: (str)Path of model file, e.g ./yolov5.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv5, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv5(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv5 initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        self.postprocessor.conf_threshold = conf_threshold\n        self.postprocessor.nms_threshold = nms_iou_threshold\n        return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get YOLOv5Preprocessor object of the loaded model\n\n        :return YOLOv5Preprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get YOLOv5Postprocessor object of the loaded model\n\n        :return YOLOv5Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class FastestDetPreprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for FastestDet\n        \"\"\"\n        self._preprocessor = C.vision.detection.FastestDetPreprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for FastestDet\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)\n\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [352, 352]\n        \"\"\"\n        return self._preprocessor.size\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh",
  "class FastestDetPostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for FastestDet\n        \"\"\"\n        self._postprocessor = C.vision.detection.FastestDetPostprocessor()\n\n    def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for FastestDet\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)\n\n    @property\n    def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.65\n        \"\"\"\n        return self._postprocessor.conf_threshold\n\n    @property\n    def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.45\n        \"\"\"\n        return self._postprocessor.nms_threshold\n\n    @conf_threshold.setter\n    def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold\n\n    @nms_threshold.setter\n    def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "class FastestDet(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a FastestDet model exported by FastestDet.\n\n        :param model_file: (str)Path of model file, e.g ./FastestDet.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(FastestDet, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"FastestDet only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.detection.FastestDet(\n            model_file, params_file, self._runtime_option, model_format)\n\n        assert self.initialized, \"FastestDet initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: DetectionResult\n        \"\"\"\n        assert input_image is not None, \"Input image is None.\"\n        return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        assert len(images) == 1,\"FastestDet is only support 1 image in batch_predict\"\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get FastestDetPreprocessor object of the loaded model\n\n        :return FastestDetPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get FastestDetPostprocessor object of the loaded model\n\n        :return FastestDetPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self):\n        \"\"\"Create a preprocessor for FastestDet\n        \"\"\"\n        self._preprocessor = C.vision.detection.FastestDetPreprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for FastestDet\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [352, 352]\n        \"\"\"\n        return self._preprocessor.size",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh",
  "def __init__(self):\n        \"\"\"Create a postprocessor for FastestDet\n        \"\"\"\n        self._postprocessor = C.vision.detection.FastestDetPostprocessor()",
  "def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for FastestDet\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)",
  "def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.65\n        \"\"\"\n        return self._postprocessor.conf_threshold",
  "def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.45\n        \"\"\"\n        return self._postprocessor.nms_threshold",
  "def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold",
  "def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a FastestDet model exported by FastestDet.\n\n        :param model_file: (str)Path of model file, e.g ./FastestDet.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(FastestDet, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"FastestDet only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.detection.FastestDet(\n            model_file, params_file, self._runtime_option, model_format)\n\n        assert self.initialized, \"FastestDet initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: DetectionResult\n        \"\"\"\n        assert input_image is not None, \"Input image is None.\"\n        return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        assert len(images) == 1,\"FastestDet is only support 1 image in batch_predict\"\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get FastestDetPreprocessor object of the loaded model\n\n        :return FastestDetPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get FastestDetPostprocessor object of the loaded model\n\n        :return FastestDetPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class YOLOv7End2EndORT(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv7End2EndORT model exported by YOLOv7.\n\n        :param model_file: (str)Path of model file, e.g ./yolov7end2end_ort.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv7End2EndORT, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv7End2EndORT(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv7End2End initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold)\n\n    # \u4e00\u4e9b\u8ddf\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [1280, 1280]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size\n\n    @property\n    def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value\n\n    @property\n    def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad\n\n    @property\n    def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad\n\n    @property\n    def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up\n\n    @property\n    def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value\n\n    @is_no_pad.setter\n    def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value\n\n    @is_mini_pad.setter\n    def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value\n\n    @stride.setter\n    def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv7End2EndORT model exported by YOLOv7.\n\n        :param model_file: (str)Path of model file, e.g ./yolov7end2end_ort.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv7End2EndORT, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv7End2EndORT(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv7End2End initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size",
  "def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value",
  "def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad",
  "def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad",
  "def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up",
  "def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value",
  "def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value",
  "def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value",
  "def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value",
  "class YOLOv7Preprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for YOLOv7\n        \"\"\"\n        self._preprocessor = C.vision.detection.YOLOv7Preprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for YOLOv7\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)\n\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size\n\n    @property\n    def padding_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_value\n\n    @property\n    def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._preprocessor.padding_value = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value",
  "class YOLOv7Postprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for YOLOv7\n        \"\"\"\n        self._postprocessor = C.vision.detection.YOLOv7Postprocessor()\n\n    def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for YOLOv7\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)\n\n    @property\n    def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.25\n        \"\"\"\n        return self._postprocessor.conf_threshold\n\n    @property\n    def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.nms_threshold\n\n    @conf_threshold.setter\n    def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold\n\n    @nms_threshold.setter\n    def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "class YOLOv7(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv7 model exported by YOLOv7.\n\n        :param model_file: (str)Path of model file, e.g ./yolov7.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv7, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv7(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv7 initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        self.postprocessor.conf_threshold = conf_threshold\n        self.postprocessor.nms_threshold = nms_iou_threshold\n        return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get YOLOv7Preprocessor object of the loaded model\n\n        :return YOLOv7Preprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get YOLOv7Postprocessor object of the loaded model\n\n        :return YOLOv7Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self):\n        \"\"\"Create a preprocessor for YOLOv7\n        \"\"\"\n        self._preprocessor = C.vision.detection.YOLOv7Preprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for YOLOv7\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size",
  "def padding_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_value",
  "def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._preprocessor.padding_value = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value",
  "def __init__(self):\n        \"\"\"Create a postprocessor for YOLOv7\n        \"\"\"\n        self._postprocessor = C.vision.detection.YOLOv7Postprocessor()",
  "def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for YOLOv7\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)",
  "def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.25\n        \"\"\"\n        return self._postprocessor.conf_threshold",
  "def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.nms_threshold",
  "def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold",
  "def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv7 model exported by YOLOv7.\n\n        :param model_file: (str)Path of model file, e.g ./yolov7.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv7, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOv7(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv7 initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        self.postprocessor.conf_threshold = conf_threshold\n        self.postprocessor.nms_threshold = nms_iou_threshold\n        return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get YOLOv7Preprocessor object of the loaded model\n\n        :return YOLOv7Preprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get YOLOv7Postprocessor object of the loaded model\n\n        :return YOLOv7Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class YOLOR(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOR model exported by YOLOR\n\n        :param model_file: (str)Path of model file, e.g ./yolor.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOR, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOR(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOR initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)\n\n    # \u4e00\u4e9b\u8ddfYOLOR\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [1280, 1280]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size\n\n    @property\n    def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value\n\n    @property\n    def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad\n\n    @property\n    def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad\n\n    @property\n    def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up\n\n    @property\n    def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride\n\n    @property\n    def max_wh(self):\n        # for offseting the boxes by classes when using NMS\n        return self._model.max_wh\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value\n\n    @is_no_pad.setter\n    def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value\n\n    @is_mini_pad.setter\n    def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value\n\n    @stride.setter\n    def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value\n\n    @max_wh.setter\n    def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOR model exported by YOLOR\n\n        :param model_file: (str)Path of model file, e.g ./yolor.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOR, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOR(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOR initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size",
  "def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value",
  "def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad",
  "def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad",
  "def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up",
  "def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride",
  "def max_wh(self):\n        # for offseting the boxes by classes when using NMS\n        return self._model.max_wh",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value",
  "def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value",
  "def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value",
  "def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value",
  "def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value",
  "class YOLOX(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOX model exported by YOLOX.\n\n        :param model_file: (str)Path of model file, e.g ./yolox.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOX, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOX(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOX initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)\n\n    # \u4e00\u4e9b\u8ddfYOLOX\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [1280, 1280]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size\n\n    @property\n    def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value\n\n    @property\n    def is_decode_exported(self):\n        \"\"\"\n        whether the model_file was exported with decode module.\n        The official YOLOX/tools/export_onnx.py script will export ONNX file without decode module.\n        Please set it 'true' manually if the model file was exported with decode module.\n        Defalut False.\n        \"\"\"\n        return self._model.is_decode_exported\n\n    @property\n    def downsample_strides(self):\n        \"\"\"\n        downsample strides for YOLOX to generate anchors, will take (8,16,32) as default values, might have stride=64.\n        \"\"\"\n        return self._model.downsample_strides\n\n    @property\n    def max_wh(self):\n        # for offseting the boxes by classes when using NMS\n        return self._model.max_wh\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value\n\n    @is_decode_exported.setter\n    def is_decode_exported(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_decode_exported` must be type of bool.\"\n        self._model.is_decode_exported = value\n\n    @downsample_strides.setter\n    def downsample_strides(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `downsample_strides` must be type of list.\"\n        self._model.downsample_strides = value\n\n    @max_wh.setter\n    def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOX model exported by YOLOX.\n\n        :param model_file: (str)Path of model file, e.g ./yolox.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOX, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.YOLOX(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOX initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._model.size",
  "def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value",
  "def is_decode_exported(self):\n        \"\"\"\n        whether the model_file was exported with decode module.\n        The official YOLOX/tools/export_onnx.py script will export ONNX file without decode module.\n        Please set it 'true' manually if the model file was exported with decode module.\n        Defalut False.\n        \"\"\"\n        return self._model.is_decode_exported",
  "def downsample_strides(self):\n        \"\"\"\n        downsample strides for YOLOX to generate anchors, will take (8,16,32) as default values, might have stride=64.\n        \"\"\"\n        return self._model.downsample_strides",
  "def max_wh(self):\n        # for offseting the boxes by classes when using NMS\n        return self._model.max_wh",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value",
  "def is_decode_exported(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_decode_exported` must be type of bool.\"\n        self._model.is_decode_exported = value",
  "def downsample_strides(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `downsample_strides` must be type of list.\"\n        self._model.downsample_strides = value",
  "def max_wh(self, value):\n        assert isinstance(\n            value, float), \"The value to set `max_wh` must be type of float.\"\n        self._model.max_wh = value",
  "class RKYOLOPreprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for RKYOLOV5\n        \"\"\"\n        self._preprocessor = C.vision.detection.RKYOLOPreprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for RKYOLOV5\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)\n\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size\n\n    @property\n    def padding_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_value\n\n    @property\n    def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)), \\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2, \\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n                len(wh))\n        self._preprocessor.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._preprocessor.padding_value = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value",
  "class RKYOLOPostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for RKYOLOV5\n        \"\"\"\n        self._postprocessor = C.vision.detection.RKYOLOPostprocessor()\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for RKYOLOV5\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)\n\n    def set_anchor(self, anchor):\n        self._postprocessor.set_anchor(anchor)\n\n    @property\n    def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.25\n        \"\"\"\n        return self._postprocessor.conf_threshold\n\n    @property\n    def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.nms_threshold\n\n    @property\n    def class_num(self):\n        \"\"\"\n        class_num for postprocessing, default is 80\n        \"\"\"\n        return self._postprocessor.class_num\n\n    @conf_threshold.setter\n    def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float), \\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold\n\n    @nms_threshold.setter\n    def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float), \\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold\n\n    @class_num.setter\n    def class_num(self, class_num):\n        \"\"\"\n        class_num for postprocessing, default is 80\n        \"\"\"\n        assert isinstance(class_num, int), \\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.class_num = class_num",
  "class RKYOLOV5(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.RKNN):\n        \"\"\"Load a RKYOLOV5 model exported by RKYOLOV5.\n\n        :param model_file: (str)Path of model file, e.g ./yolov5.rknn\n        :param params_file: (str)Path of parameters file, e.g , if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(RKYOLOV5, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.RKYOLOV5(\n            model_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"RKYOLOV5 initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        self.postprocessor.conf_threshold = conf_threshold\n        self.postprocessor.nms_threshold = nms_iou_threshold\n        return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get RKYOLOV5Preprocessor object of the loaded model\n\n        :return RKYOLOV5Preprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get RKYOLOV5Postprocessor object of the loaded model\n\n        :return RKYOLOV5Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class RKYOLOX(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.RKNN):\n        \"\"\"Load a RKYOLOX model exported by RKYOLOX.\n\n        :param model_file: (str)Path of model file, e.g ./yolox.rknn\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(RKYOLOX, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.RKYOLOX(\n            model_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"RKYOLOV5 initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        self.postprocessor.conf_threshold = conf_threshold\n        self.postprocessor.nms_threshold = nms_iou_threshold\n        return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get RKYOLOV5Preprocessor object of the loaded model\n\n        :return RKYOLOV5Preprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get RKYOLOV5Postprocessor object of the loaded model\n\n        :return RKYOLOV5Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class RKYOLOV7(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.RKNN):\n        \"\"\"Load a RKYOLOX model exported by RKYOLOV7.\n\n        :param model_file: (str)Path of model file, e.g ./yolov7.rknn\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(RKYOLOV7, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.RKYOLOV7(\n            model_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"RKYOLOV5 initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        self.postprocessor.conf_threshold = conf_threshold\n        self.postprocessor.nms_threshold = nms_iou_threshold\n        return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get RKYOLOV5Preprocessor object of the loaded model\n\n        :return RKYOLOV5Preprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get RKYOLOV5Postprocessor object of the loaded model\n\n        :return RKYOLOV5Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self):\n        \"\"\"Create a preprocessor for RKYOLOV5\n        \"\"\"\n        self._preprocessor = C.vision.detection.RKYOLOPreprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for RKYOLOV5\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size",
  "def padding_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_value",
  "def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)), \\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2, \\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n                len(wh))\n        self._preprocessor.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._preprocessor.padding_value = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value",
  "def __init__(self):\n        \"\"\"Create a postprocessor for RKYOLOV5\n        \"\"\"\n        self._postprocessor = C.vision.detection.RKYOLOPostprocessor()",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for RKYOLOV5\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def set_anchor(self, anchor):\n        self._postprocessor.set_anchor(anchor)",
  "def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.25\n        \"\"\"\n        return self._postprocessor.conf_threshold",
  "def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.nms_threshold",
  "def class_num(self):\n        \"\"\"\n        class_num for postprocessing, default is 80\n        \"\"\"\n        return self._postprocessor.class_num",
  "def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float), \\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold",
  "def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float), \\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "def class_num(self, class_num):\n        \"\"\"\n        class_num for postprocessing, default is 80\n        \"\"\"\n        assert isinstance(class_num, int), \\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.class_num = class_num",
  "def __init__(self,\n                 model_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.RKNN):\n        \"\"\"Load a RKYOLOV5 model exported by RKYOLOV5.\n\n        :param model_file: (str)Path of model file, e.g ./yolov5.rknn\n        :param params_file: (str)Path of parameters file, e.g , if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(RKYOLOV5, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.RKYOLOV5(\n            model_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"RKYOLOV5 initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        self.postprocessor.conf_threshold = conf_threshold\n        self.postprocessor.nms_threshold = nms_iou_threshold\n        return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get RKYOLOV5Preprocessor object of the loaded model\n\n        :return RKYOLOV5Preprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get RKYOLOV5Postprocessor object of the loaded model\n\n        :return RKYOLOV5Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self,\n                 model_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.RKNN):\n        \"\"\"Load a RKYOLOX model exported by RKYOLOX.\n\n        :param model_file: (str)Path of model file, e.g ./yolox.rknn\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(RKYOLOX, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.RKYOLOX(\n            model_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"RKYOLOV5 initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        self.postprocessor.conf_threshold = conf_threshold\n        self.postprocessor.nms_threshold = nms_iou_threshold\n        return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get RKYOLOV5Preprocessor object of the loaded model\n\n        :return RKYOLOV5Preprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get RKYOLOV5Postprocessor object of the loaded model\n\n        :return RKYOLOV5Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self,\n                 model_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.RKNN):\n        \"\"\"Load a RKYOLOX model exported by RKYOLOV7.\n\n        :param model_file: (str)Path of model file, e.g ./yolov7.rknn\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(RKYOLOV7, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.RKYOLOV7(\n            model_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"RKYOLOV5 initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: DetectionResult\n        \"\"\"\n\n        self.postprocessor.conf_threshold = conf_threshold\n        self.postprocessor.nms_threshold = nms_iou_threshold\n        return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get RKYOLOV5Preprocessor object of the loaded model\n\n        :return RKYOLOV5Preprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get RKYOLOV5Postprocessor object of the loaded model\n\n        :return RKYOLOV5Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class PaddleDetPreprocessor(ProcessorManager):\n    def __init__(self, config_file):\n        \"\"\"Create a preprocessor for PaddleDetection Model from configuration file\n\n        :param config_file: (str)Path of configuration file, e.g ppyoloe/infer_cfg.yml\n        \"\"\"\n        self._manager = C.vision.detection.PaddleDetPreprocessor(config_file)\n\n    def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()\n\n    def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()",
  "class NMSOption:\n    def __init__(self):\n        self.nms_option = C.vision.detection.NMSOption()\n\n    @property\n    def background_label(self):\n        return self.nms_option.background_label",
  "class NMSRotatedOption:\n    def __init__(self):\n        self.nms_rotated_option = C.vision.detection.NMSRotatedOption()\n\n    @property\n    def background_label(self):\n        return self.nms_rotated_option.background_label",
  "class PaddleDetPostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for PaddleDetection Model\n\n        \"\"\"\n        self._postprocessor = C.vision.detection.PaddleDetPostprocessor()\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for PaddleDetection Model\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of ClassifyResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)\n\n    def apply_nms(self):\n        self._postprocessor.apply_nms()\n\n    def set_nms_option(self, nms_option=None):\n        \"\"\"This function will enable decode and nms in postprocess step.\n        \"\"\"\n        if nms_option is None:\n            nms_option = NMSOption()\n        self._postprocessor.set_nms_option(self, nms_option.nms_option)\n\n    def set_nms_rotated_option(self, nms_rotated_option=None):\n        \"\"\"This function will enable decode and rotated nms in postprocess step.\n        \"\"\"\n        if nms_rotated_option is None:\n            nms_rotated_option = NMSRotatedOption()\n        self._postprocessor.set_nms_rotated_option(\n            self, nms_rotated_option.nms_rotated_option)",
  "class PPYOLOE(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPYOLOE model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ppyoloe/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyoloe/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.PPYOLOE(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPYOLOE model initialize failed.\"\n\n    def predict(self, im):\n        \"\"\"Detect an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: DetectionResult\n        \"\"\"\n\n        assert im is not None, \"The input image data is None.\"\n        return self._model.predict(im)\n\n    def batch_predict(self, images):\n        \"\"\"Detect a batch of input image list\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    def clone(self):\n        \"\"\"Clone PPYOLOE object\n\n        :return: a new PPYOLOE object\n        \"\"\"\n\n        class PPYOLOEClone(PPYOLOE):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PPYOLOEClone(self._model.clone())\n        return clone_model\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get PaddleDetPreprocessor object of the loaded model\n\n        :return PaddleDetPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get PaddleDetPostprocessor object of the loaded model\n\n        :return PaddleDetPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class PPYOLO(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPYOLO model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ppyolo/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyolo/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPYOLO model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PPYOLO(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPYOLO model initialize failed.\"\n\n    def clone(self):\n        \"\"\"Clone PPYOLO object\n\n        :return: a new PPYOLO object\n        \"\"\"\n\n        class PPYOLOClone(PPYOLO):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PPYOLOClone(self._model.clone())\n        return clone_model",
  "class PaddleYOLOX(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOX model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolox/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PaddleYOLOX model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PaddleYOLOX(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleYOLOX model initialize failed.\"\n\n    def clone(self):\n        \"\"\"Clone PaddleYOLOX object\n\n        :return: a new PaddleYOLOX object\n        \"\"\"\n\n        class PaddleYOLOXClone(PaddleYOLOX):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PaddleYOLOXClone(self._model.clone())\n        return clone_model",
  "class PicoDet(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PicoDet model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g picodet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g picodet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.PicoDet(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PicoDet model initialize failed.\"\n\n    def clone(self):\n        \"\"\"Clone PicoDet object\n\n        :return: a new PicoDet object\n        \"\"\"\n\n        class PicoDetClone(PicoDet):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PicoDetClone(self._model.clone())\n        return clone_model",
  "class FasterRCNN(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a FasterRCNN model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g fasterrcnn/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g fasterrcnn/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"FasterRCNN model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.FasterRCNN(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"FasterRCNN model initialize failed.\"\n\n    def clone(self):\n        \"\"\"Clone FasterRCNN object\n\n        :return: a new FasterRCNN object\n        \"\"\"\n\n        class FasterRCNNClone(FasterRCNN):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = FasterRCNNClone(self._model.clone())\n        return clone_model",
  "class YOLOv3(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOv3 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolov3/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolov3/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"YOLOv3 model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.YOLOv3(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"YOLOv3 model initialize failed.\"\n\n    def clone(self):\n        \"\"\"Clone YOLOv3 object\n\n        :return: a new YOLOv3 object\n        \"\"\"\n\n        class YOLOv3Clone(YOLOv3):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = YOLOv3Clone(self._model.clone())\n        return clone_model",
  "class SOLOv2(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a SOLOv2 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g solov2/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g solov2/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g solov2/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"SOLOv2 model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.SOLOv2(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"SOLOv2 model initialize failed.\"\n\n    def clone(self):\n        \"\"\"Clone SOLOv2 object\n\n        :return: a new SOLOv2 object\n        \"\"\"\n\n        class SOLOv2Clone(SOLOv2):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = SOLOv2Clone(self._model.clone())\n        return clone_model",
  "class MaskRCNN(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a MaskRCNN model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g fasterrcnn/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g fasterrcnn/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"MaskRCNN model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.MaskRCNN(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"MaskRCNN model initialize failed.\"\n\n    def batch_predict(self, images):\n        \"\"\"Detect a batch of input image list, batch_predict is not supported for maskrcnn now.\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        raise Exception(\n            \"batch_predict is not supported for MaskRCNN model now.\")\n\n    def clone(self):\n        \"\"\"Clone MaskRCNN object\n\n        :return: a new MaskRCNN object\n        \"\"\"\n\n        class MaskRCNNClone(MaskRCNN):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = MaskRCNNClone(self._model.clone())\n        return clone_model",
  "class SSD(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a SSD model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ssd/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ssd/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"SSD model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.SSD(model_file, params_file,\n                                             config_file, self._runtime_option,\n                                             model_format)\n        assert self.initialized, \"SSD model initialize failed.\"\n\n    def clone(self):\n        \"\"\"Clone SSD object\n\n        :return: a new SSD object\n        \"\"\"\n\n        class SSDClone(SSD):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = SSDClone(self._model.clone())\n        return clone_model",
  "class PaddleYOLOv5(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOv5 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolov5/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolov5/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PaddleYOLOv5 model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PaddleYOLOv5(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleYOLOv5 model initialize failed.\"",
  "class PaddleYOLOv6(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOv6 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolov6/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolov6/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PaddleYOLOv6 model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PaddleYOLOv6(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleYOLOv6 model initialize failed.\"",
  "class PaddleYOLOv7(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOv7 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolov7/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolov7/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PaddleYOLOv7 model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PaddleYOLOv7(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleYOLOv7 model initialize failed.\"",
  "class PaddleYOLOv8(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOv8 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolov8/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolov8/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g yolov8/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.PaddleYOLOv8(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleYOLOv8 model initialize failed.\"",
  "class RTMDet(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a RTMDet model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g rtmdet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g rtmdet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"RTMDet model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.RTMDet(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"RTMDet model initialize failed.\"",
  "class CascadeRCNN(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a CascadeRCNN model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g cascadercnn/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g cascadercnn/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"CascadeRCNN model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.CascadeRCNN(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"CascadeRCNN model initialize failed.\"",
  "class PSSDet(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PSSDet model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g pssdet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g pssdet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PSSDet model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PSSDet(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PSSDet model initialize failed.\"",
  "class RetinaNet(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a RetinaNet model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g retinanet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g retinanet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"RetinaNet model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.RetinaNet(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"RetinaNet model initialize failed.\"",
  "class PPYOLOESOD(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPYOLOESOD model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ppyoloesod/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyoloesod/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPYOLOESOD model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PPYOLOESOD(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPYOLOESOD model initialize failed.\"",
  "class FCOS(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a FCOS model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g fcos/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g fcos/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"FCOS model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.FCOS(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"FCOS model initialize failed.\"",
  "class TTFNet(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a TTFNet model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ttfnet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ttfnet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"TTFNet model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.TTFNet(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"TTFNet model initialize failed.\"",
  "class TOOD(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a TOOD model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g tood/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g tood/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"TOOD model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.TOOD(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"TOOD model initialize failed.\"",
  "class GFL(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a GFL model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g gfl/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g gfl/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"GFL model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.GFL(model_file, params_file,\n                                             config_file, self._runtime_option,\n                                             model_format)\n        assert self.initialized, \"GFL model initialize failed.\"",
  "class PaddleDetectionModel(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PaddleDetectionModel model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ppyoloe/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyoloe/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PaddleDetectionModel, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.PaddleDetectionModel(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleDetectionModel model initialize failed.\"\n\n    def predict(self, im):\n        \"\"\"Detect an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: DetectionResult\n        \"\"\"\n\n        assert im is not None, \"The input image data is None.\"\n        return self._model.predict(im)\n\n    def batch_predict(self, images):\n        \"\"\"Detect a batch of input image list\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    def clone(self):\n        \"\"\"Clone PPYOLOE object\n\n        :return: a new PPYOLOE object\n        \"\"\"\n\n        class PPYOLOEClone(PPYOLOE):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PPYOLOEClone(self._model.clone())\n        return clone_model\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get PaddleDetPreprocessor object of the loaded model\n\n        :return PaddleDetPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get PaddleDetPostprocessor object of the loaded model\n\n        :return PaddleDetPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class PPYOLOER(PPYOLOE):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPYOLOER model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ppyoloe_r/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyoloe_r/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe_r/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.PPYOLOER(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PicoDet model initialize failed.\"\n\n    def clone(self):\n        \"\"\"Clone PPYOLOER object\n\n        :return: a new PPYOLOER object\n        \"\"\"\n\n        class PPYOLOERClone(PPYOLOER):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PPYOLOERClone(self._model.clone())\n        return clone_model",
  "def __init__(self, config_file):\n        \"\"\"Create a preprocessor for PaddleDetection Model from configuration file\n\n        :param config_file: (str)Path of configuration file, e.g ppyoloe/infer_cfg.yml\n        \"\"\"\n        self._manager = C.vision.detection.PaddleDetPreprocessor(config_file)",
  "def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()",
  "def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()",
  "def __init__(self):\n        self.nms_option = C.vision.detection.NMSOption()",
  "def background_label(self):\n        return self.nms_option.background_label",
  "def __init__(self):\n        self.nms_rotated_option = C.vision.detection.NMSRotatedOption()",
  "def background_label(self):\n        return self.nms_rotated_option.background_label",
  "def __init__(self):\n        \"\"\"Create a postprocessor for PaddleDetection Model\n\n        \"\"\"\n        self._postprocessor = C.vision.detection.PaddleDetPostprocessor()",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for PaddleDetection Model\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of ClassifyResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def apply_nms(self):\n        self._postprocessor.apply_nms()",
  "def set_nms_option(self, nms_option=None):\n        \"\"\"This function will enable decode and nms in postprocess step.\n        \"\"\"\n        if nms_option is None:\n            nms_option = NMSOption()\n        self._postprocessor.set_nms_option(self, nms_option.nms_option)",
  "def set_nms_rotated_option(self, nms_rotated_option=None):\n        \"\"\"This function will enable decode and rotated nms in postprocess step.\n        \"\"\"\n        if nms_rotated_option is None:\n            nms_rotated_option = NMSRotatedOption()\n        self._postprocessor.set_nms_rotated_option(\n            self, nms_rotated_option.nms_rotated_option)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPYOLOE model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ppyoloe/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyoloe/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.PPYOLOE(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPYOLOE model initialize failed.\"",
  "def predict(self, im):\n        \"\"\"Detect an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: DetectionResult\n        \"\"\"\n\n        assert im is not None, \"The input image data is None.\"\n        return self._model.predict(im)",
  "def batch_predict(self, images):\n        \"\"\"Detect a batch of input image list\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def clone(self):\n        \"\"\"Clone PPYOLOE object\n\n        :return: a new PPYOLOE object\n        \"\"\"\n\n        class PPYOLOEClone(PPYOLOE):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PPYOLOEClone(self._model.clone())\n        return clone_model",
  "def preprocessor(self):\n        \"\"\"Get PaddleDetPreprocessor object of the loaded model\n\n        :return PaddleDetPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get PaddleDetPostprocessor object of the loaded model\n\n        :return PaddleDetPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPYOLO model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ppyolo/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyolo/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPYOLO model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PPYOLO(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPYOLO model initialize failed.\"",
  "def clone(self):\n        \"\"\"Clone PPYOLO object\n\n        :return: a new PPYOLO object\n        \"\"\"\n\n        class PPYOLOClone(PPYOLO):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PPYOLOClone(self._model.clone())\n        return clone_model",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOX model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolox/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PaddleYOLOX model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PaddleYOLOX(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleYOLOX model initialize failed.\"",
  "def clone(self):\n        \"\"\"Clone PaddleYOLOX object\n\n        :return: a new PaddleYOLOX object\n        \"\"\"\n\n        class PaddleYOLOXClone(PaddleYOLOX):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PaddleYOLOXClone(self._model.clone())\n        return clone_model",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PicoDet model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g picodet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g picodet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.PicoDet(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PicoDet model initialize failed.\"",
  "def clone(self):\n        \"\"\"Clone PicoDet object\n\n        :return: a new PicoDet object\n        \"\"\"\n\n        class PicoDetClone(PicoDet):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PicoDetClone(self._model.clone())\n        return clone_model",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a FasterRCNN model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g fasterrcnn/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g fasterrcnn/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"FasterRCNN model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.FasterRCNN(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"FasterRCNN model initialize failed.\"",
  "def clone(self):\n        \"\"\"Clone FasterRCNN object\n\n        :return: a new FasterRCNN object\n        \"\"\"\n\n        class FasterRCNNClone(FasterRCNN):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = FasterRCNNClone(self._model.clone())\n        return clone_model",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOv3 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolov3/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolov3/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"YOLOv3 model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.YOLOv3(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"YOLOv3 model initialize failed.\"",
  "def clone(self):\n        \"\"\"Clone YOLOv3 object\n\n        :return: a new YOLOv3 object\n        \"\"\"\n\n        class YOLOv3Clone(YOLOv3):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = YOLOv3Clone(self._model.clone())\n        return clone_model",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a SOLOv2 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g solov2/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g solov2/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g solov2/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"SOLOv2 model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.SOLOv2(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"SOLOv2 model initialize failed.\"",
  "def clone(self):\n        \"\"\"Clone SOLOv2 object\n\n        :return: a new SOLOv2 object\n        \"\"\"\n\n        class SOLOv2Clone(SOLOv2):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = SOLOv2Clone(self._model.clone())\n        return clone_model",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a MaskRCNN model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g fasterrcnn/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g fasterrcnn/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"MaskRCNN model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.MaskRCNN(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"MaskRCNN model initialize failed.\"",
  "def batch_predict(self, images):\n        \"\"\"Detect a batch of input image list, batch_predict is not supported for maskrcnn now.\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        raise Exception(\n            \"batch_predict is not supported for MaskRCNN model now.\")",
  "def clone(self):\n        \"\"\"Clone MaskRCNN object\n\n        :return: a new MaskRCNN object\n        \"\"\"\n\n        class MaskRCNNClone(MaskRCNN):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = MaskRCNNClone(self._model.clone())\n        return clone_model",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a SSD model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ssd/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ssd/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"SSD model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.SSD(model_file, params_file,\n                                             config_file, self._runtime_option,\n                                             model_format)\n        assert self.initialized, \"SSD model initialize failed.\"",
  "def clone(self):\n        \"\"\"Clone SSD object\n\n        :return: a new SSD object\n        \"\"\"\n\n        class SSDClone(SSD):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = SSDClone(self._model.clone())\n        return clone_model",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOv5 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolov5/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolov5/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PaddleYOLOv5 model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PaddleYOLOv5(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleYOLOv5 model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOv6 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolov6/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolov6/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PaddleYOLOv6 model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PaddleYOLOv6(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleYOLOv6 model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOv7 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolov7/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolov7/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PaddleYOLOv7 model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PaddleYOLOv7(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleYOLOv7 model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a YOLOv8 model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g yolov8/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g yolov8/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g yolov8/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.PaddleYOLOv8(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleYOLOv8 model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a RTMDet model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g rtmdet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g rtmdet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"RTMDet model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.RTMDet(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"RTMDet model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a CascadeRCNN model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g cascadercnn/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g cascadercnn/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"CascadeRCNN model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.CascadeRCNN(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"CascadeRCNN model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PSSDet model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g pssdet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g pssdet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PSSDet model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PSSDet(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PSSDet model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a RetinaNet model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g retinanet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g retinanet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"RetinaNet model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.RetinaNet(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"RetinaNet model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPYOLOESOD model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ppyoloesod/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyoloesod/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPYOLOESOD model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.PPYOLOESOD(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPYOLOESOD model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a FCOS model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g fcos/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g fcos/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"FCOS model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.FCOS(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"FCOS model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a TTFNet model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ttfnet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ttfnet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"TTFNet model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.TTFNet(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"TTFNet model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a TOOD model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g tood/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g tood/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"TOOD model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.TOOD(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"TOOD model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a GFL model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g gfl/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g gfl/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"GFL model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.detection.GFL(model_file, params_file,\n                                             config_file, self._runtime_option,\n                                             model_format)\n        assert self.initialized, \"GFL model initialize failed.\"",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PaddleDetectionModel model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ppyoloe/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyoloe/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PaddleDetectionModel, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.PaddleDetectionModel(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleDetectionModel model initialize failed.\"",
  "def predict(self, im):\n        \"\"\"Detect an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: DetectionResult\n        \"\"\"\n\n        assert im is not None, \"The input image data is None.\"\n        return self._model.predict(im)",
  "def batch_predict(self, images):\n        \"\"\"Detect a batch of input image list\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def clone(self):\n        \"\"\"Clone PPYOLOE object\n\n        :return: a new PPYOLOE object\n        \"\"\"\n\n        class PPYOLOEClone(PPYOLOE):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PPYOLOEClone(self._model.clone())\n        return clone_model",
  "def preprocessor(self):\n        \"\"\"Get PaddleDetPreprocessor object of the loaded model\n\n        :return PaddleDetPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get PaddleDetPostprocessor object of the loaded model\n\n        :return PaddleDetPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPYOLOER model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g ppyoloe_r/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyoloe_r/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe_r/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPYOLOE, self).__init__(runtime_option)\n\n        self._model = C.vision.detection.PPYOLOER(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PicoDet model initialize failed.\"",
  "def clone(self):\n        \"\"\"Clone PPYOLOER object\n\n        :return: a new PPYOLOER object\n        \"\"\"\n\n        class PPYOLOERClone(PPYOLOER):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PPYOLOERClone(self._model.clone())\n        return clone_model",
  "class PPYOLOEClone(PPYOLOE):\n            def __init__(self, model):\n                self._model = model",
  "class PPYOLOClone(PPYOLO):\n            def __init__(self, model):\n                self._model = model",
  "class PaddleYOLOXClone(PaddleYOLOX):\n            def __init__(self, model):\n                self._model = model",
  "class PicoDetClone(PicoDet):\n            def __init__(self, model):\n                self._model = model",
  "class FasterRCNNClone(FasterRCNN):\n            def __init__(self, model):\n                self._model = model",
  "class YOLOv3Clone(YOLOv3):\n            def __init__(self, model):\n                self._model = model",
  "class SOLOv2Clone(SOLOv2):\n            def __init__(self, model):\n                self._model = model",
  "class MaskRCNNClone(MaskRCNN):\n            def __init__(self, model):\n                self._model = model",
  "class SSDClone(SSD):\n            def __init__(self, model):\n                self._model = model",
  "class PPYOLOEClone(PPYOLOE):\n            def __init__(self, model):\n                self._model = model",
  "class PPYOLOERClone(PPYOLOER):\n            def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "class FSANet(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a headpose model exported by FSANet.\n\n        :param model_file: (str)Path of model file, e.g fsanet/fsanet-var.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n\n        super(FSANet, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"FSANet only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.headpose.FSANet(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"FSANet initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image headpose\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: HeadPoseResult\n        \"\"\"\n\n        return self._model.predict(input_image)\n\n    @property\n    def size(self):\n        \"\"\"\n        Returns the preprocess image size, default (64, 64)\n        \"\"\"\n        return self._model.size\n\n    @size.setter\n    def size(self, wh):\n        \"\"\"\n        Set the preprocess image size, default (64, 64)\n        \"\"\"\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a headpose model exported by FSANet.\n\n        :param model_file: (str)Path of model file, e.g fsanet/fsanet-var.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n\n        super(FSANet, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"FSANet only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.headpose.FSANet(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"FSANet initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Predict an input image headpose\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: HeadPoseResult\n        \"\"\"\n\n        return self._model.predict(input_image)",
  "def size(self):\n        \"\"\"\n        Returns the preprocess image size, default (64, 64)\n        \"\"\"\n        return self._model.size",
  "def size(self, wh):\n        \"\"\"\n        Set the preprocess image size, default (64, 64)\n        \"\"\"\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "class AnimeGANPreprocessor:\n    def __init__(self, config_file):\n        \"\"\"Create a preprocessor for AnimeGAN.\n        \"\"\"\n        self._preprocessor = C.vision.generation.AnimeGANPreprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for AnimeGAN.\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "class AnimeGANPostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for AnimeGAN.\n        \"\"\"\n        self._postprocessor = C.vision.generation.AnimeGANPostprocessor()\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for AnimeGAN\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: results: (list) Final results\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "class AnimeGAN(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a AnimeGAN model.\n\n        :param model_file: (str)Path of model file, e.g ./model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ./model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # call super constructor to initialize self._runtime_option\n        super(AnimeGAN, self).__init__(runtime_option)\n\n        self._model = C.vision.generation.AnimeGAN(\n            model_file, params_file, self._runtime_option, model_format)\n        # assert self.initialized to confirm initialization successfully.\n        assert self.initialized, \"AnimeGAN initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\" Predict the style transfer result for an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: style transfer result\n        \"\"\"\n        return self._model.predict(input_image)\n\n    def batch_predict(self, input_images):\n        \"\"\" Predict the style transfer result for multiple input images\n\n        :param input_images: (list of numpy.ndarray)The list of input image data, each image is a 3-D array with layout HWC, BGR format\n        :return: a list of style transfer results\n        \"\"\"\n        return self._model.batch_predict(input_images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get AnimeGANPreprocessor object of the loaded model\n\n        :return AnimeGANPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get AnimeGANPostprocessor object of the loaded model\n\n        :return AnimeGANPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self, config_file):\n        \"\"\"Create a preprocessor for AnimeGAN.\n        \"\"\"\n        self._preprocessor = C.vision.generation.AnimeGANPreprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for AnimeGAN.\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def __init__(self):\n        \"\"\"Create a postprocessor for AnimeGAN.\n        \"\"\"\n        self._postprocessor = C.vision.generation.AnimeGANPostprocessor()",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for AnimeGAN\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: results: (list) Final results\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a AnimeGAN model.\n\n        :param model_file: (str)Path of model file, e.g ./model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ./model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # call super constructor to initialize self._runtime_option\n        super(AnimeGAN, self).__init__(runtime_option)\n\n        self._model = C.vision.generation.AnimeGAN(\n            model_file, params_file, self._runtime_option, model_format)\n        # assert self.initialized to confirm initialization successfully.\n        assert self.initialized, \"AnimeGAN initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\" Predict the style transfer result for an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: style transfer result\n        \"\"\"\n        return self._model.predict(input_image)",
  "def batch_predict(self, input_images):\n        \"\"\" Predict the style transfer result for multiple input images\n\n        :param input_images: (list of numpy.ndarray)The list of input image data, each image is a 3-D array with layout HWC, BGR format\n        :return: a list of style transfer results\n        \"\"\"\n        return self._model.batch_predict(input_images)",
  "def preprocessor(self):\n        \"\"\"Get AnimeGANPreprocessor object of the loaded model\n\n        :return AnimeGANPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get AnimeGANPostprocessor object of the loaded model\n\n        :return AnimeGANPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class FaceLandmark1000(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a face alignment model exported by FaceLandmark1000.\n\n        :param model_file: (str)Path of model file, e.g ./FaceLandmark1000.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n\n        super(FaceLandmark1000, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"FaceLandmark1000 only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.facealign.FaceLandmark1000(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"FaceLandmark1000 initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Detect an input image landmarks\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: FaceAlignmentResult\n        \"\"\"\n\n        return self._model.predict(input_image)\n\n    @property\n    def size(self):\n        \"\"\"\n        Returns the preprocess image size, default (128, 128)\n        \"\"\"\n        return self._model.size\n\n    @size.setter\n    def size(self, wh):\n        \"\"\"\n        Set the preprocess image size, default (128, 128)\n        \"\"\"\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a face alignment model exported by FaceLandmark1000.\n\n        :param model_file: (str)Path of model file, e.g ./FaceLandmark1000.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n\n        super(FaceLandmark1000, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"FaceLandmark1000 only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.facealign.FaceLandmark1000(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"FaceLandmark1000 initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Detect an input image landmarks\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: FaceAlignmentResult\n        \"\"\"\n\n        return self._model.predict(input_image)",
  "def size(self):\n        \"\"\"\n        Returns the preprocess image size, default (128, 128)\n        \"\"\"\n        return self._model.size",
  "def size(self, wh):\n        \"\"\"\n        Set the preprocess image size, default (128, 128)\n        \"\"\"\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "class PIPNet(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a face alignment model exported by PIPNet.\n\n        :param model_file: (str)Path of model file, e.g ./PIPNet.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n\n        super(PIPNet, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"PIPNet only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.facealign.PIPNet(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"PIPNet initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Detect an input image landmarks\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: FaceAlignmentResult\n        \"\"\"\n\n        return self._model.predict(input_image)\n\n    @property\n    def size(self):\n        \"\"\"\n        Returns the preprocess image size, default (256, 256)\n        \"\"\"\n        return self._model.size\n\n    @property\n    def mean_vals(self):\n        \"\"\"\n        Returns the mean value of normlization, default mean_vals = [0.485f, 0.456f, 0.406f];\n        \"\"\"\n        return self._model.mean_vals\n\n    @property\n    def std_vals(self):\n        \"\"\"\n        Returns the std value of normlization, default std_vals = [0.229f, 0.224f, 0.225f];\n        \"\"\"\n        return self._model.std_vals\n\n    @property\n    def num_landmarks(self):\n        \"\"\"\n        Returns the number of landmarks\n        \"\"\"\n        return self._model.num_landmarks\n\n    @size.setter\n    def size(self, wh):\n        \"\"\"\n        Set the preprocess image size, default (256, 256)\n        \"\"\"\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @mean_vals.setter\n    def mean_vals(self, value):\n        assert isinstance(\n            value, list), \"The value to set `mean_vals` must be type of list.\"\n        self._model.mean_vals = value\n\n    @std_vals.setter\n    def std_vals(self, value):\n        assert isinstance(\n            value, list), \"The value to set `std_vals` must be type of list.\"\n        self._model.std_vals = value\n\n    @num_landmarks.setter\n    def num_landmarks(self, value):\n        assert isinstance(\n            value, int), \"The value to set `std_vals` must be type of int.\"\n        self._model.num_landmarks = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a face alignment model exported by PIPNet.\n\n        :param model_file: (str)Path of model file, e.g ./PIPNet.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n\n        super(PIPNet, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"PIPNet only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.facealign.PIPNet(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"PIPNet initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Detect an input image landmarks\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: FaceAlignmentResult\n        \"\"\"\n\n        return self._model.predict(input_image)",
  "def size(self):\n        \"\"\"\n        Returns the preprocess image size, default (256, 256)\n        \"\"\"\n        return self._model.size",
  "def mean_vals(self):\n        \"\"\"\n        Returns the mean value of normlization, default mean_vals = [0.485f, 0.456f, 0.406f];\n        \"\"\"\n        return self._model.mean_vals",
  "def std_vals(self):\n        \"\"\"\n        Returns the std value of normlization, default std_vals = [0.229f, 0.224f, 0.225f];\n        \"\"\"\n        return self._model.std_vals",
  "def num_landmarks(self):\n        \"\"\"\n        Returns the number of landmarks\n        \"\"\"\n        return self._model.num_landmarks",
  "def size(self, wh):\n        \"\"\"\n        Set the preprocess image size, default (256, 256)\n        \"\"\"\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def mean_vals(self, value):\n        assert isinstance(\n            value, list), \"The value to set `mean_vals` must be type of list.\"\n        self._model.mean_vals = value",
  "def std_vals(self, value):\n        assert isinstance(\n            value, list), \"The value to set `std_vals` must be type of list.\"\n        self._model.std_vals = value",
  "def num_landmarks(self, value):\n        assert isinstance(\n            value, int), \"The value to set `std_vals` must be type of int.\"\n        self._model.num_landmarks = value",
  "class PFLD(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a face alignment model exported by PFLD.\n\n        :param model_file: (str)Path of model file, e.g pfld/pfld-106-v3.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n\n        super(PFLD, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"PFLD only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.facealign.PFLD(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"PFLD initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Detect an input image landmarks\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: FaceAlignmentResult\n        \"\"\"\n\n        return self._model.predict(input_image)\n\n    @property\n    def size(self):\n        \"\"\"\n        Returns the preprocess image size, default (112, 112)\n        \"\"\"\n        return self._model.size\n\n    @size.setter\n    def size(self, wh):\n        \"\"\"\n        Set the preprocess image size, default (112, 112)\n        \"\"\"\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a face alignment model exported by PFLD.\n\n        :param model_file: (str)Path of model file, e.g pfld/pfld-106-v3.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n\n        super(PFLD, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"PFLD only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.facealign.PFLD(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"PFLD initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Detect an input image landmarks\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: FaceAlignmentResult\n        \"\"\"\n\n        return self._model.predict(input_image)",
  "def size(self):\n        \"\"\"\n        Returns the preprocess image size, default (112, 112)\n        \"\"\"\n        return self._model.size",
  "def size(self, wh):\n        \"\"\"\n        Set the preprocess image size, default (112, 112)\n        \"\"\"\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def eval_segmentation(model, data_dir, batch_size=1):\n    import cv2\n    from .utils import Cityscapes\n    from .utils import f1_score, calculate_area, mean_iou, accuracy, kappa\n    assert os.path.isdir(\n        data_dir), \"The image_file_path:{} is not a directory.\".format(\n            data_dir)\n    eval_dataset = Cityscapes(dataset_root=data_dir, mode=\"val\")\n    file_list = eval_dataset.file_list\n    image_num = eval_dataset.num_samples\n    num_classes = eval_dataset.num_classes\n    intersect_area_all = 0\n    pred_area_all = 0\n    label_area_all = 0\n    conf_mat_all = []\n    twenty_percent_image_num = math.ceil(image_num * 0.2)\n    start_time = 0\n    end_time = 0\n    average_inference_time = 0\n    im_list = []\n    label_list = []\n    for image_label_path, i in zip(file_list,\n                                   trange(\n                                       image_num, desc=\"Inference Progress\")):\n        if i == twenty_percent_image_num:\n            start_time = time.time()\n        im = cv2.imread(image_label_path[0])\n        label = cv2.imread(image_label_path[1], cv2.IMREAD_GRAYSCALE)\n        label_list.append(label)\n        if batch_size == 1:\n            result = model.predict(im)\n            results = [result]\n        else:\n            im_list.append(im)\n            # If the batch_size is not satisfied, the remaining pictures are formed into a batch\n            if (i + 1) % batch_size != 0 and i != image_num - 1:\n                continue\n            results = model.batch_predict(im_list)\n        if i == image_num - 1:\n            end_time = time.time()\n            average_inference_time = round(\n                (end_time - start_time) /\n                (image_num - twenty_percent_image_num), 4)\n        for result, label in zip(results, label_list):\n            pred = np.array(result.label_map).reshape(result.shape[0],\n                                                      result.shape[1])\n            intersect_area, pred_area, label_area = calculate_area(pred, label,\n                                                                   num_classes)\n            intersect_area_all = intersect_area_all + intersect_area\n            pred_area_all = pred_area_all + pred_area\n            label_area_all = label_area_all + label_area\n        im_list.clear()\n        label_list.clear()\n\n    class_iou, miou = mean_iou(intersect_area_all, pred_area_all,\n                               label_area_all)\n    class_acc, oacc = accuracy(intersect_area_all, pred_area_all)\n    kappa_res = kappa(intersect_area_all, pred_area_all, label_area_all)\n    category_f1score = f1_score(intersect_area_all, pred_area_all,\n                                label_area_all)\n\n    eval_metrics = collections.OrderedDict(\n        zip([\n            'miou', 'category_iou', 'oacc', 'category_acc', 'kappa',\n            'category_F1-score', 'average_inference_time(s)'\n        ], [\n            miou, class_iou, oacc, class_acc, kappa_res, category_f1score,\n            average_inference_time\n        ]))\n    return eval_metrics",
  "def eval_detection(model,\n                   data_dir,\n                   ann_file,\n                   conf_threshold=None,\n                   nms_iou_threshold=None,\n                   plot=False,\n                   batch_size=1):\n    from .utils import CocoDetection\n    from .utils import COCOMetric\n    import cv2\n    from tqdm import trange\n    import time\n\n    if conf_threshold is not None or nms_iou_threshold is not None:\n        assert conf_threshold is not None and nms_iou_threshold is not None, \"The conf_threshold and nms_iou_threshold should be setted at the same time\"\n        assert isinstance(conf_threshold, (\n            float,\n            int)), \"The conf_threshold:{} need to be int or float\".format(\n                conf_threshold)\n        assert isinstance(nms_iou_threshold, (\n            float,\n            int)), \"The nms_iou_threshold:{} need to be int or float\".format(\n                nms_iou_threshold)\n    eval_dataset = CocoDetection(\n        data_dir=data_dir, ann_file=ann_file, shuffle=False)\n    all_image_info = eval_dataset.file_list\n    image_num = eval_dataset.num_samples\n    eval_dataset.data_fields = {\n        'im_id', 'image_shape', 'image', 'gt_bbox', 'gt_class', 'is_crowd'\n    }\n    eval_metric = COCOMetric(\n        coco_gt=copy.deepcopy(eval_dataset.coco_gt), classwise=False)\n    scores = collections.OrderedDict()\n    twenty_percent_image_num = math.ceil(image_num * 0.2)\n    start_time = 0\n    end_time = 0\n    average_inference_time = 0\n    im_list = list()\n    im_id_list = list()\n    for image_info, i in zip(all_image_info,\n                             trange(\n                                 image_num, desc=\"Inference Progress\")):\n        if i == twenty_percent_image_num:\n            start_time = time.time()\n        im = cv2.imread(image_info[\"image\"])\n        im_id = image_info[\"im_id\"]\n        if batch_size == 1:\n            if conf_threshold is None and nms_iou_threshold is None:\n                result = model.predict(im.copy())\n            else:\n                result = model.predict(im, conf_threshold, nms_iou_threshold)\n            pred = {\n                'bbox': [[c] + [s] + b\n                         for b, s, c in zip(result.boxes, result.scores,\n                                            result.label_ids)],\n                'bbox_num': len(result.boxes),\n                'im_id': im_id\n            }\n            eval_metric.update(im_id, pred)\n        else:\n            im_list.append(im)\n            im_id_list.append(im_id)\n            # If the batch_size is not satisfied, the remaining pictures are formed into a batch\n            if (i + 1) % batch_size != 0 and i != image_num - 1:\n                continue\n            if conf_threshold is None and nms_iou_threshold is None:\n                results = model.batch_predict(im_list)\n            else:\n                model.postprocessor.conf_threshold = conf_threshold\n                model.postprocessor.nms_threshold = nms_iou_threshold\n                results = model.batch_predict(im_list)\n            for k in range(len(im_list)):\n                pred = {\n                    'bbox': [[c] + [s] + b\n                             for b, s, c in zip(results[k].boxes, results[\n                                 k].scores, results[k].label_ids)],\n                    'bbox_num': len(results[k].boxes),\n                    'im_id': im_id_list[k]\n                }\n                eval_metric.update(im_id_list[k], pred)\n            im_list.clear()\n            im_id_list.clear()\n\n        if i == image_num - 1:\n            end_time = time.time()\n    average_inference_time = round(\n        (end_time - start_time) / (image_num - twenty_percent_image_num), 4)\n    eval_metric.accumulate()\n    eval_details = eval_metric.details\n    scores.update(eval_metric.get())\n    scores.update({'average_inference_time(s)': average_inference_time})\n    eval_metric.reset()\n    return scores",
  "def topk_accuracy(topk_list, label_list):\n    match_array = np.logical_or.reduce(topk_list == label_list, axis=1)\n    topk_acc_score = match_array.sum() / match_array.shape[0]\n    return topk_acc_score",
  "def eval_classify(model, image_file_path, label_file_path, topk=5):\n    from tqdm import trange\n    import cv2\n    import math\n\n    result_list = []\n    label_list = []\n    image_label_dict = {}\n    assert os.path.isdir(\n        image_file_path), \"The image_file_path:{} is not a directory.\".format(\n            image_file_path)\n    assert os.path.isfile(\n        label_file_path), \"The label_file_path:{} is not a file.\".format(\n            label_file_path)\n    assert isinstance(topk, int), \"The tok:{} is not int type\".format(topk)\n\n    with open(label_file_path, 'r') as file:\n        lines = file.readlines()\n        for line in lines:\n            items = line.strip().split()\n            image_name = items[0]\n            label = items[1]\n            image_label_dict[image_name] = int(label)\n    images_num = len(image_label_dict)\n    twenty_percent_images_num = math.ceil(images_num * 0.2)\n    start_time = 0\n    end_time = 0\n    average_inference_time = 0\n    scores = collections.OrderedDict()\n    for (image, label), i in zip(image_label_dict.items(),\n                                 trange(\n                                     images_num, desc='Inference Progress')):\n        if i == twenty_percent_images_num:\n            start_time = time.time()\n\n        label_list.append([label])\n        image_path = os.path.join(image_file_path, image)\n        im = cv2.imread(image_path)\n        result = model.predict(im, topk)\n        result_list.append(result.label_ids)\n        if i == images_num - 1:\n            end_time = time.time()\n    average_inference_time = round(\n        (end_time - start_time) / (images_num - twenty_percent_images_num), 4)\n    topk_acc_score = topk_accuracy(np.array(result_list), np.array(label_list))\n    if topk == 1:\n        scores.update({'topk1': topk_acc_score})\n        scores.update({\n            'topk1_average_inference_time(s)': average_inference_time\n        })\n    elif topk == 5:\n        scores.update({'topk5': topk_acc_score})\n        scores.update({\n            'topk5_average_inference_time(s)': average_inference_time\n        })\n    return scores",
  "def is_pic(img_name):\n    valid_suffix = ['JPEG', 'jpeg', 'JPG', 'jpg', 'BMP', 'bmp', 'PNG', 'png']\n    suffix = img_name.split('.')[-1]\n    if suffix not in valid_suffix:\n        return False\n    return True",
  "def get_num_workers(num_workers):\n    if not platform.system() == 'Linux':\n        # Dataloader with multi-process model is not supported\n        # on MacOS and Windows currently.\n        return 0\n    if num_workers == 'auto':\n        num_workers = mp.cpu_count() // 2 if mp.cpu_count() // 2 < 2 else 2\n    return num_workers",
  "def log(level=2, message=\"\", use_color=False):\n    current_time = time.time()\n    time_array = time.localtime(current_time)\n    current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time_array)\n    if use_color:\n        print(\"\\033[1;31;40m{} [{}]\\t{}\\033[0m\".format(current_time, levels[\n            level], message).encode(\"utf-8\").decode(\"latin1\"))\n    else:\n        print(\"{} [{}]\\t{}\".format(current_time, levels[level], message)\n              .encode(\"utf-8\").decode(\"latin1\"))\n    sys.stdout.flush()",
  "def debug(message=\"\", use_color=False):\n    log(level=3, message=message, use_color=use_color)",
  "def info(message=\"\", use_color=False):\n    log(level=2, message=message, use_color=use_color)",
  "def warning(message=\"\", use_color=True):\n    log(level=1, message=message, use_color=use_color)",
  "def error(message=\"\", use_color=True, exit=True):\n    log(level=0, message=message, use_color=use_color)\n    if exit:\n        sys.exit(-1)",
  "def get_det_res(bboxes, bbox_nums, image_id, label_to_cat_id_map, bias=0):\n    det_res = []\n    for i in range(bbox_nums):\n        cur_image_id = int(image_id)\n        dt = bboxes[i]\n        num_id, score, xmin, ymin, xmax, ymax = dt\n        if int(num_id) < 0:\n            continue\n        category_id = label_to_cat_id_map[int(num_id)]\n        w = xmax - xmin + bias\n        h = ymax - ymin + bias\n        bbox = [xmin, ymin, w, h]\n        dt_res = {\n            'image_id': cur_image_id,\n            'category_id': category_id,\n            'bbox': bbox,\n            'score': score\n        }\n        det_res.append(dt_res)\n    return det_res",
  "def get_det_poly_res(bboxes, bbox_nums, image_id, label_to_cat_id_map, bias=0):\n    det_res = []\n    k = 0\n    for i in range(len(bbox_nums)):\n        cur_image_id = int(image_id[i][0])\n        det_nums = bbox_nums[i]\n        for j in range(det_nums):\n            dt = bboxes[k]\n            k = k + 1\n            num_id, score, x1, y1, x2, y2, x3, y3, x4, y4 = dt.tolist()\n            if int(num_id) < 0:\n                continue\n            category_id = label_to_cat_id_map[int(num_id)]\n            rbox = [x1, y1, x2, y2, x3, y3, x4, y4]\n            dt_res = {\n                'image_id': cur_image_id,\n                'category_id': category_id,\n                'bbox': rbox,\n                'score': score\n            }\n            det_res.append(dt_res)\n    return det_res",
  "def strip_mask(mask):\n    row = mask[0, 0, :]\n    col = mask[0, :, 0]\n    im_h = len(col) - np.count_nonzero(col == -1)\n    im_w = len(row) - np.count_nonzero(row == -1)\n    return mask[:, :im_h, :im_w]",
  "def get_seg_res(masks, bboxes, mask_nums, image_id, label_to_cat_id_map):\n    import pycocotools.mask as mask_util\n    seg_res = []\n    k = 0\n    for i in range(len(mask_nums)):\n        cur_image_id = int(image_id[i][0])\n        det_nums = mask_nums[i]\n        mask_i = masks[k:k + det_nums]\n        mask_i = strip_mask(mask_i)\n        for j in range(det_nums):\n            mask = mask_i[j].astype(np.uint8)\n            score = float(bboxes[k][1])\n            label = int(bboxes[k][0])\n            k = k + 1\n            if label == -1:\n                continue\n            cat_id = label_to_cat_id_map[label]\n            rle = mask_util.encode(\n                np.array(\n                    mask[:, :, None], order=\"F\", dtype=\"uint8\"))[0]\n            if six.PY3:\n                if 'counts' in rle:\n                    rle['counts'] = rle['counts'].decode(\"utf8\")\n            sg_res = {\n                'image_id': cur_image_id,\n                'category_id': cat_id,\n                'segmentation': rle,\n                'score': score\n            }\n            seg_res.append(sg_res)\n    return seg_res",
  "def get_solov2_segm_res(results, image_id, num_id_to_cat_id_map):\n    import pycocotools.mask as mask_util\n    segm_res = []\n    # for each batch\n    segms = results['segm'].astype(np.uint8)\n    clsid_labels = results['cate_label']\n    clsid_scores = results['cate_score']\n    lengths = segms.shape[0]\n    im_id = int(image_id[0][0])\n    if lengths == 0 or segms is None:\n        return None\n    # for each sample\n    for i in range(lengths - 1):\n        clsid = int(clsid_labels[i])\n        catid = num_id_to_cat_id_map[clsid]\n        score = float(clsid_scores[i])\n        mask = segms[i]\n        segm = mask_util.encode(np.array(mask[:, :, np.newaxis], order='F'))[0]\n        segm['counts'] = segm['counts'].decode('utf8')\n        coco_res = {\n            'image_id': im_id,\n            'category_id': catid,\n            'segmentation': segm,\n            'score': score\n        }\n        segm_res.append(coco_res)\n    return segm_res",
  "def get_keypoint_res(results, im_id):\n    anns = []\n    preds = results['keypoint']\n    for idx in range(im_id.shape[0]):\n        image_id = im_id[idx].item()\n        kpts, scores = preds[idx]\n        for kpt, score in zip(kpts, scores):\n            kpt = kpt.flatten()\n            ann = {\n                'image_id': image_id,\n                'category_id': 1,  # XXX hard code\n                'keypoints': kpt.tolist(),\n                'score': float(score)\n            }\n            x = kpt[0::3]\n            y = kpt[1::3]\n            x0, x1, y0, y1 = np.min(x).item(), np.max(x).item(), np.min(\n                y).item(), np.max(y).item()\n            ann['area'] = (x1 - x0) * (y1 - y0)\n            ann['bbox'] = [x0, y0, x1 - x0, y1 - y0]\n            anns.append(ann)\n    return anns",
  "def f1_score(intersect_area, pred_area, label_area):\n    class_f1_sco = []\n    for i in range(len(intersect_area)):\n        if pred_area[i] + label_area[i] == 0:\n            f1_sco = 0\n        elif pred_area[i] == 0:\n            f1_sco = 0\n        else:\n            prec = intersect_area[i] / pred_area[i]\n            rec = intersect_area[i] / label_area[i]\n            f1_sco = 2 * prec * rec / (prec + rec)\n        class_f1_sco.append(f1_sco)\n    return np.array(class_f1_sco)",
  "def calculate_area(pred, label, num_classes, ignore_index=255):\n    \"\"\"\n    Calculate intersect, prediction and label area\n\n    Args:\n        pred (np.ndarray): The prediction by model.\n        label (np.ndarray): The ground truth of image.\n        num_classes (int): The unique number of target classes.\n        ignore_index (int): Specifies a target value that is ignored. Default: 255.\n\n    Returns:\n        Numpy Array: The intersection area of prediction and the ground on all class.\n        Numpy Array: The prediction area on all class.\n        Numpy Array: The ground truth area on all class\n    \"\"\"\n    if not pred.shape == label.shape:\n        raise ValueError('Shape of `pred` and `label should be equal, '\n                         'but there are {} and {}.'.format(pred.shape,\n                                                           label.shape))\n\n    mask = label != ignore_index\n    pred = pred + 1\n    label = label + 1\n    pred = pred * mask\n    label = label * mask\n    pred = np.eye(num_classes + 1)[pred]\n    label = np.eye(num_classes + 1)[label]\n    pred = pred[:, 1:]\n    label = label[:, 1:]\n\n    pred_area = []\n    label_area = []\n    intersect_area = []\n\n    for i in range(num_classes):\n        pred_i = pred[:, :, i]\n        label_i = label[:, :, i]\n        pred_area_i = np.sum(pred_i)\n        label_area_i = np.sum(label_i)\n        intersect_area_i = np.sum(pred_i * label_i)\n        pred_area.append(pred_area_i)\n        label_area.append(label_area_i)\n        intersect_area.append(intersect_area_i)\n    return np.array(intersect_area), np.array(pred_area), np.array(label_area)",
  "def mean_iou(intersect_area, pred_area, label_area):\n    \"\"\"\n    Calculate iou.\n\n    Args:\n        intersect_area (np.ndarray): The intersection area of prediction and ground truth on all classes.\n        pred_area (np.ndarray): The prediction area on all classes.\n        label_area (np.ndarray): The ground truth area on all classes.\n\n    Returns:\n        np.ndarray: iou on all classes.\n        float: mean iou of all classes.\n    \"\"\"\n    union = pred_area + label_area - intersect_area\n    class_iou = []\n    for i in range(len(intersect_area)):\n        if union[i] == 0:\n            iou = 0\n        else:\n            iou = intersect_area[i] / union[i]\n        class_iou.append(iou)\n    miou = np.mean(class_iou)\n    return np.array(class_iou), miou",
  "def accuracy(intersect_area, pred_area):\n    \"\"\"\n    Calculate accuracy\n\n    Args:\n        intersect_area (np.ndarray): The intersection area of prediction and ground truth on all classes..\n        pred_area (np.ndarray): The prediction area on all classes.\n\n    Returns:\n        np.ndarray: accuracy on all classes.\n        float: mean accuracy.\n    \"\"\"\n    class_acc = []\n    for i in range(len(intersect_area)):\n        if pred_area[i] == 0:\n            acc = 0\n        else:\n            acc = intersect_area[i] / pred_area[i]\n        class_acc.append(acc)\n    macc = np.sum(intersect_area) / np.sum(pred_area)\n    return np.array(class_acc), macc",
  "def kappa(intersect_area, pred_area, label_area):\n    \"\"\"\n    Calculate kappa coefficient\n\n    Args:\n        intersect_area (np.ndarray): The intersection area of prediction and ground truth on all classes..\n        pred_area (np.ndarray): The prediction area on all classes.\n        label_area (np.ndarray): The ground truth area on all classes.\n\n    Returns:\n        float: kappa coefficient.\n    \"\"\"\n    total_area = np.sum(label_area)\n    po = np.sum(intersect_area) / total_area\n    pe = np.sum(pred_area * label_area) / (total_area * total_area)\n    kappa = (po - pe) / (1 - pe)\n    return kappa",
  "def loadRes(coco_obj, anns):\n    \"\"\"\n    Load result file and return a result api object.\n    :param   resFile (str)     : file name of result file\n    :return: res (obj)         : result api object\n    \"\"\"\n\n    # This function has the same functionality as pycocotools.COCO.loadRes,\n    # except that the input anns is list of results rather than a json file.\n    # Refer to\n    # https://github.com/cocodataset/cocoapi/blob/8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9/PythonAPI/pycocotools/coco.py#L305,\n\n    # matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n    # or matplotlib.backends is imported for the first time\n    # pycocotools import matplotlib\n    import matplotlib\n    matplotlib.use('Agg')\n    from pycocotools.coco import COCO\n    import pycocotools.mask as maskUtils\n    import time\n    res = COCO()\n    res.dataset['images'] = [img for img in coco_obj.dataset['images']]\n\n    tic = time.time()\n    assert type(anns) == list, 'results in not an array of objects'\n    annsImgIds = [ann['image_id'] for ann in anns]\n    assert set(annsImgIds) == (set(annsImgIds) & set(coco_obj.getImgIds())), \\\n        'Results do not correspond to current coco set'\n    if 'caption' in anns[0]:\n        imgIds = set([img['id'] for img in res.dataset['images']]) & set(\n            [ann['image_id'] for ann in anns])\n        res.dataset['images'] = [\n            img for img in res.dataset['images'] if img['id'] in imgIds\n        ]\n        for id, ann in enumerate(anns):\n            ann['id'] = id + 1\n    elif 'bbox' in anns[0] and not anns[0]['bbox'] == []:\n        res.dataset['categories'] = copy.deepcopy(coco_obj.dataset[\n            'categories'])\n        for id, ann in enumerate(anns):\n            bb = ann['bbox']\n            x1, x2, y1, y2 = [bb[0], bb[0] + bb[2], bb[1], bb[1] + bb[3]]\n            if not 'segmentation' in ann:\n                ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n            ann['area'] = bb[2] * bb[3]\n            ann['id'] = id + 1\n            ann['iscrowd'] = 0\n    elif 'segmentation' in anns[0]:\n        res.dataset['categories'] = copy.deepcopy(coco_obj.dataset[\n            'categories'])\n        for id, ann in enumerate(anns):\n            # now only support compressed RLE format as segmentation results\n            ann['area'] = maskUtils.area(ann['segmentation'])\n            if not 'bbox' in ann:\n                ann['bbox'] = maskUtils.toBbox(ann['segmentation'])\n            ann['id'] = id + 1\n            ann['iscrowd'] = 0\n    elif 'keypoints' in anns[0]:\n        res.dataset['categories'] = copy.deepcopy(coco_obj.dataset[\n            'categories'])\n        for id, ann in enumerate(anns):\n            s = ann['keypoints']\n            x = s[0::3]\n            y = s[1::3]\n            x0, x1, y0, y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n            ann['area'] = (x1 - x0) * (y1 - y0)\n            ann['id'] = id + 1\n            ann['bbox'] = [x0, y0, x1 - x0, y1 - y0]\n\n    res.dataset['annotations'] = anns\n    res.createIndex()\n    return res",
  "def get_infer_results(outs, catid, bias=0):\n    \"\"\"\n    Get result at the stage of inference.\n    The output format is dictionary containing bbox or mask result.\n\n    For example, bbox result is a list and each element contains\n    image_id, category_id, bbox and score.\n    \"\"\"\n    if outs is None or len(outs) == 0:\n        raise ValueError(\n            'The number of valid detection result if zero. Please use reasonable model and check input data.'\n        )\n\n    im_id = outs['im_id']\n\n    infer_res = {}\n    if 'bbox' in outs:\n        if len(outs['bbox']) > 0 and len(outs['bbox'][0]) > 6:\n            infer_res['bbox'] = get_det_poly_res(\n                outs['bbox'], outs['bbox_num'], im_id, catid, bias=bias)\n        else:\n            infer_res['bbox'] = get_det_res(\n                outs['bbox'], outs['bbox_num'], im_id, catid, bias=bias)\n\n    if 'mask' in outs:\n        # mask post process\n        infer_res['mask'] = get_seg_res(outs['mask'], outs['bbox'],\n                                        outs['bbox_num'], im_id, catid)\n\n    if 'segm' in outs:\n        infer_res['segm'] = get_solov2_segm_res(outs, im_id, catid)\n\n    return infer_res",
  "def cocoapi_eval(anns,\n                 style,\n                 coco_gt=None,\n                 anno_file=None,\n                 max_dets=(100, 300, 1000),\n                 classwise=False):\n    \"\"\"\n    Args:\n        anns: Evaluation result.\n        style (str): COCOeval style, can be `bbox` , `segm` and `proposal`.\n        coco_gt (str): Whether to load COCOAPI through anno_file,\n                 eg: coco_gt = COCO(anno_file)\n        anno_file (str): COCO annotations file.\n        max_dets (tuple): COCO evaluation maxDets.\n        classwise (bool): Whether per-category AP and draw P-R Curve or not.\n    \"\"\"\n    assert coco_gt is not None or anno_file is not None\n    from pycocotools.coco import COCO\n    from pycocotools.cocoeval import COCOeval\n\n    if coco_gt is None:\n        coco_gt = COCO(anno_file)\n    logging.info(\"Start evaluate...\")\n    coco_dt = loadRes(coco_gt, anns)\n    if style == 'proposal':\n        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n        coco_eval.params.useCats = 0\n        coco_eval.params.maxDets = list(max_dets)\n    else:\n        coco_eval = COCOeval(coco_gt, coco_dt, style)\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n    if classwise:\n        # Compute per-category AP and PR curve\n        try:\n            from terminaltables import AsciiTable\n        except Exception as e:\n            logging.error(\n                'terminaltables not found, plaese install terminaltables. '\n                'for example: `pip install terminaltables`.')\n            raise e\n        precisions = coco_eval.eval['precision']\n        cat_ids = coco_gt.getCatIds()\n        # precision: (iou, recall, cls, area range, max dets)\n        assert len(cat_ids) == precisions.shape[2]\n        results_per_category = []\n        for idx, catId in enumerate(cat_ids):\n            # area range index 0: all area ranges\n            # max dets index -1: typically 100 per image\n            nm = coco_gt.loadCats(catId)[0]\n            precision = precisions[:, :, idx, 0, -1]\n            precision = precision[precision > -1]\n            if precision.size:\n                ap = np.mean(precision)\n            else:\n                ap = float('nan')\n            results_per_category.append(\n                (str(nm[\"name\"]), '{:0.3f}'.format(float(ap))))\n            pr_array = precisions[0, :, idx, 0, 2]\n            recall_array = np.arange(0.0, 1.01, 0.01)\n            draw_pr_curve(\n                pr_array,\n                recall_array,\n                out_dir=style + '_pr_curve',\n                file_name='{}_precision_recall_curve.jpg'.format(nm[\"name\"]))\n\n        num_columns = min(6, len(results_per_category) * 2)\n\n        import itertools\n        results_flatten = list(itertools.chain(*results_per_category))\n        headers = ['category', 'AP'] * (num_columns // 2)\n        results_2d = itertools.zip_longest(\n            * [results_flatten[i::num_columns] for i in range(num_columns)])\n        table_data = [headers]\n        table_data += [result for result in results_2d]\n        table = AsciiTable(table_data)\n        logging.info('Per-category of {} AP: \\n{}'.format(style, table.table))\n        logging.info(\"per-category PR curve has output to {} folder.\".format(\n            style + '_pr_curve'))\n    # flush coco evaluation result\n    sys.stdout.flush()\n    return coco_eval.stats",
  "class Cityscapes(object):\n    \"\"\"\n    Cityscapes dataset `https://www.cityscapes-dataset.com/`.\n    The folder structure is as follow:\n\n        cityscapes\n        |\n        |--leftImg8bit\n        |  |--train\n        |  |--val\n        |  |--test\n        |\n        |--gtFine\n        |  |--train\n        |  |--val\n        |  |--test\n\n    Args:\n        dataset_root (str): Cityscapes dataset directory.\n    \"\"\"\n    NUM_CLASSES = 19\n\n    def __init__(self, dataset_root, mode):\n        self.dataset_root = dataset_root\n        self.file_list = list()\n        mode = mode.lower()\n        self.mode = mode\n        self.num_classes = self.NUM_CLASSES\n        self.ignore_index = 255\n\n        img_dir = os.path.join(self.dataset_root, 'leftImg8bit')\n        label_dir = os.path.join(self.dataset_root, 'gtFine')\n        if self.dataset_root is None or not os.path.isdir(\n                self.dataset_root) or not os.path.isdir(\n                    img_dir) or not os.path.isdir(label_dir):\n            raise ValueError(\n                \"The dataset is not Found or the folder structure is nonconfoumance.\"\n            )\n\n        label_files = sorted(\n            glob.glob(\n                os.path.join(label_dir, mode, '*',\n                             '*_gtFine_labelTrainIds.png')))\n        img_files = sorted(\n            glob.glob(os.path.join(img_dir, mode, '*', '*_leftImg8bit.png')))\n\n        self.file_list = [\n            [img_path, label_path]\n            for img_path, label_path in zip(img_files, label_files)\n        ]\n\n        self.num_samples = len(self.file_list)\n        logging.info(\"{} samples in file {}\".format(self.num_samples, img_dir))",
  "def __init__(self, dataset_root, mode):\n        self.dataset_root = dataset_root\n        self.file_list = list()\n        mode = mode.lower()\n        self.mode = mode\n        self.num_classes = self.NUM_CLASSES\n        self.ignore_index = 255\n\n        img_dir = os.path.join(self.dataset_root, 'leftImg8bit')\n        label_dir = os.path.join(self.dataset_root, 'gtFine')\n        if self.dataset_root is None or not os.path.isdir(\n                self.dataset_root) or not os.path.isdir(\n                    img_dir) or not os.path.isdir(label_dir):\n            raise ValueError(\n                \"The dataset is not Found or the folder structure is nonconfoumance.\"\n            )\n\n        label_files = sorted(\n            glob.glob(\n                os.path.join(label_dir, mode, '*',\n                             '*_gtFine_labelTrainIds.png')))\n        img_files = sorted(\n            glob.glob(os.path.join(img_dir, mode, '*', '*_leftImg8bit.png')))\n\n        self.file_list = [\n            [img_path, label_path]\n            for img_path, label_path in zip(img_files, label_files)\n        ]\n\n        self.num_samples = len(self.file_list)\n        logging.info(\"{} samples in file {}\".format(self.num_samples, img_dir))",
  "def draw_pr_curve(precision,\n                  recall,\n                  iou=0.5,\n                  out_dir='pr_curve',\n                  file_name='precision_recall_curve.jpg'):\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    output_path = os.path.join(out_dir, file_name)\n    try:\n        import matplotlib.pyplot as plt\n    except Exception as e:\n        # logger.error('Matplotlib not found, plaese install matplotlib.'\n        #              'for example: `pip install matplotlib`.')\n        raise e\n    plt.cla()\n    plt.figure('P-R Curve')\n    plt.title('Precision/Recall Curve(IoU={})'.format(iou))\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.grid(True)\n    plt.plot(recall, precision)\n    plt.savefig(output_path)",
  "class COCOMetric(object):\n    def __init__(self, coco_gt, **kwargs):\n        self.clsid2catid = {\n            i: cat['id']\n            for i, cat in enumerate(coco_gt.loadCats(coco_gt.getCatIds()))\n        }\n        self.coco_gt = coco_gt\n        self.classwise = kwargs.get('classwise', False)\n        self.bias = 0\n        self.reset()\n\n    def reset(self):\n        # only bbox and mask evaluation support currently\n        self.details = {\n            'gt': copy.deepcopy(self.coco_gt.dataset),\n            'bbox': [],\n            'mask': []\n        }\n        self.eval_stats = {}\n\n    def update(self, im_id, outputs):\n        outs = {}\n        # outputs Tensor -> numpy.ndarray\n        for k, v in outputs.items():\n            outs[k] = v\n\n        outs['im_id'] = im_id\n        infer_results = get_infer_results(\n            outs, self.clsid2catid, bias=self.bias)\n        self.details['bbox'] += infer_results[\n            'bbox'] if 'bbox' in infer_results else []\n        self.details['mask'] += infer_results[\n            'mask'] if 'mask' in infer_results else []\n\n    def accumulate(self):\n        if len(self.details['bbox']) > 0:\n            bbox_stats = cocoapi_eval(\n                copy.deepcopy(self.details['bbox']),\n                'bbox',\n                coco_gt=self.coco_gt,\n                classwise=self.classwise)\n            self.eval_stats['bbox'] = bbox_stats\n            sys.stdout.flush()\n\n        if len(self.details['mask']) > 0:\n            seg_stats = cocoapi_eval(\n                copy.deepcopy(self.details['mask']),\n                'segm',\n                coco_gt=self.coco_gt,\n                classwise=self.classwise)\n            self.eval_stats['mask'] = seg_stats\n            sys.stdout.flush()\n\n    def log(self):\n        pass\n\n    def get(self):\n        if 'bbox' not in self.eval_stats:\n            return {'bbox_mmap': 0.}\n        if 'mask' in self.eval_stats:\n            return OrderedDict(\n                zip(['bbox_mmap', 'segm_mmap'],\n                    [self.eval_stats['bbox'][0], self.eval_stats['mask'][0]]))\n        else:\n            return {'bbox_mmap': self.eval_stats['bbox'][0]}",
  "def __init__(self, coco_gt, **kwargs):\n        self.clsid2catid = {\n            i: cat['id']\n            for i, cat in enumerate(coco_gt.loadCats(coco_gt.getCatIds()))\n        }\n        self.coco_gt = coco_gt\n        self.classwise = kwargs.get('classwise', False)\n        self.bias = 0\n        self.reset()",
  "def reset(self):\n        # only bbox and mask evaluation support currently\n        self.details = {\n            'gt': copy.deepcopy(self.coco_gt.dataset),\n            'bbox': [],\n            'mask': []\n        }\n        self.eval_stats = {}",
  "def update(self, im_id, outputs):\n        outs = {}\n        # outputs Tensor -> numpy.ndarray\n        for k, v in outputs.items():\n            outs[k] = v\n\n        outs['im_id'] = im_id\n        infer_results = get_infer_results(\n            outs, self.clsid2catid, bias=self.bias)\n        self.details['bbox'] += infer_results[\n            'bbox'] if 'bbox' in infer_results else []\n        self.details['mask'] += infer_results[\n            'mask'] if 'mask' in infer_results else []",
  "def accumulate(self):\n        if len(self.details['bbox']) > 0:\n            bbox_stats = cocoapi_eval(\n                copy.deepcopy(self.details['bbox']),\n                'bbox',\n                coco_gt=self.coco_gt,\n                classwise=self.classwise)\n            self.eval_stats['bbox'] = bbox_stats\n            sys.stdout.flush()\n\n        if len(self.details['mask']) > 0:\n            seg_stats = cocoapi_eval(\n                copy.deepcopy(self.details['mask']),\n                'segm',\n                coco_gt=self.coco_gt,\n                classwise=self.classwise)\n            self.eval_stats['mask'] = seg_stats\n            sys.stdout.flush()",
  "def log(self):\n        pass",
  "def get(self):\n        if 'bbox' not in self.eval_stats:\n            return {'bbox_mmap': 0.}\n        if 'mask' in self.eval_stats:\n            return OrderedDict(\n                zip(['bbox_mmap', 'segm_mmap'],\n                    [self.eval_stats['bbox'][0], self.eval_stats['mask'][0]]))\n        else:\n            return {'bbox_mmap': self.eval_stats['bbox'][0]}",
  "class CocoDetection(object):\n    \"\"\"\u8bfb\u53d6MSCOCO\u683c\u5f0f\u7684\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u6837\u672c\u8fdb\u884c\u76f8\u5e94\u7684\u5904\u7406\uff0c\u8be5\u683c\u5f0f\u7684\u6570\u636e\u96c6\u540c\u6837\u53ef\u4ee5\u5e94\u7528\u5230\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u7684\u8bad\u7ec3\u4e2d\u3002\n\n    Args:\n        data_dir (str): \u6570\u636e\u96c6\u6240\u5728\u7684\u76ee\u5f55\u8def\u5f84\u3002\n        ann_file (str): \u6570\u636e\u96c6\u7684\u6807\u6ce8\u6587\u4ef6\uff0c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684json\u683c\u5f0f\u6587\u4ef6\u3002\n        num_workers (int|str): \u6570\u636e\u96c6\u4e2d\u6837\u672c\u5728\u9884\u5904\u7406\u8fc7\u7a0b\u4e2d\u7684\u7ebf\u7a0b\u6216\u8fdb\u7a0b\u6570\u3002\u9ed8\u8ba4\u4e3a'auto'\u3002\u5f53\u8bbe\u4e3a'auto'\u65f6\uff0c\u6839\u636e\n            \u7cfb\u7edf\u7684\u5b9e\u9645CPU\u6838\u6570\u8bbe\u7f6e`num_workers`: \u5982\u679cCPU\u6838\u6570\u7684\u4e00\u534a\u5927\u4e8e8\uff0c\u5219`num_workers`\u4e3a8\uff0c\u5426\u5219\u4e3aCPU\u6838\u6570\u7684\u4e00\u534a\u3002\n        shuffle (bool): \u662f\u5426\u9700\u8981\u5bf9\u6570\u636e\u96c6\u4e2d\u6837\u672c\u6253\u4e71\u987a\u5e8f\u3002\u9ed8\u8ba4\u4e3aFalse\u3002\n        allow_empty (bool): \u662f\u5426\u52a0\u8f7d\u8d1f\u6837\u672c\u3002\u9ed8\u8ba4\u4e3aFalse\u3002\n        empty_ratio (float): \u7528\u4e8e\u6307\u5b9a\u8d1f\u6837\u672c\u5360\u603b\u6837\u672c\u6570\u7684\u6bd4\u4f8b\u3002\u5982\u679c\u5c0f\u4e8e0\u6216\u5927\u4e8e\u7b49\u4e8e1\uff0c\u5219\u4fdd\u7559\u5168\u90e8\u7684\u8d1f\u6837\u672c\u3002\u9ed8\u8ba4\u4e3a1\u3002\n    \"\"\"\n\n    def __init__(self,\n                 data_dir,\n                 ann_file,\n                 num_workers='auto',\n                 shuffle=False,\n                 allow_empty=False,\n                 empty_ratio=1.):\n\n        from pycocotools.coco import COCO\n        self.data_dir = data_dir\n        self.data_fields = None\n        self.num_max_boxes = 1000\n        self.num_workers = get_num_workers(num_workers)\n        self.shuffle = shuffle\n        self.allow_empty = allow_empty\n        self.empty_ratio = empty_ratio\n        self.file_list = list()\n        neg_file_list = list()\n        self.labels = list()\n\n        coco = COCO(ann_file)\n        self.coco_gt = coco\n        img_ids = sorted(coco.getImgIds())\n        cat_ids = coco.getCatIds()\n        catid2clsid = dict({catid: i for i, catid in enumerate(cat_ids)})\n        cname2clsid = dict({\n            coco.loadCats(catid)[0]['name']: clsid\n            for catid, clsid in catid2clsid.items()\n        })\n        for label, cid in sorted(cname2clsid.items(), key=lambda d: d[1]):\n            self.labels.append(label)\n        logging.info(\"Starting to read file list from dataset...\")\n\n        ct = 0\n        for img_id in img_ids:\n            is_empty = False\n            img_anno = coco.loadImgs(img_id)[0]\n            im_fname = osp.join(data_dir, img_anno['file_name'])\n            if not is_pic(im_fname):\n                continue\n            im_w = float(img_anno['width'])\n            im_h = float(img_anno['height'])\n            ins_anno_ids = coco.getAnnIds(imgIds=img_id, iscrowd=False)\n            instances = coco.loadAnns(ins_anno_ids)\n\n            bboxes = []\n            for inst in instances:\n                x, y, box_w, box_h = inst['bbox']\n                x1 = max(0, x)\n                y1 = max(0, y)\n                x2 = min(im_w - 1, x1 + max(0, box_w))\n                y2 = min(im_h - 1, y1 + max(0, box_h))\n                if inst['area'] > 0 and x2 >= x1 and y2 >= y1:\n                    inst['clean_bbox'] = [x1, y1, x2, y2]\n                    bboxes.append(inst)\n                else:\n                    logging.warning(\n                        \"Found an invalid bbox in annotations: \"\n                        \"im_id: {}, area: {} x1: {}, y1: {}, x2: {}, y2: {}.\"\n                        .format(img_id, float(inst['area']), x1, y1, x2, y2))\n            num_bbox = len(bboxes)\n            if num_bbox == 0 and not self.allow_empty:\n                continue\n            elif num_bbox == 0:\n                is_empty = True\n\n            gt_bbox = np.zeros((num_bbox, 4), dtype=np.float32)\n            gt_class = np.zeros((num_bbox, 1), dtype=np.int32)\n            gt_score = np.ones((num_bbox, 1), dtype=np.float32)\n            is_crowd = np.zeros((num_bbox, 1), dtype=np.int32)\n            difficult = np.zeros((num_bbox, 1), dtype=np.int32)\n            gt_poly = [None] * num_bbox\n\n            has_segmentation = False\n            for i, box in reversed(list(enumerate(bboxes))):\n                catid = box['category_id']\n                gt_class[i][0] = catid2clsid[catid]\n                gt_bbox[i, :] = box['clean_bbox']\n                is_crowd[i][0] = box['iscrowd']\n                if 'segmentation' in box and box['iscrowd'] == 1:\n                    gt_poly[i] = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n                elif 'segmentation' in box and box['segmentation']:\n                    if not np.array(\n                            box['segmentation'],\n                            dtype=object).size > 0 and not self.allow_empty:\n                        gt_poly.pop(i)\n                        is_crowd = np.delete(is_crowd, i)\n                        gt_class = np.delete(gt_class, i)\n                        gt_bbox = np.delete(gt_bbox, i)\n                    else:\n                        gt_poly[i] = box['segmentation']\n                    has_segmentation = True\n            if has_segmentation and not any(gt_poly) and not self.allow_empty:\n                continue\n\n            im_info = {\n                'im_id': np.array([img_id]).astype('int32'),\n                'image_shape': np.array([im_h, im_w]).astype('int32'),\n            }\n            label_info = {\n                'is_crowd': is_crowd,\n                'gt_class': gt_class,\n                'gt_bbox': gt_bbox,\n                'gt_score': gt_score,\n                'gt_poly': gt_poly,\n                'difficult': difficult\n            }\n\n            if is_empty:\n                neg_file_list.append({\n                    'image': im_fname,\n                    **\n                    im_info,\n                    **\n                    label_info\n                })\n            else:\n                self.file_list.append({\n                    'image': im_fname,\n                    **\n                    im_info,\n                    **\n                    label_info\n                })\n            ct += 1\n\n            self.num_max_boxes = max(self.num_max_boxes, len(instances))\n\n        if not ct:\n            logging.error(\n                \"No coco record found in %s' % (ann_file)\", exit=True)\n        self.pos_num = len(self.file_list)\n        if self.allow_empty and neg_file_list:\n            self.file_list += self._sample_empty(neg_file_list)\n        logging.info(\n            \"{} samples in file {}, including {} positive samples and {} negative samples.\".\n            format(\n                len(self.file_list), ann_file, self.pos_num,\n                len(self.file_list) - self.pos_num))\n        self.num_samples = len(self.file_list)\n\n        self._epoch = 0",
  "def __init__(self,\n                 data_dir,\n                 ann_file,\n                 num_workers='auto',\n                 shuffle=False,\n                 allow_empty=False,\n                 empty_ratio=1.):\n\n        from pycocotools.coco import COCO\n        self.data_dir = data_dir\n        self.data_fields = None\n        self.num_max_boxes = 1000\n        self.num_workers = get_num_workers(num_workers)\n        self.shuffle = shuffle\n        self.allow_empty = allow_empty\n        self.empty_ratio = empty_ratio\n        self.file_list = list()\n        neg_file_list = list()\n        self.labels = list()\n\n        coco = COCO(ann_file)\n        self.coco_gt = coco\n        img_ids = sorted(coco.getImgIds())\n        cat_ids = coco.getCatIds()\n        catid2clsid = dict({catid: i for i, catid in enumerate(cat_ids)})\n        cname2clsid = dict({\n            coco.loadCats(catid)[0]['name']: clsid\n            for catid, clsid in catid2clsid.items()\n        })\n        for label, cid in sorted(cname2clsid.items(), key=lambda d: d[1]):\n            self.labels.append(label)\n        logging.info(\"Starting to read file list from dataset...\")\n\n        ct = 0\n        for img_id in img_ids:\n            is_empty = False\n            img_anno = coco.loadImgs(img_id)[0]\n            im_fname = osp.join(data_dir, img_anno['file_name'])\n            if not is_pic(im_fname):\n                continue\n            im_w = float(img_anno['width'])\n            im_h = float(img_anno['height'])\n            ins_anno_ids = coco.getAnnIds(imgIds=img_id, iscrowd=False)\n            instances = coco.loadAnns(ins_anno_ids)\n\n            bboxes = []\n            for inst in instances:\n                x, y, box_w, box_h = inst['bbox']\n                x1 = max(0, x)\n                y1 = max(0, y)\n                x2 = min(im_w - 1, x1 + max(0, box_w))\n                y2 = min(im_h - 1, y1 + max(0, box_h))\n                if inst['area'] > 0 and x2 >= x1 and y2 >= y1:\n                    inst['clean_bbox'] = [x1, y1, x2, y2]\n                    bboxes.append(inst)\n                else:\n                    logging.warning(\n                        \"Found an invalid bbox in annotations: \"\n                        \"im_id: {}, area: {} x1: {}, y1: {}, x2: {}, y2: {}.\"\n                        .format(img_id, float(inst['area']), x1, y1, x2, y2))\n            num_bbox = len(bboxes)\n            if num_bbox == 0 and not self.allow_empty:\n                continue\n            elif num_bbox == 0:\n                is_empty = True\n\n            gt_bbox = np.zeros((num_bbox, 4), dtype=np.float32)\n            gt_class = np.zeros((num_bbox, 1), dtype=np.int32)\n            gt_score = np.ones((num_bbox, 1), dtype=np.float32)\n            is_crowd = np.zeros((num_bbox, 1), dtype=np.int32)\n            difficult = np.zeros((num_bbox, 1), dtype=np.int32)\n            gt_poly = [None] * num_bbox\n\n            has_segmentation = False\n            for i, box in reversed(list(enumerate(bboxes))):\n                catid = box['category_id']\n                gt_class[i][0] = catid2clsid[catid]\n                gt_bbox[i, :] = box['clean_bbox']\n                is_crowd[i][0] = box['iscrowd']\n                if 'segmentation' in box and box['iscrowd'] == 1:\n                    gt_poly[i] = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n                elif 'segmentation' in box and box['segmentation']:\n                    if not np.array(\n                            box['segmentation'],\n                            dtype=object).size > 0 and not self.allow_empty:\n                        gt_poly.pop(i)\n                        is_crowd = np.delete(is_crowd, i)\n                        gt_class = np.delete(gt_class, i)\n                        gt_bbox = np.delete(gt_bbox, i)\n                    else:\n                        gt_poly[i] = box['segmentation']\n                    has_segmentation = True\n            if has_segmentation and not any(gt_poly) and not self.allow_empty:\n                continue\n\n            im_info = {\n                'im_id': np.array([img_id]).astype('int32'),\n                'image_shape': np.array([im_h, im_w]).astype('int32'),\n            }\n            label_info = {\n                'is_crowd': is_crowd,\n                'gt_class': gt_class,\n                'gt_bbox': gt_bbox,\n                'gt_score': gt_score,\n                'gt_poly': gt_poly,\n                'difficult': difficult\n            }\n\n            if is_empty:\n                neg_file_list.append({\n                    'image': im_fname,\n                    **\n                    im_info,\n                    **\n                    label_info\n                })\n            else:\n                self.file_list.append({\n                    'image': im_fname,\n                    **\n                    im_info,\n                    **\n                    label_info\n                })\n            ct += 1\n\n            self.num_max_boxes = max(self.num_max_boxes, len(instances))\n\n        if not ct:\n            logging.error(\n                \"No coco record found in %s' % (ann_file)\", exit=True)\n        self.pos_num = len(self.file_list)\n        if self.allow_empty and neg_file_list:\n            self.file_list += self._sample_empty(neg_file_list)\n        logging.info(\n            \"{} samples in file {}, including {} positive samples and {} negative samples.\".\n            format(\n                len(self.file_list), ann_file, self.pos_num,\n                len(self.file_list) - self.pos_num))\n        self.num_samples = len(self.file_list)\n\n        self._epoch = 0",
  "def vis_detection(im_data,\n                  det_result,\n                  labels=[],\n                  score_threshold=0.0,\n                  line_size=1,\n                  font_size=0.5,\n                  font_color=[255, 255, 255],\n                  font_thickness=1):\n    \"\"\"Show the visualized results for detection models\n\n    :param im_data: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n    :param det_result: the result produced by model\n    :param labels: (list of str) the visualized result will show the bounding box contain class label\n    :param score_threshold: (float) score_threshold threshold for result scores, the bounding box will not be shown if the score is less than score_threshold\n    :param line_size: (float) line_size line size for bounding boxes\n    :param font_size: (float) font_size font size for text\n    :param font_color: (list of int) font_color  for text\n    :param font_thickness: (int) font_thickness for text\n    :return: (numpy.ndarray) image with visualized results\n    \"\"\"\n    return C.vision.vis_detection(im_data, det_result, labels, score_threshold,\n                                  line_size, font_size, font_color,\n                                  font_thickness)",
  "def vis_perception(im_data,\n                   det_result,\n                   config_file,\n                   score_threshold=0.0,\n                   line_size=1,\n                   font_size=0.5):\n    \"\"\"Show the visualized results for 3d detection models\n\n    :param im_data: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n    :param det_result: the result produced by model\n    :param config_file: the config file for detection and visualization\n    :param score_threshold: (float) score_threshold threshold for result scores, the bounding box will not be shown if the score is less than score_threshold\n    :param line_size: (float) line_size line size for bounding boxes\n    :param font_size: (float) font_size font size for text\n    :return: (numpy.ndarray) image with visualized results\n    \"\"\"\n    return C.vision.vis_perception(im_data, det_result, config_file,\n                                   score_threshold, line_size, font_size)",
  "def vis_keypoint_detection(im_data, keypoint_det_result, conf_threshold=0.5):\n    \"\"\"Show the visualized results for keypoint detection models\n\n    :param im_data: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n    :param keypoint_det_result: the result produced by model\n    :param conf_threshold: (float) conf_threshold threshold for result scores, the bounding box will not be shown if the score is less than conf_threshold\n    :return: (numpy.ndarray) image with visualized results\n    \"\"\"\n    return C.vision.Visualize.vis_keypoint_detection(\n        im_data, keypoint_det_result, conf_threshold)",
  "def vis_face_detection(im_data, face_det_result, line_size=1, font_size=0.5):\n    \"\"\"Show the visualized results for face detection models\n\n    :param im_data: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n    :param face_det_result: the result produced by model\n    :param line_size: (float) line_size line size for bounding boxes\n    :param font_size: (float) font_size font size for text\n    :return: (numpy.ndarray) image with visualized results\n    \"\"\"\n    return C.vision.vis_face_detection(im_data, face_det_result, line_size,\n                                       font_size)",
  "def vis_face_alignment(im_data, face_align_result, line_size=1):\n    \"\"\"Show the visualized results for face alignment models\n\n    :param im_data: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n    :param face_align_result: the result produced by model\n    :param line_size: (float)line_size line size for circle point\n    :return: (numpy.ndarray) image with visualized results\n    \"\"\"\n    return C.vision.vis_face_alignment(im_data, face_align_result, line_size)",
  "def vis_segmentation(im_data, seg_result, weight=0.5):\n    \"\"\"Show the visualized results for segmentation models\n\n    :param im_data: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n    :param seg_result: the result produced by model\n    :param weight: (float)transparent weight of visualized result image\n    :return: (numpy.ndarray) image with visualized results\n    \"\"\"\n    return C.vision.vis_segmentation(im_data, seg_result, weight)",
  "def vis_matting_alpha(im_data,\n                      matting_result,\n                      remove_small_connected_area=False):\n    logging.warning(\n        \"DEPRECATED: fastdeploy.vision.vis_matting_alpha is deprecated, please use fastdeploy.vision.vis_matting function instead.\"\n    )\n    return C.vision.vis_matting(im_data, matting_result,\n                                remove_small_connected_area)",
  "def vis_matting(im_data,\n                matting_result,\n                transparent_background=False,\n                transparent_threshold=0.99,\n                remove_small_connected_area=False):\n    \"\"\"Show the visualized results for matting models\n\n    :param im_data: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n    :param matting_result: the result produced by model\n    :param transparent_background: whether visulizing matting result with transparent background\n    :param transparent_threshold: since the alpha value in MattringResult is a float between [0, 1], transparent_threshold is used to filter background pixel\n    :param remove_small_connected_area: (bool) if remove_small_connected_area==True, the visualized result will not include the small connected areas\n    :return: (numpy.ndarray) image with visualized results\n    \"\"\"\n    return C.vision.vis_matting(im_data, matting_result,\n                                transparent_background, transparent_threshold,\n                                remove_small_connected_area)",
  "def swap_background_matting(im_data,\n                            background,\n                            result,\n                            remove_small_connected_area=False):\n    logging.warning(\n        \"DEPRECATED: fastdeploy.vision.swap_background_matting is deprecated, please use fastdeploy.vision.swap_background function instead.\"\n    )\n    assert isinstance(\n        result,\n        C.vision.MattingResult), \"The result must be MattingResult type\"\n    return C.vision.Visualize.swap_background_matting(\n        im_data, background, result, remove_small_connected_area)",
  "def swap_background_segmentation(im_data, background, background_label,\n                                 result):\n    logging.warning(\n        \"DEPRECATED: fastdeploy.vision.swap_background_segmentation is deprecated, please use fastdeploy.vision.swap_background function instead.\"\n    )\n    assert isinstance(\n        result, C.vision.\n        SegmentationResult), \"The result must be SegmentaitonResult type\"\n    return C.vision.Visualize.swap_background_segmentation(\n        im_data, background, background_label, result)",
  "def swap_background(im_data,\n                    background,\n                    result,\n                    remove_small_connected_area=False,\n                    background_label=0):\n    \"\"\"Swap the image background with MattingResult or SegmentationResult\n\n    :param im_data: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n    :param background: (numpy.ndarray)The background image data, 3-D array with layout HWC, BGR format\n    :param result: The result produced by model, MattingResult or SegmentationResult\n    :param remove_small_connected_area: (bool) If remove_small_connected_area==True, the visualized result will not include the small connected areas\n    :param background_label: (int)The background label number in SegmentationResult\n    :return: (numpy.ndarray) image with visualized results\n    \"\"\"\n    if isinstance(result, C.vision.MattingResult):\n        return C.vision.swap_background(im_data, background, result,\n                                        remove_small_connected_area)\n    elif isinstance(result, C.vision.SegmentationResult):\n        return C.vision.swap_background(im_data, background, result,\n                                        background_label)\n    else:\n        raise Exception(\n            \"Only support result type of MattingResult or SegmentationResult, but now the data type is {}.\".\n            format(type(result)))",
  "def vis_ppocr(im_data, det_result):\n    \"\"\"Show the visualized results for ocr models\n\n    :param im_data: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n    :param det_result: the result produced by model\n    :return: (numpy.ndarray) image with visualized results\n    \"\"\"\n    return C.vision.vis_ppocr(im_data, det_result)",
  "def vis_mot(im_data, mot_result, score_threshold=0.0, records=None):\n    return C.vision.vis_mot(im_data, mot_result, score_threshold, records)",
  "def vis_headpose(im_data, headpose_result, size=50, line_size=1):\n    return C.vision.vis_headpose(im_data, headpose_result, size, line_size)",
  "class PaddleSegModel(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a image segmentation model exported by PaddleSeg.\n\n        :param model_file: (str)Path of model file, e.g unet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g unet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str) Path of configuration file for deploy, e.g unet/deploy.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PaddleSegModel, self).__init__(runtime_option)\n\n        # assert model_format == ModelFormat.PADDLE, \"PaddleSeg only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.segmentation.PaddleSegModel(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleSeg model initialize failed.\"\n\n    def predict(self, image):\n        \"\"\"Predict the segmentation result for an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: SegmentationResult\n        \"\"\"\n        return self._model.predict(image)\n\n    def batch_predict(self, image_list):\n        \"\"\"Predict the segmentation results for a batch of input images\n\n        :param image_list: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: list of SegmentationResult\n        \"\"\"\n        return self._model.batch_predict(image_list)\n\n    def clone(self):\n        \"\"\"Clone PaddleSegModel object\n\n        :return: a new PaddleSegModel object\n        \"\"\"\n\n        class PaddleSegCloneModel(PaddleSegModel):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PaddleSegCloneModel(self._model.clone())\n        return clone_model\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get PaddleSegPreprocessor object of the loaded model\n\n        :return: PaddleSegPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get PaddleSegPostprocessor object of the loaded model\n\n        :return: PaddleSegPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class PaddleSegPreprocessor(ProcessorManager):\n    def __init__(self, config_file):\n        \"\"\"Create a preprocessor for PaddleSegModel from configuration file\n\n        :param config_file: (str)Path of configuration file, e.g ppliteseg/deploy.yaml\n        \"\"\"\n        self._manager = C.vision.segmentation.PaddleSegPreprocessor(\n            config_file)\n\n    def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()\n\n    def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()\n\n    @property\n    def is_vertical_screen(self):\n        \"\"\"Atrribute of PP-HumanSeg model. Stating Whether the input image is vertical image(height > width), default value is False\n\n        :return: value of is_vertical_screen(bool)\n        \"\"\"\n        return self._manager.is_vertical_screen\n\n    @is_vertical_screen.setter\n    def is_vertical_screen(self, value):\n        \"\"\"Set attribute is_vertical_screen of PP-HumanSeg model.\n\n        :param value: (bool)The value to set is_vertical_screen\n        \"\"\"\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_vertical_screen` must be type of bool.\"\n        self._manager.is_vertical_screen = value",
  "class PaddleSegPostprocessor:\n    def __init__(self, config_file):\n        \"\"\"Create a postprocessor for PaddleSegModel from configuration file\n\n        :param config_file: (str)Path of configuration file, e.g ppliteseg/deploy.yaml\n        \"\"\"\n        self._postprocessor = C.vision.segmentation.PaddleSegPostprocessor(\n            config_file)\n\n    def run(self, runtime_results, imgs_info):\n        \"\"\"Postprocess the runtime results for PaddleSegModel\n\n        :param runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param imgs_info: The original input images shape info map, key is \"shape_info\", value is [[image_height, image_width]]\n        :return: list of SegmentationResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, imgs_info)\n\n    @property\n    def apply_softmax(self):\n        \"\"\"Atrribute of PaddleSeg model. Stating Whether applying softmax operator in the postprocess, default value is False\n\n        :return: value of apply_softmax(bool)\n        \"\"\"\n        return self._postprocessor.apply_softmax\n\n    @apply_softmax.setter\n    def apply_softmax(self, value):\n        \"\"\"Set attribute apply_softmax of PaddleSeg model.\n\n        :param value: (bool)The value to set apply_softmax\n        \"\"\"\n        assert isinstance(\n            value,\n            bool), \"The value to set `apply_softmax` must be type of bool.\"\n        self._postprocessor.apply_softmax = value\n\n    @property\n    def store_score_map(self):\n        \"\"\"Atrribute of PaddleSeg model. Stating Whether storing score map in the SegmentationResult, default value is False\n\n        :return: value of store_score_map(bool)\n        \"\"\"\n        return self._postprocessor.store_score_map\n\n    @store_score_map.setter\n    def store_score_map(self, value):\n        \"\"\"Set attribute store_score_map of PaddleSeg model.\n\n        :param value: (bool)The value to set store_score_map\n        \"\"\"\n        assert isinstance(\n            value,\n            bool), \"The value to set `store_score_map` must be type of bool.\"\n        self._postprocessor.store_score_map = value",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a image segmentation model exported by PaddleSeg.\n\n        :param model_file: (str)Path of model file, e.g unet/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g unet/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str) Path of configuration file for deploy, e.g unet/deploy.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PaddleSegModel, self).__init__(runtime_option)\n\n        # assert model_format == ModelFormat.PADDLE, \"PaddleSeg only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.segmentation.PaddleSegModel(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleSeg model initialize failed.\"",
  "def predict(self, image):\n        \"\"\"Predict the segmentation result for an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: SegmentationResult\n        \"\"\"\n        return self._model.predict(image)",
  "def batch_predict(self, image_list):\n        \"\"\"Predict the segmentation results for a batch of input images\n\n        :param image_list: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: list of SegmentationResult\n        \"\"\"\n        return self._model.batch_predict(image_list)",
  "def clone(self):\n        \"\"\"Clone PaddleSegModel object\n\n        :return: a new PaddleSegModel object\n        \"\"\"\n\n        class PaddleSegCloneModel(PaddleSegModel):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PaddleSegCloneModel(self._model.clone())\n        return clone_model",
  "def preprocessor(self):\n        \"\"\"Get PaddleSegPreprocessor object of the loaded model\n\n        :return: PaddleSegPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get PaddleSegPostprocessor object of the loaded model\n\n        :return: PaddleSegPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self, config_file):\n        \"\"\"Create a preprocessor for PaddleSegModel from configuration file\n\n        :param config_file: (str)Path of configuration file, e.g ppliteseg/deploy.yaml\n        \"\"\"\n        self._manager = C.vision.segmentation.PaddleSegPreprocessor(\n            config_file)",
  "def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()",
  "def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()",
  "def is_vertical_screen(self):\n        \"\"\"Atrribute of PP-HumanSeg model. Stating Whether the input image is vertical image(height > width), default value is False\n\n        :return: value of is_vertical_screen(bool)\n        \"\"\"\n        return self._manager.is_vertical_screen",
  "def is_vertical_screen(self, value):\n        \"\"\"Set attribute is_vertical_screen of PP-HumanSeg model.\n\n        :param value: (bool)The value to set is_vertical_screen\n        \"\"\"\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_vertical_screen` must be type of bool.\"\n        self._manager.is_vertical_screen = value",
  "def __init__(self, config_file):\n        \"\"\"Create a postprocessor for PaddleSegModel from configuration file\n\n        :param config_file: (str)Path of configuration file, e.g ppliteseg/deploy.yaml\n        \"\"\"\n        self._postprocessor = C.vision.segmentation.PaddleSegPostprocessor(\n            config_file)",
  "def run(self, runtime_results, imgs_info):\n        \"\"\"Postprocess the runtime results for PaddleSegModel\n\n        :param runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param imgs_info: The original input images shape info map, key is \"shape_info\", value is [[image_height, image_width]]\n        :return: list of SegmentationResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, imgs_info)",
  "def apply_softmax(self):\n        \"\"\"Atrribute of PaddleSeg model. Stating Whether applying softmax operator in the postprocess, default value is False\n\n        :return: value of apply_softmax(bool)\n        \"\"\"\n        return self._postprocessor.apply_softmax",
  "def apply_softmax(self, value):\n        \"\"\"Set attribute apply_softmax of PaddleSeg model.\n\n        :param value: (bool)The value to set apply_softmax\n        \"\"\"\n        assert isinstance(\n            value,\n            bool), \"The value to set `apply_softmax` must be type of bool.\"\n        self._postprocessor.apply_softmax = value",
  "def store_score_map(self):\n        \"\"\"Atrribute of PaddleSeg model. Stating Whether storing score map in the SegmentationResult, default value is False\n\n        :return: value of store_score_map(bool)\n        \"\"\"\n        return self._postprocessor.store_score_map",
  "def store_score_map(self, value):\n        \"\"\"Set attribute store_score_map of PaddleSeg model.\n\n        :param value: (bool)The value to set store_score_map\n        \"\"\"\n        assert isinstance(\n            value,\n            bool), \"The value to set `store_score_map` must be type of bool.\"\n        self._postprocessor.store_score_map = value",
  "class PaddleSegCloneModel(PaddleSegModel):\n            def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "class CenterpointPreprocessor:\n    def __init__(self, config_file):\n        \"\"\"Create a preprocessor for Centerpoint\n        \"\"\"\n        self._preprocessor = C.vision.perception.CenterpointPreprocessor(\n            config_file)\n\n    def run(self, point_dirs, num_point_dim, with_timelag):\n        \"\"\"Preprocess input images for Centerpoint\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(point_dirs, num_point_dim, with_timelag)",
  "class Centerpoint(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a Centerpoint model exported by Centerpoint.\n\n        :param model_file: (str)Path of model file, e.g ./Centerpoint.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ./Centerpoint.pdiparams\n        :param config_file: (str)Path of config file, e.g ./infer_cfg.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(Centerpoint, self).__init__(runtime_option)\n\n        self._model = C.vision.perception.Centerpoint(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"Centerpoint initialize failed.\"\n\n    def predict(self, point_dir):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: PerceptionResult\n        \"\"\"\n        return self._model.predict(point_dir)\n\n    def batch_predict(self, points_dir):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of PerceptionResult\n        \"\"\"\n\n        return self._model.batch_predict(points_dir)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get CenterpointPreprocessor object of the loaded model\n\n        :return CenterpointPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get CenterpointPostprocessor object of the loaded model\n\n        :return CenterpointPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self, config_file):\n        \"\"\"Create a preprocessor for Centerpoint\n        \"\"\"\n        self._preprocessor = C.vision.perception.CenterpointPreprocessor(\n            config_file)",
  "def run(self, point_dirs, num_point_dim, with_timelag):\n        \"\"\"Preprocess input images for Centerpoint\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(point_dirs, num_point_dim, with_timelag)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a Centerpoint model exported by Centerpoint.\n\n        :param model_file: (str)Path of model file, e.g ./Centerpoint.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ./Centerpoint.pdiparams\n        :param config_file: (str)Path of config file, e.g ./infer_cfg.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(Centerpoint, self).__init__(runtime_option)\n\n        self._model = C.vision.perception.Centerpoint(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"Centerpoint initialize failed.\"",
  "def predict(self, point_dir):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: PerceptionResult\n        \"\"\"\n        return self._model.predict(point_dir)",
  "def batch_predict(self, points_dir):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of PerceptionResult\n        \"\"\"\n\n        return self._model.batch_predict(points_dir)",
  "def preprocessor(self):\n        \"\"\"Get CenterpointPreprocessor object of the loaded model\n\n        :return CenterpointPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get CenterpointPostprocessor object of the loaded model\n\n        :return CenterpointPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class SmokePreprocessor:\n    def __init__(self, config_file):\n        \"\"\"Create a preprocessor for Smoke\n        \"\"\"\n        self._preprocessor = C.vision.perception.SmokePreprocessor(config_file)\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for Smoke\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "class SmokePostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for Smoke\n        \"\"\"\n        self._postprocessor = C.vision.perception.SmokePostprocessor()\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for Smoke\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of PerceptionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "class Smoke(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a SMoke model exported by Smoke.\n\n        :param model_file: (str)Path of model file, e.g ./smoke.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ./smoke.pdiparams\n        :param config_file: (str)Path of config file, e.g ./infer_cfg.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(Smoke, self).__init__(runtime_option)\n\n        self._model = C.vision.perception.Smoke(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"Smoke initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: PerceptionResult\n        \"\"\"\n        return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of PerceptionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get SmokePreprocessor object of the loaded model\n\n        :return SmokePreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get SmokePostprocessor object of the loaded model\n\n        :return SmokePostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self, config_file):\n        \"\"\"Create a preprocessor for Smoke\n        \"\"\"\n        self._preprocessor = C.vision.perception.SmokePreprocessor(config_file)",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for Smoke\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def __init__(self):\n        \"\"\"Create a postprocessor for Smoke\n        \"\"\"\n        self._postprocessor = C.vision.perception.SmokePostprocessor()",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for Smoke\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of PerceptionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a SMoke model exported by Smoke.\n\n        :param model_file: (str)Path of model file, e.g ./smoke.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ./smoke.pdiparams\n        :param config_file: (str)Path of config file, e.g ./infer_cfg.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(Smoke, self).__init__(runtime_option)\n\n        self._model = C.vision.perception.Smoke(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"Smoke initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: PerceptionResult\n        \"\"\"\n        return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of PerceptionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get SmokePreprocessor object of the loaded model\n\n        :return SmokePreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get SmokePostprocessor object of the loaded model\n\n        :return SmokePostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class CaddnPreprocessor:\n    def __init__(self, config_file):\n        \"\"\"Create a preprocessor for Caddn\n        \"\"\"\n        self._preprocessor = C.vision.perception.CaddnPreprocessor(config_file)\n\n    def run(self, input_ims, cam_data, lidar_data):\n        \"\"\"Preprocess input images for Caddn\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims, cam_data, lidar_data)",
  "class CaddnPostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for Caddn\n        \"\"\"\n        self._postprocessor = C.vision.perception.CaddnPostprocessor()\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for Caddn\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of PerceptionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "class Caddn(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a Caddn model exported by Caddn.\n\n        :param model_file: (str)Path of model file, e.g ./Caddn.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ./Caddn.pdiparams\n        :param config_file: (str)Path of config file, e.g ./infer_cfg.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(Caddn, self).__init__(runtime_option)\n\n        self._model = C.vision.perception.Caddn(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"Caddn initialize failed.\"\n\n    def predict(self, input_image, cam_data, lidar_data):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param: cam_data: (list)The input camera data\n        :param: lidar_data: (list)The input lidar data\n        :return: PerceptionResult\n        \"\"\"\n        return self._model.predict(input_image, cam_data, lidar_data)\n\n    def batch_predict(self, images, cam_data, lidar_data):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :param: cam_data: (list)The input camera data\n        :param: lidar_data: (list)The input lidar data\n        :return list of PerceptionResult\n        \"\"\"\n\n        return self._model.batch_predict(images, cam_data, lidar_data)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get CaddnPreprocessor object of the loaded model\n\n        :return CaddnPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get CaddnPostprocessor object of the loaded model\n\n        :return CaddnPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self, config_file):\n        \"\"\"Create a preprocessor for Caddn\n        \"\"\"\n        self._preprocessor = C.vision.perception.CaddnPreprocessor(config_file)",
  "def run(self, input_ims, cam_data, lidar_data):\n        \"\"\"Preprocess input images for Caddn\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims, cam_data, lidar_data)",
  "def __init__(self):\n        \"\"\"Create a postprocessor for Caddn\n        \"\"\"\n        self._postprocessor = C.vision.perception.CaddnPostprocessor()",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for Caddn\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of PerceptionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a Caddn model exported by Caddn.\n\n        :param model_file: (str)Path of model file, e.g ./Caddn.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ./Caddn.pdiparams\n        :param config_file: (str)Path of config file, e.g ./infer_cfg.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(Caddn, self).__init__(runtime_option)\n\n        self._model = C.vision.perception.Caddn(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"Caddn initialize failed.\"",
  "def predict(self, input_image, cam_data, lidar_data):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param: cam_data: (list)The input camera data\n        :param: lidar_data: (list)The input lidar data\n        :return: PerceptionResult\n        \"\"\"\n        return self._model.predict(input_image, cam_data, lidar_data)",
  "def batch_predict(self, images, cam_data, lidar_data):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :param: cam_data: (list)The input camera data\n        :param: lidar_data: (list)The input lidar data\n        :return list of PerceptionResult\n        \"\"\"\n\n        return self._model.batch_predict(images, cam_data, lidar_data)",
  "def preprocessor(self):\n        \"\"\"Get CaddnPreprocessor object of the loaded model\n\n        :return CaddnPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get CaddnPostprocessor object of the loaded model\n\n        :return CaddnPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class PetrPreprocessor:\n    def __init__(self, config_file):\n        \"\"\"Create a preprocessor for Petr\n        \"\"\"\n        self._preprocessor = C.vision.perception.PetrPreprocessor(config_file)\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for Petr\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "class PetrPostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for Petr\n        \"\"\"\n        self._postprocessor = C.vision.perception.PetrPostprocessor()\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for Petr\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of PerceptionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "class Petr(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a SMoke model exported by Petr.\n\n        :param model_file: (str)Path of model file, e.g ./petr.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ./petr.pdiparams\n        :param config_file: (str)Path of config file, e.g ./infer_cfg.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(Petr, self).__init__(runtime_option)\n\n        self._model = C.vision.perception.Petr(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"Petr initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: PerceptionResult\n        \"\"\"\n        return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of PerceptionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get PetrPreprocessor object of the loaded model\n\n        :return PetrPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get PetrPostprocessor object of the loaded model\n\n        :return PetrPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self, config_file):\n        \"\"\"Create a preprocessor for Petr\n        \"\"\"\n        self._preprocessor = C.vision.perception.PetrPreprocessor(config_file)",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for Petr\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def __init__(self):\n        \"\"\"Create a postprocessor for Petr\n        \"\"\"\n        self._postprocessor = C.vision.perception.PetrPostprocessor()",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for Petr\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of PerceptionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a SMoke model exported by Petr.\n\n        :param model_file: (str)Path of model file, e.g ./petr.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ./petr.pdiparams\n        :param config_file: (str)Path of config file, e.g ./infer_cfg.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(Petr, self).__init__(runtime_option)\n\n        self._model = C.vision.perception.Petr(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"Petr initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Detect an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threshold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threshold for NMS, default is 0.5\n        :return: PerceptionResult\n        \"\"\"\n        return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of PerceptionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get PetrPreprocessor object of the loaded model\n\n        :return PetrPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get PetrPostprocessor object of the loaded model\n\n        :return PetrPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class ProcessorManager:\n    def __init__(self):\n        self._manager = None\n\n    def run(self, input_ims):\n        \"\"\"Process input image\n\n        :param: input_ims: (list of numpy.ndarray) The input images\n        :return: list of FDTensor\n        \"\"\"\n        return self._manager.run(input_ims)\n\n    def use_cuda(self, enable_cv_cuda=False, gpu_id=-1):\n        \"\"\"Use CUDA processors\n\n        :param: enable_cv_cuda: Ture: use CV-CUDA, False: use CUDA only\n        :param: gpu_id: GPU device id\n        \"\"\"\n        return self._manager.use_cuda(enable_cv_cuda, gpu_id)",
  "class PyProcessorManager(ABC):\n    \"\"\"\n    PyProcessorManager is used to define a customized processor in python\n    \"\"\"\n\n    def __init__(self):\n        self._manager = C.vision.processors.ProcessorManager()\n\n    def use_cuda(self, enable_cv_cuda=False, gpu_id=-1):\n        \"\"\"Use CUDA processors\n\n        :param: enable_cv_cuda: Ture: use CV-CUDA, False: use CUDA only\n        :param: gpu_id: GPU device id\n        \"\"\"\n        return self._manager.use_cuda(enable_cv_cuda, gpu_id)\n\n    def __call__(self, images):\n        image_batch = C.vision.FDMatBatch()\n        image_batch.from_mats(images)\n\n        self._manager.pre_apply(image_batch)\n        outputs = self.apply(image_batch)\n        self._manager.post_apply()\n        return outputs\n\n    @abstractmethod\n    def apply(self, image_batch):\n        print(\"This function has to be implemented.\")\n        return []",
  "def __init__(self):\n        self._manager = None",
  "def run(self, input_ims):\n        \"\"\"Process input image\n\n        :param: input_ims: (list of numpy.ndarray) The input images\n        :return: list of FDTensor\n        \"\"\"\n        return self._manager.run(input_ims)",
  "def use_cuda(self, enable_cv_cuda=False, gpu_id=-1):\n        \"\"\"Use CUDA processors\n\n        :param: enable_cv_cuda: Ture: use CV-CUDA, False: use CUDA only\n        :param: gpu_id: GPU device id\n        \"\"\"\n        return self._manager.use_cuda(enable_cv_cuda, gpu_id)",
  "def __init__(self):\n        self._manager = C.vision.processors.ProcessorManager()",
  "def use_cuda(self, enable_cv_cuda=False, gpu_id=-1):\n        \"\"\"Use CUDA processors\n\n        :param: enable_cv_cuda: Ture: use CV-CUDA, False: use CUDA only\n        :param: gpu_id: GPU device id\n        \"\"\"\n        return self._manager.use_cuda(enable_cv_cuda, gpu_id)",
  "def __call__(self, images):\n        image_batch = C.vision.FDMatBatch()\n        image_batch.from_mats(images)\n\n        self._manager.pre_apply(image_batch)\n        outputs = self.apply(image_batch)\n        self._manager.post_apply()\n        return outputs",
  "def apply(self, image_batch):\n        print(\"This function has to be implemented.\")\n        return []",
  "class Processor():\n    def __init__(self):\n        self.processor = None\n\n    def __call__(self, mat):\n        \"\"\"call for processing input.\n\n        :param mat: The input data FDMat or FDMatBatch.\n        \"\"\"\n        self.processor(mat)",
  "class ResizeByShort(Processor):\n    def __init__(self, target_size: int, interp=1, use_scale=True, max_hw=[]):\n        \"\"\"Create a ResizeByShort operation with the given parameters.\n\n        :param target_size: The target short size to resize the image\n        :param interp: Optionally, the interpolation mode for resizing image\n        :param use_scale: Optionally, whether to scale image\n        :param max_hw: Max spatial size which is used by ResizeByShort\n        \"\"\"\n        self.processor = C.vision.processors.ResizeByShort(target_size, interp,\n                                                           use_scale, max_hw)",
  "class CenterCrop(Processor):\n    def __init__(self, width, height):\n        \"\"\"Create a CenterCrop operation with the given parameters.\n\n        :param width: Desired width of the cropped image\n        :param height: Desired height of the cropped image\n        \"\"\"\n        self.processor = C.vision.processors.CenterCrop(width, height)",
  "class Pad(Processor):\n    def __init__(self, top: int, bottom: int, left: int, right: int, value=[]):\n        \"\"\"Create a Pad operation with the given parameters.\n\n        :param top: The top padding\n        :param bottom: The bottom padding\n        :param left: The left padding\n        :param right: The right padding\n        :param value: the value that is used to pad on the input image\n        \"\"\"\n        self.processor = C.vision.processors.Pad(top, bottom, left, right,\n                                                 value)",
  "class NormalizeAndPermute(Processor):\n    def __init__(self,\n                 mean=[],\n                 std=[],\n                 is_scale=True,\n                 min=[],\n                 max=[],\n                 swap_rb=False):\n        \"\"\"Creae a Normalize and a Permute operation with the given parameters.\n\n        :param mean: A list containing the mean of each channel\n        :param std: A list containing the standard deviation of each channel\n        :param is_scale: Specifies if the image are being scaled or not\n        :param min: A list containing the minimum value of each channel\n        :param max: A list containing the maximum value of each channel\n        \"\"\"\n        self.processor = C.vision.processors.NormalizeAndPermute(\n            mean, std, is_scale, min, max, swap_rb)",
  "class Cast(Processor):\n    def __init__(self, dtype=\"float\"):\n        \"\"\"Creat a new cast opereaton with given dtype\n\n        :param dtype: Target dtype of the output\n        \"\"\"\n        self.processor = C.vision.processors.Cast(dtype)",
  "class HWC2CHW(Processor):\n    def __init__(self):\n        \"\"\"Creat a new hwc2chw processor with default dtype.\n\n        :return An instance of processor `HWC2CHW`\n        \"\"\"\n        self.processor = C.vision.processors.HWC2CHW()",
  "class Normalize(Processor):\n    def __init__(self, mean, std, is_scale=True, min=[], max=[],\n                 swap_rb=False):\n        \"\"\"Creat a new normalize opereator with given paremeters.\n\n        :param mean: A list containing the mean of each channel\n        :param std: A list containing the standard deviation of each channel\n        :param is_scale: Specifies if the image are being scaled or not\n        :param min: A list containing the minimum value of each channel\n        :param max: A list containing the maximum value of each channel\n        \"\"\"\n        self.processor = C.vision.processors.Normalize(mean, std, is_scale,\n                                                       min, max, swap_rb)",
  "class PadToSize(Processor):\n    def __init__(self, width, height, value=[]):\n        \"\"\"Create a new PadToSize opereator with given parameters.\n\n        :param width: Desired width of the output image\n        :param height: Desired height of the output image\n        :param value: Values to pad with\n        \"\"\"\n        self.processor = C.vision.processors.PadToSize(width, height, value)",
  "class Resize(Processor):\n    def __init__(self,\n                 width,\n                 height,\n                 scale_w=-1.0,\n                 scale_h=-1.0,\n                 interp=1,\n                 use_scale=False):\n        \"\"\"Create a Resize operation with the given parameters.\n\n        :param width: Desired width of the output image\n        :param height: Desired height of the output image\n        :param scale_w: Scales the width in x-direction\n        :param scale_h: Scales the height in y-direction\n        :param interp: Optionally, the interpolation mode for resizing image\n        :param use_scale: Optionally, whether to scale image\n        \"\"\"\n        self.processor = C.vision.processors.Resize(width, height, scale_w,\n                                                    scale_h, interp, use_scale)",
  "class StridePad(Processor):\n    def __init__(self, stride, value=[]):\n        \"\"\"Create a StridePad processor with given parameters.\n\n        :param stride: Stride of the processor\n        :param value: Values to pad with\n        \"\"\"\n        self.processor = C.vision.processors.StridePad(stride, value)",
  "def __init__(self):\n        self.processor = None",
  "def __call__(self, mat):\n        \"\"\"call for processing input.\n\n        :param mat: The input data FDMat or FDMatBatch.\n        \"\"\"\n        self.processor(mat)",
  "def __init__(self, target_size: int, interp=1, use_scale=True, max_hw=[]):\n        \"\"\"Create a ResizeByShort operation with the given parameters.\n\n        :param target_size: The target short size to resize the image\n        :param interp: Optionally, the interpolation mode for resizing image\n        :param use_scale: Optionally, whether to scale image\n        :param max_hw: Max spatial size which is used by ResizeByShort\n        \"\"\"\n        self.processor = C.vision.processors.ResizeByShort(target_size, interp,\n                                                           use_scale, max_hw)",
  "def __init__(self, width, height):\n        \"\"\"Create a CenterCrop operation with the given parameters.\n\n        :param width: Desired width of the cropped image\n        :param height: Desired height of the cropped image\n        \"\"\"\n        self.processor = C.vision.processors.CenterCrop(width, height)",
  "def __init__(self, top: int, bottom: int, left: int, right: int, value=[]):\n        \"\"\"Create a Pad operation with the given parameters.\n\n        :param top: The top padding\n        :param bottom: The bottom padding\n        :param left: The left padding\n        :param right: The right padding\n        :param value: the value that is used to pad on the input image\n        \"\"\"\n        self.processor = C.vision.processors.Pad(top, bottom, left, right,\n                                                 value)",
  "def __init__(self,\n                 mean=[],\n                 std=[],\n                 is_scale=True,\n                 min=[],\n                 max=[],\n                 swap_rb=False):\n        \"\"\"Creae a Normalize and a Permute operation with the given parameters.\n\n        :param mean: A list containing the mean of each channel\n        :param std: A list containing the standard deviation of each channel\n        :param is_scale: Specifies if the image are being scaled or not\n        :param min: A list containing the minimum value of each channel\n        :param max: A list containing the maximum value of each channel\n        \"\"\"\n        self.processor = C.vision.processors.NormalizeAndPermute(\n            mean, std, is_scale, min, max, swap_rb)",
  "def __init__(self, dtype=\"float\"):\n        \"\"\"Creat a new cast opereaton with given dtype\n\n        :param dtype: Target dtype of the output\n        \"\"\"\n        self.processor = C.vision.processors.Cast(dtype)",
  "def __init__(self):\n        \"\"\"Creat a new hwc2chw processor with default dtype.\n\n        :return An instance of processor `HWC2CHW`\n        \"\"\"\n        self.processor = C.vision.processors.HWC2CHW()",
  "def __init__(self, mean, std, is_scale=True, min=[], max=[],\n                 swap_rb=False):\n        \"\"\"Creat a new normalize opereator with given paremeters.\n\n        :param mean: A list containing the mean of each channel\n        :param std: A list containing the standard deviation of each channel\n        :param is_scale: Specifies if the image are being scaled or not\n        :param min: A list containing the minimum value of each channel\n        :param max: A list containing the maximum value of each channel\n        \"\"\"\n        self.processor = C.vision.processors.Normalize(mean, std, is_scale,\n                                                       min, max, swap_rb)",
  "def __init__(self, width, height, value=[]):\n        \"\"\"Create a new PadToSize opereator with given parameters.\n\n        :param width: Desired width of the output image\n        :param height: Desired height of the output image\n        :param value: Values to pad with\n        \"\"\"\n        self.processor = C.vision.processors.PadToSize(width, height, value)",
  "def __init__(self,\n                 width,\n                 height,\n                 scale_w=-1.0,\n                 scale_h=-1.0,\n                 interp=1,\n                 use_scale=False):\n        \"\"\"Create a Resize operation with the given parameters.\n\n        :param width: Desired width of the output image\n        :param height: Desired height of the output image\n        :param scale_w: Scales the width in x-direction\n        :param scale_h: Scales the height in y-direction\n        :param interp: Optionally, the interpolation mode for resizing image\n        :param use_scale: Optionally, whether to scale image\n        \"\"\"\n        self.processor = C.vision.processors.Resize(width, height, scale_w,\n                                                    scale_h, interp, use_scale)",
  "def __init__(self, stride, value=[]):\n        \"\"\"Create a StridePad processor with given parameters.\n\n        :param stride: Stride of the processor\n        :param value: Values to pad with\n        \"\"\"\n        self.processor = C.vision.processors.StridePad(stride, value)",
  "class PPTracking(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPTracking model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g pptracking/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyoloe/model.pdiparams\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPTracking, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPTracking model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.tracking.PPTracking(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPTracking model initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Predict the MOT result for an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: MOTResult\n        \"\"\"\n        assert input_image is not None, \"The input image data is None.\"\n        return self._model.predict(input_image)\n\n    def bind_recorder(self, val):\n        \"\"\" Binding tracking trail\n\n        :param val: (TrailRecorder) trail recorder, which is contained object's id and center point sequence\n        :return: None\n        \"\"\"\n        self._model.bind_recorder(val)\n\n    def unbind_recorder(self):\n        \"\"\" cancel binding of tracking trail\n\n        :return:\n        \"\"\"\n        self._model.unbind_recorder()",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPTracking model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g pptracking/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g ppyoloe/model.pdiparams\n        :param config_file: (str)Path of configuration file for deployment, e.g ppyoloe/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPTracking, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPTracking model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.tracking.PPTracking(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPTracking model initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Predict the MOT result for an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: MOTResult\n        \"\"\"\n        assert input_image is not None, \"The input image data is None.\"\n        return self._model.predict(input_image)",
  "def bind_recorder(self, val):\n        \"\"\" Binding tracking trail\n\n        :param val: (TrailRecorder) trail recorder, which is contained object's id and center point sequence\n        :return: None\n        \"\"\"\n        self._model.bind_recorder(val)",
  "def unbind_recorder(self):\n        \"\"\" cancel binding of tracking trail\n\n        :return:\n        \"\"\"\n        self._model.unbind_recorder()",
  "class PPMatting(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPMatting model exported by PaddleSeg.\n\n        :param model_file: (str)Path of model file, e.g PPMatting-512/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g PPMatting-512/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g PPMatting-512/deploy.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPMatting, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPMatting model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.matting.PPMatting(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPMatting model initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\" Predict the matting result for an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: MattingResult\n        \"\"\"\n        assert input_image is not None, \"The input image data is None.\"\n        return self._model.predict(input_image)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a PPMatting model exported by PaddleSeg.\n\n        :param model_file: (str)Path of model file, e.g PPMatting-512/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g PPMatting-512/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g PPMatting-512/deploy.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPMatting, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPMatting model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.matting.PPMatting(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPMatting model initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\" Predict the matting result for an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: MattingResult\n        \"\"\"\n        assert input_image is not None, \"The input image data is None.\"\n        return self._model.predict(input_image)",
  "class MODNet(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a MODNet model exported by MODNet.\n\n        :param model_file: (str)Path of model file, e.g ./modnet.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(MODNet, self).__init__(runtime_option)\n\n        self._model = C.vision.matting.MODNet(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"MODNet initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\" Predict the matting result for an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: MattingResult\n        \"\"\"\n        return self._model.predict(input_image)\n\n    # \u4e00\u4e9b\u8ddf\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [256, 256]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [256,256]\n        \"\"\"\n        return self._model.size\n\n    @property\n    def alpha(self):\n        \"\"\"\n        Argument for image preprocessing step, alpha value for normalization, default alpha = {1.f / 127.5f, 1.f / 127.5f, 1.f / 127.5f}\n        \"\"\"\n        return self._model.alpha\n\n    @property\n    def beta(self):\n        \"\"\"\n        Argument for image preprocessing step, beta value for normalization, default beta = {-1.f, -1.f, -1.f}\n        \"\"\"\n        return self._model.beta\n\n    @property\n    def swap_rb(self):\n        \"\"\"\n        Argument for image preprocessing step, whether to swap the B and R channel, such as BGR->RGB, default True.\n        \"\"\"\n        return self._model.swap_rb\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @alpha.setter\n    def alpha(self, value):\n        assert isinstance(value, (list, tuple)),\\\n            \"The value to set `alpha` must be type of tuple or list.\"\n        assert len(value) == 3,\\\n            \"The value to set `alpha` must contatins 3 elements for each channels, but now it contains {} elements.\".format(\n            len(value))\n        self._model.alpha = value\n\n    @beta.setter\n    def beta(self, value):\n        assert isinstance(value, (list, tuple)),\\\n            \"The value to set `beta` must be type of tuple or list.\"\n        assert len(value) == 3,\\\n            \"The value to set `beta` must contatins 3 elements for each channels, but now it contains {} elements.\".format(\n            len(value))\n        self._model.beta = value\n\n    @swap_rb.setter\n    def swap_rb(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `swap_rb` must be type of bool.\"\n        self._model.swap_rb = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a MODNet model exported by MODNet.\n\n        :param model_file: (str)Path of model file, e.g ./modnet.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(MODNet, self).__init__(runtime_option)\n\n        self._model = C.vision.matting.MODNet(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"MODNet initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\" Predict the matting result for an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: MattingResult\n        \"\"\"\n        return self._model.predict(input_image)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [256,256]\n        \"\"\"\n        return self._model.size",
  "def alpha(self):\n        \"\"\"\n        Argument for image preprocessing step, alpha value for normalization, default alpha = {1.f / 127.5f, 1.f / 127.5f, 1.f / 127.5f}\n        \"\"\"\n        return self._model.alpha",
  "def beta(self):\n        \"\"\"\n        Argument for image preprocessing step, beta value for normalization, default beta = {-1.f, -1.f, -1.f}\n        \"\"\"\n        return self._model.beta",
  "def swap_rb(self):\n        \"\"\"\n        Argument for image preprocessing step, whether to swap the B and R channel, such as BGR->RGB, default True.\n        \"\"\"\n        return self._model.swap_rb",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def alpha(self, value):\n        assert isinstance(value, (list, tuple)),\\\n            \"The value to set `alpha` must be type of tuple or list.\"\n        assert len(value) == 3,\\\n            \"The value to set `alpha` must contatins 3 elements for each channels, but now it contains {} elements.\".format(\n            len(value))\n        self._model.alpha = value",
  "def beta(self, value):\n        assert isinstance(value, (list, tuple)),\\\n            \"The value to set `beta` must be type of tuple or list.\"\n        assert len(value) == 3,\\\n            \"The value to set `beta` must contatins 3 elements for each channels, but now it contains {} elements.\".format(\n            len(value))\n        self._model.beta = value",
  "def swap_rb(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `swap_rb` must be type of bool.\"\n        self._model.swap_rb = value",
  "class RobustVideoMatting(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a video matting model exported by RobustVideoMatting.\n\n        :param model_file: (str)Path of model file, e.g rvm/rvm_mobilenetv3_fp32.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n        super(RobustVideoMatting, self).__init__(runtime_option)\n\n        self._model = C.vision.matting.RobustVideoMatting(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"RobustVideoMatting initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Matting an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: MattingResult\n        \"\"\"\n        return self._model.predict(input_image)\n\n    @property\n    def size(self):\n        \"\"\"\n        Returns the preprocess image size\n        \"\"\"\n        return self._model.size\n\n    @property\n    def video_mode(self):\n        \"\"\"\n        Whether to open the video mode, if there are some irrelevant pictures, set it to fasle, the default is true\n        \"\"\"\n        return self._model.video_mode\n\n    @property\n    def swap_rb(self):\n        \"\"\"\n        Whether convert to RGB, Set to false if you have converted YUV format images to RGB outside the model, dafault true\n        \"\"\"\n        return self._model.swap_rb\n\n    @size.setter\n    def size(self, wh):\n        \"\"\"\n        Set the preprocess image size\n        \"\"\"\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @video_mode.setter\n    def video_mode(self, value):\n        \"\"\"\n        Set video_mode property, the default is true\n        \"\"\"\n        assert isinstance(\n            value, bool), \"The value to set `video_mode` must be type of bool.\"\n        self._model.video_mode = value\n\n    @swap_rb.setter\n    def swap_rb(self, value):\n        \"\"\"\n        Set swap_rb property, the default is true\n        \"\"\"\n        assert isinstance(\n            value, bool), \"The value to set `swap_rb` must be type of bool.\"\n        self._model.swap_rb = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a video matting model exported by RobustVideoMatting.\n\n        :param model_file: (str)Path of model file, e.g rvm/rvm_mobilenetv3_fp32.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n        super(RobustVideoMatting, self).__init__(runtime_option)\n\n        self._model = C.vision.matting.RobustVideoMatting(\n            model_file, params_file, self._runtime_option, model_format)\n        assert self.initialized, \"RobustVideoMatting initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Matting an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: MattingResult\n        \"\"\"\n        return self._model.predict(input_image)",
  "def size(self):\n        \"\"\"\n        Returns the preprocess image size\n        \"\"\"\n        return self._model.size",
  "def video_mode(self):\n        \"\"\"\n        Whether to open the video mode, if there are some irrelevant pictures, set it to fasle, the default is true\n        \"\"\"\n        return self._model.video_mode",
  "def swap_rb(self):\n        \"\"\"\n        Whether convert to RGB, Set to false if you have converted YUV format images to RGB outside the model, dafault true\n        \"\"\"\n        return self._model.swap_rb",
  "def size(self, wh):\n        \"\"\"\n        Set the preprocess image size\n        \"\"\"\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def video_mode(self, value):\n        \"\"\"\n        Set video_mode property, the default is true\n        \"\"\"\n        assert isinstance(\n            value, bool), \"The value to set `video_mode` must be type of bool.\"\n        self._model.video_mode = value",
  "def swap_rb(self, value):\n        \"\"\"\n        Set swap_rb property, the default is true\n        \"\"\"\n        assert isinstance(\n            value, bool), \"The value to set `swap_rb` must be type of bool.\"\n        self._model.swap_rb = value",
  "class PPTinyPose(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"load a PPTinyPose model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g pptinypose/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g pptinypose/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g pptinypose/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPTinyPose, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPTinyPose model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.keypointdetection.PPTinyPose(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPTinyPose model initialize failed.\"\n\n    def predict(self, input_image, detection_result=None):\n        \"\"\"Detect keypoints in an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param detection_result: (DetectionResult)Pre-detected boxes result, default is None\n        :return: KeyPointDetectionResult\n        \"\"\"\n        assert input_image is not None, \"The input image data is None.\"\n        if detection_result:\n            return self._model.predict(input_image, detection_result)\n        else:\n            return self._model.predict(input_image)\n\n    @property\n    def use_dark(self):\n        \"\"\"Atrribute of PPTinyPose model. Stating whether using Distribution-Aware Coordinate Representation for Human Pose Estimation(DARK for short) in postprocess, default is True\n\n        :return: value of use_dark(bool)\n        \"\"\"\n        return self._model.use_dark\n\n    @use_dark.setter\n    def use_dark(self, value):\n        \"\"\"Set attribute use_dark of PPTinyPose model.\n\n        :param value: (bool)The value to set use_dark\n        \"\"\"\n        assert isinstance(\n            value, bool), \"The value to set `use_dark` must be type of bool.\"\n        self._model.use_dark = value\n\n    def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._model.disable_normalize()\n\n    def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._model.disable_permute()",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"load a PPTinyPose model exported by PaddleDetection.\n\n        :param model_file: (str)Path of model file, e.g pptinypose/model.pdmodel\n        :param params_file: (str)Path of parameters file, e.g pptinypose/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str)Path of configuration file for deployment, e.g pptinypose/infer_cfg.yml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPTinyPose, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPTinyPose model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.keypointdetection.PPTinyPose(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPTinyPose model initialize failed.\"",
  "def predict(self, input_image, detection_result=None):\n        \"\"\"Detect keypoints in an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param detection_result: (DetectionResult)Pre-detected boxes result, default is None\n        :return: KeyPointDetectionResult\n        \"\"\"\n        assert input_image is not None, \"The input image data is None.\"\n        if detection_result:\n            return self._model.predict(input_image, detection_result)\n        else:\n            return self._model.predict(input_image)",
  "def use_dark(self):\n        \"\"\"Atrribute of PPTinyPose model. Stating whether using Distribution-Aware Coordinate Representation for Human Pose Estimation(DARK for short) in postprocess, default is True\n\n        :return: value of use_dark(bool)\n        \"\"\"\n        return self._model.use_dark",
  "def use_dark(self, value):\n        \"\"\"Set attribute use_dark of PPTinyPose model.\n\n        :param value: (bool)The value to set use_dark\n        \"\"\"\n        assert isinstance(\n            value, bool), \"The value to set `use_dark` must be type of bool.\"\n        self._model.use_dark = value",
  "def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._model.disable_normalize()",
  "def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._model.disable_permute()",
  "class PPMSVSR(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a VSR model exported by PaddleGAN.\n\n        :param model_file: (str)Path of model file, e.g PPMSVSR/inference.pdmodel\n        :param params_file: (str)Path of parameters file, e.g PPMSVSR/inference.pdiparams\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPMSVSR, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPMSVSR model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.sr.PPMSVSR(model_file, params_file,\n                                          self._runtime_option, model_format)\n        assert self.initialized, \"PPMSVSR model initialize failed.\"\n\n    def predict(self, input_images):\n        \"\"\"Predict the super resolution frame sequences for an input frame sequences\n\n        :param input_images: list[numpy.ndarray] The input image data, 3-D array with layout HWC, BGR format\n        :return: list[numpy.ndarray]\n        \"\"\"\n        assert input_images is not None, \"The input image data is None.\"\n        return self._model.predict(input_images)",
  "class EDVR(PPMSVSR):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a EDVR model exported by PaddleGAN.\n\n        :param model_file: (str)Path of model file, e.g EDVR/inference.pdmodel\n        :param params_file: (str)Path of parameters file, e.g EDVR/inference.pdiparams\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPMSVSR, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"EDVR model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.sr.EDVR(model_file, params_file,\n                                       self._runtime_option, model_format)\n        assert self.initialized, \"EDVR model initialize failed.\"\n\n    def predict(self, input_images):\n        \"\"\"Predict the super resolution frame sequences for an input frame sequences\n\n        :param input_images: list[numpy.ndarray] The input image data, 3-D array with layout HWC, BGR format\n        :return: list[numpy.ndarray]\n        \"\"\"\n        assert input_images is not None, \"The input image data is None.\"\n        return self._model.predict(input_images)",
  "class BasicVSR(PPMSVSR):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a EDVR model exported by PaddleGAN.\n\n        :param model_file: (str)Path of model file, e.g BasicVSR/inference.pdmodel\n        :param params_file: (str)Path of parameters file, e.g BasicVSR/inference.pdiparams\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPMSVSR, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"BasicVSR model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.sr.BasicVSR(model_file, params_file,\n                                           self._runtime_option, model_format)\n        assert self.initialized, \"BasicVSR model initialize failed.\"\n\n    def predict(self, input_images):\n        \"\"\"Predict the super resolution frame sequences for an input frame sequences\n\n        :param input_images: list[numpy.ndarray] The input image data, 3-D array with layout HWC, BGR format\n        :return: list[numpy.ndarray]\n        \"\"\"\n        assert input_images is not None, \"The input image data is None.\"\n        return self._model.predict(input_images)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a VSR model exported by PaddleGAN.\n\n        :param model_file: (str)Path of model file, e.g PPMSVSR/inference.pdmodel\n        :param params_file: (str)Path of parameters file, e.g PPMSVSR/inference.pdiparams\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPMSVSR, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"PPMSVSR model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.sr.PPMSVSR(model_file, params_file,\n                                          self._runtime_option, model_format)\n        assert self.initialized, \"PPMSVSR model initialize failed.\"",
  "def predict(self, input_images):\n        \"\"\"Predict the super resolution frame sequences for an input frame sequences\n\n        :param input_images: list[numpy.ndarray] The input image data, 3-D array with layout HWC, BGR format\n        :return: list[numpy.ndarray]\n        \"\"\"\n        assert input_images is not None, \"The input image data is None.\"\n        return self._model.predict(input_images)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a EDVR model exported by PaddleGAN.\n\n        :param model_file: (str)Path of model file, e.g EDVR/inference.pdmodel\n        :param params_file: (str)Path of parameters file, e.g EDVR/inference.pdiparams\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPMSVSR, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"EDVR model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.sr.EDVR(model_file, params_file,\n                                       self._runtime_option, model_format)\n        assert self.initialized, \"EDVR model initialize failed.\"",
  "def predict(self, input_images):\n        \"\"\"Predict the super resolution frame sequences for an input frame sequences\n\n        :param input_images: list[numpy.ndarray] The input image data, 3-D array with layout HWC, BGR format\n        :return: list[numpy.ndarray]\n        \"\"\"\n        assert input_images is not None, \"The input image data is None.\"\n        return self._model.predict(input_images)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a EDVR model exported by PaddleGAN.\n\n        :param model_file: (str)Path of model file, e.g BasicVSR/inference.pdmodel\n        :param params_file: (str)Path of parameters file, e.g BasicVSR/inference.pdiparams\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(PPMSVSR, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.PADDLE, \"BasicVSR model only support model format of ModelFormat.Paddle now.\"\n        self._model = C.vision.sr.BasicVSR(model_file, params_file,\n                                           self._runtime_option, model_format)\n        assert self.initialized, \"BasicVSR model initialize failed.\"",
  "def predict(self, input_images):\n        \"\"\"Predict the super resolution frame sequences for an input frame sequences\n\n        :param input_images: list[numpy.ndarray] The input image data, 3-D array with layout HWC, BGR format\n        :return: list[numpy.ndarray]\n        \"\"\"\n        assert input_images is not None, \"The input image data is None.\"\n        return self._model.predict(input_images)",
  "class PaddleClasPreprocessor(ProcessorManager):\n    def __init__(self, config_file):\n        \"\"\"Create a preprocessor for PaddleClasModel from configuration file\n\n        :param config_file: (str)Path of configuration file, e.g resnet50/inference_cls.yaml\n        \"\"\"\n        super(PaddleClasPreprocessor, self).__init__()\n        self._manager = C.vision.classification.PaddleClasPreprocessor(\n            config_file)\n\n    def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()\n\n    def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()\n\n    def initial_resize_on_cpu(self, v):\n        \"\"\"\n        When the initial operator is Resize, and input image size is large,\n        maybe it's better to run resize on CPU, because the HostToDevice memcpy\n        is time consuming. Set this True to run the initial resize on CPU.\n        :param: v: True or False\n        \"\"\"\n        self._manager.initial_resize_on_cpu(v)",
  "class PaddleClasPostprocessor:\n    def __init__(self, topk=1):\n        \"\"\"Create a postprocessor for PaddleClasModel\n\n        :param topk: (int)Filter the top k classify label\n        \"\"\"\n        self._postprocessor = C.vision.classification.PaddleClasPostprocessor(\n            topk)\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for PaddleClasModel\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of ClassifyResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "class PaddleClasModel(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a image classification model exported by PaddleClas.\n\n        :param model_file: (str)Path of model file, e.g resnet50/inference.pdmodel\n        :param params_file: (str)Path of parameters file, e.g resnet50/inference.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str) Path of configuration file for deploy, e.g resnet50/inference_cls.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PaddleClasModel, self).__init__(runtime_option)\n        self._model = C.vision.classification.PaddleClasModel(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleClas model initialize failed.\"\n\n    def clone(self):\n        \"\"\"Clone PaddleClasModel object\n\n        :return: a new PaddleClasModel object\n        \"\"\"\n\n        class PaddleClasCloneModel(PaddleClasModel):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PaddleClasCloneModel(self._model.clone())\n        return clone_model\n\n    def predict(self, im, topk=1):\n        \"\"\"Classify an input image\n\n        :param im: (numpy.ndarray) The input image data, a 3-D array with layout HWC, BGR format\n        :param topk: (int) Filter the topk classify result, default 1\n        :return: ClassifyResult\n        \"\"\"\n\n        self.postprocessor.topk = topk\n        return self._model.predict(im)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of ClassifyResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get PaddleClasPreprocessor object of the loaded model\n\n        :return PaddleClasPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get PaddleClasPostprocessor object of the loaded model\n\n        :return PaddleClasPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self, config_file):\n        \"\"\"Create a preprocessor for PaddleClasModel from configuration file\n\n        :param config_file: (str)Path of configuration file, e.g resnet50/inference_cls.yaml\n        \"\"\"\n        super(PaddleClasPreprocessor, self).__init__()\n        self._manager = C.vision.classification.PaddleClasPreprocessor(\n            config_file)",
  "def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()",
  "def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()",
  "def initial_resize_on_cpu(self, v):\n        \"\"\"\n        When the initial operator is Resize, and input image size is large,\n        maybe it's better to run resize on CPU, because the HostToDevice memcpy\n        is time consuming. Set this True to run the initial resize on CPU.\n        :param: v: True or False\n        \"\"\"\n        self._manager.initial_resize_on_cpu(v)",
  "def __init__(self, topk=1):\n        \"\"\"Create a postprocessor for PaddleClasModel\n\n        :param topk: (int)Filter the top k classify label\n        \"\"\"\n        self._postprocessor = C.vision.classification.PaddleClasPostprocessor(\n            topk)",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for PaddleClasModel\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of ClassifyResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a image classification model exported by PaddleClas.\n\n        :param model_file: (str)Path of model file, e.g resnet50/inference.pdmodel\n        :param params_file: (str)Path of parameters file, e.g resnet50/inference.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str) Path of configuration file for deploy, e.g resnet50/inference_cls.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PaddleClasModel, self).__init__(runtime_option)\n        self._model = C.vision.classification.PaddleClasModel(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PaddleClas model initialize failed.\"",
  "def clone(self):\n        \"\"\"Clone PaddleClasModel object\n\n        :return: a new PaddleClasModel object\n        \"\"\"\n\n        class PaddleClasCloneModel(PaddleClasModel):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PaddleClasCloneModel(self._model.clone())\n        return clone_model",
  "def predict(self, im, topk=1):\n        \"\"\"Classify an input image\n\n        :param im: (numpy.ndarray) The input image data, a 3-D array with layout HWC, BGR format\n        :param topk: (int) Filter the topk classify result, default 1\n        :return: ClassifyResult\n        \"\"\"\n\n        self.postprocessor.topk = topk\n        return self._model.predict(im)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of ClassifyResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get PaddleClasPreprocessor object of the loaded model\n\n        :return PaddleClasPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get PaddleClasPostprocessor object of the loaded model\n\n        :return PaddleClasPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class PaddleClasCloneModel(PaddleClasModel):\n            def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "class PPShiTuV2Detector(PicoDet):\n    \"\"\"Detect main body from an input image.\n    \"\"\"\n    ...",
  "class PPShiTuV2RecognizerPreprocessor(ProcessorManager):\n    def __init__(self, config_file):\n        \"\"\"Create a preprocessor for PPShiTuV2Recognizer from configuration file\n\n        :param config_file: (str)Path of configuration file, e.g PPLCNet/inference_cls.yaml\n        \"\"\"\n        super(PPShiTuV2RecognizerPreprocessor, self).__init__()\n        self._manager = C.vision.classification.PPShiTuV2RecognizerPreprocessor(\n            config_file)\n\n    def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()\n\n    def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()\n\n    def initial_resize_on_cpu(self, v):\n        \"\"\"\n        When the initial operator is Resize, and input image size is large,\n        maybe it's better to run resize on CPU, because the HostToDevice memcpy\n        is time consuming. Set this True to run the initial resize on CPU.\n        :param: v: True or False\n        \"\"\"\n        self._manager.initial_resize_on_cpu(v)",
  "class PPShiTuV2RecognizerPostprocessor:\n    def __init__(self, topk=1):\n        \"\"\"Create a postprocessor for PPShiTuV2Recognizer\n\n        \"\"\"\n        self._postprocessor = C.vision.classification.PPShiTuV2RecognizerPostprocessor(\n        )\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for PPShiTuV2Recognizer\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of ClassifyResult, the feature vector is ClassifyResult.feature (If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "class PPShiTuV2Recognizer(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a image PPShiTuV2Recognizer model exported by PaddleClas.\n\n        :param model_file: (str)Path of model file, e.g PPLCNet/inference.pdmodel\n        :param params_file: (str)Path of parameters file, e.g PPLCNet/inference.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str) Path of configuration file for deploy, e.g PPLCNet/inference_cls.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPShiTuV2Recognizer, self).__init__(runtime_option)\n        self._model = C.vision.classification.PPShiTuV2Recognizer(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPShiTuV2Recognizer model initialize failed.\"\n\n    def clone(self):\n        \"\"\"Clone PPShiTuV2Recognizer object\n\n        :return: a new PPShiTuV2Recognizer object\n        \"\"\"\n\n        class PPShiTuV2RecognizerCloneModel(PPShiTuV2Recognizer):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PPShiTuV2RecognizerCloneModel(self._model.clone())\n        return clone_model\n\n    def predict(self, im):\n        \"\"\"Extract feature from an input image\n\n        :param im: (numpy.ndarray) The input image data, a 3-D array with layout HWC, BGR format\n        :return: ClassifyResult\n        \"\"\"\n\n        return self._model.predict(im)\n\n    def batch_predict(self, images):\n        \"\"\"Extract features from a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of ClassifyResult, the feature vector is ClassifyResult.feature\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get PPShiTuV2RecognizerPreprocessor object of the loaded model\n\n        :return PPShiTuV2RecognizerPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get PPShiTuV2RecognizerPostprocessor object of the loaded model\n\n        :return PPShiTuV2RecognizerPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self, config_file):\n        \"\"\"Create a preprocessor for PPShiTuV2Recognizer from configuration file\n\n        :param config_file: (str)Path of configuration file, e.g PPLCNet/inference_cls.yaml\n        \"\"\"\n        super(PPShiTuV2RecognizerPreprocessor, self).__init__()\n        self._manager = C.vision.classification.PPShiTuV2RecognizerPreprocessor(\n            config_file)",
  "def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()",
  "def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()",
  "def initial_resize_on_cpu(self, v):\n        \"\"\"\n        When the initial operator is Resize, and input image size is large,\n        maybe it's better to run resize on CPU, because the HostToDevice memcpy\n        is time consuming. Set this True to run the initial resize on CPU.\n        :param: v: True or False\n        \"\"\"\n        self._manager.initial_resize_on_cpu(v)",
  "def __init__(self, topk=1):\n        \"\"\"Create a postprocessor for PPShiTuV2Recognizer\n\n        \"\"\"\n        self._postprocessor = C.vision.classification.PPShiTuV2RecognizerPostprocessor(\n        )",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for PPShiTuV2Recognizer\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :return: list of ClassifyResult, the feature vector is ClassifyResult.feature (If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 config_file,\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a image PPShiTuV2Recognizer model exported by PaddleClas.\n\n        :param model_file: (str)Path of model file, e.g PPLCNet/inference.pdmodel\n        :param params_file: (str)Path of parameters file, e.g PPLCNet/inference.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param config_file: (str) Path of configuration file for deploy, e.g PPLCNet/inference_cls.yaml\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(PPShiTuV2Recognizer, self).__init__(runtime_option)\n        self._model = C.vision.classification.PPShiTuV2Recognizer(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n        assert self.initialized, \"PPShiTuV2Recognizer model initialize failed.\"",
  "def clone(self):\n        \"\"\"Clone PPShiTuV2Recognizer object\n\n        :return: a new PPShiTuV2Recognizer object\n        \"\"\"\n\n        class PPShiTuV2RecognizerCloneModel(PPShiTuV2Recognizer):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = PPShiTuV2RecognizerCloneModel(self._model.clone())\n        return clone_model",
  "def predict(self, im):\n        \"\"\"Extract feature from an input image\n\n        :param im: (numpy.ndarray) The input image data, a 3-D array with layout HWC, BGR format\n        :return: ClassifyResult\n        \"\"\"\n\n        return self._model.predict(im)",
  "def batch_predict(self, images):\n        \"\"\"Extract features from a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of ClassifyResult, the feature vector is ClassifyResult.feature\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get PPShiTuV2RecognizerPreprocessor object of the loaded model\n\n        :return PPShiTuV2RecognizerPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get PPShiTuV2RecognizerPostprocessor object of the loaded model\n\n        :return PPShiTuV2RecognizerPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class PPShiTuV2RecognizerCloneModel(PPShiTuV2Recognizer):\n            def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "class YOLOv5ClsPreprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for YOLOv5Cls\n        \"\"\"\n        self._preprocessor = C.vision.classification.YOLOv5ClsPreprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for YOLOv5Cls\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)\n\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [224, 224]\n        \"\"\"\n        return self._preprocessor.size\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh",
  "class YOLOv5ClsPostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for YOLOv5Cls\n        \"\"\"\n        self._postprocessor = C.vision.classification.YOLOv5ClsPostprocessor()\n\n    def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for YOLOv5Cls\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of ClassifyResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)\n\n    @property\n    def topk(self):\n        \"\"\"\n        topk for postprocessing, default is 1\n        \"\"\"\n        return self._postprocessor.topk\n\n    @topk.setter\n    def topk(self, topk):\n        assert isinstance(topk, int),\\\n            \"The value to set `top k` must be type of int.\"\n        self._postprocessor.topk = topk",
  "class YOLOv5Cls(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv5Cls model exported by YOLOv5Cls.\n\n        :param model_file: (str)Path of model file, e.g ./YOLOv5Cls.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(YOLOv5Cls, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"YOLOv5Cls only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.classification.YOLOv5Cls(\n            model_file, params_file, self._runtime_option, model_format)\n\n        assert self.initialized, \"YOLOv5Cls initialize failed.\"\n\n    def predict(self, input_image):\n        \"\"\"Classify an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: ClassifyResult\n        \"\"\"\n        assert input_image is not None, \"Input image is None.\"\n        return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of ClassifyResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get YOLOv5ClsPreprocessor object of the loaded model\n\n        :return YOLOv5ClsPreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get YOLOv5ClsPostprocessor object of the loaded model\n\n        :return YOLOv5ClsPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self):\n        \"\"\"Create a preprocessor for YOLOv5Cls\n        \"\"\"\n        self._preprocessor = C.vision.classification.YOLOv5ClsPreprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for YOLOv5Cls\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [224, 224]\n        \"\"\"\n        return self._preprocessor.size",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh",
  "def __init__(self):\n        \"\"\"Create a postprocessor for YOLOv5Cls\n        \"\"\"\n        self._postprocessor = C.vision.classification.YOLOv5ClsPostprocessor()",
  "def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for YOLOv5Cls\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of ClassifyResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)",
  "def topk(self):\n        \"\"\"\n        topk for postprocessing, default is 1\n        \"\"\"\n        return self._postprocessor.topk",
  "def topk(self, topk):\n        assert isinstance(topk, int),\\\n            \"The value to set `top k` must be type of int.\"\n        self._postprocessor.topk = topk",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv5Cls model exported by YOLOv5Cls.\n\n        :param model_file: (str)Path of model file, e.g ./YOLOv5Cls.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n\n        super(YOLOv5Cls, self).__init__(runtime_option)\n\n        assert model_format == ModelFormat.ONNX, \"YOLOv5Cls only support model format of ModelFormat.ONNX now.\"\n        self._model = C.vision.classification.YOLOv5Cls(\n            model_file, params_file, self._runtime_option, model_format)\n\n        assert self.initialized, \"YOLOv5Cls initialize failed.\"",
  "def predict(self, input_image):\n        \"\"\"Classify an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: ClassifyResult\n        \"\"\"\n        assert input_image is not None, \"Input image is None.\"\n        return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of ClassifyResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get YOLOv5ClsPreprocessor object of the loaded model\n\n        :return YOLOv5ClsPreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get YOLOv5ClsPostprocessor object of the loaded model\n\n        :return YOLOv5ClsPostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class ResNet(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a image classification model exported by torchvision.ResNet.\n\n        :param model_file: (str)Path of model file, e.g resnet/resnet50.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n\n        # call super() to initialize the backend_option\n        # the result of initialization will be saved in self._runtime_option\n        super(ResNet, self).__init__(runtime_option)\n\n        self._model = C.vision.classification.ResNet(\n            model_file, params_file, self._runtime_option, model_format)\n        # self.initialized shows the initialization of the model is successful or not\n\n        assert self.initialized, \"ResNet initialize failed.\"\n\n    # Predict and return the inference result of \"input_image\".\n    def predict(self, input_image, topk=1):\n        \"\"\"Classify an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param topk: (int)The topk result by the classify confidence score, default 1\n        :return: ClassifyResult\n        \"\"\"\n        return self._model.predict(input_image, topk)\n\n    # Implement the setter and getter method for variables\n    @property\n    def size(self):\n        \"\"\"\n        Returns the preprocess image size, default size = [224, 224];\n        \"\"\"\n        return self._model.size\n\n    @property\n    def mean_vals(self):\n        \"\"\"\n        Returns the mean value of normlization, default mean_vals = [0.485f, 0.456f, 0.406f];\n        \"\"\"\n        return self._model.mean_vals\n\n    @property\n    def std_vals(self):\n        \"\"\"\n        Returns the std value of normlization, default std_vals = [0.229f, 0.224f, 0.225f];\n        \"\"\"\n        return self._model.std_vals\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @mean_vals.setter\n    def mean_vals(self, value):\n        assert isinstance(\n            value, list), \"The value to set `mean_vals` must be type of list.\"\n        self._model.mean_vals = value\n\n    @std_vals.setter\n    def std_vals(self, value):\n        assert isinstance(\n            value, list), \"The value to set `std_vals` must be type of list.\"\n        self._model.std_vals = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a image classification model exported by torchvision.ResNet.\n\n        :param model_file: (str)Path of model file, e.g resnet/resnet50.onnx\n        :param params_file: (str)Path of parameters file, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model, default is ONNX\n        \"\"\"\n\n        # call super() to initialize the backend_option\n        # the result of initialization will be saved in self._runtime_option\n        super(ResNet, self).__init__(runtime_option)\n\n        self._model = C.vision.classification.ResNet(\n            model_file, params_file, self._runtime_option, model_format)\n        # self.initialized shows the initialization of the model is successful or not\n\n        assert self.initialized, \"ResNet initialize failed.\"",
  "def predict(self, input_image, topk=1):\n        \"\"\"Classify an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param topk: (int)The topk result by the classify confidence score, default 1\n        :return: ClassifyResult\n        \"\"\"\n        return self._model.predict(input_image, topk)",
  "def size(self):\n        \"\"\"\n        Returns the preprocess image size, default size = [224, 224];\n        \"\"\"\n        return self._model.size",
  "def mean_vals(self):\n        \"\"\"\n        Returns the mean value of normlization, default mean_vals = [0.485f, 0.456f, 0.406f];\n        \"\"\"\n        return self._model.mean_vals",
  "def std_vals(self):\n        \"\"\"\n        Returns the std value of normlization, default std_vals = [0.229f, 0.224f, 0.225f];\n        \"\"\"\n        return self._model.std_vals",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def mean_vals(self, value):\n        assert isinstance(\n            value, list), \"The value to set `mean_vals` must be type of list.\"\n        self._model.mean_vals = value",
  "def std_vals(self, value):\n        assert isinstance(\n            value, list), \"The value to set `std_vals` must be type of list.\"\n        self._model.std_vals = value",
  "def sort_boxes(boxes):\n    return C.vision.ocr.sort_boxes(boxes)",
  "class DBDetectorPreprocessor(ProcessorManager):\n    def __init__(self):\n        \"\"\"\n        Create a preprocessor for DBDetectorModel\n        \"\"\"\n        super(DBDetectorPreprocessor, self).__init__()\n        self._manager = C.vision.ocr.DBDetectorPreprocessor()\n\n    @property\n    def max_side_len(self):\n        \"\"\"Get max_side_len value.\n        \"\"\"\n        return self._manager.max_side_len\n\n    @max_side_len.setter\n    def max_side_len(self, value):\n        \"\"\"Set max_side_len value.\n        :param: value: (int) max_side_len value\n        \"\"\"\n        assert isinstance(\n            value, int), \"The value to set `max_side_len` must be type of int.\"\n        self._manager.max_side_len = value\n\n    def set_normalize(self, mean, std, is_scale):\n        \"\"\"Set preprocess normalize parameters, please call this API to\n           customize the normalize parameters, otherwise it will use the default\n           normalize parameters.\n        :param: mean: (list of float) mean values\n        :param: std: (list of float) std values\n        :param: is_scale: (boolean) whether to scale\n        \"\"\"\n        self._manager.set_normalize(mean, std, is_scale)\n\n    @property\n    def static_shape_infer(self):\n        return self._manager.static_shape_infer\n\n    @static_shape_infer.setter\n    def static_shape_infer(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `static_shape_infer` must be type of bool.\"\n        self._manager.static_shape_infer = value\n\n    def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()\n\n    def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()",
  "class DBDetectorPostprocessor:\n    def __init__(self):\n        \"\"\"\n        Create a postprocessor for DBDetectorModel\n        \"\"\"\n        self._postprocessor = C.vision.ocr.DBDetectorPostprocessor()\n\n    def run(self, runtime_results, batch_det_img_info):\n        \"\"\"Postprocess the runtime results for DBDetectorModel\n\n        :param: runtime_results: (list of FDTensor or list of pyArray)The output FDTensor results from runtime\n        :param: batch_det_img_info: (list of std::array<int, 4>)The output of det_preprocessor\n        :return: list of Result(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, batch_det_img_info)\n\n    @property\n    def det_db_thresh(self):\n        \"\"\"\n        Return the det_db_thresh of DBDetectorPostprocessor\n        \"\"\"\n        return self._postprocessor.det_db_thresh\n\n    @det_db_thresh.setter\n    def det_db_thresh(self, value):\n        \"\"\"Set the det_db_thresh for DBDetectorPostprocessor\n\n        :param: value : the det_db_thresh value\n        \"\"\"\n        assert isinstance(\n            value,\n            float), \"The value to set `det_db_thresh` must be type of float.\"\n        self._postprocessor.det_db_thresh = value\n\n    @property\n    def det_db_box_thresh(self):\n        \"\"\"\n        Return the det_db_box_thresh of DBDetectorPostprocessor\n        \"\"\"\n        return self._postprocessor.det_db_box_thresh\n\n    @det_db_box_thresh.setter\n    def det_db_box_thresh(self, value):\n        \"\"\"Set the det_db_box_thresh for DBDetectorPostprocessor\n\n        :param: value : the det_db_box_thresh value\n        \"\"\"\n        assert isinstance(\n            value, float\n        ), \"The value to set `det_db_box_thresh` must be type of float.\"\n        self._postprocessor.det_db_box_thresh = value\n\n    @property\n    def det_db_unclip_ratio(self):\n        \"\"\"\n        Return the det_db_unclip_ratio of DBDetectorPostprocessor\n        \"\"\"\n        return self._postprocessor.det_db_unclip_ratio\n\n    @det_db_unclip_ratio.setter\n    def det_db_unclip_ratio(self, value):\n        \"\"\"Set the det_db_unclip_ratio for DBDetectorPostprocessor\n\n        :param: value : the det_db_unclip_ratio value\n        \"\"\"\n        assert isinstance(\n            value, float\n        ), \"The value to set `det_db_unclip_ratio` must be type of float.\"\n        self._postprocessor.det_db_unclip_ratio = value\n\n    @property\n    def det_db_score_mode(self):\n        \"\"\"\n        Return the det_db_score_mode of DBDetectorPostprocessor\n        \"\"\"\n        return self._postprocessor.det_db_score_mode\n\n    @det_db_score_mode.setter\n    def det_db_score_mode(self, value):\n        \"\"\"Set the det_db_score_mode for DBDetectorPostprocessor\n\n        :param: value : the det_db_score_mode value\n        \"\"\"\n        assert isinstance(\n            value,\n            str), \"The value to set `det_db_score_mode` must be type of str.\"\n        self._postprocessor.det_db_score_mode = value\n\n    @property\n    def use_dilation(self):\n        \"\"\"\n        Return the use_dilation of DBDetectorPostprocessor\n        \"\"\"\n        return self._postprocessor.use_dilation\n\n    @use_dilation.setter\n    def use_dilation(self, value):\n        \"\"\"Set the use_dilation for DBDetectorPostprocessor\n\n        :param: value : the use_dilation value\n        \"\"\"\n        assert isinstance(\n            value,\n            bool), \"The value to set `use_dilation` must be type of bool.\"\n        self._postprocessor.use_dilation = value",
  "class DBDetector(FastDeployModel):\n    def __init__(self,\n                 model_file=\"\",\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load OCR detection model provided by PaddleOCR.\n\n        :param model_file: (str)Path of model file, e.g ./ch_PP-OCRv3_det_infer/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./ch_PP-OCRv3_det_infer/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(DBDetector, self).__init__(runtime_option)\n\n        if (len(model_file) == 0):\n            self._model = C.vision.ocr.DBDetector()\n            self._runnable = False\n        else:\n            self._model = C.vision.ocr.DBDetector(\n                model_file, params_file, self._runtime_option, model_format)\n            assert self.initialized, \"DBDetector initialize failed.\"\n            self._runnable = True\n\n    def clone(self):\n        \"\"\"Clone OCR detection model object\n\n        :return: a new OCR detection model object\n        \"\"\"\n\n        class DBDetectorClone(DBDetector):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = DBDetectorClone(self._model.clone())\n        return clone_model\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: boxes\n        \"\"\"\n        if self._runnable:\n            return self._model.predict(input_image)\n        return False\n\n    def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: batch_boxes\n        \"\"\"\n        if self._runnable:\n            return self._model.batch_predict(images)\n        return False\n\n    @property\n    def preprocessor(self):\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        return self._model.postprocessor\n\n    # Det Preprocessor Property\n    @property\n    def max_side_len(self):\n        return self._model.preprocessor.max_side_len\n\n    @max_side_len.setter\n    def max_side_len(self, value):\n        assert isinstance(\n            value, int), \"The value to set `max_side_len` must be type of int.\"\n        self._model.preprocessor.max_side_len = value\n\n    # Det Ppstprocessor Property\n    @property\n    def det_db_thresh(self):\n        return self._model.postprocessor.det_db_thresh\n\n    @det_db_thresh.setter\n    def det_db_thresh(self, value):\n        assert isinstance(\n            value,\n            float), \"The value to set `det_db_thresh` must be type of float.\"\n        self._model.postprocessor.det_db_thresh = value\n\n    @property\n    def det_db_box_thresh(self):\n        return self._model.postprocessor.det_db_box_thresh\n\n    @det_db_box_thresh.setter\n    def det_db_box_thresh(self, value):\n        assert isinstance(\n            value, float\n        ), \"The value to set `det_db_box_thresh` must be type of float.\"\n        self._model.postprocessor.det_db_box_thresh = value\n\n    @property\n    def det_db_unclip_ratio(self):\n        return self._model.postprocessor.det_db_unclip_ratio\n\n    @det_db_unclip_ratio.setter\n    def det_db_unclip_ratio(self, value):\n        assert isinstance(\n            value, float\n        ), \"The value to set `det_db_unclip_ratio` must be type of float.\"\n        self._model.postprocessor.det_db_unclip_ratio = value\n\n    @property\n    def det_db_score_mode(self):\n        return self._model.postprocessor.det_db_score_mode\n\n    @det_db_score_mode.setter\n    def det_db_score_mode(self, value):\n        assert isinstance(\n            value,\n            str), \"The value to set `det_db_score_mode` must be type of str.\"\n        self._model.postprocessor.det_db_score_mode = value\n\n    @property\n    def use_dilation(self):\n        return self._model.postprocessor.use_dilation\n\n    @use_dilation.setter\n    def use_dilation(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `use_dilation` must be type of bool.\"\n        self._model.postprocessor.use_dilation = value",
  "class ClassifierPreprocessor(ProcessorManager):\n    def __init__(self):\n        \"\"\"Create a preprocessor for ClassifierModel\n        \"\"\"\n        super(ClassifierPreprocessor, self).__init__()\n        self._manager = C.vision.ocr.ClassifierPreprocessor()\n\n    def set_normalize(self, mean, std, is_scale):\n        \"\"\"Set preprocess normalize parameters, please call this API to\n           customize the normalize parameters, otherwise it will use the default\n           normalize parameters.\n        :param: mean: (list of float) mean values\n        :param: std: (list of float) std values\n        :param: is_scale: (boolean) whether to scale\n        \"\"\"\n        self._manager.set_normalize(mean, std, is_scale)\n\n    @property\n    def cls_image_shape(self):\n        return self._manager.cls_image_shape\n\n    @cls_image_shape.setter\n    def cls_image_shape(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `cls_image_shape` must be type of list.\"\n        self._manager.cls_image_shape = value\n\n    def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()\n\n    def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()",
  "class ClassifierPostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for ClassifierModel\n        \"\"\"\n        self._postprocessor = C.vision.ocr.ClassifierPostprocessor()\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for ClassifierModel\n        :param: runtime_results: (list of FDTensor or list of pyArray)The output FDTensor results from runtime\n        :return: list of Result(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)\n\n    @property\n    def cls_thresh(self):\n        \"\"\"\n        Return the cls_thresh of ClassifierPostprocessor\n        \"\"\"\n        return self._postprocessor.cls_thresh\n\n    @cls_thresh.setter\n    def cls_thresh(self, value):\n        \"\"\"Set the cls_thresh for ClassifierPostprocessor\n\n        :param: value: the value of cls_thresh\n        \"\"\"\n        assert isinstance(\n            value,\n            float), \"The value to set `cls_thresh` must be type of float.\"\n        self._postprocessor.cls_thresh = value",
  "class Classifier(FastDeployModel):\n    def __init__(self,\n                 model_file=\"\",\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load OCR classification model provided by PaddleOCR.\n\n        :param model_file: (str)Path of model file, e.g ./ch_ppocr_mobile_v2.0_cls_infer/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./ch_ppocr_mobile_v2.0_cls_infer/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(Classifier, self).__init__(runtime_option)\n\n        if (len(model_file) == 0):\n            self._model = C.vision.ocr.Classifier()\n            self._runnable = False\n        else:\n            self._model = C.vision.ocr.Classifier(\n                model_file, params_file, self._runtime_option, model_format)\n            assert self.initialized, \"Classifier initialize failed.\"\n            self._runnable = True\n\n    def clone(self):\n        \"\"\"Clone OCR classification model object\n        :return: a new OCR classification model object\n        \"\"\"\n\n        class ClassifierClone(Classifier):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = ClassifierClone(self._model.clone())\n        return clone_model\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: cls_label, cls_score\n        \"\"\"\n        if self._runnable:\n            return self._model.predict(input_image)\n        return False\n\n    def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: list of cls_label, list of cls_score\n        \"\"\"\n        if self._runnable:\n            return self._model.batch_predict(images)\n        return False\n\n    @property\n    def preprocessor(self):\n        return self._model.preprocessor\n\n    @preprocessor.setter\n    def preprocessor(self, value):\n        self._model.preprocessor = value\n\n    @property\n    def postprocessor(self):\n        return self._model.postprocessor\n\n    @postprocessor.setter\n    def postprocessor(self, value):\n        self._model.postprocessor = value\n\n    @property\n    def cls_image_shape(self):\n        return self._model.preprocessor.cls_image_shape\n\n    @cls_image_shape.setter\n    def cls_image_shape(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `cls_image_shape` must be type of list.\"\n        self._model.preprocessor.cls_image_shape = value\n\n    # Cls Postprocessor Property\n    @property\n    def cls_thresh(self):\n        return self._model.postprocessor.cls_thresh\n\n    @cls_thresh.setter\n    def cls_thresh(self, value):\n        assert isinstance(\n            value,\n            float), \"The value to set `cls_thresh` must be type of float.\"\n        self._model.postprocessor.cls_thresh = value",
  "class RecognizerPreprocessor(ProcessorManager):\n    def __init__(self):\n        \"\"\"Create a preprocessor for RecognizerModel\n        \"\"\"\n        super(RecognizerPreprocessor, self).__init__()\n        self._manager = C.vision.ocr.RecognizerPreprocessor()\n\n    @property\n    def static_shape_infer(self):\n        return self._manager.static_shape_infer\n\n    @static_shape_infer.setter\n    def static_shape_infer(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `static_shape_infer` must be type of bool.\"\n        self._manager.static_shape_infer = value\n\n    def set_normalize(self, mean, std, is_scale):\n        \"\"\"Set preprocess normalize parameters, please call this API to\n           customize the normalize parameters, otherwise it will use the default\n           normalize parameters.\n        :param: mean: (list of float) mean values\n        :param: std: (list of float) std values\n        :param: is_scale: (boolean) whether to scale\n        \"\"\"\n        self._manager.set_normalize(mean, std, is_scale)\n\n    @property\n    def rec_image_shape(self):\n        return self._manager.rec_image_shape\n\n    @rec_image_shape.setter\n    def rec_image_shape(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `rec_image_shape` must be type of list.\"\n        self._manager.rec_image_shape = value\n\n    def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()\n\n    def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()",
  "class RecognizerPostprocessor:\n    def __init__(self, label_path):\n        \"\"\"Create a postprocessor for RecognizerModel\n        :param label_path: (str)Path of label file\n        \"\"\"\n        self._postprocessor = C.vision.ocr.RecognizerPostprocessor(label_path)\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for RecognizerModel\n        :param: runtime_results: (list of FDTensor or list of pyArray)The output FDTensor results from runtime\n        :return: list of Result(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "class Recognizer(FastDeployModel):\n    def __init__(self,\n                 model_file=\"\",\n                 params_file=\"\",\n                 label_path=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load OCR recognition model provided by PaddleOCR\n\n        :param model_file: (str)Path of model file, e.g ./ch_PP-OCRv3_rec_infer/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./ch_PP-OCRv3_rec_infer/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param label_path: (str)Path of label file used by OCR recognition model. e.g ./ppocr_keys_v1.txt\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(Recognizer, self).__init__(runtime_option)\n\n        if (len(model_file) == 0):\n            self._model = C.vision.ocr.Recognizer()\n            self._runnable = False\n        else:\n            self._model = C.vision.ocr.Recognizer(\n                model_file, params_file, label_path, self._runtime_option,\n                model_format)\n            assert self.initialized, \"Recognizer initialize failed.\"\n            self._runnable = True\n\n    def clone(self):\n        \"\"\"Clone OCR recognition model object\n        :return: a new OCR recognition model object\n        \"\"\"\n\n        class RecognizerClone(Recognizer):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = RecognizerClone(self._model.clone())\n        return clone_model\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: rec_text, rec_score\n        \"\"\"\n        if self._runnable:\n            return self._model.predict(input_image)\n        return False\n\n    def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: list of rec_text, list of rec_score\n        \"\"\"\n        if self._runnable:\n            return self._model.batch_predict(images)\n        return False\n\n    @property\n    def preprocessor(self):\n        return self._model.preprocessor\n\n    @preprocessor.setter\n    def preprocessor(self, value):\n        self._model.preprocessor = value\n\n    @property\n    def postprocessor(self):\n        return self._model.postprocessor\n\n    @postprocessor.setter\n    def postprocessor(self, value):\n        self._model.postprocessor = value\n\n    @property\n    def static_shape_infer(self):\n        return self._model.preprocessor.static_shape_infer\n\n    @static_shape_infer.setter\n    def static_shape_infer(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `static_shape_infer` must be type of bool.\"\n        self._model.preprocessor.static_shape_infer = value\n\n    @property\n    def rec_image_shape(self):\n        return self._model.preprocessor.rec_image_shape\n\n    @rec_image_shape.setter\n    def rec_image_shape(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `rec_image_shape` must be type of list.\"\n        self._model.preprocessor.rec_image_shape = value",
  "class StructureV2TablePreprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for StructureV2Table Model\n        \"\"\"\n        self._preprocessor = C.vision.ocr.StructureV2TablePreprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for StructureV2TableModel\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "class StructureV2TablePostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for StructureV2Table Model\n        \"\"\"\n        self._postprocessor = C.vision.ocr.StructureV2TablePostprocessor()\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for StructureV2Table Model\n        :param: runtime_results: (list of FDTensor or list of pyArray)The output FDTensor results from runtime\n        :return: list of Result(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "class StructureV2Table(FastDeployModel):\n    def __init__(self,\n                 model_file=\"\",\n                 params_file=\"\",\n                 table_char_dict_path=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load StructureV2Table model provided by PP-StructureV2.\n\n        :param model_file: (str)Path of model file, e.g ./ch_ppocr_mobile_v2.0_cls_infer/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./ch_ppocr_mobile_v2.0_cls_infer/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param table_char_dict_path: (str)Path of table_char_dict file, e.g ../ppocr/utils/dict/table_structure_dict_ch.txt\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(StructureV2Table, self).__init__(runtime_option)\n\n        if (len(model_file) == 0):\n            self._model = C.vision.ocr.StructureV2Table()\n            self._runnable = False\n        else:\n            self._model = C.vision.ocr.StructureV2Table(\n                model_file, params_file, table_char_dict_path,\n                self._runtime_option, model_format)\n            assert self.initialized, \"Classifier initialize failed.\"\n            self._runnable = True\n\n    def clone(self):\n        \"\"\"Clone StructureV2Table model object\n        :return: a new StructureV2Table model object\n        \"\"\"\n\n        class StructureV2TableClone(StructureV2Table):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = StructureV2TableClone(self._model.clone())\n        return clone_model\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: bbox, structure\n        \"\"\"\n        if self._runnable:\n            return self._model.predict(input_image)\n        return False\n\n    def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: list of bbox list, list of structure\n        \"\"\"\n        if self._runnable:\n            return self._model.batch_predict(images)\n        return False\n\n    @property\n    def preprocessor(self):\n        return self._model.preprocessor\n\n    @preprocessor.setter\n    def preprocessor(self, value):\n        self._model.preprocessor = value\n\n    @property\n    def postprocessor(self):\n        return self._model.postprocessor\n\n    @postprocessor.setter\n    def postprocessor(self, value):\n        self._model.postprocessor = value",
  "class StructureV2LayoutPreprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for StructureV2Layout Model\n        \"\"\"\n        self._preprocessor = C.vision.ocr.StructureV2LayoutPreprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for StructureV2Layout Model\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "class StructureV2LayoutPostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for StructureV2Layout Model\n        \"\"\"\n        self._postprocessor = C.vision.ocr.StructureV2LayoutPostprocessor()\n\n    def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for StructureV2Layout Model\n        :param: runtime_results: (list of FDTensor or list of pyArray)The output FDTensor results from runtime\n        :return: list of Result(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "class StructureV2Layout(FastDeployModel):\n    def __init__(self,\n                 model_file=\"\",\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load StructureV2Layout model provided by PP-StructureV2.\n\n        :param model_file: (str)Path of model file, e.g ./picodet_lcnet_x1_0_fgd_layout_infer/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./picodet_lcnet_x1_0_fgd_layout_infer/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(StructureV2Layout, self).__init__(runtime_option)\n\n        if (len(model_file) == 0):\n            self._model = C.vision.ocr.StructureV2Layout()\n            self._runnable = False\n        else:\n            self._model = C.vision.ocr.StructureV2Layout(\n                model_file, params_file, self._runtime_option, model_format)\n            assert self.initialized, \"StructureV2Layout model initialize failed.\"\n            self._runnable = True\n\n    def clone(self):\n        \"\"\"Clone StructureV2Layout model object\n        :return: a new StructureV2Table model object\n        \"\"\"\n\n        class StructureV2LayoutClone(StructureV2Layout):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = StructureV2LayoutClone(self._model.clone())\n        return clone_model\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: bboxes\n        \"\"\"\n        if self._runnable:\n            return self._model.predict(input_image)\n        return False\n\n    def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: list of bboxes list\n        \"\"\"\n        if self._runnable:\n            return self._model.batch_predict(images)\n        return False\n\n    @property\n    def preprocessor(self):\n        return self._model.preprocessor\n\n    @preprocessor.setter\n    def preprocessor(self, value):\n        self._model.preprocessor = value\n\n    @property\n    def postprocessor(self):\n        return self._model.postprocessor\n\n    @postprocessor.setter\n    def postprocessor(self, value):\n        self._model.postprocessor = value",
  "class PPOCRv4(FastDeployModel):\n    def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        \"\"\"Consruct a pipeline with text detector, direction classifier and text recognizer models\n\n        :param det_model: (FastDeployModel) The detection model object created by fastdeploy.vision.ocr.DBDetector.\n        :param cls_model: (FastDeployModel) The classification model object created by fastdeploy.vision.ocr.Classifier.\n        :param rec_model: (FastDeployModel) The recognition model object created by fastdeploy.vision.ocr.Recognizer.\n        \"\"\"\n        assert det_model is not None and rec_model is not None, \"The det_model and rec_model cannot be None.\"\n        if cls_model is None:\n            self.system_ = C.vision.ocr.PPOCRv4(det_model._model,\n                                                rec_model._model)\n        else:\n            self.system_ = C.vision.ocr.PPOCRv4(\n                det_model._model, cls_model._model, rec_model._model)\n\n    def clone(self):\n        \"\"\"Clone PPOCRv4 pipeline object\n        :return: a new PPOCRv4 pipeline object\n        \"\"\"\n\n        class PPOCRv4Clone(PPOCRv4):\n            def __init__(self, system):\n                self.system_ = system\n\n        clone_model = PPOCRv4Clone(self.system_.clone())\n        return clone_model\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: OCRResult\n        \"\"\"\n        return self.system_.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: OCRBatchResult\n        \"\"\"\n        return self.system_.batch_predict(images)\n\n    @property\n    def cls_batch_size(self):\n        return self.system_.cls_batch_size\n\n    @cls_batch_size.setter\n    def cls_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `cls_batch_size` must be type of int.\"\n        self.system_.cls_batch_size = value\n\n    @property\n    def rec_batch_size(self):\n        return self.system_.rec_batch_size\n\n    @rec_batch_size.setter\n    def rec_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `rec_batch_size` must be type of int.\"\n        self.system_.rec_batch_size = value",
  "class PPOCRSystemv4(PPOCRv4):\n    def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        logging.warning(\n            \"DEPRECATED: fd.vision.ocr.PPOCRSystemv4 is deprecated, \"\n            \"please use fd.vision.ocr.PPOCRv4 instead.\")\n        super(PPOCRSystemv4, self).__init__(det_model, cls_model, rec_model)\n\n    def predict(self, input_image):\n        return super(PPOCRSystemv4, self).predict(input_image)",
  "class PPOCRv3(FastDeployModel):\n    def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        \"\"\"Consruct a pipeline with text detector, direction classifier and text recognizer models\n\n        :param det_model: (FastDeployModel) The detection model object created by fastdeploy.vision.ocr.DBDetector.\n        :param cls_model: (FastDeployModel) The classification model object created by fastdeploy.vision.ocr.Classifier.\n        :param rec_model: (FastDeployModel) The recognition model object created by fastdeploy.vision.ocr.Recognizer.\n        \"\"\"\n        assert det_model is not None and rec_model is not None, \"The det_model and rec_model cannot be None.\"\n        if cls_model is None:\n            self.system_ = C.vision.ocr.PPOCRv3(det_model._model,\n                                                rec_model._model)\n        else:\n            self.system_ = C.vision.ocr.PPOCRv3(\n                det_model._model, cls_model._model, rec_model._model)\n\n    def clone(self):\n        \"\"\"Clone PPOCRv3 pipeline object\n        :return: a new PPOCRv3 pipeline object\n        \"\"\"\n\n        class PPOCRv3Clone(PPOCRv3):\n            def __init__(self, system):\n                self.system_ = system\n\n        clone_model = PPOCRv3Clone(self.system_.clone())\n        return clone_model\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: OCRResult\n        \"\"\"\n        return self.system_.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: OCRBatchResult\n        \"\"\"\n        return self.system_.batch_predict(images)\n\n    @property\n    def cls_batch_size(self):\n        return self.system_.cls_batch_size\n\n    @cls_batch_size.setter\n    def cls_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `cls_batch_size` must be type of int.\"\n        self.system_.cls_batch_size = value\n\n    @property\n    def rec_batch_size(self):\n        return self.system_.rec_batch_size\n\n    @rec_batch_size.setter\n    def rec_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `rec_batch_size` must be type of int.\"\n        self.system_.rec_batch_size = value",
  "class PPOCRSystemv3(PPOCRv3):\n    def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        logging.warning(\n            \"DEPRECATED: fd.vision.ocr.PPOCRSystemv3 is deprecated, \"\n            \"please use fd.vision.ocr.PPOCRv3 instead.\")\n        super(PPOCRSystemv3, self).__init__(det_model, cls_model, rec_model)\n\n    def predict(self, input_image):\n        return super(PPOCRSystemv3, self).predict(input_image)",
  "class PPOCRv2(FastDeployModel):\n    def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        \"\"\"Consruct a pipeline with text detector, direction classifier and text recognizer models\n\n        :param det_model: (FastDeployModel) The detection model object created by fastdeploy.vision.ocr.DBDetector.\n        :param cls_model: (FastDeployModel) The classification model object created by fastdeploy.vision.ocr.Classifier.\n        :param rec_model: (FastDeployModel) The recognition model object created by fastdeploy.vision.ocr.Recognizer.\n        \"\"\"\n        assert det_model is not None and rec_model is not None, \"The det_model and rec_model cannot be None.\"\n        if cls_model is None:\n            self.system_ = C.vision.ocr.PPOCRv2(det_model._model,\n                                                rec_model._model)\n        else:\n            self.system_ = C.vision.ocr.PPOCRv2(\n                det_model._model, cls_model._model, rec_model._model)\n\n    def clone(self):\n        \"\"\"Clone PPOCRv3 pipeline object\n        :return: a new PPOCRv3 pipeline object\n        \"\"\"\n\n        class PPOCRv2Clone(PPOCRv2):\n            def __init__(self, system):\n                self.system_ = system\n\n        clone_model = PPOCRv2Clone(self.system_.clone())\n        return clone_model\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: OCRResult\n        \"\"\"\n        return self.system_.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: OCRBatchResult\n        \"\"\"\n\n        return self.system_.batch_predict(images)\n\n    @property\n    def cls_batch_size(self):\n        return self.system_.cls_batch_size\n\n    @cls_batch_size.setter\n    def cls_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `cls_batch_size` must be type of int.\"\n        self.system_.cls_batch_size = value\n\n    @property\n    def rec_batch_size(self):\n        return self.system_.rec_batch_size\n\n    @rec_batch_size.setter\n    def rec_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `rec_batch_size` must be type of int.\"\n        self.system_.rec_batch_size = value",
  "class PPOCRSystemv2(PPOCRv2):\n    def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        logging.warning(\n            \"DEPRECATED: fd.vision.ocr.PPOCRSystemv2 is deprecated, \"\n            \"please use fd.vision.ocr.PPOCRv2 instead.\")\n        super(PPOCRSystemv2, self).__init__(det_model, cls_model, rec_model)\n\n    def predict(self, input_image):\n        return super(PPOCRSystemv2, self).predict(input_image)",
  "class PPStructureV2Table(FastDeployModel):\n    def __init__(self, det_model=None, rec_model=None, table_model=None):\n        \"\"\"Consruct a pipeline with text detector, text recognizer and table recognizer models\n\n        :param det_model: (FastDeployModel) The detection model object created by fastdeploy.vision.ocr.DBDetector.\n        :param rec_model: (FastDeployModel) The recognition model object created by fastdeploy.vision.ocr.Recognizer.\n        :param table_model: (FastDeployModel) The table recognition model object created by fastdeploy.vision.ocr.Table.\n        \"\"\"\n        assert det_model is not None and rec_model is not None and table_model is not None, \"The det_model, rec_model and table_model cannot be None.\"\n        self.system_ = C.vision.ocr.PPStructureV2Table(\n            det_model._model,\n            rec_model._model,\n            table_model._model, )\n\n    def clone(self):\n        \"\"\"Clone PPStructureV2Table pipeline object\n        :return: a new PPStructureV2Table pipeline object\n        \"\"\"\n\n        class PPStructureV2TableClone(PPStructureV2Table):\n            def __init__(self, system):\n                self.system_ = system\n\n        clone_model = PPStructureV2TableClone(self.system_.clone())\n        return clone_model\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: OCRResult\n        \"\"\"\n        return self.system_.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: OCRBatchResult\n        \"\"\"\n\n        return self.system_.batch_predict(images)",
  "class PPStructureV2TableSystem(PPStructureV2Table):\n    def __init__(self, det_model=None, rec_model=None, table_model=None):\n        logging.warning(\n            \"DEPRECATED: fd.vision.ocr.PPStructureV2TableSystem is deprecated, \"\n            \"please use fd.vision.ocr.PPStructureV2Table instead.\")\n        super(PPStructureV2TableSystem, self).__init__(det_model, rec_model,\n                                                       table_model)\n\n    def predict(self, input_image):\n        return super(PPStructureV2TableSystem, self).predict(input_image)",
  "class StructureV2SERViLayoutXLMModelPreprocessor():\n    def __init__(self, ser_dict_path, use_gpu=True):\n        \"\"\"Create a preprocessor for Ser-Vi-LayoutXLM model.\n        :param: ser_dict_path: (str) class file path\n        :param: use_gpu: (bool) whether use gpu to OCR process\n        \"\"\"\n        self._manager = None\n        from paddleocr import PaddleOCR\n        self.ocr_engine = PaddleOCR(\n            use_angle_cls=False,\n            det_model_dir=None,\n            rec_model_dir=None,\n            show_log=False,\n            use_gpu=use_gpu)\n\n        pre_process_list = [{\n            'VQATokenLabelEncode': {\n                'class_path': ser_dict_path,\n                'contains_re': False,\n                'ocr_engine': self.ocr_engine,\n                'order_method': \"tb-yx\"\n            }\n        }, {\n            'VQATokenPad': {\n                'max_seq_len': 512,\n                'return_attention_mask': True\n            }\n        }, {\n            'VQASerTokenChunk': {\n                'max_seq_len': 512,\n                'return_attention_mask': True\n            }\n        }, {\n            'Resize': {\n                'size': [224, 224]\n            }\n        }, {\n            'NormalizeImage': {\n                'std': [58.395, 57.12, 57.375],\n                'mean': [123.675, 116.28, 103.53],\n                'scale': '1',\n                'order': 'hwc'\n            }\n        }, {\n            'ToCHWImage': None\n        }, {\n            'KeepKeys': {\n                'keep_keys': [\n                    'input_ids', 'bbox', 'attention_mask', 'token_type_ids',\n                    'image', 'labels', 'segment_offset_id', 'ocr_info',\n                    'entities'\n                ]\n            }\n        }]\n\n        self.preprocess_op = create_operators(pre_process_list,\n                                              {'infer_mode': True})\n\n    def _transform(self, data, ops=None):\n        \"\"\" transform \"\"\"\n        if ops is None:\n            ops = []\n        for op in ops:\n            data = op(data)\n            if data is None:\n                return None\n        return data\n\n    def run(self, input_im):\n        \"\"\"Run preprocess of  Ser-Vi-LayoutXLM model\n        :param: input_ims: (numpy.ndarray) input image\n        \"\"\"\n        ori_im = input_im.copy()\n        data = {'image': input_im}\n        data = transform(data, self.preprocess_op)\n\n        for idx in range(len(data)):\n            if isinstance(data[idx], np.ndarray):\n                data[idx] = np.expand_dims(data[idx], axis=0)\n            else:\n                data[idx] = [data[idx]]\n\n        return data",
  "class StructureV2SERViLayoutXLMModelPostprocessor():\n    def __init__(self, class_path):\n        \"\"\"Create a postprocessor for Ser-Vi-LayoutXLM model.\n        :param: class_path: (string) class file path\n        \"\"\"\n        self.postprocessor_op = VQASerTokenLayoutLMPostProcess(class_path)\n\n    def run(self, preds, batch=None, *args, **kwargs):\n        \"\"\"Run postprocess of  Ser-Vi-LayoutXLM model.\n        :param: preds: (list) results of infering\n        \"\"\"\n        return self.postprocessor_op(preds, batch, *args, **kwargs)",
  "class StructureV2SERViLayoutXLMModel(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file,\n                 ser_dict_path,\n                 class_path,\n                 config_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load SERViLayoutXLM model provided by PP-StructureV2.\n\n        :param model_file: (str)Path of model file, e.g ./ser_vi_layout_xlm/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./ser_vi_layout_xlm/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param ser_dict_path: (str) class file path\n        :param class_path: (str) class file path\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(StructureV2SERViLayoutXLMModel, self).__init__(runtime_option)\n\n        assert self._runtime_option.backend != 0, \\\n            \"Runtime Option required backend setting.\"\n        self._model = C.vision.ocr.StructureV2SERViLayoutXLMModel(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n\n        assert self.initialized, \"SERViLayoutXLM model initialize failed.\"\n\n        self.preprocessor = StructureV2SERViLayoutXLMModelPreprocessor(\n            ser_dict_path)\n        self.postprocesser = StructureV2SERViLayoutXLMModelPostprocessor(\n            class_path)\n\n        self.input_name_0 = self._model.get_input_info(0).name\n        self.input_name_1 = self._model.get_input_info(1).name\n        self.input_name_2 = self._model.get_input_info(2).name\n        self.input_name_3 = self._model.get_input_info(3).name\n\n    def predict(self, image):\n        assert isinstance(image,\n                          np.ndarray), \"predict recives numpy.ndarray(BGR)\"\n\n        data = self.preprocessor.run(image)\n        infer_input = {\n            self.input_name_0: data[0],\n            self.input_name_1: data[1],\n            self.input_name_2: data[2],\n            self.input_name_3: data[3],\n        }\n\n        infer_result = self._model.infer(infer_input)\n        infer_result = infer_result[0]\n\n        post_result = self.postprocesser.run(infer_result,\n                                             segment_offset_ids=data[6],\n                                             ocr_infos=data[7])\n\n        return post_result\n\n    def batch_predict(self, image_list):\n        assert isinstance(image_list, list) and \\\n             isinstance(image_list[0], np.ndarray), \\\n              \"batch_predict recives list of numpy.ndarray(BGR)\"\n\n        # reading and preprocessing images\n        datas = None\n        for image in image_list:\n            data = self.preprocessor.run(image)\n\n            # concatenate data to batch\n            if datas == None:\n                datas = data\n            else:\n                for idx in range(len(data)):\n                    if isinstance(data[idx], np.ndarray):\n                        datas[idx] = np.concatenate(\n                            (datas[idx], data[idx]), axis=0)\n                    else:\n                        datas[idx].extend(data[idx])\n\n        # infer\n        infer_inputs = {\n            self.input_name_0: datas[0],\n            self.input_name_1: datas[1],\n            self.input_name_2: datas[2],\n            self.input_name_3: datas[3],\n        }\n\n        infer_results = self._model.infer(infer_inputs)\n        infer_results = infer_results[0]\n\n        # postprocessing\n        post_results = self.postprocesser.run(infer_results,\n                                              segment_offset_ids=datas[6],\n                                              ocr_infos=datas[7])\n\n        return post_results",
  "def __init__(self):\n        \"\"\"\n        Create a preprocessor for DBDetectorModel\n        \"\"\"\n        super(DBDetectorPreprocessor, self).__init__()\n        self._manager = C.vision.ocr.DBDetectorPreprocessor()",
  "def max_side_len(self):\n        \"\"\"Get max_side_len value.\n        \"\"\"\n        return self._manager.max_side_len",
  "def max_side_len(self, value):\n        \"\"\"Set max_side_len value.\n        :param: value: (int) max_side_len value\n        \"\"\"\n        assert isinstance(\n            value, int), \"The value to set `max_side_len` must be type of int.\"\n        self._manager.max_side_len = value",
  "def set_normalize(self, mean, std, is_scale):\n        \"\"\"Set preprocess normalize parameters, please call this API to\n           customize the normalize parameters, otherwise it will use the default\n           normalize parameters.\n        :param: mean: (list of float) mean values\n        :param: std: (list of float) std values\n        :param: is_scale: (boolean) whether to scale\n        \"\"\"\n        self._manager.set_normalize(mean, std, is_scale)",
  "def static_shape_infer(self):\n        return self._manager.static_shape_infer",
  "def static_shape_infer(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `static_shape_infer` must be type of bool.\"\n        self._manager.static_shape_infer = value",
  "def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()",
  "def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()",
  "def __init__(self):\n        \"\"\"\n        Create a postprocessor for DBDetectorModel\n        \"\"\"\n        self._postprocessor = C.vision.ocr.DBDetectorPostprocessor()",
  "def run(self, runtime_results, batch_det_img_info):\n        \"\"\"Postprocess the runtime results for DBDetectorModel\n\n        :param: runtime_results: (list of FDTensor or list of pyArray)The output FDTensor results from runtime\n        :param: batch_det_img_info: (list of std::array<int, 4>)The output of det_preprocessor\n        :return: list of Result(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, batch_det_img_info)",
  "def det_db_thresh(self):\n        \"\"\"\n        Return the det_db_thresh of DBDetectorPostprocessor\n        \"\"\"\n        return self._postprocessor.det_db_thresh",
  "def det_db_thresh(self, value):\n        \"\"\"Set the det_db_thresh for DBDetectorPostprocessor\n\n        :param: value : the det_db_thresh value\n        \"\"\"\n        assert isinstance(\n            value,\n            float), \"The value to set `det_db_thresh` must be type of float.\"\n        self._postprocessor.det_db_thresh = value",
  "def det_db_box_thresh(self):\n        \"\"\"\n        Return the det_db_box_thresh of DBDetectorPostprocessor\n        \"\"\"\n        return self._postprocessor.det_db_box_thresh",
  "def det_db_box_thresh(self, value):\n        \"\"\"Set the det_db_box_thresh for DBDetectorPostprocessor\n\n        :param: value : the det_db_box_thresh value\n        \"\"\"\n        assert isinstance(\n            value, float\n        ), \"The value to set `det_db_box_thresh` must be type of float.\"\n        self._postprocessor.det_db_box_thresh = value",
  "def det_db_unclip_ratio(self):\n        \"\"\"\n        Return the det_db_unclip_ratio of DBDetectorPostprocessor\n        \"\"\"\n        return self._postprocessor.det_db_unclip_ratio",
  "def det_db_unclip_ratio(self, value):\n        \"\"\"Set the det_db_unclip_ratio for DBDetectorPostprocessor\n\n        :param: value : the det_db_unclip_ratio value\n        \"\"\"\n        assert isinstance(\n            value, float\n        ), \"The value to set `det_db_unclip_ratio` must be type of float.\"\n        self._postprocessor.det_db_unclip_ratio = value",
  "def det_db_score_mode(self):\n        \"\"\"\n        Return the det_db_score_mode of DBDetectorPostprocessor\n        \"\"\"\n        return self._postprocessor.det_db_score_mode",
  "def det_db_score_mode(self, value):\n        \"\"\"Set the det_db_score_mode for DBDetectorPostprocessor\n\n        :param: value : the det_db_score_mode value\n        \"\"\"\n        assert isinstance(\n            value,\n            str), \"The value to set `det_db_score_mode` must be type of str.\"\n        self._postprocessor.det_db_score_mode = value",
  "def use_dilation(self):\n        \"\"\"\n        Return the use_dilation of DBDetectorPostprocessor\n        \"\"\"\n        return self._postprocessor.use_dilation",
  "def use_dilation(self, value):\n        \"\"\"Set the use_dilation for DBDetectorPostprocessor\n\n        :param: value : the use_dilation value\n        \"\"\"\n        assert isinstance(\n            value,\n            bool), \"The value to set `use_dilation` must be type of bool.\"\n        self._postprocessor.use_dilation = value",
  "def __init__(self,\n                 model_file=\"\",\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load OCR detection model provided by PaddleOCR.\n\n        :param model_file: (str)Path of model file, e.g ./ch_PP-OCRv3_det_infer/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./ch_PP-OCRv3_det_infer/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(DBDetector, self).__init__(runtime_option)\n\n        if (len(model_file) == 0):\n            self._model = C.vision.ocr.DBDetector()\n            self._runnable = False\n        else:\n            self._model = C.vision.ocr.DBDetector(\n                model_file, params_file, self._runtime_option, model_format)\n            assert self.initialized, \"DBDetector initialize failed.\"\n            self._runnable = True",
  "def clone(self):\n        \"\"\"Clone OCR detection model object\n\n        :return: a new OCR detection model object\n        \"\"\"\n\n        class DBDetectorClone(DBDetector):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = DBDetectorClone(self._model.clone())\n        return clone_model",
  "def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: boxes\n        \"\"\"\n        if self._runnable:\n            return self._model.predict(input_image)\n        return False",
  "def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: batch_boxes\n        \"\"\"\n        if self._runnable:\n            return self._model.batch_predict(images)\n        return False",
  "def preprocessor(self):\n        return self._model.preprocessor",
  "def postprocessor(self):\n        return self._model.postprocessor",
  "def max_side_len(self):\n        return self._model.preprocessor.max_side_len",
  "def max_side_len(self, value):\n        assert isinstance(\n            value, int), \"The value to set `max_side_len` must be type of int.\"\n        self._model.preprocessor.max_side_len = value",
  "def det_db_thresh(self):\n        return self._model.postprocessor.det_db_thresh",
  "def det_db_thresh(self, value):\n        assert isinstance(\n            value,\n            float), \"The value to set `det_db_thresh` must be type of float.\"\n        self._model.postprocessor.det_db_thresh = value",
  "def det_db_box_thresh(self):\n        return self._model.postprocessor.det_db_box_thresh",
  "def det_db_box_thresh(self, value):\n        assert isinstance(\n            value, float\n        ), \"The value to set `det_db_box_thresh` must be type of float.\"\n        self._model.postprocessor.det_db_box_thresh = value",
  "def det_db_unclip_ratio(self):\n        return self._model.postprocessor.det_db_unclip_ratio",
  "def det_db_unclip_ratio(self, value):\n        assert isinstance(\n            value, float\n        ), \"The value to set `det_db_unclip_ratio` must be type of float.\"\n        self._model.postprocessor.det_db_unclip_ratio = value",
  "def det_db_score_mode(self):\n        return self._model.postprocessor.det_db_score_mode",
  "def det_db_score_mode(self, value):\n        assert isinstance(\n            value,\n            str), \"The value to set `det_db_score_mode` must be type of str.\"\n        self._model.postprocessor.det_db_score_mode = value",
  "def use_dilation(self):\n        return self._model.postprocessor.use_dilation",
  "def use_dilation(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `use_dilation` must be type of bool.\"\n        self._model.postprocessor.use_dilation = value",
  "def __init__(self):\n        \"\"\"Create a preprocessor for ClassifierModel\n        \"\"\"\n        super(ClassifierPreprocessor, self).__init__()\n        self._manager = C.vision.ocr.ClassifierPreprocessor()",
  "def set_normalize(self, mean, std, is_scale):\n        \"\"\"Set preprocess normalize parameters, please call this API to\n           customize the normalize parameters, otherwise it will use the default\n           normalize parameters.\n        :param: mean: (list of float) mean values\n        :param: std: (list of float) std values\n        :param: is_scale: (boolean) whether to scale\n        \"\"\"\n        self._manager.set_normalize(mean, std, is_scale)",
  "def cls_image_shape(self):\n        return self._manager.cls_image_shape",
  "def cls_image_shape(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `cls_image_shape` must be type of list.\"\n        self._manager.cls_image_shape = value",
  "def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()",
  "def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()",
  "def __init__(self):\n        \"\"\"Create a postprocessor for ClassifierModel\n        \"\"\"\n        self._postprocessor = C.vision.ocr.ClassifierPostprocessor()",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for ClassifierModel\n        :param: runtime_results: (list of FDTensor or list of pyArray)The output FDTensor results from runtime\n        :return: list of Result(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def cls_thresh(self):\n        \"\"\"\n        Return the cls_thresh of ClassifierPostprocessor\n        \"\"\"\n        return self._postprocessor.cls_thresh",
  "def cls_thresh(self, value):\n        \"\"\"Set the cls_thresh for ClassifierPostprocessor\n\n        :param: value: the value of cls_thresh\n        \"\"\"\n        assert isinstance(\n            value,\n            float), \"The value to set `cls_thresh` must be type of float.\"\n        self._postprocessor.cls_thresh = value",
  "def __init__(self,\n                 model_file=\"\",\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load OCR classification model provided by PaddleOCR.\n\n        :param model_file: (str)Path of model file, e.g ./ch_ppocr_mobile_v2.0_cls_infer/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./ch_ppocr_mobile_v2.0_cls_infer/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(Classifier, self).__init__(runtime_option)\n\n        if (len(model_file) == 0):\n            self._model = C.vision.ocr.Classifier()\n            self._runnable = False\n        else:\n            self._model = C.vision.ocr.Classifier(\n                model_file, params_file, self._runtime_option, model_format)\n            assert self.initialized, \"Classifier initialize failed.\"\n            self._runnable = True",
  "def clone(self):\n        \"\"\"Clone OCR classification model object\n        :return: a new OCR classification model object\n        \"\"\"\n\n        class ClassifierClone(Classifier):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = ClassifierClone(self._model.clone())\n        return clone_model",
  "def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: cls_label, cls_score\n        \"\"\"\n        if self._runnable:\n            return self._model.predict(input_image)\n        return False",
  "def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: list of cls_label, list of cls_score\n        \"\"\"\n        if self._runnable:\n            return self._model.batch_predict(images)\n        return False",
  "def preprocessor(self):\n        return self._model.preprocessor",
  "def preprocessor(self, value):\n        self._model.preprocessor = value",
  "def postprocessor(self):\n        return self._model.postprocessor",
  "def postprocessor(self, value):\n        self._model.postprocessor = value",
  "def cls_image_shape(self):\n        return self._model.preprocessor.cls_image_shape",
  "def cls_image_shape(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `cls_image_shape` must be type of list.\"\n        self._model.preprocessor.cls_image_shape = value",
  "def cls_thresh(self):\n        return self._model.postprocessor.cls_thresh",
  "def cls_thresh(self, value):\n        assert isinstance(\n            value,\n            float), \"The value to set `cls_thresh` must be type of float.\"\n        self._model.postprocessor.cls_thresh = value",
  "def __init__(self):\n        \"\"\"Create a preprocessor for RecognizerModel\n        \"\"\"\n        super(RecognizerPreprocessor, self).__init__()\n        self._manager = C.vision.ocr.RecognizerPreprocessor()",
  "def static_shape_infer(self):\n        return self._manager.static_shape_infer",
  "def static_shape_infer(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `static_shape_infer` must be type of bool.\"\n        self._manager.static_shape_infer = value",
  "def set_normalize(self, mean, std, is_scale):\n        \"\"\"Set preprocess normalize parameters, please call this API to\n           customize the normalize parameters, otherwise it will use the default\n           normalize parameters.\n        :param: mean: (list of float) mean values\n        :param: std: (list of float) std values\n        :param: is_scale: (boolean) whether to scale\n        \"\"\"\n        self._manager.set_normalize(mean, std, is_scale)",
  "def rec_image_shape(self):\n        return self._manager.rec_image_shape",
  "def rec_image_shape(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `rec_image_shape` must be type of list.\"\n        self._manager.rec_image_shape = value",
  "def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._manager.disable_normalize()",
  "def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._manager.disable_permute()",
  "def __init__(self, label_path):\n        \"\"\"Create a postprocessor for RecognizerModel\n        :param label_path: (str)Path of label file\n        \"\"\"\n        self._postprocessor = C.vision.ocr.RecognizerPostprocessor(label_path)",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for RecognizerModel\n        :param: runtime_results: (list of FDTensor or list of pyArray)The output FDTensor results from runtime\n        :return: list of Result(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def __init__(self,\n                 model_file=\"\",\n                 params_file=\"\",\n                 label_path=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load OCR recognition model provided by PaddleOCR\n\n        :param model_file: (str)Path of model file, e.g ./ch_PP-OCRv3_rec_infer/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./ch_PP-OCRv3_rec_infer/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param label_path: (str)Path of label file used by OCR recognition model. e.g ./ppocr_keys_v1.txt\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(Recognizer, self).__init__(runtime_option)\n\n        if (len(model_file) == 0):\n            self._model = C.vision.ocr.Recognizer()\n            self._runnable = False\n        else:\n            self._model = C.vision.ocr.Recognizer(\n                model_file, params_file, label_path, self._runtime_option,\n                model_format)\n            assert self.initialized, \"Recognizer initialize failed.\"\n            self._runnable = True",
  "def clone(self):\n        \"\"\"Clone OCR recognition model object\n        :return: a new OCR recognition model object\n        \"\"\"\n\n        class RecognizerClone(Recognizer):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = RecognizerClone(self._model.clone())\n        return clone_model",
  "def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: rec_text, rec_score\n        \"\"\"\n        if self._runnable:\n            return self._model.predict(input_image)\n        return False",
  "def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: list of rec_text, list of rec_score\n        \"\"\"\n        if self._runnable:\n            return self._model.batch_predict(images)\n        return False",
  "def preprocessor(self):\n        return self._model.preprocessor",
  "def preprocessor(self, value):\n        self._model.preprocessor = value",
  "def postprocessor(self):\n        return self._model.postprocessor",
  "def postprocessor(self, value):\n        self._model.postprocessor = value",
  "def static_shape_infer(self):\n        return self._model.preprocessor.static_shape_infer",
  "def static_shape_infer(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `static_shape_infer` must be type of bool.\"\n        self._model.preprocessor.static_shape_infer = value",
  "def rec_image_shape(self):\n        return self._model.preprocessor.rec_image_shape",
  "def rec_image_shape(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `rec_image_shape` must be type of list.\"\n        self._model.preprocessor.rec_image_shape = value",
  "def __init__(self):\n        \"\"\"Create a preprocessor for StructureV2Table Model\n        \"\"\"\n        self._preprocessor = C.vision.ocr.StructureV2TablePreprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for StructureV2TableModel\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def __init__(self):\n        \"\"\"Create a postprocessor for StructureV2Table Model\n        \"\"\"\n        self._postprocessor = C.vision.ocr.StructureV2TablePostprocessor()",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for StructureV2Table Model\n        :param: runtime_results: (list of FDTensor or list of pyArray)The output FDTensor results from runtime\n        :return: list of Result(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def __init__(self,\n                 model_file=\"\",\n                 params_file=\"\",\n                 table_char_dict_path=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load StructureV2Table model provided by PP-StructureV2.\n\n        :param model_file: (str)Path of model file, e.g ./ch_ppocr_mobile_v2.0_cls_infer/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./ch_ppocr_mobile_v2.0_cls_infer/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param table_char_dict_path: (str)Path of table_char_dict file, e.g ../ppocr/utils/dict/table_structure_dict_ch.txt\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(StructureV2Table, self).__init__(runtime_option)\n\n        if (len(model_file) == 0):\n            self._model = C.vision.ocr.StructureV2Table()\n            self._runnable = False\n        else:\n            self._model = C.vision.ocr.StructureV2Table(\n                model_file, params_file, table_char_dict_path,\n                self._runtime_option, model_format)\n            assert self.initialized, \"Classifier initialize failed.\"\n            self._runnable = True",
  "def clone(self):\n        \"\"\"Clone StructureV2Table model object\n        :return: a new StructureV2Table model object\n        \"\"\"\n\n        class StructureV2TableClone(StructureV2Table):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = StructureV2TableClone(self._model.clone())\n        return clone_model",
  "def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: bbox, structure\n        \"\"\"\n        if self._runnable:\n            return self._model.predict(input_image)\n        return False",
  "def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: list of bbox list, list of structure\n        \"\"\"\n        if self._runnable:\n            return self._model.batch_predict(images)\n        return False",
  "def preprocessor(self):\n        return self._model.preprocessor",
  "def preprocessor(self, value):\n        self._model.preprocessor = value",
  "def postprocessor(self):\n        return self._model.postprocessor",
  "def postprocessor(self, value):\n        self._model.postprocessor = value",
  "def __init__(self):\n        \"\"\"Create a preprocessor for StructureV2Layout Model\n        \"\"\"\n        self._preprocessor = C.vision.ocr.StructureV2LayoutPreprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for StructureV2Layout Model\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def __init__(self):\n        \"\"\"Create a postprocessor for StructureV2Layout Model\n        \"\"\"\n        self._postprocessor = C.vision.ocr.StructureV2LayoutPostprocessor()",
  "def run(self, runtime_results):\n        \"\"\"Postprocess the runtime results for StructureV2Layout Model\n        :param: runtime_results: (list of FDTensor or list of pyArray)The output FDTensor results from runtime\n        :return: list of Result(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results)",
  "def __init__(self,\n                 model_file=\"\",\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load StructureV2Layout model provided by PP-StructureV2.\n\n        :param model_file: (str)Path of model file, e.g ./picodet_lcnet_x1_0_fgd_layout_infer/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./picodet_lcnet_x1_0_fgd_layout_infer/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(StructureV2Layout, self).__init__(runtime_option)\n\n        if (len(model_file) == 0):\n            self._model = C.vision.ocr.StructureV2Layout()\n            self._runnable = False\n        else:\n            self._model = C.vision.ocr.StructureV2Layout(\n                model_file, params_file, self._runtime_option, model_format)\n            assert self.initialized, \"StructureV2Layout model initialize failed.\"\n            self._runnable = True",
  "def clone(self):\n        \"\"\"Clone StructureV2Layout model object\n        :return: a new StructureV2Table model object\n        \"\"\"\n\n        class StructureV2LayoutClone(StructureV2Layout):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = StructureV2LayoutClone(self._model.clone())\n        return clone_model",
  "def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: bboxes\n        \"\"\"\n        if self._runnable:\n            return self._model.predict(input_image)\n        return False",
  "def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: list of bboxes list\n        \"\"\"\n        if self._runnable:\n            return self._model.batch_predict(images)\n        return False",
  "def preprocessor(self):\n        return self._model.preprocessor",
  "def preprocessor(self, value):\n        self._model.preprocessor = value",
  "def postprocessor(self):\n        return self._model.postprocessor",
  "def postprocessor(self, value):\n        self._model.postprocessor = value",
  "def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        \"\"\"Consruct a pipeline with text detector, direction classifier and text recognizer models\n\n        :param det_model: (FastDeployModel) The detection model object created by fastdeploy.vision.ocr.DBDetector.\n        :param cls_model: (FastDeployModel) The classification model object created by fastdeploy.vision.ocr.Classifier.\n        :param rec_model: (FastDeployModel) The recognition model object created by fastdeploy.vision.ocr.Recognizer.\n        \"\"\"\n        assert det_model is not None and rec_model is not None, \"The det_model and rec_model cannot be None.\"\n        if cls_model is None:\n            self.system_ = C.vision.ocr.PPOCRv4(det_model._model,\n                                                rec_model._model)\n        else:\n            self.system_ = C.vision.ocr.PPOCRv4(\n                det_model._model, cls_model._model, rec_model._model)",
  "def clone(self):\n        \"\"\"Clone PPOCRv4 pipeline object\n        :return: a new PPOCRv4 pipeline object\n        \"\"\"\n\n        class PPOCRv4Clone(PPOCRv4):\n            def __init__(self, system):\n                self.system_ = system\n\n        clone_model = PPOCRv4Clone(self.system_.clone())\n        return clone_model",
  "def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: OCRResult\n        \"\"\"\n        return self.system_.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: OCRBatchResult\n        \"\"\"\n        return self.system_.batch_predict(images)",
  "def cls_batch_size(self):\n        return self.system_.cls_batch_size",
  "def cls_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `cls_batch_size` must be type of int.\"\n        self.system_.cls_batch_size = value",
  "def rec_batch_size(self):\n        return self.system_.rec_batch_size",
  "def rec_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `rec_batch_size` must be type of int.\"\n        self.system_.rec_batch_size = value",
  "def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        logging.warning(\n            \"DEPRECATED: fd.vision.ocr.PPOCRSystemv4 is deprecated, \"\n            \"please use fd.vision.ocr.PPOCRv4 instead.\")\n        super(PPOCRSystemv4, self).__init__(det_model, cls_model, rec_model)",
  "def predict(self, input_image):\n        return super(PPOCRSystemv4, self).predict(input_image)",
  "def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        \"\"\"Consruct a pipeline with text detector, direction classifier and text recognizer models\n\n        :param det_model: (FastDeployModel) The detection model object created by fastdeploy.vision.ocr.DBDetector.\n        :param cls_model: (FastDeployModel) The classification model object created by fastdeploy.vision.ocr.Classifier.\n        :param rec_model: (FastDeployModel) The recognition model object created by fastdeploy.vision.ocr.Recognizer.\n        \"\"\"\n        assert det_model is not None and rec_model is not None, \"The det_model and rec_model cannot be None.\"\n        if cls_model is None:\n            self.system_ = C.vision.ocr.PPOCRv3(det_model._model,\n                                                rec_model._model)\n        else:\n            self.system_ = C.vision.ocr.PPOCRv3(\n                det_model._model, cls_model._model, rec_model._model)",
  "def clone(self):\n        \"\"\"Clone PPOCRv3 pipeline object\n        :return: a new PPOCRv3 pipeline object\n        \"\"\"\n\n        class PPOCRv3Clone(PPOCRv3):\n            def __init__(self, system):\n                self.system_ = system\n\n        clone_model = PPOCRv3Clone(self.system_.clone())\n        return clone_model",
  "def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: OCRResult\n        \"\"\"\n        return self.system_.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: OCRBatchResult\n        \"\"\"\n        return self.system_.batch_predict(images)",
  "def cls_batch_size(self):\n        return self.system_.cls_batch_size",
  "def cls_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `cls_batch_size` must be type of int.\"\n        self.system_.cls_batch_size = value",
  "def rec_batch_size(self):\n        return self.system_.rec_batch_size",
  "def rec_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `rec_batch_size` must be type of int.\"\n        self.system_.rec_batch_size = value",
  "def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        logging.warning(\n            \"DEPRECATED: fd.vision.ocr.PPOCRSystemv3 is deprecated, \"\n            \"please use fd.vision.ocr.PPOCRv3 instead.\")\n        super(PPOCRSystemv3, self).__init__(det_model, cls_model, rec_model)",
  "def predict(self, input_image):\n        return super(PPOCRSystemv3, self).predict(input_image)",
  "def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        \"\"\"Consruct a pipeline with text detector, direction classifier and text recognizer models\n\n        :param det_model: (FastDeployModel) The detection model object created by fastdeploy.vision.ocr.DBDetector.\n        :param cls_model: (FastDeployModel) The classification model object created by fastdeploy.vision.ocr.Classifier.\n        :param rec_model: (FastDeployModel) The recognition model object created by fastdeploy.vision.ocr.Recognizer.\n        \"\"\"\n        assert det_model is not None and rec_model is not None, \"The det_model and rec_model cannot be None.\"\n        if cls_model is None:\n            self.system_ = C.vision.ocr.PPOCRv2(det_model._model,\n                                                rec_model._model)\n        else:\n            self.system_ = C.vision.ocr.PPOCRv2(\n                det_model._model, cls_model._model, rec_model._model)",
  "def clone(self):\n        \"\"\"Clone PPOCRv3 pipeline object\n        :return: a new PPOCRv3 pipeline object\n        \"\"\"\n\n        class PPOCRv2Clone(PPOCRv2):\n            def __init__(self, system):\n                self.system_ = system\n\n        clone_model = PPOCRv2Clone(self.system_.clone())\n        return clone_model",
  "def predict(self, input_image):\n        \"\"\"Predict an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: OCRResult\n        \"\"\"\n        return self.system_.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: OCRBatchResult\n        \"\"\"\n\n        return self.system_.batch_predict(images)",
  "def cls_batch_size(self):\n        return self.system_.cls_batch_size",
  "def cls_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `cls_batch_size` must be type of int.\"\n        self.system_.cls_batch_size = value",
  "def rec_batch_size(self):\n        return self.system_.rec_batch_size",
  "def rec_batch_size(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `rec_batch_size` must be type of int.\"\n        self.system_.rec_batch_size = value",
  "def __init__(self, det_model=None, cls_model=None, rec_model=None):\n        logging.warning(\n            \"DEPRECATED: fd.vision.ocr.PPOCRSystemv2 is deprecated, \"\n            \"please use fd.vision.ocr.PPOCRv2 instead.\")\n        super(PPOCRSystemv2, self).__init__(det_model, cls_model, rec_model)",
  "def predict(self, input_image):\n        return super(PPOCRSystemv2, self).predict(input_image)",
  "def __init__(self, det_model=None, rec_model=None, table_model=None):\n        \"\"\"Consruct a pipeline with text detector, text recognizer and table recognizer models\n\n        :param det_model: (FastDeployModel) The detection model object created by fastdeploy.vision.ocr.DBDetector.\n        :param rec_model: (FastDeployModel) The recognition model object created by fastdeploy.vision.ocr.Recognizer.\n        :param table_model: (FastDeployModel) The table recognition model object created by fastdeploy.vision.ocr.Table.\n        \"\"\"\n        assert det_model is not None and rec_model is not None and table_model is not None, \"The det_model, rec_model and table_model cannot be None.\"\n        self.system_ = C.vision.ocr.PPStructureV2Table(\n            det_model._model,\n            rec_model._model,\n            table_model._model, )",
  "def clone(self):\n        \"\"\"Clone PPStructureV2Table pipeline object\n        :return: a new PPStructureV2Table pipeline object\n        \"\"\"\n\n        class PPStructureV2TableClone(PPStructureV2Table):\n            def __init__(self, system):\n                self.system_ = system\n\n        clone_model = PPStructureV2TableClone(self.system_.clone())\n        return clone_model",
  "def predict(self, input_image):\n        \"\"\"Predict an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: OCRResult\n        \"\"\"\n        return self.system_.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: OCRBatchResult\n        \"\"\"\n\n        return self.system_.batch_predict(images)",
  "def __init__(self, det_model=None, rec_model=None, table_model=None):\n        logging.warning(\n            \"DEPRECATED: fd.vision.ocr.PPStructureV2TableSystem is deprecated, \"\n            \"please use fd.vision.ocr.PPStructureV2Table instead.\")\n        super(PPStructureV2TableSystem, self).__init__(det_model, rec_model,\n                                                       table_model)",
  "def predict(self, input_image):\n        return super(PPStructureV2TableSystem, self).predict(input_image)",
  "def __init__(self, ser_dict_path, use_gpu=True):\n        \"\"\"Create a preprocessor for Ser-Vi-LayoutXLM model.\n        :param: ser_dict_path: (str) class file path\n        :param: use_gpu: (bool) whether use gpu to OCR process\n        \"\"\"\n        self._manager = None\n        from paddleocr import PaddleOCR\n        self.ocr_engine = PaddleOCR(\n            use_angle_cls=False,\n            det_model_dir=None,\n            rec_model_dir=None,\n            show_log=False,\n            use_gpu=use_gpu)\n\n        pre_process_list = [{\n            'VQATokenLabelEncode': {\n                'class_path': ser_dict_path,\n                'contains_re': False,\n                'ocr_engine': self.ocr_engine,\n                'order_method': \"tb-yx\"\n            }\n        }, {\n            'VQATokenPad': {\n                'max_seq_len': 512,\n                'return_attention_mask': True\n            }\n        }, {\n            'VQASerTokenChunk': {\n                'max_seq_len': 512,\n                'return_attention_mask': True\n            }\n        }, {\n            'Resize': {\n                'size': [224, 224]\n            }\n        }, {\n            'NormalizeImage': {\n                'std': [58.395, 57.12, 57.375],\n                'mean': [123.675, 116.28, 103.53],\n                'scale': '1',\n                'order': 'hwc'\n            }\n        }, {\n            'ToCHWImage': None\n        }, {\n            'KeepKeys': {\n                'keep_keys': [\n                    'input_ids', 'bbox', 'attention_mask', 'token_type_ids',\n                    'image', 'labels', 'segment_offset_id', 'ocr_info',\n                    'entities'\n                ]\n            }\n        }]\n\n        self.preprocess_op = create_operators(pre_process_list,\n                                              {'infer_mode': True})",
  "def _transform(self, data, ops=None):\n        \"\"\" transform \"\"\"\n        if ops is None:\n            ops = []\n        for op in ops:\n            data = op(data)\n            if data is None:\n                return None\n        return data",
  "def run(self, input_im):\n        \"\"\"Run preprocess of  Ser-Vi-LayoutXLM model\n        :param: input_ims: (numpy.ndarray) input image\n        \"\"\"\n        ori_im = input_im.copy()\n        data = {'image': input_im}\n        data = transform(data, self.preprocess_op)\n\n        for idx in range(len(data)):\n            if isinstance(data[idx], np.ndarray):\n                data[idx] = np.expand_dims(data[idx], axis=0)\n            else:\n                data[idx] = [data[idx]]\n\n        return data",
  "def __init__(self, class_path):\n        \"\"\"Create a postprocessor for Ser-Vi-LayoutXLM model.\n        :param: class_path: (string) class file path\n        \"\"\"\n        self.postprocessor_op = VQASerTokenLayoutLMPostProcess(class_path)",
  "def run(self, preds, batch=None, *args, **kwargs):\n        \"\"\"Run postprocess of  Ser-Vi-LayoutXLM model.\n        :param: preds: (list) results of infering\n        \"\"\"\n        return self.postprocessor_op(preds, batch, *args, **kwargs)",
  "def __init__(self,\n                 model_file,\n                 params_file,\n                 ser_dict_path,\n                 class_path,\n                 config_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load SERViLayoutXLM model provided by PP-StructureV2.\n\n        :param model_file: (str)Path of model file, e.g ./ser_vi_layout_xlm/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./ser_vi_layout_xlm/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param ser_dict_path: (str) class file path\n        :param class_path: (str) class file path\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(StructureV2SERViLayoutXLMModel, self).__init__(runtime_option)\n\n        assert self._runtime_option.backend != 0, \\\n            \"Runtime Option required backend setting.\"\n        self._model = C.vision.ocr.StructureV2SERViLayoutXLMModel(\n            model_file, params_file, config_file, self._runtime_option,\n            model_format)\n\n        assert self.initialized, \"SERViLayoutXLM model initialize failed.\"\n\n        self.preprocessor = StructureV2SERViLayoutXLMModelPreprocessor(\n            ser_dict_path)\n        self.postprocesser = StructureV2SERViLayoutXLMModelPostprocessor(\n            class_path)\n\n        self.input_name_0 = self._model.get_input_info(0).name\n        self.input_name_1 = self._model.get_input_info(1).name\n        self.input_name_2 = self._model.get_input_info(2).name\n        self.input_name_3 = self._model.get_input_info(3).name",
  "def predict(self, image):\n        assert isinstance(image,\n                          np.ndarray), \"predict recives numpy.ndarray(BGR)\"\n\n        data = self.preprocessor.run(image)\n        infer_input = {\n            self.input_name_0: data[0],\n            self.input_name_1: data[1],\n            self.input_name_2: data[2],\n            self.input_name_3: data[3],\n        }\n\n        infer_result = self._model.infer(infer_input)\n        infer_result = infer_result[0]\n\n        post_result = self.postprocesser.run(infer_result,\n                                             segment_offset_ids=data[6],\n                                             ocr_infos=data[7])\n\n        return post_result",
  "def batch_predict(self, image_list):\n        assert isinstance(image_list, list) and \\\n             isinstance(image_list[0], np.ndarray), \\\n              \"batch_predict recives list of numpy.ndarray(BGR)\"\n\n        # reading and preprocessing images\n        datas = None\n        for image in image_list:\n            data = self.preprocessor.run(image)\n\n            # concatenate data to batch\n            if datas == None:\n                datas = data\n            else:\n                for idx in range(len(data)):\n                    if isinstance(data[idx], np.ndarray):\n                        datas[idx] = np.concatenate(\n                            (datas[idx], data[idx]), axis=0)\n                    else:\n                        datas[idx].extend(data[idx])\n\n        # infer\n        infer_inputs = {\n            self.input_name_0: datas[0],\n            self.input_name_1: datas[1],\n            self.input_name_2: datas[2],\n            self.input_name_3: datas[3],\n        }\n\n        infer_results = self._model.infer(infer_inputs)\n        infer_results = infer_results[0]\n\n        # postprocessing\n        post_results = self.postprocesser.run(infer_results,\n                                              segment_offset_ids=datas[6],\n                                              ocr_infos=datas[7])\n\n        return post_results",
  "class DBDetectorClone(DBDetector):\n            def __init__(self, model):\n                self._model = model",
  "class ClassifierClone(Classifier):\n            def __init__(self, model):\n                self._model = model",
  "class RecognizerClone(Recognizer):\n            def __init__(self, model):\n                self._model = model",
  "class StructureV2TableClone(StructureV2Table):\n            def __init__(self, model):\n                self._model = model",
  "class StructureV2LayoutClone(StructureV2Layout):\n            def __init__(self, model):\n                self._model = model",
  "class PPOCRv4Clone(PPOCRv4):\n            def __init__(self, system):\n                self.system_ = system",
  "class PPOCRv3Clone(PPOCRv3):\n            def __init__(self, system):\n                self.system_ = system",
  "class PPOCRv2Clone(PPOCRv2):\n            def __init__(self, system):\n                self.system_ = system",
  "class PPStructureV2TableClone(PPStructureV2Table):\n            def __init__(self, system):\n                self.system_ = system",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, model):\n                self._model = model",
  "def __init__(self, system):\n                self.system_ = system",
  "def __init__(self, system):\n                self.system_ = system",
  "def __init__(self, system):\n                self.system_ = system",
  "def __init__(self, system):\n                self.system_ = system",
  "def order_by_tbyx(ocr_info):\n    res = sorted(ocr_info, key=lambda r: (r[\"bbox\"][1], r[\"bbox\"][0]))\n    for i in range(len(res) - 1):\n        for j in range(i, 0, -1):\n            if abs(res[j + 1][\"bbox\"][1] - res[j][\"bbox\"][1]) < 20 and \\\n                    (res[j + 1][\"bbox\"][0] < res[j][\"bbox\"][0]):\n                tmp = deepcopy(res[j])\n                res[j] = deepcopy(res[j + 1])\n                res[j + 1] = deepcopy(tmp)\n            else:\n                break\n    return res",
  "def load_vqa_bio_label_maps(label_map_path):\n    with open(label_map_path, \"r\", encoding='utf-8') as fin:\n        lines = fin.readlines()\n    old_lines = [line.strip() for line in lines]\n    lines = [\"O\"]\n    for line in old_lines:\n        # \"O\" has already been in lines\n        if line.upper() in [\"OTHER\", \"OTHERS\", \"IGNORE\"]:\n            continue\n        lines.append(line)\n    labels = [\"O\"]\n    for line in lines[1:]:\n        labels.append(\"B-\" + line)\n        labels.append(\"I-\" + line)\n    label2id_map = {label.upper(): idx for idx, label in enumerate(labels)}\n    id2label_map = {idx: label.upper() for idx, label in enumerate(labels)}\n    return label2id_map, id2label_map",
  "class VQATokenLabelEncode(object):\n    \"\"\"\n    Label encode for NLP VQA methods\n    \"\"\"\n\n    def __init__(self,\n                 class_path,\n                 contains_re=False,\n                 add_special_ids=False,\n                 algorithm='LayoutXLM',\n                 use_textline_bbox_info=True,\n                 order_method=None,\n                 infer_mode=False,\n                 ocr_engine=None,\n                 **kwargs):\n        super(VQATokenLabelEncode, self).__init__()\n        from paddlenlp.transformers import LayoutXLMTokenizer, LayoutLMTokenizer, LayoutLMv2Tokenizer\n        tokenizer_dict = {\n            'LayoutXLM': {\n                'class': LayoutXLMTokenizer,\n                'pretrained_model': 'layoutxlm-base-uncased'\n            },\n            'LayoutLM': {\n                'class': LayoutLMTokenizer,\n                'pretrained_model': 'layoutlm-base-uncased'\n            },\n            'LayoutLMv2': {\n                'class': LayoutLMv2Tokenizer,\n                'pretrained_model': 'layoutlmv2-base-uncased'\n            }\n        }\n        self.contains_re = contains_re\n        tokenizer_config = tokenizer_dict[algorithm]\n        self.tokenizer = tokenizer_config['class'].from_pretrained(\n            tokenizer_config['pretrained_model'])\n        self.label2id_map, id2label_map = load_vqa_bio_label_maps(class_path)\n        self.add_special_ids = add_special_ids\n        self.infer_mode = infer_mode\n        self.ocr_engine = ocr_engine\n        self.use_textline_bbox_info = use_textline_bbox_info\n        self.order_method = order_method\n        assert self.order_method in [None, \"tb-yx\"]\n\n    def split_bbox(self, bbox, text, tokenizer):\n        words = text.split()\n        token_bboxes = []\n        curr_word_idx = 0\n        x1, y1, x2, y2 = bbox\n        unit_w = (x2 - x1) / len(text)\n        for idx, word in enumerate(words):\n            curr_w = len(word) * unit_w\n            word_bbox = [x1, y1, x1 + curr_w, y2]\n            token_bboxes.extend([word_bbox] * len(tokenizer.tokenize(word)))\n            x1 += (len(word) + 1) * unit_w\n        return token_bboxes\n\n    def filter_empty_contents(self, ocr_info):\n        \"\"\"\n        find out the empty texts and remove the links\n        \"\"\"\n        new_ocr_info = []\n        empty_index = []\n        for idx, info in enumerate(ocr_info):\n            if len(info[\"transcription\"]) > 0:\n                new_ocr_info.append(copy.deepcopy(info))\n            else:\n                empty_index.append(info[\"id\"])\n\n        for idx, info in enumerate(new_ocr_info):\n            new_link = []\n            for link in info[\"linking\"]:\n                if link[0] in empty_index or link[1] in empty_index:\n                    continue\n                new_link.append(link)\n            new_ocr_info[idx][\"linking\"] = new_link\n        return new_ocr_info\n\n    def __call__(self, data):\n        # load bbox and label info\n        ocr_info = self._load_ocr_info(data)\n\n        for idx in range(len(ocr_info)):\n            if \"bbox\" not in ocr_info[idx]:\n                ocr_info[idx][\"bbox\"] = self.trans_poly_to_bbox(ocr_info[idx][\n                    \"points\"])\n\n        if self.order_method == \"tb-yx\":\n            ocr_info = order_by_tbyx(ocr_info)\n\n        # for re\n        train_re = self.contains_re and not self.infer_mode\n        if train_re:\n            ocr_info = self.filter_empty_contents(ocr_info)\n\n        height, width, _ = data['image'].shape\n\n        words_list = []\n        bbox_list = []\n        input_ids_list = []\n        token_type_ids_list = []\n        segment_offset_id = []\n        gt_label_list = []\n\n        entities = []\n\n        if train_re:\n            relations = []\n            id2label = {}\n            entity_id_to_index_map = {}\n            empty_entity = set()\n\n        data['ocr_info'] = copy.deepcopy(ocr_info)\n\n        for info in ocr_info:\n            text = info[\"transcription\"]\n            if len(text) <= 0:\n                continue\n            if train_re:\n                # for re\n                if len(text) == 0:\n                    empty_entity.add(info[\"id\"])\n                    continue\n                id2label[info[\"id\"]] = info[\"label\"]\n                relations.extend([tuple(sorted(l)) for l in info[\"linking\"]])\n            # smooth_box\n            info[\"bbox\"] = self.trans_poly_to_bbox(info[\"points\"])\n\n            encode_res = self.tokenizer.encode(\n                text,\n                pad_to_max_seq_len=False,\n                return_attention_mask=True,\n                return_token_type_ids=True)\n\n            if not self.add_special_ids:\n                # TODO: use tok.all_special_ids to remove\n                encode_res[\"input_ids\"] = encode_res[\"input_ids\"][1:-1]\n                encode_res[\"token_type_ids\"] = encode_res[\"token_type_ids\"][1:\n                                                                            -1]\n                encode_res[\"attention_mask\"] = encode_res[\"attention_mask\"][1:\n                                                                            -1]\n\n            if self.use_textline_bbox_info:\n                bbox = [info[\"bbox\"]] * len(encode_res[\"input_ids\"])\n            else:\n                bbox = self.split_bbox(info[\"bbox\"], info[\"transcription\"],\n                                       self.tokenizer)\n            if len(bbox) <= 0:\n                continue\n            bbox = self._smooth_box(bbox, height, width)\n            if self.add_special_ids:\n                bbox.insert(0, [0, 0, 0, 0])\n                bbox.append([0, 0, 0, 0])\n\n            # parse label\n            if not self.infer_mode:\n                label = info['label']\n                gt_label = self._parse_label(label, encode_res)\n\n            # construct entities for re\n            if train_re:\n                if gt_label[0] != self.label2id_map[\"O\"]:\n                    entity_id_to_index_map[info[\"id\"]] = len(entities)\n                    label = label.upper()\n                    entities.append({\n                        \"start\": len(input_ids_list),\n                        \"end\":\n                        len(input_ids_list) + len(encode_res[\"input_ids\"]),\n                        \"label\": label.upper(),\n                    })\n            else:\n                entities.append({\n                    \"start\": len(input_ids_list),\n                    \"end\": len(input_ids_list) + len(encode_res[\"input_ids\"]),\n                    \"label\": 'O',\n                })\n            input_ids_list.extend(encode_res[\"input_ids\"])\n            token_type_ids_list.extend(encode_res[\"token_type_ids\"])\n            bbox_list.extend(bbox)\n            words_list.append(text)\n            segment_offset_id.append(len(input_ids_list))\n            if not self.infer_mode:\n                gt_label_list.extend(gt_label)\n\n        data['input_ids'] = input_ids_list\n        data['token_type_ids'] = token_type_ids_list\n        data['bbox'] = bbox_list\n        data['attention_mask'] = [1] * len(input_ids_list)\n        data['labels'] = gt_label_list\n        data['segment_offset_id'] = segment_offset_id\n        data['tokenizer_params'] = dict(\n            padding_side=self.tokenizer.padding_side,\n            pad_token_type_id=self.tokenizer.pad_token_type_id,\n            pad_token_id=self.tokenizer.pad_token_id)\n        data['entities'] = entities\n\n        if train_re:\n            data['relations'] = relations\n            data['id2label'] = id2label\n            data['empty_entity'] = empty_entity\n            data['entity_id_to_index_map'] = entity_id_to_index_map\n        return data\n\n    def trans_poly_to_bbox(self, poly):\n        x1 = int(np.min([p[0] for p in poly]))\n        x2 = int(np.max([p[0] for p in poly]))\n        y1 = int(np.min([p[1] for p in poly]))\n        y2 = int(np.max([p[1] for p in poly]))\n        return [x1, y1, x2, y2]\n\n    def _load_ocr_info(self, data):\n        if self.infer_mode:\n            ocr_result = self.ocr_engine.ocr(data['image'], cls=False)[0]\n            ocr_info = []\n            for res in ocr_result:\n                ocr_info.append({\n                    \"transcription\": res[1][0],\n                    \"bbox\": self.trans_poly_to_bbox(res[0]),\n                    \"points\": res[0],\n                })\n            return ocr_info\n        else:\n            info = data['label']\n            # read text info\n            info_dict = json.loads(info)\n            return info_dict\n\n    def _smooth_box(self, bboxes, height, width):\n        bboxes = np.array(bboxes)\n        bboxes[:, 0] = bboxes[:, 0] * 1000 / width\n        bboxes[:, 2] = bboxes[:, 2] * 1000 / width\n        bboxes[:, 1] = bboxes[:, 1] * 1000 / height\n        bboxes[:, 3] = bboxes[:, 3] * 1000 / height\n        bboxes = bboxes.astype(\"int64\").tolist()\n        return bboxes\n\n    def _parse_label(self, label, encode_res):\n        gt_label = []\n        if label.lower() in [\"other\", \"others\", \"ignore\"]:\n            gt_label.extend([0] * len(encode_res[\"input_ids\"]))\n        else:\n            gt_label.append(self.label2id_map[(\"b-\" + label).upper()])\n            gt_label.extend([self.label2id_map[(\"i-\" + label).upper()]] *\n                            (len(encode_res[\"input_ids\"]) - 1))\n        return gt_label",
  "class VQATokenPad(object):\n    def __init__(self,\n                 max_seq_len=512,\n                 pad_to_max_seq_len=True,\n                 return_attention_mask=True,\n                 return_token_type_ids=True,\n                 truncation_strategy=\"longest_first\",\n                 return_overflowing_tokens=False,\n                 return_special_tokens_mask=False,\n                 infer_mode=False,\n                 **kwargs):\n\n        self.max_seq_len = max_seq_len\n        self.pad_to_max_seq_len = max_seq_len\n        self.return_attention_mask = return_attention_mask\n        self.return_token_type_ids = return_token_type_ids\n        self.truncation_strategy = truncation_strategy\n        self.return_overflowing_tokens = return_overflowing_tokens\n        self.return_special_tokens_mask = return_special_tokens_mask\n        self.infer_mode = infer_mode\n\n    def __call__(self, data):\n        import paddle\n        self.pad_token_label_id = paddle.nn.CrossEntropyLoss().ignore_index\n        needs_to_be_padded = self.pad_to_max_seq_len and len(data[\n            \"input_ids\"]) < self.max_seq_len\n\n        if needs_to_be_padded:\n            if 'tokenizer_params' in data:\n                tokenizer_params = data.pop('tokenizer_params')\n            else:\n                tokenizer_params = dict(\n                    padding_side='right', pad_token_type_id=0, pad_token_id=1)\n\n            difference = self.max_seq_len - len(data[\"input_ids\"])\n            if tokenizer_params['padding_side'] == 'right':\n                if self.return_attention_mask:\n                    data[\"attention_mask\"] = [1] * len(data[\n                        \"input_ids\"]) + [0] * difference\n                if self.return_token_type_ids:\n                    data[\"token_type_ids\"] = (\n                        data[\"token_type_ids\"] +\n                        [tokenizer_params['pad_token_type_id']] * difference)\n                if self.return_special_tokens_mask:\n                    data[\"special_tokens_mask\"] = data[\n                        \"special_tokens_mask\"] + [1] * difference\n                data[\"input_ids\"] = data[\"input_ids\"] + [\n                    tokenizer_params['pad_token_id']\n                ] * difference\n                if not self.infer_mode:\n                    data[\"labels\"] = data[\n                        \"labels\"] + [self.pad_token_label_id] * difference\n                data[\"bbox\"] = data[\"bbox\"] + [[0, 0, 0, 0]] * difference\n            elif tokenizer_params['padding_side'] == 'left':\n                if self.return_attention_mask:\n                    data[\"attention_mask\"] = [0] * difference + [\n                        1\n                    ] * len(data[\"input_ids\"])\n                if self.return_token_type_ids:\n                    data[\"token_type_ids\"] = (\n                        [tokenizer_params['pad_token_type_id']] * difference +\n                        data[\"token_type_ids\"])\n                if self.return_special_tokens_mask:\n                    data[\"special_tokens_mask\"] = [\n                        1\n                    ] * difference + data[\"special_tokens_mask\"]\n                data[\"input_ids\"] = [tokenizer_params['pad_token_id']\n                                     ] * difference + data[\"input_ids\"]\n                if not self.infer_mode:\n                    data[\"labels\"] = [self.pad_token_label_id\n                                      ] * difference + data[\"labels\"]\n                data[\"bbox\"] = [[0, 0, 0, 0]] * difference + data[\"bbox\"]\n        else:\n            if self.return_attention_mask:\n                data[\"attention_mask\"] = [1] * len(data[\"input_ids\"])\n\n        for key in data:\n            if key in [\n                    'input_ids', 'labels', 'token_type_ids', 'bbox',\n                    'attention_mask'\n            ]:\n                if self.infer_mode:\n                    if key != 'labels':\n                        length = min(len(data[key]), self.max_seq_len)\n                        data[key] = data[key][:length]\n                    else:\n                        continue\n                data[key] = np.array(data[key], dtype='int64')\n        return data",
  "class VQASerTokenChunk(object):\n    def __init__(self, max_seq_len=512, infer_mode=False, **kwargs):\n        self.max_seq_len = max_seq_len\n        self.infer_mode = infer_mode\n\n    def __call__(self, data):\n        encoded_inputs_all = []\n        seq_len = len(data['input_ids'])\n        for index in range(0, seq_len, self.max_seq_len):\n            chunk_beg = index\n            chunk_end = min(index + self.max_seq_len, seq_len)\n            encoded_inputs_example = {}\n            for key in data:\n                if key in [\n                        'label', 'input_ids', 'labels', 'token_type_ids',\n                        'bbox', 'attention_mask'\n                ]:\n                    if self.infer_mode and key == 'labels':\n                        encoded_inputs_example[key] = data[key]\n                    else:\n                        encoded_inputs_example[key] = data[key][chunk_beg:\n                                                                chunk_end]\n                else:\n                    encoded_inputs_example[key] = data[key]\n\n            encoded_inputs_all.append(encoded_inputs_example)\n        if len(encoded_inputs_all) == 0:\n            return None\n        return encoded_inputs_all[0]",
  "class VQAReTokenChunk(object):\n    def __init__(self,\n                 max_seq_len=512,\n                 entities_labels=None,\n                 infer_mode=False,\n                 **kwargs):\n        self.max_seq_len = max_seq_len\n        self.entities_labels = {\n            'HEADER': 0,\n            'QUESTION': 1,\n            'ANSWER': 2\n        } if entities_labels is None else entities_labels\n        self.infer_mode = infer_mode\n\n    def __call__(self, data):\n        # prepare data\n        entities = data.pop('entities')\n        relations = data.pop('relations')\n        encoded_inputs_all = []\n        for index in range(0, len(data[\"input_ids\"]), self.max_seq_len):\n            item = {}\n            for key in data:\n                if key in [\n                        'label', 'input_ids', 'labels', 'token_type_ids',\n                        'bbox', 'attention_mask'\n                ]:\n                    if self.infer_mode and key == 'labels':\n                        item[key] = data[key]\n                    else:\n                        item[key] = data[key][index:index + self.max_seq_len]\n                else:\n                    item[key] = data[key]\n            # select entity in current chunk\n            entities_in_this_span = []\n            global_to_local_map = {}  #\n            for entity_id, entity in enumerate(entities):\n                if (index <= entity[\"start\"] < index + self.max_seq_len and\n                        index <= entity[\"end\"] < index + self.max_seq_len):\n                    entity[\"start\"] = entity[\"start\"] - index\n                    entity[\"end\"] = entity[\"end\"] - index\n                    global_to_local_map[entity_id] = len(entities_in_this_span)\n                    entities_in_this_span.append(entity)\n\n            # select relations in current chunk\n            relations_in_this_span = []\n            for relation in relations:\n                if (index <= relation[\"start_index\"] < index + self.max_seq_len\n                        and index <= relation[\"end_index\"] <\n                        index + self.max_seq_len):\n                    relations_in_this_span.append({\n                        \"head\": global_to_local_map[relation[\"head\"]],\n                        \"tail\": global_to_local_map[relation[\"tail\"]],\n                        \"start_index\": relation[\"start_index\"] - index,\n                        \"end_index\": relation[\"end_index\"] - index,\n                    })\n            item.update({\n                \"entities\": self.reformat(entities_in_this_span),\n                \"relations\": self.reformat(relations_in_this_span),\n            })\n            if len(item['entities']) > 0:\n                item['entities']['label'] = [\n                    self.entities_labels[x] for x in item['entities']['label']\n                ]\n                encoded_inputs_all.append(item)\n        if len(encoded_inputs_all) == 0:\n            return None\n        return encoded_inputs_all[0]\n\n    def reformat(self, data):\n        new_data = defaultdict(list)\n        for item in data:\n            for k, v in item.items():\n                new_data[k].append(v)\n        return new_data",
  "class VQASerTokenLayoutLMPostProcess(object):\n    \"\"\" Convert between text-label and text-index \"\"\"\n\n    def __init__(self, class_path, **kwargs):\n        super(VQASerTokenLayoutLMPostProcess, self).__init__()\n        label2id_map, self.id2label_map = load_vqa_bio_label_maps(class_path)\n\n        self.label2id_map_for_draw = dict()\n        for key in label2id_map:\n            if key.startswith(\"I-\"):\n                self.label2id_map_for_draw[key] = label2id_map[\"B\" + key[1:]]\n            else:\n                self.label2id_map_for_draw[key] = label2id_map[key]\n\n        self.id2label_map_for_show = dict()\n        for key in self.label2id_map_for_draw:\n            val = self.label2id_map_for_draw[key]\n            if key == \"O\":\n                self.id2label_map_for_show[val] = key\n            if key.startswith(\"B-\") or key.startswith(\"I-\"):\n                self.id2label_map_for_show[val] = key[2:]\n            else:\n                self.id2label_map_for_show[val] = key\n\n    def __call__(self, preds, batch=None, *args, **kwargs):\n        import paddle\n        if isinstance(preds, tuple):\n            preds = preds[0]\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n\n        if batch is not None:\n            return self._metric(preds, batch[5])\n        else:\n            return self._infer(preds, **kwargs)\n\n    def _metric(self, preds, label):\n        pred_idxs = preds.argmax(axis=2)\n        decode_out_list = [[] for _ in range(pred_idxs.shape[0])]\n        label_decode_out_list = [[] for _ in range(pred_idxs.shape[0])]\n\n        for i in range(pred_idxs.shape[0]):\n            for j in range(pred_idxs.shape[1]):\n                if label[i, j] != -100:\n                    label_decode_out_list[i].append(self.id2label_map[label[\n                        i, j]])\n                    decode_out_list[i].append(self.id2label_map[pred_idxs[i,\n                                                                          j]])\n        return decode_out_list, label_decode_out_list\n\n    def _infer(self, preds, segment_offset_ids, ocr_infos):\n        results = []\n\n        for pred, segment_offset_id, ocr_info in zip(preds, segment_offset_ids,\n                                                     ocr_infos):\n            pred = np.argmax(pred, axis=1)\n            pred = [self.id2label_map[idx] for idx in pred]\n\n            for idx in range(len(segment_offset_id)):\n                if idx == 0:\n                    start_id = 0\n                else:\n                    start_id = segment_offset_id[idx - 1]\n\n                end_id = segment_offset_id[idx]\n\n                curr_pred = pred[start_id:end_id]\n                curr_pred = [self.label2id_map_for_draw[p] for p in curr_pred]\n\n                if len(curr_pred) <= 0:\n                    pred_id = 0\n                else:\n                    counts = np.bincount(curr_pred)\n                    pred_id = np.argmax(counts)\n                ocr_info[idx][\"pred_id\"] = int(pred_id)\n                ocr_info[idx][\"pred\"] = self.id2label_map_for_show[int(\n                    pred_id)]\n            results.append(ocr_info)\n        return results",
  "def __init__(self,\n                 class_path,\n                 contains_re=False,\n                 add_special_ids=False,\n                 algorithm='LayoutXLM',\n                 use_textline_bbox_info=True,\n                 order_method=None,\n                 infer_mode=False,\n                 ocr_engine=None,\n                 **kwargs):\n        super(VQATokenLabelEncode, self).__init__()\n        from paddlenlp.transformers import LayoutXLMTokenizer, LayoutLMTokenizer, LayoutLMv2Tokenizer\n        tokenizer_dict = {\n            'LayoutXLM': {\n                'class': LayoutXLMTokenizer,\n                'pretrained_model': 'layoutxlm-base-uncased'\n            },\n            'LayoutLM': {\n                'class': LayoutLMTokenizer,\n                'pretrained_model': 'layoutlm-base-uncased'\n            },\n            'LayoutLMv2': {\n                'class': LayoutLMv2Tokenizer,\n                'pretrained_model': 'layoutlmv2-base-uncased'\n            }\n        }\n        self.contains_re = contains_re\n        tokenizer_config = tokenizer_dict[algorithm]\n        self.tokenizer = tokenizer_config['class'].from_pretrained(\n            tokenizer_config['pretrained_model'])\n        self.label2id_map, id2label_map = load_vqa_bio_label_maps(class_path)\n        self.add_special_ids = add_special_ids\n        self.infer_mode = infer_mode\n        self.ocr_engine = ocr_engine\n        self.use_textline_bbox_info = use_textline_bbox_info\n        self.order_method = order_method\n        assert self.order_method in [None, \"tb-yx\"]",
  "def split_bbox(self, bbox, text, tokenizer):\n        words = text.split()\n        token_bboxes = []\n        curr_word_idx = 0\n        x1, y1, x2, y2 = bbox\n        unit_w = (x2 - x1) / len(text)\n        for idx, word in enumerate(words):\n            curr_w = len(word) * unit_w\n            word_bbox = [x1, y1, x1 + curr_w, y2]\n            token_bboxes.extend([word_bbox] * len(tokenizer.tokenize(word)))\n            x1 += (len(word) + 1) * unit_w\n        return token_bboxes",
  "def filter_empty_contents(self, ocr_info):\n        \"\"\"\n        find out the empty texts and remove the links\n        \"\"\"\n        new_ocr_info = []\n        empty_index = []\n        for idx, info in enumerate(ocr_info):\n            if len(info[\"transcription\"]) > 0:\n                new_ocr_info.append(copy.deepcopy(info))\n            else:\n                empty_index.append(info[\"id\"])\n\n        for idx, info in enumerate(new_ocr_info):\n            new_link = []\n            for link in info[\"linking\"]:\n                if link[0] in empty_index or link[1] in empty_index:\n                    continue\n                new_link.append(link)\n            new_ocr_info[idx][\"linking\"] = new_link\n        return new_ocr_info",
  "def __call__(self, data):\n        # load bbox and label info\n        ocr_info = self._load_ocr_info(data)\n\n        for idx in range(len(ocr_info)):\n            if \"bbox\" not in ocr_info[idx]:\n                ocr_info[idx][\"bbox\"] = self.trans_poly_to_bbox(ocr_info[idx][\n                    \"points\"])\n\n        if self.order_method == \"tb-yx\":\n            ocr_info = order_by_tbyx(ocr_info)\n\n        # for re\n        train_re = self.contains_re and not self.infer_mode\n        if train_re:\n            ocr_info = self.filter_empty_contents(ocr_info)\n\n        height, width, _ = data['image'].shape\n\n        words_list = []\n        bbox_list = []\n        input_ids_list = []\n        token_type_ids_list = []\n        segment_offset_id = []\n        gt_label_list = []\n\n        entities = []\n\n        if train_re:\n            relations = []\n            id2label = {}\n            entity_id_to_index_map = {}\n            empty_entity = set()\n\n        data['ocr_info'] = copy.deepcopy(ocr_info)\n\n        for info in ocr_info:\n            text = info[\"transcription\"]\n            if len(text) <= 0:\n                continue\n            if train_re:\n                # for re\n                if len(text) == 0:\n                    empty_entity.add(info[\"id\"])\n                    continue\n                id2label[info[\"id\"]] = info[\"label\"]\n                relations.extend([tuple(sorted(l)) for l in info[\"linking\"]])\n            # smooth_box\n            info[\"bbox\"] = self.trans_poly_to_bbox(info[\"points\"])\n\n            encode_res = self.tokenizer.encode(\n                text,\n                pad_to_max_seq_len=False,\n                return_attention_mask=True,\n                return_token_type_ids=True)\n\n            if not self.add_special_ids:\n                # TODO: use tok.all_special_ids to remove\n                encode_res[\"input_ids\"] = encode_res[\"input_ids\"][1:-1]\n                encode_res[\"token_type_ids\"] = encode_res[\"token_type_ids\"][1:\n                                                                            -1]\n                encode_res[\"attention_mask\"] = encode_res[\"attention_mask\"][1:\n                                                                            -1]\n\n            if self.use_textline_bbox_info:\n                bbox = [info[\"bbox\"]] * len(encode_res[\"input_ids\"])\n            else:\n                bbox = self.split_bbox(info[\"bbox\"], info[\"transcription\"],\n                                       self.tokenizer)\n            if len(bbox) <= 0:\n                continue\n            bbox = self._smooth_box(bbox, height, width)\n            if self.add_special_ids:\n                bbox.insert(0, [0, 0, 0, 0])\n                bbox.append([0, 0, 0, 0])\n\n            # parse label\n            if not self.infer_mode:\n                label = info['label']\n                gt_label = self._parse_label(label, encode_res)\n\n            # construct entities for re\n            if train_re:\n                if gt_label[0] != self.label2id_map[\"O\"]:\n                    entity_id_to_index_map[info[\"id\"]] = len(entities)\n                    label = label.upper()\n                    entities.append({\n                        \"start\": len(input_ids_list),\n                        \"end\":\n                        len(input_ids_list) + len(encode_res[\"input_ids\"]),\n                        \"label\": label.upper(),\n                    })\n            else:\n                entities.append({\n                    \"start\": len(input_ids_list),\n                    \"end\": len(input_ids_list) + len(encode_res[\"input_ids\"]),\n                    \"label\": 'O',\n                })\n            input_ids_list.extend(encode_res[\"input_ids\"])\n            token_type_ids_list.extend(encode_res[\"token_type_ids\"])\n            bbox_list.extend(bbox)\n            words_list.append(text)\n            segment_offset_id.append(len(input_ids_list))\n            if not self.infer_mode:\n                gt_label_list.extend(gt_label)\n\n        data['input_ids'] = input_ids_list\n        data['token_type_ids'] = token_type_ids_list\n        data['bbox'] = bbox_list\n        data['attention_mask'] = [1] * len(input_ids_list)\n        data['labels'] = gt_label_list\n        data['segment_offset_id'] = segment_offset_id\n        data['tokenizer_params'] = dict(\n            padding_side=self.tokenizer.padding_side,\n            pad_token_type_id=self.tokenizer.pad_token_type_id,\n            pad_token_id=self.tokenizer.pad_token_id)\n        data['entities'] = entities\n\n        if train_re:\n            data['relations'] = relations\n            data['id2label'] = id2label\n            data['empty_entity'] = empty_entity\n            data['entity_id_to_index_map'] = entity_id_to_index_map\n        return data",
  "def trans_poly_to_bbox(self, poly):\n        x1 = int(np.min([p[0] for p in poly]))\n        x2 = int(np.max([p[0] for p in poly]))\n        y1 = int(np.min([p[1] for p in poly]))\n        y2 = int(np.max([p[1] for p in poly]))\n        return [x1, y1, x2, y2]",
  "def _load_ocr_info(self, data):\n        if self.infer_mode:\n            ocr_result = self.ocr_engine.ocr(data['image'], cls=False)[0]\n            ocr_info = []\n            for res in ocr_result:\n                ocr_info.append({\n                    \"transcription\": res[1][0],\n                    \"bbox\": self.trans_poly_to_bbox(res[0]),\n                    \"points\": res[0],\n                })\n            return ocr_info\n        else:\n            info = data['label']\n            # read text info\n            info_dict = json.loads(info)\n            return info_dict",
  "def _smooth_box(self, bboxes, height, width):\n        bboxes = np.array(bboxes)\n        bboxes[:, 0] = bboxes[:, 0] * 1000 / width\n        bboxes[:, 2] = bboxes[:, 2] * 1000 / width\n        bboxes[:, 1] = bboxes[:, 1] * 1000 / height\n        bboxes[:, 3] = bboxes[:, 3] * 1000 / height\n        bboxes = bboxes.astype(\"int64\").tolist()\n        return bboxes",
  "def _parse_label(self, label, encode_res):\n        gt_label = []\n        if label.lower() in [\"other\", \"others\", \"ignore\"]:\n            gt_label.extend([0] * len(encode_res[\"input_ids\"]))\n        else:\n            gt_label.append(self.label2id_map[(\"b-\" + label).upper()])\n            gt_label.extend([self.label2id_map[(\"i-\" + label).upper()]] *\n                            (len(encode_res[\"input_ids\"]) - 1))\n        return gt_label",
  "def __init__(self,\n                 max_seq_len=512,\n                 pad_to_max_seq_len=True,\n                 return_attention_mask=True,\n                 return_token_type_ids=True,\n                 truncation_strategy=\"longest_first\",\n                 return_overflowing_tokens=False,\n                 return_special_tokens_mask=False,\n                 infer_mode=False,\n                 **kwargs):\n\n        self.max_seq_len = max_seq_len\n        self.pad_to_max_seq_len = max_seq_len\n        self.return_attention_mask = return_attention_mask\n        self.return_token_type_ids = return_token_type_ids\n        self.truncation_strategy = truncation_strategy\n        self.return_overflowing_tokens = return_overflowing_tokens\n        self.return_special_tokens_mask = return_special_tokens_mask\n        self.infer_mode = infer_mode",
  "def __call__(self, data):\n        import paddle\n        self.pad_token_label_id = paddle.nn.CrossEntropyLoss().ignore_index\n        needs_to_be_padded = self.pad_to_max_seq_len and len(data[\n            \"input_ids\"]) < self.max_seq_len\n\n        if needs_to_be_padded:\n            if 'tokenizer_params' in data:\n                tokenizer_params = data.pop('tokenizer_params')\n            else:\n                tokenizer_params = dict(\n                    padding_side='right', pad_token_type_id=0, pad_token_id=1)\n\n            difference = self.max_seq_len - len(data[\"input_ids\"])\n            if tokenizer_params['padding_side'] == 'right':\n                if self.return_attention_mask:\n                    data[\"attention_mask\"] = [1] * len(data[\n                        \"input_ids\"]) + [0] * difference\n                if self.return_token_type_ids:\n                    data[\"token_type_ids\"] = (\n                        data[\"token_type_ids\"] +\n                        [tokenizer_params['pad_token_type_id']] * difference)\n                if self.return_special_tokens_mask:\n                    data[\"special_tokens_mask\"] = data[\n                        \"special_tokens_mask\"] + [1] * difference\n                data[\"input_ids\"] = data[\"input_ids\"] + [\n                    tokenizer_params['pad_token_id']\n                ] * difference\n                if not self.infer_mode:\n                    data[\"labels\"] = data[\n                        \"labels\"] + [self.pad_token_label_id] * difference\n                data[\"bbox\"] = data[\"bbox\"] + [[0, 0, 0, 0]] * difference\n            elif tokenizer_params['padding_side'] == 'left':\n                if self.return_attention_mask:\n                    data[\"attention_mask\"] = [0] * difference + [\n                        1\n                    ] * len(data[\"input_ids\"])\n                if self.return_token_type_ids:\n                    data[\"token_type_ids\"] = (\n                        [tokenizer_params['pad_token_type_id']] * difference +\n                        data[\"token_type_ids\"])\n                if self.return_special_tokens_mask:\n                    data[\"special_tokens_mask\"] = [\n                        1\n                    ] * difference + data[\"special_tokens_mask\"]\n                data[\"input_ids\"] = [tokenizer_params['pad_token_id']\n                                     ] * difference + data[\"input_ids\"]\n                if not self.infer_mode:\n                    data[\"labels\"] = [self.pad_token_label_id\n                                      ] * difference + data[\"labels\"]\n                data[\"bbox\"] = [[0, 0, 0, 0]] * difference + data[\"bbox\"]\n        else:\n            if self.return_attention_mask:\n                data[\"attention_mask\"] = [1] * len(data[\"input_ids\"])\n\n        for key in data:\n            if key in [\n                    'input_ids', 'labels', 'token_type_ids', 'bbox',\n                    'attention_mask'\n            ]:\n                if self.infer_mode:\n                    if key != 'labels':\n                        length = min(len(data[key]), self.max_seq_len)\n                        data[key] = data[key][:length]\n                    else:\n                        continue\n                data[key] = np.array(data[key], dtype='int64')\n        return data",
  "def __init__(self, max_seq_len=512, infer_mode=False, **kwargs):\n        self.max_seq_len = max_seq_len\n        self.infer_mode = infer_mode",
  "def __call__(self, data):\n        encoded_inputs_all = []\n        seq_len = len(data['input_ids'])\n        for index in range(0, seq_len, self.max_seq_len):\n            chunk_beg = index\n            chunk_end = min(index + self.max_seq_len, seq_len)\n            encoded_inputs_example = {}\n            for key in data:\n                if key in [\n                        'label', 'input_ids', 'labels', 'token_type_ids',\n                        'bbox', 'attention_mask'\n                ]:\n                    if self.infer_mode and key == 'labels':\n                        encoded_inputs_example[key] = data[key]\n                    else:\n                        encoded_inputs_example[key] = data[key][chunk_beg:\n                                                                chunk_end]\n                else:\n                    encoded_inputs_example[key] = data[key]\n\n            encoded_inputs_all.append(encoded_inputs_example)\n        if len(encoded_inputs_all) == 0:\n            return None\n        return encoded_inputs_all[0]",
  "def __init__(self,\n                 max_seq_len=512,\n                 entities_labels=None,\n                 infer_mode=False,\n                 **kwargs):\n        self.max_seq_len = max_seq_len\n        self.entities_labels = {\n            'HEADER': 0,\n            'QUESTION': 1,\n            'ANSWER': 2\n        } if entities_labels is None else entities_labels\n        self.infer_mode = infer_mode",
  "def __call__(self, data):\n        # prepare data\n        entities = data.pop('entities')\n        relations = data.pop('relations')\n        encoded_inputs_all = []\n        for index in range(0, len(data[\"input_ids\"]), self.max_seq_len):\n            item = {}\n            for key in data:\n                if key in [\n                        'label', 'input_ids', 'labels', 'token_type_ids',\n                        'bbox', 'attention_mask'\n                ]:\n                    if self.infer_mode and key == 'labels':\n                        item[key] = data[key]\n                    else:\n                        item[key] = data[key][index:index + self.max_seq_len]\n                else:\n                    item[key] = data[key]\n            # select entity in current chunk\n            entities_in_this_span = []\n            global_to_local_map = {}  #\n            for entity_id, entity in enumerate(entities):\n                if (index <= entity[\"start\"] < index + self.max_seq_len and\n                        index <= entity[\"end\"] < index + self.max_seq_len):\n                    entity[\"start\"] = entity[\"start\"] - index\n                    entity[\"end\"] = entity[\"end\"] - index\n                    global_to_local_map[entity_id] = len(entities_in_this_span)\n                    entities_in_this_span.append(entity)\n\n            # select relations in current chunk\n            relations_in_this_span = []\n            for relation in relations:\n                if (index <= relation[\"start_index\"] < index + self.max_seq_len\n                        and index <= relation[\"end_index\"] <\n                        index + self.max_seq_len):\n                    relations_in_this_span.append({\n                        \"head\": global_to_local_map[relation[\"head\"]],\n                        \"tail\": global_to_local_map[relation[\"tail\"]],\n                        \"start_index\": relation[\"start_index\"] - index,\n                        \"end_index\": relation[\"end_index\"] - index,\n                    })\n            item.update({\n                \"entities\": self.reformat(entities_in_this_span),\n                \"relations\": self.reformat(relations_in_this_span),\n            })\n            if len(item['entities']) > 0:\n                item['entities']['label'] = [\n                    self.entities_labels[x] for x in item['entities']['label']\n                ]\n                encoded_inputs_all.append(item)\n        if len(encoded_inputs_all) == 0:\n            return None\n        return encoded_inputs_all[0]",
  "def reformat(self, data):\n        new_data = defaultdict(list)\n        for item in data:\n            for k, v in item.items():\n                new_data[k].append(v)\n        return new_data",
  "def __init__(self, class_path, **kwargs):\n        super(VQASerTokenLayoutLMPostProcess, self).__init__()\n        label2id_map, self.id2label_map = load_vqa_bio_label_maps(class_path)\n\n        self.label2id_map_for_draw = dict()\n        for key in label2id_map:\n            if key.startswith(\"I-\"):\n                self.label2id_map_for_draw[key] = label2id_map[\"B\" + key[1:]]\n            else:\n                self.label2id_map_for_draw[key] = label2id_map[key]\n\n        self.id2label_map_for_show = dict()\n        for key in self.label2id_map_for_draw:\n            val = self.label2id_map_for_draw[key]\n            if key == \"O\":\n                self.id2label_map_for_show[val] = key\n            if key.startswith(\"B-\") or key.startswith(\"I-\"):\n                self.id2label_map_for_show[val] = key[2:]\n            else:\n                self.id2label_map_for_show[val] = key",
  "def __call__(self, preds, batch=None, *args, **kwargs):\n        import paddle\n        if isinstance(preds, tuple):\n            preds = preds[0]\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n\n        if batch is not None:\n            return self._metric(preds, batch[5])\n        else:\n            return self._infer(preds, **kwargs)",
  "def _metric(self, preds, label):\n        pred_idxs = preds.argmax(axis=2)\n        decode_out_list = [[] for _ in range(pred_idxs.shape[0])]\n        label_decode_out_list = [[] for _ in range(pred_idxs.shape[0])]\n\n        for i in range(pred_idxs.shape[0]):\n            for j in range(pred_idxs.shape[1]):\n                if label[i, j] != -100:\n                    label_decode_out_list[i].append(self.id2label_map[label[\n                        i, j]])\n                    decode_out_list[i].append(self.id2label_map[pred_idxs[i,\n                                                                          j]])\n        return decode_out_list, label_decode_out_list",
  "def _infer(self, preds, segment_offset_ids, ocr_infos):\n        results = []\n\n        for pred, segment_offset_id, ocr_info in zip(preds, segment_offset_ids,\n                                                     ocr_infos):\n            pred = np.argmax(pred, axis=1)\n            pred = [self.id2label_map[idx] for idx in pred]\n\n            for idx in range(len(segment_offset_id)):\n                if idx == 0:\n                    start_id = 0\n                else:\n                    start_id = segment_offset_id[idx - 1]\n\n                end_id = segment_offset_id[idx]\n\n                curr_pred = pred[start_id:end_id]\n                curr_pred = [self.label2id_map_for_draw[p] for p in curr_pred]\n\n                if len(curr_pred) <= 0:\n                    pred_id = 0\n                else:\n                    counts = np.bincount(curr_pred)\n                    pred_id = np.argmax(counts)\n                ocr_info[idx][\"pred_id\"] = int(pred_id)\n                ocr_info[idx][\"pred\"] = self.id2label_map_for_show[int(\n                    pred_id)]\n            results.append(ocr_info)\n        return results",
  "class Resize(object):\n    def __init__(self, size=(640, 640), **kwargs):\n        self.size = size\n\n    def resize_image(self, img):\n        resize_h, resize_w = self.size\n        ori_h, ori_w = img.shape[:2]  # (h, w, c)\n        ratio_h = float(resize_h) / ori_h\n        ratio_w = float(resize_w) / ori_w\n        img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        return img, [ratio_h, ratio_w]\n\n    def __call__(self, data):\n        img = data['image']\n        if 'polys' in data:\n            text_polys = data['polys']\n\n        img_resize, [ratio_h, ratio_w] = self.resize_image(img)\n        if 'polys' in data:\n            new_boxes = []\n            for box in text_polys:\n                new_box = []\n                for cord in box:\n                    new_box.append([cord[0] * ratio_w, cord[1] * ratio_h])\n                new_boxes.append(new_box)\n            data['polys'] = np.array(new_boxes, dtype=np.float32)\n        data['image'] = img_resize\n        return data",
  "class NormalizeImage(object):\n    \"\"\" normalize image such as substract mean, divide std\n    \"\"\"\n\n    def __init__(self, scale=None, mean=None, std=None, order='chw', **kwargs):\n        if isinstance(scale, str):\n            scale = eval(scale)\n        self.scale = np.float32(scale if scale is not None else 1.0 / 255.0)\n        mean = mean if mean is not None else [0.485, 0.456, 0.406]\n        std = std if std is not None else [0.229, 0.224, 0.225]\n\n        shape = (3, 1, 1) if order == 'chw' else (1, 1, 3)\n        self.mean = np.array(mean).reshape(shape).astype('float32')\n        self.std = np.array(std).reshape(shape).astype('float32')\n\n    def __call__(self, data):\n        img = data['image']\n        from PIL import Image\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        assert isinstance(img,\n                          np.ndarray), \"invalid input 'img' in NormalizeImage\"\n        data['image'] = (\n            img.astype('float32') * self.scale - self.mean) / self.std\n        return data",
  "class ToCHWImage(object):\n    \"\"\" convert hwc image to chw image\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, data):\n        img = data['image']\n        from PIL import Image\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data",
  "class KeepKeys(object):\n    def __init__(self, keep_keys, **kwargs):\n        self.keep_keys = keep_keys\n\n    def __call__(self, data):\n        data_list = []\n        for key in self.keep_keys:\n            data_list.append(data[key])\n        return data_list",
  "def __init__(self, size=(640, 640), **kwargs):\n        self.size = size",
  "def resize_image(self, img):\n        resize_h, resize_w = self.size\n        ori_h, ori_w = img.shape[:2]  # (h, w, c)\n        ratio_h = float(resize_h) / ori_h\n        ratio_w = float(resize_w) / ori_w\n        img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        return img, [ratio_h, ratio_w]",
  "def __call__(self, data):\n        img = data['image']\n        if 'polys' in data:\n            text_polys = data['polys']\n\n        img_resize, [ratio_h, ratio_w] = self.resize_image(img)\n        if 'polys' in data:\n            new_boxes = []\n            for box in text_polys:\n                new_box = []\n                for cord in box:\n                    new_box.append([cord[0] * ratio_w, cord[1] * ratio_h])\n                new_boxes.append(new_box)\n            data['polys'] = np.array(new_boxes, dtype=np.float32)\n        data['image'] = img_resize\n        return data",
  "def __init__(self, scale=None, mean=None, std=None, order='chw', **kwargs):\n        if isinstance(scale, str):\n            scale = eval(scale)\n        self.scale = np.float32(scale if scale is not None else 1.0 / 255.0)\n        mean = mean if mean is not None else [0.485, 0.456, 0.406]\n        std = std if std is not None else [0.229, 0.224, 0.225]\n\n        shape = (3, 1, 1) if order == 'chw' else (1, 1, 3)\n        self.mean = np.array(mean).reshape(shape).astype('float32')\n        self.std = np.array(std).reshape(shape).astype('float32')",
  "def __call__(self, data):\n        img = data['image']\n        from PIL import Image\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        assert isinstance(img,\n                          np.ndarray), \"invalid input 'img' in NormalizeImage\"\n        data['image'] = (\n            img.astype('float32') * self.scale - self.mean) / self.std\n        return data",
  "def __init__(self, **kwargs):\n        pass",
  "def __call__(self, data):\n        img = data['image']\n        from PIL import Image\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data",
  "def __init__(self, keep_keys, **kwargs):\n        self.keep_keys = keep_keys",
  "def __call__(self, data):\n        data_list = []\n        for key in self.keep_keys:\n            data_list.append(data[key])\n        return data_list",
  "def transform(data, ops=None):\n    \"\"\" transform \"\"\"\n    if ops is None:\n        ops = []\n    for op in ops:\n        data = op(data)\n        if data is None:\n            return None\n    return data",
  "def create_operators(op_param_list, global_config=None):\n    \"\"\"\n    create operators based on the config\n\n    Args:\n        params(list): a dict list, used to create some operators\n    \"\"\"\n    assert isinstance(op_param_list, list), (\n        'operator config should be a list')\n    ops = []\n    for operator in op_param_list:\n        assert isinstance(operator,\n                          dict) and len(operator) == 1, \"yaml format error\"\n        op_name = list(operator)[0]\n        param = {} if operator[op_name] is None else operator[op_name]\n        if global_config is not None:\n            param.update(global_config)\n        op = eval(op_name)(**param)\n        ops.append(op)\n    return ops",
  "class Yolov7FacePreprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for Yolov7Face\n        \"\"\"\n        self._preprocessor = C.vision.facedet.Yolov7Preprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for Yolov7Face\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)\n\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size\n\n    @property\n    def padding_color_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_color_value\n\n    @property\n    def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh\n\n    @padding_color_value.setter\n    def padding_color_value(self, value):\n        assert isinstance(\n            value, list\n        ), \"The value to set `padding_color_value` must be type of list.\"\n        self._preprocessor.padding_color_value = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value",
  "class Yolov7FacePostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for Yolov7Face\n        \"\"\"\n        self._postprocessor = C.vision.facedet.Yolov7FacePostprocessor()\n\n    def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for Yolov7Face\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)\n\n    @property\n    def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.conf_threshold\n\n    @property\n    def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.45\n        \"\"\"\n        return self._postprocessor.nms_threshold\n\n    @property\n    def landmarks_per_face(self):\n        \"\"\"\n        landmarks per face for postprocessing, default is 5\n        \"\"\"\n        return self._postprocessor.landmarks_per_face\n\n    @conf_threshold.setter\n    def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold\n\n    @nms_threshold.setter\n    def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold\n\n    @landmarks_per_face.setter\n    def landmarks_per_face(self, landmarks_per_face):\n        assert isinstance(landmarks_per_face, int),\\\n            \"The value to set `landmarks_per_face` must be type of int.\"\n        self._postprocessor.landmarks_per_face = landmarks_per_face",
  "class YOLOv7Face(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv7Face model exported by YOLOv7Face.\n\n        :param model_file: (str)Path of model file, e.g ./yolov7face.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(YOLOv7Face, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.YOLOv7Face(\n            model_file, params_file, self._runtime_option, model_format)\n\n        assert self.initialized, \"YOLOv7Face initialize failed.\"\n\n    def predict(self, input_image):\n         \"\"\"Detect the location and key points of human faces from an input image\n         :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n         :return: FaceDetectionResult\n         \"\"\"\n         return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get YOLOv7Preprocessor object of the loaded model\n\n        :return YOLOv7Preprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get YOLOv7Postprocessor object of the loaded model\n\n        :return YOLOv7Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self):\n        \"\"\"Create a preprocessor for Yolov7Face\n        \"\"\"\n        self._preprocessor = C.vision.facedet.Yolov7Preprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for Yolov7Face\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size",
  "def padding_color_value(self):\n        \"\"\"\n        padding value for preprocessing, default [114.0, 114.0, 114.0]\n        \"\"\"\n        #  padding value, size should be the same as channels\n        return self._preprocessor.padding_color_value",
  "def is_scale_up(self):\n        \"\"\"\n        is_scale_up for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_up",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh",
  "def padding_color_value(self, value):\n        assert isinstance(\n            value, list\n        ), \"The value to set `padding_color_value` must be type of list.\"\n        self._preprocessor.padding_color_value = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._preprocessor.is_scale_up = value",
  "def __init__(self):\n        \"\"\"Create a postprocessor for Yolov7Face\n        \"\"\"\n        self._postprocessor = C.vision.facedet.Yolov7FacePostprocessor()",
  "def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for Yolov7Face\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)",
  "def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.conf_threshold",
  "def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.45\n        \"\"\"\n        return self._postprocessor.nms_threshold",
  "def landmarks_per_face(self):\n        \"\"\"\n        landmarks per face for postprocessing, default is 5\n        \"\"\"\n        return self._postprocessor.landmarks_per_face",
  "def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold",
  "def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "def landmarks_per_face(self, landmarks_per_face):\n        assert isinstance(landmarks_per_face, int),\\\n            \"The value to set `landmarks_per_face` must be type of int.\"\n        self._postprocessor.landmarks_per_face = landmarks_per_face",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv7Face model exported by YOLOv7Face.\n\n        :param model_file: (str)Path of model file, e.g ./yolov7face.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(YOLOv7Face, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.YOLOv7Face(\n            model_file, params_file, self._runtime_option, model_format)\n\n        assert self.initialized, \"YOLOv7Face initialize failed.\"",
  "def predict(self, input_image):\n         \"\"\"Detect the location and key points of human faces from an input image\n         :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n         :return: FaceDetectionResult\n         \"\"\"\n         return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get YOLOv7Preprocessor object of the loaded model\n\n        :return YOLOv7Preprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get YOLOv7Postprocessor object of the loaded model\n\n        :return YOLOv7Postprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class BlazeFacePreprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for BlazeFace\n        \"\"\"\n        self._preprocessor = C.vision.facedet.BlazeFacePreprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for BlazeFace\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)\n\n    @property\n    def is_scale_(self):\n        \"\"\"\n        is_scale_ for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_\n\n    @is_scale_.setter\n    def is_scale_(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_` must be type of bool.\"\n        self._preprocessor.is_scale_ = value",
  "class BlazeFacePostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for BlazeFace\n        \"\"\"\n        self._postprocessor = C.vision.facedet.BlazeFacePostprocessor()\n\n    def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for BlazeFace\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)\n\n    @property\n    def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.conf_threshold\n\n    @property\n    def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.3\n        \"\"\"\n        return self._postprocessor.nms_threshold\n\n    @conf_threshold.setter\n    def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold\n\n    @nms_threshold.setter\n    def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "class BlazeFace(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 config_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a BlazeFace model exported by BlazeFace.\n\n        :param model_file: (str)Path of model file, e.g ./Blazeface.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(BlazeFace, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.BlazeFace(\n            model_file, params_file, config_file, self._runtime_option, model_format)\n\n        assert self.initialized, \"BlazeFace initialize failed.\"\n\n    def predict(self, input_image):\n         \"\"\"Detect the location and key points of human faces from an input image\n         :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n         :return: FaceDetectionResult\n         \"\"\"\n         return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of FaceDetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get BlazefacePreprocessor object of the loaded model\n\n        :return BlazefacePreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get BlazefacePostprocessor object of the loaded model\n\n        :return BlazefacePostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self):\n        \"\"\"Create a preprocessor for BlazeFace\n        \"\"\"\n        self._preprocessor = C.vision.facedet.BlazeFacePreprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for BlazeFace\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def is_scale_(self):\n        \"\"\"\n        is_scale_ for preprocessing, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0, default true\n        \"\"\"\n        return self._preprocessor.is_scale_",
  "def is_scale_(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_` must be type of bool.\"\n        self._preprocessor.is_scale_ = value",
  "def __init__(self):\n        \"\"\"Create a postprocessor for BlazeFace\n        \"\"\"\n        self._postprocessor = C.vision.facedet.BlazeFacePostprocessor()",
  "def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for BlazeFace\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)",
  "def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.conf_threshold",
  "def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.3\n        \"\"\"\n        return self._postprocessor.nms_threshold",
  "def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold",
  "def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 config_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load a BlazeFace model exported by BlazeFace.\n\n        :param model_file: (str)Path of model file, e.g ./Blazeface.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(BlazeFace, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.BlazeFace(\n            model_file, params_file, config_file, self._runtime_option, model_format)\n\n        assert self.initialized, \"BlazeFace initialize failed.\"",
  "def predict(self, input_image):\n         \"\"\"Detect the location and key points of human faces from an input image\n         :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n         :return: FaceDetectionResult\n         \"\"\"\n         return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of FaceDetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get BlazefacePreprocessor object of the loaded model\n\n        :return BlazefacePreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get BlazefacePostprocessor object of the loaded model\n\n        :return BlazefacePostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class UltraFace(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a UltraFace model exported by UltraFace.\n\n        :param model_file: (str)Path of model file, e.g ./ultraface.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(UltraFace, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.UltraFace(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"UltraFace initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.7, nms_iou_threshold=0.3):\n        \"\"\"Detect the location and key points of human faces from an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.7\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.3\n        :return: FaceDetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)\n\n    # \u4e00\u4e9b\u8ddfUltraFace\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [640, 480]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default (320, 240)\n        \"\"\"\n        return self._model.size\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a UltraFace model exported by UltraFace.\n\n        :param model_file: (str)Path of model file, e.g ./ultraface.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(UltraFace, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.UltraFace(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"UltraFace initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.7, nms_iou_threshold=0.3):\n        \"\"\"Detect the location and key points of human faces from an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.7\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.3\n        :return: FaceDetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default (320, 240)\n        \"\"\"\n        return self._model.size",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "class CenterFacePreprocessor:\n    def __init__(self):\n        \"\"\"Create a preprocessor for CenterFace\n        \"\"\"\n        self._preprocessor = C.vision.facedet.CenterFacePreprocessor()\n\n    def run(self, input_ims):\n        \"\"\"Preprocess input images for CenterFace\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)\n\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh",
  "class CenterFacePostprocessor:\n    def __init__(self):\n        \"\"\"Create a postprocessor for CenterFace\n        \"\"\"\n        self._postprocessor = C.vision.facedet.CenterFacePostprocessor()\n\n    def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for CenterFace\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)\n\n    @property\n    def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.conf_threshold\n\n    @property\n    def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.3\n        \"\"\"\n        return self._postprocessor.nms_threshold\n\n    @conf_threshold.setter\n    def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold\n\n    @nms_threshold.setter\n    def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "class CenterFace(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a CenterFace model exported by CenterFace.\n\n        :param model_file: (str)Path of model file, e.g ./CenterFace.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(CenterFace, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.CenterFace(\n            model_file, params_file, self._runtime_option, model_format)\n\n        assert self.initialized, \"CenterFace initialize failed.\"\n\n    def predict(self, input_image):\n         \"\"\"Detect the location and key points of human faces from an input image\n         :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n         :return: FaceDetectionResult\n         \"\"\"\n         return self._model.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)\n\n    @property\n    def preprocessor(self):\n        \"\"\"Get CenterFacePreprocessor object of the loaded model\n\n        :return CenterFacePreprocessor\n        \"\"\"\n        return self._model.preprocessor\n\n    @property\n    def postprocessor(self):\n        \"\"\"Get CenterFacePostprocessor object of the loaded model\n\n        :return CenterFacePostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "def __init__(self):\n        \"\"\"Create a preprocessor for CenterFace\n        \"\"\"\n        self._preprocessor = C.vision.facedet.CenterFacePreprocessor()",
  "def run(self, input_ims):\n        \"\"\"Preprocess input images for CenterFace\n\n        :param: input_ims: (list of numpy.ndarray)The input image\n        :return: list of FDTensor\n        \"\"\"\n        return self._preprocessor.run(input_ims)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640, 640]\n        \"\"\"\n        return self._preprocessor.size",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._preprocessor.size = wh",
  "def __init__(self):\n        \"\"\"Create a postprocessor for CenterFace\n        \"\"\"\n        self._postprocessor = C.vision.facedet.CenterFacePostprocessor()",
  "def run(self, runtime_results, ims_info):\n        \"\"\"Postprocess the runtime results for CenterFace\n\n        :param: runtime_results: (list of FDTensor)The output FDTensor results from runtime\n        :param: ims_info: (list of dict)Record input_shape and output_shape\n        :return: list of DetectionResult(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\n        \"\"\"\n        return self._postprocessor.run(runtime_results, ims_info)",
  "def conf_threshold(self):\n        \"\"\"\n        confidence threshold for postprocessing, default is 0.5\n        \"\"\"\n        return self._postprocessor.conf_threshold",
  "def nms_threshold(self):\n        \"\"\"\n        nms threshold for postprocessing, default is 0.3\n        \"\"\"\n        return self._postprocessor.nms_threshold",
  "def conf_threshold(self, conf_threshold):\n        assert isinstance(conf_threshold, float),\\\n            \"The value to set `conf_threshold` must be type of float.\"\n        self._postprocessor.conf_threshold = conf_threshold",
  "def nms_threshold(self, nms_threshold):\n        assert isinstance(nms_threshold, float),\\\n            \"The value to set `nms_threshold` must be type of float.\"\n        self._postprocessor.nms_threshold = nms_threshold",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a CenterFace model exported by CenterFace.\n\n        :param model_file: (str)Path of model file, e.g ./CenterFace.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        super(CenterFace, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.CenterFace(\n            model_file, params_file, self._runtime_option, model_format)\n\n        assert self.initialized, \"CenterFace initialize failed.\"",
  "def predict(self, input_image):\n         \"\"\"Detect the location and key points of human faces from an input image\n         :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n         :return: FaceDetectionResult\n         \"\"\"\n         return self._model.predict(input_image)",
  "def batch_predict(self, images):\n        \"\"\"Classify a batch of input image\n\n        :param im: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return list of DetectionResult\n        \"\"\"\n\n        return self._model.batch_predict(images)",
  "def preprocessor(self):\n        \"\"\"Get CenterFacePreprocessor object of the loaded model\n\n        :return CenterFacePreprocessor\n        \"\"\"\n        return self._model.preprocessor",
  "def postprocessor(self):\n        \"\"\"Get CenterFacePostprocessor object of the loaded model\n\n        :return CenterFacePostprocessor\n        \"\"\"\n        return self._model.postprocessor",
  "class RetinaFace(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a RetinaFace model exported by RetinaFace.\n\n        :param model_file: (str)Path of model file, e.g ./retinaface.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(RetinaFace, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.RetinaFace(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"RetinaFace initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.7, nms_iou_threshold=0.3):\n        \"\"\"Detect the location and key points of human faces from an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.7\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.3\n        :return: FaceDetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)\n\n    # \u4e00\u4e9b\u8ddf\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [640, 480]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default (640, 640)\n        \"\"\"\n        return self._model.size\n\n    @property\n    def variance(self):\n        \"\"\"\n        Argument for image postprocessing step, variance in RetinaFace's prior-box(anchor) generate process, default (0.1, 0.2)\n        \"\"\"\n        return self._model.variance\n\n    @property\n    def downsample_strides(self):\n        \"\"\"\n        Argument for image postprocessing step, downsample strides (namely, steps) for RetinaFace to generate anchors, will take (8,16,32) as default values\n        \"\"\"\n        return self._model.downsample_strides\n\n    @property\n    def min_sizes(self):\n        \"\"\"\n        Argument for image postprocessing step, min sizes, width and height for each anchor, default min_sizes = [[16, 32], [64, 128], [256, 512]]\n        \"\"\"\n        return self._model.min_sizes\n\n    @property\n    def landmarks_per_face(self):\n        \"\"\"\n        Argument for image postprocessing step, landmarks_per_face, default 5 in RetinaFace\n        \"\"\"\n        return self._model.landmarks_per_face\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @variance.setter\n    def variance(self, value):\n        assert isinstance(v, (list, tuple)),\\\n            \"The value to set `variance` must be type of tuple or list.\"\n        assert len(value) == 2,\\\n            \"The value to set `variance` must contatins 2 elements\".format(\n            len(value))\n        self._model.variance = value\n\n    @downsample_strides.setter\n    def downsample_strides(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `downsample_strides` must be type of list.\"\n        self._model.downsample_strides = value\n\n    @min_sizes.setter\n    def min_sizes(self, value):\n        assert isinstance(\n            value, list), \"The value to set `min_sizes` must be type of list.\"\n        self._model.min_sizes = value\n\n    @landmarks_per_face.setter\n    def landmarks_per_face(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `landmarks_per_face` must be type of int.\"\n        self._model.landmarks_per_face = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a RetinaFace model exported by RetinaFace.\n\n        :param model_file: (str)Path of model file, e.g ./retinaface.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(RetinaFace, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.RetinaFace(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"RetinaFace initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.7, nms_iou_threshold=0.3):\n        \"\"\"Detect the location and key points of human faces from an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.7\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.3\n        :return: FaceDetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default (640, 640)\n        \"\"\"\n        return self._model.size",
  "def variance(self):\n        \"\"\"\n        Argument for image postprocessing step, variance in RetinaFace's prior-box(anchor) generate process, default (0.1, 0.2)\n        \"\"\"\n        return self._model.variance",
  "def downsample_strides(self):\n        \"\"\"\n        Argument for image postprocessing step, downsample strides (namely, steps) for RetinaFace to generate anchors, will take (8,16,32) as default values\n        \"\"\"\n        return self._model.downsample_strides",
  "def min_sizes(self):\n        \"\"\"\n        Argument for image postprocessing step, min sizes, width and height for each anchor, default min_sizes = [[16, 32], [64, 128], [256, 512]]\n        \"\"\"\n        return self._model.min_sizes",
  "def landmarks_per_face(self):\n        \"\"\"\n        Argument for image postprocessing step, landmarks_per_face, default 5 in RetinaFace\n        \"\"\"\n        return self._model.landmarks_per_face",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def variance(self, value):\n        assert isinstance(v, (list, tuple)),\\\n            \"The value to set `variance` must be type of tuple or list.\"\n        assert len(value) == 2,\\\n            \"The value to set `variance` must contatins 2 elements\".format(\n            len(value))\n        self._model.variance = value",
  "def downsample_strides(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `downsample_strides` must be type of list.\"\n        self._model.downsample_strides = value",
  "def min_sizes(self, value):\n        assert isinstance(\n            value, list), \"The value to set `min_sizes` must be type of list.\"\n        self._model.min_sizes = value",
  "def landmarks_per_face(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `landmarks_per_face` must be type of int.\"\n        self._model.landmarks_per_face = value",
  "class YOLOv5Face(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv5Face model exported by YOLOv5Face.\n\n        :param model_file: (str)Path of model file, e.g ./yolov5face.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv5Face, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.YOLOv5Face(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv5Face initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect the location and key points of human faces from an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: FaceDetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)\n\n    # \u4e00\u4e9b\u8ddfYOLOv5Face\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [1280, 1280]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640,640]\n        \"\"\"\n        return self._model.size\n\n    @property\n    def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value\n\n    @property\n    def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad\n\n    @property\n    def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad\n\n    @property\n    def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up\n\n    @property\n    def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride\n\n    @property\n    def landmarks_per_face(self):\n        \"\"\"\n        Argument for image postprocessing step, landmarks_per_face, default 5 in YOLOv5Face\n        \"\"\"\n        return self._model.landmarks_per_face\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value\n\n    @is_no_pad.setter\n    def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value\n\n    @is_mini_pad.setter\n    def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value\n\n    @stride.setter\n    def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value\n\n    @landmarks_per_face.setter\n    def landmarks_per_face(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `landmarks_per_face` must be type of int.\"\n        self._model.landmarks_per_face = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a YOLOv5Face model exported by YOLOv5Face.\n\n        :param model_file: (str)Path of model file, e.g ./yolov5face.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(YOLOv5Face, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.YOLOv5Face(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"YOLOv5Face initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.25, nms_iou_threshold=0.5):\n        \"\"\"Detect the location and key points of human faces from an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.25\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.5\n        :return: FaceDetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default size = [640,640]\n        \"\"\"\n        return self._model.size",
  "def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value",
  "def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad",
  "def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad",
  "def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up",
  "def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride",
  "def landmarks_per_face(self):\n        \"\"\"\n        Argument for image postprocessing step, landmarks_per_face, default 5 in YOLOv5Face\n        \"\"\"\n        return self._model.landmarks_per_face",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value",
  "def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value",
  "def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value",
  "def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value",
  "def landmarks_per_face(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `landmarks_per_face` must be type of int.\"\n        self._model.landmarks_per_face = value",
  "class SCRFD(FastDeployModel):\n    def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a SCRFD model exported by SCRFD.\n\n        :param model_file: (str)Path of model file, e.g ./scrfd.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(SCRFD, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.SCRFD(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"SCRFD initialize failed.\"\n\n    def predict(self, input_image, conf_threshold=0.7, nms_iou_threshold=0.3):\n        \"\"\"Detect the location and key points of human faces from an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.7\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.3\n        :return: FaceDetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)\n\n    def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._model.disable_normalize()\n\n    def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._model.disable_permute()\n\n    # \u4e00\u4e9b\u8ddfSCRFD\u6a21\u578b\u6709\u5173\u7684\u5c5e\u6027\u5c01\u88c5\n    # \u591a\u6570\u662f\u9884\u5904\u7406\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u5982model.size = [640, 640]\u6539\u53d8\u9884\u5904\u7406\u65f6resize\u7684\u5927\u5c0f\uff08\u524d\u63d0\u662f\u6a21\u578b\u652f\u6301\uff09\n    @property\n    def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default (640, 640)\n        \"\"\"\n        return self._model.size\n\n    @property\n    def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value\n\n    @property\n    def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad\n\n    @property\n    def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad\n\n    @property\n    def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up\n\n    @property\n    def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride\n\n    @property\n    def downsample_strides(self):\n        \"\"\"\n        Argument for image postprocessing step,\n        downsample strides (namely, steps) for SCRFD to generate anchors,\n        will take (8,16,32) as default values\n        \"\"\"\n        return self._model.downsample_strides\n\n    @property\n    def landmarks_per_face(self):\n        \"\"\"\n        Argument for image postprocessing step, landmarks_per_face, default 5 in SCRFD\n        \"\"\"\n        return self._model.landmarks_per_face\n\n    @property\n    def use_kps(self):\n        \"\"\"\n        Argument for image postprocessing step,\n        the outputs of onnx file with key points features or not, default true\n        \"\"\"\n        return self._model.use_kps\n\n    @property\n    def max_nms(self):\n        \"\"\"\n        Argument for image postprocessing step, the upperbond number of boxes processed by nms, default 30000\n        \"\"\"\n        return self._model.max_nms\n\n    @property\n    def num_anchors(self):\n        \"\"\"\n        Argument for image postprocessing step, anchor number of each stride, default 2\n        \"\"\"\n        return self._model.num_anchors\n\n    @size.setter\n    def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh\n\n    @padding_value.setter\n    def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value\n\n    @is_no_pad.setter\n    def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value\n\n    @is_mini_pad.setter\n    def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value\n\n    @is_scale_up.setter\n    def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value\n\n    @stride.setter\n    def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value\n\n    @downsample_strides.setter\n    def downsample_strides(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `downsample_strides` must be type of list.\"\n        self._model.downsample_strides = value\n\n    @landmarks_per_face.setter\n    def landmarks_per_face(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `landmarks_per_face` must be type of int.\"\n        self._model.landmarks_per_face = value\n\n    @use_kps.setter\n    def use_kps(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `use_kps` must be type of bool.\"\n        self._model.use_kps = value\n\n    @max_nms.setter\n    def max_nms(self, value):\n        assert isinstance(\n            value, int), \"The value to set `max_nms` must be type of int.\"\n        self._model.max_nms = value\n\n    @num_anchors.setter\n    def num_anchors(self, value):\n        assert isinstance(\n            value, int), \"The value to set `num_anchors` must be type of int.\"\n        self._model.num_anchors = value",
  "def __init__(self,\n                 model_file,\n                 params_file=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.ONNX):\n        \"\"\"Load a SCRFD model exported by SCRFD.\n\n        :param model_file: (str)Path of model file, e.g ./scrfd.onnx\n        :param params_file: (str)Path of parameters file, e.g yolox/model.pdiparams, if the model_fomat is ModelFormat.ONNX, this param will be ignored, can be set as empty string\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model\n        \"\"\"\n        # \u8c03\u7528\u57fa\u51fd\u6570\u8fdb\u884cbackend_option\u7684\u521d\u59cb\u5316\n        # \u521d\u59cb\u5316\u540e\u7684option\u4fdd\u5b58\u5728self._runtime_option\n        super(SCRFD, self).__init__(runtime_option)\n\n        self._model = C.vision.facedet.SCRFD(\n            model_file, params_file, self._runtime_option, model_format)\n        # \u901a\u8fc7self.initialized\u5224\u65ad\u6574\u4e2a\u6a21\u578b\u7684\u521d\u59cb\u5316\u662f\u5426\u6210\u529f\n        assert self.initialized, \"SCRFD initialize failed.\"",
  "def predict(self, input_image, conf_threshold=0.7, nms_iou_threshold=0.3):\n        \"\"\"Detect the location and key points of human faces from an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :param conf_threshold: confidence threashold for postprocessing, default is 0.7\n        :param nms_iou_threshold: iou threashold for NMS, default is 0.3\n        :return: FaceDetectionResult\n        \"\"\"\n        return self._model.predict(input_image, conf_threshold,\n                                   nms_iou_threshold)",
  "def disable_normalize(self):\n        \"\"\"\n        This function will disable normalize in preprocessing step.\n        \"\"\"\n        self._model.disable_normalize()",
  "def disable_permute(self):\n        \"\"\"\n        This function will disable hwc2chw in preprocessing step.\n        \"\"\"\n        self._model.disable_permute()",
  "def size(self):\n        \"\"\"\n        Argument for image preprocessing step, the preprocess image size, tuple of (width, height), default (640, 640)\n        \"\"\"\n        return self._model.size",
  "def padding_value(self):\n        #  padding value, size should be the same as channels\n        return self._model.padding_value",
  "def is_no_pad(self):\n        # while is_mini_pad = false and is_no_pad = true, will resize the image to the set size\n        return self._model.is_no_pad",
  "def is_mini_pad(self):\n        # only pad to the minimum rectange which height and width is times of stride\n        return self._model.is_mini_pad",
  "def is_scale_up(self):\n        # if is_scale_up is false, the input image only can be zoom out, the maximum resize scale cannot exceed 1.0\n        return self._model.is_scale_up",
  "def stride(self):\n        # padding stride, for is_mini_pad\n        return self._model.stride",
  "def downsample_strides(self):\n        \"\"\"\n        Argument for image postprocessing step,\n        downsample strides (namely, steps) for SCRFD to generate anchors,\n        will take (8,16,32) as default values\n        \"\"\"\n        return self._model.downsample_strides",
  "def landmarks_per_face(self):\n        \"\"\"\n        Argument for image postprocessing step, landmarks_per_face, default 5 in SCRFD\n        \"\"\"\n        return self._model.landmarks_per_face",
  "def use_kps(self):\n        \"\"\"\n        Argument for image postprocessing step,\n        the outputs of onnx file with key points features or not, default true\n        \"\"\"\n        return self._model.use_kps",
  "def max_nms(self):\n        \"\"\"\n        Argument for image postprocessing step, the upperbond number of boxes processed by nms, default 30000\n        \"\"\"\n        return self._model.max_nms",
  "def num_anchors(self):\n        \"\"\"\n        Argument for image postprocessing step, anchor number of each stride, default 2\n        \"\"\"\n        return self._model.num_anchors",
  "def size(self, wh):\n        assert isinstance(wh, (list, tuple)),\\\n            \"The value to set `size` must be type of tuple or list.\"\n        assert len(wh) == 2,\\\n            \"The value to set `size` must contatins 2 elements means [width, height], but now it contains {} elements.\".format(\n            len(wh))\n        self._model.size = wh",
  "def padding_value(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `padding_value` must be type of list.\"\n        self._model.padding_value = value",
  "def is_no_pad(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `is_no_pad` must be type of bool.\"\n        self._model.is_no_pad = value",
  "def is_mini_pad(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_mini_pad` must be type of bool.\"\n        self._model.is_mini_pad = value",
  "def is_scale_up(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `is_scale_up` must be type of bool.\"\n        self._model.is_scale_up = value",
  "def stride(self, value):\n        assert isinstance(\n            value, int), \"The value to set `stride` must be type of int.\"\n        self._model.stride = value",
  "def downsample_strides(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `downsample_strides` must be type of list.\"\n        self._model.downsample_strides = value",
  "def landmarks_per_face(self, value):\n        assert isinstance(\n            value,\n            int), \"The value to set `landmarks_per_face` must be type of int.\"\n        self._model.landmarks_per_face = value",
  "def use_kps(self, value):\n        assert isinstance(\n            value, bool), \"The value to set `use_kps` must be type of bool.\"\n        self._model.use_kps = value",
  "def max_nms(self, value):\n        assert isinstance(\n            value, int), \"The value to set `max_nms` must be type of int.\"\n        self._model.max_nms = value",
  "def num_anchors(self, value):\n        assert isinstance(\n            value, int), \"The value to set `num_anchors` must be type of int.\"\n        self._model.num_anchors = value",
  "def generate_key():\n    \"\"\"generate a key for encryption\n    :return: key(str)\n    \"\"\"\n    return C.encryption.generate_key()",
  "def encrypt(input, key=None):\n    \"\"\"Encrypt a input string with key.\n    :param: input: (str) The input str for encryption\n    :param: key: (str,optional) The key for encryption(if not given, generate automatically.)\n    :return: pair(str, str) [encrypted string, key]\n    \"\"\"\n    if key is None:\n        key = generate_key()\n    return C.encryption.encrypt(input, key)",
  "def decrypt(cipher, key):\n    \"\"\"Decrypt a input cipher with key.\n    :param: cipher: (str) The input str for decryption\n    :param: key: (str) The key for decryption\n    :return: str(The decrypted str)\n    \"\"\"\n    return C.encryption.decrypt(cipher, key)",
  "def lock_predictor(lock):\n    lock.acquire()\n    try:\n        yield\n    finally:\n        lock.release()",
  "def cv2_to_base64(image):\n    data = cv2.imencode('.jpg', image)[1]\n    return base64.b64encode(data.tobytes()).decode('utf8')",
  "def base64_to_cv2(b64str):\n    data = base64.b64decode(b64str.encode('utf8'))\n    data = np.fromstring(data, np.uint8)\n    data = cv2.imdecode(data, cv2.IMREAD_COLOR)\n    return data",
  "class SimpleServer(FastAPI):\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initial function for the FastDeploy SimpleServer.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._router_manager = HttpRouterManager(self)\n        self._model_manager = None\n        self._service_name = \"FastDeploy SimpleServer\"\n        self._service_type = None\n\n    def register(self, task_name, model_handler, predictor):\n        \"\"\"\n        The register function for the SimpleServer, the main register argrument as follows:\n\n        Args:\n            task_name(str): API URL path.\n            model_handler: To process request data, run predictor,\n                and can also add your custom post processing on top of the predictor result\n            predictor: To run model predict\n        \"\"\"\n        self._server_type = \"models\"\n        model_manager = ModelManager(model_handler, predictor)\n        self._model_manager = model_manager\n        # Register model server router\n        self._router_manager.register_models_router(task_name)",
  "def __init__(self, **kwargs):\n        \"\"\"\n        Initial function for the FastDeploy SimpleServer.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._router_manager = HttpRouterManager(self)\n        self._model_manager = None\n        self._service_name = \"FastDeploy SimpleServer\"\n        self._service_type = None",
  "def register(self, task_name, model_handler, predictor):\n        \"\"\"\n        The register function for the SimpleServer, the main register argrument as follows:\n\n        Args:\n            task_name(str): API URL path.\n            model_handler: To process request data, run predictor,\n                and can also add your custom post processing on top of the predictor result\n            predictor: To run model predict\n        \"\"\"\n        self._server_type = \"models\"\n        model_manager = ModelManager(model_handler, predictor)\n        self._model_manager = model_manager\n        # Register model server router\n        self._router_manager.register_models_router(task_name)",
  "class ModelManager:\n    def __init__(self, model_handler, predictor):\n        self._model_handler = model_handler\n        self._predictors = []\n        self._predictor_locks = []\n        self._register(predictor)\n\n    def _register(self, predictor):\n        # Get the model handler\n        if not issubclass(self._model_handler, BaseModelHandler):\n            raise TypeError(\n                \"The model_handler must be subclass of BaseModelHandler, please check the type.\"\n            )\n\n        # TODO: Create multiple predictors to run on different GPUs or different CPU threads\n        self._predictors.append(predictor)\n        self._predictor_locks.append(threading.Lock())\n\n    def _get_predict_id(self):\n        t = time.time()\n        t = int(round(t * 1000))\n        predictor_id = t % len(self._predictors)\n        logging.info(\"The predictor id: {} is selected by running the model.\".\n                     format(predictor_id))\n        return predictor_id\n\n    def predict(self, data, parameters):\n        predictor_id = self._get_predict_id()\n        with lock_predictor(self._predictor_locks[predictor_id]):\n            model_output = self._model_handler.process(\n                self._predictors[predictor_id], data, parameters)\n            return model_output",
  "def __init__(self, model_handler, predictor):\n        self._model_handler = model_handler\n        self._predictors = []\n        self._predictor_locks = []\n        self._register(predictor)",
  "def _register(self, predictor):\n        # Get the model handler\n        if not issubclass(self._model_handler, BaseModelHandler):\n            raise TypeError(\n                \"The model_handler must be subclass of BaseModelHandler, please check the type.\"\n            )\n\n        # TODO: Create multiple predictors to run on different GPUs or different CPU threads\n        self._predictors.append(predictor)\n        self._predictor_locks.append(threading.Lock())",
  "def _get_predict_id(self):\n        t = time.time()\n        t = int(round(t * 1000))\n        predictor_id = t % len(self._predictors)\n        logging.info(\"The predictor id: {} is selected by running the model.\".\n                     format(predictor_id))\n        return predictor_id",
  "def predict(self, data, parameters):\n        predictor_id = self._get_predict_id()\n        with lock_predictor(self._predictor_locks[predictor_id]):\n            model_output = self._model_handler.process(\n                self._predictors[predictor_id], data, parameters)\n            return model_output",
  "class BaseModelHandler(metaclass=ABCMeta):\n    def __init__(self):\n        super().__init__()\n\n    @classmethod\n    @abstractmethod\n    def process(cls, predictor, data, parameters):\n        pass",
  "def __init__(self):\n        super().__init__()",
  "def process(cls, predictor, data, parameters):\n        pass",
  "class VisionModelHandler(BaseModelHandler):\n    def __init__(self):\n        super().__init__()\n\n    @classmethod\n    def process(cls, predictor, data, parameters):\n        # TODO: support batch predict\n        im = base64_to_cv2(data['image'])\n        result = predictor.predict(im)\n        r_str = fd_result_to_json(result)\n        return r_str",
  "def __init__(self):\n        super().__init__()",
  "def process(cls, predictor, data, parameters):\n        # TODO: support batch predict\n        im = base64_to_cv2(data['image'])\n        result = predictor.predict(im)\n        r_str = fd_result_to_json(result)\n        return r_str",
  "class ResponseBase(BaseModel):\n    text: Optional[str] = None",
  "class RequestBase(BaseModel, extra=Extra.forbid):\n    parameters: Optional[dict] = {}",
  "class HttpRouterManager(BaseRouterManager):\n    def register_models_router(self, task_name):\n\n        # Url path to register the model\n        paths = [f\"/{task_name}\"]\n        for path in paths:\n            logging.info(\"FastDeploy Model request [path]={} is genereated.\".\n                         format(path))\n\n        # Unique name to create the pydantic model\n        unique_name = hashlib.md5(task_name.encode()).hexdigest()\n\n        # Create request model\n        req_model = create_model(\n            \"RequestModel\" + unique_name,\n            data=(typing.Any, ...),\n            __base__=RequestBase, )\n\n        # Create response model\n        resp_model = create_model(\n            \"ResponseModel\" + unique_name,\n            result=(typing.Any, ...),\n            __base__=ResponseBase, )\n\n        # Template predict endpoint function to dynamically serve different models\n        def predict(request: Request, inference_request: req_model):\n            try:\n                result = self._app._model_manager.predict(\n                    inference_request.data, inference_request.parameters)\n            except Exception as e:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Error occurred while running predict: {str(e)}\")\n            return {\"result\": result}\n\n        # Register the route and add to the app\n        router = APIRouter()\n        for path in paths:\n            router.add_api_route(\n                path,\n                predict,\n                methods=[\"post\"],\n                summary=f\"{task_name.title()}\",\n                response_model=resp_model,\n                response_model_exclude_unset=True,\n                response_model_exclude_none=True, )\n        self._app.include_router(router)",
  "def register_models_router(self, task_name):\n\n        # Url path to register the model\n        paths = [f\"/{task_name}\"]\n        for path in paths:\n            logging.info(\"FastDeploy Model request [path]={} is genereated.\".\n                         format(path))\n\n        # Unique name to create the pydantic model\n        unique_name = hashlib.md5(task_name.encode()).hexdigest()\n\n        # Create request model\n        req_model = create_model(\n            \"RequestModel\" + unique_name,\n            data=(typing.Any, ...),\n            __base__=RequestBase, )\n\n        # Create response model\n        resp_model = create_model(\n            \"ResponseModel\" + unique_name,\n            result=(typing.Any, ...),\n            __base__=ResponseBase, )\n\n        # Template predict endpoint function to dynamically serve different models\n        def predict(request: Request, inference_request: req_model):\n            try:\n                result = self._app._model_manager.predict(\n                    inference_request.data, inference_request.parameters)\n            except Exception as e:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Error occurred while running predict: {str(e)}\")\n            return {\"result\": result}\n\n        # Register the route and add to the app\n        router = APIRouter()\n        for path in paths:\n            router.add_api_route(\n                path,\n                predict,\n                methods=[\"post\"],\n                summary=f\"{task_name.title()}\",\n                response_model=resp_model,\n                response_model_exclude_unset=True,\n                response_model_exclude_none=True, )\n        self._app.include_router(router)",
  "def predict(request: Request, inference_request: req_model):\n            try:\n                result = self._app._model_manager.predict(\n                    inference_request.data, inference_request.parameters)\n            except Exception as e:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Error occurred while running predict: {str(e)}\")\n            return {\"result\": result}",
  "class BaseRouterManager(abc.ABC):\n    _app = None\n\n    def __init__(self, app):\n        super().__init__()\n        self._app = app\n\n    @abc.abstractmethod\n    def register_models_router(self):\n        return NotImplemented",
  "def __init__(self, app):\n        super().__init__()\n        self._app = app",
  "def register_models_router(self):\n        return NotImplemented",
  "def _get_user_home():\n    return os.path.expanduser('~')",
  "def _get_hub_home():\n    if 'FASTDEPLOY_HUB_HOME' in os.environ:\n        home_path = os.environ['FASTDEPLOY_HUB_HOME']\n        if os.path.exists(home_path):\n            if os.path.isdir(home_path):\n                return home_path\n            else:\n                raise RuntimeError(\n                    'The environment variable FASTDEPLOY_HUB_HOME {} is not a directory.'.\n                    format(home_path))\n        else:\n            return home_path\n    return os.path.join(_get_user_home(), '.fastdeploy')",
  "def _get_sub_home(directory):\n    home = os.path.join(_get_hub_home(), directory)\n    os.makedirs(home, exist_ok=True)\n    return home",
  "def get_detection_test_image(path=None):\n    if path is None:\n        path = hub_env.RESOURCE_HOME\n    fullpath = fd.download(\n        url='https://bj.bcebos.com/paddlehub/fastdeploy/example/detection_test_image.jpg',\n        path=path)\n    return fullpath",
  "class ServerConnectionError(Exception):\n    def __init__(self, url: str):\n        self.url = url\n\n    def __str__(self):\n        tips = 'Can\\'t connect to FastDeploy Model Server: {}'.format(self.url)\n        return tips",
  "class ModelServer(object):\n    '''\n    FastDeploy server source\n\n    Args:\n        url(str) : Url of the server\n        timeout(int) : Request timeout\n    '''\n\n    def __init__(self, url: str, timeout: int=10):\n        self._url = url\n        self._timeout = timeout\n\n    def search_model(self, name: str, format: str=None,\n                     version: str=None) -> List[dict]:\n        '''\n        Search model from model server.\n\n        Args:\n            name(str) : FastDeploy model name\n            format(str): FastDeploy model format\n            version(str) : FastDeploy model version\n        Return:\n            result(list): search results\n        '''\n        params = {}\n        params['name'] = name\n        if format:\n            params['format'] = format\n        if version:\n            params['version'] = version\n        result = self.request(path='fastdeploy_search', params=params)\n        if result['status'] == 0 and len(result['data']) > 0:\n            return result['data']\n        return None\n\n    def stat_model(self, name: str, format: str, version: str):\n        '''\n        Note a record when download a model for statistics.\n\n        Args:\n            name(str) : FastDeploy model name\n            format(str): FastDeploy model format\n            version(str) : FastDeploy model version\n        Return:\n            is_successful(bool): True if successful, False otherwise\n        '''\n        params = {}\n        params['name'] = name\n        params['format'] = format\n        params['version'] = version\n        params['from'] = 'fastdeploy'\n        try:\n            result = self.request(path='stat', params=params)\n        except Exception:\n            return False\n        if result['status'] == 0:\n            return True\n        else:\n            return False\n\n    def request(self, path: str, params: dict) -> dict:\n        '''Request server.'''\n        api = '{}/{}'.format(self._url, path)\n        try:\n            result = requests.get(api, params, timeout=self._timeout)\n            return result.json()\n        except requests.exceptions.ConnectionError as e:\n            raise ServerConnectionError(self._url)\n\n    def get_model_list(self):\n        '''\n        Get all pre-trained models information in dataset.\n        Return:\n            result(dict): key is category name, value is a list which contains models \\\n                information such as name, format and version.\n        '''\n        api = '{}/{}'.format(self._url, 'fastdeploy_listmodels')\n        try:\n            result = requests.get(api, timeout=self._timeout)\n            return result.json()\n        except requests.exceptions.ConnectionError as e:\n            raise ServerConnectionError(self._url)\n\n    def is_connected(self):\n        return self.check(self._url)\n\n    @classmethod\n    def check(cls, url: str) -> bool:\n        '''\n        Check if the specified url is a valid model server\n\n        Args:\n            url(str) : Url to check\n        '''\n        try:\n            r = requests.get(url + '/search')\n            return r.status_code == 200\n        except:\n            return False",
  "def __init__(self, url: str):\n        self.url = url",
  "def __str__(self):\n        tips = 'Can\\'t connect to FastDeploy Model Server: {}'.format(self.url)\n        return tips",
  "def __init__(self, url: str, timeout: int=10):\n        self._url = url\n        self._timeout = timeout",
  "def search_model(self, name: str, format: str=None,\n                     version: str=None) -> List[dict]:\n        '''\n        Search model from model server.\n\n        Args:\n            name(str) : FastDeploy model name\n            format(str): FastDeploy model format\n            version(str) : FastDeploy model version\n        Return:\n            result(list): search results\n        '''\n        params = {}\n        params['name'] = name\n        if format:\n            params['format'] = format\n        if version:\n            params['version'] = version\n        result = self.request(path='fastdeploy_search', params=params)\n        if result['status'] == 0 and len(result['data']) > 0:\n            return result['data']\n        return None",
  "def stat_model(self, name: str, format: str, version: str):\n        '''\n        Note a record when download a model for statistics.\n\n        Args:\n            name(str) : FastDeploy model name\n            format(str): FastDeploy model format\n            version(str) : FastDeploy model version\n        Return:\n            is_successful(bool): True if successful, False otherwise\n        '''\n        params = {}\n        params['name'] = name\n        params['format'] = format\n        params['version'] = version\n        params['from'] = 'fastdeploy'\n        try:\n            result = self.request(path='stat', params=params)\n        except Exception:\n            return False\n        if result['status'] == 0:\n            return True\n        else:\n            return False",
  "def request(self, path: str, params: dict) -> dict:\n        '''Request server.'''\n        api = '{}/{}'.format(self._url, path)\n        try:\n            result = requests.get(api, params, timeout=self._timeout)\n            return result.json()\n        except requests.exceptions.ConnectionError as e:\n            raise ServerConnectionError(self._url)",
  "def get_model_list(self):\n        '''\n        Get all pre-trained models information in dataset.\n        Return:\n            result(dict): key is category name, value is a list which contains models \\\n                information such as name, format and version.\n        '''\n        api = '{}/{}'.format(self._url, 'fastdeploy_listmodels')\n        try:\n            result = requests.get(api, timeout=self._timeout)\n            return result.json()\n        except requests.exceptions.ConnectionError as e:\n            raise ServerConnectionError(self._url)",
  "def is_connected(self):\n        return self.check(self._url)",
  "def check(cls, url: str) -> bool:\n        '''\n        Check if the specified url is a valid model server\n\n        Args:\n            url(str) : Url to check\n        '''\n        try:\n            r = requests.get(url + '/search')\n            return r.status_code == 200\n        except:\n            return False",
  "class HubConfig:\n    '''\n    FastDeploy model management configuration class.\n    '''\n\n    def __init__(self):\n        self._initialize()\n        self.file = os.path.join(hubenv.CONF_HOME, 'config.yaml')\n\n        if not os.path.exists(self.file):\n            self.flush()\n            return\n\n        with open(self.file, 'r') as file:\n            try:\n                cfg = yaml.load(file, Loader=yaml.FullLoader)\n                self.data.update(cfg)\n            except:\n                ...\n\n    def _initialize(self):\n        # Set default configuration values.\n        self.data = {}\n        self.data['server'] = 'http://paddlepaddle.org.cn/paddlehub'\n\n    def reset(self):\n        '''Reset configuration to default.'''\n        self._initialize()\n        self.flush()\n\n    @property\n    def server(self):\n        '''Model server url.'''\n        return self.data['server']\n\n    @server.setter\n    def server(self, url: str):\n        self.data['server'] = url\n        self.flush()\n\n    def flush(self):\n        '''Flush the current configuration into the configuration file.'''\n        with open(self.file, 'w') as file:\n            cfg = json.loads(json.dumps(self.data))\n            yaml.dump(cfg, file)\n\n    def __str__(self):\n        cfg = json.loads(json.dumps(self.data))\n        return yaml.dump(cfg)",
  "def __init__(self):\n        self._initialize()\n        self.file = os.path.join(hubenv.CONF_HOME, 'config.yaml')\n\n        if not os.path.exists(self.file):\n            self.flush()\n            return\n\n        with open(self.file, 'r') as file:\n            try:\n                cfg = yaml.load(file, Loader=yaml.FullLoader)\n                self.data.update(cfg)\n            except:\n                ...",
  "def _initialize(self):\n        # Set default configuration values.\n        self.data = {}\n        self.data['server'] = 'http://paddlepaddle.org.cn/paddlehub'",
  "def reset(self):\n        '''Reset configuration to default.'''\n        self._initialize()\n        self.flush()",
  "def server(self):\n        '''Model server url.'''\n        return self.data['server']",
  "def server(self, url: str):\n        self.data['server'] = url\n        self.flush()",
  "def flush(self):\n        '''Flush the current configuration into the configuration file.'''\n        with open(self.file, 'w') as file:\n            cfg = json.loads(json.dumps(self.data))\n            yaml.dump(cfg, file)",
  "def __str__(self):\n        cfg = json.loads(json.dumps(self.data))\n        return yaml.dump(cfg)",
  "class PPTinyPose(object):\n    def __init__(self, det_model=None, pptinypose_model=None):\n        \"\"\"Set initialized detection model object and pptinypose model object\n\n        :param det_model: (fastdeploy.vision.detection.PicoDet)Initialized detection model object\n        :param pptinypose_model: (fastdeploy.vision.keypointdetection.PPTinyPose)Initialized pptinypose model object\n        \"\"\"\n        assert det_model is not None or pptinypose_model is not None, \"The det_model and pptinypose_model cannot be None.\"\n        self._pipeline = C.pipeline.PPTinyPose(det_model._model,\n                                               pptinypose_model._model)\n\n    def predict(self, input_image):\n        \"\"\"Predict the keypoint detection result for an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: KeyPointDetectionResult\n        \"\"\"\n        return self._pipeline.predict(input_image)\n\n    @property\n    def detection_model_score_threshold(self):\n        \"\"\"Atrribute of PPTinyPose pipeline model. Stating the score threshold for detectin model to filter bbox before inputting pptinypose model\n\n        :return: value of detection_model_score_threshold(float)\n        \"\"\"\n        return self._pipeline.detection_model_score_threshold\n\n    @detection_model_score_threshold.setter\n    def detection_model_score_threshold(self, value):\n        \"\"\"Set attribute detection_model_score_threshold of PPTinyPose pipeline model.\n\n        :param value: (float)The value to set use_dark\n        \"\"\"\n        assert isinstance(\n            value, float\n        ), \"The value to set `detection_model_score_threshold` must be type of float.\"\n        self._pipeline.detection_model_score_threshold = value",
  "def __init__(self, det_model=None, pptinypose_model=None):\n        \"\"\"Set initialized detection model object and pptinypose model object\n\n        :param det_model: (fastdeploy.vision.detection.PicoDet)Initialized detection model object\n        :param pptinypose_model: (fastdeploy.vision.keypointdetection.PPTinyPose)Initialized pptinypose model object\n        \"\"\"\n        assert det_model is not None or pptinypose_model is not None, \"The det_model and pptinypose_model cannot be None.\"\n        self._pipeline = C.pipeline.PPTinyPose(det_model._model,\n                                               pptinypose_model._model)",
  "def predict(self, input_image):\n        \"\"\"Predict the keypoint detection result for an input image\n\n        :param im: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: KeyPointDetectionResult\n        \"\"\"\n        return self._pipeline.predict(input_image)",
  "def detection_model_score_threshold(self):\n        \"\"\"Atrribute of PPTinyPose pipeline model. Stating the score threshold for detectin model to filter bbox before inputting pptinypose model\n\n        :return: value of detection_model_score_threshold(float)\n        \"\"\"\n        return self._pipeline.detection_model_score_threshold",
  "def detection_model_score_threshold(self, value):\n        \"\"\"Set attribute detection_model_score_threshold of PPTinyPose pipeline model.\n\n        :param value: (float)The value to set use_dark\n        \"\"\"\n        assert isinstance(\n            value, float\n        ), \"The value to set `detection_model_score_threshold` must be type of float.\"\n        self._pipeline.detection_model_score_threshold = value",
  "def process_paddle_inference(paddle_inference_so_file):\n    if platform.system().lower() != \"linux\":\n        return\n    rpaths = [\n        \"$ORIGIN\", \"$ORIGIN/../../third_party/install/mkldnn/lib/\",\n        \"$ORIGIN/../../third_party/install/mklml/lib/\",\n        \"$ORIGIN/../../third_party/install/xpu/lib/\",\n        \"$ORIGIN/../../third_party/install/fdmodel/lib/\",\n        \"$ORIGIN/../../../tensorrt/lib/\"\n    ]\n\n    patchelf_exe = os.getenv(\"PATCHELF_EXE\", \"patchelf\")\n    command = \"{} --force-rpath --set-rpath '{}' {}\".format(\n        patchelf_exe, \":\".join(rpaths), paddle_inference_so_file)\n    if platform.machine() != 'sw_64' and platform.machine() != 'mips64':\n        assert os.system(\n            command) == 0, \"patchelf {} failed, the command: {}\".format(\n                paddle_inference_so_file, command)",
  "def process_paddle_lite(paddle_lite_so_path):\n    if platform.system().lower() != \"linux\":\n        return\n    rpaths = [\"$ORIGIN\", \"$ORIGIN/mklml/lib/\"]\n    patchelf_exe = os.getenv(\"PATCHELF_EXE\", \"patchelf\")\n    for root, dirs, files in os.walk(paddle_lite_so_path):\n        for lib in files:\n            if \".so\" in lib:\n                paddle_lite_so_file = os.path.join(root, lib)\n                command = \"{} --set-rpath '{}' {}\".format(\n                    patchelf_exe, \":\".join(rpaths), paddle_lite_so_file)\n                if platform.machine() != 'sw_64' and platform.machine(\n                ) != 'mips64':\n                    assert os.system(\n                        command\n                    ) == 0, \"patchelf {} failed, the command: {}\".format(\n                        paddle_lite_so_file, command)",
  "def copy_directory(src, dst):\n    if os.path.exists(dst):\n        raise Exception(\"Destination {} is already exist.\".format(dst))\n    if not os.path.exists(src):\n        raise Exception(\"Source {} is not exist.\".format(src))\n    try:\n        shutil.copytree(src, dst, symlinks=True)\n    except:\n        raise Exception(\"Copy {} to {} failed.\".format(src, dst))",
  "def parse_arguments():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--use_cvcuda\",\n        required=False,\n        type=bool,\n        help=\"Use CV-CUDA in preprocess\")\n    return parser.parse_args()",
  "class CustomProcessor(PyProcessorManager):\n    def __init__(self) -> None:\n        super().__init__()\n        # create op\n        self.resize_short_op = ResizeByShort(\n            target_size=100, interp=1, use_scale=True, max_hw=[500, 500])\n        self.normalize_permute_op = NormalizeAndPermute(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            is_scale=True,\n            min=[],\n            max=[],\n            swap_rb=False)\n        self.centercrop_op = CenterCrop(width=50, height=50)\n        self.pad_op = Pad(top=5,\n                          bottom=5,\n                          left=5,\n                          right=5,\n                          value=[225, 225, 225])\n        self.cast_op = Cast(dtype=\"float\")\n        self.hwc2chw_op = HWC2CHW()\n        self.normalize_op = Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            is_scale=True,\n            min=[],\n            max=[],\n            swap_rb=False)\n        self.pad_to_size_op = PadToSize(\n            height=160, width=160, value=[225, 225, 225])\n        self.resize_op = Resize(\n            width=50,\n            height=50,\n            scale_w=-1.0,\n            scale_h=-1.0,\n            interp=1,\n            use_scale=False)\n        self.stride_pad_op = StridePad(stride=3, value=[225, 225, 225])\n\n    def apply(self, image_batch):\n        outputs = []\n        self.resize_short_op(image_batch)\n        self.centercrop_op(image_batch)\n        self.pad_op(image_batch)\n        self.pad_to_size_op(image_batch)\n        self.normalize_permute_op(image_batch)\n\n        for i in range(len(image_batch.mats)):\n            outputs.append(image_batch.mats[i])\n\n        return outputs",
  "def __init__(self) -> None:\n        super().__init__()\n        # create op\n        self.resize_short_op = ResizeByShort(\n            target_size=100, interp=1, use_scale=True, max_hw=[500, 500])\n        self.normalize_permute_op = NormalizeAndPermute(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            is_scale=True,\n            min=[],\n            max=[],\n            swap_rb=False)\n        self.centercrop_op = CenterCrop(width=50, height=50)\n        self.pad_op = Pad(top=5,\n                          bottom=5,\n                          left=5,\n                          right=5,\n                          value=[225, 225, 225])\n        self.cast_op = Cast(dtype=\"float\")\n        self.hwc2chw_op = HWC2CHW()\n        self.normalize_op = Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            is_scale=True,\n            min=[],\n            max=[],\n            swap_rb=False)\n        self.pad_to_size_op = PadToSize(\n            height=160, width=160, value=[225, 225, 225])\n        self.resize_op = Resize(\n            width=50,\n            height=50,\n            scale_w=-1.0,\n            scale_h=-1.0,\n            interp=1,\n            use_scale=False)\n        self.stride_pad_op = StridePad(stride=3, value=[225, 225, 225])",
  "def apply(self, image_batch):\n        outputs = []\n        self.resize_short_op(image_batch)\n        self.centercrop_op(image_batch)\n        self.pad_op(image_batch)\n        self.pad_to_size_op(image_batch)\n        self.normalize_permute_op(image_batch)\n\n        for i in range(len(image_batch.mats)):\n            outputs.append(image_batch.mats[i])\n\n        return outputs",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model\", required=True, help=\"Path of PaddleClas model.\")\n    parser.add_argument(\n        \"--image_path\",\n        type=str,\n        required=True,\n        help=\"The directory or path or file list of the images to be predicted.\"\n    )\n    parser.add_argument(\n        \"--topk\", type=int, default=1, help=\"Return topk results.\")\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='cpu',\n        help=\"Type of inference device, support 'cpu' or 'gpu' or 'ipu'.\")\n    parser.add_argument(\n        \"--use_trt\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"Wether to use tensorrt.\")\n    parser.add_argument(\"--thread_num\", type=int, default=1, help=\"thread num\")\n    parser.add_argument(\n        \"--use_multi_process\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"Wether to use multi process.\")\n    parser.add_argument(\n        \"--process_num\", type=int, default=1, help=\"process num\")\n    return parser.parse_args()",
  "def get_image_list(image_path):\n    image_list = []\n    if os.path.isfile(image_path):\n        image_list.append(image_path)\n    # load image in a directory\n    elif os.path.isdir(image_path):\n        for root, dirs, files in os.walk(image_path):\n            for f in files:\n                image_list.append(os.path.join(root, f))\n    else:\n        raise FileNotFoundError(\n            '{} is not found. it should be a path of image, or a directory including images.'.\n            format(image_path))\n\n    if len(image_list) == 0:\n        raise RuntimeError(\n            'There are not image file in `--image_path`={}'.format(image_path))\n\n    return image_list",
  "def build_option(args):\n    option = fd.RuntimeOption()\n\n    if args.device.lower() == \"gpu\":\n        option.use_paddle_backend()\n        option.use_gpu()\n\n    if args.device.lower() == \"ipu\":\n        option.use_ipu()\n\n    if args.use_trt:\n        option.use_trt_backend()\n    return option",
  "def load_model(args, runtime_option):\n    model_file = os.path.join(args.model, \"inference.pdmodel\")\n    params_file = os.path.join(args.model, \"inference.pdiparams\")\n    config_file = os.path.join(args.model, \"inference_cls.yaml\")\n    global model\n    model = fd.vision.classification.PaddleClasModel(\n        model_file, params_file, config_file, runtime_option=runtime_option)",
  "def predict(model, img_list, topk):\n    result_list = []\n    # predict classification result\n    for image in img_list:\n        im = cv2.imread(image)\n        result = model.predict(im, topk)\n        result_list.append(result)\n    return result_list",
  "def process_predict(image):\n    # predict classification result\n    im = cv2.imread(image)\n    result = model.predict(im, args.topk)\n    print(result)",
  "class WrapperThread(Thread):\n    def __init__(self, func, args):\n        super(WrapperThread, self).__init__()\n        self.func = func\n        self.args = args\n\n    def run(self):\n        self.result = self.func(*self.args)\n\n    def get_result(self):\n        return self.result",
  "def __init__(self, func, args):\n        super(WrapperThread, self).__init__()\n        self.func = func\n        self.args = args",
  "def run(self):\n        self.result = self.func(*self.args)",
  "def get_result(self):\n        return self.result",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--det_model\", required=True, help=\"Path of Detection model of PPOCR.\")\n    parser.add_argument(\n        \"--cls_model\",\n        required=True,\n        help=\"Path of Classification model of PPOCR.\")\n    parser.add_argument(\n        \"--rec_model\",\n        required=True,\n        help=\"Path of Recognization model of PPOCR.\")\n    parser.add_argument(\n        \"--rec_label_file\",\n        required=True,\n        help=\"Path of Recognization model of PPOCR.\")\n    parser.add_argument(\n        \"--image_path\",\n        type=str,\n        required=True,\n        help=\"The directory or path or file list of the images to be predicted.\"\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='cpu',\n        help=\"Type of inference device, support 'cpu', 'kunlunxin' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"default\",\n        help=\"Type of inference backend, support ort/trt/paddle/openvino, default 'openvino' for cpu, 'tensorrt' for gpu\"\n    )\n    parser.add_argument(\n        \"--device_id\",\n        type=int,\n        default=0,\n        help=\"Define which GPU card used to run model.\")\n    parser.add_argument(\n        \"--cpu_thread_num\",\n        type=int,\n        default=9,\n        help=\"Number of threads while inference on CPU.\")\n    parser.add_argument(\n        \"--cls_bs\",\n        type=int,\n        default=1,\n        help=\"Classification model inference batch size.\")\n    parser.add_argument(\n        \"--rec_bs\",\n        type=int,\n        default=6,\n        help=\"Recognition model inference batch size\")\n    parser.add_argument(\"--thread_num\", type=int, default=1, help=\"thread num\")\n    parser.add_argument(\n        \"--use_multi_process\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"Wether to use multi process.\")\n    parser.add_argument(\n        \"--process_num\", type=int, default=1, help=\"process num\")\n    return parser.parse_args()",
  "def get_image_list(image_path):\n    image_list = []\n    if os.path.isfile(image_path):\n        image_list.append(image_path)\n    # load image in a directory\n    elif os.path.isdir(image_path):\n        for root, dirs, files in os.walk(image_path):\n            for f in files:\n                image_list.append(os.path.join(root, f))\n    else:\n        raise FileNotFoundError(\n            '{} is not found. it should be a path of image, or a directory including images.'.\n            format(image_path))\n\n    if len(image_list) == 0:\n        raise RuntimeError(\n            'There are not image file in `--image_path`={}'.format(image_path))\n\n    return image_list",
  "def build_option(args):\n    option = fd.RuntimeOption()\n    if args.device.lower() == \"gpu\":\n        option.use_gpu(args.device_id)\n\n    option.set_cpu_thread_num(args.cpu_thread_num)\n\n    if args.device.lower() == \"kunlunxin\":\n        option.use_kunlunxin()\n        return option\n\n    if args.backend.lower() == \"trt\":\n        assert args.device.lower(\n        ) == \"gpu\", \"TensorRT backend require inference on device GPU.\"\n        option.use_trt_backend()\n    elif args.backend.lower() == \"pptrt\":\n        assert args.device.lower(\n        ) == \"gpu\", \"Paddle-TensorRT backend require inference on device GPU.\"\n        option.use_trt_backend()\n        option.enable_paddle_trt_collect_shape()\n        option.enable_paddle_to_trt()\n    elif args.backend.lower() == \"ort\":\n        option.use_ort_backend()\n    elif args.backend.lower() == \"paddle\":\n        option.use_paddle_infer_backend()\n    elif args.backend.lower() == \"openvino\":\n        assert args.device.lower(\n        ) == \"cpu\", \"OpenVINO backend require inference on device CPU.\"\n        option.use_openvino_backend()\n    return option",
  "def load_model(args, runtime_option):\n    # Detection\u6a21\u578b, \u68c0\u6d4b\u6587\u5b57\u6846\n    det_model_file = os.path.join(args.det_model, \"inference.pdmodel\")\n    det_params_file = os.path.join(args.det_model, \"inference.pdiparams\")\n    # Classification\u6a21\u578b\uff0c\u65b9\u5411\u5206\u7c7b\uff0c\u53ef\u9009\n    cls_model_file = os.path.join(args.cls_model, \"inference.pdmodel\")\n    cls_params_file = os.path.join(args.cls_model, \"inference.pdiparams\")\n    # Recognition\u6a21\u578b\uff0c\u6587\u5b57\u8bc6\u522b\u6a21\u578b\n    rec_model_file = os.path.join(args.rec_model, \"inference.pdmodel\")\n    rec_params_file = os.path.join(args.rec_model, \"inference.pdiparams\")\n    rec_label_file = args.rec_label_file\n\n    # PPOCR\u7684cls\u548crec\u6a21\u578b\u73b0\u5728\u5df2\u7ecf\u652f\u6301\u63a8\u7406\u4e00\u4e2aBatch\u7684\u6570\u636e\n    # \u5b9a\u4e49\u4e0b\u9762\u4e24\u4e2a\u53d8\u91cf\u540e, \u53ef\u7528\u4e8e\u8bbe\u7f6etrt\u8f93\u5165shape, \u5e76\u5728PPOCR\u6a21\u578b\u521d\u59cb\u5316\u540e, \u5b8c\u6210Batch\u63a8\u7406\u8bbe\u7f6e\n    cls_batch_size = 1\n    rec_batch_size = 6\n\n    # \u5f53\u4f7f\u7528TRT\u65f6\uff0c\u5206\u522b\u7ed9\u4e09\u4e2a\u6a21\u578b\u7684runtime\u8bbe\u7f6e\u52a8\u6001shape,\u5e76\u5b8c\u6210\u6a21\u578b\u7684\u521b\u5efa.\n    # \u6ce8\u610f: \u9700\u8981\u5728\u68c0\u6d4b\u6a21\u578b\u521b\u5efa\u5b8c\u6210\u540e\uff0c\u518d\u8bbe\u7f6e\u5206\u7c7b\u6a21\u578b\u7684\u52a8\u6001\u8f93\u5165\u5e76\u521b\u5efa\u5206\u7c7b\u6a21\u578b, \u8bc6\u522b\u6a21\u578b\u540c\u7406.\n    # \u5982\u679c\u7528\u6237\u60f3\u8981\u81ea\u5df1\u6539\u52a8\u68c0\u6d4b\u6a21\u578b\u7684\u8f93\u5165shape, \u6211\u4eec\u5efa\u8bae\u7528\u6237\u628a\u68c0\u6d4b\u6a21\u578b\u7684\u957f\u548c\u9ad8\u8bbe\u7f6e\u4e3a32\u7684\u500d\u6570.\n    det_option = runtime_option\n    det_option.set_trt_input_shape(\"x\", [1, 3, 64, 64], [1, 3, 640, 640],\n                                   [1, 3, 960, 960])\n    # \u7528\u6237\u53ef\u4ee5\u628aTRT\u5f15\u64ce\u6587\u4ef6\u4fdd\u5b58\u81f3\u672c\u5730\n    #det_option.set_trt_cache_file(args.det_model  + \"/det_trt_cache.trt\")\n    global det_model\n    det_model = fd.vision.ocr.DBDetector(\n        det_model_file, det_params_file, runtime_option=det_option)\n\n    cls_option = runtime_option\n    cls_option.set_trt_input_shape(\"x\", [1, 3, 48, 10],\n                                   [cls_batch_size, 3, 48, 320],\n                                   [cls_batch_size, 3, 48, 1024])\n    # \u7528\u6237\u53ef\u4ee5\u628aTRT\u5f15\u64ce\u6587\u4ef6\u4fdd\u5b58\u81f3\u672c\u5730\n    #cls_option.set_trt_cache_file(args.cls_model  + \"/cls_trt_cache.trt\")\n    global cls_model\n    cls_model = fd.vision.ocr.Classifier(\n        cls_model_file, cls_params_file, runtime_option=cls_option)\n\n    rec_option = runtime_option\n    rec_option.set_trt_input_shape(\"x\", [1, 3, 48, 10],\n                                   [rec_batch_size, 3, 48, 320],\n                                   [rec_batch_size, 3, 48, 2304])\n    # \u7528\u6237\u53ef\u4ee5\u628aTRT\u5f15\u64ce\u6587\u4ef6\u4fdd\u5b58\u81f3\u672c\u5730\n    #rec_option.set_trt_cache_file(args.rec_model  + \"/rec_trt_cache.trt\")\n    global rec_model\n    rec_model = fd.vision.ocr.Recognizer(\n        rec_model_file,\n        rec_params_file,\n        rec_label_file,\n        runtime_option=rec_option)\n\n    # \u521b\u5efaPP-OCR\uff0c\u4e32\u80543\u4e2a\u6a21\u578b\uff0c\u5176\u4e2dcls_model\u53ef\u9009\uff0c\u5982\u65e0\u9700\u6c42\uff0c\u53ef\u8bbe\u7f6e\u4e3aNone\n    global ppocr_v3\n    ppocr_v3 = fd.vision.ocr.PPOCRv3(\n        det_model=det_model, cls_model=cls_model, rec_model=rec_model)\n\n    # \u7ed9cls\u548crec\u6a21\u578b\u8bbe\u7f6e\u63a8\u7406\u65f6\u7684batch size\n    # \u6b64\u503c\u80fd\u4e3a-1, \u548c1\u5230\u6b63\u65e0\u7a77\n    # \u5f53\u6b64\u503c\u4e3a-1\u65f6, cls\u548crec\u6a21\u578b\u7684batch size\u5c06\u9ed8\u8ba4\u548cdet\u6a21\u578b\u68c0\u6d4b\u51fa\u7684\u6846\u7684\u6570\u91cf\u76f8\u540c\n    ppocr_v3.cls_batch_size = cls_batch_size\n    ppocr_v3.rec_batch_size = rec_batch_size",
  "def predict(model, img_list):\n    result_list = []\n    # predict ppocr result\n    for image in img_list:\n        im = cv2.imread(image)\n        result = model.predict(im)\n        result_list.append(result)\n    return result_list",
  "def process_predict(image):\n    # predict ppocr result\n    im = cv2.imread(image)\n    result = ppocr_v3.predict(im)\n    print(result)",
  "class WrapperThread(Thread):\n    def __init__(self, func, args):\n        super(WrapperThread, self).__init__()\n        self.func = func\n        self.args = args\n\n    def run(self):\n        self.result = self.func(*self.args)\n\n    def get_result(self):\n        return self.result",
  "def __init__(self, func, args):\n        super(WrapperThread, self).__init__()\n        self.func = func\n        self.args = args",
  "def run(self):\n        self.result = self.func(*self.args)",
  "def get_result(self):\n        return self.result",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model\", required=True, help=\"Path of PaddleClas model.\")\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\")\n    parser.add_argument(\n        \"--topk\", type=int, default=1, help=\"Return topk results.\")\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='cpu',\n        help=\"Type of inference device, support 'cpu' or 'intel_gpu'.\")\n    return parser.parse_args()",
  "def build_option(args):\n    option = fd.RuntimeOption()\n    option.use_openvino_backend()\n\n    assert args.device.lower(\n    ) in [\"cpu\", \"intel_gpu\"], \"--device only support ['cpu', 'intel_gpu']\"\n\n    if args.device.lower() == \"intel_gpu\":\n        option.set_openvino_device(\"GPU\")\n        option.set_openvino_shape_info({\"inputs\": [1, 3, 224, 224]})\n\n    return option",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model\", required=True, help=\"Path of PP-YOLOE model.\")\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\")\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='cpu',\n        help=\"Type of inference device, support 'cpu' or 'intel_gpu'.\")\n    return parser.parse_args()",
  "def build_option(args):\n    option = fd.RuntimeOption()\n    option.use_openvino_backend()\n\n    assert args.device.lower(\n    ) in [\"cpu\", \"intel_gpu\"], \"--device only support ['cpu', 'intel_gpu']\"\n\n    if args.device.lower() == \"intel_gpu\":\n        option.set_openvino_device(\"HETERO:GPU,CPU\")\n        option.set_openvino_shape_info({\n            \"image\": [1, 3, 640, 640],\n            \"scale_factor\": [1, 2]\n        })\n        option.set_openvino_cpu_operators([\"MulticlassNms\"])\n    return option",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--encrypted_model_dir\",\n        required=False,\n        help=\"Path of model directory.\")\n    parser.add_argument(\n        \"--model_file\", required=True, help=\"Path of model file directory.\")\n    parser.add_argument(\n        \"--params_file\",\n        required=True,\n        help=\"Path of parameters file directory.\")\n    return parser.parse_args()",
  "def try_import(module_name):\n    \"\"\"Try importing a module, with an informative error message on failure.\"\"\"\n    install_name = module_name\n    try:\n        mod = importlib.import_module(module_name)\n        return mod\n    except ImportError:\n        err_msg = (\n            \"Failed importing {}. This likely means that some modules \"\n            \"requires additional dependencies that have to be \"\n            \"manually installed (usually with `pip install {}`). \").format(\n                module_name, install_name)\n        raise ImportError(err_msg)",
  "def check_model(onnx_model):\n    onnx = try_import('onnx')\n    try:\n        onnx.checker.check_model(onnx_model)\n    except Exception:\n        raise Exception('ONNX model is not valid.')\n    finally:\n        logging.info('ONNX model generated is valid.')",
  "class logging():\n    log_level = 2\n\n    @staticmethod\n    def log(level=2, message=\"\", use_color=False):\n        current_time = time.time()\n        time_array = time.localtime(current_time)\n        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time_array)\n        if logging.log_level >= level:\n            if use_color:\n                print(\"\\033[1;31;40m{} [{}]\\t{}\\033[0m\".format(\n                    current_time, levels[level], message).encode(\"utf-8\")\n                      .decode(\"latin1\"))\n            else:\n                print(\"{} [{}]\\t{}\".format(current_time, levels[level], message)\n                      .encode(\"utf-8\").decode(\"latin1\"))\n            sys.stdout.flush()\n\n    @staticmethod\n    def debug(message=\"\", use_color=False):\n        logging.log(level=3, message=message, use_color=use_color)\n\n    @staticmethod\n    def info(message=\"\", use_color=False):\n        logging.log(level=2, message=message, use_color=use_color)\n\n    @staticmethod\n    def warning(message=\"\", use_color=True):\n        logging.log(level=1, message=message, use_color=use_color)\n\n    @staticmethod\n    def error(message=\"\", use_color=True, exit=True):\n        logging.log(level=0, message=message, use_color=use_color)\n        if exit:\n            sys.exit(-1)",
  "def compare_value(a, b, cond):\n    if cond == 'equal':\n        if a != b:\n            return False\n        return True\n    if cond == 'greater_than':\n        if a <= b:\n            return False\n        return True\n    if cond == 'greater_equal':\n        if a < b:\n            return False\n        return True\n    if cond == 'less_equal':\n        if a > b:\n            return False\n        return True\n    if cond == 'less_than':\n        if a >= b:\n            return False\n        return True",
  "def compare_attr(actual_value, target_value, attr_name, cond='equal'):\n    if not compare_value(actual_value, target_value, cond):\n        raise ValueError('Support {} {} {}, actually got {}=={}.'.format(\n            attr_name, cond, target_value, attr_name, actual_value))",
  "def compare_attr_between_dims(attr, dims, attr_name, cond='equal'):\n    if not compare_value(attr[dims[0]], attr[dims[1]], cond):\n        expect_info = 'Support {}[{}] {} {}[{}], '.format(\n            attr_name, dims[0], cond, attr_name, dims[1])\n        actual_info = 'actually got {}[{}]=={}, not {} {}[{}]=={}.'.format(\n            attr_name, dims[0], attr[dims[0]], cond, attr_name, dims[1],\n            attr[dims[1]])\n        raise ValueError(expect_info + actual_info)",
  "def require_fixed_shape(op_name=None):\n    logging.error(\n        \"[{}]Fixed shape is required, refer this doc for more information: https://github.com/PaddlePaddle/Paddle2ONNX/blob/develop/docs/zh/FAQ.md\".\n        format(op_name))",
  "def log(level=2, message=\"\", use_color=False):\n        current_time = time.time()\n        time_array = time.localtime(current_time)\n        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time_array)\n        if logging.log_level >= level:\n            if use_color:\n                print(\"\\033[1;31;40m{} [{}]\\t{}\\033[0m\".format(\n                    current_time, levels[level], message).encode(\"utf-8\")\n                      .decode(\"latin1\"))\n            else:\n                print(\"{} [{}]\\t{}\".format(current_time, levels[level], message)\n                      .encode(\"utf-8\").decode(\"latin1\"))\n            sys.stdout.flush()",
  "def debug(message=\"\", use_color=False):\n        logging.log(level=3, message=message, use_color=use_color)",
  "def info(message=\"\", use_color=False):\n        logging.log(level=2, message=message, use_color=use_color)",
  "def warning(message=\"\", use_color=True):\n        logging.log(level=1, message=message, use_color=use_color)",
  "def error(message=\"\", use_color=True, exit=True):\n        logging.log(level=0, message=message, use_color=use_color)\n        if exit:\n            sys.exit(-1)",
  "def parse_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--input_model_path',\n        required=True,\n        help='The path of input onnx model file.')\n    parser.add_argument(\n        '--output_model_path',\n        required=True,\n        help='The file path to write optimized onnx model file.')\n    return parser.parse_args()",
  "def str2list(v):\n    if len(v) == 0:\n        return None\n    v = v.replace(\" \", \"\")\n    v = eval(v)\n    return v",
  "def arg_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_dir\",\n        \"-m\",\n        type=_text_type,\n        default=None,\n        help=\"PaddlePaddle model directory, if params stored in single file, you need define '--model_filename' and 'params_filename'.\"\n    )\n    parser.add_argument(\n        \"--model_filename\",\n        \"-mf\",\n        type=_text_type,\n        default=None,\n        help=\"PaddlePaddle model's network file name, which under directory seted by --model_dir\"\n    )\n    parser.add_argument(\n        \"--params_filename\",\n        \"-pf\",\n        type=_text_type,\n        default=None,\n        help=\"PaddlePaddle model's param file name(param files combined in single file), which under directory seted by --model_dir.\"\n    )\n    parser.add_argument(\n        \"--save_file\",\n        \"-s\",\n        type=_text_type,\n        default=None,\n        help=\"file path to save onnx model\")\n    parser.add_argument(\n        \"--opset_version\",\n        \"-ov\",\n        type=int,\n        default=9,\n        help=\"set onnx opset version to export\")\n    parser.add_argument(\n       \"--input_shape_dict\",\n       \"-isd\",\n       type=_text_type,\n       default=\"None\",\n       help=\"define input shapes, e.g --input_shape_dict=\\\"{'image':[1, 3, 608, 608]}\\\" or\" \\\n       \"--input_shape_dict=\\\"{'image':[1, 3, 608, 608], 'im_shape': [1, 2], 'scale_factor': [1, 2]}\\\"\")\n    parser.add_argument(\n        \"--enable_dev_version\",\n        type=ast.literal_eval,\n        default=True,\n        help=\"whether to use new version of Paddle2ONNX which is under developing, default True\"\n    )\n    parser.add_argument(\n        \"--deploy_backend\",\n        \"-d\",\n        type=_text_type,\n        default=\"onnxruntime\",\n        choices=[\"onnxruntime\", \"tensorrt\", \"rknn\", \"others\"],\n        help=\"Quantize model deploy backend, default onnxruntime.\")\n    parser.add_argument(\n        \"--save_calibration_file\",\n        type=_text_type,\n        default=\"calibration.cache\",\n        help=\"The calibration cache for TensorRT deploy, default calibration.cache.\"\n    )\n    parser.add_argument(\n        \"--enable_onnx_checker\",\n        type=ast.literal_eval,\n        default=True,\n        help=\"whether check onnx model validity, default True\")\n    parser.add_argument(\n        \"--enable_paddle_fallback\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether use PaddleFallback for custom op, default is False\")\n    parser.add_argument(\n        \"--version\",\n        \"-v\",\n        action=\"store_true\",\n        default=False,\n        help=\"get version of paddle2onnx\")\n    parser.add_argument(\n        \"--output_names\",\n        \"-on\",\n        type=str2list,\n        default=None,\n        help=\"define output names, e.g --output_names=\\\"[\\\"output1\\\"]\\\" or \\\n       --output_names=\\\"[\\\"output1\\\", \\\"output2\\\", \\\"output3\\\"]\\\" or \\\n       --output_names=\\\"{\\\"Paddleoutput\\\":\\\"Onnxoutput\\\"}\\\"\")\n    parser.add_argument(\n        \"--enable_auto_update_opset\",\n        type=ast.literal_eval,\n        default=True,\n        help=\"whether enable auto_update_opset, default is True\")\n    parser.add_argument(\n        \"--external_filename\",\n        type=_text_type,\n        default=None,\n        help=\"The filename of external_data when the model is bigger than 2G.\")\n    parser.add_argument(\n        \"--export_fp16_model\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"Whether export FP16 model for ORT-GPU, default False\")\n    parser.add_argument(\n        \"--custom_ops\",\n        type=_text_type,\n        default=\"{}\",\n        help=\"Ops that needs to be converted to custom op, e.g --custom_ops '{\\\"paddle_op\\\":\\\"onnx_op\\\"}', default {}\"\n    )\n    return parser",
  "def c_paddle_to_onnx(model_file,\n                     params_file=\"\",\n                     save_file=None,\n                     opset_version=7,\n                     auto_upgrade_opset=True,\n                     verbose=True,\n                     enable_onnx_checker=True,\n                     enable_experimental_op=True,\n                     enable_optimize=True,\n                     deploy_backend=\"onnxruntime\",\n                     calibration_file=\"\",\n                     external_file=\"\",\n                     export_fp16_model=False,\n                     custom_ops={}):\n    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o\n    onnx_model_str = c_p2o.export(\n        model_file, params_file, opset_version, auto_upgrade_opset, verbose,\n        enable_onnx_checker, enable_experimental_op, enable_optimize,\n        custom_ops, deploy_backend, calibration_file, external_file,\n        export_fp16_model)\n    if save_file is not None:\n        with open(save_file, \"wb\") as f:\n            f.write(onnx_model_str)\n    else:\n        return onnx_model_str",
  "def program2onnx(model_dir,\n                 save_file,\n                 model_filename=None,\n                 params_filename=None,\n                 opset_version=9,\n                 enable_onnx_checker=False,\n                 operator_export_type=\"ONNX\",\n                 input_shape_dict=None,\n                 output_names=None,\n                 auto_update_opset=True):\n    logging.warning(\n        \"[Deprecated] `paddle2onnx.command.program2onnx` will be deprecated in the future version, the recommended usage is `paddle2onnx.export`\"\n    )\n    from paddle2onnx.legacy.command import program2onnx\n    return program2onnx(model_dir, save_file, model_filename, params_filename,\n                        opset_version, enable_onnx_checker,\n                        operator_export_type, input_shape_dict, output_names,\n                        auto_update_opset)",
  "def main():\n    if len(sys.argv) < 2:\n        logging.info(\"Use \\\"paddle2onnx -h\\\" to print the help information\")\n        logging.info(\n            \"For more information, please follow our github repo below:\")\n        logging.info(\"Github: https://github.com/PaddlePaddle/paddle2onnx.git\")\n        return\n\n    parser = arg_parser()\n    args = parser.parse_args()\n\n    if args.version:\n        import paddle2onnx\n        logging.info(\"paddle2onnx-{} with python>=3.6, paddlepaddle>=2.0.0\".\n                     format(paddle2onnx.__version__))\n        return\n\n    assert args.model_dir is not None, \"--model_dir should be defined while translating paddle model to onnx\"\n    assert args.save_file is not None, \"--save_file should be defined while translating paddle model to onnx\"\n\n    input_shape_dict = eval(args.input_shape_dict)\n\n    operator_export_type = \"ONNX\"\n    if args.enable_paddle_fallback:\n        logging.warning(\n            \"[Deprecated] The flag `--enable_paddle_fallback` will be deprecated, and only works while `--enable_dev_version False` now.\"\n        )\n        operator_export_type = \"PaddleFallback\"\n\n    if args.output_names is not None and args.enable_dev_version:\n        logging.warning(\n            \"[Deprecated] The flag `--output_names` is deprecated, if you need to modify the output name, please refer to this tool https://github.com/jiangjiajun/PaddleUtils/tree/main/onnx \"\n        )\n        if not isinstance(args.output_names, (list, dict)):\n            raise TypeError(\n                \"The output_names should be 'list' or 'dict', but received type is %s.\"\n                % type(args.output_names))\n\n    if input_shape_dict is not None and args.enable_dev_version:\n        logging.warning(\n            \"[Deprecated] The flag `--input_shape_dict` is deprecated, if you need to modify the input shape of PaddlePaddle model, please refer to this tool https://github.com/jiangjiajun/PaddleUtils/tree/main/paddle \"\n        )\n\n    if args.enable_dev_version:\n        model_file = os.path.join(args.model_dir, args.model_filename)\n        if args.params_filename is None:\n            params_file = \"\"\n        else:\n            params_file = os.path.join(args.model_dir, args.params_filename)\n\n        if args.external_filename is None:\n            args.external_filename = \"external_data\"\n\n        base_path = os.path.dirname(args.save_file)\n        if base_path and not os.path.exists(base_path):\n            os.mkdir(base_path)\n        external_file = os.path.join(base_path, args.external_filename)\n\n        custom_ops_dict = eval(args.custom_ops)\n\n        calibration_file = args.save_calibration_file\n        c_paddle_to_onnx(\n            model_file=model_file,\n            params_file=params_file,\n            save_file=args.save_file,\n            opset_version=args.opset_version,\n            auto_upgrade_opset=args.enable_auto_update_opset,\n            verbose=True,\n            enable_onnx_checker=args.enable_onnx_checker,\n            enable_experimental_op=True,\n            enable_optimize=True,\n            deploy_backend=args.deploy_backend,\n            calibration_file=calibration_file,\n            external_file=external_file,\n            export_fp16_model=args.export_fp16_model,\n            custom_ops=custom_ops_dict)\n        logging.info(\"===============Make PaddlePaddle Better!================\")\n        logging.info(\"A little survey: https://iwenjuan.baidu.com/?code=r8hu2s\")\n        return\n\n    program2onnx(\n        args.model_dir,\n        args.save_file,\n        args.model_filename,\n        args.params_filename,\n        opset_version=args.opset_version,\n        enable_onnx_checker=args.enable_onnx_checker,\n        operator_export_type=operator_export_type,\n        input_shape_dict=input_shape_dict,\n        output_names=args.output_names,\n        auto_update_opset=args.enable_auto_update_opset)\n    logging.info(\"===============Make PaddlePaddle Better!================\")\n    logging.info(\"A little survey: https://iwenjuan.baidu.com/?code=r8hu2s\")",
  "def run_convert(model, input_shape_dict=None, scope=None, opset_version=9):\n    logging.warning(\n        \"[Deprecated] `paddle2onnx.run_convert` will be deprecated in the future version, the recommended usage is `paddle2onnx.export`\"\n    )\n    from paddle2onnx.legacy import run_convert\n    return run_convert(model, input_shape_dict, scope, opset_version)",
  "def export(model_file,\n           params_file=\"\",\n           save_file=None,\n           opset_version=11,\n           auto_upgrade_opset=True,\n           verbose=True,\n           enable_onnx_checker=True,\n           enable_experimental_op=True,\n           enable_optimize=True,\n           custom_op_info=None,\n           deploy_backend=\"onnxruntime\",\n           calibration_file=\"\",\n           external_file=\"\",\n           export_fp16_model=False):\n    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o\n    deploy_backend = deploy_backend.lower()\n    if custom_op_info is None:\n        onnx_model_str = c_p2o.export(\n            model_file, params_file, opset_version, auto_upgrade_opset, verbose,\n            enable_onnx_checker, enable_experimental_op, enable_optimize, {},\n            deploy_backend, calibration_file, external_file, export_fp16_model)\n    else:\n        onnx_model_str = c_p2o.export(\n            model_file, params_file, opset_version, auto_upgrade_opset, verbose,\n            enable_onnx_checker, enable_experimental_op, enable_optimize,\n            custom_op_info, deploy_backend, calibration_file, external_file,\n            export_fp16_model)\n    if save_file is not None:\n        with open(save_file, \"wb\") as f:\n            f.write(onnx_model_str)\n    else:\n        return onnx_model_str",
  "def export_onnx(paddle_graph,\n                save_file,\n                opset_version=9,\n                enable_onnx_checker=False,\n                operator_export_type=\"ONNX\",\n                verbose=False,\n                auto_update_opset=True,\n                output_names=None):\n    from paddle2onnx.legacy.convert import export_onnx\n    return export_onnx(paddle_graph, save_file, opset_version, opset_version,\n                       enable_onnx_checker, operator_export_type, verbose,\n                       auto_update_opset, output_names)",
  "def dygraph2onnx(layer, save_file, input_spec=None, opset_version=9, **configs):\n    if \"enable_dev_version\" in configs and not configs[\"enable_dev_version\"]:\n        from paddle2onnx.legacy.convert import dygraph2onnx\n        return dygraph2onnx(layer, save_file, input_spec, opset_version,\n                            **configs)\n\n    import os\n    import paddle2onnx\n    import paddle\n    dirname = os.path.split(save_file)[0]\n    paddle_model_dir = os.path.join(dirname,\n                                    \"paddle_model_static_onnx_temp_dir\")\n    model_file = os.path.join(paddle_model_dir, \"model.pdmodel\")\n    params_file = os.path.join(paddle_model_dir, \"model.pdiparams\")\n\n    if os.path.exists(paddle_model_dir):\n        if os.path.isfile(paddle_model_dir):\n            logging.info(\"File {} exists, will remove it.\".format(\n                paddle_model_dir))\n            os.remove(paddle_model_dir)\n        if os.path.isfile(model_file):\n            os.remove(model_file)\n        if os.path.isfile(params_file):\n            os.remove(params_file)\n    paddle.jit.save(layer, os.path.join(paddle_model_dir, \"model\"), input_spec)\n    logging.info(\"Static PaddlePaddle model saved in {}.\".format(\n        paddle_model_dir))\n    if not os.path.isfile(params_file):\n        params_file = \"\"\n\n    if save_file is None:\n        return paddle2onnx.export(model_file, params_file, save_file,\n                                  opset_version)\n    else:\n        paddle2onnx.export(model_file, params_file, save_file, opset_version)\n    logging.info(\"ONNX model saved in {}.\".format(save_file))",
  "def program2onnx(program,\n                 scope,\n                 save_file,\n                 feed_var_names=None,\n                 target_vars=None,\n                 opset_version=9,\n                 enable_onnx_checker=False,\n                 operator_export_type=\"ONNX\",\n                 auto_update_opset=True,\n                 **configs):\n    from paddle2onnx.legacy.convert import program2onnx\n    return program2onnx(program, scope, save_file, feed_var_names, target_vars,\n                        opset_version, enable_onnx_checker,\n                        operator_export_type, auto_update_opset, **configs)",
  "def parse_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--input_model',\n        required=True,\n        help='The path of input onnx model file.')\n    parser.add_argument(\n        '--output_model',\n        required=True,\n        help='The file path to write optimized onnx model file.')\n    parser.add_argument(\n        '--input_shape_dict',\n        default=\"\",\n        help=\"The shape infos of inputs, e.g --input_shape_dict=\\\"{'image': [1, 3, 608, 608], 'scale_factor': [1, 2]}\\\"\"\n    )\n    return parser.parse_args()",
  "def str2list(v):\n    if len(v) == 0:\n        return None\n    v = v.replace(\" \", \"\")\n    v = eval(v)\n    return v",
  "def arg_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_dir\",\n        \"-m\",\n        type=_text_type,\n        default=None,\n        help=\"PaddlePaddle model directory, if params stored in single file, you need define '--model_filename' and 'params_filename'.\"\n    )\n    parser.add_argument(\n        \"--model_filename\",\n        \"-mf\",\n        type=_text_type,\n        default=None,\n        help=\"PaddlePaddle model's network file name, which under directory seted by --model_dir\"\n    )\n    parser.add_argument(\n        \"--params_filename\",\n        \"-pf\",\n        type=_text_type,\n        default=None,\n        help=\"PaddlePaddle model's param file name(param files combined in single file), which under directory seted by --model_dir.\"\n    )\n    parser.add_argument(\n        \"--save_file\",\n        \"-s\",\n        type=_text_type,\n        default=None,\n        help=\"file path to save onnx model\")\n    parser.add_argument(\n        \"--opset_version\",\n        \"-ov\",\n        type=int,\n        default=9,\n        help=\"set onnx opset version to export\")\n    parser.add_argument(\n       \"--input_shape_dict\",\n       \"-isd\",\n       type=_text_type,\n       default=\"None\",\n       help=\"define input shapes, e.g --input_shape_dict=\\\"{'image':[1, 3, 608, 608]}\\\" or\" \\\n       \"--input_shape_dict=\\\"{'image':[1, 3, 608, 608], 'im_shape': [1, 2], 'scale_factor': [1, 2]}\\\"\")\n    parser.add_argument(\n        \"--enable_dev_version\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether to use new version of Paddle2ONNX which is under developing, default False\"\n    )\n    parser.add_argument(\n        \"--enable_onnx_checker\",\n        type=ast.literal_eval,\n        default=True,\n        help=\"whether check onnx model validity, default True\")\n    parser.add_argument(\n        \"--enable_paddle_fallback\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether use PaddleFallback for custom op, default is False\")\n    parser.add_argument(\n        \"--version\",\n        \"-v\",\n        action=\"store_true\",\n        default=False,\n        help=\"get version of paddle2onnx\")\n    parser.add_argument(\n        \"--output_names\",\n        \"-on\",\n        type=str2list,\n        default=None,\n        help=\"define output names, e.g --output_names=\\\"[\\\"output1\\\"]\\\" or \\\n       --output_names=\\\"[\\\"output1\\\", \\\"output2\\\", \\\"output3\\\"]\\\" or \\\n       --output_names=\\\"{\\\"Paddleoutput\\\":\\\"Onnxoutput\\\"}\\\"\")\n    parser.add_argument(\n        \"--enable_auto_update_opset\",\n        type=ast.literal_eval,\n        default=True,\n        help=\"whether enable auto_update_opset, default is True\")\n    return parser",
  "def c_paddle_to_onnx(model_file,\n                     params_file=\"\",\n                     save_file=None,\n                     opset_version=7,\n                     auto_upgrade_opset=True,\n                     verbose=True,\n                     enable_onnx_checker=True,\n                     enable_experimental_op=True,\n                     enable_optimize=True):\n    import paddle2onnx.paddle2onnx_cpp2py_export as c_p2o\n    onnx_model_str = c_p2o.export(\n        model_file, params_file, opset_version, auto_upgrade_opset, verbose,\n        enable_onnx_checker, enable_experimental_op, enable_optimize)\n    if save_file is not None:\n        with open(save_file, \"wb\") as f:\n            f.write(onnx_model_str)\n    else:\n        return onnx_model_str",
  "def program2onnx(model_dir,\n                 save_file,\n                 model_filename=None,\n                 params_filename=None,\n                 opset_version=9,\n                 enable_onnx_checker=False,\n                 operator_export_type=\"ONNX\",\n                 input_shape_dict=None,\n                 output_names=None,\n                 auto_update_opset=True):\n    try:\n        import paddle\n    except:\n        logging.error(\n            \"paddlepaddle not installed, use \\\"pip install paddlepaddle\\\"\")\n\n    v0, v1, v2 = paddle.__version__.split('.')\n    if v0 == '0' and v1 == '0' and v2 == '0':\n        logging.warning(\"You are use develop version of paddlepaddle\")\n    elif int(v0) <= 1 and int(v1) < 8:\n        raise ImportError(\"paddlepaddle>=1.8.0 is required\")\n\n    import paddle2onnx as p2o\n    # convert model save with 'paddle.fluid.io.save_inference_model'\n    if hasattr(paddle, 'enable_static'):\n        paddle.enable_static()\n    exe = fluid.Executor(fluid.CPUPlace())\n    if model_filename is None and params_filename is None:\n        [program, feed_var_names, fetch_vars] = fluid.io.load_inference_model(\n            model_dir, exe)\n    else:\n        [program, feed_var_names, fetch_vars] = fluid.io.load_inference_model(\n            model_dir,\n            exe,\n            model_filename=model_filename,\n            params_filename=params_filename)\n\n    OP_WITHOUT_KERNEL_SET = {\n        'feed', 'fetch', 'recurrent', 'go', 'rnn_memory_helper_grad',\n        'conditional_block', 'while', 'send', 'recv', 'listen_and_serv',\n        'fl_listen_and_serv', 'ncclInit', 'select', 'checkpoint_notify',\n        'gen_bkcl_id', 'c_gen_bkcl_id', 'gen_nccl_id', 'c_gen_nccl_id',\n        'c_comm_init', 'c_sync_calc_stream', 'c_sync_comm_stream',\n        'queue_generator', 'dequeue', 'enqueue', 'heter_listen_and_serv',\n        'c_wait_comm', 'c_wait_compute', 'c_gen_hccl_id', 'c_comm_init_hccl',\n        'copy_cross_scope'\n    }\n    if input_shape_dict is not None:\n        import paddle2onnx\n        paddle2onnx.legacy.process_old_ops_desc(program)\n        paddle_version = paddle.__version__\n        model_version = program.desc._version()\n        major_ver = model_version // 1000000\n        minor_ver = (model_version - major_ver * 1000000) // 1000\n        patch_ver = model_version - major_ver * 1000000 - minor_ver * 1000\n        model_version = \"{}.{}.{}\".format(major_ver, minor_ver, patch_ver)\n        if model_version != paddle_version:\n            logging.warning(\n                \"The model is saved by paddlepaddle v{}, but now your paddlepaddle is version of {}, this difference may cause error, it is recommend you reinstall a same version of paddlepaddle for this model\".\n                format(model_version, paddle_version))\n\n        for k, v in input_shape_dict.items():\n            program.blocks[0].var(k).desc.set_shape(v)\n        for i in range(len(program.blocks[0].ops)):\n            if program.blocks[0].ops[i].type in OP_WITHOUT_KERNEL_SET:\n                continue\n            program.blocks[0].ops[i].desc.infer_shape(program.blocks[0].desc)\n    p2o.program2onnx(\n        program,\n        fluid.global_scope(),\n        save_file,\n        feed_var_names=feed_var_names,\n        target_vars=fetch_vars,\n        opset_version=opset_version,\n        enable_onnx_checker=enable_onnx_checker,\n        operator_export_type=operator_export_type,\n        auto_update_opset=auto_update_opset,\n        output_names=output_names)",
  "def main():\n    if len(sys.argv) < 2:\n        logging.info(\"Use \\\"paddle2onnx -h\\\" to print the help information\")\n        logging.info(\n            \"For more information, please follow our github repo below:\")\n        logging.info(\"Github: https://github.com/PaddlePaddle/paddle2onnx.git\")\n        return\n\n    parser = arg_parser()\n    args = parser.parse_args()\n\n    if args.version:\n        import paddle2onnx\n        logging.info(\"paddle2onnx-{} with python>=2.7, paddlepaddle>=1.8.0\".\n                     format(paddle2onnx.__version__))\n        return\n\n    assert args.model_dir is not None, \"--model_dir should be defined while translating paddle model to onnx\"\n    assert args.save_file is not None, \"--save_file should be defined while translating paddle model to onnx\"\n\n    input_shape_dict = eval(args.input_shape_dict)\n\n    operator_export_type = \"ONNX\"\n    if args.enable_paddle_fallback:\n        operator_export_type = \"PaddleFallback\"\n\n    if args.output_names is not None:\n        if not isinstance(args.output_names, (list, dict)):\n            raise TypeError(\n                \"The output_names should be 'list' or 'dict', but received type is %s.\"\n                % type(args.output_names))\n\n    if args.enable_dev_version:\n        if args.enable_paddle_fallback:\n            logging.warn(\n                \"--enable_paddle_fallback is deprecated while --enable_dev_version=True.\"\n            )\n        if args.output_names is not None:\n            logging.warn(\n                \"--output_names is deprecated while --enable_dev_version=True.\")\n        if input_shape_dict is not None:\n            logging.warn(\n                \"--input_shape_dict is deprecated while --enable_dev_version=True.\"\n            )\n        model_file = os.path.join(args.model_dir, args.model_filename)\n        if args.params_filename is None:\n            params_file = \"\"\n        else:\n            params_file = os.path.join(args.model_dir, args.params_filename)\n        return c_paddle_to_onnx(\n            model_file=model_file,\n            params_file=params_file,\n            save_file=args.save_file,\n            opset_version=args.opset_version,\n            auto_upgrade_opset=args.enable_auto_update_opset,\n            verbose=True,\n            enable_onnx_checker=args.enable_onnx_checker,\n            enable_experimental_op=True,\n            enable_optimize=True)\n\n    program2onnx(\n        args.model_dir,\n        args.save_file,\n        args.model_filename,\n        args.params_filename,\n        opset_version=args.opset_version,\n        enable_onnx_checker=args.enable_onnx_checker,\n        operator_export_type=operator_export_type,\n        input_shape_dict=input_shape_dict,\n        output_names=args.output_names,\n        auto_update_opset=args.enable_auto_update_opset)",
  "def process_old_ops_desc(model):\n    for i in range(len(model.blocks[0].ops)):\n        if model.blocks[0].ops[i].type == \"matmul\":\n            if not model.blocks[0].ops[i].has_attr(\"head_number\"):\n                model.blocks[0].ops[i]._set_attr(\"head_number\", 1)\n        elif model.blocks[0].ops[i].type == \"yolo_box\":\n            if not model.blocks[0].ops[i].has_attr(\"iou_aware\"):\n                model.blocks[0].ops[i]._set_attr(\"iou_aware\", False)\n            if not model.blocks[0].ops[i].has_attr(\"iou_aware_factor\"):\n                model.blocks[0].ops[i]._set_attr(\"iou_aware_factor\", 0.5)",
  "def get_all_registered_ops(save_file=None):\n    ops = list(OpMapper.OPSETS.keys())\n    logging.warning(\"The number of all registered OPs is: {}\".format(len(ops)))\n    if save_file is None:\n        return\n    with open(save_file, \"w\") as f:\n        logging.warning(\"All registered OPs will be written to the file: {}\".\n                        format(save_file))\n        f.write(\"Total OPs num: {} \\n\".format(len(ops)))\n        for index in range(len(ops)):\n            op = ops[index]\n            f.write(str(index + 1) + \". \" + op + \"\\n\")\n        return",
  "def run_convert(model, input_shape_dict=None, scope=None, opset_version=9):\n    paddle_version = paddle.__version__\n    if isinstance(model, paddle.static.Program):\n        process_old_ops_desc(model)\n        if input_shape_dict is not None:\n            model_version = model.desc._version()\n            major_ver = model_version // 1000000\n            minor_ver = (model_version - major_ver * 1000000) // 1000\n            patch_ver = model_version - major_ver * 1000000 - minor_ver * 1000\n            model_version = \"{}.{}.{}\".format(major_ver, minor_ver, patch_ver)\n            if model_version != paddle_version:\n                logging.warning(\n                    \"The model is saved by paddlepaddle v{}, but now your paddlepaddle is version of {}, this difference may cause error, it is recommend you reinstall a same version of paddlepaddle for this model\".\n                    format(model_version, paddle_version))\n            for k, v in input_shape_dict.items():\n                model.blocks[0].var(k).desc.set_shape(v)\n            for i in range(len(model.blocks[0].ops)):\n                if model.blocks[0].ops[i].type in OP_WITHOUT_KERNEL_SET:\n                    continue\n                model.blocks[0].ops[i].desc.infer_shape(model.blocks[0].desc)\n        if scope is None:\n            scope = paddle.static.global_scope()\n        input_names = list()\n        output_vars = list()\n        for i in range(len(model.blocks[0].ops)):\n            if model.blocks[0].ops[i].type == \"feed\":\n                input_names.append(model.blocks[0].ops[i].output(\"Out\")[0])\n            if model.blocks[0].ops[i].type == \"fetch\":\n                output_vars.append(model.blocks[0].var(model.blocks[0].ops[i]\n                                                       .input(\"X\")[0]))\n        return program2onnx(\n            model,\n            scope,\n            save_file=None,\n            feed_var_names=input_names,\n            target_vars=output_vars,\n            opset_version=opset_version,\n            enable_onnx_checker=True)\n    elif isinstance(model, paddle.jit.TranslatedLayer):\n        process_old_ops_desc(model.program())\n        model_version = model.program().desc._version()\n        major_ver = model_version // 1000000\n        minor_ver = (model_version - major_ver * 1000000) // 1000\n        patch_ver = model_version - major_ver * 1000000 - minor_ver * 1000\n        model_version = \"{}.{}.{}\".format(major_ver, minor_ver, patch_ver)\n        if model_version != paddle_version:\n            logging.warning(\n                \"The model is saved by paddlepaddle v{}, but now your paddlepaddle is version of {}, this difference may cause error, it is recommend you reinstall a same version of paddlepaddle for this model\".\n                format(model_version, paddle_version))\n\n        if input_shape_dict is not None:\n            for k, v in input_shape_dict.items():\n                model.program().blocks[0].var(k).desc.set_shape(v)\n            for i in range(len(model.program().blocks[0].ops)):\n                if model.program().blocks[0].ops[\n                        i].type in OP_WITHOUT_KERNEL_SET:\n                    continue\n                model.program().blocks[0].ops[i].desc.infer_shape(model.program(\n                ).blocks[0].desc)\n        return dygraph2onnx(model, save_file=None, opset_version=opset_version)\n    else:\n        raise Exception(\n            \"Only support model loaded from paddle.static.load_inference_model() or paddle.jit.load()\"\n        )",
  "def export_onnx(paddle_graph,\n                save_file,\n                opset_version=9,\n                enable_onnx_checker=False,\n                operator_export_type=\"ONNX\",\n                verbose=False,\n                auto_update_opset=True,\n                output_names=None):\n    onnx_graph = ONNXGraph.build(paddle_graph, opset_version,\n                                 operator_export_type, verbose,\n                                 auto_update_opset)\n    onnx_graph = PassManager.run_pass(\n        onnx_graph, ['dumplicate_names_pass', 'inplace_node_pass'])\n    onnx_proto = onnx_graph.export_proto(enable_onnx_checker, output_names)\n\n    if save_file is None:\n        return onnx_proto\n\n    path, _ = os.path.split(save_file)\n    if path != '' and not os.path.isdir(path):\n        os.makedirs(path)\n    with open(save_file, 'wb') as f:\n        f.write(onnx_proto.SerializeToString())\n    logging.info(\"ONNX model saved in {}\".format(save_file))",
  "def program2onnx(program,\n                 scope,\n                 save_file,\n                 feed_var_names=None,\n                 target_vars=None,\n                 opset_version=9,\n                 enable_onnx_checker=False,\n                 operator_export_type=\"ONNX\",\n                 auto_update_opset=True,\n                 **configs):\n    from paddle import fluid\n    if hasattr(paddle, 'enable_static'):\n        paddle.enable_static()\n    if isinstance(program, paddle.fluid.framework.Program):\n        if feed_var_names is not None:\n            if isinstance(feed_var_names, six.string_types):\n                feed_var_names = [feed_var_names]\n            else:\n                if not (bool(feed_var_names) and all(\n                        isinstance(name, six.string_types)\n                        for name in feed_var_names)):\n                    raise TypeError(\"'feed_var_names' should be a list of str.\")\n\n        if target_vars is not None:\n            if isinstance(target_vars, Variable):\n                target_vars = [target_vars]\n            else:\n                if not (bool(target_vars) and\n                        all(isinstance(var, Variable) for var in target_vars)):\n                    raise TypeError(\n                        \"'target_vars' should be a list of variable.\")\n\n        paddle_graph = PaddleGraph.build_from_program(program, feed_var_names,\n                                                      target_vars, scope)\n        output_names = None\n        if 'output_names' in configs:\n            output_names = configs['output_names']\n            if output_names is not None and not isinstance(output_names,\n                                                           (list, dict)):\n                raise TypeError(\n                    \"The output_names should be 'list' or dict, but received type is %s.\"\n                    % type(output_names))\n        return export_onnx(\n            paddle_graph,\n            save_file,\n            opset_version,\n            enable_onnx_checker,\n            operator_export_type,\n            auto_update_opset=auto_update_opset,\n            output_names=output_names)\n    else:\n        raise TypeError(\n            \"the input 'program' should be 'Program', but received type is %s.\"\n            % type(program))",
  "def dygraph2onnx(layer, save_file, input_spec=None, opset_version=9, **configs):\n    from paddle.nn import Layer\n    from paddle.fluid import core\n    from paddle.fluid.framework import Variable\n    from paddle.fluid.dygraph.dygraph_to_static import program_translator\n    from paddle.fluid import dygraph\n    if not isinstance(layer, Layer):\n        raise TypeError(\n            \"the input 'layer' should be 'Layer', 'TranslatedLayer', but received type is %s.\"\n            % type(layer))\n\n    inner_input_spec = None\n    if input_spec is not None:\n        if not isinstance(input_spec, list):\n            raise TypeError(\n                \"The input input_spec should be 'list', but received type is %s.\"\n                % type(input_spec))\n        inner_input_spec = []\n        for var in input_spec:\n            if isinstance(var, paddle.static.InputSpec):\n                inner_input_spec.append(var)\n            elif isinstance(var, (core.VarBase, Variable)):\n                inner_input_spec.append(\n                    paddle.static.InputSpec.from_tensor(var))\n            else:\n                raise TypeError(\n                    \"The element in input_spec list should be 'Variable' or `paddle.static.InputSpec`, but received element's type is %s.\"\n                    % type(var))\n\n    output_spec = None\n    if 'output_spec' in configs:\n        output_spec = configs['output_spec']\n        if not isinstance(output_spec, list):\n            raise TypeError(\n                \"The output_spec should be 'list', but received type is %s.\" %\n                type(output_spec))\n        for var in output_spec:\n            if not isinstance(var, (core.VarBase, Variable)):\n                raise TypeError(\n                    \"The element in output_spec list should be 'Variable', but received element's type is %s.\"\n                    % type(var))\n\n    verbose = False\n    if 'verbose' in configs:\n        if isinstance(configs['verbose'], bool):\n            verbose = configs['verbose']\n        else:\n            raise TypeError(\n                \"The verbose should be 'bool', but received type is %s.\" %\n                type(configs['verbose']))\n\n    enable_onnx_checker = False\n    if 'enable_onnx_checker' in configs:\n        if isinstance(configs['enable_onnx_checker'], bool):\n            enable_onnx_checker = configs['enable_onnx_checker']\n        else:\n            raise TypeError(\n                \"The 'enable_onnx_checker' should be 'bool', but received type is %s.\"\n                % type(configs['enable_onnx_checker']))\n\n    operator_export_type = \"ONNX\"\n    enable_paddle_fallback = False\n    if 'enable_paddle_fallback' in configs:\n        if isinstance(configs['enable_paddle_fallback'], bool):\n            enable_paddle_fallback = configs['enable_paddle_fallback']\n            if enable_paddle_fallback:\n                operator_export_type = \"PaddleFallback\"\n        else:\n            raise TypeError(\n                \"The 'enable_paddle_fallback' should be 'bool', but received type is %s.\"\n                % type(configs['enable_paddle_fallback']))\n\n    paddle_graph = PaddleGraph.build_from_dygraph(layer, inner_input_spec,\n                                                  output_spec)\n\n    if 'get_paddle_graph' in configs:\n        return paddle_graph\n\n    auto_update_opset = True\n    if 'auto_update_opset' in configs:\n        if isinstance(configs['auto_update_opset'], bool):\n            auto_update_opset = configs['auto_update_opset']\n        else:\n            raise TypeError(\n                \"The auto_update_opset should be 'bool', but received type is %s.\"\n                % type(configs['auto_update_opset']))\n\n    output_names = None\n    if 'output_names' in configs:\n        output_names = configs['output_names']\n        if not isinstance(output_names, (list, dict)):\n            raise TypeError(\n                \"The output_names should be 'list' or dict, but received type is %s.\"\n                % type(output_names))\n\n    return export_onnx(paddle_graph, save_file, opset_version,\n                       enable_onnx_checker, operator_export_type, verbose,\n                       auto_update_opset, output_names)",
  "class NodeDomain():\n    ONNX = 'onnx'\n    PADDLE = 'paddle'\n    CUSTOM = 'custom'\n    RAW = 'raw'",
  "class PassManager(object):\n    PASSES = {}\n\n    def __init__(self, name, **kwargs):\n        self.name = name\n        self.kwargs = kwargs\n\n    def __call__(self, cls):\n        for k, v in inspect.getmembers(cls, inspect.ismethod):\n            if k == 'run_pass':\n                self.PASSES[self.name] = (v, self.kwargs)\n\n    @staticmethod\n    def run_pass(graph, custom_pass_list):\n        for pass_name in custom_pass_list:\n            try:\n                pass_func, kw = PassManager.PASSES[pass_name]\n                pass_func(graph, **kw)\n            except:\n                raise Exception(\"Error happened when excute pass: {}\".format(\n                    pass_name))\n        return graph",
  "def __init__(self, name, **kwargs):\n        self.name = name\n        self.kwargs = kwargs",
  "def __call__(self, cls):\n        for k, v in inspect.getmembers(cls, inspect.ismethod):\n            if k == 'run_pass':\n                self.PASSES[self.name] = (v, self.kwargs)",
  "def run_pass(graph, custom_pass_list):\n        for pass_name in custom_pass_list:\n            try:\n                pass_func, kw = PassManager.PASSES[pass_name]\n                pass_func(graph, **kw)\n            except:\n                raise Exception(\"Error happened when excute pass: {}\".format(\n                    pass_name))\n        return graph",
  "def get_repeated_output(inputs, outputs):\n    repeated_output = {}\n    for idx in range(len(outputs)):\n        opt = outputs[idx]\n        if opt in inputs:\n            repeated_output[opt] = idx\n    return repeated_output",
  "class InplaceNodePass(object):\n\n    name_count = dict()\n\n    @classmethod\n    def generate_new_name(cls, name):\n        if name in cls.name_count:\n            cls.name_count[name] += 1\n        else:\n            cls.name_count[name] = 1\n        new_name = name + '.' + str(cls.name_count[name])\n        return new_name\n\n    @classmethod\n    def run_pass(cls, onnx_graph):\n        node_map = list(onnx_graph.node_map.items())\n        name_mapping = {}\n        for idx in range(len(node_map)):\n            name, node = node_map[idx]\n            inputs = node.inputs\n            outputs = node.outputs\n            for idx in range(len(inputs)):\n                ipt = inputs[idx]\n                if ipt in name_mapping:\n                    inputs[idx] = name_mapping[ipt]\n            repeated_output = get_repeated_output(inputs, outputs)\n            if len(repeated_output) != 0:\n                for opt, idx in repeated_output.items():\n                    name_mapping[opt] = cls.generate_new_name(opt)\n                    outputs[idx] = name_mapping[opt]\n            node.set_inputs(inputs)\n            node.set_outputs(outputs)\n            onnx_graph.update_node(node)\n\n        return onnx_graph",
  "def generate_new_name(cls, name):\n        if name in cls.name_count:\n            cls.name_count[name] += 1\n        else:\n            cls.name_count[name] = 1\n        new_name = name + '.' + str(cls.name_count[name])\n        return new_name",
  "def run_pass(cls, onnx_graph):\n        node_map = list(onnx_graph.node_map.items())\n        name_mapping = {}\n        for idx in range(len(node_map)):\n            name, node = node_map[idx]\n            inputs = node.inputs\n            outputs = node.outputs\n            for idx in range(len(inputs)):\n                ipt = inputs[idx]\n                if ipt in name_mapping:\n                    inputs[idx] = name_mapping[ipt]\n            repeated_output = get_repeated_output(inputs, outputs)\n            if len(repeated_output) != 0:\n                for opt, idx in repeated_output.items():\n                    name_mapping[opt] = cls.generate_new_name(opt)\n                    outputs[idx] = name_mapping[opt]\n            node.set_inputs(inputs)\n            node.set_outputs(outputs)\n            onnx_graph.update_node(node)\n\n        return onnx_graph",
  "class DumplicateNamesPass(object):\n\n    name_count = dict()\n\n    @classmethod\n    def generate_new_name(cls, name):\n        for saved_name in cls.name_count:\n            if name.startswith(saved_name):\n                cls.name_count[saved_name] += 1\n                new_name = saved_name + '.' + str(cls.name_count[saved_name])\n                return new_name\n        cls.name_count[name] = 1\n        new_name = name + '.' + str(cls.name_count[name])\n        return new_name\n\n    @classmethod\n    def run_pass(cls, onnx_graph):\n        renamer = {}\n        tensor_names = set()\n        for name, node in onnx_graph.parameters.items():\n            output = node.output\n            for opt in output:\n                assert opt not in tensor_names, \"There's dumplicate names in parameters.\"\n                tensor_names.add(opt)\n\n        for ipt in onnx_graph.input_nodes:\n            assert ipt.name not in tensor_names, \"There's dumplicate names in exported parameters and inputs.\"\n            tensor_names.add(ipt.name)\n\n        for name, node in onnx_graph.node_map.items():\n            inputs = node.inputs\n            outputs = node.outputs\n            update_node = False\n            for idx in range(len(inputs)):\n                ipt = inputs[idx]\n                if ipt not in renamer:\n                    continue\n                updated_name = renamer[ipt]\n                while updated_name in renamer:\n                    updated_name = renamer[updated_name]\n                inputs[idx] = updated_name\n                update_node = True\n\n            for idx in range(len(outputs)):\n                opt = outputs[idx]\n                if opt not in tensor_names:\n                    tensor_names.add(opt)\n                    continue\n                renamed_tensor_name = opt\n                while renamed_tensor_name in renamer:\n                    renamed_tensor_name = renamer[renamed_tensor_name]\n                new_name = cls.generate_new_name(renamed_tensor_name)\n                logging.warning(\"[Renamer Pass] Will rename {}, to {}\".format(\n                    renamed_tensor_name, new_name))\n                outputs[idx] = new_name\n                update_node = True\n                renamer[renamed_tensor_name] = new_name\n\n            if update_node:\n                node.set_inputs(inputs)\n                node.set_outputs(outputs)\n                onnx_graph.update_node(node)\n\n        for opt in onnx_graph.output_nodes:\n            if opt.name not in renamer:\n                continue\n            updated_name = renamer[opt.name]\n            while updated_name in renamer:\n                updated_name = renamer[updated_name]\n            opt.name = updated_name\n\n        return onnx_graph",
  "def generate_new_name(cls, name):\n        for saved_name in cls.name_count:\n            if name.startswith(saved_name):\n                cls.name_count[saved_name] += 1\n                new_name = saved_name + '.' + str(cls.name_count[saved_name])\n                return new_name\n        cls.name_count[name] = 1\n        new_name = name + '.' + str(cls.name_count[name])\n        return new_name",
  "def run_pass(cls, onnx_graph):\n        renamer = {}\n        tensor_names = set()\n        for name, node in onnx_graph.parameters.items():\n            output = node.output\n            for opt in output:\n                assert opt not in tensor_names, \"There's dumplicate names in parameters.\"\n                tensor_names.add(opt)\n\n        for ipt in onnx_graph.input_nodes:\n            assert ipt.name not in tensor_names, \"There's dumplicate names in exported parameters and inputs.\"\n            tensor_names.add(ipt.name)\n\n        for name, node in onnx_graph.node_map.items():\n            inputs = node.inputs\n            outputs = node.outputs\n            update_node = False\n            for idx in range(len(inputs)):\n                ipt = inputs[idx]\n                if ipt not in renamer:\n                    continue\n                updated_name = renamer[ipt]\n                while updated_name in renamer:\n                    updated_name = renamer[updated_name]\n                inputs[idx] = updated_name\n                update_node = True\n\n            for idx in range(len(outputs)):\n                opt = outputs[idx]\n                if opt not in tensor_names:\n                    tensor_names.add(opt)\n                    continue\n                renamed_tensor_name = opt\n                while renamed_tensor_name in renamer:\n                    renamed_tensor_name = renamer[renamed_tensor_name]\n                new_name = cls.generate_new_name(renamed_tensor_name)\n                logging.warning(\"[Renamer Pass] Will rename {}, to {}\".format(\n                    renamed_tensor_name, new_name))\n                outputs[idx] = new_name\n                update_node = True\n                renamer[renamed_tensor_name] = new_name\n\n            if update_node:\n                node.set_inputs(inputs)\n                node.set_outputs(outputs)\n                onnx_graph.update_node(node)\n\n        for opt in onnx_graph.output_nodes:\n            if opt.name not in renamer:\n                continue\n            updated_name = renamer[opt.name]\n            while updated_name in renamer:\n                updated_name = renamer[updated_name]\n            opt.name = updated_name\n\n        return onnx_graph",
  "class ONNXNode(Node):\n    def __init__(self, op_type, inputs, outputs, attrs, layer_name, domain):\n        super(ONNXNode, self).__init__(op_type, inputs, outputs, attrs,\n                                       layer_name, domain)\n        self.domain = domain\n        self.onnx_node = self.make_onnx_node()\n\n    def make_onnx_constant_node(self):\n        dtype = self.attr('dtype')\n        value = self.attr('value')\n        if isinstance(value, list):\n            dims = (len(value), )\n        elif value is None:\n            dims = ()\n            value = []\n        else:\n            dims = ()\n            value = [value]\n\n        if 'dims' in self.attrs:\n            dims = self.attrs['dims']\n\n        tensor = helper.make_tensor(\n            name=self.layer_name, data_type=dtype, dims=dims, vals=value)\n\n        onnx_node = helper.make_node(\n            self.type, inputs=self.inputs, outputs=self.outputs, value=tensor)\n\n        return onnx_node\n\n    def make_onnx_node(self):\n        if self.type in ['Constant', 'ConstantOfShape']:\n            onnx_node = self.make_onnx_constant_node()\n        else:\n            onnx_node = helper.make_node(\n                self.type,\n                inputs=self.inputs,\n                outputs=self.outputs,\n                name=self.layer_name,\n                domain=self.domain,\n                **self.attrs)\n        return onnx_node",
  "class ONNXGraph(Graph):\n    def __init__(self,\n                 paddle_graph,\n                 opset_version,\n                 operator_export_type=\"ONNX\",\n                 block=None,\n                 auto_update_opset=True):\n        super(ONNXGraph, self).__init__()\n        self.opset_version = opset_version\n        self.operator_export_type = operator_export_type\n        self.ctx = paddle_graph\n        self.custom = []\n        if auto_update_opset:\n            self.update_opset_version()\n\n    def __str__(self):\n        graph_str = 'graph { \\n'\n        for node in self.input_nodes:\n            graph_str += \" input: {} \\n\".format(node)\n        for node in self.output_nodes:\n            graph_str += \" output: {} \\n \\n\".format(node)\n        for name, node in self.node_map.items():\n            graph_str += node.__str__()\n        graph_str += ' }'\n        return graph_str\n\n    def make_node(self,\n                  op_type,\n                  inputs=[],\n                  outputs=[],\n                  attrs=None,\n                  layer_name=None,\n                  domain=None,\n                  **kw):\n        if layer_name is None:\n            layer_name = self.generate_node_name(op_type)\n\n        if domain is not None:\n            if domain not in self.custom:\n                self.custom.append(domain)\n\n        if attrs is None:\n            attrs = kw\n        attrs.update(kw)\n\n        if inputs is None:\n            inputs = []\n\n        real_outputs = None\n        if outputs is None:\n            real_outputs = [layer_name]\n        elif isinstance(outputs, int):\n            real_outputs = []\n            for i in range(outputs):\n                real_outputs.append(self.generate_node_name(op_type))\n        elif isinstance(outputs, list):\n            real_outputs = []\n            if len(outputs) == 0:\n                real_outputs = [layer_name]\n            else:\n                for opt in outputs:\n                    if isinstance(opt, Node):\n                        real_outputs.append(opt.layer_name)\n                    elif isinstance(opt, int):\n                        real_outputs.append(self.generate_node_name(op_type))\n                    else:\n                        real_outputs.append(opt)\n        else:\n            real_outputs = outputs\n\n        node = ONNXNode(op_type, inputs, real_outputs, attrs, layer_name,\n                        domain)\n\n        self.insert_node(node)\n        if len(node.outputs) == 1:\n            return node.outputs[0]\n        else:\n            return node.outputs\n\n    def update_node(self,\n                    node,\n                    op_type=None,\n                    inputs=None,\n                    outputs=None,\n                    attrs=None,\n                    **kw):\n        if op_type is None:\n            op_type = node.type\n        if inputs is None:\n            inputs = node.inputs\n        if outputs is None:\n            outputs = node.outputs\n        if attrs is None:\n            attrs = node.attrs\n        attrs.update(kw)\n\n        node = ONNXNode(op_type, inputs, outputs, attrs, node.layer_name,\n                        node.domain)\n        self.insert_node(node)\n        return node\n\n    def build_parameters(self, parameters):\n        # build weight nodes\n        for name, param in parameters.items():\n            weight = param['data']\n            if weight is not np.ndarray:\n                weight = np.array(weight)\n            tensor = helper.make_tensor(\n                name=name,\n                dims=param['shape'],\n                data_type=dtypes.DTYPE_PADDLE_ONNX_MAP[param['dtype']],\n                vals=weight.flatten().tolist())\n            node = helper.make_node(\n                'Constant', inputs=[], outputs=[name], value=tensor)\n            self.parameters[name] = node\n\n    def build_input_nodes(self, input_nodes):\n        # build input nodes\n        for ipt in input_nodes:\n            self.add_input_node(ipt.layer_name,\n                                ipt.attr('shape'), ipt.attr('dtype'))\n\n    def build_output_nodes(self, output_nodes):\n        # build output nodes\n        for opt in output_nodes:\n            self.add_output_node(opt.layer_name,\n                                 opt.attr('shape'), opt.attr('dtype'))\n\n    def update_opset_version(self):\n        node_map = self.ctx.node_map\n        self.opset_version = OpMapper.get_recommend_opset_version(\n            node_map, self.opset_version)\n\n    def build_op_nodes(self, node_map):\n        OpMapper.check_support_status(node_map, self.opset_version)\n        # build op nodes\n        for name, node in list(node_map.items()):\n            OpMapper.mapping(self, node, self.operator_export_type)\n\n    def make_value_info(self, name, shape, dtype):\n        tensor_info = helper.make_tensor_value_info(\n            name=name,\n            shape=shape,\n            elem_type=dtypes.DTYPE_PADDLE_ONNX_MAP[dtype])\n        return tensor_info\n\n    def add_input_node(self, name, shape, dtype):\n        vi = self.make_value_info(name, shape, dtype)\n        self.input_nodes.append(vi)\n\n    def add_output_node(self, name, shape, dtype):\n        vi = self.make_value_info(name, shape, dtype)\n        self.output_nodes.append(vi)\n\n    def find_index(self, node_inout, name):\n        for i in range(len(node_inout)):\n            if node_inout[i] == name:\n                return i\n        return -1\n\n    def change_output_names(self, onnx_proto, output_names):\n        logging.info(\"The output of the ONNX model is set to: {}\".format(\n            output_names))\n        if isinstance(output_names, list):\n            assert len(output_names) == len(\n                onnx_proto.graph.output\n            ), \"The provided output names are inconsistent with the output number of the onnx model when output_names is list\"\n            origin_output_names = []\n            for i in range(len(onnx_proto.graph.output)):\n                origin_output_names.append(onnx_proto.graph.output[i].name)\n                onnx_proto.graph.output[i].name = output_names[i]\n\n            for i in range(len(onnx_proto.graph.node)):\n                node = onnx_proto.graph.node[i]\n                # Prevent changed names from being changed again\n                output_visited_node = []\n                input_visited_node = []\n                for j in range(len(origin_output_names)):\n                    if origin_output_names[j] in node.output:\n                        index = self.find_index(node.output,\n                                                origin_output_names[j])\n                        if index in output_visited_node:\n                            continue\n                        output_visited_node.append(index)\n                        onnx_proto.graph.node[i].output[index] = output_names[j]\n                    if origin_output_names[j] in node.input:\n                        index = self.find_index(node.input,\n                                                origin_output_names[j])\n                        if index in input_visited_node:\n                            continue\n                        input_visited_node.append(index)\n                        onnx_proto.graph.node[i].input[index] = output_names[j]\n        if isinstance(output_names, dict):\n            for i in range(len(onnx_proto.graph.output)):\n                for key, value in output_names.items():\n                    if onnx_proto.graph.output[i].name == key:\n                        onnx_proto.graph.output[i].name = value\n                        break\n\n            for i in range(len(onnx_proto.graph.node)):\n                node = onnx_proto.graph.node[i]\n                # Prevent changed names from being changed again\n                output_visited_node = []\n                input_visited_node = []\n                for key, value in output_names.items():\n                    if key in node.output:\n                        index = self.find_index(node.output, key)\n                        if index in output_visited_node:\n                            continue\n                        output_visited_node.append(index)\n                        onnx_proto.graph.node[i].output[index] = value\n                    if key in node.input:\n                        index = self.find_index(node.input, key)\n                        if index in input_visited_node:\n                            continue\n                        input_visited_node.append(index)\n                        onnx_proto.graph.node[i].input[index] = value\n\n        return onnx_proto\n\n    def export_proto(self, enable_onnx_checker=False, output_names=None):\n\n        op_nodes = [node.onnx_node for node in self.node_map.values()]\n        weight_nodes = [node for node in self.parameters.values()]\n\n        onnx_graph = helper.make_graph(\n            nodes=weight_nodes + op_nodes,\n            name='paddle-onnx',\n            initializer=[],\n            inputs=self.input_nodes,\n            outputs=self.output_nodes)\n\n        opset_imports = [helper.make_opsetid(\"\", self.opset_version)]\n        for custom_domain in self.custom:\n            opset_imports.append(helper.make_opsetid(custom_domain, 1))\n        onnx_proto = helper.make_model(\n            onnx_graph, producer_name=PRODUCER, opset_imports=opset_imports)\n        if output_names is not None:\n            onnx_proto = self.change_output_names(onnx_proto, output_names)\n\n        if enable_onnx_checker:\n            check_model(onnx_proto)\n\n        return onnx_proto\n\n    @staticmethod\n    def build(paddle_graph,\n              opset_version,\n              operator_export_type=\"ONNX\",\n              verbose=False,\n              auto_update_opset=True):\n        onnx_graph = ONNXGraph(\n            paddle_graph,\n            opset_version=opset_version,\n            operator_export_type=operator_export_type,\n            auto_update_opset=auto_update_opset)\n        onnx_graph.build_parameters(paddle_graph.parameters)\n        onnx_graph.build_input_nodes(paddle_graph.input_nodes)\n        onnx_graph.build_output_nodes(paddle_graph.output_nodes)\n        onnx_graph.build_op_nodes(paddle_graph.node_map)\n\n        return onnx_graph",
  "def __init__(self, op_type, inputs, outputs, attrs, layer_name, domain):\n        super(ONNXNode, self).__init__(op_type, inputs, outputs, attrs,\n                                       layer_name, domain)\n        self.domain = domain\n        self.onnx_node = self.make_onnx_node()",
  "def make_onnx_constant_node(self):\n        dtype = self.attr('dtype')\n        value = self.attr('value')\n        if isinstance(value, list):\n            dims = (len(value), )\n        elif value is None:\n            dims = ()\n            value = []\n        else:\n            dims = ()\n            value = [value]\n\n        if 'dims' in self.attrs:\n            dims = self.attrs['dims']\n\n        tensor = helper.make_tensor(\n            name=self.layer_name, data_type=dtype, dims=dims, vals=value)\n\n        onnx_node = helper.make_node(\n            self.type, inputs=self.inputs, outputs=self.outputs, value=tensor)\n\n        return onnx_node",
  "def make_onnx_node(self):\n        if self.type in ['Constant', 'ConstantOfShape']:\n            onnx_node = self.make_onnx_constant_node()\n        else:\n            onnx_node = helper.make_node(\n                self.type,\n                inputs=self.inputs,\n                outputs=self.outputs,\n                name=self.layer_name,\n                domain=self.domain,\n                **self.attrs)\n        return onnx_node",
  "def __init__(self,\n                 paddle_graph,\n                 opset_version,\n                 operator_export_type=\"ONNX\",\n                 block=None,\n                 auto_update_opset=True):\n        super(ONNXGraph, self).__init__()\n        self.opset_version = opset_version\n        self.operator_export_type = operator_export_type\n        self.ctx = paddle_graph\n        self.custom = []\n        if auto_update_opset:\n            self.update_opset_version()",
  "def __str__(self):\n        graph_str = 'graph { \\n'\n        for node in self.input_nodes:\n            graph_str += \" input: {} \\n\".format(node)\n        for node in self.output_nodes:\n            graph_str += \" output: {} \\n \\n\".format(node)\n        for name, node in self.node_map.items():\n            graph_str += node.__str__()\n        graph_str += ' }'\n        return graph_str",
  "def make_node(self,\n                  op_type,\n                  inputs=[],\n                  outputs=[],\n                  attrs=None,\n                  layer_name=None,\n                  domain=None,\n                  **kw):\n        if layer_name is None:\n            layer_name = self.generate_node_name(op_type)\n\n        if domain is not None:\n            if domain not in self.custom:\n                self.custom.append(domain)\n\n        if attrs is None:\n            attrs = kw\n        attrs.update(kw)\n\n        if inputs is None:\n            inputs = []\n\n        real_outputs = None\n        if outputs is None:\n            real_outputs = [layer_name]\n        elif isinstance(outputs, int):\n            real_outputs = []\n            for i in range(outputs):\n                real_outputs.append(self.generate_node_name(op_type))\n        elif isinstance(outputs, list):\n            real_outputs = []\n            if len(outputs) == 0:\n                real_outputs = [layer_name]\n            else:\n                for opt in outputs:\n                    if isinstance(opt, Node):\n                        real_outputs.append(opt.layer_name)\n                    elif isinstance(opt, int):\n                        real_outputs.append(self.generate_node_name(op_type))\n                    else:\n                        real_outputs.append(opt)\n        else:\n            real_outputs = outputs\n\n        node = ONNXNode(op_type, inputs, real_outputs, attrs, layer_name,\n                        domain)\n\n        self.insert_node(node)\n        if len(node.outputs) == 1:\n            return node.outputs[0]\n        else:\n            return node.outputs",
  "def update_node(self,\n                    node,\n                    op_type=None,\n                    inputs=None,\n                    outputs=None,\n                    attrs=None,\n                    **kw):\n        if op_type is None:\n            op_type = node.type\n        if inputs is None:\n            inputs = node.inputs\n        if outputs is None:\n            outputs = node.outputs\n        if attrs is None:\n            attrs = node.attrs\n        attrs.update(kw)\n\n        node = ONNXNode(op_type, inputs, outputs, attrs, node.layer_name,\n                        node.domain)\n        self.insert_node(node)\n        return node",
  "def build_parameters(self, parameters):\n        # build weight nodes\n        for name, param in parameters.items():\n            weight = param['data']\n            if weight is not np.ndarray:\n                weight = np.array(weight)\n            tensor = helper.make_tensor(\n                name=name,\n                dims=param['shape'],\n                data_type=dtypes.DTYPE_PADDLE_ONNX_MAP[param['dtype']],\n                vals=weight.flatten().tolist())\n            node = helper.make_node(\n                'Constant', inputs=[], outputs=[name], value=tensor)\n            self.parameters[name] = node",
  "def build_input_nodes(self, input_nodes):\n        # build input nodes\n        for ipt in input_nodes:\n            self.add_input_node(ipt.layer_name,\n                                ipt.attr('shape'), ipt.attr('dtype'))",
  "def build_output_nodes(self, output_nodes):\n        # build output nodes\n        for opt in output_nodes:\n            self.add_output_node(opt.layer_name,\n                                 opt.attr('shape'), opt.attr('dtype'))",
  "def update_opset_version(self):\n        node_map = self.ctx.node_map\n        self.opset_version = OpMapper.get_recommend_opset_version(\n            node_map, self.opset_version)",
  "def build_op_nodes(self, node_map):\n        OpMapper.check_support_status(node_map, self.opset_version)\n        # build op nodes\n        for name, node in list(node_map.items()):\n            OpMapper.mapping(self, node, self.operator_export_type)",
  "def make_value_info(self, name, shape, dtype):\n        tensor_info = helper.make_tensor_value_info(\n            name=name,\n            shape=shape,\n            elem_type=dtypes.DTYPE_PADDLE_ONNX_MAP[dtype])\n        return tensor_info",
  "def add_input_node(self, name, shape, dtype):\n        vi = self.make_value_info(name, shape, dtype)\n        self.input_nodes.append(vi)",
  "def add_output_node(self, name, shape, dtype):\n        vi = self.make_value_info(name, shape, dtype)\n        self.output_nodes.append(vi)",
  "def find_index(self, node_inout, name):\n        for i in range(len(node_inout)):\n            if node_inout[i] == name:\n                return i\n        return -1",
  "def change_output_names(self, onnx_proto, output_names):\n        logging.info(\"The output of the ONNX model is set to: {}\".format(\n            output_names))\n        if isinstance(output_names, list):\n            assert len(output_names) == len(\n                onnx_proto.graph.output\n            ), \"The provided output names are inconsistent with the output number of the onnx model when output_names is list\"\n            origin_output_names = []\n            for i in range(len(onnx_proto.graph.output)):\n                origin_output_names.append(onnx_proto.graph.output[i].name)\n                onnx_proto.graph.output[i].name = output_names[i]\n\n            for i in range(len(onnx_proto.graph.node)):\n                node = onnx_proto.graph.node[i]\n                # Prevent changed names from being changed again\n                output_visited_node = []\n                input_visited_node = []\n                for j in range(len(origin_output_names)):\n                    if origin_output_names[j] in node.output:\n                        index = self.find_index(node.output,\n                                                origin_output_names[j])\n                        if index in output_visited_node:\n                            continue\n                        output_visited_node.append(index)\n                        onnx_proto.graph.node[i].output[index] = output_names[j]\n                    if origin_output_names[j] in node.input:\n                        index = self.find_index(node.input,\n                                                origin_output_names[j])\n                        if index in input_visited_node:\n                            continue\n                        input_visited_node.append(index)\n                        onnx_proto.graph.node[i].input[index] = output_names[j]\n        if isinstance(output_names, dict):\n            for i in range(len(onnx_proto.graph.output)):\n                for key, value in output_names.items():\n                    if onnx_proto.graph.output[i].name == key:\n                        onnx_proto.graph.output[i].name = value\n                        break\n\n            for i in range(len(onnx_proto.graph.node)):\n                node = onnx_proto.graph.node[i]\n                # Prevent changed names from being changed again\n                output_visited_node = []\n                input_visited_node = []\n                for key, value in output_names.items():\n                    if key in node.output:\n                        index = self.find_index(node.output, key)\n                        if index in output_visited_node:\n                            continue\n                        output_visited_node.append(index)\n                        onnx_proto.graph.node[i].output[index] = value\n                    if key in node.input:\n                        index = self.find_index(node.input, key)\n                        if index in input_visited_node:\n                            continue\n                        input_visited_node.append(index)\n                        onnx_proto.graph.node[i].input[index] = value\n\n        return onnx_proto",
  "def export_proto(self, enable_onnx_checker=False, output_names=None):\n\n        op_nodes = [node.onnx_node for node in self.node_map.values()]\n        weight_nodes = [node for node in self.parameters.values()]\n\n        onnx_graph = helper.make_graph(\n            nodes=weight_nodes + op_nodes,\n            name='paddle-onnx',\n            initializer=[],\n            inputs=self.input_nodes,\n            outputs=self.output_nodes)\n\n        opset_imports = [helper.make_opsetid(\"\", self.opset_version)]\n        for custom_domain in self.custom:\n            opset_imports.append(helper.make_opsetid(custom_domain, 1))\n        onnx_proto = helper.make_model(\n            onnx_graph, producer_name=PRODUCER, opset_imports=opset_imports)\n        if output_names is not None:\n            onnx_proto = self.change_output_names(onnx_proto, output_names)\n\n        if enable_onnx_checker:\n            check_model(onnx_proto)\n\n        return onnx_proto",
  "def build(paddle_graph,\n              opset_version,\n              operator_export_type=\"ONNX\",\n              verbose=False,\n              auto_update_opset=True):\n        onnx_graph = ONNXGraph(\n            paddle_graph,\n            opset_version=opset_version,\n            operator_export_type=operator_export_type,\n            auto_update_opset=auto_update_opset)\n        onnx_graph.build_parameters(paddle_graph.parameters)\n        onnx_graph.build_input_nodes(paddle_graph.input_nodes)\n        onnx_graph.build_output_nodes(paddle_graph.output_nodes)\n        onnx_graph.build_op_nodes(paddle_graph.node_map)\n\n        return onnx_graph",
  "def prepend_feed_ops(inference_program,\n                     feed_target_names,\n                     feed_holder_name='feed'):\n    if len(feed_target_names) == 0:\n        return\n    global_block = inference_program.global_block()\n    feed_var = global_block.create_var(\n        name=feed_holder_name,\n        type=core.VarDesc.VarType.FEED_MINIBATCH,\n        persistable=True)\n    for i, name in enumerate(feed_target_names):\n        if not global_block.has_var(name):\n            raise ValueError(\n                \"The feed_var_names[{i}]: '{name}' doesn't exist in pruned inference program. \"\n                \"Please check whether '{name}' is a valid feed_var name, or remove it from feed_var_names \"\n                \"if '{name}' is not involved in the fetch_vars calculation.\".\n                format(\n                    i=i, name=name))\n        out = global_block.var(name)\n        global_block._prepend_op(\n            type='feed',\n            inputs={'X': [feed_var]},\n            outputs={'Out': [out]},\n            attrs={'col': i})",
  "def append_fetch_ops(inference_program,\n                     fetch_target_names,\n                     fetch_holder_name='fetch'):\n    global_block = inference_program.global_block()\n    fetch_var = global_block.create_var(\n        name=fetch_holder_name,\n        type=core.VarDesc.VarType.FETCH_LIST,\n        persistable=True)\n    for i, name in enumerate(fetch_target_names):\n        global_block.append_op(\n            type='fetch',\n            inputs={'X': [name]},\n            outputs={'Out': [fetch_var]},\n            attrs={'col': i})",
  "def get_program(program, feed_var_names, fetch_vars):\n    global_block = program.global_block()\n    need_to_remove_op_index = []\n    for i, op in enumerate(global_block.ops):\n        op.desc.set_is_target(False)\n        if op.type == \"feed\" or op.type == \"fetch\":\n            need_to_remove_op_index.append(i)\n    for index in need_to_remove_op_index[::-1]:\n        global_block._remove_op(index)\n    program.desc.flush()\n    program = program._prune_with_input(\n        feeded_var_names=feed_var_names, targets=fetch_vars)\n    program = program._inference_optimize(prune_read_op=True)\n    fetch_var_names = [v.name for v in fetch_vars]\n    prepend_feed_ops(program, feed_var_names)\n    append_fetch_ops(program, fetch_var_names)\n    return program",
  "class Node(object):\n    def __init__(self,\n                 op_type,\n                 inputs,\n                 outputs,\n                 attrs,\n                 layer_name,\n                 domain=NodeDomain.RAW):\n        self.domain = domain\n        self.type = op_type\n        self.attrs = attrs\n        self.layer_name = layer_name\n        self.set_inputs(inputs)\n        self.set_outputs(outputs)\n\n    def __hash__(self):\n        return hash(self.layer_name)\n\n    def __eq__(self, other):\n        if self.layer_name == other.layer_name:\n            return True\n        return False\n\n    def __str__(self):\n        node_str = ''\n        attrs = ''\n        for key, value in self.attrs.items():\n            attrs += ', ' + key + '=' + str(value)\n        node_str += \"  {} = {}::{}(inputs={}{}) \\n\".format(\n            self.outputs, self.domain, self.type, self.inputs, attrs)\n        return node_str\n\n    def input(self, idx=None):\n        if idx is None:\n            return self.inputs\n        return self.inputs[idx]\n\n    def output(self, idx=None):\n        if idx is None:\n            return self.outputs\n        return self.outputs[idx]\n\n    def attr(self, name):\n        if name in self.attrs:\n            return self.attrs[name]\n        return None\n\n    def set_inputs(self, inputs):\n        if isinstance(inputs, list):\n            self.inputs = [\n                ipt.layer_name if isinstance(ipt, Node) else ipt\n                for ipt in inputs\n            ]\n        elif isinstance(inputs, six.string_types):\n            self.inputs = [inputs]\n        elif isinstance(inputs, Node):\n            self.inputs = [inputs.layer_name]\n        else:\n            raise TypeError(\n                'Inputs of node must be type: list, Node, or String but got {}'.\n                format(type(inputs)))\n\n    def set_outputs(self, outputs):\n        if isinstance(outputs, list):\n            self.outputs = [\n                opt.layer_name if isinstance(opt, Node) else opt\n                for opt in outputs\n            ]\n        elif isinstance(outputs, six.string_types):\n            self.outputs = [outputs]\n        elif isinstance(outputs, Node):\n            self.outputs = [outputs.layer_name]\n        else:\n            raise TypeError(\n                'Outputs of node must be type: list, Node, or String but got {}'.\n                format(type(outputs)))",
  "class Graph(object):\n    def __init__(self):\n        self.parameters = {}\n        self.node_map = collections.OrderedDict()\n        self.input_nodes = list()\n        self.output_nodes = list()\n        self.op_type_count = dict()\n\n    def __hash__(self):\n        return hash(self.id)\n\n    def __eq__(self, other):\n        if self.id == other.id:\n            return True\n        return False\n\n    def __str__(self):\n        graph_str = 'graph { \\n'\n        for node in self.input_nodes:\n            graph_str += \" input: {} \\n\".format(node.layer_name)\n        for node in self.output_nodes:\n            graph_str += \" output: {} \\n \\n\".format(node.layer_name)\n        for name, node in self.node_map.items():\n            graph_str += node.__str__()\n        graph_str += ' }'\n        return graph_str\n\n    def set_output_nodes(self, node_list):\n        if isinstance(node_list, list):\n            self.output_nodes = node_list\n        else:\n            raise TypeError(\n                'output_nodes of Graph must be type: list, but got {}'.format(\n                    type(node_list)))\n\n    def set_node_map(self, node_map):\n        if isinstance(node_map, dict):\n            self.node_map = node_map\n            self.generate_topo_sort()\n        else:\n            raise TypeError('node_map of Graph must be type: list, but got {}'.\n                            format(type(node_map)))\n\n    def set_input_nodes(self, node_list):\n        if isinstance(node_list, list):\n            self.input_nodes = node_list\n        else:\n            raise TypeError(\n                'input_nodes of Graph must be type: list, but got {}'.format(\n                    type(node_list)))\n\n    def set_parameters(self, parameters):\n        if isinstance(parameters, dict):\n            self.parameters = parameters\n        else:\n            raise TypeError(\n                'parameters of Graph must be type: dict, but got {}'.format(\n                    type(parameters)))\n\n    def generate_node_name(self, op_type):\n        if op_type in self.op_type_count:\n            self.op_type_count[op_type] += 1\n        else:\n            self.op_type_count[op_type] = 1\n        # layer_name need follow https://github.com/onnx/onnx/blob/master/docs/OpConventions.md\n        layer_name = op_type + '_' + str(self.op_type_count[op_type] - 1)\n        return layer_name\n\n    def insert_node(self, node):\n        if node.type not in ['feed', 'fetch']:\n            self.node_map[node.layer_name] = node\n\n    def make_node(self,\n                  op_type,\n                  inputs=None,\n                  outputs=None,\n                  attrs=None,\n                  layer_name=None,\n                  domain=None,\n                  **kw):\n        if layer_name is None:\n            layer_name = self.generate_node_name(op_type)\n\n        if attrs is None:\n            attrs = kw\n        attrs.update(kw)\n\n        if inputs is None:\n            inputs = []\n        if outputs is None:\n            outputs = [layer_name]\n        node = Node(op_type, layer_name, inputs, outputs, attrs, domain)\n        self.insert_node(node)\n        return node\n\n    def update_node(self,\n                    node,\n                    op_type=None,\n                    inputs=None,\n                    outputs=None,\n                    attrs=None,\n                    block=None,\n                    move_to_end=True,\n                    domain=None,\n                    **kw):\n        if op_type is not None:\n            node.type = op_type\n        if inputs is not None:\n            node.set_inputs(inputs)\n        if outputs is not None:\n            node.set_outputs(outputs)\n        if attrs is None:\n            attrs = kw\n        attrs.update(kw)\n        node.attrs = attrs\n        if domain is not None:\n            node.domain = domain\n        if move_to_end:\n            self.node_map.pop(node.layer_name)\n        self.node_map[node.layer_name] = node\n        return node\n\n    def get_node(self, name, copy=False):\n        if name not in self.node_map:\n            raise TypeError('Node with name:{} not in graph'.format(name))\n        if copy:\n            node = copy.copy(self.node_map[name])\n        else:\n            node = self.node_map[name]\n        return node\n\n    def remove_node_by_name(self, name):\n        if name in self.node_map:\n            node = self.node_map.pop(name)\n            return node\n        raise TypeError('Node with name:{} not in graph'.format(name))\n\n    def remove_node(self, node):\n        if isinstance(node, Node):\n            node = self.remove_node_by_name(node.layer_name)\n            return node\n        else:\n            node = self.remove_node_by_name(node)\n            return node\n\n    def get_output_nodes_of_node(self, node):\n        if node in self.edge_map:\n            return self.edge_map[node]\n        elif self.get_node(node.layer_name, copy=False):\n            return []\n        else:\n            raise KeyError('Node with layer_name {} not in graph.egde_map'.\n                           format(node.layer_name))\n\n    def get_adjacency_map(self):\n        adjacency_map = {}\n        for layer_name, current_node in self.node_map.items():\n            inputs = current_node.inputs\n            for ipt in inputs:\n                for layer_name, node in self.node_map.items():\n                    if current_node == node:\n                        continue\n                    outputs = node.outputs\n                    if ipt in outputs:\n                        if node not in adjacency_map:\n                            adjacency_map[node] = set([current_node])\n                        else:\n                            adjacency_map[node].add(current_node)\n        return adjacency_map\n\n    def get_topo_sort_list(self):\n        topo_sort_list = list()\n        adjacency_map = self.get_adjacency_map()\n        for layer_name, node in self.node_map.items():\n            if node not in adjacency_map:\n                topo_sort_list.append(node)\n        idx = 0\n        while idx < len(topo_sort_list):\n            current_node = topo_sort_list[idx]\n            for input_node, output_nodes in adjacency_map.items():\n                if current_node in output_nodes:\n                    adjacency_map[input_node].remove(current_node)\n                    if len(adjacency_map[input_node]) == 0:\n                        topo_sort_list.append(input_node)\n            idx += 1\n        return topo_sort_list[::-1]",
  "def __init__(self,\n                 op_type,\n                 inputs,\n                 outputs,\n                 attrs,\n                 layer_name,\n                 domain=NodeDomain.RAW):\n        self.domain = domain\n        self.type = op_type\n        self.attrs = attrs\n        self.layer_name = layer_name\n        self.set_inputs(inputs)\n        self.set_outputs(outputs)",
  "def __hash__(self):\n        return hash(self.layer_name)",
  "def __eq__(self, other):\n        if self.layer_name == other.layer_name:\n            return True\n        return False",
  "def __str__(self):\n        node_str = ''\n        attrs = ''\n        for key, value in self.attrs.items():\n            attrs += ', ' + key + '=' + str(value)\n        node_str += \"  {} = {}::{}(inputs={}{}) \\n\".format(\n            self.outputs, self.domain, self.type, self.inputs, attrs)\n        return node_str",
  "def input(self, idx=None):\n        if idx is None:\n            return self.inputs\n        return self.inputs[idx]",
  "def output(self, idx=None):\n        if idx is None:\n            return self.outputs\n        return self.outputs[idx]",
  "def attr(self, name):\n        if name in self.attrs:\n            return self.attrs[name]\n        return None",
  "def set_inputs(self, inputs):\n        if isinstance(inputs, list):\n            self.inputs = [\n                ipt.layer_name if isinstance(ipt, Node) else ipt\n                for ipt in inputs\n            ]\n        elif isinstance(inputs, six.string_types):\n            self.inputs = [inputs]\n        elif isinstance(inputs, Node):\n            self.inputs = [inputs.layer_name]\n        else:\n            raise TypeError(\n                'Inputs of node must be type: list, Node, or String but got {}'.\n                format(type(inputs)))",
  "def set_outputs(self, outputs):\n        if isinstance(outputs, list):\n            self.outputs = [\n                opt.layer_name if isinstance(opt, Node) else opt\n                for opt in outputs\n            ]\n        elif isinstance(outputs, six.string_types):\n            self.outputs = [outputs]\n        elif isinstance(outputs, Node):\n            self.outputs = [outputs.layer_name]\n        else:\n            raise TypeError(\n                'Outputs of node must be type: list, Node, or String but got {}'.\n                format(type(outputs)))",
  "def __init__(self):\n        self.parameters = {}\n        self.node_map = collections.OrderedDict()\n        self.input_nodes = list()\n        self.output_nodes = list()\n        self.op_type_count = dict()",
  "def __hash__(self):\n        return hash(self.id)",
  "def __eq__(self, other):\n        if self.id == other.id:\n            return True\n        return False",
  "def __str__(self):\n        graph_str = 'graph { \\n'\n        for node in self.input_nodes:\n            graph_str += \" input: {} \\n\".format(node.layer_name)\n        for node in self.output_nodes:\n            graph_str += \" output: {} \\n \\n\".format(node.layer_name)\n        for name, node in self.node_map.items():\n            graph_str += node.__str__()\n        graph_str += ' }'\n        return graph_str",
  "def set_output_nodes(self, node_list):\n        if isinstance(node_list, list):\n            self.output_nodes = node_list\n        else:\n            raise TypeError(\n                'output_nodes of Graph must be type: list, but got {}'.format(\n                    type(node_list)))",
  "def set_node_map(self, node_map):\n        if isinstance(node_map, dict):\n            self.node_map = node_map\n            self.generate_topo_sort()\n        else:\n            raise TypeError('node_map of Graph must be type: list, but got {}'.\n                            format(type(node_map)))",
  "def set_input_nodes(self, node_list):\n        if isinstance(node_list, list):\n            self.input_nodes = node_list\n        else:\n            raise TypeError(\n                'input_nodes of Graph must be type: list, but got {}'.format(\n                    type(node_list)))",
  "def set_parameters(self, parameters):\n        if isinstance(parameters, dict):\n            self.parameters = parameters\n        else:\n            raise TypeError(\n                'parameters of Graph must be type: dict, but got {}'.format(\n                    type(parameters)))",
  "def generate_node_name(self, op_type):\n        if op_type in self.op_type_count:\n            self.op_type_count[op_type] += 1\n        else:\n            self.op_type_count[op_type] = 1\n        # layer_name need follow https://github.com/onnx/onnx/blob/master/docs/OpConventions.md\n        layer_name = op_type + '_' + str(self.op_type_count[op_type] - 1)\n        return layer_name",
  "def insert_node(self, node):\n        if node.type not in ['feed', 'fetch']:\n            self.node_map[node.layer_name] = node",
  "def make_node(self,\n                  op_type,\n                  inputs=None,\n                  outputs=None,\n                  attrs=None,\n                  layer_name=None,\n                  domain=None,\n                  **kw):\n        if layer_name is None:\n            layer_name = self.generate_node_name(op_type)\n\n        if attrs is None:\n            attrs = kw\n        attrs.update(kw)\n\n        if inputs is None:\n            inputs = []\n        if outputs is None:\n            outputs = [layer_name]\n        node = Node(op_type, layer_name, inputs, outputs, attrs, domain)\n        self.insert_node(node)\n        return node",
  "def update_node(self,\n                    node,\n                    op_type=None,\n                    inputs=None,\n                    outputs=None,\n                    attrs=None,\n                    block=None,\n                    move_to_end=True,\n                    domain=None,\n                    **kw):\n        if op_type is not None:\n            node.type = op_type\n        if inputs is not None:\n            node.set_inputs(inputs)\n        if outputs is not None:\n            node.set_outputs(outputs)\n        if attrs is None:\n            attrs = kw\n        attrs.update(kw)\n        node.attrs = attrs\n        if domain is not None:\n            node.domain = domain\n        if move_to_end:\n            self.node_map.pop(node.layer_name)\n        self.node_map[node.layer_name] = node\n        return node",
  "def get_node(self, name, copy=False):\n        if name not in self.node_map:\n            raise TypeError('Node with name:{} not in graph'.format(name))\n        if copy:\n            node = copy.copy(self.node_map[name])\n        else:\n            node = self.node_map[name]\n        return node",
  "def remove_node_by_name(self, name):\n        if name in self.node_map:\n            node = self.node_map.pop(name)\n            return node\n        raise TypeError('Node with name:{} not in graph'.format(name))",
  "def remove_node(self, node):\n        if isinstance(node, Node):\n            node = self.remove_node_by_name(node.layer_name)\n            return node\n        else:\n            node = self.remove_node_by_name(node)\n            return node",
  "def get_output_nodes_of_node(self, node):\n        if node in self.edge_map:\n            return self.edge_map[node]\n        elif self.get_node(node.layer_name, copy=False):\n            return []\n        else:\n            raise KeyError('Node with layer_name {} not in graph.egde_map'.\n                           format(node.layer_name))",
  "def get_adjacency_map(self):\n        adjacency_map = {}\n        for layer_name, current_node in self.node_map.items():\n            inputs = current_node.inputs\n            for ipt in inputs:\n                for layer_name, node in self.node_map.items():\n                    if current_node == node:\n                        continue\n                    outputs = node.outputs\n                    if ipt in outputs:\n                        if node not in adjacency_map:\n                            adjacency_map[node] = set([current_node])\n                        else:\n                            adjacency_map[node].add(current_node)\n        return adjacency_map",
  "def get_topo_sort_list(self):\n        topo_sort_list = list()\n        adjacency_map = self.get_adjacency_map()\n        for layer_name, node in self.node_map.items():\n            if node not in adjacency_map:\n                topo_sort_list.append(node)\n        idx = 0\n        while idx < len(topo_sort_list):\n            current_node = topo_sort_list[idx]\n            for input_node, output_nodes in adjacency_map.items():\n                if current_node in output_nodes:\n                    adjacency_map[input_node].remove(current_node)\n                    if len(adjacency_map[input_node]) == 0:\n                        topo_sort_list.append(input_node)\n            idx += 1\n        return topo_sort_list[::-1]",
  "class PaddleNode(Node):\n    def __init__(self, paddle_op, inputs, outputs, attrs, layer_name, block):\n        super(PaddleNode, self).__init__(paddle_op.type, inputs, outputs, attrs,\n                                         layer_name, NodeDomain.PADDLE)\n        self.paddle_op = paddle_op\n        self.block = block\n\n    def __str__(self):\n        node_str = ''\n        attrs = ''\n        for key, value in self.attrs.items():\n            if key == 'op_callstack':\n                continue\n            attrs += ', ' + key + '=' + str(value)\n        node_str += \"  {} = {}::{}(inputs={}{}) \\n\".format(\n            self.outputs, self.domain, self.type, self.inputs, attrs)\n        return node_str\n\n    @property\n    def input_names(self):\n        return [name for name in self.inputs.keys()]\n\n    @property\n    def output_names(self):\n        return [name for name in self.outputs.keys()]\n\n    def input(self, name, idx=None):\n        if name not in self.inputs:\n            return None\n        if idx is None:\n            return self.inputs[name]\n        if len(self.inputs[name]) <= idx:\n            return None\n        return self.inputs[name][idx]\n\n    def output(self, name, idx=None):\n        if idx is None:\n            return self.outputs[name]\n        return self.outputs[name][idx]\n\n    def output_shape(self, name, idx):\n        return self.block.var(self.output(name, idx)).shape\n\n    def input_shape(self, name, idx):\n        return self.block.var(self.input(name, idx)).shape\n\n    def input_var(self, name, idx):\n        return self.block.var(self.input(name, idx))\n\n    def input_dtype(self, name, idx):\n        return self.block.var(self.input(name, idx)).dtype\n\n    def output_dtype(self, name, idx):\n        return self.block.var(self.output(name, idx)).dtype\n\n    def attr(self, name, default=None):\n        if name in self.attrs:\n            return self.attrs[name]\n        return default\n\n    def set_inputs(self, inputs):\n        if isinstance(inputs, dict):\n            # input of node in paddle, which stored by dict \n            self.inputs = inputs\n        else:\n            raise TypeError('Inputs of node must be type: dict, but got {}'.\n                            format(type(inputs)))\n\n    def set_outputs(self, outputs):\n        if isinstance(outputs, dict):\n            # output of node in paddle, which stored by dict \n            self.outputs = outputs\n        else:\n            raise TypeError('Outputs of node must be type: dict, but got {}'.\n                            format(type(outputs)))",
  "class PaddleGraph(Graph):\n    def __init__(self, program, parameters, feed_var_names, fetch_vars):\n        super(PaddleGraph, self).__init__()\n        self.build_graph(program, parameters, feed_var_names, fetch_vars)\n\n    def make_node(self,\n                  op,\n                  inputs=None,\n                  outputs=None,\n                  attrs=None,\n                  block=None,\n                  layer_name=None,\n                  **kw):\n        if layer_name is None:\n            layer_name = self.generate_node_name(op.type)\n\n        if attrs is None:\n            attrs = kw\n        attrs.update(kw)\n\n        if inputs is None:\n            inputs = {}\n        if outputs is None:\n            outputs = {'Out': layer_name}\n        node = PaddleNode(op, inputs, outputs, attrs, layer_name, block)\n        self.insert_node(node)\n        return node\n\n    def add_input_node(self, inputs, block=None):\n        for ipt in inputs:\n            # parse feed_names\n            layer_name = ipt\n            var = block.var(ipt)\n            attrs = {}\n            attrs['shape'] = var.shape\n            attrs['dtype'] = var.dtype\n            node = Node('feed', [], [layer_name], attrs, layer_name)\n            self.input_nodes.append(node)\n\n    def add_output_node(self, outputs, block=None):\n        from paddle.fluid.framework import Variable\n        for opt in outputs:\n            # parse fetch_target_vars \n            layer_name = opt.name\n            attrs = {}\n            attrs['shape'] = opt.shape\n            attrs['dtype'] = opt.dtype\n            node = Node('fetch', [layer_name], [], attrs, layer_name)\n            self.output_nodes.append(node)\n\n    def get_adjacency_map(self):\n        adjacency_map = {}\n        for layer_name, current_node in self.node_map.items():\n            inputs = current_node.inputs.values()\n            inputs = [x for j in inputs for x in j]\n            for ipt in inputs:\n                for layer_name, node in self.node_map.items():\n                    if current_node == node:\n                        continue\n                    outputs = node.outputs.values()\n                    outputs = [x for j in outputs for x in j]\n                    if ipt in outputs:\n                        if node not in adjacency_map:\n                            adjacency_map[node] = set([current_node])\n                        else:\n                            adjacency_map[node].add(current_node)\n        return adjacency_map\n\n    def build_graph(self,\n                    program,\n                    parameters,\n                    feed_var_names=None,\n                    target_vars=None):\n        self.program = program\n        self.set_parameters(parameters)\n        self.add_input_node(feed_var_names, program.global_block())\n        self.add_output_node(target_vars, program.global_block())\n        for block in program.blocks:\n            for i, op in enumerate(block.ops):\n                if op.type in ['feed', 'fetch']:\n                    continue\n                else:\n                    inputs = {}\n                    outputs = {}\n                    for ipt in op.input_names:\n                        inputs[ipt] = op.input(ipt)\n                    for opt in op.output_names:\n                        outputs[opt] = op.output(opt)\n                    node = self.make_node(op, inputs, outputs,\n                                          op.all_attrs(), block)\n\n    @staticmethod\n    def build_from_program(program,\n                           feed_var_names=None,\n                           fetch_vars=None,\n                           scope=None):\n        parameters_dict = {}\n        vars = program.global_block().vars\n        for name in vars:\n            var = program.global_block().var(name)\n            if name.endswith('feed') or name.endswith('fetch'):\n                continue\n            if not var.persistable:\n                continue\n            parameters_dict[name] = {\n                'data': np.array(scope.var(name).get_tensor()),\n                'dtype': var.dtype,\n                'shape': var.shape\n            }\n\n        graph = PaddleGraph(program, parameters_dict, feed_var_names,\n                            fetch_vars)\n        return graph\n\n    @staticmethod\n    def build_from_dygraph(layer, input_spec=None, output_spec=None):\n        from paddle.nn import Layer\n        from paddle.fluid import core\n        from paddle.fluid.framework import Variable\n        from paddle2onnx.legacy.graph import dygraph_helper as dg_helper\n        if isinstance(layer, dygraph.TranslatedLayer):\n            program = layer.program()\n            parameters_dict = {}\n            pruned_vars = program.global_block().vars\n            for param in layer.parameters():\n                if param.name.endswith('feed') or param.name.endswith('fetch'):\n                    continue\n                if not param.persistable:\n                    continue\n                if param.name in pruned_vars:\n                    parameters_dict[param.name] = {\n                        'data': np.array(param.value().get_tensor()),\n                        'dtype': param.dtype,\n                        'shape': param.shape\n                    }\n            for param in layer.buffers():\n                if param.name.endswith('feed') or param.name.endswith('fetch'):\n                    continue\n                if not param.value().get_tensor()._is_initialized():\n                    continue\n                if param.name in pruned_vars:\n                    parameters_dict[param.name] = {\n                        'data': np.array(param.value().get_tensor()),\n                        'dtype': param.dtype,\n                        'shape': param.shape\n                    }\n            if input_spec is not None:\n                logging.warning(\n                    \"Although input_spec is specified, TranslatedLayer is not support prune. An Complete network will be exported.\"\n                )\n                input_spec = layer._input_spec()\n            if output_spec is not None:\n                logging.warning(\n                    \"Although output_spec is specified, TranslatedLayer is not support prune. An Complete network will be exported.\"\n                )\n            feed_var_names = [ipt.name for ipt in layer._input_spec()]\n            fetch_vars = [\n                program.global_block().var(opt.name)\n                for opt in layer._output_spec()\n            ]\n            graph = PaddleGraph(program, parameters_dict, feed_var_names,\n                                fetch_vars)\n            return graph\n        elif isinstance(layer, Layer):\n            program, feed_var_names, fetch_vars = dg_helper.get_program(\n                layer, input_spec, output_spec)\n            parameters_dict = {}\n            pruned_vars = program.global_block().vars\n            for param in layer.parameters():\n                if param.name.endswith('feed') or param.name.endswith('fetch'):\n                    continue\n                if not param.persistable:\n                    continue\n                if param.name in pruned_vars:\n                    parameters_dict[param.name] = {\n                        'data': np.array(param.value().get_tensor()),\n                        'dtype': param.dtype,\n                        'shape': param.shape\n                    }\n            for param in layer.buffers():\n                if param.name.endswith('feed') or param.name.endswith('fetch'):\n                    continue\n                if not param.value().get_tensor()._is_initialized():\n                    continue\n                if param.name in pruned_vars:\n                    parameters_dict[param.name] = {\n                        'data': np.array(param.value().get_tensor()),\n                        'dtype': param.dtype,\n                        'shape': param.shape\n                    }\n            graph = PaddleGraph(program, parameters_dict, feed_var_names,\n                                fetch_vars)\n            return graph\n        else:\n            raise TypeError(\n                \"The input Layer should be 'Layer' or 'TranslatedLayer', but received  type is %s.\"\n                % type(layer))",
  "def __init__(self, paddle_op, inputs, outputs, attrs, layer_name, block):\n        super(PaddleNode, self).__init__(paddle_op.type, inputs, outputs, attrs,\n                                         layer_name, NodeDomain.PADDLE)\n        self.paddle_op = paddle_op\n        self.block = block",
  "def __str__(self):\n        node_str = ''\n        attrs = ''\n        for key, value in self.attrs.items():\n            if key == 'op_callstack':\n                continue\n            attrs += ', ' + key + '=' + str(value)\n        node_str += \"  {} = {}::{}(inputs={}{}) \\n\".format(\n            self.outputs, self.domain, self.type, self.inputs, attrs)\n        return node_str",
  "def input_names(self):\n        return [name for name in self.inputs.keys()]",
  "def output_names(self):\n        return [name for name in self.outputs.keys()]",
  "def input(self, name, idx=None):\n        if name not in self.inputs:\n            return None\n        if idx is None:\n            return self.inputs[name]\n        if len(self.inputs[name]) <= idx:\n            return None\n        return self.inputs[name][idx]",
  "def output(self, name, idx=None):\n        if idx is None:\n            return self.outputs[name]\n        return self.outputs[name][idx]",
  "def output_shape(self, name, idx):\n        return self.block.var(self.output(name, idx)).shape",
  "def input_shape(self, name, idx):\n        return self.block.var(self.input(name, idx)).shape",
  "def input_var(self, name, idx):\n        return self.block.var(self.input(name, idx))",
  "def input_dtype(self, name, idx):\n        return self.block.var(self.input(name, idx)).dtype",
  "def output_dtype(self, name, idx):\n        return self.block.var(self.output(name, idx)).dtype",
  "def attr(self, name, default=None):\n        if name in self.attrs:\n            return self.attrs[name]\n        return default",
  "def set_inputs(self, inputs):\n        if isinstance(inputs, dict):\n            # input of node in paddle, which stored by dict \n            self.inputs = inputs\n        else:\n            raise TypeError('Inputs of node must be type: dict, but got {}'.\n                            format(type(inputs)))",
  "def set_outputs(self, outputs):\n        if isinstance(outputs, dict):\n            # output of node in paddle, which stored by dict \n            self.outputs = outputs\n        else:\n            raise TypeError('Outputs of node must be type: dict, but got {}'.\n                            format(type(outputs)))",
  "def __init__(self, program, parameters, feed_var_names, fetch_vars):\n        super(PaddleGraph, self).__init__()\n        self.build_graph(program, parameters, feed_var_names, fetch_vars)",
  "def make_node(self,\n                  op,\n                  inputs=None,\n                  outputs=None,\n                  attrs=None,\n                  block=None,\n                  layer_name=None,\n                  **kw):\n        if layer_name is None:\n            layer_name = self.generate_node_name(op.type)\n\n        if attrs is None:\n            attrs = kw\n        attrs.update(kw)\n\n        if inputs is None:\n            inputs = {}\n        if outputs is None:\n            outputs = {'Out': layer_name}\n        node = PaddleNode(op, inputs, outputs, attrs, layer_name, block)\n        self.insert_node(node)\n        return node",
  "def add_input_node(self, inputs, block=None):\n        for ipt in inputs:\n            # parse feed_names\n            layer_name = ipt\n            var = block.var(ipt)\n            attrs = {}\n            attrs['shape'] = var.shape\n            attrs['dtype'] = var.dtype\n            node = Node('feed', [], [layer_name], attrs, layer_name)\n            self.input_nodes.append(node)",
  "def add_output_node(self, outputs, block=None):\n        from paddle.fluid.framework import Variable\n        for opt in outputs:\n            # parse fetch_target_vars \n            layer_name = opt.name\n            attrs = {}\n            attrs['shape'] = opt.shape\n            attrs['dtype'] = opt.dtype\n            node = Node('fetch', [layer_name], [], attrs, layer_name)\n            self.output_nodes.append(node)",
  "def get_adjacency_map(self):\n        adjacency_map = {}\n        for layer_name, current_node in self.node_map.items():\n            inputs = current_node.inputs.values()\n            inputs = [x for j in inputs for x in j]\n            for ipt in inputs:\n                for layer_name, node in self.node_map.items():\n                    if current_node == node:\n                        continue\n                    outputs = node.outputs.values()\n                    outputs = [x for j in outputs for x in j]\n                    if ipt in outputs:\n                        if node not in adjacency_map:\n                            adjacency_map[node] = set([current_node])\n                        else:\n                            adjacency_map[node].add(current_node)\n        return adjacency_map",
  "def build_graph(self,\n                    program,\n                    parameters,\n                    feed_var_names=None,\n                    target_vars=None):\n        self.program = program\n        self.set_parameters(parameters)\n        self.add_input_node(feed_var_names, program.global_block())\n        self.add_output_node(target_vars, program.global_block())\n        for block in program.blocks:\n            for i, op in enumerate(block.ops):\n                if op.type in ['feed', 'fetch']:\n                    continue\n                else:\n                    inputs = {}\n                    outputs = {}\n                    for ipt in op.input_names:\n                        inputs[ipt] = op.input(ipt)\n                    for opt in op.output_names:\n                        outputs[opt] = op.output(opt)\n                    node = self.make_node(op, inputs, outputs,\n                                          op.all_attrs(), block)",
  "def build_from_program(program,\n                           feed_var_names=None,\n                           fetch_vars=None,\n                           scope=None):\n        parameters_dict = {}\n        vars = program.global_block().vars\n        for name in vars:\n            var = program.global_block().var(name)\n            if name.endswith('feed') or name.endswith('fetch'):\n                continue\n            if not var.persistable:\n                continue\n            parameters_dict[name] = {\n                'data': np.array(scope.var(name).get_tensor()),\n                'dtype': var.dtype,\n                'shape': var.shape\n            }\n\n        graph = PaddleGraph(program, parameters_dict, feed_var_names,\n                            fetch_vars)\n        return graph",
  "def build_from_dygraph(layer, input_spec=None, output_spec=None):\n        from paddle.nn import Layer\n        from paddle.fluid import core\n        from paddle.fluid.framework import Variable\n        from paddle2onnx.legacy.graph import dygraph_helper as dg_helper\n        if isinstance(layer, dygraph.TranslatedLayer):\n            program = layer.program()\n            parameters_dict = {}\n            pruned_vars = program.global_block().vars\n            for param in layer.parameters():\n                if param.name.endswith('feed') or param.name.endswith('fetch'):\n                    continue\n                if not param.persistable:\n                    continue\n                if param.name in pruned_vars:\n                    parameters_dict[param.name] = {\n                        'data': np.array(param.value().get_tensor()),\n                        'dtype': param.dtype,\n                        'shape': param.shape\n                    }\n            for param in layer.buffers():\n                if param.name.endswith('feed') or param.name.endswith('fetch'):\n                    continue\n                if not param.value().get_tensor()._is_initialized():\n                    continue\n                if param.name in pruned_vars:\n                    parameters_dict[param.name] = {\n                        'data': np.array(param.value().get_tensor()),\n                        'dtype': param.dtype,\n                        'shape': param.shape\n                    }\n            if input_spec is not None:\n                logging.warning(\n                    \"Although input_spec is specified, TranslatedLayer is not support prune. An Complete network will be exported.\"\n                )\n                input_spec = layer._input_spec()\n            if output_spec is not None:\n                logging.warning(\n                    \"Although output_spec is specified, TranslatedLayer is not support prune. An Complete network will be exported.\"\n                )\n            feed_var_names = [ipt.name for ipt in layer._input_spec()]\n            fetch_vars = [\n                program.global_block().var(opt.name)\n                for opt in layer._output_spec()\n            ]\n            graph = PaddleGraph(program, parameters_dict, feed_var_names,\n                                fetch_vars)\n            return graph\n        elif isinstance(layer, Layer):\n            program, feed_var_names, fetch_vars = dg_helper.get_program(\n                layer, input_spec, output_spec)\n            parameters_dict = {}\n            pruned_vars = program.global_block().vars\n            for param in layer.parameters():\n                if param.name.endswith('feed') or param.name.endswith('fetch'):\n                    continue\n                if not param.persistable:\n                    continue\n                if param.name in pruned_vars:\n                    parameters_dict[param.name] = {\n                        'data': np.array(param.value().get_tensor()),\n                        'dtype': param.dtype,\n                        'shape': param.shape\n                    }\n            for param in layer.buffers():\n                if param.name.endswith('feed') or param.name.endswith('fetch'):\n                    continue\n                if not param.value().get_tensor()._is_initialized():\n                    continue\n                if param.name in pruned_vars:\n                    parameters_dict[param.name] = {\n                        'data': np.array(param.value().get_tensor()),\n                        'dtype': param.dtype,\n                        'shape': param.shape\n                    }\n            graph = PaddleGraph(program, parameters_dict, feed_var_names,\n                                fetch_vars)\n            return graph\n        else:\n            raise TypeError(\n                \"The input Layer should be 'Layer' or 'TranslatedLayer', but received  type is %s.\"\n                % type(layer))",
  "def _get_input_var_names(inputs, input_spec):\n    name_none_error = \"The %s's name is None. \" \\\n        \"When using jit.save, please set InputSepc's name in \" \\\n        \"to_static(input_spec=[]) and jit.save(input_spec=[]) \" \\\n        \"and make sure they are consistent.\"\n    name_no_exists_error = \"The tensor `%s` does not exists. \" \\\n        \"Please make sure the name of InputSpec or example Tensor \" \\\n        \"in input_spec is the same as the name of InputSpec in \" \\\n        \"`to_static` decorated on the Layer.forward method.\"\n    result_list = []\n    input_var_names = [\n        var.name for var in flatten(inputs) if isinstance(var, Variable)\n    ]\n    if input_spec is None:\n        # no prune\n        return input_var_names\n    else:\n        # fileter out non-tensor type spec infos.\n        input_spec = [\n            spec for spec in input_spec\n            if isinstance(spec, paddle.static.InputSpec)\n        ]\n\n    if len(input_spec) == len(input_var_names):\n        # no prune\n        result_list = input_var_names\n        # if input spec name not in input_var_names, only raise warning\n        for spec in input_spec:\n            if spec.name is None:\n                warnings.warn(name_none_error % spec)\n            elif spec.name not in input_var_names:\n                warnings.warn(name_no_exists_error % spec.name)\n            else:\n                # do nothing\n                pass\n    else:\n        # prune\n        for spec in input_spec:\n            if spec.name is None:\n                # name is None, the input_spec only can be InputSpec\n                raise ValueError(name_none_error % spec)\n            elif spec.name not in input_var_names:\n                # the input_spec can be `InputSpec` or `VarBase`\n                raise ValueError(name_no_exists_error % spec.name)\n            else:\n                result_list.append(spec.name)\n\n    return result_list",
  "def _get_output_vars(outputs, output_spec):\n    name_no_exists_error = \"The tensor `%s` does not exists. \" \\\n        \"Please make sure the name of example Tensor \" \\\n        \"in configs.output_spec is the output tensor of \" \\\n        \"Layer.forward method.\"\n    result_list = []\n    output_vars_dict = OrderedDict()\n    for var in flatten(outputs):\n        if isinstance(var, Variable):\n            output_vars_dict[var.name] = var\n    if output_spec is None:\n        result_list = output_vars_dict.values()\n    elif output_spec is not None and len(output_spec) == len(output_vars_dict):\n        result_list = output_vars_dict.values()\n        for var in output_spec:\n            if var.name not in output_vars_dict:\n                warnings.warn(name_no_exists_error % var.name)\n    else:\n        for var in output_spec:\n            if var.name not in output_vars_dict:\n                raise ValueError(name_no_exists_error % var.name)\n            else:\n                result_list.append(output_vars_dict[var.name])\n    return result_list",
  "def get_program(layer, input_spec, output_spec, **configs):\n    paddle.jit.set_verbosity(0)\n    prog_translator = ProgramTranslator()\n    if not prog_translator.enable_to_static:\n        raise RuntimeError(\n            \"The Paddle2onnx doesn't work when setting ProgramTranslator.enable to False.\"\n        )\n\n    if not isinstance(layer, Layer):\n        raise TypeError(\n            \"The input of paddle2onnx should be 'Layer', but received input type is %s.\"\n            % type(layer))\n\n    if isinstance(layer, paddle.DataParallel):\n        inner_layer = layer._layers\n    else:\n        inner_layer = layer\n\n    # avoid change user given input_spec\n    inner_input_spec = None\n    if input_spec is not None:\n        for attr_func in dir(inner_layer):\n            static_func = getattr(inner_layer, attr_func, None)\n            if isinstance(static_func,\n                          StaticFunction) and 'forward' != attr_func:\n                raise ValueError(\n                    \"If there are static functions other than 'forward' that need to be saved, the input 'input_spec' should be None, but received the type of 'input_spec' is %s.\"\n                    % type(input_spec))\n\n        if not isinstance(input_spec, (list, tuple)):\n            raise TypeError(\n                \"The input input_spec should be 'list', but received input_spec's type is %s.\"\n                % type(input_spec))\n        inner_input_spec = []\n        for var in flatten(input_spec):\n            if isinstance(var, paddle.static.InputSpec):\n                inner_input_spec.append(var)\n            elif isinstance(var, (core.VarBase, core.eager.Tensor, Variable)):\n                inner_input_spec.append(\n                    paddle.static.InputSpec.from_tensor(var))\n            else:\n                # NOTE(Aurelius84): Support non-Tensor type in `input_spec`.\n                inner_input_spec.append(var)\n\n    extra_var_info = dict()\n    functions = dir(inner_layer)\n    for attr_func in functions:\n        static_func = getattr(inner_layer, attr_func, None)\n        if isinstance(static_func, StaticFunction):\n            concrete_program = static_func.concrete_program_specify_input_spec(\n                inner_input_spec)\n        elif 'forward' == attr_func:\n            # transform in jit.save, if input_spec is incomplete, declarative will throw error\n            # inner_input_spec is list[InputSpec], it should be packed with same structure\n            # as original input_spec here.\n            if inner_input_spec:\n                inner_input_spec = pack_sequence_as(input_spec,\n                                                    inner_input_spec)\n            static_forward = declarative(\n                inner_layer.forward, input_spec=inner_input_spec)\n            concrete_program = static_forward.concrete_program\n            # the input_spec has been used in declarative, which is equal to\n            # @declarative with input_spec and jit.save without input_spec,\n            # avoid needless warning\n            inner_input_spec = None\n        else:\n            continue\n\n        input_var_names = _get_input_var_names(concrete_program.inputs,\n                                               inner_input_spec)\n\n        # NOTE(chenweihang): [ Get output variables ]\n        # the rule is like [ Get input variables name ]. For output var,\n        # we only support VarBase spec, and actually, we only need the\n        # var name of output, and we don't recommended to use output_spec\n        output_vars = _get_output_vars(concrete_program.outputs, output_spec)\n\n    feeded_var_names = input_var_names\n    target_vars = output_vars\n    main_program = concrete_program.main_program.clone()\n    export_for_deployment = True\n\n    if isinstance(feeded_var_names, six.string_types):\n        feeded_var_names = [feeded_var_names]\n    elif export_for_deployment:\n        if len(feeded_var_names) > 0:\n            # TODO(paddle-dev): polish these code blocks\n            if not (bool(feeded_var_names) and all(\n                    isinstance(name, six.string_types)\n                    for name in feeded_var_names)):\n                raise ValueError(\"'feed_var_names' should be a list of str.\")\n\n    if isinstance(target_vars, Variable):\n        target_vars = [target_vars]\n    elif export_for_deployment:\n        if not (bool(target_vars) and\n                all(isinstance(var, Variable) for var in target_vars)):\n            raise ValueError(\"'target_vars' should be a list of Variable.\")\n\n    main_program = _get_valid_program(main_program)\n\n    # remind user to set auc_states to zeros if the program contains auc op\n    all_ops = main_program.global_block().ops\n    for op in all_ops:\n        # clear device of Op\n        device_attr_name = core.op_proto_and_checker_maker.kOpDeviceAttrName()\n        op._set_attr(device_attr_name, \"\")\n        if op.type == 'auc':\n            warnings.warn(\n                \"please ensure that you have set the auc states to zeros before saving inference model\"\n            )\n            break\n\n    with program_guard(main_program):\n        uniq_target_vars = []\n        for i, var in enumerate(target_vars):\n            uniq_target_vars.append(var)\n        target_vars = uniq_target_vars\n    target_var_name_list = [var.name for var in target_vars]\n\n    origin_program = main_program.clone()\n\n    main_program = main_program.clone()\n    global_block = main_program.global_block()\n    need_to_remove_op_index = []\n    for i, op in enumerate(global_block.ops):\n        op.desc.set_is_target(False)\n        if op.type == \"feed\" or op.type == \"fetch\":\n            need_to_remove_op_index.append(i)\n\n    for index in need_to_remove_op_index[::-1]:\n        global_block._remove_op(index)\n\n    main_program.desc.flush()\n\n    main_program = main_program._prune_with_input(\n        feeded_var_names=feeded_var_names, targets=target_vars)\n    main_program = main_program._inference_optimize(prune_read_op=True)\n    fetch_var_names = [v.name for v in target_vars]\n\n    for target_v in target_vars:\n        if not main_program.global_block().has_var(target_v.name):\n            main_program.global_block().create_var(\n                name=target_v.name,\n                shape=target_v.shape,\n                dtype=target_v.dtype,\n                persistable=target_v.persistable)\n\n    prepend_feed_ops(main_program, feeded_var_names)\n    append_fetch_ops(main_program, fetch_var_names)\n\n    main_program.desc._set_version()\n    paddle.fluid.core.save_op_version_info(main_program.desc)\n\n    main_program._copy_dist_param_info_from(origin_program)\n\n    return main_program, feeded_var_names, target_vars",
  "def is_static_shape(shape):\n    if len(shape) > 1 and shape[1:].count(-1) > 0:\n        raise Exception(\n            \"Converting this model to ONNX need with static input shape,\" \\\n            \" please fix input shape of this model, see doc Q2 in\" \\\n            \" https://github.com/PaddlePaddle/paddle2onnx/blob/develop/docs/en/FAQ.md.\"\n        )",
  "def shape_helper(graph, input, dim=None):\n    if dim is None:\n        shape_node = graph.make_node('Shape', inputs=[input])\n        return shape_node\n    full_shape = graph.make_node('Shape', inputs=[input])\n    shape_node = slice_helper(graph, full_shape, [0], [dim], [dim + 1])\n    return shape_node",
  "def unsqueeze_helper(graph, input, axes, outputs=None):\n    inputs = []\n    if not isinstance(input, list):\n        input = [input]\n    inputs.append(input[0])\n    if not isinstance(axes, list):\n        axes = [axes]\n    if outputs is not None and isinstance(outputs, six.string_types):\n        outputs = [outputs]\n\n    if graph.opset_version < 13:\n        unsqueeze_node = graph.make_node(\n            \"Unsqueeze\", inputs=inputs, outputs=outputs, axes=axes)\n        return unsqueeze_node\n    else:\n        axes_node = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=axes)\n        inputs = inputs + [axes_node]\n        unsqueeze_node = graph.make_node(\n            \"Unsqueeze\", inputs=inputs, outputs=outputs)\n        return unsqueeze_node",
  "def split_helper(graph, input, axis=0, split=None, outputs=None):\n    assert outputs is not None, \"outputs can not be None in split_helper.\"\n    inputs = []\n    if not isinstance(input, list):\n        input = [input]\n    inputs.append(input[0])\n    if split is not None and not isinstance(split, list):\n        split = [split]\n    if split is None:\n        split_node = graph.make_node(\n            \"Split\", inputs=inputs, outputs=outputs, axis=axis)\n        return split_node\n    if graph.opset_version < 13:\n        split_node = graph.make_node(\n            \"Split\", inputs=inputs, outputs=outputs, axis=axis, split=split)\n        return split_node\n    else:\n        split = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=split)\n        inputs = inputs + [split]\n        split_node = graph.make_node(\n            \"Split\", inputs=inputs, axis=axis, outputs=outputs)\n        return split_node",
  "def slice_helper(graph,\n                 input,\n                 axes,\n                 starts,\n                 ends,\n                 outputs=None,\n                 dtype=dtypes.ONNX.INT64):\n    inputs = []\n    if not isinstance(input, list):\n        input = [input]\n    inputs.append(input[0])\n    if axes is not None and not isinstance(axes, list):\n        axes = [axes]\n    if starts is not None and not isinstance(starts, (list, six.string_types)):\n        starts = [starts]\n    if ends is not None and not isinstance(ends, (list, six.string_types)):\n        ends = [ends]\n\n    if graph.opset_version < 10:\n        attrs = {\n            'starts': starts,\n            'ends': ends,\n        }\n        if axes not in [None, []]:\n            attrs['axes'] = axes\n        slice_node = graph.make_node(\n            \"Slice\", inputs=inputs, outputs=outputs, attrs=attrs)\n        return slice_node\n    else:\n        if not isinstance(starts, six.string_types):\n            starts = graph.make_node('Constant', dtype=dtype, value=starts)\n        if not isinstance(ends, six.string_types):\n            ends = graph.make_node('Constant', dtype=dtype, value=ends)\n        inputs = inputs + [starts, ends]\n        if axes not in [None, []]:\n            axes_node = graph.make_node('Constant', dtype=dtype, value=axes)\n            inputs.append(axes_node)\n        slice_node = graph.make_node(\"Slice\", inputs=inputs, outputs=outputs)\n        return slice_node",
  "def squeeze_helper(graph, input, axes=None, outputs=None):\n    inputs = []\n    if not isinstance(input, list):\n        input = [input]\n    inputs.append(input[0])\n    if axes is not None and not isinstance(axes, list):\n        axes = [axes]\n    if graph.opset_version < 13:\n        squeeze_node = graph.make_node(\n            \"Squeeze\", inputs=inputs, axes=axes, outputs=outputs)\n        return squeeze_node\n    else:\n        if axes is not None:\n            axes_node = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=axes)\n            inputs.append(axes_node)\n        squeeze_node = graph.make_node(\n            \"Squeeze\", inputs=inputs, outputs=outputs)\n        return squeeze_node",
  "def unsqueeze_helper(graph, input, axes, outputs=None):\n    inputs = []\n    if isinstance(input, list):\n        input = input[0]\n    inputs.append(input)\n    if not isinstance(axes, list):\n        axes = [axes]\n    if graph.opset_version < 13:\n        unsqueeze_node = graph.make_node(\n            'Unsqueeze', inputs=inputs, axes=axes, outputs=outputs)\n    else:\n        axes_node = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                               'value': axes})\n        inputs.append(axes_node)\n        unsqueeze_node = graph.make_node(\n            'Unsqueeze', inputs=inputs, outputs=outputs)\n    return unsqueeze_node",
  "def split_helper(graph, inputs, outputs, axis, split, dtype=paddle.float32):\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n\n    if not isinstance(outputs, int) and not isinstance(outputs, (list, tuple)):\n        outputs = [outputs]\n\n    if dtype == paddle.float64:\n        cast_inputs = []\n        for i in range(len(inputs)):\n            one = graph.make_node(\n                'Cast', inputs=[inputs[i]], to=TensorProto.FLOAT)\n            cast_inputs.append(one)\n        if graph.opset_version < 13:\n            split_node = graph.make_node(\n                \"Split\",\n                inputs=cast_inputs,\n                outputs=outputs,\n                axis=axis,\n                split=split)\n        else:\n            split_const = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=split)\n            split_node = graph.make_node(\n                \"Split\",\n                inputs=cast_inputs + [split_const],\n                outputs=outputs,\n                axis=axis)\n        casted_output = []\n        for i in range(len(outputs)):\n            one = graph.make_node(\n                'Cast',\n                inputs=[split_node[i]],\n                outputs=[outputs[i]],\n                to=TensorProto.DOUBLE)\n            casted_output.append(one)\n        return casted_output\n    else:\n        if graph.opset_version < 13:\n            split_node = graph.make_node(\n                \"Split\", inputs=inputs, outputs=outputs, axis=axis, split=split)\n        else:\n            split_const = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=split)\n            split_node = graph.make_node(\n                \"Split\",\n                inputs=inputs + [split_const],\n                outputs=outputs,\n                axis=axis)\n        return split_node",
  "def constant_helper(graph, dtype, value, shape=None, outputs=[]):\n    constant = graph.make_node(\n        'Constant',\n        inputs=[],\n        outputs=outputs,\n        attrs={\n            'dims': shape,\n            'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[dtype],\n            'value': value\n        })\n    return constant",
  "def clip_helper(graph, node, input, max, min, output=[]):\n    x_dtype = node.input_dtype('X', 0)\n    if (isinstance(min, six.string_types) or\n            isinstance(max, six.string_types)) and graph.opset_version < 11:\n        raise Exception(\n            \"min or max of Clip is Tensor, please try with higher onnx opset_version.\"\n        )\n    if graph.opset_version < 11:\n        if x_dtype != paddle.float32:\n            input = graph.make_node(\n                'Cast', inputs=[input], to=dtypes.ONNX.FLOAT)\n            clip = graph.make_node('Clip', inputs=input, max=max, min=min)\n            clip = graph.make_node(\n                'Cast',\n                inputs=[clip],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],\n                outputs=output)\n        else:\n            clip = graph.make_node(\n                'Clip', inputs=input, max=max, min=min, outputs=output)\n    else:\n        if x_dtype != paddle.float32:\n            input = graph.make_node(\n                'Cast', inputs=[input], to=dtypes.ONNX.FLOAT)\n\n        if not isinstance(min, six.string_types):\n            min = graph.make_node(\n                'Constant',\n                attrs={\n                    'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32],\n                    'value': min\n                })\n        else:\n            if node.input_dtype('Min', 0) != paddle.float32:\n                min = graph.make_node(\n                    'Cast',\n                    inputs=min,\n                    attrs={'to': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32]})\n            min = graph.make_node('Squeeze', min)\n\n        if not isinstance(max, six.string_types):\n            max = graph.make_node(\n                'Constant',\n                attrs={\n                    'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32],\n                    'value': max\n                })\n        else:\n            if node.input_dtype('Max', 0) != paddle.float32:\n                max = graph.make_node(\n                    'Cast',\n                    inputs=max,\n                    attrs={'to': dtypes.DTYPE_PADDLE_ONNX_MAP[paddle.float32]})\n            max = graph.make_node('Squeeze', max)\n        if x_dtype != paddle.float32:\n            clip_pre = graph.make_node('Clip', inputs=[input, min, max])\n            clip = graph.make_node(\n                'Cast',\n                inputs=[clip_pre],\n                outputs=output,\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n        else:\n            clip = graph.make_node(\n                'Clip', inputs=[input, min, max], outputs=output)\n    return clip",
  "def dtype_alignment(graph, nodes, node_dtypes, to=None):\n    assert len(nodes) == len(\n        node_dtypes), \"Length of nodes and node_dtypes should be equal.\"\n    dtype_order = [\n        core.VarDesc.VarType.BOOL,\n        core.VarDesc.VarType.INT16,\n        core.VarDesc.VarType.INT32,\n        core.VarDesc.VarType.INT64,\n        core.VarDesc.VarType.FP16,\n        core.VarDesc.VarType.FP32,\n        core.VarDesc.VarType.FP64,\n    ]\n    max_index = -1\n    for dtype in node_dtypes:\n        index = dtype_order.index(dtype)\n        if index > max_index:\n            max_index = index\n\n    if max_index < 0:\n        return nodes\n\n    casted_nodes = list()\n    cast_dtype = dtype_order[max_index]\n    cast_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[cast_dtype]\n    for i, dtype in enumerate(node_dtypes):\n        index = dtype_order.index(dtype)\n        if to is not None:\n            cast_dtype = to\n            condition = dtypes.DTYPE_PADDLE_ONNX_MAP[index] != cast_dtype\n        else:\n            condition = index != max_index\n        if condition:\n            cast_node = graph.make_node(\n                'Cast', inputs=[nodes[i]], to=cast_dtype)\n            casted_nodes.append(cast_node)\n        else:\n            casted_nodes.append(nodes[i])\n    return casted_nodes",
  "def cast(graph, input, origin_dtype, target_dtype):\n    if not isinstance(origin_dtype, six.string_types):\n        origin_dtype = dtypes.DTYPE_PADDLE_STR_MAP[origin_dtype]\n    if origin_dtype != target_dtype:\n        cast_node = graph.make_node(\n            'Cast', inputs=input, to=dtypes.DTYPE_ONNX_STR_MAP[target_dtype])\n        return cast_node\n    return input",
  "def shape_alignment(graph, nodes, node_shapes):\n    assert len(nodes) == len(\n        node_shapes), \"Length of nodes and node_shapes should be equal.\"\n    max_dim = -1\n    for shape in node_shapes:\n        dim = len(shape)\n        if dim > max_dim:\n            max_dim = dim\n\n    if max_dim < 0:\n        return nodes\n\n    assert max_dim == 1 or max_dim == 0, \"max_dim is only supported when max_dim is 1 or 0.\"\n    max_dim = 1 if max_dim == 0 else max_dim\n    unsqueeze_nodes = list()\n    for i, shape in enumerate(node_shapes):\n        dim = len(shape)\n        if dim != max_dim:\n            unsqueeze_node = nodes[i]\n            for j in range(max_dim - dim):\n                unsqueeze_node = unsqueeze_helper(graph, unsqueeze_node, [0])\n            unsqueeze_nodes.append(unsqueeze_node)\n        else:\n            unsqueeze_nodes.append(nodes[i])\n    return unsqueeze_nodes",
  "def get_tensor_list_node(graph, node, name, dtype=None):\n    node_list = node.input(name)\n    node_dtypes = [node.input_dtype(name, i) for i in range(len(node_list))]\n    node_list = dtype_alignment(graph, node_list, node_dtypes, dtype)\n\n    node_shapes = [node.input_shape(name, i) for i in range(len(node_list))]\n    node_list = shape_alignment(graph, node_list, node_shapes)\n    node = graph.make_node(\"Concat\", inputs=node_list, axis=0)\n    return node",
  "def get_value_from_parameters(graph, input_node):\n    assert input_node in graph.parameters, \"{} is not in graph.parameters\".format(\n        input_node)\n    data = graph.parameters[input_node].attribute[0].t.int32_data\n    if data is None or len(data) < 1:\n        data = graph.parameters[input_node].attribute[0].t.int64_data\n    value = [val for _, val in enumerate(data)]\n    return value",
  "def get_node_attr_value(graph,\n                        node,\n                        attr_name=None,\n                        attr_tensor_name=None,\n                        attr_tensor_list_name=None,\n                        return_list=False,\n                        dtype=None):\n    attr_tensor = node.input(attr_tensor_name)\n    attr_tensor_list = node.input(attr_tensor_list_name)\n    if attr_tensor is not None and len(attr_tensor) > 0:\n        value = node.input(attr_tensor_name)[0]\n        if return_list:\n            try:\n                value = get_value_from_parameters(graph, value)\n                return value, False  # value, is_tensor\n            except Exception:\n                return value, True\n        else:\n            input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(\n                attr_tensor_name, 0)]\n            if input_dtype != dtype:\n                value = graph.make_node('Cast', inputs=[value], to=dtype)\n            return value, True\n    elif attr_tensor_list is not None and len(attr_tensor_list) > 0:\n        value = get_tensor_list_node(graph, node, attr_tensor_list_name, dtype)\n        return value, True\n    else:\n        value = node.attr(attr_name)\n        return value, False",
  "class Conv():\n    support_opset_version_range = (1, 12)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        kernel_shape = node.input_shape('Filter', 0)\n        dilations = node.attr('dilations')\n        kernel_shape = kernel_shape[2:]\n        strides = node.attr('strides')\n        group = node.attr('groups')\n        pads = node.attr('paddings')\n        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == 'NCDHW' or node.attrs['data_format'] == \"AnyLayout\",  \\\n                            \"The conv data format should be 'NCHW' or 'NCDHW', but received data format \" \\\n                            \"is %s.\" % node.attrs['data_format']\n        # onnx padding is [x1_begin, x2_begin...x1_end, x2_end, ...]\n        if len(pads) == 2 or len(pads) == 3:\n            pads = pads + pads\n        elif len(pads) == 4:\n            pads = [pads[i] for i in [0, 2, 1, 3]]\n        elif len(pads) == 6:\n            pads = [pads[i] for i in [0, 2, 4, 1, 3, 5]]\n        attrs = {\n            'dilations': dilations,\n            'kernel_shape': kernel_shape,\n            'strides': strides,\n            'group': group\n        }\n        auto_pad = node.attr('padding_algorithm')\n        if auto_pad == 'SAME':\n            attrs['auto_pad'] = 'SAME_UPPER'\n        elif auto_pad == 'VALID':\n            attrs['auto_pad'] = 'VALID'\n        else:\n            attrs['pads'] = pads\n        graph.make_node(\n            'Conv',\n            inputs=node.input('Input') + node.input('Filter'),\n            outputs=node.output('Output'),\n            attrs=attrs)",
  "class ConvTranspose():\n    support_opset_version_range = (1, 12)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        output_padding = node.attr('output_padding')\n        kernel_shape = node.input_shape('Filter', 0)\n        dilations = node.attr('dilations')\n        kernel_shape = kernel_shape[2:]\n        strides = node.attr('strides')\n        group = node.attr('groups')\n        pads = node.attr('paddings')\n        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == 'NCDHW', \\\n            \"The conv data format should be 'NCHW' or 'NCDHW', but received data format \" \\\n            \"is %s.\" % node.attrs['data_format']\n\n        if len(pads) == 2 or len(pads) == 3:\n            pads = pads + pads\n        elif len(pads) == 4:\n            pads = [pads[i] for i in [0, 2, 1, 3]]\n        elif len(pads) == 6:\n            pads = [pads[i] for i in [0, 2, 4, 1, 3, 5]]\n\n        attrs = {\n            'dilations': dilations,\n            'kernel_shape': kernel_shape,\n            'strides': strides,\n            'group': group\n        }\n        auto_pad = node.attr('padding_algorithm')\n        if auto_pad == 'SAME':\n            attrs['auto_pad'] = 'SAME_UPPER'\n        elif auto_pad == 'VALID':\n            attrs['auto_pad'] = 'VALID'\n        else:\n            attrs['pads'] = pads\n        if output_padding and len(output_padding) > 0:\n            attrs['output_padding'] = output_padding\n        graph.make_node(\n            'ConvTranspose',\n            inputs=node.input('Input') + node.input('Filter'),\n            outputs=node.output('Output'),\n            attrs=attrs)",
  "class Pool():\n    support_opset_version_range = (1, 12)\n    pool_type = {\n        'max': ('MaxPool', 'GlobalMaxPool'),\n        'avg': ('AveragePool', 'GlobalAveragePool')\n    }\n\n    @classmethod\n    def is_same_span(cls, in_size, out_size):\n        spans = []\n        for i in range(out_size):\n            start = math.floor(i * (in_size / out_size))\n            end = math.ceil((i + 1) * (in_size / out_size))\n            spans.append(end - start)\n        if len(set(spans)) == 1:\n            return True\n        return False\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == \"AnyLayout\",  \\\n                            \"The conv data format should be 'NCHW', but received data format \" \\\n                            \"is %s.\" % node.attrs['data_format']\n        x_dtype = node.input_dtype('X', 0)\n        need_dtype_convert = False\n        input_name = node.input('X', 0)\n        if x_dtype != paddle.float32:\n            need_dtype_convert = True\n            input_name = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.FLOAT)\n\n        if node.attr('global_pooling') or (node.attr('adaptive') and\n                                           node.attr('ksize') == [1, 1]):\n            if need_dtype_convert:\n                onnx_node = graph.make_node(\n                    cls.pool_type[node.attr('pooling_type')][1],\n                    inputs=[input_name])\n                graph.make_node(\n                    'Cast',\n                    inputs=[onnx_node],\n                    outputs=node.output('Out'),\n                    to=dtypes.ONNX.DOUBLE)\n            else:\n                onnx_node = graph.make_node(\n                    cls.pool_type[node.attr('pooling_type')][1],\n                    inputs=[input_name],\n                    outputs=node.output('Out'))\n        elif node.attr('adaptive'):\n            # if pool is adaptive, check if input shape of pool is fixed.\n            if node.input_shape('X', 0)[2:].count(-1) > 0:\n                raise Exception(\n                    \"Converting this model to ONNX need with static input shape,\" \\\n                    \" please fix input shape of this model, see doc Q2 in\" \\\n                    \" https://github.com/PaddlePaddle/paddle2onnx/blob/develop/docs/en/FAQ.md.\"\n                )\n            input_h, input_w = node.input_shape('X', 0)[2:]\n            output_h, output_w = node.output_shape('Out', 0)[2:]\n            stride_h = int(input_h / output_h)\n            stride_w = int(input_w / output_w)\n\n            kernel_h = input_h - (output_h - 1) * stride_h\n            kernel_w = input_w - (output_w - 1) * stride_w\n\n            #check if kernel_size is fixed.\n            if not cls.is_same_span(input_h, output_h) or not cls.is_same_span(\n                    input_w, output_w):\n                raise Exception(\n                    \"Cannot convert adaptive pool with input_size: {}, output_size: {}\"\n                    .format(\n                        node.input_shape('X', 0), node.output_shape('Out', 0)))\n            else:\n                attrs = {\n                    'kernel_shape': (kernel_h, kernel_w),\n                    'strides': (stride_h, stride_w),\n                }\n                if node.attr('ceil_mode') and graph.opset_version < 10:\n                    raise Exception(\n                        \"Cannot convert pool with ceil_model == True to ONNX Opset version < 10.\"\n                    )\n                elif graph.opset_version > 10:\n                    attrs['ceil_mode'] = node.attr('ceil_mode')\n                auto_pad = node.attr('padding_algorithm')\n                if auto_pad == 'SAME':\n                    attrs['auto_pad'] = 'SAME_UPPER'\n                elif auto_pad == 'VALID':\n                    attrs['auto_pad'] = 'VALID'\n                if node.attr('pooling_type') == 'avg':\n                    attrs['count_include_pad'] = not node.attr('exclusive')\n                if need_dtype_convert:\n                    onnx_node = graph.make_node(\n                        cls.pool_type[node.attr('pooling_type')][0],\n                        inputs=[input_name],\n                        attrs=attrs)\n                    graph.make_node(\n                        'Cast',\n                        inputs=[onnx_node],\n                        outputs=node.output('Out'),\n                        to=dtypes.ONNX.DOUBLE)\n                else:\n                    onnx_node = graph.make_node(\n                        cls.pool_type[node.attr('pooling_type')][0],\n                        inputs=[input_name],\n                        outputs=node.output('Out'),\n                        attrs=attrs)\n        else:\n            input_shape = node.input_shape('X', 0)\n            k_size = node.attr('ksize')\n            pads = node.attr('paddings')\n            strides = node.attr('strides')\n\n            if len(pads) == 2:\n                pads = pads + pads\n            elif len(pads) == 4:\n                pads = [pads[i] for i in [0, 2, 1, 3]]\n\n            if input_shape[2] > 0 and input_shape[2] + pads[0] < k_size[0]:\n                k_size[0] = input_shape[2] + pads[0]\n            if input_shape[3] > 0 and input_shape[3] + pads[1] < k_size[1]:\n                k_size[1] = input_shape[3] + pads[1]\n\n            input_x = [input_name]\n            if max(k_size) <= max(pads):\n                onnx_paddings = [0, 0, pads[0], pads[1], 0, 0, pads[2], pads[3]]\n                attrs_pad = {'mode': 'constant', }\n                if graph.opset_version >= 11:\n                    pads_node = graph.make_node(\n                        'Constant',\n                        attrs={\n                            'dtype': dtypes.ONNX.INT64,\n                            'value': onnx_paddings\n                        })\n                    value_node = graph.make_node(\n                        'Constant',\n                        attrs={'dtype': dtypes.ONNX.FLOAT,\n                               'value': 0.0})\n                    input_x = input_x + [pads_node, value_node]\n                else:\n                    attrs_pad['pads'] = onnx_paddings\n                    attrs_pad['value'] = 0.0\n                input_x = graph.make_node(\n                    'Pad', inputs=input_x, attrs=attrs_pad)\n                pads = [0, 0, 0, 0]\n\n            attrs = {\n                'kernel_shape': k_size,\n                'strides': strides,\n            }\n            auto_pad = node.attr('padding_algorithm')\n            if auto_pad == 'SAME':\n                attrs['auto_pad'] = 'SAME_UPPER'\n            elif auto_pad == 'VALID':\n                attrs['auto_pad'] = 'VALID'\n            else:\n                attrs['pads'] = pads\n            if node.attr('ceil_mode') and graph.opset_version < 10:\n                raise Exception(\n                    \"Cannot convert pool with ceil_model == True to ONNX Opset version < 10\"\n                )\n            elif graph.opset_version >= 10:\n                attrs['ceil_mode'] = node.attr('ceil_mode')\n\n            if node.attr('pooling_type') == 'avg':\n                attrs['count_include_pad'] = not node.attr('exclusive')\n            if need_dtype_convert:\n                onnx_node = graph.make_node(\n                    cls.pool_type[node.attr('pooling_type')][0],\n                    inputs=input_x,\n                    attrs=attrs)\n                graph.make_node(\n                    'Cast',\n                    inputs=[onnx_node],\n                    outputs=node.output('Out'),\n                    to=dtypes.ONNX.DOUBLE)\n            else:\n                onnx_node = graph.make_node(\n                    cls.pool_type[node.attr('pooling_type')][0],\n                    inputs=input_x,\n                    outputs=node.output('Out'),\n                    attrs=attrs)",
  "class Pool3D():\n    support_opset_version_range = (1, 12)\n    pool_type = {\n        'max': ('MaxPool', 'GlobalMaxPool'),\n        'avg': ('AveragePool', 'GlobalAveragePool')\n    }\n\n    @classmethod\n    def is_same_span(cls, in_size, out_size):\n        spans = []\n        for i in range(out_size):\n            start = math.floor(i * (in_size / out_size))\n            end = math.ceil((i + 1) * (in_size / out_size))\n            spans.append(end - start)\n        if len(set(spans)) == 1:\n            return True\n        return False\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        assert node.attrs['data_format'] == 'NCDHW' or node.attrs['data_format'] == \"AnyLayout\",  \\\n                            \"The conv data format should be 'NCDHW', but received data format \" \\\n                            \"is %s.\" % node.attrs['data_format']\n\n        if node.attr('global_pooling') or (node.attr('adaptive') and\n                                           node.attr('ksize') == [1, 1, 1]):\n            onnx_node = graph.make_node(\n                cls.pool_type[node.attr('pooling_type')][1],\n                inputs=node.input('X'),\n                outputs=node.output('Out'))\n        elif node.attr('adaptive'):\n            # if pool is adaptive, check if input shape of pool is fixed.\n            if node.input_shape('X', 0)[2:].count(-1) > 0:\n                raise Exception(\n                    \"Converting this model to ONNX need with static input shape,\" \\\n                    \" please fix input shape of this model, see doc Q2 in\" \\\n                    \" https://github.com/PaddlePaddle/paddle2onnx/blob/develop/docs/en/FAQ.md.\"\n                )\n            input_d, input_h, input_w = node.input_shape('X', 0)[2:]\n            output_d, output_h, output_w = node.output_shape('Out', 0)[2:]\n            stride_d = int(input_d / output_d)\n            stride_h = int(input_h / output_h)\n            stride_w = int(input_w / output_w)\n\n            kernel_d = input_d - (output_d - 1) * stride_d\n            kernel_h = input_h - (output_h - 1) * stride_h\n            kernel_w = input_w - (output_w - 1) * stride_w\n\n            #check if kernel_size is fixed.\n            if not cls.is_same_span(input_h, output_h) or not cls.is_same_span(\n                    input_w, output_w) or not cls.is_same_span(input_d,\n                                                               output_d):\n                raise Exception(\n                    \"Cannot convert adaptive pool with input_size: {}, output_size: {}\"\n                    .format(\n                        node.input_shape('X', 0), node.output_shape('Out', 0)))\n            else:\n                attrs = {\n                    'kernel_shape': (kernel_d, kernel_h, kernel_w),\n                    'strides': (stride_d, stride_h, stride_w),\n                }\n                if node.attr('ceil_mode') and graph.opset_version < 10:\n                    raise Exception(\n                        \"Cannot convert pool with ceil_model == True to ONNX Opset version < 10.\"\n                    )\n                elif graph.opset_version > 10:\n                    attrs['ceil_mode'] = node.attr('ceil_mode')\n                auto_pad = node.attr('padding_algorithm')\n                if auto_pad == 'SAME':\n                    attrs['auto_pad'] = 'SAME_UPPER'\n                elif auto_pad == 'VALID':\n                    attrs['auto_pad'] = 'VALID'\n                if node.attr('pooling_type') == 'avg':\n                    attrs['count_include_pad'] = not node.attr('exclusive')\n                onnx_node = graph.make_node(\n                    cls.pool_type[node.attr('pooling_type')][0],\n                    inputs=node.input('X'),\n                    outputs=node.output('Out'),\n                    attrs=attrs)\n        else:\n            input_shape = node.input_shape('X', 0)\n            k_size = node.attr('ksize')\n            paddings = node.attr('paddings')\n            if input_shape[2] > 0 and input_shape[2] + paddings[0] < k_size[0]:\n                k_size[0] = input_shape[2] + paddings[0]\n            if input_shape[3] > 0 and input_shape[3] + paddings[1] < k_size[1]:\n                k_size[1] = input_shape[3] + paddings[1]\n            if input_shape[4] > 0 and input_shape[4] + paddings[2] < k_size[2]:\n                k_size[2] = input_shape[4] + paddings[2]\n            attrs = {\n                'kernel_shape': k_size,\n                'strides': node.attr('strides'),\n                'pads': node.attr('paddings') + node.attr('paddings'),\n            }\n            if node.attr('ceil_mode') and graph.opset_version < 10:\n                raise Exception(\n                    \"Cannot convert pool with ceil_model == True to ONNX Opset version < 10\"\n                )\n            elif graph.opset_version >= 10:\n                attrs['ceil_mode'] = node.attr('ceil_mode')\n\n            if node.attr('pooling_type') == 'avg':\n                attrs['count_include_pad'] = not node.attr('exclusive')\n            onnx_node = graph.make_node(\n                cls.pool_type[node.attr('pooling_type')][0],\n                inputs=node.input('X'),\n                outputs=node.output('Out'),\n                attrs=attrs)",
  "class ELU():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'Elu',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            alpha=node.attr('alpha'))",
  "class SoftSign():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Softsign', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Hardshrink():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        node = graph.make_node(\n            'Shrink',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            lambd=node.attr('threshold'))",
  "class LogSigmoid():\n    support_opset_version_range = (1, 12)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        sigmoid_node = graph.make_node('Sigmoid', inputs=node.input('X'))\n        graph.make_node('Log', inputs=sigmoid_node, outputs=node.output('Out'))",
  "class Norm():\n    support_opset_version_range = (1, 12)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'LpNormalization',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            axis=node.attr('axis'))",
  "class SoftShrink():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Shrink',\n            inputs=node.input('X'),\n            bias=node.attr('lambda'),\n            lambd=node.attr('lambda'),\n            outputs=node.output('Out'))",
  "class TanhShrink():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        tanh_node = graph.make_node(\n            'Tanh',\n            inputs=node.input('X', 0), )\n        graph.make_node(\n            'Sub',\n            inputs=[node.input('X', 0), tanh_node],\n            outputs=node.output('Out'))",
  "class LogSoftmax():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        axis = node.attr('axis')\n        shape = node.output_shape('Out', 0)\n        if axis is None:\n            axis = -1\n        if axis < 0:\n            axis += len(shape)\n        if axis == len(shape) - 1:\n            node = graph.make_node(\n                'LogSoftmax',\n                inputs=node.input('X'),\n                outputs=node.output('Out'),\n                attrs={'axis': axis})\n        else:\n            perm = [i for i in range(len(shape))]\n            perm[-1] = axis\n            perm[axis] = len(shape) - 1\n            transpose_node = graph.make_node(\n                'Transpose', inputs=node.input('X'), attrs={'perm': perm})\n            softmax_node = graph.make_node(\n                'LogSoftmax', inputs=[transpose_node], axis=-1)\n            transpose_node1 = graph.make_node(\n                'Transpose',\n                inputs=[softmax_node],\n                outputs=node.output('Out'),\n                attrs={'perm': perm})\n\n    @classmethod\n    def opset_13(cls, graph, node, **kw):\n        graph.make_node(\n            'LogSoftmax',\n            inputs=node.input('X'),\n            axis=node.attr('axis'),\n            outputs=node.output('Out'))",
  "class LayerNorm():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        ipt = node.input('X', 0)\n        ipt_dims = len(node.input_shape('X', 0))\n        normalized_shape = node.attr('begin_norm_axis')\n        axes = None\n        if isinstance(normalized_shape, collections.Iterable):\n            axes = [-i for i in range(len(normalized_shape), 0, -1)]\n        else:\n            axes = [i for i in range(normalized_shape, ipt_dims)]\n        dtype = node.block.vars[node.input('X', 0)].dtype\n        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n        epsilon = graph.make_node(\n            'Constant', dtype=dtype, value=node.attr('epsilon'))\n        two = graph.make_node('Constant', dtype=dtype, value=2.0)\n        mean = graph.make_node(\"ReduceMean\", inputs=[ipt], axes=axes)\n        numerator = graph.make_node(\"Sub\", inputs=[ipt, mean])\n        pow_num = graph.make_node(\"Pow\", inputs=[numerator, two])\n        variance = graph.make_node(\"ReduceMean\", inputs=[pow_num], axes=axes)\n        add_eps = graph.make_node(\"Add\", inputs=[variance, epsilon])\n        denominator = graph.make_node(\"Sqrt\", inputs=[add_eps])\n\n        ipt_shape = graph.make_node(\"Shape\", inputs=[ipt])\n        weight_shape = mapper_helper.slice_helper(\n            graph, ipt_shape, [0], [ipt_dims - len(axes)], [ipt_dims])\n        if 'Bias' in node.inputs and 'Scale' in node.inputs and len(\n                node.input('Scale')) > 0 and len(node.input('Bias')) > 0:\n            if normalized_shape == ipt_dims - 1:\n                shape_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])\n                scale = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Scale', 0), shape_const])\n                bias = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Bias', 0), shape_const])\n            else:\n                scale = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Scale', 0), weight_shape])\n                bias = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Bias', 0), weight_shape])\n            layer_norm = graph.make_node(\"Div\", inputs=[numerator, denominator])\n            layer_norm = graph.make_node(\"Mul\", inputs=[layer_norm, scale])\n            graph.make_node(\n                \"Add\", inputs=[layer_norm, bias], outputs=node.output('Y'))\n        elif 'Bias' in node.inputs and len(node.input('Bias')) > 0:\n            if normalized_shape == ipt_dims - 1:\n                shape_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])\n                bias = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Bias', 0), shape_const])\n            else:\n                bias = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Bias', 0), weight_shape])\n            layer_norm = graph.make_node(\"Div\", inputs=[numerator, denominator])\n            graph.make_node(\n                \"Add\", inputs=[layer_norm, bias], outputs=node.output('Y'))\n        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:\n            if normalized_shape == ipt_dims - 1:\n                shape_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])\n                scale = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Scale', 0), shape_const])\n            else:\n                scale = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Scale', 0), weight_shape])\n            layer_norm = graph.make_node(\"Div\", inputs=[numerator, denominator])\n            graph.make_node(\n                \"Mul\", inputs=[layer_norm, scale], outputs=node.output('Y'))\n        else:\n            layer_norm = graph.make_node(\n                \"Div\",\n                inputs=[numerator, denominator],\n                outputs=node.output('Y'))",
  "class BatchNorm():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def make_attrs_and_inputs(cls, graph, node, **kw):\n        onnx_attr = {\n            'epsilon': node.attr('epsilon'),\n            'momentum': node.attr('momentum')\n        }\n        inputs = node.input('X') + node.input('Scale') + node.input(\n            'Bias') + node.input('Mean') + node.input('Variance')\n        return onnx_attr, inputs\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        onnx_attr, inputs = cls.make_attrs_and_inputs(graph, node, **kw)\n        onnx_node = graph.make_node(\n            'BatchNormalization',\n            inputs=inputs,\n            outputs=node.output('Y'),\n            **onnx_attr)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        onnx_attr, inputs = cls.make_attrs_and_inputs(graph, node, **kw)\n        onnx_attr['spatial'] = 1\n        onnx_node = graph.make_node(\n            'BatchNormalization',\n            inputs=inputs,\n            outputs=node.output('Y'),\n            **onnx_attr)",
  "class GroupNorm():\n    support_opset_version_range = (6, 15)\n\n    @classmethod\n    def opset_6(cls, graph, node, **kw):\n        num_groups = node.attr('groups')\n        epsilon = node.attr('epsilon')\n        ipt = node.input('X')[0]\n\n        ipt_shape = node.input_shape('X', 0)\n        assert len(\n            ipt_shape) == 4, \"Only support 4D-Tensor as input for GroupNorm\"\n\n        dtype = node.block.vars[node.input('X', 0)].dtype\n        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n\n        shape = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[0, num_groups, -1])\n        reshape_input = graph.make_node('Reshape', inputs=[ipt, shape])\n        scale_ = graph.make_node(\n            'Constant', dtype=dtype, value=[1.0] * num_groups)\n        bias_ = graph.make_node(\n            'Constant', dtype=dtype, value=[0.0] * num_groups)\n        reshaped_output = graph.make_node(\n            'InstanceNormalization',\n            inputs=[reshape_input, scale_, bias_],\n            epsilon=epsilon)\n        origin_shape = graph.make_node('Shape', inputs=[ipt])\n\n        if len(node.input('Scale')) > 0 and len(node.input('Bias')) > 0:\n            output = graph.make_node(\n                'Reshape', inputs=[reshaped_output, origin_shape])\n            unsqueezed_scale = mapper_helper.unsqueeze_helper(\n                graph, node.input('Scale', 0), [1, 2])\n            unsqueezed_bias = mapper_helper.unsqueeze_helper(\n                graph, node.input('Bias', 0), [1, 2])\n            part0 = graph.make_node('Mul', inputs=[output, unsqueezed_scale])\n            graph.make_node(\n                'Add',\n                inputs=[part0, unsqueezed_bias],\n                outputs=node.output('Y'))\n        else:\n            output = graph.make_node(\n                'Reshape',\n                inputs=[reshaped_output, origin_shape],\n                outputs=node.output('Y'))",
  "class InstanceNorm():\n    support_opset_version_range = (6, 15)\n\n    @classmethod\n    def opset_6(cls, graph, node, **kw):\n        onnx_attr = {'epsilon': node.attr('epsilon'), }\n        num_groups = node.block.vars[node.input('X')[0]].shape[1]\n\n        dtype = node.block.vars[node.input('X', 0)].dtype\n        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n\n        if len(node.input('Scale')) == 0:\n            scale_ = graph.make_node(\n                'Constant', dtype=dtype, value=[1.0] * num_groups)\n        else:\n            scale_ = node.input('Scale')[0]\n        if len(node.input('Bias')) == 0:\n            bias_ = graph.make_node(\n                'Constant', dtype=dtype, value=[0.0] * num_groups)\n        else:\n            bias_ = node.input('Bias')[0]\n\n        inputs = node.input('X') + [scale_] + [bias_]\n        onnx_node = graph.make_node(\n            'InstanceNormalization',\n            inputs=inputs,\n            outputs=node.output('Y'),\n            **onnx_attr)",
  "class Dropout():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        dropout_mode = node.attr('dropout_implementation')\n        dropout_prob = node.attr('dropout_prob')\n        if dropout_mode == 'upscale_in_train':\n            onnx_node = graph.make_node(\n                'Identity', inputs=node.input('X'), outputs=node.output('Out'))\n        elif dropout_mode == 'downgrade_in_infer':\n            scale_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.FLOAT,\n                       'value': 1 - dropout_prob})\n            graph.make_node(\n                \"Mul\",\n                inputs=[node.input('X')[0], scale_node],\n                outputs=node.output('Out'))\n        else:\n            raise Exception(\"Unexpected situation happend\")",
  "class RoiAlign():\n    support_opset_version_range = (10, 16)\n\n    @classmethod\n    def opset_10(cls, graph, node, **kw):\n        if node.attr('aligned') and graph.opset_version < 16:\n            raise Exception(\n                'when aligned is true, onnx opset should be (onnx_opset>= 16)')\n        rois_shape = graph.make_node('Shape', inputs=[node.input('ROIs', 0)])\n        starts = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                               'value': [0]})\n        ends = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                               'value': [1]})\n        num_rois = graph.make_node('Slice', inputs=[rois_shape, starts, ends])\n        zero = graph.make_node(\n            'Constant', dims=[1], dtype=dtypes.ONNX.INT64, value=[0])\n        batch_indices = graph.make_node('Expand', inputs=[zero, num_rois])\n        node = graph.make_node(\n            'RoiAlign',\n            inputs=[node.input('X', 0), node.input('ROIs', 0), batch_indices],\n            outputs=node.output('Out'),\n            mode='avg',\n            output_height=node.attr('pooled_height'),\n            output_width=node.attr('pooled_width'),\n            sampling_ratio=node.attr('sampling_ratio'),\n            spatial_scale=node.attr('spatial_scale'))",
  "class RNN():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def make_param_inputs(cls, graph, node, layer, hidden_size, num_layers):\n        # weight assign order:\n        # (F_whi F_whh B_whi B_whh\uff09* layer_num  + (F_bias_hi F_bias_hh B_bias_hi  B_bias_hi)* layer_num\n        def reform_weights(g, w, n, intervals):\n            slices = [\n                mapper_helper.slice_helper(\n                    g, w, axes=[1], starts=[x * n], ends=[y * n])\n                for x, y in intervals\n            ]\n            return g.make_node('Concat', slices, axis=1)\n\n        def transform_weight_with_bias(g, weights, n, intervals):\n            return [reform_weights(g, w, n, intervals) for w in weights]\n\n        if node.attr('mode') == 'LSTM':\n            reform_permutation = [(0, 1), (3, 4), (1, 3)]\n        elif node.attr('mode') == 'GRU':\n            reform_permutation = [(1, 2), (0, 1), (2, 3)]\n        bidirect_len = 4 if node.attr('is_bidirec') else 2\n        all_layer_param_len = len(node.input('WeightList'))\n        weight_list = node.input('WeightList')[:all_layer_param_len // 2]\n        bias_list = node.input('WeightList')[all_layer_param_len // 2:]\n        single_layer_param_len = all_layer_param_len // num_layers\n\n        unsqueeze_weights = []\n        layer_weight_list = weight_list[layer * bidirect_len:layer *\n                                        bidirect_len + bidirect_len]\n        layer_bias_list = bias_list[layer * bidirect_len:layer * bidirect_len +\n                                    bidirect_len]\n        param_list = layer_weight_list + layer_bias_list\n        param_list_len = len(param_list)\n        for i in range(param_list_len):\n            weight = mapper_helper.unsqueeze_helper(graph, param_list[i], [0])\n            unsqueeze_weights.append(weight)\n\n        input_weights = unsqueeze_weights[0:param_list_len // 2:2]\n        hidden_weights = unsqueeze_weights[1:param_list_len // 2:2]\n\n        input_weight = graph.make_node('Concat', inputs=input_weights, axis=0)\n        hidden_weight = graph.make_node('Concat', inputs=hidden_weights, axis=0)\n        input_bias = unsqueeze_weights[param_list_len // 2:param_list_len:2]\n        hidden_bias = unsqueeze_weights[param_list_len // 2 + 1:param_list_len:\n                                        2]\n\n        input_bias = graph.make_node('Concat', inputs=input_bias, axis=0)\n        hidden_bias = graph.make_node('Concat', inputs=hidden_bias, axis=0)\n        input_weight, hidden_weight, input_bias, hidden_bias = transform_weight_with_bias(\n            graph, [input_weight, hidden_weight, input_bias, hidden_bias],\n            hidden_size, reform_permutation)\n        bias = graph.make_node(\n            'Concat', inputs=[input_bias, hidden_bias], axis=1)\n        return [input_weight, hidden_weight, bias, '']\n\n    @classmethod\n    def make_init_param_inputs(cls, graph, node, layer):\n        if node.attr('mode') == 'LSTM':\n            all_init_h, all_init_c = node.input('PreState')\n            bidirect_len = 2 if node.attr('is_bidirec') else 1\n            init_h = mapper_helper.slice_helper(\n                graph, all_init_h, [0], [layer * bidirect_len],\n                [layer * bidirect_len + bidirect_len])\n            init_c = mapper_helper.slice_helper(\n                graph, all_init_c, [0], [layer * bidirect_len],\n                [layer * bidirect_len + bidirect_len])\n            return [init_h, init_c]\n        elif node.attr('mode') == 'GRU':\n            all_init_h = node.input('PreState', 0)\n            bidirect_len = 2 if node.attr('is_bidirec') else 1\n            init_h = mapper_helper.slice_helper(\n                graph, all_init_h, [0], [layer * bidirect_len],\n                [layer * bidirect_len + bidirect_len])\n            return [init_h]\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        mode = node.attr('mode')\n        hidden_size = node.attr('hidden_size')\n        num_layers = node.attr('num_layers')\n        prev_output = node.input('Input', 0)\n        if node.attr('mode') == 'LSTM':\n            for layer in range(num_layers):\n                param_inputs = cls.make_param_inputs(graph, node, layer,\n                                                     hidden_size, num_layers)\n                init_param_inputs = cls.make_init_param_inputs(graph, node,\n                                                               layer)\n                if layer + 1 < num_layers:\n                    rnn_outputs = 3\n                    output_y = None\n                else:\n                    rnn_outputs = [1] + node.output('State')\n                    output_y = node.output('Out')\n                prev_output, h_out, c_out = graph.make_node(\n                    node.attr('mode'),\n                    inputs=[prev_output] + param_inputs + init_param_inputs,\n                    outputs=rnn_outputs,\n                    direction='bidirectional'\n                    if node.attr('is_bidirec') else 'forward',\n                    hidden_size=node.attr('hidden_size'))\n                prev_output = graph.make_node(\n                    'Transpose', inputs=[prev_output], perm=[0, 2, 1, 3])\n\n                prev_shape = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[0, 0, -1])\n                prev_output = graph.make_node(\n                    'Reshape',\n                    inputs=[prev_output, prev_shape],\n                    outputs=output_y)\n        elif node.attr('mode') == 'GRU':\n            for layer in range(num_layers):\n                param_inputs = cls.make_param_inputs(graph, node, layer,\n                                                     hidden_size, num_layers)\n                init_param_inputs = cls.make_init_param_inputs(graph, node,\n                                                               layer)\n                if layer + 1 < num_layers:\n                    rnn_outputs = 2\n                    output_y = None\n                else:\n                    rnn_outputs = [1] + node.output('State')\n                    output_y = node.output('Out')\n                attrs = {\n                    'direction': 'bidirectional'\n                    if node.attr('is_bidirec') else 'forward',\n                    'hidden_size': node.attr('hidden_size'),\n                    'linear_before_reset': 1,\n                }\n                prev_output, h_out = graph.make_node(\n                    node.attr('mode'),\n                    inputs=[prev_output] + param_inputs + init_param_inputs,\n                    outputs=rnn_outputs,\n                    attrs=attrs)\n                prev_output = graph.make_node(\n                    'Transpose', inputs=[prev_output], perm=[0, 2, 1, 3])\n                prev_shape = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[0, 0, -1])\n                prev_output = graph.make_node(\n                    'Reshape',\n                    inputs=[prev_output, prev_shape],\n                    outputs=output_y)",
  "class ThresholdedRelu():\n    support_opset_version_range = (10, 15)\n\n    @classmethod\n    def opset_10(cls, graph, node, **kw):\n        x_dtype = node.input_dtype('X', 0)\n        if x_dtype != paddle.float32:\n            x = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.FLOAT)\n            threshholdedrelu_node = graph.make_node(\n                'ThresholdedRelu', inputs=[x], alpha=node.attr('threshold'))\n            graph.make_node(\n                'Cast',\n                inputs=[threshholdedrelu_node],\n                outputs=node.output('Out'),\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n        else:\n            graph.make_node(\n                'ThresholdedRelu',\n                inputs=node.input('X'),\n                alpha=node.attr('threshold'),\n                outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        kernel_shape = node.input_shape('Filter', 0)\n        dilations = node.attr('dilations')\n        kernel_shape = kernel_shape[2:]\n        strides = node.attr('strides')\n        group = node.attr('groups')\n        pads = node.attr('paddings')\n        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == 'NCDHW' or node.attrs['data_format'] == \"AnyLayout\",  \\\n                            \"The conv data format should be 'NCHW' or 'NCDHW', but received data format \" \\\n                            \"is %s.\" % node.attrs['data_format']\n        # onnx padding is [x1_begin, x2_begin...x1_end, x2_end, ...]\n        if len(pads) == 2 or len(pads) == 3:\n            pads = pads + pads\n        elif len(pads) == 4:\n            pads = [pads[i] for i in [0, 2, 1, 3]]\n        elif len(pads) == 6:\n            pads = [pads[i] for i in [0, 2, 4, 1, 3, 5]]\n        attrs = {\n            'dilations': dilations,\n            'kernel_shape': kernel_shape,\n            'strides': strides,\n            'group': group\n        }\n        auto_pad = node.attr('padding_algorithm')\n        if auto_pad == 'SAME':\n            attrs['auto_pad'] = 'SAME_UPPER'\n        elif auto_pad == 'VALID':\n            attrs['auto_pad'] = 'VALID'\n        else:\n            attrs['pads'] = pads\n        graph.make_node(\n            'Conv',\n            inputs=node.input('Input') + node.input('Filter'),\n            outputs=node.output('Output'),\n            attrs=attrs)",
  "def opset_1(cls, graph, node, **kw):\n        output_padding = node.attr('output_padding')\n        kernel_shape = node.input_shape('Filter', 0)\n        dilations = node.attr('dilations')\n        kernel_shape = kernel_shape[2:]\n        strides = node.attr('strides')\n        group = node.attr('groups')\n        pads = node.attr('paddings')\n        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == 'NCDHW', \\\n            \"The conv data format should be 'NCHW' or 'NCDHW', but received data format \" \\\n            \"is %s.\" % node.attrs['data_format']\n\n        if len(pads) == 2 or len(pads) == 3:\n            pads = pads + pads\n        elif len(pads) == 4:\n            pads = [pads[i] for i in [0, 2, 1, 3]]\n        elif len(pads) == 6:\n            pads = [pads[i] for i in [0, 2, 4, 1, 3, 5]]\n\n        attrs = {\n            'dilations': dilations,\n            'kernel_shape': kernel_shape,\n            'strides': strides,\n            'group': group\n        }\n        auto_pad = node.attr('padding_algorithm')\n        if auto_pad == 'SAME':\n            attrs['auto_pad'] = 'SAME_UPPER'\n        elif auto_pad == 'VALID':\n            attrs['auto_pad'] = 'VALID'\n        else:\n            attrs['pads'] = pads\n        if output_padding and len(output_padding) > 0:\n            attrs['output_padding'] = output_padding\n        graph.make_node(\n            'ConvTranspose',\n            inputs=node.input('Input') + node.input('Filter'),\n            outputs=node.output('Output'),\n            attrs=attrs)",
  "def is_same_span(cls, in_size, out_size):\n        spans = []\n        for i in range(out_size):\n            start = math.floor(i * (in_size / out_size))\n            end = math.ceil((i + 1) * (in_size / out_size))\n            spans.append(end - start)\n        if len(set(spans)) == 1:\n            return True\n        return False",
  "def opset_1(cls, graph, node, **kw):\n        assert node.attrs['data_format'] == 'NCHW' or node.attrs['data_format'] == \"AnyLayout\",  \\\n                            \"The conv data format should be 'NCHW', but received data format \" \\\n                            \"is %s.\" % node.attrs['data_format']\n        x_dtype = node.input_dtype('X', 0)\n        need_dtype_convert = False\n        input_name = node.input('X', 0)\n        if x_dtype != paddle.float32:\n            need_dtype_convert = True\n            input_name = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.FLOAT)\n\n        if node.attr('global_pooling') or (node.attr('adaptive') and\n                                           node.attr('ksize') == [1, 1]):\n            if need_dtype_convert:\n                onnx_node = graph.make_node(\n                    cls.pool_type[node.attr('pooling_type')][1],\n                    inputs=[input_name])\n                graph.make_node(\n                    'Cast',\n                    inputs=[onnx_node],\n                    outputs=node.output('Out'),\n                    to=dtypes.ONNX.DOUBLE)\n            else:\n                onnx_node = graph.make_node(\n                    cls.pool_type[node.attr('pooling_type')][1],\n                    inputs=[input_name],\n                    outputs=node.output('Out'))\n        elif node.attr('adaptive'):\n            # if pool is adaptive, check if input shape of pool is fixed.\n            if node.input_shape('X', 0)[2:].count(-1) > 0:\n                raise Exception(\n                    \"Converting this model to ONNX need with static input shape,\" \\\n                    \" please fix input shape of this model, see doc Q2 in\" \\\n                    \" https://github.com/PaddlePaddle/paddle2onnx/blob/develop/docs/en/FAQ.md.\"\n                )\n            input_h, input_w = node.input_shape('X', 0)[2:]\n            output_h, output_w = node.output_shape('Out', 0)[2:]\n            stride_h = int(input_h / output_h)\n            stride_w = int(input_w / output_w)\n\n            kernel_h = input_h - (output_h - 1) * stride_h\n            kernel_w = input_w - (output_w - 1) * stride_w\n\n            #check if kernel_size is fixed.\n            if not cls.is_same_span(input_h, output_h) or not cls.is_same_span(\n                    input_w, output_w):\n                raise Exception(\n                    \"Cannot convert adaptive pool with input_size: {}, output_size: {}\"\n                    .format(\n                        node.input_shape('X', 0), node.output_shape('Out', 0)))\n            else:\n                attrs = {\n                    'kernel_shape': (kernel_h, kernel_w),\n                    'strides': (stride_h, stride_w),\n                }\n                if node.attr('ceil_mode') and graph.opset_version < 10:\n                    raise Exception(\n                        \"Cannot convert pool with ceil_model == True to ONNX Opset version < 10.\"\n                    )\n                elif graph.opset_version > 10:\n                    attrs['ceil_mode'] = node.attr('ceil_mode')\n                auto_pad = node.attr('padding_algorithm')\n                if auto_pad == 'SAME':\n                    attrs['auto_pad'] = 'SAME_UPPER'\n                elif auto_pad == 'VALID':\n                    attrs['auto_pad'] = 'VALID'\n                if node.attr('pooling_type') == 'avg':\n                    attrs['count_include_pad'] = not node.attr('exclusive')\n                if need_dtype_convert:\n                    onnx_node = graph.make_node(\n                        cls.pool_type[node.attr('pooling_type')][0],\n                        inputs=[input_name],\n                        attrs=attrs)\n                    graph.make_node(\n                        'Cast',\n                        inputs=[onnx_node],\n                        outputs=node.output('Out'),\n                        to=dtypes.ONNX.DOUBLE)\n                else:\n                    onnx_node = graph.make_node(\n                        cls.pool_type[node.attr('pooling_type')][0],\n                        inputs=[input_name],\n                        outputs=node.output('Out'),\n                        attrs=attrs)\n        else:\n            input_shape = node.input_shape('X', 0)\n            k_size = node.attr('ksize')\n            pads = node.attr('paddings')\n            strides = node.attr('strides')\n\n            if len(pads) == 2:\n                pads = pads + pads\n            elif len(pads) == 4:\n                pads = [pads[i] for i in [0, 2, 1, 3]]\n\n            if input_shape[2] > 0 and input_shape[2] + pads[0] < k_size[0]:\n                k_size[0] = input_shape[2] + pads[0]\n            if input_shape[3] > 0 and input_shape[3] + pads[1] < k_size[1]:\n                k_size[1] = input_shape[3] + pads[1]\n\n            input_x = [input_name]\n            if max(k_size) <= max(pads):\n                onnx_paddings = [0, 0, pads[0], pads[1], 0, 0, pads[2], pads[3]]\n                attrs_pad = {'mode': 'constant', }\n                if graph.opset_version >= 11:\n                    pads_node = graph.make_node(\n                        'Constant',\n                        attrs={\n                            'dtype': dtypes.ONNX.INT64,\n                            'value': onnx_paddings\n                        })\n                    value_node = graph.make_node(\n                        'Constant',\n                        attrs={'dtype': dtypes.ONNX.FLOAT,\n                               'value': 0.0})\n                    input_x = input_x + [pads_node, value_node]\n                else:\n                    attrs_pad['pads'] = onnx_paddings\n                    attrs_pad['value'] = 0.0\n                input_x = graph.make_node(\n                    'Pad', inputs=input_x, attrs=attrs_pad)\n                pads = [0, 0, 0, 0]\n\n            attrs = {\n                'kernel_shape': k_size,\n                'strides': strides,\n            }\n            auto_pad = node.attr('padding_algorithm')\n            if auto_pad == 'SAME':\n                attrs['auto_pad'] = 'SAME_UPPER'\n            elif auto_pad == 'VALID':\n                attrs['auto_pad'] = 'VALID'\n            else:\n                attrs['pads'] = pads\n            if node.attr('ceil_mode') and graph.opset_version < 10:\n                raise Exception(\n                    \"Cannot convert pool with ceil_model == True to ONNX Opset version < 10\"\n                )\n            elif graph.opset_version >= 10:\n                attrs['ceil_mode'] = node.attr('ceil_mode')\n\n            if node.attr('pooling_type') == 'avg':\n                attrs['count_include_pad'] = not node.attr('exclusive')\n            if need_dtype_convert:\n                onnx_node = graph.make_node(\n                    cls.pool_type[node.attr('pooling_type')][0],\n                    inputs=input_x,\n                    attrs=attrs)\n                graph.make_node(\n                    'Cast',\n                    inputs=[onnx_node],\n                    outputs=node.output('Out'),\n                    to=dtypes.ONNX.DOUBLE)\n            else:\n                onnx_node = graph.make_node(\n                    cls.pool_type[node.attr('pooling_type')][0],\n                    inputs=input_x,\n                    outputs=node.output('Out'),\n                    attrs=attrs)",
  "def is_same_span(cls, in_size, out_size):\n        spans = []\n        for i in range(out_size):\n            start = math.floor(i * (in_size / out_size))\n            end = math.ceil((i + 1) * (in_size / out_size))\n            spans.append(end - start)\n        if len(set(spans)) == 1:\n            return True\n        return False",
  "def opset_1(cls, graph, node, **kw):\n        assert node.attrs['data_format'] == 'NCDHW' or node.attrs['data_format'] == \"AnyLayout\",  \\\n                            \"The conv data format should be 'NCDHW', but received data format \" \\\n                            \"is %s.\" % node.attrs['data_format']\n\n        if node.attr('global_pooling') or (node.attr('adaptive') and\n                                           node.attr('ksize') == [1, 1, 1]):\n            onnx_node = graph.make_node(\n                cls.pool_type[node.attr('pooling_type')][1],\n                inputs=node.input('X'),\n                outputs=node.output('Out'))\n        elif node.attr('adaptive'):\n            # if pool is adaptive, check if input shape of pool is fixed.\n            if node.input_shape('X', 0)[2:].count(-1) > 0:\n                raise Exception(\n                    \"Converting this model to ONNX need with static input shape,\" \\\n                    \" please fix input shape of this model, see doc Q2 in\" \\\n                    \" https://github.com/PaddlePaddle/paddle2onnx/blob/develop/docs/en/FAQ.md.\"\n                )\n            input_d, input_h, input_w = node.input_shape('X', 0)[2:]\n            output_d, output_h, output_w = node.output_shape('Out', 0)[2:]\n            stride_d = int(input_d / output_d)\n            stride_h = int(input_h / output_h)\n            stride_w = int(input_w / output_w)\n\n            kernel_d = input_d - (output_d - 1) * stride_d\n            kernel_h = input_h - (output_h - 1) * stride_h\n            kernel_w = input_w - (output_w - 1) * stride_w\n\n            #check if kernel_size is fixed.\n            if not cls.is_same_span(input_h, output_h) or not cls.is_same_span(\n                    input_w, output_w) or not cls.is_same_span(input_d,\n                                                               output_d):\n                raise Exception(\n                    \"Cannot convert adaptive pool with input_size: {}, output_size: {}\"\n                    .format(\n                        node.input_shape('X', 0), node.output_shape('Out', 0)))\n            else:\n                attrs = {\n                    'kernel_shape': (kernel_d, kernel_h, kernel_w),\n                    'strides': (stride_d, stride_h, stride_w),\n                }\n                if node.attr('ceil_mode') and graph.opset_version < 10:\n                    raise Exception(\n                        \"Cannot convert pool with ceil_model == True to ONNX Opset version < 10.\"\n                    )\n                elif graph.opset_version > 10:\n                    attrs['ceil_mode'] = node.attr('ceil_mode')\n                auto_pad = node.attr('padding_algorithm')\n                if auto_pad == 'SAME':\n                    attrs['auto_pad'] = 'SAME_UPPER'\n                elif auto_pad == 'VALID':\n                    attrs['auto_pad'] = 'VALID'\n                if node.attr('pooling_type') == 'avg':\n                    attrs['count_include_pad'] = not node.attr('exclusive')\n                onnx_node = graph.make_node(\n                    cls.pool_type[node.attr('pooling_type')][0],\n                    inputs=node.input('X'),\n                    outputs=node.output('Out'),\n                    attrs=attrs)\n        else:\n            input_shape = node.input_shape('X', 0)\n            k_size = node.attr('ksize')\n            paddings = node.attr('paddings')\n            if input_shape[2] > 0 and input_shape[2] + paddings[0] < k_size[0]:\n                k_size[0] = input_shape[2] + paddings[0]\n            if input_shape[3] > 0 and input_shape[3] + paddings[1] < k_size[1]:\n                k_size[1] = input_shape[3] + paddings[1]\n            if input_shape[4] > 0 and input_shape[4] + paddings[2] < k_size[2]:\n                k_size[2] = input_shape[4] + paddings[2]\n            attrs = {\n                'kernel_shape': k_size,\n                'strides': node.attr('strides'),\n                'pads': node.attr('paddings') + node.attr('paddings'),\n            }\n            if node.attr('ceil_mode') and graph.opset_version < 10:\n                raise Exception(\n                    \"Cannot convert pool with ceil_model == True to ONNX Opset version < 10\"\n                )\n            elif graph.opset_version >= 10:\n                attrs['ceil_mode'] = node.attr('ceil_mode')\n\n            if node.attr('pooling_type') == 'avg':\n                attrs['count_include_pad'] = not node.attr('exclusive')\n            onnx_node = graph.make_node(\n                cls.pool_type[node.attr('pooling_type')][0],\n                inputs=node.input('X'),\n                outputs=node.output('Out'),\n                attrs=attrs)",
  "def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'Elu',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            alpha=node.attr('alpha'))",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Softsign', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_9(cls, graph, node, **kw):\n        node = graph.make_node(\n            'Shrink',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            lambd=node.attr('threshold'))",
  "def opset_1(cls, graph, node, **kw):\n        sigmoid_node = graph.make_node('Sigmoid', inputs=node.input('X'))\n        graph.make_node('Log', inputs=sigmoid_node, outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'LpNormalization',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            axis=node.attr('axis'))",
  "def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Shrink',\n            inputs=node.input('X'),\n            bias=node.attr('lambda'),\n            lambd=node.attr('lambda'),\n            outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        tanh_node = graph.make_node(\n            'Tanh',\n            inputs=node.input('X', 0), )\n        graph.make_node(\n            'Sub',\n            inputs=[node.input('X', 0), tanh_node],\n            outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        axis = node.attr('axis')\n        shape = node.output_shape('Out', 0)\n        if axis is None:\n            axis = -1\n        if axis < 0:\n            axis += len(shape)\n        if axis == len(shape) - 1:\n            node = graph.make_node(\n                'LogSoftmax',\n                inputs=node.input('X'),\n                outputs=node.output('Out'),\n                attrs={'axis': axis})\n        else:\n            perm = [i for i in range(len(shape))]\n            perm[-1] = axis\n            perm[axis] = len(shape) - 1\n            transpose_node = graph.make_node(\n                'Transpose', inputs=node.input('X'), attrs={'perm': perm})\n            softmax_node = graph.make_node(\n                'LogSoftmax', inputs=[transpose_node], axis=-1)\n            transpose_node1 = graph.make_node(\n                'Transpose',\n                inputs=[softmax_node],\n                outputs=node.output('Out'),\n                attrs={'perm': perm})",
  "def opset_13(cls, graph, node, **kw):\n        graph.make_node(\n            'LogSoftmax',\n            inputs=node.input('X'),\n            axis=node.attr('axis'),\n            outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        ipt = node.input('X', 0)\n        ipt_dims = len(node.input_shape('X', 0))\n        normalized_shape = node.attr('begin_norm_axis')\n        axes = None\n        if isinstance(normalized_shape, collections.Iterable):\n            axes = [-i for i in range(len(normalized_shape), 0, -1)]\n        else:\n            axes = [i for i in range(normalized_shape, ipt_dims)]\n        dtype = node.block.vars[node.input('X', 0)].dtype\n        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n        epsilon = graph.make_node(\n            'Constant', dtype=dtype, value=node.attr('epsilon'))\n        two = graph.make_node('Constant', dtype=dtype, value=2.0)\n        mean = graph.make_node(\"ReduceMean\", inputs=[ipt], axes=axes)\n        numerator = graph.make_node(\"Sub\", inputs=[ipt, mean])\n        pow_num = graph.make_node(\"Pow\", inputs=[numerator, two])\n        variance = graph.make_node(\"ReduceMean\", inputs=[pow_num], axes=axes)\n        add_eps = graph.make_node(\"Add\", inputs=[variance, epsilon])\n        denominator = graph.make_node(\"Sqrt\", inputs=[add_eps])\n\n        ipt_shape = graph.make_node(\"Shape\", inputs=[ipt])\n        weight_shape = mapper_helper.slice_helper(\n            graph, ipt_shape, [0], [ipt_dims - len(axes)], [ipt_dims])\n        if 'Bias' in node.inputs and 'Scale' in node.inputs and len(\n                node.input('Scale')) > 0 and len(node.input('Bias')) > 0:\n            if normalized_shape == ipt_dims - 1:\n                shape_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])\n                scale = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Scale', 0), shape_const])\n                bias = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Bias', 0), shape_const])\n            else:\n                scale = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Scale', 0), weight_shape])\n                bias = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Bias', 0), weight_shape])\n            layer_norm = graph.make_node(\"Div\", inputs=[numerator, denominator])\n            layer_norm = graph.make_node(\"Mul\", inputs=[layer_norm, scale])\n            graph.make_node(\n                \"Add\", inputs=[layer_norm, bias], outputs=node.output('Y'))\n        elif 'Bias' in node.inputs and len(node.input('Bias')) > 0:\n            if normalized_shape == ipt_dims - 1:\n                shape_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])\n                bias = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Bias', 0), shape_const])\n            else:\n                bias = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Bias', 0), weight_shape])\n            layer_norm = graph.make_node(\"Div\", inputs=[numerator, denominator])\n            graph.make_node(\n                \"Add\", inputs=[layer_norm, bias], outputs=node.output('Y'))\n        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:\n            if normalized_shape == ipt_dims - 1:\n                shape_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[-1])\n                scale = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Scale', 0), shape_const])\n            else:\n                scale = graph.make_node(\n                    \"Reshape\", inputs=[node.input('Scale', 0), weight_shape])\n            layer_norm = graph.make_node(\"Div\", inputs=[numerator, denominator])\n            graph.make_node(\n                \"Mul\", inputs=[layer_norm, scale], outputs=node.output('Y'))\n        else:\n            layer_norm = graph.make_node(\n                \"Div\",\n                inputs=[numerator, denominator],\n                outputs=node.output('Y'))",
  "def make_attrs_and_inputs(cls, graph, node, **kw):\n        onnx_attr = {\n            'epsilon': node.attr('epsilon'),\n            'momentum': node.attr('momentum')\n        }\n        inputs = node.input('X') + node.input('Scale') + node.input(\n            'Bias') + node.input('Mean') + node.input('Variance')\n        return onnx_attr, inputs",
  "def opset_9(cls, graph, node, **kw):\n        onnx_attr, inputs = cls.make_attrs_and_inputs(graph, node, **kw)\n        onnx_node = graph.make_node(\n            'BatchNormalization',\n            inputs=inputs,\n            outputs=node.output('Y'),\n            **onnx_attr)",
  "def opset_7(cls, graph, node, **kw):\n        onnx_attr, inputs = cls.make_attrs_and_inputs(graph, node, **kw)\n        onnx_attr['spatial'] = 1\n        onnx_node = graph.make_node(\n            'BatchNormalization',\n            inputs=inputs,\n            outputs=node.output('Y'),\n            **onnx_attr)",
  "def opset_6(cls, graph, node, **kw):\n        num_groups = node.attr('groups')\n        epsilon = node.attr('epsilon')\n        ipt = node.input('X')[0]\n\n        ipt_shape = node.input_shape('X', 0)\n        assert len(\n            ipt_shape) == 4, \"Only support 4D-Tensor as input for GroupNorm\"\n\n        dtype = node.block.vars[node.input('X', 0)].dtype\n        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n\n        shape = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[0, num_groups, -1])\n        reshape_input = graph.make_node('Reshape', inputs=[ipt, shape])\n        scale_ = graph.make_node(\n            'Constant', dtype=dtype, value=[1.0] * num_groups)\n        bias_ = graph.make_node(\n            'Constant', dtype=dtype, value=[0.0] * num_groups)\n        reshaped_output = graph.make_node(\n            'InstanceNormalization',\n            inputs=[reshape_input, scale_, bias_],\n            epsilon=epsilon)\n        origin_shape = graph.make_node('Shape', inputs=[ipt])\n\n        if len(node.input('Scale')) > 0 and len(node.input('Bias')) > 0:\n            output = graph.make_node(\n                'Reshape', inputs=[reshaped_output, origin_shape])\n            unsqueezed_scale = mapper_helper.unsqueeze_helper(\n                graph, node.input('Scale', 0), [1, 2])\n            unsqueezed_bias = mapper_helper.unsqueeze_helper(\n                graph, node.input('Bias', 0), [1, 2])\n            part0 = graph.make_node('Mul', inputs=[output, unsqueezed_scale])\n            graph.make_node(\n                'Add',\n                inputs=[part0, unsqueezed_bias],\n                outputs=node.output('Y'))\n        else:\n            output = graph.make_node(\n                'Reshape',\n                inputs=[reshaped_output, origin_shape],\n                outputs=node.output('Y'))",
  "def opset_6(cls, graph, node, **kw):\n        onnx_attr = {'epsilon': node.attr('epsilon'), }\n        num_groups = node.block.vars[node.input('X')[0]].shape[1]\n\n        dtype = node.block.vars[node.input('X', 0)].dtype\n        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n\n        if len(node.input('Scale')) == 0:\n            scale_ = graph.make_node(\n                'Constant', dtype=dtype, value=[1.0] * num_groups)\n        else:\n            scale_ = node.input('Scale')[0]\n        if len(node.input('Bias')) == 0:\n            bias_ = graph.make_node(\n                'Constant', dtype=dtype, value=[0.0] * num_groups)\n        else:\n            bias_ = node.input('Bias')[0]\n\n        inputs = node.input('X') + [scale_] + [bias_]\n        onnx_node = graph.make_node(\n            'InstanceNormalization',\n            inputs=inputs,\n            outputs=node.output('Y'),\n            **onnx_attr)",
  "def opset_7(cls, graph, node, **kw):\n        dropout_mode = node.attr('dropout_implementation')\n        dropout_prob = node.attr('dropout_prob')\n        if dropout_mode == 'upscale_in_train':\n            onnx_node = graph.make_node(\n                'Identity', inputs=node.input('X'), outputs=node.output('Out'))\n        elif dropout_mode == 'downgrade_in_infer':\n            scale_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.FLOAT,\n                       'value': 1 - dropout_prob})\n            graph.make_node(\n                \"Mul\",\n                inputs=[node.input('X')[0], scale_node],\n                outputs=node.output('Out'))\n        else:\n            raise Exception(\"Unexpected situation happend\")",
  "def opset_10(cls, graph, node, **kw):\n        if node.attr('aligned') and graph.opset_version < 16:\n            raise Exception(\n                'when aligned is true, onnx opset should be (onnx_opset>= 16)')\n        rois_shape = graph.make_node('Shape', inputs=[node.input('ROIs', 0)])\n        starts = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                               'value': [0]})\n        ends = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                               'value': [1]})\n        num_rois = graph.make_node('Slice', inputs=[rois_shape, starts, ends])\n        zero = graph.make_node(\n            'Constant', dims=[1], dtype=dtypes.ONNX.INT64, value=[0])\n        batch_indices = graph.make_node('Expand', inputs=[zero, num_rois])\n        node = graph.make_node(\n            'RoiAlign',\n            inputs=[node.input('X', 0), node.input('ROIs', 0), batch_indices],\n            outputs=node.output('Out'),\n            mode='avg',\n            output_height=node.attr('pooled_height'),\n            output_width=node.attr('pooled_width'),\n            sampling_ratio=node.attr('sampling_ratio'),\n            spatial_scale=node.attr('spatial_scale'))",
  "def make_param_inputs(cls, graph, node, layer, hidden_size, num_layers):\n        # weight assign order:\n        # (F_whi F_whh B_whi B_whh\uff09* layer_num  + (F_bias_hi F_bias_hh B_bias_hi  B_bias_hi)* layer_num\n        def reform_weights(g, w, n, intervals):\n            slices = [\n                mapper_helper.slice_helper(\n                    g, w, axes=[1], starts=[x * n], ends=[y * n])\n                for x, y in intervals\n            ]\n            return g.make_node('Concat', slices, axis=1)\n\n        def transform_weight_with_bias(g, weights, n, intervals):\n            return [reform_weights(g, w, n, intervals) for w in weights]\n\n        if node.attr('mode') == 'LSTM':\n            reform_permutation = [(0, 1), (3, 4), (1, 3)]\n        elif node.attr('mode') == 'GRU':\n            reform_permutation = [(1, 2), (0, 1), (2, 3)]\n        bidirect_len = 4 if node.attr('is_bidirec') else 2\n        all_layer_param_len = len(node.input('WeightList'))\n        weight_list = node.input('WeightList')[:all_layer_param_len // 2]\n        bias_list = node.input('WeightList')[all_layer_param_len // 2:]\n        single_layer_param_len = all_layer_param_len // num_layers\n\n        unsqueeze_weights = []\n        layer_weight_list = weight_list[layer * bidirect_len:layer *\n                                        bidirect_len + bidirect_len]\n        layer_bias_list = bias_list[layer * bidirect_len:layer * bidirect_len +\n                                    bidirect_len]\n        param_list = layer_weight_list + layer_bias_list\n        param_list_len = len(param_list)\n        for i in range(param_list_len):\n            weight = mapper_helper.unsqueeze_helper(graph, param_list[i], [0])\n            unsqueeze_weights.append(weight)\n\n        input_weights = unsqueeze_weights[0:param_list_len // 2:2]\n        hidden_weights = unsqueeze_weights[1:param_list_len // 2:2]\n\n        input_weight = graph.make_node('Concat', inputs=input_weights, axis=0)\n        hidden_weight = graph.make_node('Concat', inputs=hidden_weights, axis=0)\n        input_bias = unsqueeze_weights[param_list_len // 2:param_list_len:2]\n        hidden_bias = unsqueeze_weights[param_list_len // 2 + 1:param_list_len:\n                                        2]\n\n        input_bias = graph.make_node('Concat', inputs=input_bias, axis=0)\n        hidden_bias = graph.make_node('Concat', inputs=hidden_bias, axis=0)\n        input_weight, hidden_weight, input_bias, hidden_bias = transform_weight_with_bias(\n            graph, [input_weight, hidden_weight, input_bias, hidden_bias],\n            hidden_size, reform_permutation)\n        bias = graph.make_node(\n            'Concat', inputs=[input_bias, hidden_bias], axis=1)\n        return [input_weight, hidden_weight, bias, '']",
  "def make_init_param_inputs(cls, graph, node, layer):\n        if node.attr('mode') == 'LSTM':\n            all_init_h, all_init_c = node.input('PreState')\n            bidirect_len = 2 if node.attr('is_bidirec') else 1\n            init_h = mapper_helper.slice_helper(\n                graph, all_init_h, [0], [layer * bidirect_len],\n                [layer * bidirect_len + bidirect_len])\n            init_c = mapper_helper.slice_helper(\n                graph, all_init_c, [0], [layer * bidirect_len],\n                [layer * bidirect_len + bidirect_len])\n            return [init_h, init_c]\n        elif node.attr('mode') == 'GRU':\n            all_init_h = node.input('PreState', 0)\n            bidirect_len = 2 if node.attr('is_bidirec') else 1\n            init_h = mapper_helper.slice_helper(\n                graph, all_init_h, [0], [layer * bidirect_len],\n                [layer * bidirect_len + bidirect_len])\n            return [init_h]",
  "def opset_7(cls, graph, node, **kw):\n        mode = node.attr('mode')\n        hidden_size = node.attr('hidden_size')\n        num_layers = node.attr('num_layers')\n        prev_output = node.input('Input', 0)\n        if node.attr('mode') == 'LSTM':\n            for layer in range(num_layers):\n                param_inputs = cls.make_param_inputs(graph, node, layer,\n                                                     hidden_size, num_layers)\n                init_param_inputs = cls.make_init_param_inputs(graph, node,\n                                                               layer)\n                if layer + 1 < num_layers:\n                    rnn_outputs = 3\n                    output_y = None\n                else:\n                    rnn_outputs = [1] + node.output('State')\n                    output_y = node.output('Out')\n                prev_output, h_out, c_out = graph.make_node(\n                    node.attr('mode'),\n                    inputs=[prev_output] + param_inputs + init_param_inputs,\n                    outputs=rnn_outputs,\n                    direction='bidirectional'\n                    if node.attr('is_bidirec') else 'forward',\n                    hidden_size=node.attr('hidden_size'))\n                prev_output = graph.make_node(\n                    'Transpose', inputs=[prev_output], perm=[0, 2, 1, 3])\n\n                prev_shape = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[0, 0, -1])\n                prev_output = graph.make_node(\n                    'Reshape',\n                    inputs=[prev_output, prev_shape],\n                    outputs=output_y)\n        elif node.attr('mode') == 'GRU':\n            for layer in range(num_layers):\n                param_inputs = cls.make_param_inputs(graph, node, layer,\n                                                     hidden_size, num_layers)\n                init_param_inputs = cls.make_init_param_inputs(graph, node,\n                                                               layer)\n                if layer + 1 < num_layers:\n                    rnn_outputs = 2\n                    output_y = None\n                else:\n                    rnn_outputs = [1] + node.output('State')\n                    output_y = node.output('Out')\n                attrs = {\n                    'direction': 'bidirectional'\n                    if node.attr('is_bidirec') else 'forward',\n                    'hidden_size': node.attr('hidden_size'),\n                    'linear_before_reset': 1,\n                }\n                prev_output, h_out = graph.make_node(\n                    node.attr('mode'),\n                    inputs=[prev_output] + param_inputs + init_param_inputs,\n                    outputs=rnn_outputs,\n                    attrs=attrs)\n                prev_output = graph.make_node(\n                    'Transpose', inputs=[prev_output], perm=[0, 2, 1, 3])\n                prev_shape = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[0, 0, -1])\n                prev_output = graph.make_node(\n                    'Reshape',\n                    inputs=[prev_output, prev_shape],\n                    outputs=output_y)",
  "def opset_10(cls, graph, node, **kw):\n        x_dtype = node.input_dtype('X', 0)\n        if x_dtype != paddle.float32:\n            x = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.FLOAT)\n            threshholdedrelu_node = graph.make_node(\n                'ThresholdedRelu', inputs=[x], alpha=node.attr('threshold'))\n            graph.make_node(\n                'Cast',\n                inputs=[threshholdedrelu_node],\n                outputs=node.output('Out'),\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n        else:\n            graph.make_node(\n                'ThresholdedRelu',\n                inputs=node.input('X'),\n                alpha=node.attr('threshold'),\n                outputs=node.output('Out'))",
  "def reform_weights(g, w, n, intervals):\n            slices = [\n                mapper_helper.slice_helper(\n                    g, w, axes=[1], starts=[x * n], ends=[y * n])\n                for x, y in intervals\n            ]\n            return g.make_node('Concat', slices, axis=1)",
  "def transform_weight_with_bias(g, weights, n, intervals):\n            return [reform_weights(g, w, n, intervals) for w in weights]",
  "class GreaterOrEqual():\n    support_opset_version_range = (12, 15)\n\n    @classmethod\n    def opset_12(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'GreaterOrEqual',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "class Equal():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        if node.input_dtype('X', 0) in [paddle.float32, paddle.float64]:\n            warning_info = \"Operator 'Equal' only support input with dtype of int/bool, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.\".format(\n                node.input_dtype('X', 0))\n            logging.warning(warning_info)\n            x_node = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)\n            y_node = graph.make_node(\n                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)\n            onnx_node = graph.make_node(\n                'Equal', inputs=[x_node, y_node], outputs=node.output('Out'))\n        else:\n            onnx_node = graph.make_node(\n                'Equal',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'Equal',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "class NotEqual():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        equal_val = None\n        if node.input_dtype('X', 0) in [paddle.float32, paddle.float64]:\n            warning_info = \"Operator 'not_equal' only support input with dtype of int/bool, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.\".format(\n                node.input_dtype('X', 0))\n            logging.warning(warning_info)\n            x_node = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)\n            y_node = graph.make_node(\n                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)\n            equal_val = graph.make_node(\n                'Equal', inputs=[x_node, y_node], outputs=node.output('Out'))\n        else:\n            equal_val = graph.make_node(\n                'Equal',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))\n        k_node = graph.make_node(\n            'Cast', inputs=[equal_val], to=dtypes.ONNX.INT64)\n        const = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=1)\n        sub_ = graph.make_node('Sub', inputs=[const, k_node])\n        graph.make_node(\n            'Cast',\n            inputs=[sub_],\n            outputs=node.output('Out'),\n            to=dtypes.ONNX.BOOL)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        equal_val = graph.make_node(\n            'Equal', inputs=[node.input('X', 0), node.input('Y', 0)])\n        k_node = graph.make_node(\n            'Cast', inputs=[equal_val], to=dtypes.ONNX.INT64)\n        const = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=1)\n        sub_ = graph.make_node('Sub', inputs=[const, k_node])\n        graph.make_node(\n            'Cast',\n            inputs=[sub_],\n            outputs=node.output('Out'),\n            to=dtypes.ONNX.BOOL)",
  "class GreaterThan():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        if node.input_dtype('X', 0) in [paddle.int32, paddle.int64]:\n            warning_info = \"Operator 'greater_than' only support input with dtype of float/double, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.\".format(\n                node.input_dtype('X', 0))\n            logging.warning(warning_info)\n            x_node = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)\n            y_node = graph.make_node(\n                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)\n            graph.make_node(\n                'Greater',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Greater',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'Greater',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "class LogicalAnd():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'And',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "class LogicalNot():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Not', inputs=node.input('X'), outputs=node.output('Out'))",
  "class LogicalOr():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Or',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "class LogicalXOr():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Xor',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "class LessOrEqual():\n    support_opset_version_range = (12, 15)\n\n    @classmethod\n    def opset_12(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'LessOrEqual',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "class Less_than():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        if node.input_dtype('X', 0) in [paddle.int32, paddle.int64]:\n            warning_info = \"Operator 'less_than' only support input with dtype of float/double, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.\".format(\n                node.input_dtype('X', 0))\n            logging.warning(warning_info)\n            x_node = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)\n            y_node = graph.make_node(\n                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)\n            graph.make_node(\n                'Less',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Less',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Less',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'), )",
  "class Isfinite():\n    support_opset_version_range = (10, 15)\n\n    @classmethod\n    def opset_10(cls, graph, node, **kw):\n        is_inf = graph.make_node('IsInf', inputs=node.input('X', 0))\n        is_nan = graph.make_node('IsNaN', inputs=node.input('X', 0))\n        finite = graph.make_node('Or', inputs=[is_inf, is_nan])\n        graph.make_node('Not', inputs=[finite], outputs=node.output('Out'))",
  "class IsInf():\n    support_opset_version_range = (10, 15)\n\n    @classmethod\n    def opset_10(cls, graph, node, **kw):\n        graph.make_node(\n            'IsInf', inputs=node.input('X'), outputs=node.output('Out'))",
  "class IsNaN():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'IsNaN', inputs=node.input('X'), outputs=node.output('Out'))",
  "class IsNaN():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        isnan = graph.make_node('IsNaN', inputs=node.input('X'))\n        cast_node = graph.make_node(\n            'Cast', inputs=isnan, attrs={'to': dtypes.ONNX.FLOAT})\n        reduce_node = graph.make_node(\n            'ReduceMax', inputs=[cast_node], keepdims=False)\n        mapper_helper.unsqueeze_helper(graph, reduce_node, [0],\n                                       node.output('Out'))",
  "def opset_12(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'GreaterOrEqual',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        if node.input_dtype('X', 0) in [paddle.float32, paddle.float64]:\n            warning_info = \"Operator 'Equal' only support input with dtype of int/bool, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.\".format(\n                node.input_dtype('X', 0))\n            logging.warning(warning_info)\n            x_node = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)\n            y_node = graph.make_node(\n                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)\n            onnx_node = graph.make_node(\n                'Equal', inputs=[x_node, y_node], outputs=node.output('Out'))\n        else:\n            onnx_node = graph.make_node(\n                'Equal',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))",
  "def opset_11(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'Equal',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        equal_val = None\n        if node.input_dtype('X', 0) in [paddle.float32, paddle.float64]:\n            warning_info = \"Operator 'not_equal' only support input with dtype of int/bool, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.\".format(\n                node.input_dtype('X', 0))\n            logging.warning(warning_info)\n            x_node = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)\n            y_node = graph.make_node(\n                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)\n            equal_val = graph.make_node(\n                'Equal', inputs=[x_node, y_node], outputs=node.output('Out'))\n        else:\n            equal_val = graph.make_node(\n                'Equal',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))\n        k_node = graph.make_node(\n            'Cast', inputs=[equal_val], to=dtypes.ONNX.INT64)\n        const = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=1)\n        sub_ = graph.make_node('Sub', inputs=[const, k_node])\n        graph.make_node(\n            'Cast',\n            inputs=[sub_],\n            outputs=node.output('Out'),\n            to=dtypes.ONNX.BOOL)",
  "def opset_11(cls, graph, node, **kw):\n        equal_val = graph.make_node(\n            'Equal', inputs=[node.input('X', 0), node.input('Y', 0)])\n        k_node = graph.make_node(\n            'Cast', inputs=[equal_val], to=dtypes.ONNX.INT64)\n        const = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=1)\n        sub_ = graph.make_node('Sub', inputs=[const, k_node])\n        graph.make_node(\n            'Cast',\n            inputs=[sub_],\n            outputs=node.output('Out'),\n            to=dtypes.ONNX.BOOL)",
  "def opset_7(cls, graph, node, **kw):\n        if node.input_dtype('X', 0) in [paddle.int32, paddle.int64]:\n            warning_info = \"Operator 'greater_than' only support input with dtype of float/double, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.\".format(\n                node.input_dtype('X', 0))\n            logging.warning(warning_info)\n            x_node = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)\n            y_node = graph.make_node(\n                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)\n            graph.make_node(\n                'Greater',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Greater',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))",
  "def opset_11(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'Greater',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'And',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Not', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Or',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Xor',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "def opset_12(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'LessOrEqual',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        if node.input_dtype('X', 0) in [paddle.int32, paddle.int64]:\n            warning_info = \"Operator 'less_than' only support input with dtype of float/double, now the dtype of input is {}, this may cause wrong results, it is more recommend converting this model with opset version >= 11.\".format(\n                node.input_dtype('X', 0))\n            logging.warning(warning_info)\n            x_node = graph.make_node(\n                'Cast', inputs=node.input('X'), to=dtypes.ONNX.INT32)\n            y_node = graph.make_node(\n                'Cast', inputs=node.input('Y'), to=dtypes.ONNX.INT32)\n            graph.make_node(\n                'Less',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Less',\n                inputs=[node.input('X', 0), node.input('Y', 0)],\n                outputs=node.output('Out'))",
  "def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Less',\n            inputs=[node.input('X', 0), node.input('Y', 0)],\n            outputs=node.output('Out'), )",
  "def opset_10(cls, graph, node, **kw):\n        is_inf = graph.make_node('IsInf', inputs=node.input('X', 0))\n        is_nan = graph.make_node('IsNaN', inputs=node.input('X', 0))\n        finite = graph.make_node('Or', inputs=[is_inf, is_nan])\n        graph.make_node('Not', inputs=[finite], outputs=node.output('Out'))",
  "def opset_10(cls, graph, node, **kw):\n        graph.make_node(\n            'IsInf', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'IsNaN', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_9(cls, graph, node, **kw):\n        isnan = graph.make_node('IsNaN', inputs=node.input('X'))\n        cast_node = graph.make_node(\n            'Cast', inputs=isnan, attrs={'to': dtypes.ONNX.FLOAT})\n        reduce_node = graph.make_node(\n            'ReduceMax', inputs=[cast_node], keepdims=False)\n        mapper_helper.unsqueeze_helper(graph, reduce_node, [0],\n                                       node.output('Out'))",
  "class ActivationOps():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        onnx_type = kw['mapper_dict'][node.type]\n        onnx_node = graph.make_node(\n            onnx_type, inputs=node.input('X'), outputs=node.output('Out'))",
  "class Silu():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        x = node.input('X')[0]\n        out = graph.make_node('Sigmoid', inputs=[x])\n        graph.make_node('Mul', inputs=[x, out], outputs=node.output('Out'))",
  "class LeakyRelu():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'LeakyRelu',\n            inputs=[node.input('X')[0]],\n            outputs=node.output('Out'),\n            alpha=node.attr('alpha'))",
  "class Softplus():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        beta = node.attr('beta')\n        threshold = node.attr('threshold')\n        if np.isclose(beta, 1.0, 1e-06, 1e-06) and \\\n            np.isclose(threshold, 20.0, 1e-06, 1e-06):\n            onnx_node = graph.make_node(\n                'Softplus',\n                inputs=[node.input('X')[0]],\n                outputs=node.output('Out'))\n        else:\n            raise Exception(\"[ERROR] Operator softplus \" \\\n            \"only supported while beta==1.0 and threshold==20.0\")",
  "class PRelu():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        slope_shape = node.input_shape('Alpha', 0)\n        input_shape = node.input_shape('X', 0)\n\n        slope_node = node.input('Alpha')[0]\n        if len(input_shape) != len(slope_shape):\n            assert len(\n                slope_shape) == 1, \"Slope shape is not expected for prelu\"\n            broadcast_shape = [-1] + [1] * (len(input_shape) - 2)\n            broadcast_shape = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=broadcast_shape)\n            slope_node = graph.make_node(\n                'Reshape', inputs=[node.input('Alpha')[0], broadcast_shape])\n        x = node.input('X')[0]\n        x_dtype = node.input_dtype('X', 0)\n        slope_dtype = node.input_dtype('Alpha', 0)\n        if slope_dtype != paddle.float32:\n            slope_node = graph.make_node(\n                'Cast', inputs=[slope_node], to=dtypes.ONNX.FLOAT)\n        if x_dtype != paddle.float32:\n            x = graph.make_node('Cast', inputs=[x], to=dtypes.ONNX.FLOAT)\n            onnx_node = graph.make_node('PRelu', inputs=[x, slope_node])\n            graph.make_node(\n                'Cast',\n                inputs=[onnx_node],\n                outputs=node.output('Out'),\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n        else:\n            onnx_node = graph.make_node(\n                'PRelu', inputs=[x, slope_node], outputs=node.output('Out'))",
  "class Relu6():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        mapper_helper.clip_helper(graph, node,\n                                  node.input('X', 0),\n                                  node.attr('threshold'), 0.0,\n                                  node.output('Out', 0))",
  "class Gelu():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        input = node.input('X', 0)\n        x_dtype = node.input_dtype('X', 0)\n        # onnxruntime only support float32 Erf\n        if x_dtype != paddle.float32:\n            input = graph.make_node(\n                'Cast', inputs=[input], to=dtypes.ONNX.FLOAT)\n        sqrt2 = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.FLOAT, value=[1.4142135623730951])\n        zero_point_five = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.FLOAT, value=[0.5])\n        one = graph.make_node('Constant', dtype=dtypes.ONNX.FLOAT, value=[1])\n        x = graph.make_node('Div', inputs=[input, sqrt2])\n        x = graph.make_node('Erf', inputs=x)\n        x = graph.make_node('Add', inputs=[x, one])\n        x = graph.make_node('Mul', inputs=[input, x])\n        if x_dtype != paddle.float32:\n            mul_node = graph.make_node('Mul', inputs=[x, zero_point_five])\n            graph.make_node(\n                'Cast',\n                inputs=[mul_node],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Mul', inputs=[x, zero_point_five], outputs=node.output('Out'))",
  "class Selu():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_6(cls, graph, node, **kw):\n        graph.make_node(\n            'Selu',\n            inputs=node.input('X'),\n            alpha=node.attr('alpha'),\n            gamma=node.attr('scale'),\n            outputs=node.output('Out'))",
  "class HardSigmoid():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        slope = node.attr('slope')\n        offset = node.attr('offset')\n        graph.make_node(\n            'HardSigmoid',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            alpha=slope,\n            beta=offset)",
  "class Swish():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        x = node.input('X')[0]\n        if math.fabs(node.attr(\"beta\") - 1.0) > 1e-05:\n            beta_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.FLOAT,\n                       'value': [node.attr('beta')]})\n            x = graph.make_node(\n                'Mul', inputs=[x, beta_node])\n        sigmoid_node = graph.make_node('Sigmoid', inputs=[x])\n        graph.make_node(\n            'Mul',\n            inputs=[x, sigmoid_node],\n            outputs=node.output('Out'))",
  "class Mish():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        inputs = node.input('X', 0)\n        dtype = node.input_dtype(\"X\", 0)\n        if dtype != paddle.float32:\n            inputs = graph.make_node(\n                'Cast', inputs=[inputs], to=dtypes.ONNX.FLOAT)\n            dtype = paddle.float32\n        threshold = node.attr('threshold')\n        assert np.fabs(\n            threshold - 20\n        ) < 1e-4, \"In mish OP, the threshold only supports 20, no other values are supported\"\n        softplus_node = graph.make_node('Softplus', inputs=[inputs])\n        tanh_node = graph.make_node('Tanh', inputs=[softplus_node])\n        if node.input_dtype(\"X\", 0) != paddle.float32:\n            mul_node = graph.make_node('Mul', inputs=[inputs, tanh_node])\n            inputs = graph.make_node(\n                'Cast',\n                inputs=[mul_node],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(\"X\", 0)],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Mul', inputs=[inputs, tanh_node], outputs=node.output('Out'))",
  "class HardSwish():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        scale_node = graph.make_node(\n            'Constant',\n            attrs={'dtype': dtypes.ONNX.FLOAT,\n                   'value': node.attr('scale')})\n        offset_node = graph.make_node(\n            'Constant',\n            attrs={'dtype': dtypes.ONNX.FLOAT,\n                   'value': node.attr('offset')})\n\n        node0 = graph.make_node('Add', inputs=[node.input('X')[0], offset_node])\n        node1 = mapper_helper.clip_helper(graph, node, node0,\n                                          node.attr('threshold'), 0.0)\n        node2 = graph.make_node('Mul', inputs=[node.input('X')[0], node1])\n        node3 = graph.make_node(\n            'Div', inputs=[node2, scale_node], outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        onnx_type = kw['mapper_dict'][node.type]\n        onnx_node = graph.make_node(\n            onnx_type, inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        x = node.input('X')[0]\n        out = graph.make_node('Sigmoid', inputs=[x])\n        graph.make_node('Mul', inputs=[x, out], outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        onnx_node = graph.make_node(\n            'LeakyRelu',\n            inputs=[node.input('X')[0]],\n            outputs=node.output('Out'),\n            alpha=node.attr('alpha'))",
  "def opset_1(cls, graph, node, **kw):\n        beta = node.attr('beta')\n        threshold = node.attr('threshold')\n        if np.isclose(beta, 1.0, 1e-06, 1e-06) and \\\n            np.isclose(threshold, 20.0, 1e-06, 1e-06):\n            onnx_node = graph.make_node(\n                'Softplus',\n                inputs=[node.input('X')[0]],\n                outputs=node.output('Out'))\n        else:\n            raise Exception(\"[ERROR] Operator softplus \" \\\n            \"only supported while beta==1.0 and threshold==20.0\")",
  "def opset_9(cls, graph, node, **kw):\n        slope_shape = node.input_shape('Alpha', 0)\n        input_shape = node.input_shape('X', 0)\n\n        slope_node = node.input('Alpha')[0]\n        if len(input_shape) != len(slope_shape):\n            assert len(\n                slope_shape) == 1, \"Slope shape is not expected for prelu\"\n            broadcast_shape = [-1] + [1] * (len(input_shape) - 2)\n            broadcast_shape = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=broadcast_shape)\n            slope_node = graph.make_node(\n                'Reshape', inputs=[node.input('Alpha')[0], broadcast_shape])\n        x = node.input('X')[0]\n        x_dtype = node.input_dtype('X', 0)\n        slope_dtype = node.input_dtype('Alpha', 0)\n        if slope_dtype != paddle.float32:\n            slope_node = graph.make_node(\n                'Cast', inputs=[slope_node], to=dtypes.ONNX.FLOAT)\n        if x_dtype != paddle.float32:\n            x = graph.make_node('Cast', inputs=[x], to=dtypes.ONNX.FLOAT)\n            onnx_node = graph.make_node('PRelu', inputs=[x, slope_node])\n            graph.make_node(\n                'Cast',\n                inputs=[onnx_node],\n                outputs=node.output('Out'),\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n        else:\n            onnx_node = graph.make_node(\n                'PRelu', inputs=[x, slope_node], outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        mapper_helper.clip_helper(graph, node,\n                                  node.input('X', 0),\n                                  node.attr('threshold'), 0.0,\n                                  node.output('Out', 0))",
  "def opset_9(cls, graph, node, **kw):\n        input = node.input('X', 0)\n        x_dtype = node.input_dtype('X', 0)\n        # onnxruntime only support float32 Erf\n        if x_dtype != paddle.float32:\n            input = graph.make_node(\n                'Cast', inputs=[input], to=dtypes.ONNX.FLOAT)\n        sqrt2 = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.FLOAT, value=[1.4142135623730951])\n        zero_point_five = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.FLOAT, value=[0.5])\n        one = graph.make_node('Constant', dtype=dtypes.ONNX.FLOAT, value=[1])\n        x = graph.make_node('Div', inputs=[input, sqrt2])\n        x = graph.make_node('Erf', inputs=x)\n        x = graph.make_node('Add', inputs=[x, one])\n        x = graph.make_node('Mul', inputs=[input, x])\n        if x_dtype != paddle.float32:\n            mul_node = graph.make_node('Mul', inputs=[x, zero_point_five])\n            graph.make_node(\n                'Cast',\n                inputs=[mul_node],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Mul', inputs=[x, zero_point_five], outputs=node.output('Out'))",
  "def opset_6(cls, graph, node, **kw):\n        graph.make_node(\n            'Selu',\n            inputs=node.input('X'),\n            alpha=node.attr('alpha'),\n            gamma=node.attr('scale'),\n            outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        slope = node.attr('slope')\n        offset = node.attr('offset')\n        graph.make_node(\n            'HardSigmoid',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            alpha=slope,\n            beta=offset)",
  "def opset_7(cls, graph, node, **kw):\n        x = node.input('X')[0]\n        if math.fabs(node.attr(\"beta\") - 1.0) > 1e-05:\n            beta_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.FLOAT,\n                       'value': [node.attr('beta')]})\n            x = graph.make_node(\n                'Mul', inputs=[x, beta_node])\n        sigmoid_node = graph.make_node('Sigmoid', inputs=[x])\n        graph.make_node(\n            'Mul',\n            inputs=[x, sigmoid_node],\n            outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        inputs = node.input('X', 0)\n        dtype = node.input_dtype(\"X\", 0)\n        if dtype != paddle.float32:\n            inputs = graph.make_node(\n                'Cast', inputs=[inputs], to=dtypes.ONNX.FLOAT)\n            dtype = paddle.float32\n        threshold = node.attr('threshold')\n        assert np.fabs(\n            threshold - 20\n        ) < 1e-4, \"In mish OP, the threshold only supports 20, no other values are supported\"\n        softplus_node = graph.make_node('Softplus', inputs=[inputs])\n        tanh_node = graph.make_node('Tanh', inputs=[softplus_node])\n        if node.input_dtype(\"X\", 0) != paddle.float32:\n            mul_node = graph.make_node('Mul', inputs=[inputs, tanh_node])\n            inputs = graph.make_node(\n                'Cast',\n                inputs=[mul_node],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(\"X\", 0)],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Mul', inputs=[inputs, tanh_node], outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        scale_node = graph.make_node(\n            'Constant',\n            attrs={'dtype': dtypes.ONNX.FLOAT,\n                   'value': node.attr('scale')})\n        offset_node = graph.make_node(\n            'Constant',\n            attrs={'dtype': dtypes.ONNX.FLOAT,\n                   'value': node.attr('offset')})\n\n        node0 = graph.make_node('Add', inputs=[node.input('X')[0], offset_node])\n        node1 = mapper_helper.clip_helper(graph, node, node0,\n                                          node.attr('threshold'), 0.0)\n        node2 = graph.make_node('Mul', inputs=[node.input('X')[0], node1])\n        node3 = graph.make_node(\n            'Div', inputs=[node2, scale_node], outputs=node.output('Out'))",
  "class WhereIndex():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        nonzero_node = graph.make_node(\n            'NonZero', inputs=node.input('Condition'))\n        graph.make_node(\n            'Transpose',\n            inputs=[nonzero_node],\n            outputs=node.output('Out'),\n            perm=[1, 0])",
  "class TopKV2():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        sorted = node.attr('sorted')\n        # for paddle, In gpu device, it always return the sorted value\n        # if not sorted:\n        #     sorted = True\n        if 'K' in node.inputs and len(node.input('K')) > 0:\n            k_node = node.input('K', 0)\n            k_node_dtype = node.input_dtype('K', 0)\n            if dtypes.DTYPE_PADDLE_STR_MAP[k_node_dtype] != 'int64':\n                k_node = graph.make_node(\n                    'Cast', inputs=[k_node], to=dtypes.ONNX.INT64)\n            graph.make_node(\n                'TopK',\n                inputs=[node.input('X', 0), k_node],\n                outputs=[node.output('Out', 0), node.output('Indices', 0)],\n                largest=node.attr('largest'),\n                sorted=sorted,\n                axis=node.attr('axis'))\n        else:\n            k = node.attr('k')\n            k_node = graph.make_node(\n                'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                                   'value': [k]})\n            graph.make_node(\n                'TopK',\n                inputs=[node.input('X', 0), k_node],\n                outputs=[node.output('Out', 0), node.output('Indices', 0)],\n                largest=node.attr('largest'),\n                sorted=sorted,\n                axis=node.attr('axis'))",
  "class TopK():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        if 'K' in node.inputs and len(node.input('K')) > 0:\n            k_node = node.input('K', 0)\n            k_node_dtype = node.input_dtype('K', 0)\n            if dtypes.DTYPE_PADDLE_STR_MAP[k_node_dtype] != 'int64':\n                k_node = graph.make_node(\n                    'Cast', inputs=[k_node], to=dtypes.ONNX.INT64)\n            graph.make_node(\n                'TopK',\n                inputs=[node.input('X', 0), k_node],\n                outputs=[node.output('Out', 0), node.output('Indices', 0)])\n        else:\n            k = node.attr('k')\n            k_node = graph.make_node(\n                'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                                   'value': [k]})\n            graph.make_node(\n                'TopK',\n                inputs=[node.input('X', 0), k_node],\n                outputs=[node.output('Out', 0), node.output('Indices', 0)])",
  "class ArgSort():\n    support_opset_version_range = (6, 15)\n\n    @classmethod\n    def opset_10(cls, graph, node, **kw):\n        shape = graph.make_node('Shape', inputs=node.input('X', 0))\n        from paddle2onnx.legacy.op_mapper import mapper_helper\n        axis = node.attr('axis')\n        if axis < 0:\n            axis = axis + len(node.input_shape('X', 0))\n        dim_size = mapper_helper.slice_helper(\n            graph, shape, axes=[0], starts=[axis], ends=[axis + 1])\n        if graph.opset_version > 10:\n            if not node.attr('descending'):\n                graph.make_node(\n                    'TopK',\n                    inputs=[node.input('X', 0), dim_size],\n                    outputs=[node.output('Out', 0), node.output('Indices', 0)],\n                    axis=node.attr('axis'),\n                    largest=0)\n            else:\n                graph.make_node(\n                    'TopK',\n                    inputs=[node.input('X', 0), dim_size],\n                    outputs=[node.output('Out', 0), node.output('Indices', 0)],\n                    axis=node.attr('axis'),\n                    largest=1)\n        else:\n            if not node.attr('descending'):\n                raise Exception(\n                    \"descending=False only support opset version>=11.\")\n            else:\n                graph.make_node(\n                    'TopK',\n                    inputs=[node.input('X', 0), dim_size],\n                    outputs=[node.output('Out', 0), node.output('Indices', 0)],\n                    axis=node.attr('axis'))\n\n    @classmethod\n    def opset_6(cls, graph, node, **kw):\n        shape = node.input_shape('X', 0)\n        k = shape[node.attr('axis')]\n        assert k > 0, \"while input shape is dynamic, it only support opset version>=10.\"\n        input_dtype = node.input_dtype('X', 0)\n        dtype = dtypes.DTYPE_PADDLE_STR_MAP[input_dtype]\n        inputs = node.input('X', 0)\n        if dtype in [\"int32\", \"int64\"]:\n            inputs = graph.make_node(\n                'Cast', inputs=inputs, to=dtypes.ONNX.FLOAT)\n        if not node.attr('descending'):\n            raise Exception(\"descending=False only support opset version>=11.\")\n        else:\n            output_node = node.output('Out', 0)\n            graph.make_node(\n                'TopK',\n                inputs=[inputs],\n                outputs=[output_node, node.output('Indices', 0)],\n                axis=node.attr('axis'),\n                k=k)\n            if dtype in [\"int32\", \"int64\"]:\n                graph.make_node(\n                    'Cast',\n                    inputs=[output_node],\n                    to=dtypes.DTYPE_PADDLE_ONNX_MAP[input_dtype],\n                    outputs=[output_node])",
  "class IndexSelect():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Gather',\n            inputs=[node.input('X', 0), node.input('Index', 0)],\n            axis=node.attr('dim'),\n            outputs=node.output('Out'))",
  "class Unique():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        if node.attr('axis') == []:\n            graph.make_node(\n                'Unique',\n                inputs=node.input('X'),\n                outputs=[\n                    node.output('Out', 0), node.output('Indices', 0),\n                    node.output('Index', 0), node.output('Counts', 0)\n                ])\n        else:\n            graph.make_node(\n                'Unique',\n                inputs=node.input('X'),\n                axis=node.attr('axis')[0],\n                outputs=[\n                    node.output('Out', 0), node.output('Indices', 0),\n                    node.output('Index', 0), node.output('Counts', 0)\n                ])",
  "class Where():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Where',\n            inputs=[\n                node.input('Condition', 0), node.input('X', 0),\n                node.input('Y', 0)\n            ],\n            outputs=node.output('Out'))",
  "class MaskSelect():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        index = graph.make_node('NonZero', inputs=node.input('Mask', 0))\n        index = graph.make_node('Transpose', inputs=[index], perm=[1, 0])\n        graph.make_node(\n            'GatherND',\n            inputs=[node.input('X', 0), index],\n            outputs=node.output('Y'))",
  "def opset_9(cls, graph, node, **kw):\n        nonzero_node = graph.make_node(\n            'NonZero', inputs=node.input('Condition'))\n        graph.make_node(\n            'Transpose',\n            inputs=[nonzero_node],\n            outputs=node.output('Out'),\n            perm=[1, 0])",
  "def opset_11(cls, graph, node, **kw):\n        sorted = node.attr('sorted')\n        # for paddle, In gpu device, it always return the sorted value\n        # if not sorted:\n        #     sorted = True\n        if 'K' in node.inputs and len(node.input('K')) > 0:\n            k_node = node.input('K', 0)\n            k_node_dtype = node.input_dtype('K', 0)\n            if dtypes.DTYPE_PADDLE_STR_MAP[k_node_dtype] != 'int64':\n                k_node = graph.make_node(\n                    'Cast', inputs=[k_node], to=dtypes.ONNX.INT64)\n            graph.make_node(\n                'TopK',\n                inputs=[node.input('X', 0), k_node],\n                outputs=[node.output('Out', 0), node.output('Indices', 0)],\n                largest=node.attr('largest'),\n                sorted=sorted,\n                axis=node.attr('axis'))\n        else:\n            k = node.attr('k')\n            k_node = graph.make_node(\n                'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                                   'value': [k]})\n            graph.make_node(\n                'TopK',\n                inputs=[node.input('X', 0), k_node],\n                outputs=[node.output('Out', 0), node.output('Indices', 0)],\n                largest=node.attr('largest'),\n                sorted=sorted,\n                axis=node.attr('axis'))",
  "def opset_11(cls, graph, node, **kw):\n        if 'K' in node.inputs and len(node.input('K')) > 0:\n            k_node = node.input('K', 0)\n            k_node_dtype = node.input_dtype('K', 0)\n            if dtypes.DTYPE_PADDLE_STR_MAP[k_node_dtype] != 'int64':\n                k_node = graph.make_node(\n                    'Cast', inputs=[k_node], to=dtypes.ONNX.INT64)\n            graph.make_node(\n                'TopK',\n                inputs=[node.input('X', 0), k_node],\n                outputs=[node.output('Out', 0), node.output('Indices', 0)])\n        else:\n            k = node.attr('k')\n            k_node = graph.make_node(\n                'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                                   'value': [k]})\n            graph.make_node(\n                'TopK',\n                inputs=[node.input('X', 0), k_node],\n                outputs=[node.output('Out', 0), node.output('Indices', 0)])",
  "def opset_10(cls, graph, node, **kw):\n        shape = graph.make_node('Shape', inputs=node.input('X', 0))\n        from paddle2onnx.legacy.op_mapper import mapper_helper\n        axis = node.attr('axis')\n        if axis < 0:\n            axis = axis + len(node.input_shape('X', 0))\n        dim_size = mapper_helper.slice_helper(\n            graph, shape, axes=[0], starts=[axis], ends=[axis + 1])\n        if graph.opset_version > 10:\n            if not node.attr('descending'):\n                graph.make_node(\n                    'TopK',\n                    inputs=[node.input('X', 0), dim_size],\n                    outputs=[node.output('Out', 0), node.output('Indices', 0)],\n                    axis=node.attr('axis'),\n                    largest=0)\n            else:\n                graph.make_node(\n                    'TopK',\n                    inputs=[node.input('X', 0), dim_size],\n                    outputs=[node.output('Out', 0), node.output('Indices', 0)],\n                    axis=node.attr('axis'),\n                    largest=1)\n        else:\n            if not node.attr('descending'):\n                raise Exception(\n                    \"descending=False only support opset version>=11.\")\n            else:\n                graph.make_node(\n                    'TopK',\n                    inputs=[node.input('X', 0), dim_size],\n                    outputs=[node.output('Out', 0), node.output('Indices', 0)],\n                    axis=node.attr('axis'))",
  "def opset_6(cls, graph, node, **kw):\n        shape = node.input_shape('X', 0)\n        k = shape[node.attr('axis')]\n        assert k > 0, \"while input shape is dynamic, it only support opset version>=10.\"\n        input_dtype = node.input_dtype('X', 0)\n        dtype = dtypes.DTYPE_PADDLE_STR_MAP[input_dtype]\n        inputs = node.input('X', 0)\n        if dtype in [\"int32\", \"int64\"]:\n            inputs = graph.make_node(\n                'Cast', inputs=inputs, to=dtypes.ONNX.FLOAT)\n        if not node.attr('descending'):\n            raise Exception(\"descending=False only support opset version>=11.\")\n        else:\n            output_node = node.output('Out', 0)\n            graph.make_node(\n                'TopK',\n                inputs=[inputs],\n                outputs=[output_node, node.output('Indices', 0)],\n                axis=node.attr('axis'),\n                k=k)\n            if dtype in [\"int32\", \"int64\"]:\n                graph.make_node(\n                    'Cast',\n                    inputs=[output_node],\n                    to=dtypes.DTYPE_PADDLE_ONNX_MAP[input_dtype],\n                    outputs=[output_node])",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Gather',\n            inputs=[node.input('X', 0), node.input('Index', 0)],\n            axis=node.attr('dim'),\n            outputs=node.output('Out'))",
  "def opset_11(cls, graph, node, **kw):\n        if node.attr('axis') == []:\n            graph.make_node(\n                'Unique',\n                inputs=node.input('X'),\n                outputs=[\n                    node.output('Out', 0), node.output('Indices', 0),\n                    node.output('Index', 0), node.output('Counts', 0)\n                ])\n        else:\n            graph.make_node(\n                'Unique',\n                inputs=node.input('X'),\n                axis=node.attr('axis')[0],\n                outputs=[\n                    node.output('Out', 0), node.output('Indices', 0),\n                    node.output('Index', 0), node.output('Counts', 0)\n                ])",
  "def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Where',\n            inputs=[\n                node.input('Condition', 0), node.input('X', 0),\n                node.input('Y', 0)\n            ],\n            outputs=node.output('Out'))",
  "def opset_11(cls, graph, node, **kw):\n        index = graph.make_node('NonZero', inputs=node.input('Mask', 0))\n        index = graph.make_node('Transpose', inputs=[index], perm=[1, 0])\n        graph.make_node(\n            'GatherND',\n            inputs=[node.input('X', 0), index],\n            outputs=node.output('Y'))",
  "class MatMul():\n    support_opset_version_range = (1, 12)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        x = node.input('X', idx=0)\n        y = node.input('Y', idx=0)\n        if node.attr('transpose_X'):\n            perm = list(range(len(node.input_shape('X', 0))))\n            perm[-1], perm[-2] = perm[-2], perm[-1]\n            if node.input_dtype('X', 0) == paddle.float64:\n                x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)\n            x = graph.make_node('Transpose', inputs=[x], perm=perm)\n        if node.attr('transpose_Y'):\n            perm = list(range(len(node.input_shape('Y', 0))))\n            perm[-1], perm[-2] = perm[-2], perm[-1]\n            if node.input_dtype('Y', 0) == paddle.float64:\n                y = graph.make_node('Cast', inputs=y, to=dtypes.ONNX.FLOAT)\n            y = graph.make_node('Transpose', inputs=[y], perm=perm)\n        if node.attr('alpha') == 1.0:\n            if node.input_dtype('X', 0) == paddle.float64:\n                output_node = graph.make_node('MatMul', inputs=[x, y])\n                graph.make_node(\n                    'Cast',\n                    inputs=output_node,\n                    to=dtypes.ONNX.DOUBLE,\n                    outputs=node.output('Out'))\n            else:\n                graph.make_node(\n                    'MatMul', inputs=[x, y], outputs=node.output('Out'))\n        else:\n            if node.input_dtype('X', 0) == paddle.float64:\n                output_node = graph.make_node('MatMul', inputs=[x, y])\n                matmul = graph.make_node(\n                    'Cast', inputs=output_node, to=dtypes.ONNX.DOUBLE)\n                scale = graph.make_node(\n                    'Constant',\n                    dtype=dtypes.ONNX.DOUBLE,\n                    value=node.attr('alpha'))\n            else:\n                matmul = graph.make_node('MatMul', inputs=[x, y])\n                scale = graph.make_node(\n                    'Constant',\n                    dtype=dtypes.ONNX.FLOAT,\n                    value=node.attr('alpha'))\n\n            onnx_node = graph.make_node(\n                'Mul', inputs=[matmul, scale], outputs=node.output('Out'))",
  "class MatMul():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        x = node.input('X', idx=0)\n        y = node.input('Y', idx=0)\n        out = node.output('Out')\n        ## TODO(wangjunjie06): The current addition of cast op is only for onnxruntime optimization, after onnxruntime is repaired, remove this logic\n        if node.attr('trans_x'):\n            perm = list(range(len(node.input_shape('X', 0))))\n            perm[-1], perm[-2] = perm[-2], perm[-1]\n            if node.input_dtype('X', 0) == paddle.float64:\n                x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)\n            x = graph.make_node('Transpose', inputs=[x], perm=perm)\n        if node.attr('trans_y'):\n            perm = list(range(len(node.input_shape('Y', 0))))\n            perm[-1], perm[-2] = perm[-2], perm[-1]\n            if node.input_dtype('Y', 0) == paddle.float64:\n                y = graph.make_node('Cast', inputs=y, to=dtypes.ONNX.FLOAT)\n            y = graph.make_node('Transpose', inputs=[y], perm=perm)\n        if node.input_dtype('X', 0) == paddle.float64:\n            output_node = graph.make_node('MatMul', inputs=[x, y])\n            graph.make_node(\n                'Cast', inputs=output_node, to=dtypes.ONNX.DOUBLE, outputs=out)\n        else:\n            graph.make_node('MatMul', inputs=[x, y], outputs=out)",
  "class Exp():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Exp', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Abs:\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Abs', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Erf():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        x_dtype = node.input_dtype('X', 0)\n        x = node.input('X', 0)\n        # onnxruntime only support float32 Erf\n        if x_dtype != paddle.float32:\n            x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)\n            erf_node = graph.make_node('Erf', inputs=[x])\n            graph.make_node(\n                'Cast',\n                inputs=[erf_node],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node('Erf', inputs=[x], outputs=node.output('Out'))",
  "class Acos():\n    supports_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Acos', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Asin():\n    supports_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Asin', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Sinh():\n    supports_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Sinh', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Sin():\n    supports_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Sin', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Atan():\n    supports_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Atan', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Tan():\n    supports_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Tan', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Ceil():\n    supports_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_6(cls, graph, node, **kw):\n        graph.make_node(\n            'Ceil', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Cos():\n    supports_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Cos', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Cosh():\n    supports_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Cosh', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Log2():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        _ln2 = 0.693147180559945309\n        dtype = dtypes.ONNX.FLOAT\n        if node.input_dtype('X', 0) == paddle.float64:\n            dtype = dtypes.ONNX.DOUBLE\n        _ln2 = graph.make_node('Constant', dtype=dtype, value=_ln2)\n        lnx = graph.make_node('Log', inputs=node.input('X'))\n        graph.make_node('Div', inputs=[lnx, _ln2], outputs=node.output('Out'))",
  "class LogSumExp():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n\n        if node.attr('reduce_all'):\n            if not node.attr('keepdim'):\n                reduce_node = graph.make_node(\n                    'ReduceLogSumExp',\n                    inputs=node.input('X'),\n                    keepdims=node.attr('keepdim'))\n                mapper_helper.unsqueeze_helper(graph, reduce_node, [0],\n                                               node.output('Out'))\n            else:\n                graph.make_node(\n                    'ReduceLogSumExp',\n                    inputs=node.input('X'),\n                    keepdims=node.attr('keepdim'),\n                    outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'ReduceLogSumExp',\n                inputs=node.input('X'),\n                keepdims=node.attr('keepdim'),\n                axes=node.attr('axis'),\n                outputs=node.output('Out'))",
  "class ElementwiseOps():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        x_shape = node.input_shape('X', 0)\n        y_shape = node.input_shape('Y', 0)\n        if node.type in [\"elementwise_min\", \"elementwise_max\"]:\n            assert False, \"{} op is not supported when opset version < 8\".format(\n                node.type)\n\n        op_type = kw['mapper_dict'][node.type]\n        axis = node.attr('axis')\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n\n        if axis == -1 or axis == (len(x_shape) - 1\n                                  ) or len(x_shape) == len(y_shape):\n            onnx_node = graph.make_node(\n                op_type, inputs=[x, y], outputs=node.output('Out'))\n        else:\n            broadcast_shape = [1] * len(x_shape)\n            broadcast_shape[axis:axis + len(y_shape)] = y_shape\n            broadcast_shape_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=list(broadcast_shape))\n            y_node = graph.make_node(\n                'Reshape', inputs=[y, broadcast_shape_node])\n            onnx_node = graph.make_node(\n                op_type, inputs=[x, y_node], outputs=node.output('Out'))\n\n    @classmethod\n    def opset_8(cls, graph, node, **kw):\n        op_type = kw['mapper_dict'][node.type]\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n        axis = node.attr('axis')\n        x_shape = node.input_shape('X', 0)\n        y_shape = node.input_shape('Y', 0)\n        if axis == -1 or axis == (len(x_shape) - 1\n                                  ) or len(x_shape) == len(y_shape):\n            onnx_node = graph.make_node(\n                op_type, inputs=[x, y], outputs=node.output('Out'))\n        else:\n            broadcast_shape = [1] * len(x_shape)\n            broadcast_shape[axis:axis + len(y_shape)] = y_shape\n            broadcast_shape_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=list(broadcast_shape))\n            y_node = graph.make_node(\n                'Reshape', inputs=[y, broadcast_shape_node])\n            onnx_node = graph.make_node(\n                op_type, inputs=[x, y_node], outputs=node.output('Out'))",
  "class ElementWiseMod():\n    support_opset_version_range = (10, 15)\n\n    @classmethod\n    def opset_10(cls, graph, node, **kw):\n        x_shape = node.input_shape('X', 0)\n        y_shape = node.input_shape('Y', 0)\n        axis = node.attr('axis')\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n\n        if node.input_dtype('Y', 0) == paddle.int32 or node.input_dtype(\n                'Y', 0) == paddle.int64:\n            onnx_node = graph.make_node(\n                \"Mod\", inputs=[x, y], outputs=node.output('Out'))\n            return\n\n        fmod = 1\n\n        abs_x_node = graph.make_node(\"Abs\", inputs=[x])\n        abs_y_node = graph.make_node(\"Abs\", inputs=[y])\n\n        dtype = dtypes.ONNX.FLOAT\n        val_0 = [0.0]\n        val_1 = [-1.0]\n        if node.input_dtype('Y', 0) == paddle.float64:\n            dtype = dtypes.ONNX.DOUBLE\n        if node.input_dtype('Y', 0) == paddle.int32:\n            dtype = dtypes.ONNX.INT32\n            val_0 = [0]\n            val_1 = [-1]\n        if node.input_dtype('Y', 0) == paddle.int64:\n            dtype = dtypes.ONNX.INT64\n            val_0 = [0]\n            val_1 = [-1]\n        zero_node = graph.make_node('Constant', dtype=dtype, value=val_0)\n        one_node = graph.make_node('Constant', dtype=dtype, value=val_1)\n\n        mod_node = graph.make_node(\n            \"Mod\", inputs=[abs_x_node, abs_y_node], fmod=fmod)\n\n        minus_node = graph.make_node(\"Mul\", inputs=[mod_node, one_node])\n\n        condition_dtype = graph.make_node(\"Less\", inputs=[x, zero_node])\n        condition = graph.make_node(\n            'Cast', inputs=[condition_dtype], to=dtypes.ONNX.BOOL)\n\n        mod_res = graph.make_node(\n            \"Where\", inputs=[condition, minus_node, mod_node])\n\n        add_node = graph.make_node(\"Add\", inputs=[mod_res, y])\n\n        mod_y_mul_node = graph.make_node(\"Mul\", inputs=[mod_res, y])\n        condition_dtype_1 = graph.make_node(\n            \"Less\", inputs=[mod_y_mul_node, zero_node])\n        condition_1 = graph.make_node(\n            'Cast', inputs=[condition_dtype_1], to=dtypes.ONNX.BOOL)\n\n        graph.make_node(\n            \"Where\",\n            inputs=[condition_1, add_node, mod_res],\n            outputs=node.output('Out'))",
  "class ElementWiseFloorDiv():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n        axis = node.attr('axis')\n        x_shape = node.input_shape('X', 0)\n        y_shape = node.input_shape('Y', 0)\n        x_dtype = node.input_dtype('X', 0)\n        y_dtype = node.input_dtype('Y', 0)\n        x_dtype = dtypes.DTYPE_PADDLE_STR_MAP[x_dtype]\n        y_dtype = dtypes.DTYPE_PADDLE_STR_MAP[y_dtype]\n        is_int = False\n        if x_dtype.count('int') > 0 and y_dtype.count('int') > 0:\n            is_int = True\n        if axis == -1 or axis == (len(x_shape) - 1\n                                  ) or len(x_shape) == len(y_shape):\n            if is_int:\n                graph.make_node(\n                    'Div', inputs=[x, y], outputs=node.output('Out'))\n            else:\n                div_node = graph.make_node('Div', inputs=[x, y])\n                graph.make_node(\n                    'Floor', inputs=[div_node], outputs=node.output('Out'))\n        else:\n            broadcast_shape = [1] * len(x_shape)\n            broadcast_shape[axis:axis + len(y_shape)] = y_shape\n            broadcast_shape_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=list(broadcast_shape))\n            y_node = graph.make_node(\n                'Reshape', inputs=[y, broadcast_shape_node])\n            if is_int:\n                div_node = graph.make_node(\n                    'Div', inputs=[x, y_node], outputs=node.output('Out'))\n            else:\n                div_node = graph.make_node('Div', inputs=[x, y_node])\n                graph.make_node(\n                    'Floor', inputs=[div_node], outputs=node.output('Out'))",
  "class Pow():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        x_dtype = node.input_dtype('X', 0)\n        factor = node.attr('factor')\n        # Pow-7 Only support input type as float and double\n        if x_dtype == paddle.int32 or x_dtype == paddle.int64:\n            x = graph.make_node('Cast', inputs=[x], to=dtypes.ONNX.FLOAT)\n            factor_node = graph.make_node(\n                'Constant',\n                inputs=[],\n                dims=[1],\n                dtype=dtypes.ONNX.FLOAT,\n                value=factor)\n        else:\n            factor_node = graph.make_node(\n                'Constant',\n                inputs=[],\n                dims=[1],\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],\n                value=factor)\n        if x_dtype == paddle.int32 or x_dtype == paddle.int64:\n            pow_node = graph.make_node('Pow', inputs=[x, factor_node])\n            graph.make_node(\n                'Cast',\n                inputs=[pow_node],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Pow', inputs=[x, factor_node], outputs=node.output('Out'))\n\n    @classmethod\n    def opset_12(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        factor = node.attr('factor')\n        factor_node = graph.make_node(\n            'Constant',\n            inputs=[],\n            dims=[1],\n            dtype=dtypes.ONNX.FLOAT,\n            value=factor)\n        pow_node = graph.make_node(\n            'Pow', inputs=[x, factor_node], outputs=node.output('Out'))",
  "class Square():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        onnx_node = graph.make_node(\n            'Mul', inputs=[x, x], outputs=node.output('Out'))",
  "class CumSum():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n\n        axis = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=node.attr('axis'))\n        graph.make_node(\n            'CumSum',\n            inputs=[node.input('X', 0), axis],\n            outputs=node.output('Out'))",
  "class Mul():\n    support_opset_version_range = (5, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n        out = node.output('Out', 0)\n        x_num_col_dims = node.attr('x_num_col_dims')\n        y_num_col_dims = node.attr('y_num_col_dims')\n        flatten_x = graph.make_node(\n            'Flatten', inputs=node.input('X'), attrs={'axis': x_num_col_dims})\n        flatten_y = graph.make_node(\n            'Flatten', inputs=node.input('Y'), attrs={'axis': y_num_col_dims})\n        mul_node = graph.make_node('MatMul', inputs=[flatten_x, flatten_y])\n\n        x_shape = graph.make_node('Shape', inputs=[x])\n        l_shape = mapper_helper.slice_helper(\n            graph, x_shape, axes=[0], starts=[0], ends=[x_num_col_dims])\n        y_shape = graph.make_node('Shape', inputs=[y])\n        y_rank = len(node.input_shape('Y', 0))\n        r_shape = mapper_helper.slice_helper(\n            graph, y_shape, axes=[0], starts=[y_num_col_dims], ends=[y_rank])\n\n        out_shape = graph.make_node('Concat', inputs=[l_shape, r_shape], axis=0)\n        graph.make_node('Reshape', [mul_node, out_shape], node.output('Out'))",
  "class AffineChannel():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        if \"data_layout\" in node.attrs.keys():\n            assert node.attrs['data_layout'] == 'NCHW' or node.attrs['data_layout'] == \"AnyLayout\",  \\\n                                \"The affine_channel data format should be 'NCHW', but received data format \" \\\n                                \"is %s.\" % node.attrs['data_layout']\n        x = node.input('X', 0)\n        bias = node.input('Bias', 0)\n        scale = node.input('Scale', 0)\n        scale = mapper_helper.unsqueeze_helper(graph, scale, [0, 2, 3])\n        bias = mapper_helper.unsqueeze_helper(graph, bias, [0, 2, 3])\n        x = graph.make_node('Mul', inputs=[x, scale])\n        x = graph.make_node('Add', inputs=[x, bias], outputs=node.output('Out'))",
  "class BMM():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n        mul_node = graph.make_node(\n            'MatMul', inputs=[x, y], outputs=node.output('Out'))",
  "class PNorm():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        axis = node.attr('axis')\n        if isinstance(axis, (int, float)):\n            axis = [axis]\n        p = node.attr('porder')\n        keepdim = node.attr('keepdim')\n        dtype = dtypes.ONNX.FLOAT\n        if node.input_dtype('X', 0) == paddle.float64:\n            dtype = dtypes.ONNX.DOUBLE\n\n        pnode = graph.make_node('Constant', dtype=dtype, value=[p])\n\n        abs_node = graph.make_node('Abs', inputs=[x])\n        pow_node = graph.make_node('Pow', inputs=[abs_node, pnode])\n        reduce_sum = graph.make_node(\n            'ReduceSum', inputs=[pow_node], axes=axis, keepdims=keepdim)\n        pnode1 = graph.make_node('Constant', dtype=dtype, value=[1.0 / p])\n        graph.make_node(\n            'Pow', inputs=[reduce_sum, pnode1], outputs=node.output('Out'))\n\n    @classmethod\n    def opset_13(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        axis = node.attr('axis')\n        if isinstance(axis, (int, float)):\n            axis = [axis]\n        p = node.attr('porder')\n        keepdim = node.attr('keepdim')\n        pnode = graph.make_node('Constant', dtype=dtypes.ONNX.FLOAT, value=[p])\n        abs_node = graph.make_node('Abs', inputs=[x])\n        pow_node = graph.make_node('Pow', inputs=[abs_node, pnode])\n        axes = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=axis)\n        reduce_sum = graph.make_node(\n            'ReduceSum', inputs=[pow_node, axes], keepdims=keepdim)\n        pnode1 = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.FLOAT, value=[1.0 / p])\n        graph.make_node(\n            'Pow', inputs=[reduce_sum, pnode1], outputs=node.output('Out'))",
  "class Sum():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Sum', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Floor():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Floor', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Log10():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        _ln10 = 2.30258509299404568401\n        dtype = dtypes.ONNX.FLOAT\n        if node.input_dtype('X', 0) == paddle.float64:\n            dtype = dtypes.ONNX.DOUBLE\n        _ln10 = graph.make_node('Constant', dtype=dtype, value=_ln10)\n        lnx = graph.make_node('Log', inputs=node.input('X'))\n        graph.make_node('Div', inputs=[lnx, _ln10], outputs=node.output('Out'))",
  "class Log1p():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        dtype = dtypes.ONNX.FLOAT\n        if node.input_dtype('X', 0) == paddle.float64:\n            dtype = dtypes.ONNX.DOUBLE\n        one = graph.make_node('Constant', attrs={'dtype': dtype, 'value': [1]})\n        add_node = graph.make_node('Add', inputs=[node.input('X', 0), one])\n        graph.make_node('Log', inputs=add_node, outputs=node.output('Out'))",
  "class ReduceAll():\n    support_opset_version_range = (6, 15)\n\n    @classmethod\n    def opset_6(cls, graph, node, **kw):\n        op_type = kw['mapper_dict'][node.type]\n        input_dtype = node.block.vars[node.input('X', 0)].dtype\n        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[input_dtype]\n        all_node = graph.make_node(\n            'Cast', inputs=[node.input('X', 0)], to=dtypes.ONNX.INT32)\n\n        attrs = {'keepdims': node.attr('keep_dim'), }\n        if not node.attr('reduce_all'):\n            attrs['axes'] = node.attr('dim')\n        output_node = graph.make_node(op_type, inputs=[all_node], attrs=attrs)\n\n        if node.attr('reduce_all') and not node.attr('keep_dim'):\n            output_node = mapper_helper.unsqueeze_helper(graph, output_node,\n                                                         [0])\n        graph.make_node(\n            'Cast',\n            inputs=[output_node],\n            to=input_dtype,\n            outputs=node.output('Out'))",
  "class ReduceMean():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        op_type = kw['mapper_dict'][node.type]\n\n        output_shape = node.output_shape('Out', 0)\n        reduce_all = node.attr(\"reduce_all\")\n        axes = node.attr(\"dim\")\n        if reduce_all:\n            axes = list(range(len(node.input_shape(\"X\", 0))))\n        if len(axes) == len(node.input_shape(\"X\", 0)):\n            reduce_all = True\n        keepdims = node.attr('keep_dim')\n        if keepdims:\n            cls.create_reduce_node(graph, op_type,\n                                   node.input(\"X\"), node.output(\"Out\"), axes, 1)\n        else:\n            if reduce_all:\n                shape = graph.make_node(\n                    \"Constant\", dtype=dtypes.ONNX.INT64, value=[-1])\n                flatten_node = graph.make_node(\n                    \"Reshape\", inputs=[node.input(\"X\")[0], shape])\n                cls.create_reduce_node(graph, op_type, [flatten_node],\n                                       node.output(\"Out\"), [0], 1)\n            else:\n                cls.create_reduce_node(graph, op_type,\n                                       node.input(\"X\"),\n                                       node.output(\"Out\"), axes, 0)\n\n    @classmethod\n    def create_reduce_node(cls, graph, op_type, inputs, outputs, axes,\n                           keepdims):\n        if graph.opset_version >= 13 and op_type == \"ReduceSum\":\n            axes = graph.make_node(\n                \"Constant\", dtype=dtypes.ONNX.INT64, value=axes)\n            output = graph.make_node(\n                \"ReduceSum\",\n                inputs=inputs + [axes],\n                outputs=outputs,\n                keepdims=keepdims)\n        else:\n            output = graph.make_node(\n                op_type,\n                inputs=inputs,\n                outputs=outputs,\n                axes=axes,\n                keepdims=keepdims)",
  "class Mean():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        shape = graph.make_node(\"Constant\", dtype=dtypes.ONNX.INT64, value=[-1])\n        flatten_node = graph.make_node(\n            \"Reshape\", inputs=[node.input(\"X\")[0], shape])\n        mean_node = graph.make_node(\n            'ReduceMean',\n            inputs=flatten_node,\n            outputs=node.output(\"Out\"),\n            keepdims=1)",
  "class ArgMax():\n    support_opset_version_range = (1, 12)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        if node.attr('dtype') and node.attr('dtype') == 2:\n            arg_node = graph.make_node(\n                'ArgMax',\n                inputs=node.input('X'),\n                attrs={\n                    'axis': node.attr('axis'),\n                    'keepdims': node.attr('keepdims')\n                })\n            graph.make_node(\n                'Cast',\n                inputs=arg_node,\n                attrs={'to': dtypes.ONNX.INT32},\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'ArgMax',\n                inputs=node.input('X'),\n                outputs=node.output('Out'),\n                attrs={\n                    'axis': node.attr('axis'),\n                    'keepdims': node.attr('keepdims')\n                })",
  "class ArgMin():\n    support_opset_version_range = (1, 12)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        if node.attr('flatten'):\n            flatten = graph.make_node('Flatten', inputs=node.input('X'), axis=0)\n            squeeze_node = graph.make_node('Squeeze', inputs=flatten)\n            graph.make_node(\n                'ArgMin', inputs=squeeze_node, outputs=node.output('Out'))\n        else:\n            if node.attr('keepdims'):\n                graph.make_node(\n                    'ArgMin',\n                    inputs=node.input('X'),\n                    outputs=node.output('Out'),\n                    axis=node.attr('axis'),\n                    keepdims=1)\n            else:\n                graph.make_node(\n                    'ArgMin',\n                    inputs=node.input('X'),\n                    outputs=node.output('Out'),\n                    axis=node.attr('axis'),\n                    keepdims=0)",
  "class Hardtanh():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_6(cls, graph, node, **kw):\n        mapper_helper.clip_helper(graph, node,\n                                  node.input('X', 0),\n                                  node.attr('t_max'),\n                                  node.attr('t_min'), node.output('Out', 0))",
  "class Mv():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'MatMul',\n            inputs=[node.input('X', 0), node.input('Vec', 0)],\n            outputs=node.output('Out'))",
  "class Dot():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        mul_node = graph.make_node(\n            'Mul', inputs=[node.input('X', 0), node.input('Y', 0)])\n        graph.make_node(\n            'ReduceSum',\n            inputs=[mul_node],\n            axes=[len(node.input_shape('X', 0)) - 1],\n            outputs=node.output('Out'))\n\n    @classmethod\n    def opset_13(cls, graph, node, **kw):\n        mul_node = graph.make_node(\n            'Mul', inputs=[node.input('X', 0), node.input('Y', 0)])\n        one = graph.make_node(\n            'Constant',\n            dtype=dtypes.ONNX.INT64,\n            value=[len(node.input_shape('X', 0)) - 1])\n        graph.make_node(\n            'ReduceSum', inputs=[mul_node, one], outputs=node.output('Out'))",
  "class Dist():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        sub_node = graph.make_node(\n            'Sub', inputs=[node.input('X', 0), node.input('Y', 0)])\n        abs_node = graph.make_node('Abs', inputs=sub_node)\n        if node.attr('p') == 0:\n            assert graph.opset_version >= 9, \"When p is 0, onnx opset should be (onnx_opset>=9).\"\n            sign_node = graph.make_node('Sign', inputs=abs_node)\n            sum_node = graph.make_node(\n                'ReduceSum', inputs=sign_node, keepdims=0)\n            mapper_helper.unsqueeze_helper(graph, sum_node, [0],\n                                           node.output('Out'))\n        elif node.attr('p') == float('inf'):\n            max_node = graph.make_node('ReduceMax', inputs=abs_node, keepdims=0)\n            mapper_helper.unsqueeze_helper(graph, max_node, [0],\n                                           node.output('Out'))\n        elif node.attr('p') == float('-inf'):\n            min_node = graph.make_node('ReduceMin', inputs=abs_node, keepdims=0)\n            mapper_helper.unsqueeze_helper(graph, min_node, [0],\n                                           node.output('Out'))\n        else:\n            x_dtype = node.input_dtype('X', 0)\n            p = graph.make_node(\n                'Constant',\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],\n                value=node.attr('p'))\n            pow_node = graph.make_node(\n                'Pow',\n                inputs=[abs_node, p], )\n            sum_node = graph.make_node('ReduceSum', inputs=pow_node, keepdims=0)\n            sum_node = mapper_helper.unsqueeze_helper(graph, sum_node, [0])\n            p_1 = graph.make_node('Reciprocal', inputs=p)\n            graph.make_node(\n                'Pow', inputs=[sum_node, p_1], outputs=node.output('Out'))",
  "class Round():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        graph.make_node(\n            'Round', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Rsqrt():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_6(cls, graph, node, **kw):\n        sqrt_node = graph.make_node('Sqrt', inputs=node.input('X'))\n        graph.make_node(\n            'Reciprocal', inputs=sqrt_node, outputs=node.output('Out'))",
  "class Sign():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Sign', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Scale():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        scale = node.attr('scale')\n        bias = node.attr('bias')\n        if len(node.input('ScaleTensor')) == 0 and np.fabs(\n                scale - 1.0) < 1e-06 and np.fabs(bias - 0.0) < 1e-06:\n            graph.make_node(\n                'Identity', inputs=node.input('X'), outputs=node.output('Out'))\n        else:\n            input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)]\n            if input_dtype in [\n                    dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64\n            ]:\n                outputs = None\n                data_type = dtypes.ONNX.FLOAT\n                cast_node = graph.make_node(\n                    'Cast', inputs=node.input('X'), attrs={'to': data_type})\n            else:\n                outputs = node.output('Out')\n                data_type = input_dtype\n                cast_node = node.input('X')[0]\n\n            if len(node.input('ScaleTensor')) > 0:\n                scale_node = node.input('ScaleTensor')[0]\n                scale_type = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(\n                    'ScaleTensor', 0)]\n                if scale_type != data_type:\n                    scale_node = graph.make_node(\n                        'Cast', inputs=[scale_node], attrs={'to': data_type})\n            else:\n                scale_node = graph.make_node(\n                    'Constant', attrs={'dtype': data_type,\n                                       'value': [scale]})\n            bias_node = graph.make_node(\n                'Constant', attrs={'dtype': data_type,\n                                   'value': [bias]})\n\n            if node.attr('bias_after_scale'):\n                node1 = graph.make_node('Mul', inputs=[cast_node, scale_node])\n                node2 = graph.make_node(\n                    'Add', inputs=[node1, bias_node], outputs=outputs)\n            else:\n                node1 = graph.make_node('Add', inputs=[cast_node, bias_node])\n                node2 = graph.make_node(\n                    'Mul', inputs=[node1, scale_node], outputs=outputs)\n\n            if input_dtype in [\n                    dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64\n            ]:\n                cast_node = graph.make_node(\n                    'Cast',\n                    inputs=node2,\n                    outputs=node.output('Out'),\n                    attrs={'to': input_dtype})",
  "class Softmax():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        axis = node.attr('axis')\n        shape = node.output_shape('Out', 0)\n        if axis is None:\n            axis = -1\n        if axis < 0:\n            axis += len(shape)\n        if axis == len(shape) - 1:\n            node = graph.make_node(\n                'Softmax',\n                inputs=node.input('X'),\n                outputs=node.output('Out'),\n                attrs={'axis': axis})\n        else:\n            perm = [i for i in range(len(shape))]\n            perm[-1] = axis\n            perm[axis] = len(shape) - 1\n            transpose_node = graph.make_node(\n                'Transpose', inputs=node.input('X'), attrs={'perm': perm})\n            softmax_node = graph.make_node(\n                'Softmax', inputs=[transpose_node], axis=-1)\n            transpose_node1 = graph.make_node(\n                'Transpose',\n                inputs=[softmax_node],\n                outputs=node.output('Out'),\n                attrs={'perm': perm})\n\n    @classmethod\n    def opset_13(cls, graph, node, **kw):\n        graph.make_node(\n            'Softmax',\n            inputs=node.input('X'),\n            axis=node.attr('axis'),\n            outputs=node.output('Out'))",
  "class Unfold():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n\n        strides = node.attr('strides')\n        stride_h = strides[0]\n        stride_w = strides[1]\n\n        paddings = node.attr('paddings')\n        padding_h_1 = paddings[0]\n        padding_w_1 = paddings[1]\n        padding_h_2 = paddings[2]\n        padding_w_2 = paddings[3]\n\n        dilations = node.attr('dilations')\n        dilation_h = dilations[0]\n        dilation_w = dilations[1]\n\n        kernel_sizes = node.attr('kernel_sizes')\n        kernel_h = kernel_sizes[0]\n        kernel_w = kernel_sizes[1]\n\n        input_w = mapper_helper.shape_helper(graph, node.input('X', 0), 3)\n        blocks_row_indices_node = cls._get_im2col_indices_along_dim(\n            graph, node, 2, kernel_h, dilation_h, padding_h_1, padding_h_2,\n            stride_h)\n        blocks_col_indices_node = cls._get_im2col_indices_along_dim(\n            graph, node, 3, kernel_w, dilation_w, padding_w_1, padding_w_2,\n            stride_w)\n\n        output_shape = cls._get_im2col_output_shape(graph, node, kernel_h,\n                                                    kernel_w)\n        padded_input = cls._get_im2col_padded_input(\n            graph, node, padding_h_1, padding_h_2, padding_w_1, padding_w_2)\n\n        output = graph.make_node(\n            'Gather', inputs=[padded_input, blocks_row_indices_node], axis=2)\n\n        output = graph.make_node(\n            'Gather', inputs=[output, blocks_col_indices_node], axis=4)\n        output = graph.make_node(\n            'Transpose', inputs=[output], perm=[0, 1, 2, 4, 3, 5])\n\n        graph.make_node(\n            'Reshape', inputs=[output, output_shape], outputs=node.output('Y'))\n\n    @classmethod\n    def _get_im2col_indices_along_dim(cls, graph, node, index, kernel_size_d,\n                                      dilation_d, padding_d_1, padding_d_2,\n                                      stride_d):\n        input_shape = node.input_shape('X', 0)\n        if input_shape[index] == -1:\n            input_d_node = mapper_helper.shape_helper(graph,\n                                                      node.input('X', 0), index)\n\n            padding_d_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=[padding_d_1 + padding_d_2])\n            blocks_d_node = graph.make_node(\n                'Add', inputs=[input_d_node, padding_d_node])\n\n            dilation_kernel_size_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=[dilation_d * (kernel_size_d - 1)])\n            blocks_d_node = graph.make_node(\n                'Sub', inputs=[blocks_d_node, dilation_kernel_size_node])\n\n            zero_node = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=[0])\n            stride_node = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=[stride_d])\n            blocks_d_indices_node = graph.make_node(\n                'Range', inputs=[zero_node, blocks_d_node, stride_node])\n        else:\n            end = input_shape[\n                index] + padding_d_1 + padding_d_2 - dilation_d * (kernel_size_d\n                                                                   - 1)\n            stride = stride_d\n            blocks_d_indices = np.arange(0, end, stride)\n            blocks_d_indices_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=blocks_d_indices.flatten().tolist())\n\n        kernel_grid = np.arange(0, kernel_size_d * dilation_d, dilation_d)\n        kernel_grid_node = graph.make_node(\n            'Constant',\n            dtype=dtypes.ONNX.INT64,\n            value=kernel_grid.flatten().tolist())\n\n        shape_node = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[-1, 1])\n        kernel_mask_node = graph.make_node(\n            'Reshape', inputs=[kernel_grid_node, shape_node])\n\n        block_mask_node = graph.make_node(\n            'Add', inputs=[blocks_d_indices_node, kernel_mask_node])\n        return block_mask_node\n\n    @classmethod\n    def _get_im2col_output_shape(cls, graph, node, kernel_h, kernel_w):\n        batch_dim = mapper_helper.shape_helper(graph, node.input('X', 0), 0)\n        channel_dim = mapper_helper.shape_helper(graph, node.input('X', 0), 1)\n\n        constant_node = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[kernel_h * kernel_w])\n        channel_unfolded = graph.make_node(\n            'Mul', inputs=[channel_dim, constant_node])\n\n        concat_const_node = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[-1])\n        result_node = graph.make_node(\n            'Concat',\n            inputs=[batch_dim, channel_unfolded, concat_const_node],\n            axis=0)\n\n        return result_node\n\n    @classmethod\n    def _get_im2col_padded_input(cls, graph, node, padding_h_1, padding_h_2,\n                                 padding_w_1, padding_w_2):\n        pad_const_node = graph.make_node(\n            'Constant',\n            dtype=dtypes.ONNX.INT64,\n            value=[\n                0, 0, padding_h_1, padding_w_1, 0, 0, padding_h_2, padding_w_2\n            ])\n        result_node = graph.make_node(\n            'Pad', inputs=[node.input('X', 0), pad_const_node])\n        return result_node",
  "class SoftmaxCrossEntropyLoss():\n    support_opset_version_range = (12, 15)\n\n    @classmethod\n    def opset_12(cls, graph, node, **kw):\n        if node.attr('soft_label'):\n            raise Exception(\n                \"SoftmaxCrossEntropyLoss in onnx not support soft label.\")\n        scores = node.input('Logits', 0)\n        labels = node.input('Label', 0)\n        # Whether return_softmax is True or False, the model will have two outputs\n        outputs = [node.output('Loss', 0), node.output('Softmax', 0)]\n\n        shape = node.input_shape('Logits', 0)\n        if len(shape) < 2:\n            raise Exception(\n                \"SoftmaxCrossEntropyLoss in onnx not support 1D logits.\")\n        axis = node.attr('axis')\n        if axis < 0:\n            axis += len(shape)\n        if axis == 1:\n            squeeze_node = mapper_helper.squeeze_helper(graph, labels, [axis])\n            loss_node, softmax_node = graph.make_node(\n                'SoftmaxCrossEntropyLoss',\n                inputs=[scores, squeeze_node],\n                outputs=2,\n                ignore_index=node.attr('ignore_index'),\n                reduction='none')\n            loss_node = mapper_helper.unsqueeze_helper(graph, loss_node,\n                                                       [axis], outputs[0])\n            # onnx output is log(softmax), but paddle output is softmax\n            graph.make_node('Exp', inputs=[softmax_node], outputs=outputs[1])\n        else:\n            perm = [i for i in range(len(shape))]\n            perm[1] = axis\n            perm[axis] = 1\n            transpose_scores = graph.make_node(\n                'Transpose', inputs=[scores], perm=perm)\n            transpose_labels = graph.make_node(\n                'Transpose', inputs=[labels], perm=perm)\n            squeeze_labels = mapper_helper.squeeze_helper(\n                graph, transpose_labels, [1])\n\n            loss_node, softmax_node = graph.make_node(\n                'SoftmaxCrossEntropyLoss',\n                inputs=[transpose_scores, squeeze_labels],\n                ignore_index=node.attr('ignore_index'),\n                outputs=2,\n                reduction='none')\n            output_node = mapper_helper.unsqueeze_helper(graph, loss_node, [1])\n            graph.make_node(\n                'Transpose', inputs=output_node, outputs=outputs[0], perm=perm)\n            softmax_node = graph.make_node(\n                'Transpose', inputs=softmax_node, perm=perm)\n            # onnx output is log(softmax), but paddle output is softmax\n            graph.make_node('Exp', inputs=[softmax_node], outputs=outputs[1])",
  "def opset_1(cls, graph, node, **kw):\n        x = node.input('X', idx=0)\n        y = node.input('Y', idx=0)\n        if node.attr('transpose_X'):\n            perm = list(range(len(node.input_shape('X', 0))))\n            perm[-1], perm[-2] = perm[-2], perm[-1]\n            if node.input_dtype('X', 0) == paddle.float64:\n                x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)\n            x = graph.make_node('Transpose', inputs=[x], perm=perm)\n        if node.attr('transpose_Y'):\n            perm = list(range(len(node.input_shape('Y', 0))))\n            perm[-1], perm[-2] = perm[-2], perm[-1]\n            if node.input_dtype('Y', 0) == paddle.float64:\n                y = graph.make_node('Cast', inputs=y, to=dtypes.ONNX.FLOAT)\n            y = graph.make_node('Transpose', inputs=[y], perm=perm)\n        if node.attr('alpha') == 1.0:\n            if node.input_dtype('X', 0) == paddle.float64:\n                output_node = graph.make_node('MatMul', inputs=[x, y])\n                graph.make_node(\n                    'Cast',\n                    inputs=output_node,\n                    to=dtypes.ONNX.DOUBLE,\n                    outputs=node.output('Out'))\n            else:\n                graph.make_node(\n                    'MatMul', inputs=[x, y], outputs=node.output('Out'))\n        else:\n            if node.input_dtype('X', 0) == paddle.float64:\n                output_node = graph.make_node('MatMul', inputs=[x, y])\n                matmul = graph.make_node(\n                    'Cast', inputs=output_node, to=dtypes.ONNX.DOUBLE)\n                scale = graph.make_node(\n                    'Constant',\n                    dtype=dtypes.ONNX.DOUBLE,\n                    value=node.attr('alpha'))\n            else:\n                matmul = graph.make_node('MatMul', inputs=[x, y])\n                scale = graph.make_node(\n                    'Constant',\n                    dtype=dtypes.ONNX.FLOAT,\n                    value=node.attr('alpha'))\n\n            onnx_node = graph.make_node(\n                'Mul', inputs=[matmul, scale], outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        x = node.input('X', idx=0)\n        y = node.input('Y', idx=0)\n        out = node.output('Out')\n        ## TODO(wangjunjie06): The current addition of cast op is only for onnxruntime optimization, after onnxruntime is repaired, remove this logic\n        if node.attr('trans_x'):\n            perm = list(range(len(node.input_shape('X', 0))))\n            perm[-1], perm[-2] = perm[-2], perm[-1]\n            if node.input_dtype('X', 0) == paddle.float64:\n                x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)\n            x = graph.make_node('Transpose', inputs=[x], perm=perm)\n        if node.attr('trans_y'):\n            perm = list(range(len(node.input_shape('Y', 0))))\n            perm[-1], perm[-2] = perm[-2], perm[-1]\n            if node.input_dtype('Y', 0) == paddle.float64:\n                y = graph.make_node('Cast', inputs=y, to=dtypes.ONNX.FLOAT)\n            y = graph.make_node('Transpose', inputs=[y], perm=perm)\n        if node.input_dtype('X', 0) == paddle.float64:\n            output_node = graph.make_node('MatMul', inputs=[x, y])\n            graph.make_node(\n                'Cast', inputs=output_node, to=dtypes.ONNX.DOUBLE, outputs=out)\n        else:\n            graph.make_node('MatMul', inputs=[x, y], outputs=out)",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Exp', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Abs', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_9(cls, graph, node, **kw):\n        x_dtype = node.input_dtype('X', 0)\n        x = node.input('X', 0)\n        # onnxruntime only support float32 Erf\n        if x_dtype != paddle.float32:\n            x = graph.make_node('Cast', inputs=x, to=dtypes.ONNX.FLOAT)\n            erf_node = graph.make_node('Erf', inputs=[x])\n            graph.make_node(\n                'Cast',\n                inputs=[erf_node],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node('Erf', inputs=[x], outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Acos', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Asin', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Sinh', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Sin', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Atan', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Tan', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_6(cls, graph, node, **kw):\n        graph.make_node(\n            'Ceil', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Cos', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Cosh', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        _ln2 = 0.693147180559945309\n        dtype = dtypes.ONNX.FLOAT\n        if node.input_dtype('X', 0) == paddle.float64:\n            dtype = dtypes.ONNX.DOUBLE\n        _ln2 = graph.make_node('Constant', dtype=dtype, value=_ln2)\n        lnx = graph.make_node('Log', inputs=node.input('X'))\n        graph.make_node('Div', inputs=[lnx, _ln2], outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n\n        if node.attr('reduce_all'):\n            if not node.attr('keepdim'):\n                reduce_node = graph.make_node(\n                    'ReduceLogSumExp',\n                    inputs=node.input('X'),\n                    keepdims=node.attr('keepdim'))\n                mapper_helper.unsqueeze_helper(graph, reduce_node, [0],\n                                               node.output('Out'))\n            else:\n                graph.make_node(\n                    'ReduceLogSumExp',\n                    inputs=node.input('X'),\n                    keepdims=node.attr('keepdim'),\n                    outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'ReduceLogSumExp',\n                inputs=node.input('X'),\n                keepdims=node.attr('keepdim'),\n                axes=node.attr('axis'),\n                outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        x_shape = node.input_shape('X', 0)\n        y_shape = node.input_shape('Y', 0)\n        if node.type in [\"elementwise_min\", \"elementwise_max\"]:\n            assert False, \"{} op is not supported when opset version < 8\".format(\n                node.type)\n\n        op_type = kw['mapper_dict'][node.type]\n        axis = node.attr('axis')\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n\n        if axis == -1 or axis == (len(x_shape) - 1\n                                  ) or len(x_shape) == len(y_shape):\n            onnx_node = graph.make_node(\n                op_type, inputs=[x, y], outputs=node.output('Out'))\n        else:\n            broadcast_shape = [1] * len(x_shape)\n            broadcast_shape[axis:axis + len(y_shape)] = y_shape\n            broadcast_shape_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=list(broadcast_shape))\n            y_node = graph.make_node(\n                'Reshape', inputs=[y, broadcast_shape_node])\n            onnx_node = graph.make_node(\n                op_type, inputs=[x, y_node], outputs=node.output('Out'))",
  "def opset_8(cls, graph, node, **kw):\n        op_type = kw['mapper_dict'][node.type]\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n        axis = node.attr('axis')\n        x_shape = node.input_shape('X', 0)\n        y_shape = node.input_shape('Y', 0)\n        if axis == -1 or axis == (len(x_shape) - 1\n                                  ) or len(x_shape) == len(y_shape):\n            onnx_node = graph.make_node(\n                op_type, inputs=[x, y], outputs=node.output('Out'))\n        else:\n            broadcast_shape = [1] * len(x_shape)\n            broadcast_shape[axis:axis + len(y_shape)] = y_shape\n            broadcast_shape_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=list(broadcast_shape))\n            y_node = graph.make_node(\n                'Reshape', inputs=[y, broadcast_shape_node])\n            onnx_node = graph.make_node(\n                op_type, inputs=[x, y_node], outputs=node.output('Out'))",
  "def opset_10(cls, graph, node, **kw):\n        x_shape = node.input_shape('X', 0)\n        y_shape = node.input_shape('Y', 0)\n        axis = node.attr('axis')\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n\n        if node.input_dtype('Y', 0) == paddle.int32 or node.input_dtype(\n                'Y', 0) == paddle.int64:\n            onnx_node = graph.make_node(\n                \"Mod\", inputs=[x, y], outputs=node.output('Out'))\n            return\n\n        fmod = 1\n\n        abs_x_node = graph.make_node(\"Abs\", inputs=[x])\n        abs_y_node = graph.make_node(\"Abs\", inputs=[y])\n\n        dtype = dtypes.ONNX.FLOAT\n        val_0 = [0.0]\n        val_1 = [-1.0]\n        if node.input_dtype('Y', 0) == paddle.float64:\n            dtype = dtypes.ONNX.DOUBLE\n        if node.input_dtype('Y', 0) == paddle.int32:\n            dtype = dtypes.ONNX.INT32\n            val_0 = [0]\n            val_1 = [-1]\n        if node.input_dtype('Y', 0) == paddle.int64:\n            dtype = dtypes.ONNX.INT64\n            val_0 = [0]\n            val_1 = [-1]\n        zero_node = graph.make_node('Constant', dtype=dtype, value=val_0)\n        one_node = graph.make_node('Constant', dtype=dtype, value=val_1)\n\n        mod_node = graph.make_node(\n            \"Mod\", inputs=[abs_x_node, abs_y_node], fmod=fmod)\n\n        minus_node = graph.make_node(\"Mul\", inputs=[mod_node, one_node])\n\n        condition_dtype = graph.make_node(\"Less\", inputs=[x, zero_node])\n        condition = graph.make_node(\n            'Cast', inputs=[condition_dtype], to=dtypes.ONNX.BOOL)\n\n        mod_res = graph.make_node(\n            \"Where\", inputs=[condition, minus_node, mod_node])\n\n        add_node = graph.make_node(\"Add\", inputs=[mod_res, y])\n\n        mod_y_mul_node = graph.make_node(\"Mul\", inputs=[mod_res, y])\n        condition_dtype_1 = graph.make_node(\n            \"Less\", inputs=[mod_y_mul_node, zero_node])\n        condition_1 = graph.make_node(\n            'Cast', inputs=[condition_dtype_1], to=dtypes.ONNX.BOOL)\n\n        graph.make_node(\n            \"Where\",\n            inputs=[condition_1, add_node, mod_res],\n            outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n        axis = node.attr('axis')\n        x_shape = node.input_shape('X', 0)\n        y_shape = node.input_shape('Y', 0)\n        x_dtype = node.input_dtype('X', 0)\n        y_dtype = node.input_dtype('Y', 0)\n        x_dtype = dtypes.DTYPE_PADDLE_STR_MAP[x_dtype]\n        y_dtype = dtypes.DTYPE_PADDLE_STR_MAP[y_dtype]\n        is_int = False\n        if x_dtype.count('int') > 0 and y_dtype.count('int') > 0:\n            is_int = True\n        if axis == -1 or axis == (len(x_shape) - 1\n                                  ) or len(x_shape) == len(y_shape):\n            if is_int:\n                graph.make_node(\n                    'Div', inputs=[x, y], outputs=node.output('Out'))\n            else:\n                div_node = graph.make_node('Div', inputs=[x, y])\n                graph.make_node(\n                    'Floor', inputs=[div_node], outputs=node.output('Out'))\n        else:\n            broadcast_shape = [1] * len(x_shape)\n            broadcast_shape[axis:axis + len(y_shape)] = y_shape\n            broadcast_shape_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=list(broadcast_shape))\n            y_node = graph.make_node(\n                'Reshape', inputs=[y, broadcast_shape_node])\n            if is_int:\n                div_node = graph.make_node(\n                    'Div', inputs=[x, y_node], outputs=node.output('Out'))\n            else:\n                div_node = graph.make_node('Div', inputs=[x, y_node])\n                graph.make_node(\n                    'Floor', inputs=[div_node], outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        x_dtype = node.input_dtype('X', 0)\n        factor = node.attr('factor')\n        # Pow-7 Only support input type as float and double\n        if x_dtype == paddle.int32 or x_dtype == paddle.int64:\n            x = graph.make_node('Cast', inputs=[x], to=dtypes.ONNX.FLOAT)\n            factor_node = graph.make_node(\n                'Constant',\n                inputs=[],\n                dims=[1],\n                dtype=dtypes.ONNX.FLOAT,\n                value=factor)\n        else:\n            factor_node = graph.make_node(\n                'Constant',\n                inputs=[],\n                dims=[1],\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],\n                value=factor)\n        if x_dtype == paddle.int32 or x_dtype == paddle.int64:\n            pow_node = graph.make_node('Pow', inputs=[x, factor_node])\n            graph.make_node(\n                'Cast',\n                inputs=[pow_node],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Pow', inputs=[x, factor_node], outputs=node.output('Out'))",
  "def opset_12(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        factor = node.attr('factor')\n        factor_node = graph.make_node(\n            'Constant',\n            inputs=[],\n            dims=[1],\n            dtype=dtypes.ONNX.FLOAT,\n            value=factor)\n        pow_node = graph.make_node(\n            'Pow', inputs=[x, factor_node], outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        onnx_node = graph.make_node(\n            'Mul', inputs=[x, x], outputs=node.output('Out'))",
  "def opset_11(cls, graph, node, **kw):\n\n        axis = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=node.attr('axis'))\n        graph.make_node(\n            'CumSum',\n            inputs=[node.input('X', 0), axis],\n            outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n        out = node.output('Out', 0)\n        x_num_col_dims = node.attr('x_num_col_dims')\n        y_num_col_dims = node.attr('y_num_col_dims')\n        flatten_x = graph.make_node(\n            'Flatten', inputs=node.input('X'), attrs={'axis': x_num_col_dims})\n        flatten_y = graph.make_node(\n            'Flatten', inputs=node.input('Y'), attrs={'axis': y_num_col_dims})\n        mul_node = graph.make_node('MatMul', inputs=[flatten_x, flatten_y])\n\n        x_shape = graph.make_node('Shape', inputs=[x])\n        l_shape = mapper_helper.slice_helper(\n            graph, x_shape, axes=[0], starts=[0], ends=[x_num_col_dims])\n        y_shape = graph.make_node('Shape', inputs=[y])\n        y_rank = len(node.input_shape('Y', 0))\n        r_shape = mapper_helper.slice_helper(\n            graph, y_shape, axes=[0], starts=[y_num_col_dims], ends=[y_rank])\n\n        out_shape = graph.make_node('Concat', inputs=[l_shape, r_shape], axis=0)\n        graph.make_node('Reshape', [mul_node, out_shape], node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        if \"data_layout\" in node.attrs.keys():\n            assert node.attrs['data_layout'] == 'NCHW' or node.attrs['data_layout'] == \"AnyLayout\",  \\\n                                \"The affine_channel data format should be 'NCHW', but received data format \" \\\n                                \"is %s.\" % node.attrs['data_layout']\n        x = node.input('X', 0)\n        bias = node.input('Bias', 0)\n        scale = node.input('Scale', 0)\n        scale = mapper_helper.unsqueeze_helper(graph, scale, [0, 2, 3])\n        bias = mapper_helper.unsqueeze_helper(graph, bias, [0, 2, 3])\n        x = graph.make_node('Mul', inputs=[x, scale])\n        x = graph.make_node('Add', inputs=[x, bias], outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        y = node.input('Y', 0)\n        mul_node = graph.make_node(\n            'MatMul', inputs=[x, y], outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        axis = node.attr('axis')\n        if isinstance(axis, (int, float)):\n            axis = [axis]\n        p = node.attr('porder')\n        keepdim = node.attr('keepdim')\n        dtype = dtypes.ONNX.FLOAT\n        if node.input_dtype('X', 0) == paddle.float64:\n            dtype = dtypes.ONNX.DOUBLE\n\n        pnode = graph.make_node('Constant', dtype=dtype, value=[p])\n\n        abs_node = graph.make_node('Abs', inputs=[x])\n        pow_node = graph.make_node('Pow', inputs=[abs_node, pnode])\n        reduce_sum = graph.make_node(\n            'ReduceSum', inputs=[pow_node], axes=axis, keepdims=keepdim)\n        pnode1 = graph.make_node('Constant', dtype=dtype, value=[1.0 / p])\n        graph.make_node(\n            'Pow', inputs=[reduce_sum, pnode1], outputs=node.output('Out'))",
  "def opset_13(cls, graph, node, **kw):\n        x = node.input('X', 0)\n        axis = node.attr('axis')\n        if isinstance(axis, (int, float)):\n            axis = [axis]\n        p = node.attr('porder')\n        keepdim = node.attr('keepdim')\n        pnode = graph.make_node('Constant', dtype=dtypes.ONNX.FLOAT, value=[p])\n        abs_node = graph.make_node('Abs', inputs=[x])\n        pow_node = graph.make_node('Pow', inputs=[abs_node, pnode])\n        axes = graph.make_node('Constant', dtype=dtypes.ONNX.INT64, value=axis)\n        reduce_sum = graph.make_node(\n            'ReduceSum', inputs=[pow_node, axes], keepdims=keepdim)\n        pnode1 = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.FLOAT, value=[1.0 / p])\n        graph.make_node(\n            'Pow', inputs=[reduce_sum, pnode1], outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Sum', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Floor', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        _ln10 = 2.30258509299404568401\n        dtype = dtypes.ONNX.FLOAT\n        if node.input_dtype('X', 0) == paddle.float64:\n            dtype = dtypes.ONNX.DOUBLE\n        _ln10 = graph.make_node('Constant', dtype=dtype, value=_ln10)\n        lnx = graph.make_node('Log', inputs=node.input('X'))\n        graph.make_node('Div', inputs=[lnx, _ln10], outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        dtype = dtypes.ONNX.FLOAT\n        if node.input_dtype('X', 0) == paddle.float64:\n            dtype = dtypes.ONNX.DOUBLE\n        one = graph.make_node('Constant', attrs={'dtype': dtype, 'value': [1]})\n        add_node = graph.make_node('Add', inputs=[node.input('X', 0), one])\n        graph.make_node('Log', inputs=add_node, outputs=node.output('Out'))",
  "def opset_6(cls, graph, node, **kw):\n        op_type = kw['mapper_dict'][node.type]\n        input_dtype = node.block.vars[node.input('X', 0)].dtype\n        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[input_dtype]\n        all_node = graph.make_node(\n            'Cast', inputs=[node.input('X', 0)], to=dtypes.ONNX.INT32)\n\n        attrs = {'keepdims': node.attr('keep_dim'), }\n        if not node.attr('reduce_all'):\n            attrs['axes'] = node.attr('dim')\n        output_node = graph.make_node(op_type, inputs=[all_node], attrs=attrs)\n\n        if node.attr('reduce_all') and not node.attr('keep_dim'):\n            output_node = mapper_helper.unsqueeze_helper(graph, output_node,\n                                                         [0])\n        graph.make_node(\n            'Cast',\n            inputs=[output_node],\n            to=input_dtype,\n            outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        op_type = kw['mapper_dict'][node.type]\n\n        output_shape = node.output_shape('Out', 0)\n        reduce_all = node.attr(\"reduce_all\")\n        axes = node.attr(\"dim\")\n        if reduce_all:\n            axes = list(range(len(node.input_shape(\"X\", 0))))\n        if len(axes) == len(node.input_shape(\"X\", 0)):\n            reduce_all = True\n        keepdims = node.attr('keep_dim')\n        if keepdims:\n            cls.create_reduce_node(graph, op_type,\n                                   node.input(\"X\"), node.output(\"Out\"), axes, 1)\n        else:\n            if reduce_all:\n                shape = graph.make_node(\n                    \"Constant\", dtype=dtypes.ONNX.INT64, value=[-1])\n                flatten_node = graph.make_node(\n                    \"Reshape\", inputs=[node.input(\"X\")[0], shape])\n                cls.create_reduce_node(graph, op_type, [flatten_node],\n                                       node.output(\"Out\"), [0], 1)\n            else:\n                cls.create_reduce_node(graph, op_type,\n                                       node.input(\"X\"),\n                                       node.output(\"Out\"), axes, 0)",
  "def create_reduce_node(cls, graph, op_type, inputs, outputs, axes,\n                           keepdims):\n        if graph.opset_version >= 13 and op_type == \"ReduceSum\":\n            axes = graph.make_node(\n                \"Constant\", dtype=dtypes.ONNX.INT64, value=axes)\n            output = graph.make_node(\n                \"ReduceSum\",\n                inputs=inputs + [axes],\n                outputs=outputs,\n                keepdims=keepdims)\n        else:\n            output = graph.make_node(\n                op_type,\n                inputs=inputs,\n                outputs=outputs,\n                axes=axes,\n                keepdims=keepdims)",
  "def opset_7(cls, graph, node, **kw):\n        shape = graph.make_node(\"Constant\", dtype=dtypes.ONNX.INT64, value=[-1])\n        flatten_node = graph.make_node(\n            \"Reshape\", inputs=[node.input(\"X\")[0], shape])\n        mean_node = graph.make_node(\n            'ReduceMean',\n            inputs=flatten_node,\n            outputs=node.output(\"Out\"),\n            keepdims=1)",
  "def opset_1(cls, graph, node, **kw):\n        if node.attr('dtype') and node.attr('dtype') == 2:\n            arg_node = graph.make_node(\n                'ArgMax',\n                inputs=node.input('X'),\n                attrs={\n                    'axis': node.attr('axis'),\n                    'keepdims': node.attr('keepdims')\n                })\n            graph.make_node(\n                'Cast',\n                inputs=arg_node,\n                attrs={'to': dtypes.ONNX.INT32},\n                outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'ArgMax',\n                inputs=node.input('X'),\n                outputs=node.output('Out'),\n                attrs={\n                    'axis': node.attr('axis'),\n                    'keepdims': node.attr('keepdims')\n                })",
  "def opset_1(cls, graph, node, **kw):\n        if node.attr('flatten'):\n            flatten = graph.make_node('Flatten', inputs=node.input('X'), axis=0)\n            squeeze_node = graph.make_node('Squeeze', inputs=flatten)\n            graph.make_node(\n                'ArgMin', inputs=squeeze_node, outputs=node.output('Out'))\n        else:\n            if node.attr('keepdims'):\n                graph.make_node(\n                    'ArgMin',\n                    inputs=node.input('X'),\n                    outputs=node.output('Out'),\n                    axis=node.attr('axis'),\n                    keepdims=1)\n            else:\n                graph.make_node(\n                    'ArgMin',\n                    inputs=node.input('X'),\n                    outputs=node.output('Out'),\n                    axis=node.attr('axis'),\n                    keepdims=0)",
  "def opset_6(cls, graph, node, **kw):\n        mapper_helper.clip_helper(graph, node,\n                                  node.input('X', 0),\n                                  node.attr('t_max'),\n                                  node.attr('t_min'), node.output('Out', 0))",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'MatMul',\n            inputs=[node.input('X', 0), node.input('Vec', 0)],\n            outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        mul_node = graph.make_node(\n            'Mul', inputs=[node.input('X', 0), node.input('Y', 0)])\n        graph.make_node(\n            'ReduceSum',\n            inputs=[mul_node],\n            axes=[len(node.input_shape('X', 0)) - 1],\n            outputs=node.output('Out'))",
  "def opset_13(cls, graph, node, **kw):\n        mul_node = graph.make_node(\n            'Mul', inputs=[node.input('X', 0), node.input('Y', 0)])\n        one = graph.make_node(\n            'Constant',\n            dtype=dtypes.ONNX.INT64,\n            value=[len(node.input_shape('X', 0)) - 1])\n        graph.make_node(\n            'ReduceSum', inputs=[mul_node, one], outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        sub_node = graph.make_node(\n            'Sub', inputs=[node.input('X', 0), node.input('Y', 0)])\n        abs_node = graph.make_node('Abs', inputs=sub_node)\n        if node.attr('p') == 0:\n            assert graph.opset_version >= 9, \"When p is 0, onnx opset should be (onnx_opset>=9).\"\n            sign_node = graph.make_node('Sign', inputs=abs_node)\n            sum_node = graph.make_node(\n                'ReduceSum', inputs=sign_node, keepdims=0)\n            mapper_helper.unsqueeze_helper(graph, sum_node, [0],\n                                           node.output('Out'))\n        elif node.attr('p') == float('inf'):\n            max_node = graph.make_node('ReduceMax', inputs=abs_node, keepdims=0)\n            mapper_helper.unsqueeze_helper(graph, max_node, [0],\n                                           node.output('Out'))\n        elif node.attr('p') == float('-inf'):\n            min_node = graph.make_node('ReduceMin', inputs=abs_node, keepdims=0)\n            mapper_helper.unsqueeze_helper(graph, min_node, [0],\n                                           node.output('Out'))\n        else:\n            x_dtype = node.input_dtype('X', 0)\n            p = graph.make_node(\n                'Constant',\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype],\n                value=node.attr('p'))\n            pow_node = graph.make_node(\n                'Pow',\n                inputs=[abs_node, p], )\n            sum_node = graph.make_node('ReduceSum', inputs=pow_node, keepdims=0)\n            sum_node = mapper_helper.unsqueeze_helper(graph, sum_node, [0])\n            p_1 = graph.make_node('Reciprocal', inputs=p)\n            graph.make_node(\n                'Pow', inputs=[sum_node, p_1], outputs=node.output('Out'))",
  "def opset_11(cls, graph, node, **kw):\n        graph.make_node(\n            'Round', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_6(cls, graph, node, **kw):\n        sqrt_node = graph.make_node('Sqrt', inputs=node.input('X'))\n        graph.make_node(\n            'Reciprocal', inputs=sqrt_node, outputs=node.output('Out'))",
  "def opset_9(cls, graph, node, **kw):\n        graph.make_node(\n            'Sign', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        scale = node.attr('scale')\n        bias = node.attr('bias')\n        if len(node.input('ScaleTensor')) == 0 and np.fabs(\n                scale - 1.0) < 1e-06 and np.fabs(bias - 0.0) < 1e-06:\n            graph.make_node(\n                'Identity', inputs=node.input('X'), outputs=node.output('Out'))\n        else:\n            input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)]\n            if input_dtype in [\n                    dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64\n            ]:\n                outputs = None\n                data_type = dtypes.ONNX.FLOAT\n                cast_node = graph.make_node(\n                    'Cast', inputs=node.input('X'), attrs={'to': data_type})\n            else:\n                outputs = node.output('Out')\n                data_type = input_dtype\n                cast_node = node.input('X')[0]\n\n            if len(node.input('ScaleTensor')) > 0:\n                scale_node = node.input('ScaleTensor')[0]\n                scale_type = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(\n                    'ScaleTensor', 0)]\n                if scale_type != data_type:\n                    scale_node = graph.make_node(\n                        'Cast', inputs=[scale_node], attrs={'to': data_type})\n            else:\n                scale_node = graph.make_node(\n                    'Constant', attrs={'dtype': data_type,\n                                       'value': [scale]})\n            bias_node = graph.make_node(\n                'Constant', attrs={'dtype': data_type,\n                                   'value': [bias]})\n\n            if node.attr('bias_after_scale'):\n                node1 = graph.make_node('Mul', inputs=[cast_node, scale_node])\n                node2 = graph.make_node(\n                    'Add', inputs=[node1, bias_node], outputs=outputs)\n            else:\n                node1 = graph.make_node('Add', inputs=[cast_node, bias_node])\n                node2 = graph.make_node(\n                    'Mul', inputs=[node1, scale_node], outputs=outputs)\n\n            if input_dtype in [\n                    dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64\n            ]:\n                cast_node = graph.make_node(\n                    'Cast',\n                    inputs=node2,\n                    outputs=node.output('Out'),\n                    attrs={'to': input_dtype})",
  "def opset_1(cls, graph, node, **kw):\n        axis = node.attr('axis')\n        shape = node.output_shape('Out', 0)\n        if axis is None:\n            axis = -1\n        if axis < 0:\n            axis += len(shape)\n        if axis == len(shape) - 1:\n            node = graph.make_node(\n                'Softmax',\n                inputs=node.input('X'),\n                outputs=node.output('Out'),\n                attrs={'axis': axis})\n        else:\n            perm = [i for i in range(len(shape))]\n            perm[-1] = axis\n            perm[axis] = len(shape) - 1\n            transpose_node = graph.make_node(\n                'Transpose', inputs=node.input('X'), attrs={'perm': perm})\n            softmax_node = graph.make_node(\n                'Softmax', inputs=[transpose_node], axis=-1)\n            transpose_node1 = graph.make_node(\n                'Transpose',\n                inputs=[softmax_node],\n                outputs=node.output('Out'),\n                attrs={'perm': perm})",
  "def opset_13(cls, graph, node, **kw):\n        graph.make_node(\n            'Softmax',\n            inputs=node.input('X'),\n            axis=node.attr('axis'),\n            outputs=node.output('Out'))",
  "def opset_11(cls, graph, node, **kw):\n\n        strides = node.attr('strides')\n        stride_h = strides[0]\n        stride_w = strides[1]\n\n        paddings = node.attr('paddings')\n        padding_h_1 = paddings[0]\n        padding_w_1 = paddings[1]\n        padding_h_2 = paddings[2]\n        padding_w_2 = paddings[3]\n\n        dilations = node.attr('dilations')\n        dilation_h = dilations[0]\n        dilation_w = dilations[1]\n\n        kernel_sizes = node.attr('kernel_sizes')\n        kernel_h = kernel_sizes[0]\n        kernel_w = kernel_sizes[1]\n\n        input_w = mapper_helper.shape_helper(graph, node.input('X', 0), 3)\n        blocks_row_indices_node = cls._get_im2col_indices_along_dim(\n            graph, node, 2, kernel_h, dilation_h, padding_h_1, padding_h_2,\n            stride_h)\n        blocks_col_indices_node = cls._get_im2col_indices_along_dim(\n            graph, node, 3, kernel_w, dilation_w, padding_w_1, padding_w_2,\n            stride_w)\n\n        output_shape = cls._get_im2col_output_shape(graph, node, kernel_h,\n                                                    kernel_w)\n        padded_input = cls._get_im2col_padded_input(\n            graph, node, padding_h_1, padding_h_2, padding_w_1, padding_w_2)\n\n        output = graph.make_node(\n            'Gather', inputs=[padded_input, blocks_row_indices_node], axis=2)\n\n        output = graph.make_node(\n            'Gather', inputs=[output, blocks_col_indices_node], axis=4)\n        output = graph.make_node(\n            'Transpose', inputs=[output], perm=[0, 1, 2, 4, 3, 5])\n\n        graph.make_node(\n            'Reshape', inputs=[output, output_shape], outputs=node.output('Y'))",
  "def _get_im2col_indices_along_dim(cls, graph, node, index, kernel_size_d,\n                                      dilation_d, padding_d_1, padding_d_2,\n                                      stride_d):\n        input_shape = node.input_shape('X', 0)\n        if input_shape[index] == -1:\n            input_d_node = mapper_helper.shape_helper(graph,\n                                                      node.input('X', 0), index)\n\n            padding_d_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=[padding_d_1 + padding_d_2])\n            blocks_d_node = graph.make_node(\n                'Add', inputs=[input_d_node, padding_d_node])\n\n            dilation_kernel_size_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=[dilation_d * (kernel_size_d - 1)])\n            blocks_d_node = graph.make_node(\n                'Sub', inputs=[blocks_d_node, dilation_kernel_size_node])\n\n            zero_node = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=[0])\n            stride_node = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=[stride_d])\n            blocks_d_indices_node = graph.make_node(\n                'Range', inputs=[zero_node, blocks_d_node, stride_node])\n        else:\n            end = input_shape[\n                index] + padding_d_1 + padding_d_2 - dilation_d * (kernel_size_d\n                                                                   - 1)\n            stride = stride_d\n            blocks_d_indices = np.arange(0, end, stride)\n            blocks_d_indices_node = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=blocks_d_indices.flatten().tolist())\n\n        kernel_grid = np.arange(0, kernel_size_d * dilation_d, dilation_d)\n        kernel_grid_node = graph.make_node(\n            'Constant',\n            dtype=dtypes.ONNX.INT64,\n            value=kernel_grid.flatten().tolist())\n\n        shape_node = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[-1, 1])\n        kernel_mask_node = graph.make_node(\n            'Reshape', inputs=[kernel_grid_node, shape_node])\n\n        block_mask_node = graph.make_node(\n            'Add', inputs=[blocks_d_indices_node, kernel_mask_node])\n        return block_mask_node",
  "def _get_im2col_output_shape(cls, graph, node, kernel_h, kernel_w):\n        batch_dim = mapper_helper.shape_helper(graph, node.input('X', 0), 0)\n        channel_dim = mapper_helper.shape_helper(graph, node.input('X', 0), 1)\n\n        constant_node = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[kernel_h * kernel_w])\n        channel_unfolded = graph.make_node(\n            'Mul', inputs=[channel_dim, constant_node])\n\n        concat_const_node = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[-1])\n        result_node = graph.make_node(\n            'Concat',\n            inputs=[batch_dim, channel_unfolded, concat_const_node],\n            axis=0)\n\n        return result_node",
  "def _get_im2col_padded_input(cls, graph, node, padding_h_1, padding_h_2,\n                                 padding_w_1, padding_w_2):\n        pad_const_node = graph.make_node(\n            'Constant',\n            dtype=dtypes.ONNX.INT64,\n            value=[\n                0, 0, padding_h_1, padding_w_1, 0, 0, padding_h_2, padding_w_2\n            ])\n        result_node = graph.make_node(\n            'Pad', inputs=[node.input('X', 0), pad_const_node])\n        return result_node",
  "def opset_12(cls, graph, node, **kw):\n        if node.attr('soft_label'):\n            raise Exception(\n                \"SoftmaxCrossEntropyLoss in onnx not support soft label.\")\n        scores = node.input('Logits', 0)\n        labels = node.input('Label', 0)\n        # Whether return_softmax is True or False, the model will have two outputs\n        outputs = [node.output('Loss', 0), node.output('Softmax', 0)]\n\n        shape = node.input_shape('Logits', 0)\n        if len(shape) < 2:\n            raise Exception(\n                \"SoftmaxCrossEntropyLoss in onnx not support 1D logits.\")\n        axis = node.attr('axis')\n        if axis < 0:\n            axis += len(shape)\n        if axis == 1:\n            squeeze_node = mapper_helper.squeeze_helper(graph, labels, [axis])\n            loss_node, softmax_node = graph.make_node(\n                'SoftmaxCrossEntropyLoss',\n                inputs=[scores, squeeze_node],\n                outputs=2,\n                ignore_index=node.attr('ignore_index'),\n                reduction='none')\n            loss_node = mapper_helper.unsqueeze_helper(graph, loss_node,\n                                                       [axis], outputs[0])\n            # onnx output is log(softmax), but paddle output is softmax\n            graph.make_node('Exp', inputs=[softmax_node], outputs=outputs[1])\n        else:\n            perm = [i for i in range(len(shape))]\n            perm[1] = axis\n            perm[axis] = 1\n            transpose_scores = graph.make_node(\n                'Transpose', inputs=[scores], perm=perm)\n            transpose_labels = graph.make_node(\n                'Transpose', inputs=[labels], perm=perm)\n            squeeze_labels = mapper_helper.squeeze_helper(\n                graph, transpose_labels, [1])\n\n            loss_node, softmax_node = graph.make_node(\n                'SoftmaxCrossEntropyLoss',\n                inputs=[transpose_scores, squeeze_labels],\n                ignore_index=node.attr('ignore_index'),\n                outputs=2,\n                reduction='none')\n            output_node = mapper_helper.unsqueeze_helper(graph, loss_node, [1])\n            graph.make_node(\n                'Transpose', inputs=output_node, outputs=outputs[0], perm=perm)\n            softmax_node = graph.make_node(\n                'Transpose', inputs=softmax_node, perm=perm)\n            # onnx output is log(softmax), but paddle output is softmax\n            graph.make_node('Exp', inputs=[softmax_node], outputs=outputs[1])",
  "class SetValue():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        axes = node.attr('axes')\n        steps, is_steps_tensor = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'steps',\n            'StepsTensor',\n            'StepsTensorList',\n            return_list=True,\n            dtype=dtypes.ONNX.INT64)\n\n        starts, is_starts_tensor = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'starts',\n            'StartsTensor',\n            'StartsTensorList',\n            return_list=True,\n            dtype=dtypes.ONNX.INT64)\n\n        ends, is_ends_tensor = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'ends',\n            'EndsTensor',\n            'EndsTensorList',\n            return_list=True,\n            dtype=dtypes.ONNX.INT64)\n\n        contain_step_bigger_than_1 = False\n        for i in steps:\n            contain_step_bigger_than_1 = i > 1\n            if not isinstance(i, int) or contain_step_bigger_than_1:\n                contain_step_bigger_than_1 = True\n                break\n        condition = is_steps_tensor or is_starts_tensor or is_ends_tensor or contain_step_bigger_than_1\n        assert not condition, \"Currently not supported convert now\"\n\n        input_x_shape = node.input_shape('Input', 0)\n        onnx_paddings = [0] * len(input_x_shape) * 2\n        value_shape = list(copy.copy(node.input_shape('Input', 0)))\n        for i in range(len(axes)):\n            axis = axes[i]\n            if starts[i] < 0:\n                starts[i] = starts[i] + input_x_shape[i]\n            if ends[i] < 0:\n                ends[i] = ends[i] + input_x_shape[i]\n            onnx_paddings[axis] = starts[i]\n            value_shape[axis] = value_shape[axis] - onnx_paddings[axis]\n            onnx_paddings[axis + len(input_x_shape)] = input_x_shape[\n                axis] - ends[i]\n            if onnx_paddings[axis + len(input_x_shape)] < 0:\n                onnx_paddings[axis + len(input_x_shape)] = 0\n            value_shape[axis] = value_shape[axis] - onnx_paddings[axis + len(\n                input_x_shape)]\n        dtype_paddle = node.input_dtype('Input', 0)\n        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype_paddle]\n        value_tensor = None\n        shape = node.attr('shape')\n        if len(shape) > 0:\n            dtypes_list = [\n                'fp32_values', 'fp64_values', 'int32_values', 'int64_values',\n                'bool_values'\n            ]\n            for i in range(len(dtypes_list)):\n                value = node.attr(dtypes_list[i])\n                if value is not None:\n                    break\n            if len(value) == 1:\n                total_nums = 1\n                for i in value_shape:\n                    total_nums *= i\n                value = value * total_nums\n                value_tensor = mapper_helper.constant_helper(\n                    graph, dtype_paddle, value, shape=value_shape)\n            else:\n                value_tensor = mapper_helper.constant_helper(\n                    graph, dtype_paddle, value, shape=shape)\n        else:\n            value_tensor = node.input('ValueTensor', 0)\n        MAX_FLOAT32 = 3.402823466E+38\n        max_node = graph.make_node(\n            'Constant', attrs={'dtype': dtype,\n                               'value': [MAX_FLOAT32]})\n        pads_node = graph.make_node(\n            'Constant',\n            attrs={'dtype': dtypes.ONNX.INT64,\n                   'value': onnx_paddings})\n        value_pad_node = graph.make_node(\n            'Pad', inputs=[value_tensor, pads_node, max_node])\n\n        condition_dtype = graph.make_node(\n            \"Equal\", inputs=[value_pad_node, max_node])\n        condition_node = graph.make_node(\n            'Cast', inputs=[condition_dtype], to=dtypes.ONNX.BOOL)\n        graph.make_node(\n            \"Where\",\n            inputs=[condition_node, node.input('Input', 0), value_pad_node],\n            outputs=node.output('Out'))",
  "class OneHotV2():\n    support_opset_version_range = (9, )\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        allow_out_of_range = node.attr('allow_out_of_range')\n        assert not allow_out_of_range, \"allow_out_of_range can not be true in one_hot_v2.\"\n        in_dtype_paddle = node.input_dtype('X', 0)\n        in_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[in_dtype_paddle]\n        out_dtype = node.output_dtype('Out', 0)\n        out_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[out_dtype]\n        inputs = node.input('X', 0)\n        if in_dtype_paddle == paddle.int32:\n            inputs = graph.make_node(\n                'Cast', inputs=[inputs], to=dtypes.ONNX.INT64)\n            in_dtype = dtypes.ONNX.INT64\n        value_node = graph.make_node('Constant', dtype=out_dtype, value=[0, 1])\n        depth = node.attr('depth')\n        if node.input('depth_tensor', 0) is not None:\n            depth_node = node.input('depth_tensor', 0)\n        else:\n            depth_node = graph.make_node(\n                'Constant', dtype=in_dtype, value=[depth])\n        reshaped_input_node = graph.make_node(\n            'OneHot',\n            inputs=[inputs, depth_node, value_node],\n            outputs=node.output('Out'))",
  "class Concat():\n    support_opset_version_range = (4, 15)\n\n    @classmethod\n    def opset_4(cls, graph, node, **kw):\n        inputs = node.input('X')\n\n        input_dtypes = [node.input_dtype('X', i) for i in range(len(inputs))]\n        inputs = mapper_helper.dtype_alignment(graph, inputs, input_dtypes)\n        node_axis = node.input('AxisTensor')\n        if node_axis is not None and len(node_axis) > 0:\n            axis_node = node.input('AxisTensor')[0]\n            try:\n                axis = mapper_helper.get_value_from_parameters(graph,\n                                                               axis_node)[0]\n            except Exception as e:\n                raise Exception(\n                    \"Currently does not support the axis parameter as input tensor\"\n                    + str(e))\n        else:\n            axis = node.attr('axis')\n        if axis < 0:\n            axis = axis + len(node.input_shape('X', 0))\n\n        node = graph.make_node(\n            'Concat', inputs=inputs, outputs=node.output('Out'), axis=axis)",
  "class Assign():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        inputs = node.input('X')\n        graph.make_node('Identity', inputs=inputs, outputs=node.output('Out'))",
  "class LodReset():\n    support_opset_version_range = (1, )\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Identity', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Eye():\n    support_opset_version_range = (9, )\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        num_rows = node.attr('num_rows')\n        num_columns = node.attr('num_columns')\n        dtype = node.output_dtype('Out', 0)\n        value = [0] * num_rows * num_columns\n        value_tensor = mapper_helper.constant_helper(\n            graph, dtype, value, shape=[num_rows, num_columns])\n        graph.make_node(\n            'EyeLike', inputs=[value_tensor], outputs=node.output('Out'))",
  "class Stack():\n    support_opset_version_range = (4, 15)\n\n    @classmethod\n    def opset_4(cls, graph, node, **kw):\n        inputs = node.input('X')\n        input_dtypes = [node.input_dtype('X', i) for i in range(len(inputs))]\n        inputs = mapper_helper.dtype_alignment(graph, inputs, input_dtypes)\n        axis = node.attr('axis')\n\n        unsqueezed_inputs = list()\n        for ipt in inputs:\n            unsqueezed_ipt = mapper_helper.unsqueeze_helper(graph, ipt, [axis])\n            unsqueezed_inputs.append(unsqueezed_ipt)\n        graph.make_node(\n            'Concat',\n            inputs=unsqueezed_inputs,\n            outputs=node.output('Y'),\n            axis=axis)",
  "class Unstack():\n    support_opset_version_range = (2, 15)\n\n    @classmethod\n    def opset_2(cls, graph, node, **kw):\n        axis = node.attr('axis')\n        ndim = node.block.vars[node.input('X')[0]].ndim\n        axis = axis + ndim if axis < 0 else axis\n        output_y = mapper_helper.split_helper(\n            graph,\n            node.input('X'),\n            axis=axis,\n            split=[1] * len(node.output('Y')),\n            outputs=len(node.output('Y')))\n\n        if isinstance(output_y, six.string_types):\n            output_y = [output_y]\n\n        for i in range(len(output_y)):\n            mapper_helper.squeeze_helper(graph, output_y[i], [axis],\n                                         node.output('Y', i))",
  "class ExpandAsV2():\n    support_opset_version_range = (8, 15)\n\n    @classmethod\n    def opset_8(cls, graph, node, **kw):\n        target_shape = node.attr('target_shape')\n        if node.input('target_tensor', 0) is not None:\n            target_shape = graph.make_node(\n                'Shape', inputs=[node.input('target_tensor', 0)])\n        elif target_shape is not None:\n            target_shape = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': target_shape})\n        else:\n            raise Exception(\n                \"Not find attribute: 'target_shape' or tensor 'target_tensor'\")\n        node = graph.make_node(\n            'Expand',\n            inputs=[node.input('X', 0), target_shape],\n            outputs=node.output('Out'))",
  "class ExpandV2():\n    support_opset_version_range = (8, 15)\n\n    @classmethod\n    def opset_8(cls, graph, node, **kw):\n        expand_shape, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'shape',\n            'Shape',\n            'expand_shapes_tensor',\n            dtype=dtypes.ONNX.INT64)\n\n        input_shape = node.input_shape('X', 0)\n        input_shape_node = graph.make_node('Shape', inputs=node.input('X', 0))\n\n        node_shape = node.attr('shape')\n        node_shape_tensor = node.input('Shape')\n        node_shape_tensor_list = node.input('expand_shapes_tensor')\n        if node_shape_tensor is not None and len(node_shape_tensor) > 0:\n            diff = node.input_shape('Shape', 0)[0] - len(input_shape)\n        elif node_shape_tensor_list is not None and \\\n                len(node_shape_tensor_list) > 0:\n            diff = len(node_shape_tensor_list) - len(input_shape)\n        elif node_shape is not None and len(node_shape) > 0:\n            diff = len(node_shape) - len(input_shape)\n            expand_shape = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=expand_shape)\n\n        if diff > 0:\n            one_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': [1] * diff})\n            input_shape_node = graph.make_node(\n                'Concat', inputs=[one_node, input_shape_node], axis=0)\n\n        if graph.opset_version < 12:\n            input_shape_node = graph.make_node(\n                'Cast', inputs=[input_shape_node], to=dtypes.ONNX.FLOAT)\n            expand_shape = graph.make_node(\n                'Cast', inputs=[expand_shape], to=dtypes.ONNX.FLOAT)\n            shape = graph.make_node(\n                'Max', inputs=[input_shape_node, expand_shape])\n            shape = graph.make_node(\n                'Cast', inputs=[shape], to=dtypes.ONNX.INT64)\n        else:\n            shape = graph.make_node(\n                'Max', inputs=[input_shape_node, expand_shape])\n        node = graph.make_node(\n            'Expand',\n            inputs=[node.input('X', 0), shape],\n            outputs=node.output('Out'))",
  "class Shape():\n    support_opset_version_range = (6, 15)\n\n    @classmethod\n    def opset_6(cls, graph, node, **kw):\n        shape_node = graph.make_node('Shape', inputs=node.input('Input'))\n        graph.make_node(\n            'Cast',\n            inputs=[shape_node],\n            outputs=node.output('Out'),\n            to=dtypes.ONNX.INT32)",
  "class Numel():\n    supports_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        size_node = graph.make_node('Size', inputs=node.input('Input'))\n        mapper_helper.unsqueeze_helper(graph, size_node, [0],\n                                       node.output('Out'))",
  "class Split():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        sections = node.attr('sections')\n        axis = cls.get_axis(graph, node)\n        if isinstance(sections, list) and len(sections) == 1:\n            graph.make_node(\n                'Identity', inputs=node.input('X'), outputs=node.output('Out'))\n        else:\n            if len(sections) > 0:\n                input_shape = node.block.vars[node.input('X')[0]].shape\n                section_index = [\n                    i for i, val in enumerate(sections) if val == -1\n                ]\n                if input_shape[axis] != -1 and len(section_index) == 1:\n                    sections[section_index[0]] = input_shape[axis] - sum(\n                        sections) - 1\n                mapper_helper.split_helper(\n                    graph,\n                    node.input('X'),\n                    axis=axis,\n                    split=sections,\n                    outputs=node.output('Out'))\n            else:\n                graph.make_node(\n                    'Split',\n                    inputs=node.input('X'),\n                    outputs=node.output('Out'),\n                    axis=axis)\n\n    @classmethod\n    def get_axis(cls, graph, node):\n        if len(node.input('AxisTensor')) > 0:\n            axis_node = node.input('AxisTensor')[0]\n            # When axis is tensor, only int32 and int64 are supported\n            if axis_node not in graph.parameters:\n                raise Exception(\n                    \"Currently does not support the axis parameter as input tensor!\"\n                )\n            else:\n                axis = graph.parameters[axis_node].attribute[0].t.int32_data\n                if axis is None or len(axis) < 1:\n                    axis = graph.parameters[axis_node].attribute[\n                        0].t.int64_data[0]\n        else:\n            axis = node.attr('axis')\n        return axis",
  "class Roll():\n    support_opset_version_range = (4, 15)\n\n    @classmethod\n    def roll(cls, graph, node, input_x, dims, shifts):\n        for i in range(len(dims)):\n            if graph.opset_version >= 10 and isinstance(shifts,\n                                                        six.string_types):\n                to_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(\n                    'ShiftsTensor', 0)]\n                const_i = graph.make_node('Constant', dtype=to_dtype, value=i)\n                const_0 = graph.make_node('Constant', dtype=to_dtype, value=0)\n                shift_node = graph.make_node(\n                    'Gather', inputs=[shifts, const_i], axis=0)\n                shift_node = graph.make_node(\n                    \"Sub\", inputs=[const_0, shift_node])\n                shift_node = mapper_helper.unsqueeze_helper(graph, shift_node,\n                                                            [0])\n            elif graph.opset_version < 10 and isinstance(shifts,\n                                                         six.string_types):\n                raise Exception(\n                    \"shifts of roll is Tensor, please try with higher onnx opset_version>=10.\"\n                )\n            else:\n                shift_node = [-shifts[i]]\n                to_dtype = dtypes.ONNX.INT64\n            shapes = []\n            shape = mapper_helper.slice_helper(\n                graph, input_x, [dims[i]], shift_node, [60000], dtype=to_dtype)\n            shapes.append(shape)\n            shape = mapper_helper.slice_helper(\n                graph, input_x, [dims[i]], [0], shift_node, dtype=to_dtype)\n            shapes.append(shape)\n            input_x = graph.make_node('Concat', inputs=shapes, axis=dims[i])\n        return input_x\n\n    @classmethod\n    def flatten(cls, graph, node):\n        dims = len(node.input_shape('X', 0))\n        start_axis = 0\n        end_axis = dims - 1\n        shape_node = graph.make_node('Shape', inputs=node.input('X'))\n        if end_axis < dims - 1:\n            slice1 = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])\n            slice3 = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[end_axis + 1],\n                ends=[dims])\n            slices = [\n                slice1, graph.make_node(\n                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice3\n            ]\n        else:\n            slice1 = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])\n            slices = [\n                slice1, graph.make_node(\n                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64)\n            ]\n        final_shape = graph.make_node('Concat', inputs=slices, axis=0)\n        output = graph.make_node(\n            'Reshape', inputs=[node.input('X')[0], final_shape])\n        return output\n\n    @classmethod\n    def opset_4(cls, graph, node, **kw):\n        dims = node.attr('axis')\n        shifts = node.attr('shifts')\n        input_x = node.input('X')[0]\n        input_shape = node.input_shape('X', 0)\n        shifts_node = node.input('ShiftsTensor')\n        if len(dims) > 0:\n            axes = [\n                axis + len(input_shape) if axis < 0 else axis\n                for i, axis in enumerate(dims)\n            ]\n            if shifts_node is not None and len(shifts_node) > 0:\n                shifts = shifts_node[0]\n            else:\n                for i in range(0, len(axes)):\n                    if input_shape[axes[i]] > 0:\n                        assert -input_shape[axes[i]] <= shifts[i] <= input_shape[axes[i]], \\\n                            \"the value of shifts in axis is less than the value of input_shape in axis.\"\n\n            input_x = cls.roll(graph, node, input_x, axes, shifts)\n            graph.make_node(\n                'Identity', inputs=[input_x], outputs=node.output('Out'))\n        else:\n            if shifts_node is not None and len(shifts_node) > 0:\n                shifts = shifts_node[0]\n            input_x = cls.flatten(graph, node)\n            input_x = cls.roll(graph, node, input_x, [0], shifts)\n            shape_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': list(input_shape)})\n            graph.make_node(\n                'Reshape',\n                inputs=[input_x, shape_node],\n                outputs=node.output('Out'))",
  "class Slice():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def decrease_axis(cls, node):\n        # tensor[i,:] will decrease rank of origin input, example:\n        # paddle.slice() will not decrease rank of origin input\n        # if input shape is [2, 3], input[0, :] will generate output with shape [3], not [1, 3].\n        # paddle.slice(input, 0, 1, 0) will  generate output with shape [1, 3], not [3].\n\n        decrease_axis = node.attr('decrease_axis')\n        if len(decrease_axis) == 0:\n            return None\n        if node.output_shape('Out', 0) == [0]:\n            return decrease_axis\n        if len(node.input_shape('Input', 0)) > len(node.output_shape('Out', 0)):\n            return decrease_axis\n        return None\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        axes = node.attr('axes')\n        strides, strides_is_tensor = mapper_helper.get_node_attr_value(\n            graph, node, 'strides', 'StridesTensor', 'StridesTensorList', True)\n        strides = [1] * len(axes) if strides is None else strides\n        steps = [i for i, val in enumerate(strides) if val == 1]\n        assert len(steps) == len(axes), \\\n            \"Slice in onnx(opset<10) not support attribute 'step', Try converting with opset_version >=10\"\n\n        starts, start_is_tensor = mapper_helper.get_node_attr_value(\n            graph, node, 'starts', 'StartsTensor', 'StartsTensorList', True)\n        ends, end_is_tensor = mapper_helper.get_node_attr_value(\n            graph, node, 'ends', 'EndsTensor', 'EndsTensorList', True)\n\n        assert not strides_is_tensor and not start_is_tensor and not end_is_tensor, \\\n            \"Slice in onnx(opset<10) not support attribute 'steps','starts' or 'ends' which have tensor value, \" \\\n            \"Try converting with opset_version >=10 \"\n\n        decrease_axis = cls.decrease_axis(node)\n        if decrease_axis is None:\n            graph.make_node(\n                \"Slice\",\n                inputs=[node.input('Input')[0]],\n                outputs=node.output('Out'),\n                axes=axes,\n                starts=starts,\n                ends=ends)\n        else:\n            sliced = graph.make_node(\n                \"Slice\",\n                inputs=[node.input('Input')[0]],\n                axes=axes,\n                starts=starts,\n                ends=ends)\n            mapper_helper.squeeze_helper(graph, sliced, decrease_axis,\n                                         node.output('Out'))\n\n    @classmethod\n    def opset_10(cls, graph, node, **kw):\n        axes = node.attr('axes')\n        strides, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'strides',\n            'StridesTensor',\n            'StridesTensorList',\n            dtype=dtypes.ONNX.INT64)\n        strides = [1] * len(axes) if strides is None else strides\n\n        starts, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'starts',\n            'StartsTensor',\n            'StartsTensorList',\n            dtype=dtypes.ONNX.INT64)\n        ends, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'ends',\n            'EndsTensor',\n            'EndsTensorList',\n            dtype=dtypes.ONNX.INT64)\n\n        if isinstance(starts, list):\n            starts_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': starts})\n        else:\n            starts_node = starts\n        if isinstance(ends, list):\n            ends_node = graph.make_node(\n                'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                                   'value': ends})\n        else:\n            ends_node = ends\n\n        if isinstance(strides, list):\n            strides_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': strides})\n        else:\n            strides_node = strides\n\n        steps_node = strides_node\n        axes_node = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                               'value': axes})\n\n        decrease_axis = cls.decrease_axis(node)\n        if decrease_axis is None:\n            sliced = graph.make_node(\n                \"Slice\",\n                inputs=[\n                    node.input('Input')[0], starts_node, ends_node, axes_node,\n                    steps_node\n                ],\n                outputs=node.output('Out'))\n        else:\n            sliced = graph.make_node(\n                \"Slice\",\n                inputs=[\n                    node.input('Input')[0], starts_node, ends_node, axes_node,\n                    steps_node\n                ])\n            mapper_helper.squeeze_helper(graph, sliced, decrease_axis,\n                                         node.output('Out'))",
  "class SequenceExpand():\n    support_opset_version_range = ()\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Identity', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Expand():\n    support_opset_version_range = (6, 15)\n\n    @classmethod\n    def opset_6(cls, graph, node, **kw):\n        expand_times, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'expand_times',\n            'ExpandTimes',\n            'expand_times_tensor',\n            dtype=dtypes.ONNX.INT64)\n\n        if isinstance(expand_times, list):\n            expand_times = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': expand_times})\n\n        graph.make_node(\n            \"Tile\",\n            inputs=[node.input('X', 0), expand_times],\n            outputs=node.output('Out'))",
  "class Tile():\n    support_opset_version_range = (6, 15)\n\n    @classmethod\n    def opset_6(cls, graph, node, **kw):\n        repeat_times, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'repeat_times',\n            'RepeatTimes',\n            'repeat_times_tensor',\n            dtype=dtypes.ONNX.INT64)\n\n        if isinstance(repeat_times, list):\n            repeat_times = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': repeat_times})\n\n        graph.make_node(\n            \"Tile\",\n            inputs=[node.input('X', 0), repeat_times],\n            outputs=node.output('Out'))",
  "class Range():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        start = node.input('Start', 0)\n        end = node.input('End', 0)\n        step = node.input('Step', 0)\n        start_t = mapper_helper.squeeze_helper(graph, start, [0])\n        end_t = mapper_helper.squeeze_helper(graph, end, [0])\n        step_t = mapper_helper.squeeze_helper(graph, step, [0])\n        graph.make_node(\n            \"Range\",\n            inputs=[start_t, end_t, step_t],\n            outputs=node.output('Out'))",
  "class Constant():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def check_int_type(cls, dtype):\n        if dtype in [dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64]:\n            return True\n        return False\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        value = node.attr('value')\n        dtype = node.attr('dtype')\n        value_is_scalar_tensor = False\n        if 'ValueTensor' in node.inputs and len(node.input('ValueTensor')) > 0:\n            rank = len(node.input_shape(\"ValueTensor\", 0))\n            if rank == 1 and node.input_shape(\"ValueTensor\", 0)[0] == 1:\n                value_is_scalar_tensor = True\n                value = node.input(\"ValueTensor\")[0]\n            else:\n                raise Exception(\n                    \"paddle.full with tensor value parameter is not supported yet.\"\n                )\n\n        shape, is_shape_tensor = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'shape',\n            'ShapeTensor',\n            'ShapeTensorList',\n            dtype=dtypes.ONNX.INT64)\n\n        if graph.opset_version >= 9 and (is_shape_tensor or\n                                         value_is_scalar_tensor):\n            if not is_shape_tensor:\n                shape = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=shape)\n            input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n            if not value_is_scalar_tensor and cls.check_int_type(input_dtype):\n                to_dtype = dtypes.ONNX.DOUBLE\n                outputs = None\n            else:\n                to_dtype = input_dtype\n                outputs = node.output('Out')\n\n            if value_is_scalar_tensor:\n                base_value = graph.make_node(\n                    'ConstantOfShape',\n                    inputs=shape,\n                    attrs={'dims': [1],\n                           'dtype': to_dtype,\n                           'value': 0})\n                node2 = graph.make_node(\n                    \"Add\", inputs=[base_value, value], outputs=outputs)\n            else:\n                node2 = graph.make_node(\n                    'ConstantOfShape',\n                    inputs=shape,\n                    outputs=outputs,\n                    attrs={'dims': [1],\n                           'dtype': to_dtype,\n                           'value': value})\n\n            if not value_is_scalar_tensor and cls.check_int_type(input_dtype):\n                graph.make_node(\n                    'Cast',\n                    inputs=node2,\n                    outputs=node.output('Out'),\n                    attrs={'to': input_dtype})\n        else:\n            assert not is_shape_tensor and not value_is_scalar_tensor, \\\n                \"Currently op ['fill_constant'] does not support in onnx(opset<9) when 'shape' or 'fill_value' has \" \\\n                \"tensor, Try converting with opset_version >=9 \"\n\n            value = np.ones(shape) * value\n            value = value.astype(dtypes.DTYPE_PADDLE_NUMPY_MAP[dtype])\n            value = value.flatten().tolist()\n\n            graph.make_node(\n                'Constant',\n                inputs=[],\n                outputs=node.output('Out'),\n                attrs={\n                    'dims': shape,\n                    'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[dtype],\n                    'value': value\n                })",
  "class Embedding():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        ids = node.input('Ids', 0)\n        if node.type == 'lookup_table' and node.input_shape('Ids', 0)[-1] == 1:\n            ids = mapper_helper.squeeze_helper(graph,\n                                               node.input('Ids', 0), [-1])\n        padding_idx = node.attr('padding_idx')\n        input_shape = node.input_shape('W', 0)\n        if padding_idx != -1:\n            key = node.input('W', 0)\n            if -1 in input_shape:\n                assert False, \"opset version < 11 do not support padding_idx !=-1 and weight is tensor with dynamic shape, please set opset version > 11 or use input_spec to set input shape\"\n            else:\n                data = np.ones(shape=input_shape, dtype=np.float32)\n                data[padding_idx] = 0.0\n                dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W', 0)]\n                constant = graph.make_node(\n                    'Constant',\n                    dtype=dtype,\n                    dims=input_shape,\n                    value=data.flatten().tolist())\n                weight_node = graph.make_node(\n                    'Mul', inputs=[node.input('W', 0), constant])\n                graph.make_node(\n                    'Gather',\n                    inputs=[weight_node, ids],\n                    outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Gather',\n                inputs=[node.input('W', 0), ids],\n                outputs=node.output('Out'))\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        ids = node.input('Ids', 0)\n        if node.type == 'lookup_table' and node.input_shape('Ids', 0)[-1] == 1:\n            ids = mapper_helper.squeeze_helper(graph,\n                                               node.input('Ids', 0), [-1])\n\n        padding_idx = node.attr('padding_idx')\n        input_shape = node.input_shape('W', 0)\n        if padding_idx != -1:\n            if -1 in input_shape:\n                replace_shape = list(copy.copy(input_shape))\n                del (replace_shape[0])\n                replace_data = graph.make_node(\n                    'Constant',\n                    dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W',\n                                                                        0)],\n                    dims=replace_shape,\n                    value=[0.0] * np.prod(replace_shape))\n                index = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[padding_idx])\n                Scatter_node = graph.make_node(\n                    'ScatterND',\n                    inputs=[node.input('W', 0), index, replace_data])\n                graph.make_node(\n                    'Gather',\n                    inputs=[Scatter_node, ids],\n                    outputs=node.output('Out'))\n            else:\n                data = np.ones(shape=input_shape, dtype=np.float32)\n                data[padding_idx] = 0.0\n                dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W', 0)]\n                constant = graph.make_node(\n                    'Constant',\n                    dtype=dtype,\n                    dims=input_shape,\n                    value=data.flatten().tolist())\n                weight_node = graph.make_node(\n                    'Mul', inputs=[node.input('W', 0), constant])\n                graph.make_node(\n                    'Gather',\n                    inputs=[weight_node, ids],\n                    outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Gather',\n                inputs=[node.input('W', 0), ids],\n                outputs=node.output('Out'))",
  "class FillConstantBatchSizeLike():\n    support_opset_version_range = (9, 12)\n\n    @classmethod\n    def opset_10(cls, graph, node, **kw):\n        out_shape = node.attr('shape')\n        input_dim_idx = node.attr('input_dim_idx')\n        output_dim_idx = node.attr('output_dim_idx')\n\n        del out_shape[output_dim_idx]\n        out_shape.insert(0, 1)\n\n        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')]\n        if node.attr(\"str_value\") is not None and node.attr(\"str_value\") != \"\":\n            value = eval(node.attr(\"str_value\"))\n        else:\n            value = node.attr('value')\n        input_shape = node.input_shape('Input', 0)\n        constant = graph.make_node(\n            'Constant',\n            dtype=dtype,\n            dims=out_shape,\n            value=[value] * np.prod(out_shape))\n\n        shape = graph.make_node('Shape', inputs=node.input('Input'))\n        start = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[input_dim_idx])\n        end = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[input_dim_idx + 1])\n        batch = graph.make_node('Slice', inputs=[shape, start, end])\n        repeat = batch\n        if len(out_shape) > 1:\n            repeat = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=[1] * (len(out_shape) - 1))\n            repeat = graph.make_node('Concat', inputs=[batch, repeat], axis=-1)\n        if output_dim_idx == 0:\n            graph.make_node(\n                'Tile', inputs=[constant, repeat], outputs=node.output('Out'))\n        else:\n            out = graph.make_node('Tile', inputs=[constant, repeat])\n            perm = list(range(len(out_shape)))\n            del perm[0]\n            perm.insert(output_dim_idx, 0)\n            graph.make_node(\n                'Transpose',\n                inputs=[out],\n                perm=perm,\n                outputs=node.output('Out'))",
  "class FullLike():\n    '''\n    fill_any_like is kernel for paddle op::full_like & ones_like\n    '''\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        shape_node = graph.make_node('Shape', inputs=node.input('X'))\n        value = node.attr('value')\n        dtype = node.attr('dtype')\n        input_dtype = node.input_dtype('X', 0)\n        if dtype is None:\n            dtype = input_dtype\n        np_dtype = dtypes.DTYPE_PADDLE_STR_MAP[dtype]\n        onnx_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n        graph.make_node(\n            'ConstantOfShape',\n            inputs=[shape_node],\n            outputs=node.output('Out'),\n            dims=[1],\n            dtype=onnx_dtype,\n            value=np.array(value).astype(np_dtype).tolist())",
  "class FullZeroLike():\n    '''\n    fill_zeros_like is kernel for paddle op::zeros_like\n    '''\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        shape_node = graph.make_node('Shape', inputs=node.input('X'))\n        value = 0\n        dtype = node.attr('dtype')\n        input_dtype = node.input_dtype('X', 0)\n        if dtype is None:\n            dtype = input_dtype\n        np_dtype = dtypes.DTYPE_PADDLE_STR_MAP[dtype]\n        onnx_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n        graph.make_node(\n            'ConstantOfShape',\n            inputs=[shape_node],\n            outputs=node.output('Out'),\n            dims=[1],\n            dtype=onnx_dtype,\n            value=np.array(value).astype(np_dtype).tolist())",
  "class Gather_nd():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        data = node.input('X', 0)\n        index = node.input('Index', 0)\n        index_dtype = node.input_dtype('Index', 0)\n        index_node = None\n        if index_dtype != paddle.int64:\n            index_node = graph.make_node(\n                'Cast', inputs=[node.input('Index', 0)], to=dtypes.ONNX.INT64)\n        else:\n            index_node = index\n        graph.make_node(\n            'GatherND', inputs=[data, index_node], outputs=node.output('Out'))",
  "class Gather():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        axis = node.attr('axis')\n        if node.input('Axis', 0) != None:\n            axis_node = node.input('Axis', 0)\n            try:\n                axis = mapper_helper.get_value_from_parameters(graph,\n                                                               axis_node)[0]\n            except Exception as e:\n                raise Exception(\n                    \"Currently does not support the axis parameter as input tensor\"\n                    + str(e))\n        if axis is None:\n            axis = 0\n        if len(node.input_shape('Index', 0)) == 1:\n            # gather\n            graph.make_node(\n                'Gather',\n                inputs=[node.input('X', 0), node.input('Index', 0)],\n                outputs=node.output('Out'),\n                attrs={'axis': axis})\n        else:\n            raise Exception(\n                \"please try to convert OP:gather(indices's rank >1) with opset_version >= 11.\"\n            )\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        axis = node.attr('axis')\n        if node.input('Axis', 0) != None:\n            axis_node = node.input('Axis', 0)\n            try:\n                axis = mapper_helper.get_value_from_parameters(graph,\n                                                               axis_node)[0]\n            except Exception as e:\n                raise Exception(\n                    \"Currently does not support the axis parameter as input tensor\"\n                    + str(e))\n        if axis is None:\n            axis = 0\n        if len(node.input_shape('Index', 0)) == 1:\n            # gather\n            graph.make_node(\n                'Gather',\n                inputs=[node.input('X', 0), node.input('Index', 0)],\n                outputs=node.output('Out'),\n                attrs={'axis': axis})\n        else:\n            # gather_nd\n            index_dtype = node.input_dtype('Index', 0)\n            if index_dtype != paddle.int64:\n                index_node = graph.make_node(\n                    'Cast',\n                    inputs=[node.input('Index', 0)],\n                    to=dtypes.ONNX.INT64)\n                graph.make_node(\n                    'GatherND',\n                    inputs=[node.input('X', 0), index_node],\n                    outputs=node.output('Out'))\n            else:\n                graph.make_node(\n                    'GatherND',\n                    inputs=[node.input('X', 0), node.input('Index', 0)],\n                    outputs=node.output('Out'))",
  "class Squeeze():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        shape = node.input_shape('X', 0)\n        ret = [i for i, val in enumerate(shape) if val > 1]\n        if len(ret) == len(shape):\n            graph.make_node(\n                'Identity', inputs=node.input('X'), outputs=node.output('Out'))\n        else:\n            axes = cls.compute_axes(graph, node)\n            if len(axes) > 0:\n                axes.sort()\n                mapper_helper.squeeze_helper(graph,\n                                             node.input('X', 0), axes,\n                                             node.output('Out'))\n            else:\n                graph.make_node(\n                    'Squeeze',\n                    inputs=[node.input('X', 0)],\n                    outputs=node.output('Out'))\n\n    @classmethod\n    def compute_axes(cls, graph, node):\n        shape = node.input_shape('X', 0)\n        axes = node.attr('axes')\n        if len(axes) > 0:\n            axes = [\n                axis + len(shape) if axis < 0 else axis\n                for i, axis in enumerate(axes)\n            ]\n        return axes",
  "class Assign():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        if len(node.input_names) > 0:\n            graph.make_node(\n                'Identity', inputs=node.input('X'), outputs=node.output('Out'))\n        else:\n            parameters = {}\n            value = np.array(node.attr('fp32_values'))\n            if value is None or value.size < 1:\n                value = np.array(node.attr('int32_values'))\n            if value is None or value.size < 1:\n                value = np.array(node.attr('int64_values'))\n            parameter = {\n                'data': value,\n                'dtype': node.output_dtype(\"Out\", 0),\n                'shape': node.attr('shape')\n            }\n            parameters[node.output('Out', 0)] = parameter\n            graph.build_parameters(parameters)",
  "class Transpose():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Transpose',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            perm=node.attr('axis'))",
  "class Flatten():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)]\n        if input_dtype in [dtypes.ONNX.INT32, dtypes.ONNX.INT64\n                           ] and graph.opset_version < 9:\n            raise Exception(\n                \"int32 or int64 not supported in onnx <9, please try with higher onnx opset_version>=9.\"\n            )\n\n        graph.make_node(\n            'Flatten',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            axis=node.attr('axis'))",
  "class FlattenContiguousRange():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        dims = len(node.input_shape('X', 0))\n        start_axis = node.attr('start_axis')\n        end_axis = node.attr('stop_axis')\n        shape_node = graph.make_node('Shape', inputs=node.input('X'))\n        if start_axis < 0:\n            start_axis += dims\n        if end_axis < 0:\n            end_axis += dims\n        if start_axis == 0 and end_axis == dims - 1:\n            final_shape = graph.make_node(\n                'Constant', value=[-1], dtype=dtypes.ONNX.INT64)\n        elif start_axis == 0:\n            slice_end = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[end_axis + 1],\n                ends=[dims])\n            slices = [\n                graph.make_node(\n                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice_end\n            ]\n            final_shape = graph.make_node('Concat', inputs=slices, axis=0)\n        elif end_axis == dims - 1:\n            slice_start = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])\n            slices = [\n                slice_start, graph.make_node(\n                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64)\n            ]\n            final_shape = graph.make_node('Concat', inputs=slices, axis=0)\n        else:\n            slice_start = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])\n            slice_end = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[end_axis + 1],\n                ends=[dims])\n            slices = [\n                slice_start, graph.make_node(\n                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice_end\n            ]\n            final_shape = graph.make_node('Concat', inputs=slices, axis=0)\n        graph.make_node(\n            'Reshape',\n            inputs=[node.input('X')[0], final_shape],\n            outputs=node.output('Out'))",
  "class Reshape():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        shape_name = 'ShapeTensor'\n        if shape_name not in node.inputs or len(node.input(shape_name)) == 0:\n            shape_name = 'Shape'\n        if shape_name not in node.inputs or len(node.input(shape_name)) == 0:\n            if node.attr('shape') is None or len(node.attr('shape')) == 0:\n                raise Exception(\"shape tensor and shape attrubite all unkown.\")\n        if len(node.input(shape_name)) > 1:\n            dims = []\n            for i in range(len(node.input(shape_name))):\n                dim = node.input(shape_name)[i]\n                dim = graph.make_node(\n                    'Cast', inputs=[dim], to=dtypes.ONNX.INT64)\n                dims.append(dim)\n            shape = graph.make_node('Concat', inputs=dims, axis=-1)\n            graph.make_node(\n                'Reshape',\n                inputs=[node.input('X')[0], shape],\n                outputs=node.output('Out'))\n        elif len(node.input(shape_name)) == 1:\n            cast_shape_node = graph.make_node(\n                'Cast', inputs=node.input(shape_name), to=dtypes.ONNX.INT64)\n            graph.make_node(\n                'Reshape',\n                inputs=[node.input('X')[0], cast_shape_node],\n                outputs=node.output('Out'))\n        elif node.attr('shape') is not None and len(node.attr('shape')) > 0:\n            shape_node = graph.make_node(\n                'Constant',\n                attrs={\n                    'dtype': dtypes.ONNX.INT64,\n                    'value': node.attr('shape')\n                })\n            reshape_node = graph.make_node(\n                'Reshape',\n                inputs=[node.input('X')[0], shape_node],\n                outputs=node.output('Out'))",
  "class Unsqueeze():\n    support_opset_version_range = (1, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        axes = cls.get_axes(graph, node)\n        mapper_helper.unsqueeze_helper(graph,\n                                       node.input('X'), axes,\n                                       node.output('Out'))\n\n    @classmethod\n    def opset_13(cls, graph, node, **kw):\n        axes_node = cls.get_axes(graph, node, return_node=True)\n        graph.make_node(\n            'Unsqueeze',\n            inputs=node.input('X') + [axes_node],\n            outputs=node.output('Out'))\n\n    @classmethod\n    def get_axes(cls, graph, node, return_node=False):\n        axes_node = None\n        ndim = node.block.vars[node.input('X')[0]].ndim\n        if len(node.attr('axes')) > 0:\n            axes = node.attr('axes')\n        else:\n            axes_node = node.input('AxesTensor')[0]\n            if axes_node is not None and graph.opset_version > 12 and return_node:\n                return axes_node\n            try:\n                axes = mapper_helper.get_value_from_parameters(graph, axes_node)\n            except Exception as e:\n                raise Exception(\n                    \"Currently does not support the axes parameter as input tensor in onnx(opset<13), \"\n                    \"Try converting with opset_version >=13 \" + str(e))\n        # axes is list of non-negative integers\n        axes = [\n            axis + ndim + i + 1 if axis < 0 else axis\n            for i, axis in enumerate(axes)\n        ]\n\n        axes_copy = axes.copy()\n        assert sorted(\n            axes) == axes_copy, \"axes must be arranged in the following order\"\n        assert len(set(axes)) == len(axes), \"axes have duplicate axis\"\n\n        if return_node:\n            if axes_node is None:\n                axes_node = graph.make_node(\n                    'Constant',\n                    attrs={'dtype': dtypes.ONNX.INT64,\n                           'value': axes})\n            return axes_node\n        return axes",
  "class Reciprocal():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Reciprocal', inputs=node.input('X'), outputs=node.output('Out'))",
  "class Cast():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Cast',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('out_dtype')])",
  "class Linspace():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        start = node.input('Start', 0)\n        stop = node.input('Stop', 0)\n        num = node.input('Num', 0)\n        dtype = node.attr('dtype')\n\n        start = graph.make_node('Cast', inputs=[start], to=dtypes.ONNX.FLOAT)\n        stop = graph.make_node('Cast', inputs=[stop], to=dtypes.ONNX.FLOAT)\n\n        sub_a_node = graph.make_node('Sub', inputs=[stop, start])\n\n        one_node = graph.make_node(\n            'Constant',\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Num', 0)],\n            value=[1])\n\n        sub_b_node = graph.make_node('Sub', inputs=[num, one_node])\n\n        sub_b_float_node = graph.make_node(\n            'Cast', inputs=[sub_b_node], to=dtypes.ONNX.FLOAT)\n\n        step = graph.make_node('Div', inputs=[sub_a_node, sub_b_float_node])\n\n        range_tensor = graph.make_node(\n            'Cast', inputs=[num], to=dtypes.ONNX.INT64)\n\n        one_like_node = graph.make_node(\n            'ConstantOfShape',\n            inputs=[range_tensor],\n            dtype=dtypes.ONNX.FLOAT,\n            value=[1])\n\n        none_zero_node = graph.make_node('NonZero', inputs=[one_like_node])\n\n        trans_none_zero_node = graph.make_node(\n            'Transpose', inputs=[none_zero_node], perm=[1, 0])\n\n        trans_squeeze = mapper_helper.squeeze_helper(graph,\n                                                     trans_none_zero_node, [1])\n\n        trans_squeeze = graph.make_node(\n            'Cast', inputs=[trans_squeeze], to=dtypes.ONNX.FLOAT)\n\n        mul_node = graph.make_node('Mul', inputs=[trans_squeeze, step])\n\n        add_node = graph.make_node('Add', inputs=[mul_node, start])\n        graph.make_node(\n            'Cast',\n            inputs=[add_node],\n            outputs=node.output('Out'),\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Start', 0)])",
  "class Clip():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        min_value = node.attr('min')\n        max_value = node.attr('max')\n        if node.input('Max', 0) is None or len(node.input('Max')) == 0:\n            max_ = max_value\n        else:\n            max_ = node.input('Max', 0)\n        if node.input('Min', 0) is None or len(node.input('Min')) == 0:\n            min_ = min_value\n        else:\n            min_ = node.input('Min', 0)\n        mapper_helper.clip_helper(graph, node,\n                                  node.input('X', 0), max_, min_,\n                                  node.output('Out', 0))",
  "class Pad():\n    support_opset_version_range = (1, 12)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        if node.attr('mode') == 'replicate':\n            mode = 'edge'\n        elif node.attr('mode') == 'circular':\n            raise Exception(\"The padding mode = circular is not supported, \" \\\n                            \"Please try the other three ways\")\n        else:\n            mode = node.attr('mode')\n        pads = cls.convert_padding(node, **kw)\n        if pads is None:\n            key = node.input('Paddings', 0)\n            padding = None\n            if key in graph.parameters.keys():\n                paddings = graph.parameters[key].attribute[0].t.int32_data\n                if node.attr('data_format') == 'NCHW':\n                    pads = [\n                        0, 0, paddings[0], paddings[2], 0, 0, paddings[1],\n                        paddings[3]\n                    ]\n                elif node.attr('data_format') == 'NHWC':\n                    pads = [\n                        0, paddings[0], paddings[2], 0, 0, paddings[1],\n                        paddings[3], 0\n                    ]\n                elif node.attr('data_format') == 'NCDHW':\n                    pads = [\n                        0, 0, paddings[4], paddings[2], paddings[0], 0, 0,\n                        paddings[5], paddings[3], paddings[1]\n                    ]\n                elif node.attr('data_format') == 'NDHWC':\n                    pads = [\n                        0, paddings[4], paddings[2], paddings[0], 0, 0,\n                        paddings[5], paddings[3], paddings[1], 0\n                    ]\n            else:\n                raise Exception(\"In Pad op, padding can not be tensor\" \\\n                                \"Please set opset version >= 11\")\n\n        value = None\n        if node.attr('pad_value') is not None:\n            value = node.attr('pad_value')\n        elif node.attr('value') is not None:\n            value = node.attr('value')\n        graph.make_node(\n            'Pad',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            mode=mode,\n            value=value,\n            pads=pads)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        pads = cls.convert_padding(node, **kw)\n        if node.attr('mode') == 'replicate':\n            mode = 'edge'\n        elif node.attr('mode') == 'circular':\n            raise Exception(\"The padding mode = circular is not supported, \" \\\n                            \"Please try the other three ways\")\n        else:\n            mode = node.attr('mode')\n        pads_node = None\n        if isinstance(pads, list):\n            pads_node = graph.make_node(\n                'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                                   'value': pads})\n        else:\n            key = node.input('Paddings', 0)\n            padding = None\n            if key in graph.parameters.keys():\n                paddings = graph.parameters[key].attribute[0].t.int32_data\n                onnx_paddings = None\n                if node.attr('data_format') == 'NCHW':\n                    onnx_paddings = [\n                        0, 0, paddings[0], paddings[2], 0, 0, paddings[1],\n                        paddings[3]\n                    ]\n                elif node.attr('data_format') == 'NHWC':\n                    onnx_paddings = [\n                        0, paddings[0], paddings[2], 0, 0, paddings[1],\n                        paddings[3], 0\n                    ]\n                elif node.attr('data_format') == 'NCDHW':\n                    onnx_paddings = [\n                        0, 0, paddings[4], paddings[2], paddings[0], 0, 0,\n                        paddings[5], paddings[3], paddings[1]\n                    ]\n                elif node.attr('data_format') == 'NDHWC':\n                    onnx_paddings = [\n                        0, paddings[4], paddings[2], paddings[0], 0, 0,\n                        paddings[5], paddings[3], paddings[1], 0\n                    ]\n\n                pads_node = graph.make_node(\n                    'Constant',\n                    attrs={'dtype': dtypes.ONNX.INT64,\n                           'value': onnx_paddings})\n            else:\n                padding_node = node.input('Paddings', 0)\n                casted_padding_node = graph.make_node(\n                    'Cast', inputs=[padding_node], to=dtypes.ONNX.FLOAT)\n                zero_node = None\n                if node.attr('data_format') == 'NCHW' or node.attr(\n                        'data_format') == 'NHWC':\n                    zero_node = graph.make_node(\n                        'Constant', dtype=dtypes.ONNX.FLOAT, value=[0] * 8)\n                else:\n                    zero_node = graph.make_node(\n                        'Constant', dtype=dtypes.ONNX.FLOAT, value=[0] * 10)\n                index = None\n                if node.attr('data_format') == 'NCHW':\n                    index = graph.make_node(\n                        'Constant', dtype=dtypes.ONNX.INT32,\n                        value=[2, 6, 3, 7])\n                elif node.attr('data_format') == 'NHWC':\n                    index = graph.make_node(\n                        'Constant', dtype=dtypes.ONNX.INT32,\n                        value=[1, 5, 2, 6])\n                elif node.attr('data_format') == 'NCDHW':\n                    index = graph.make_node(\n                        'Constant',\n                        dtype=dtypes.ONNX.INT32,\n                        value=[4, 9, 3, 8, 2, 7])\n                elif node.attr('data_format') == 'NDHWC':\n                    index = graph.make_node(\n                        'Constant',\n                        dtype=dtypes.ONNX.INT32,\n                        value=[3, 8, 2, 7, 1, 6])\n\n                float_paddle_node = graph.make_node(\n                    'ScatterElements',\n                    inputs=[zero_node, index, casted_padding_node])\n                paddle_node = graph.make_node(\n                    'Cast', inputs=[float_paddle_node], to=dtypes.ONNX.INT64)\n                pads_node = paddle_node\n\n        value = None\n        if node.attr('pad_value') is not None:\n            value = node.attr('pad_value')\n        elif node.attr('value') is not None:\n            value = node.attr('value')\n        value_node = graph.make_node(\n            'Constant',\n            attrs={\n                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                'value': value\n            })\n\n        graph.make_node(\n            'Pad',\n            inputs=node.input('X') + [pads_node, value_node],\n            outputs=node.output('Out'),\n            mode=mode)\n\n    @classmethod\n    def convert_padding(cls, node, **kw):\n        x_shape = node.input_shape('X', 0)\n        paddings = node.attr('paddings')\n        if paddings == []:\n            return None\n        onnx_paddings = None\n        if node.attr('data_format') == 'NCHW':\n            onnx_paddings = [\n                0, 0, paddings[0], paddings[2], 0, 0, paddings[1], paddings[3]\n            ]\n        elif node.attr('data_format') == 'NHWC':\n            onnx_paddings = [\n                0, paddings[0], paddings[2], 0, 0, paddings[1], paddings[3], 0\n            ]\n        elif node.attr('data_format') == 'NCDHW':\n            onnx_paddings = [\n                0, 0, paddings[4], paddings[2], paddings[0], 0, 0, paddings[5],\n                paddings[3], paddings[1]\n            ]\n        elif node.attr('data_format') == 'NDHWC':\n            onnx_paddings = [\n                0, paddings[4], paddings[2], paddings[0], 0, 0, paddings[5],\n                paddings[3], paddings[1], 0\n            ]\n        return onnx_paddings",
  "class GaussianRandom():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        shape_input_list = node.input('ShapeTensorList')\n        shape_input = None\n        if len(shape_input_list) == 0:\n            shape_input = node.input('ShapeTensor')\n        else:\n            shape_input = graph.make_node(\n                \"Concat\", inputs=node.input('ShapeTensorList'), axis=0)\n        if shape_input is None or len(shape_input) == 0:\n            graph.make_node(\n                'RandomNormal',\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],\n                outputs=node.output('Out'),\n                shape=node.attr('shape'),\n                seed=float(node.attr('seed')),\n                mean=node.attr('mean'),\n                scale=node.attr('std'))\n        else:\n            cast_input_shape = graph.make_node(\n                'Cast', inputs=shape_input, to=dtypes.ONNX.INT64)\n            zero_like_node = graph.make_node(\n                'ConstantOfShape',\n                inputs=cast_input_shape,\n                dims=[1],\n                dtype=dtypes.ONNX.FLOAT,\n                value=[0])\n            graph.make_node(\n                'RandomNormalLike',\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],\n                outputs=node.output('Out'),\n                inputs=zero_like_node,\n                seed=float(node.attr('seed')),\n                mean=node.attr('mean'),\n                scale=node.attr('std'))",
  "class UniformRandom():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'RandomUniformLike',\n            inputs=node.input('Input'),\n            outputs=node.output('Out'),\n            high=node.attr('max'),\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],\n            low=node.attr('min'),\n            seed=float(node.attr('seed')), )",
  "class UniformRandom():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        shape_input_list = node.input('ShapeTensorList')\n        shape_input = None\n        if len(shape_input_list) == 0:\n            shape_input = node.input('ShapeTensor')\n        else:\n            shape_input = graph.make_node(\n                \"Concat\", inputs=node.input('ShapeTensorList'), axis=0)\n        if shape_input is None or len(shape_input) == 0:\n            graph.make_node(\n                'RandomUniform',\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],\n                outputs=node.output('Out'),\n                shape=node.attr('shape'),\n                seed=float(node.attr('seed')),\n                low=node.attr('min'),\n                high=node.attr('max'))\n        else:\n            cast_input_shape = graph.make_node(\n                'Cast', inputs=shape_input, to=dtypes.ONNX.INT64)\n            zero_like_node = graph.make_node(\n                'ConstantOfShape',\n                inputs=cast_input_shape,\n                dtype=dtypes.ONNX.FLOAT,\n                value=[0])\n            graph.make_node(\n                'RandomUniformLike',\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],\n                outputs=node.output('Out'),\n                inputs=zero_like_node,\n                seed=float(node.attr('seed')),\n                low=node.attr('min'),\n                high=node.attr('max'))",
  "class Resize():\n    support_opset_version_range = (9, 15)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        inputs = [node.input('X')[0]]\n        resize_type = kw['mapper_dict'][node.type]\n        cls.waringInfo(graph, node, resize_type)\n        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:\n            output_node = cls.compute_outsize_node(\n                graph, node, return_scale=True)\n        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:\n            output_node = cls.compute_scale_node(graph, node)\n        else:\n            output_node = cls.compute_attrs_node(graph, node, return_scale=True)\n\n        inputs = inputs + output_node\n        op = kw['opset_op_dict'][graph.opset_version]\n        graph.make_node(\n            op, inputs=inputs, outputs=node.output('Out'), mode=resize_type)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        inputs = [node.input('X')[0]]\n        resize_type = kw['mapper_dict'][node.type]\n        cls.waringInfo(graph, node, resize_type)\n        if node.attr('align_corners'):\n            coordinate_transformation_mode = 'align_corners'\n        elif node.attr('align_mode') == 1 and resize_type is not 'cubic':\n            coordinate_transformation_mode = 'asymmetric'\n        elif resize_type == 'nearest':\n            coordinate_transformation_mode = 'asymmetric'\n        else:\n            coordinate_transformation_mode = 'half_pixel'\n        roi_node = graph.make_node(\n            'Constant',\n            attrs={\n                'dtype': dtypes.ONNX.FLOAT,\n                'value': [1, 1, 1, 1, 1, 1, 1, 1]\n            })\n\n        inputs.append(roi_node)\n        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:\n            output_node = cls.compute_outsize_node(graph, node)\n        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:\n            output_node = cls.compute_scale_node(graph, node)\n        else:\n            output_node = cls.compute_attrs_node(graph, node)\n        inputs = inputs + output_node\n        attrs = {\n            'mode': resize_type,\n            'coordinate_transformation_mode': coordinate_transformation_mode\n        }\n        if resize_type == 'nearest' and coordinate_transformation_mode == 'asymmetric':\n            attrs['nearest_mode'] = 'floor'\n        graph.make_node(\n            'Resize', inputs=inputs, outputs=node.output('Out'), attrs=attrs)\n\n    @classmethod\n    def compute_outsize_node(cls, graph, node, return_scale=False):\n        dtype = dtypes.ONNX.INT64\n        if return_scale:\n            dtype = dtypes.ONNX.FLOAT\n        input_shape_node = graph.make_node('Shape', inputs=node.input('X'))\n        if dtype != dtypes.ONNX.INT64:\n            input_shape_node = graph.make_node(\n                'Cast', inputs=[input_shape_node], to=dtype)\n        shape_pre_node = mapper_helper.slice_helper(\n            graph, input_shape_node, axes=[], starts=[0], ends=[2])\n\n        out_size = [node.attr('out_d'), node.attr('out_h'), node.attr('out_w')]\n        out_size = [val for val in out_size if val > 0]\n        use_tensor = False\n        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:\n            use_tensor = True\n        if len(out_size) > 0 and not use_tensor:\n            out_size_node = graph.make_node(\n                'Constant', attrs={'dtype': dtype,\n                                   'value': out_size})\n        else:\n            out_size_node, _ = mapper_helper.get_node_attr_value(\n                graph, node, None, 'OutSize', 'SizeTensor', dtype=dtype)\n        out_size_node = graph.make_node(\n            'Concat', inputs=[shape_pre_node, out_size_node], axis=0)\n\n        if return_scale:\n            scale_node = graph.make_node(\n                'Div', inputs=[out_size_node, input_shape_node])\n            return [scale_node]\n\n        scale_empty_node = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.FLOAT,\n                               'value': []})\n        return [scale_empty_node, out_size_node]\n\n    @classmethod\n    def compute_scale_node(cls, graph, node):\n        cast_scale = graph.make_node(\n            'Cast', inputs=node.input('Scale'), to=dtypes.ONNX.FLOAT)\n        inputs_cocat = []\n        const_node = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.FLOAT,\n                               'value': [1, 1]})\n        inputs_cocat.append(const_node)\n        scale = node.attr('scale')\n        if isinstance(scale, (float, int)):\n            cast_scale = [cast_scale] * (len(node.input_shape('X', 0)) - 2)\n            inputs_cocat = inputs_cocat + cast_scale\n        else:\n            inputs_cocat = inputs_cocat + [cast_scale]\n        scale_node = graph.make_node('Concat', inputs=inputs_cocat, axis=0)\n        return [scale_node]\n\n    @classmethod\n    def compute_attrs_node(cls, graph, node, return_scale=False):\n        out_size = [node.attr('out_d'), node.attr('out_h'), node.attr('out_w')]\n        scale = node.attr('scale')\n        if isinstance(scale, (float, int)):\n            scale = [scale] * (len(node.input_shape('X', 0)) - 2)\n\n        out_size = [val for val in out_size if val > 0]\n        if len(out_size) > 0:\n            output_node = cls.compute_outsize_node(\n                graph, node, return_scale=return_scale)\n            return output_node\n\n        assert len(scale) > 0, Exception(\"scale size should > 0!\")\n        scale_node = graph.make_node(\n            'Constant',\n            attrs={'dtype': dtypes.ONNX.FLOAT,\n                   'value': [1, 1] + scale})\n        return [scale_node]\n\n    @classmethod\n    def waringInfo(cls, graph, node, resize_type):\n        assert node.attrs['data_layout'] == 'NCHW', \\\n            \"The conv data layout should be 'NCHW' , but received data format \" \\\n            \"is %s.\" % node.attrs['data_format']\n\n        if graph.opset_version < 11:\n            if node.attr('align_corners') or resize_type in [\"cubic\"]:\n                raise Exception(\n                    \"When align_corners is true or resize_type is 'cubic', the case isn't supported in onnx(opset<=10), \"\n                    \"Try converting with opset_version>= 11 \")\n            if node.attr('align_mode') == 0 and resize_type in [\n                    \"bilinear\", \"linear\", \"trilinear\"\n            ]:\n                raise Exception(\n                    \"When align_mode == 0 and resize_type is 'bilinear' or 'linear or 'trilinear', the case isn't \"\n                    \"supported in onnx(opset<=10), Try converting with opset_version>= 11 \"\n                )",
  "class PixelShuffle():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        upscale_factor = node.attr('upscale_factor')\n\n        node = graph.make_node(\n            'DepthToSpace',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            blocksize=upscale_factor,\n            mode='CRD')",
  "class Scatter():\n    support_opset_version_range = (11, 15)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        ids = node.input('Ids', 0)\n        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Ids', 0)]\n        if input_dtype != dtypes.ONNX.INT64:\n            ids = graph.make_node('Cast', inputs=[ids], to=dtypes.ONNX.INT64)\n\n        shape = graph.make_node(\n            'Constant',\n            value=[node.input_shape('Ids', 0)[0], 1],\n            dtype=dtypes.ONNX.INT64)\n        reshape_index = graph.make_node('Reshape', inputs=[ids, shape])\n        if not node.attr('overwrite'):\n            raise Exception(\"overwrite = False not support yet.\")\n        else:\n            graph.make_node(\n                'ScatterND',\n                inputs=[\n                    node.input('X', 0), reshape_index, node.input('Updates', 0)\n                ],\n                outputs=node.output('Out'))",
  "class ScatterndAdd():\n    support_opset_version_range = (11, 12)\n\n    @classmethod\n    def opset_11(cls, graph, node, **kw):\n        shape = graph.make_node('Shape', inputs=node.input('X', 0))\n        zero_like_node = graph.make_node(\n            'ConstantOfShape',\n            inputs=[shape],\n            dims=[1],\n            dtype=dtypes.ONNX.FLOAT,\n            value=[0])\n        add_node = graph.make_node(\n            'ScatterND',\n            inputs=[\n                zero_like_node, node.input('Index', 0), node.input('Updates', 0)\n            ], )\n        graph.make_node(\n            'Add',\n            inputs=[node.input('X', 0), add_node],\n            outputs=node.output('Out'))",
  "class Meshgrid():\n    support_opset_version_range = (8, 15)\n\n    @classmethod\n    def opset_8(cls, graph, node, **kw):\n        tensors = [t for t in list(node.input('X'))]\n        tensors_shape = [graph.make_node('Shape', inputs=t) for t in tensors]\n        out_shape = graph.make_node('Concat', inputs=tensors_shape, axis=0)\n        out = []\n        for i, t in enumerate(tensors):\n            shape_i = [\n                graph.make_node(\n                    'Constant',\n                    attrs={'dtype': dtypes.ONNX.INT64,\n                           'value': [1]})\n            ] * len(tensors)\n            shape_i[i] = tensors_shape[i]\n            t_reshaped = graph.make_node(\n                'Reshape',\n                inputs=[t, graph.make_node(\n                    'Concat', inputs=shape_i, axis=0)])\n            out.append(\n                graph.make_node(\n                    'Expand',\n                    inputs=[t_reshaped, out_shape],\n                    outputs=node.output('Out')[i]))",
  "class Flip():\n    support_opset_version_range = (7, 15)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        inputs = node.input('X')\n        x_dtype = node.input_dtype('X', 0)\n        if x_dtype == paddle.bool or x_dtype == paddle.float64:\n            inputs = [\n                graph.make_node(\n                    \"Cast\", inputs=inputs, to=dtypes.ONNX.FLOAT)\n            ]\n        axes = node.attr(\"axis\")\n        if not isinstance(axes, list):\n            axes = [axes]\n        input_shape = node.input_shape('X', 0)\n\n        for i, axis in enumerate(axes):\n            if axis < 0:\n                axes[i] += len(input_shape)\n            assert input_shape[\n                axis] > 0, \"The dimension in axis of input must be fixed for flip operator, but now the input shape({}) in axis({}) is unknow.\".format(\n                    input_shape, axis)\n\n        temp_input = inputs[0]\n        for i, axis in enumerate(axes):\n            if input_shape[axis] == 1:\n                if i != len(axes) - 1:\n                    continue\n                else:\n                    if x_dtype == paddle.bool or x_dtype == paddle.float64:\n                        graph.make_node(\n                            \"Cast\",\n                            inputs=[temp_input],\n                            outputs=node.output(\"Out\"),\n                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n                    else:\n                        graph.make_node(\n                            \"Identity\",\n                            inputs=[temp_input],\n                            outputs=node.output(\"Out\"))\n            else:\n                splits = graph.make_node(\n                    \"Split\",\n                    inputs=[temp_input],\n                    outputs=input_shape[axis],\n                    axis=axis,\n                    split=[1] * input_shape[axis])\n                reversed_splits = splits[::-1]\n                if i != len(axes) - 1:\n                    temp_input = graph.make_node(\n                        \"Concat\", inputs=reversed_splits, axis=axis)\n                else:\n                    if x_dtype == paddle.bool or x_dtype == paddle.float64:\n                        out = graph.make_node(\n                            \"Concat\", inputs=reversed_splits, axis=axis)\n                        graph.make_node(\n                            \"Cast\",\n                            inputs=[out],\n                            outputs=node.output(\"Out\"),\n                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n                    else:\n                        graph.make_node(\n                            \"Concat\",\n                            inputs=reversed_splits,\n                            outputs=node.output(\"Out\"),\n                            axis=axis)\n\n    @classmethod\n    def opset_13(cls, graph, node, **kw):\n        inputs = node.input('X')\n        x_dtype = node.input_dtype('X', 0)\n        if x_dtype == paddle.bool or x_dtype == paddle.float64:\n            inputs = [\n                graph.make_node(\n                    \"Cast\", inputs=inputs, to=dtypes.ONNX.FLOAT)\n            ]\n        axes = node.attr(\"axis\")\n        if not isinstance(axes, list):\n            axes = [axes]\n        input_shape = node.input_shape('X', 0)\n\n        for i, axis in enumerate(axes):\n            if axis < 0:\n                axes[i] += len(input_shape)\n            assert input_shape[\n                axis] > 0, \"The dimension in axis of input must be fixed for flip operator, but now the input shape({}) in axis({}) is unknow.\".format(\n                    input_shape, axis)\n\n        temp_input = inputs[0]\n        for i, axis in enumerate(axes):\n            if input_shape[axis] == 1:\n                if i != len(axes) - 1:\n                    continue\n                else:\n                    if x_dtype == paddle.bool or x_dtype == paddle.float64:\n                        graph.make_node(\n                            \"Cast\",\n                            inputs=[temp_input],\n                            outputs=node.output(\"Out\"),\n                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n                    else:\n                        graph.make_node(\n                            \"Identity\",\n                            inputs=[temp_input],\n                            outputs=node.output(\"Out\"))\n            else:\n                split = graph.make_node(\n                    'Constant',\n                    attrs={\n                        'dtype': dtypes.ONNX.INT64,\n                        'value': [1] * input_shape[axis]\n                    })\n                splits = graph.make_node(\n                    \"Split\",\n                    inputs=[temp_input, split],\n                    outputs=input_shape[axis],\n                    axis=axis)\n                reversed_splits = splits[::-1]\n                if i != len(axes) - 1:\n                    temp_input = graph.make_node(\n                        \"Concat\", inputs=reversed_splits, axis=axis)\n                else:\n                    if x_dtype == paddle.bool or x_dtype == paddle.float64:\n                        out = graph.make_node(\n                            \"Concat\", inputs=reversed_splits, axis=axis)\n                        graph.make_node(\n                            \"Cast\",\n                            inputs=[out],\n                            outputs=node.output(\"Out\"),\n                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n                    else:\n                        graph.make_node(\n                            \"Concat\",\n                            inputs=reversed_splits,\n                            outputs=node.output(\"Out\"),\n                            axis=axis)",
  "def opset_11(cls, graph, node, **kw):\n        axes = node.attr('axes')\n        steps, is_steps_tensor = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'steps',\n            'StepsTensor',\n            'StepsTensorList',\n            return_list=True,\n            dtype=dtypes.ONNX.INT64)\n\n        starts, is_starts_tensor = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'starts',\n            'StartsTensor',\n            'StartsTensorList',\n            return_list=True,\n            dtype=dtypes.ONNX.INT64)\n\n        ends, is_ends_tensor = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'ends',\n            'EndsTensor',\n            'EndsTensorList',\n            return_list=True,\n            dtype=dtypes.ONNX.INT64)\n\n        contain_step_bigger_than_1 = False\n        for i in steps:\n            contain_step_bigger_than_1 = i > 1\n            if not isinstance(i, int) or contain_step_bigger_than_1:\n                contain_step_bigger_than_1 = True\n                break\n        condition = is_steps_tensor or is_starts_tensor or is_ends_tensor or contain_step_bigger_than_1\n        assert not condition, \"Currently not supported convert now\"\n\n        input_x_shape = node.input_shape('Input', 0)\n        onnx_paddings = [0] * len(input_x_shape) * 2\n        value_shape = list(copy.copy(node.input_shape('Input', 0)))\n        for i in range(len(axes)):\n            axis = axes[i]\n            if starts[i] < 0:\n                starts[i] = starts[i] + input_x_shape[i]\n            if ends[i] < 0:\n                ends[i] = ends[i] + input_x_shape[i]\n            onnx_paddings[axis] = starts[i]\n            value_shape[axis] = value_shape[axis] - onnx_paddings[axis]\n            onnx_paddings[axis + len(input_x_shape)] = input_x_shape[\n                axis] - ends[i]\n            if onnx_paddings[axis + len(input_x_shape)] < 0:\n                onnx_paddings[axis + len(input_x_shape)] = 0\n            value_shape[axis] = value_shape[axis] - onnx_paddings[axis + len(\n                input_x_shape)]\n        dtype_paddle = node.input_dtype('Input', 0)\n        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype_paddle]\n        value_tensor = None\n        shape = node.attr('shape')\n        if len(shape) > 0:\n            dtypes_list = [\n                'fp32_values', 'fp64_values', 'int32_values', 'int64_values',\n                'bool_values'\n            ]\n            for i in range(len(dtypes_list)):\n                value = node.attr(dtypes_list[i])\n                if value is not None:\n                    break\n            if len(value) == 1:\n                total_nums = 1\n                for i in value_shape:\n                    total_nums *= i\n                value = value * total_nums\n                value_tensor = mapper_helper.constant_helper(\n                    graph, dtype_paddle, value, shape=value_shape)\n            else:\n                value_tensor = mapper_helper.constant_helper(\n                    graph, dtype_paddle, value, shape=shape)\n        else:\n            value_tensor = node.input('ValueTensor', 0)\n        MAX_FLOAT32 = 3.402823466E+38\n        max_node = graph.make_node(\n            'Constant', attrs={'dtype': dtype,\n                               'value': [MAX_FLOAT32]})\n        pads_node = graph.make_node(\n            'Constant',\n            attrs={'dtype': dtypes.ONNX.INT64,\n                   'value': onnx_paddings})\n        value_pad_node = graph.make_node(\n            'Pad', inputs=[value_tensor, pads_node, max_node])\n\n        condition_dtype = graph.make_node(\n            \"Equal\", inputs=[value_pad_node, max_node])\n        condition_node = graph.make_node(\n            'Cast', inputs=[condition_dtype], to=dtypes.ONNX.BOOL)\n        graph.make_node(\n            \"Where\",\n            inputs=[condition_node, node.input('Input', 0), value_pad_node],\n            outputs=node.output('Out'))",
  "def opset_9(cls, graph, node, **kw):\n        allow_out_of_range = node.attr('allow_out_of_range')\n        assert not allow_out_of_range, \"allow_out_of_range can not be true in one_hot_v2.\"\n        in_dtype_paddle = node.input_dtype('X', 0)\n        in_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[in_dtype_paddle]\n        out_dtype = node.output_dtype('Out', 0)\n        out_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[out_dtype]\n        inputs = node.input('X', 0)\n        if in_dtype_paddle == paddle.int32:\n            inputs = graph.make_node(\n                'Cast', inputs=[inputs], to=dtypes.ONNX.INT64)\n            in_dtype = dtypes.ONNX.INT64\n        value_node = graph.make_node('Constant', dtype=out_dtype, value=[0, 1])\n        depth = node.attr('depth')\n        if node.input('depth_tensor', 0) is not None:\n            depth_node = node.input('depth_tensor', 0)\n        else:\n            depth_node = graph.make_node(\n                'Constant', dtype=in_dtype, value=[depth])\n        reshaped_input_node = graph.make_node(\n            'OneHot',\n            inputs=[inputs, depth_node, value_node],\n            outputs=node.output('Out'))",
  "def opset_4(cls, graph, node, **kw):\n        inputs = node.input('X')\n\n        input_dtypes = [node.input_dtype('X', i) for i in range(len(inputs))]\n        inputs = mapper_helper.dtype_alignment(graph, inputs, input_dtypes)\n        node_axis = node.input('AxisTensor')\n        if node_axis is not None and len(node_axis) > 0:\n            axis_node = node.input('AxisTensor')[0]\n            try:\n                axis = mapper_helper.get_value_from_parameters(graph,\n                                                               axis_node)[0]\n            except Exception as e:\n                raise Exception(\n                    \"Currently does not support the axis parameter as input tensor\"\n                    + str(e))\n        else:\n            axis = node.attr('axis')\n        if axis < 0:\n            axis = axis + len(node.input_shape('X', 0))\n\n        node = graph.make_node(\n            'Concat', inputs=inputs, outputs=node.output('Out'), axis=axis)",
  "def opset_1(cls, graph, node, **kw):\n        inputs = node.input('X')\n        graph.make_node('Identity', inputs=inputs, outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Identity', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_9(cls, graph, node, **kw):\n        num_rows = node.attr('num_rows')\n        num_columns = node.attr('num_columns')\n        dtype = node.output_dtype('Out', 0)\n        value = [0] * num_rows * num_columns\n        value_tensor = mapper_helper.constant_helper(\n            graph, dtype, value, shape=[num_rows, num_columns])\n        graph.make_node(\n            'EyeLike', inputs=[value_tensor], outputs=node.output('Out'))",
  "def opset_4(cls, graph, node, **kw):\n        inputs = node.input('X')\n        input_dtypes = [node.input_dtype('X', i) for i in range(len(inputs))]\n        inputs = mapper_helper.dtype_alignment(graph, inputs, input_dtypes)\n        axis = node.attr('axis')\n\n        unsqueezed_inputs = list()\n        for ipt in inputs:\n            unsqueezed_ipt = mapper_helper.unsqueeze_helper(graph, ipt, [axis])\n            unsqueezed_inputs.append(unsqueezed_ipt)\n        graph.make_node(\n            'Concat',\n            inputs=unsqueezed_inputs,\n            outputs=node.output('Y'),\n            axis=axis)",
  "def opset_2(cls, graph, node, **kw):\n        axis = node.attr('axis')\n        ndim = node.block.vars[node.input('X')[0]].ndim\n        axis = axis + ndim if axis < 0 else axis\n        output_y = mapper_helper.split_helper(\n            graph,\n            node.input('X'),\n            axis=axis,\n            split=[1] * len(node.output('Y')),\n            outputs=len(node.output('Y')))\n\n        if isinstance(output_y, six.string_types):\n            output_y = [output_y]\n\n        for i in range(len(output_y)):\n            mapper_helper.squeeze_helper(graph, output_y[i], [axis],\n                                         node.output('Y', i))",
  "def opset_8(cls, graph, node, **kw):\n        target_shape = node.attr('target_shape')\n        if node.input('target_tensor', 0) is not None:\n            target_shape = graph.make_node(\n                'Shape', inputs=[node.input('target_tensor', 0)])\n        elif target_shape is not None:\n            target_shape = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': target_shape})\n        else:\n            raise Exception(\n                \"Not find attribute: 'target_shape' or tensor 'target_tensor'\")\n        node = graph.make_node(\n            'Expand',\n            inputs=[node.input('X', 0), target_shape],\n            outputs=node.output('Out'))",
  "def opset_8(cls, graph, node, **kw):\n        expand_shape, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'shape',\n            'Shape',\n            'expand_shapes_tensor',\n            dtype=dtypes.ONNX.INT64)\n\n        input_shape = node.input_shape('X', 0)\n        input_shape_node = graph.make_node('Shape', inputs=node.input('X', 0))\n\n        node_shape = node.attr('shape')\n        node_shape_tensor = node.input('Shape')\n        node_shape_tensor_list = node.input('expand_shapes_tensor')\n        if node_shape_tensor is not None and len(node_shape_tensor) > 0:\n            diff = node.input_shape('Shape', 0)[0] - len(input_shape)\n        elif node_shape_tensor_list is not None and \\\n                len(node_shape_tensor_list) > 0:\n            diff = len(node_shape_tensor_list) - len(input_shape)\n        elif node_shape is not None and len(node_shape) > 0:\n            diff = len(node_shape) - len(input_shape)\n            expand_shape = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=expand_shape)\n\n        if diff > 0:\n            one_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': [1] * diff})\n            input_shape_node = graph.make_node(\n                'Concat', inputs=[one_node, input_shape_node], axis=0)\n\n        if graph.opset_version < 12:\n            input_shape_node = graph.make_node(\n                'Cast', inputs=[input_shape_node], to=dtypes.ONNX.FLOAT)\n            expand_shape = graph.make_node(\n                'Cast', inputs=[expand_shape], to=dtypes.ONNX.FLOAT)\n            shape = graph.make_node(\n                'Max', inputs=[input_shape_node, expand_shape])\n            shape = graph.make_node(\n                'Cast', inputs=[shape], to=dtypes.ONNX.INT64)\n        else:\n            shape = graph.make_node(\n                'Max', inputs=[input_shape_node, expand_shape])\n        node = graph.make_node(\n            'Expand',\n            inputs=[node.input('X', 0), shape],\n            outputs=node.output('Out'))",
  "def opset_6(cls, graph, node, **kw):\n        shape_node = graph.make_node('Shape', inputs=node.input('Input'))\n        graph.make_node(\n            'Cast',\n            inputs=[shape_node],\n            outputs=node.output('Out'),\n            to=dtypes.ONNX.INT32)",
  "def opset_1(cls, graph, node, **kw):\n        size_node = graph.make_node('Size', inputs=node.input('Input'))\n        mapper_helper.unsqueeze_helper(graph, size_node, [0],\n                                       node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        sections = node.attr('sections')\n        axis = cls.get_axis(graph, node)\n        if isinstance(sections, list) and len(sections) == 1:\n            graph.make_node(\n                'Identity', inputs=node.input('X'), outputs=node.output('Out'))\n        else:\n            if len(sections) > 0:\n                input_shape = node.block.vars[node.input('X')[0]].shape\n                section_index = [\n                    i for i, val in enumerate(sections) if val == -1\n                ]\n                if input_shape[axis] != -1 and len(section_index) == 1:\n                    sections[section_index[0]] = input_shape[axis] - sum(\n                        sections) - 1\n                mapper_helper.split_helper(\n                    graph,\n                    node.input('X'),\n                    axis=axis,\n                    split=sections,\n                    outputs=node.output('Out'))\n            else:\n                graph.make_node(\n                    'Split',\n                    inputs=node.input('X'),\n                    outputs=node.output('Out'),\n                    axis=axis)",
  "def get_axis(cls, graph, node):\n        if len(node.input('AxisTensor')) > 0:\n            axis_node = node.input('AxisTensor')[0]\n            # When axis is tensor, only int32 and int64 are supported\n            if axis_node not in graph.parameters:\n                raise Exception(\n                    \"Currently does not support the axis parameter as input tensor!\"\n                )\n            else:\n                axis = graph.parameters[axis_node].attribute[0].t.int32_data\n                if axis is None or len(axis) < 1:\n                    axis = graph.parameters[axis_node].attribute[\n                        0].t.int64_data[0]\n        else:\n            axis = node.attr('axis')\n        return axis",
  "def roll(cls, graph, node, input_x, dims, shifts):\n        for i in range(len(dims)):\n            if graph.opset_version >= 10 and isinstance(shifts,\n                                                        six.string_types):\n                to_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype(\n                    'ShiftsTensor', 0)]\n                const_i = graph.make_node('Constant', dtype=to_dtype, value=i)\n                const_0 = graph.make_node('Constant', dtype=to_dtype, value=0)\n                shift_node = graph.make_node(\n                    'Gather', inputs=[shifts, const_i], axis=0)\n                shift_node = graph.make_node(\n                    \"Sub\", inputs=[const_0, shift_node])\n                shift_node = mapper_helper.unsqueeze_helper(graph, shift_node,\n                                                            [0])\n            elif graph.opset_version < 10 and isinstance(shifts,\n                                                         six.string_types):\n                raise Exception(\n                    \"shifts of roll is Tensor, please try with higher onnx opset_version>=10.\"\n                )\n            else:\n                shift_node = [-shifts[i]]\n                to_dtype = dtypes.ONNX.INT64\n            shapes = []\n            shape = mapper_helper.slice_helper(\n                graph, input_x, [dims[i]], shift_node, [60000], dtype=to_dtype)\n            shapes.append(shape)\n            shape = mapper_helper.slice_helper(\n                graph, input_x, [dims[i]], [0], shift_node, dtype=to_dtype)\n            shapes.append(shape)\n            input_x = graph.make_node('Concat', inputs=shapes, axis=dims[i])\n        return input_x",
  "def flatten(cls, graph, node):\n        dims = len(node.input_shape('X', 0))\n        start_axis = 0\n        end_axis = dims - 1\n        shape_node = graph.make_node('Shape', inputs=node.input('X'))\n        if end_axis < dims - 1:\n            slice1 = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])\n            slice3 = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[end_axis + 1],\n                ends=[dims])\n            slices = [\n                slice1, graph.make_node(\n                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice3\n            ]\n        else:\n            slice1 = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])\n            slices = [\n                slice1, graph.make_node(\n                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64)\n            ]\n        final_shape = graph.make_node('Concat', inputs=slices, axis=0)\n        output = graph.make_node(\n            'Reshape', inputs=[node.input('X')[0], final_shape])\n        return output",
  "def opset_4(cls, graph, node, **kw):\n        dims = node.attr('axis')\n        shifts = node.attr('shifts')\n        input_x = node.input('X')[0]\n        input_shape = node.input_shape('X', 0)\n        shifts_node = node.input('ShiftsTensor')\n        if len(dims) > 0:\n            axes = [\n                axis + len(input_shape) if axis < 0 else axis\n                for i, axis in enumerate(dims)\n            ]\n            if shifts_node is not None and len(shifts_node) > 0:\n                shifts = shifts_node[0]\n            else:\n                for i in range(0, len(axes)):\n                    if input_shape[axes[i]] > 0:\n                        assert -input_shape[axes[i]] <= shifts[i] <= input_shape[axes[i]], \\\n                            \"the value of shifts in axis is less than the value of input_shape in axis.\"\n\n            input_x = cls.roll(graph, node, input_x, axes, shifts)\n            graph.make_node(\n                'Identity', inputs=[input_x], outputs=node.output('Out'))\n        else:\n            if shifts_node is not None and len(shifts_node) > 0:\n                shifts = shifts_node[0]\n            input_x = cls.flatten(graph, node)\n            input_x = cls.roll(graph, node, input_x, [0], shifts)\n            shape_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': list(input_shape)})\n            graph.make_node(\n                'Reshape',\n                inputs=[input_x, shape_node],\n                outputs=node.output('Out'))",
  "def decrease_axis(cls, node):\n        # tensor[i,:] will decrease rank of origin input, example:\n        # paddle.slice() will not decrease rank of origin input\n        # if input shape is [2, 3], input[0, :] will generate output with shape [3], not [1, 3].\n        # paddle.slice(input, 0, 1, 0) will  generate output with shape [1, 3], not [3].\n\n        decrease_axis = node.attr('decrease_axis')\n        if len(decrease_axis) == 0:\n            return None\n        if node.output_shape('Out', 0) == [0]:\n            return decrease_axis\n        if len(node.input_shape('Input', 0)) > len(node.output_shape('Out', 0)):\n            return decrease_axis\n        return None",
  "def opset_1(cls, graph, node, **kw):\n        axes = node.attr('axes')\n        strides, strides_is_tensor = mapper_helper.get_node_attr_value(\n            graph, node, 'strides', 'StridesTensor', 'StridesTensorList', True)\n        strides = [1] * len(axes) if strides is None else strides\n        steps = [i for i, val in enumerate(strides) if val == 1]\n        assert len(steps) == len(axes), \\\n            \"Slice in onnx(opset<10) not support attribute 'step', Try converting with opset_version >=10\"\n\n        starts, start_is_tensor = mapper_helper.get_node_attr_value(\n            graph, node, 'starts', 'StartsTensor', 'StartsTensorList', True)\n        ends, end_is_tensor = mapper_helper.get_node_attr_value(\n            graph, node, 'ends', 'EndsTensor', 'EndsTensorList', True)\n\n        assert not strides_is_tensor and not start_is_tensor and not end_is_tensor, \\\n            \"Slice in onnx(opset<10) not support attribute 'steps','starts' or 'ends' which have tensor value, \" \\\n            \"Try converting with opset_version >=10 \"\n\n        decrease_axis = cls.decrease_axis(node)\n        if decrease_axis is None:\n            graph.make_node(\n                \"Slice\",\n                inputs=[node.input('Input')[0]],\n                outputs=node.output('Out'),\n                axes=axes,\n                starts=starts,\n                ends=ends)\n        else:\n            sliced = graph.make_node(\n                \"Slice\",\n                inputs=[node.input('Input')[0]],\n                axes=axes,\n                starts=starts,\n                ends=ends)\n            mapper_helper.squeeze_helper(graph, sliced, decrease_axis,\n                                         node.output('Out'))",
  "def opset_10(cls, graph, node, **kw):\n        axes = node.attr('axes')\n        strides, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'strides',\n            'StridesTensor',\n            'StridesTensorList',\n            dtype=dtypes.ONNX.INT64)\n        strides = [1] * len(axes) if strides is None else strides\n\n        starts, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'starts',\n            'StartsTensor',\n            'StartsTensorList',\n            dtype=dtypes.ONNX.INT64)\n        ends, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'ends',\n            'EndsTensor',\n            'EndsTensorList',\n            dtype=dtypes.ONNX.INT64)\n\n        if isinstance(starts, list):\n            starts_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': starts})\n        else:\n            starts_node = starts\n        if isinstance(ends, list):\n            ends_node = graph.make_node(\n                'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                                   'value': ends})\n        else:\n            ends_node = ends\n\n        if isinstance(strides, list):\n            strides_node = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': strides})\n        else:\n            strides_node = strides\n\n        steps_node = strides_node\n        axes_node = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                               'value': axes})\n\n        decrease_axis = cls.decrease_axis(node)\n        if decrease_axis is None:\n            sliced = graph.make_node(\n                \"Slice\",\n                inputs=[\n                    node.input('Input')[0], starts_node, ends_node, axes_node,\n                    steps_node\n                ],\n                outputs=node.output('Out'))\n        else:\n            sliced = graph.make_node(\n                \"Slice\",\n                inputs=[\n                    node.input('Input')[0], starts_node, ends_node, axes_node,\n                    steps_node\n                ])\n            mapper_helper.squeeze_helper(graph, sliced, decrease_axis,\n                                         node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Identity', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_6(cls, graph, node, **kw):\n        expand_times, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'expand_times',\n            'ExpandTimes',\n            'expand_times_tensor',\n            dtype=dtypes.ONNX.INT64)\n\n        if isinstance(expand_times, list):\n            expand_times = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': expand_times})\n\n        graph.make_node(\n            \"Tile\",\n            inputs=[node.input('X', 0), expand_times],\n            outputs=node.output('Out'))",
  "def opset_6(cls, graph, node, **kw):\n        repeat_times, _ = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'repeat_times',\n            'RepeatTimes',\n            'repeat_times_tensor',\n            dtype=dtypes.ONNX.INT64)\n\n        if isinstance(repeat_times, list):\n            repeat_times = graph.make_node(\n                'Constant',\n                attrs={'dtype': dtypes.ONNX.INT64,\n                       'value': repeat_times})\n\n        graph.make_node(\n            \"Tile\",\n            inputs=[node.input('X', 0), repeat_times],\n            outputs=node.output('Out'))",
  "def opset_11(cls, graph, node, **kw):\n        start = node.input('Start', 0)\n        end = node.input('End', 0)\n        step = node.input('Step', 0)\n        start_t = mapper_helper.squeeze_helper(graph, start, [0])\n        end_t = mapper_helper.squeeze_helper(graph, end, [0])\n        step_t = mapper_helper.squeeze_helper(graph, step, [0])\n        graph.make_node(\n            \"Range\",\n            inputs=[start_t, end_t, step_t],\n            outputs=node.output('Out'))",
  "def check_int_type(cls, dtype):\n        if dtype in [dtypes.ONNX.INT16, dtypes.ONNX.INT32, dtypes.ONNX.INT64]:\n            return True\n        return False",
  "def opset_1(cls, graph, node, **kw):\n        value = node.attr('value')\n        dtype = node.attr('dtype')\n        value_is_scalar_tensor = False\n        if 'ValueTensor' in node.inputs and len(node.input('ValueTensor')) > 0:\n            rank = len(node.input_shape(\"ValueTensor\", 0))\n            if rank == 1 and node.input_shape(\"ValueTensor\", 0)[0] == 1:\n                value_is_scalar_tensor = True\n                value = node.input(\"ValueTensor\")[0]\n            else:\n                raise Exception(\n                    \"paddle.full with tensor value parameter is not supported yet.\"\n                )\n\n        shape, is_shape_tensor = mapper_helper.get_node_attr_value(\n            graph,\n            node,\n            'shape',\n            'ShapeTensor',\n            'ShapeTensorList',\n            dtype=dtypes.ONNX.INT64)\n\n        if graph.opset_version >= 9 and (is_shape_tensor or\n                                         value_is_scalar_tensor):\n            if not is_shape_tensor:\n                shape = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=shape)\n            input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n            if not value_is_scalar_tensor and cls.check_int_type(input_dtype):\n                to_dtype = dtypes.ONNX.DOUBLE\n                outputs = None\n            else:\n                to_dtype = input_dtype\n                outputs = node.output('Out')\n\n            if value_is_scalar_tensor:\n                base_value = graph.make_node(\n                    'ConstantOfShape',\n                    inputs=shape,\n                    attrs={'dims': [1],\n                           'dtype': to_dtype,\n                           'value': 0})\n                node2 = graph.make_node(\n                    \"Add\", inputs=[base_value, value], outputs=outputs)\n            else:\n                node2 = graph.make_node(\n                    'ConstantOfShape',\n                    inputs=shape,\n                    outputs=outputs,\n                    attrs={'dims': [1],\n                           'dtype': to_dtype,\n                           'value': value})\n\n            if not value_is_scalar_tensor and cls.check_int_type(input_dtype):\n                graph.make_node(\n                    'Cast',\n                    inputs=node2,\n                    outputs=node.output('Out'),\n                    attrs={'to': input_dtype})\n        else:\n            assert not is_shape_tensor and not value_is_scalar_tensor, \\\n                \"Currently op ['fill_constant'] does not support in onnx(opset<9) when 'shape' or 'fill_value' has \" \\\n                \"tensor, Try converting with opset_version >=9 \"\n\n            value = np.ones(shape) * value\n            value = value.astype(dtypes.DTYPE_PADDLE_NUMPY_MAP[dtype])\n            value = value.flatten().tolist()\n\n            graph.make_node(\n                'Constant',\n                inputs=[],\n                outputs=node.output('Out'),\n                attrs={\n                    'dims': shape,\n                    'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[dtype],\n                    'value': value\n                })",
  "def opset_1(cls, graph, node, **kw):\n        ids = node.input('Ids', 0)\n        if node.type == 'lookup_table' and node.input_shape('Ids', 0)[-1] == 1:\n            ids = mapper_helper.squeeze_helper(graph,\n                                               node.input('Ids', 0), [-1])\n        padding_idx = node.attr('padding_idx')\n        input_shape = node.input_shape('W', 0)\n        if padding_idx != -1:\n            key = node.input('W', 0)\n            if -1 in input_shape:\n                assert False, \"opset version < 11 do not support padding_idx !=-1 and weight is tensor with dynamic shape, please set opset version > 11 or use input_spec to set input shape\"\n            else:\n                data = np.ones(shape=input_shape, dtype=np.float32)\n                data[padding_idx] = 0.0\n                dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W', 0)]\n                constant = graph.make_node(\n                    'Constant',\n                    dtype=dtype,\n                    dims=input_shape,\n                    value=data.flatten().tolist())\n                weight_node = graph.make_node(\n                    'Mul', inputs=[node.input('W', 0), constant])\n                graph.make_node(\n                    'Gather',\n                    inputs=[weight_node, ids],\n                    outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Gather',\n                inputs=[node.input('W', 0), ids],\n                outputs=node.output('Out'))",
  "def opset_11(cls, graph, node, **kw):\n        ids = node.input('Ids', 0)\n        if node.type == 'lookup_table' and node.input_shape('Ids', 0)[-1] == 1:\n            ids = mapper_helper.squeeze_helper(graph,\n                                               node.input('Ids', 0), [-1])\n\n        padding_idx = node.attr('padding_idx')\n        input_shape = node.input_shape('W', 0)\n        if padding_idx != -1:\n            if -1 in input_shape:\n                replace_shape = list(copy.copy(input_shape))\n                del (replace_shape[0])\n                replace_data = graph.make_node(\n                    'Constant',\n                    dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W',\n                                                                        0)],\n                    dims=replace_shape,\n                    value=[0.0] * np.prod(replace_shape))\n                index = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[padding_idx])\n                Scatter_node = graph.make_node(\n                    'ScatterND',\n                    inputs=[node.input('W', 0), index, replace_data])\n                graph.make_node(\n                    'Gather',\n                    inputs=[Scatter_node, ids],\n                    outputs=node.output('Out'))\n            else:\n                data = np.ones(shape=input_shape, dtype=np.float32)\n                data[padding_idx] = 0.0\n                dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('W', 0)]\n                constant = graph.make_node(\n                    'Constant',\n                    dtype=dtype,\n                    dims=input_shape,\n                    value=data.flatten().tolist())\n                weight_node = graph.make_node(\n                    'Mul', inputs=[node.input('W', 0), constant])\n                graph.make_node(\n                    'Gather',\n                    inputs=[weight_node, ids],\n                    outputs=node.output('Out'))\n        else:\n            graph.make_node(\n                'Gather',\n                inputs=[node.input('W', 0), ids],\n                outputs=node.output('Out'))",
  "def opset_10(cls, graph, node, **kw):\n        out_shape = node.attr('shape')\n        input_dim_idx = node.attr('input_dim_idx')\n        output_dim_idx = node.attr('output_dim_idx')\n\n        del out_shape[output_dim_idx]\n        out_shape.insert(0, 1)\n\n        dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')]\n        if node.attr(\"str_value\") is not None and node.attr(\"str_value\") != \"\":\n            value = eval(node.attr(\"str_value\"))\n        else:\n            value = node.attr('value')\n        input_shape = node.input_shape('Input', 0)\n        constant = graph.make_node(\n            'Constant',\n            dtype=dtype,\n            dims=out_shape,\n            value=[value] * np.prod(out_shape))\n\n        shape = graph.make_node('Shape', inputs=node.input('Input'))\n        start = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[input_dim_idx])\n        end = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[input_dim_idx + 1])\n        batch = graph.make_node('Slice', inputs=[shape, start, end])\n        repeat = batch\n        if len(out_shape) > 1:\n            repeat = graph.make_node(\n                'Constant',\n                dtype=dtypes.ONNX.INT64,\n                value=[1] * (len(out_shape) - 1))\n            repeat = graph.make_node('Concat', inputs=[batch, repeat], axis=-1)\n        if output_dim_idx == 0:\n            graph.make_node(\n                'Tile', inputs=[constant, repeat], outputs=node.output('Out'))\n        else:\n            out = graph.make_node('Tile', inputs=[constant, repeat])\n            perm = list(range(len(out_shape)))\n            del perm[0]\n            perm.insert(output_dim_idx, 0)\n            graph.make_node(\n                'Transpose',\n                inputs=[out],\n                perm=perm,\n                outputs=node.output('Out'))",
  "def opset_9(cls, graph, node, **kw):\n        shape_node = graph.make_node('Shape', inputs=node.input('X'))\n        value = node.attr('value')\n        dtype = node.attr('dtype')\n        input_dtype = node.input_dtype('X', 0)\n        if dtype is None:\n            dtype = input_dtype\n        np_dtype = dtypes.DTYPE_PADDLE_STR_MAP[dtype]\n        onnx_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n        graph.make_node(\n            'ConstantOfShape',\n            inputs=[shape_node],\n            outputs=node.output('Out'),\n            dims=[1],\n            dtype=onnx_dtype,\n            value=np.array(value).astype(np_dtype).tolist())",
  "def opset_9(cls, graph, node, **kw):\n        shape_node = graph.make_node('Shape', inputs=node.input('X'))\n        value = 0\n        dtype = node.attr('dtype')\n        input_dtype = node.input_dtype('X', 0)\n        if dtype is None:\n            dtype = input_dtype\n        np_dtype = dtypes.DTYPE_PADDLE_STR_MAP[dtype]\n        onnx_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[dtype]\n        graph.make_node(\n            'ConstantOfShape',\n            inputs=[shape_node],\n            outputs=node.output('Out'),\n            dims=[1],\n            dtype=onnx_dtype,\n            value=np.array(value).astype(np_dtype).tolist())",
  "def opset_11(cls, graph, node, **kw):\n        data = node.input('X', 0)\n        index = node.input('Index', 0)\n        index_dtype = node.input_dtype('Index', 0)\n        index_node = None\n        if index_dtype != paddle.int64:\n            index_node = graph.make_node(\n                'Cast', inputs=[node.input('Index', 0)], to=dtypes.ONNX.INT64)\n        else:\n            index_node = index\n        graph.make_node(\n            'GatherND', inputs=[data, index_node], outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        axis = node.attr('axis')\n        if node.input('Axis', 0) != None:\n            axis_node = node.input('Axis', 0)\n            try:\n                axis = mapper_helper.get_value_from_parameters(graph,\n                                                               axis_node)[0]\n            except Exception as e:\n                raise Exception(\n                    \"Currently does not support the axis parameter as input tensor\"\n                    + str(e))\n        if axis is None:\n            axis = 0\n        if len(node.input_shape('Index', 0)) == 1:\n            # gather\n            graph.make_node(\n                'Gather',\n                inputs=[node.input('X', 0), node.input('Index', 0)],\n                outputs=node.output('Out'),\n                attrs={'axis': axis})\n        else:\n            raise Exception(\n                \"please try to convert OP:gather(indices's rank >1) with opset_version >= 11.\"\n            )",
  "def opset_11(cls, graph, node, **kw):\n        axis = node.attr('axis')\n        if node.input('Axis', 0) != None:\n            axis_node = node.input('Axis', 0)\n            try:\n                axis = mapper_helper.get_value_from_parameters(graph,\n                                                               axis_node)[0]\n            except Exception as e:\n                raise Exception(\n                    \"Currently does not support the axis parameter as input tensor\"\n                    + str(e))\n        if axis is None:\n            axis = 0\n        if len(node.input_shape('Index', 0)) == 1:\n            # gather\n            graph.make_node(\n                'Gather',\n                inputs=[node.input('X', 0), node.input('Index', 0)],\n                outputs=node.output('Out'),\n                attrs={'axis': axis})\n        else:\n            # gather_nd\n            index_dtype = node.input_dtype('Index', 0)\n            if index_dtype != paddle.int64:\n                index_node = graph.make_node(\n                    'Cast',\n                    inputs=[node.input('Index', 0)],\n                    to=dtypes.ONNX.INT64)\n                graph.make_node(\n                    'GatherND',\n                    inputs=[node.input('X', 0), index_node],\n                    outputs=node.output('Out'))\n            else:\n                graph.make_node(\n                    'GatherND',\n                    inputs=[node.input('X', 0), node.input('Index', 0)],\n                    outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        shape = node.input_shape('X', 0)\n        ret = [i for i, val in enumerate(shape) if val > 1]\n        if len(ret) == len(shape):\n            graph.make_node(\n                'Identity', inputs=node.input('X'), outputs=node.output('Out'))\n        else:\n            axes = cls.compute_axes(graph, node)\n            if len(axes) > 0:\n                axes.sort()\n                mapper_helper.squeeze_helper(graph,\n                                             node.input('X', 0), axes,\n                                             node.output('Out'))\n            else:\n                graph.make_node(\n                    'Squeeze',\n                    inputs=[node.input('X', 0)],\n                    outputs=node.output('Out'))",
  "def compute_axes(cls, graph, node):\n        shape = node.input_shape('X', 0)\n        axes = node.attr('axes')\n        if len(axes) > 0:\n            axes = [\n                axis + len(shape) if axis < 0 else axis\n                for i, axis in enumerate(axes)\n            ]\n        return axes",
  "def opset_1(cls, graph, node, **kw):\n        if len(node.input_names) > 0:\n            graph.make_node(\n                'Identity', inputs=node.input('X'), outputs=node.output('Out'))\n        else:\n            parameters = {}\n            value = np.array(node.attr('fp32_values'))\n            if value is None or value.size < 1:\n                value = np.array(node.attr('int32_values'))\n            if value is None or value.size < 1:\n                value = np.array(node.attr('int64_values'))\n            parameter = {\n                'data': value,\n                'dtype': node.output_dtype(\"Out\", 0),\n                'shape': node.attr('shape')\n            }\n            parameters[node.output('Out', 0)] = parameter\n            graph.build_parameters(parameters)",
  "def opset_7(cls, graph, node, **kw):\n        graph.make_node(\n            'Transpose',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            perm=node.attr('axis'))",
  "def opset_1(cls, graph, node, **kw):\n        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)]\n        if input_dtype in [dtypes.ONNX.INT32, dtypes.ONNX.INT64\n                           ] and graph.opset_version < 9:\n            raise Exception(\n                \"int32 or int64 not supported in onnx <9, please try with higher onnx opset_version>=9.\"\n            )\n\n        graph.make_node(\n            'Flatten',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            axis=node.attr('axis'))",
  "def opset_7(cls, graph, node, **kw):\n        dims = len(node.input_shape('X', 0))\n        start_axis = node.attr('start_axis')\n        end_axis = node.attr('stop_axis')\n        shape_node = graph.make_node('Shape', inputs=node.input('X'))\n        if start_axis < 0:\n            start_axis += dims\n        if end_axis < 0:\n            end_axis += dims\n        if start_axis == 0 and end_axis == dims - 1:\n            final_shape = graph.make_node(\n                'Constant', value=[-1], dtype=dtypes.ONNX.INT64)\n        elif start_axis == 0:\n            slice_end = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[end_axis + 1],\n                ends=[dims])\n            slices = [\n                graph.make_node(\n                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice_end\n            ]\n            final_shape = graph.make_node('Concat', inputs=slices, axis=0)\n        elif end_axis == dims - 1:\n            slice_start = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])\n            slices = [\n                slice_start, graph.make_node(\n                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64)\n            ]\n            final_shape = graph.make_node('Concat', inputs=slices, axis=0)\n        else:\n            slice_start = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[0], ends=[start_axis])\n            slice_end = mapper_helper.slice_helper(\n                graph, shape_node, axes=[0], starts=[end_axis + 1],\n                ends=[dims])\n            slices = [\n                slice_start, graph.make_node(\n                    'Constant', value=[-1], dtype=dtypes.ONNX.INT64), slice_end\n            ]\n            final_shape = graph.make_node('Concat', inputs=slices, axis=0)\n        graph.make_node(\n            'Reshape',\n            inputs=[node.input('X')[0], final_shape],\n            outputs=node.output('Out'))",
  "def opset_7(cls, graph, node, **kw):\n        shape_name = 'ShapeTensor'\n        if shape_name not in node.inputs or len(node.input(shape_name)) == 0:\n            shape_name = 'Shape'\n        if shape_name not in node.inputs or len(node.input(shape_name)) == 0:\n            if node.attr('shape') is None or len(node.attr('shape')) == 0:\n                raise Exception(\"shape tensor and shape attrubite all unkown.\")\n        if len(node.input(shape_name)) > 1:\n            dims = []\n            for i in range(len(node.input(shape_name))):\n                dim = node.input(shape_name)[i]\n                dim = graph.make_node(\n                    'Cast', inputs=[dim], to=dtypes.ONNX.INT64)\n                dims.append(dim)\n            shape = graph.make_node('Concat', inputs=dims, axis=-1)\n            graph.make_node(\n                'Reshape',\n                inputs=[node.input('X')[0], shape],\n                outputs=node.output('Out'))\n        elif len(node.input(shape_name)) == 1:\n            cast_shape_node = graph.make_node(\n                'Cast', inputs=node.input(shape_name), to=dtypes.ONNX.INT64)\n            graph.make_node(\n                'Reshape',\n                inputs=[node.input('X')[0], cast_shape_node],\n                outputs=node.output('Out'))\n        elif node.attr('shape') is not None and len(node.attr('shape')) > 0:\n            shape_node = graph.make_node(\n                'Constant',\n                attrs={\n                    'dtype': dtypes.ONNX.INT64,\n                    'value': node.attr('shape')\n                })\n            reshape_node = graph.make_node(\n                'Reshape',\n                inputs=[node.input('X')[0], shape_node],\n                outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        axes = cls.get_axes(graph, node)\n        mapper_helper.unsqueeze_helper(graph,\n                                       node.input('X'), axes,\n                                       node.output('Out'))",
  "def opset_13(cls, graph, node, **kw):\n        axes_node = cls.get_axes(graph, node, return_node=True)\n        graph.make_node(\n            'Unsqueeze',\n            inputs=node.input('X') + [axes_node],\n            outputs=node.output('Out'))",
  "def get_axes(cls, graph, node, return_node=False):\n        axes_node = None\n        ndim = node.block.vars[node.input('X')[0]].ndim\n        if len(node.attr('axes')) > 0:\n            axes = node.attr('axes')\n        else:\n            axes_node = node.input('AxesTensor')[0]\n            if axes_node is not None and graph.opset_version > 12 and return_node:\n                return axes_node\n            try:\n                axes = mapper_helper.get_value_from_parameters(graph, axes_node)\n            except Exception as e:\n                raise Exception(\n                    \"Currently does not support the axes parameter as input tensor in onnx(opset<13), \"\n                    \"Try converting with opset_version >=13 \" + str(e))\n        # axes is list of non-negative integers\n        axes = [\n            axis + ndim + i + 1 if axis < 0 else axis\n            for i, axis in enumerate(axes)\n        ]\n\n        axes_copy = axes.copy()\n        assert sorted(\n            axes) == axes_copy, \"axes must be arranged in the following order\"\n        assert len(set(axes)) == len(axes), \"axes have duplicate axis\"\n\n        if return_node:\n            if axes_node is None:\n                axes_node = graph.make_node(\n                    'Constant',\n                    attrs={'dtype': dtypes.ONNX.INT64,\n                           'value': axes})\n            return axes_node\n        return axes",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Reciprocal', inputs=node.input('X'), outputs=node.output('Out'))",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'Cast',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('out_dtype')])",
  "def opset_9(cls, graph, node, **kw):\n        start = node.input('Start', 0)\n        stop = node.input('Stop', 0)\n        num = node.input('Num', 0)\n        dtype = node.attr('dtype')\n\n        start = graph.make_node('Cast', inputs=[start], to=dtypes.ONNX.FLOAT)\n        stop = graph.make_node('Cast', inputs=[stop], to=dtypes.ONNX.FLOAT)\n\n        sub_a_node = graph.make_node('Sub', inputs=[stop, start])\n\n        one_node = graph.make_node(\n            'Constant',\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Num', 0)],\n            value=[1])\n\n        sub_b_node = graph.make_node('Sub', inputs=[num, one_node])\n\n        sub_b_float_node = graph.make_node(\n            'Cast', inputs=[sub_b_node], to=dtypes.ONNX.FLOAT)\n\n        step = graph.make_node('Div', inputs=[sub_a_node, sub_b_float_node])\n\n        range_tensor = graph.make_node(\n            'Cast', inputs=[num], to=dtypes.ONNX.INT64)\n\n        one_like_node = graph.make_node(\n            'ConstantOfShape',\n            inputs=[range_tensor],\n            dtype=dtypes.ONNX.FLOAT,\n            value=[1])\n\n        none_zero_node = graph.make_node('NonZero', inputs=[one_like_node])\n\n        trans_none_zero_node = graph.make_node(\n            'Transpose', inputs=[none_zero_node], perm=[1, 0])\n\n        trans_squeeze = mapper_helper.squeeze_helper(graph,\n                                                     trans_none_zero_node, [1])\n\n        trans_squeeze = graph.make_node(\n            'Cast', inputs=[trans_squeeze], to=dtypes.ONNX.FLOAT)\n\n        mul_node = graph.make_node('Mul', inputs=[trans_squeeze, step])\n\n        add_node = graph.make_node('Add', inputs=[mul_node, start])\n        graph.make_node(\n            'Cast',\n            inputs=[add_node],\n            outputs=node.output('Out'),\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Start', 0)])",
  "def opset_1(cls, graph, node, **kw):\n        min_value = node.attr('min')\n        max_value = node.attr('max')\n        if node.input('Max', 0) is None or len(node.input('Max')) == 0:\n            max_ = max_value\n        else:\n            max_ = node.input('Max', 0)\n        if node.input('Min', 0) is None or len(node.input('Min')) == 0:\n            min_ = min_value\n        else:\n            min_ = node.input('Min', 0)\n        mapper_helper.clip_helper(graph, node,\n                                  node.input('X', 0), max_, min_,\n                                  node.output('Out', 0))",
  "def opset_1(cls, graph, node, **kw):\n        if node.attr('mode') == 'replicate':\n            mode = 'edge'\n        elif node.attr('mode') == 'circular':\n            raise Exception(\"The padding mode = circular is not supported, \" \\\n                            \"Please try the other three ways\")\n        else:\n            mode = node.attr('mode')\n        pads = cls.convert_padding(node, **kw)\n        if pads is None:\n            key = node.input('Paddings', 0)\n            padding = None\n            if key in graph.parameters.keys():\n                paddings = graph.parameters[key].attribute[0].t.int32_data\n                if node.attr('data_format') == 'NCHW':\n                    pads = [\n                        0, 0, paddings[0], paddings[2], 0, 0, paddings[1],\n                        paddings[3]\n                    ]\n                elif node.attr('data_format') == 'NHWC':\n                    pads = [\n                        0, paddings[0], paddings[2], 0, 0, paddings[1],\n                        paddings[3], 0\n                    ]\n                elif node.attr('data_format') == 'NCDHW':\n                    pads = [\n                        0, 0, paddings[4], paddings[2], paddings[0], 0, 0,\n                        paddings[5], paddings[3], paddings[1]\n                    ]\n                elif node.attr('data_format') == 'NDHWC':\n                    pads = [\n                        0, paddings[4], paddings[2], paddings[0], 0, 0,\n                        paddings[5], paddings[3], paddings[1], 0\n                    ]\n            else:\n                raise Exception(\"In Pad op, padding can not be tensor\" \\\n                                \"Please set opset version >= 11\")\n\n        value = None\n        if node.attr('pad_value') is not None:\n            value = node.attr('pad_value')\n        elif node.attr('value') is not None:\n            value = node.attr('value')\n        graph.make_node(\n            'Pad',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            mode=mode,\n            value=value,\n            pads=pads)",
  "def opset_11(cls, graph, node, **kw):\n        pads = cls.convert_padding(node, **kw)\n        if node.attr('mode') == 'replicate':\n            mode = 'edge'\n        elif node.attr('mode') == 'circular':\n            raise Exception(\"The padding mode = circular is not supported, \" \\\n                            \"Please try the other three ways\")\n        else:\n            mode = node.attr('mode')\n        pads_node = None\n        if isinstance(pads, list):\n            pads_node = graph.make_node(\n                'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                                   'value': pads})\n        else:\n            key = node.input('Paddings', 0)\n            padding = None\n            if key in graph.parameters.keys():\n                paddings = graph.parameters[key].attribute[0].t.int32_data\n                onnx_paddings = None\n                if node.attr('data_format') == 'NCHW':\n                    onnx_paddings = [\n                        0, 0, paddings[0], paddings[2], 0, 0, paddings[1],\n                        paddings[3]\n                    ]\n                elif node.attr('data_format') == 'NHWC':\n                    onnx_paddings = [\n                        0, paddings[0], paddings[2], 0, 0, paddings[1],\n                        paddings[3], 0\n                    ]\n                elif node.attr('data_format') == 'NCDHW':\n                    onnx_paddings = [\n                        0, 0, paddings[4], paddings[2], paddings[0], 0, 0,\n                        paddings[5], paddings[3], paddings[1]\n                    ]\n                elif node.attr('data_format') == 'NDHWC':\n                    onnx_paddings = [\n                        0, paddings[4], paddings[2], paddings[0], 0, 0,\n                        paddings[5], paddings[3], paddings[1], 0\n                    ]\n\n                pads_node = graph.make_node(\n                    'Constant',\n                    attrs={'dtype': dtypes.ONNX.INT64,\n                           'value': onnx_paddings})\n            else:\n                padding_node = node.input('Paddings', 0)\n                casted_padding_node = graph.make_node(\n                    'Cast', inputs=[padding_node], to=dtypes.ONNX.FLOAT)\n                zero_node = None\n                if node.attr('data_format') == 'NCHW' or node.attr(\n                        'data_format') == 'NHWC':\n                    zero_node = graph.make_node(\n                        'Constant', dtype=dtypes.ONNX.FLOAT, value=[0] * 8)\n                else:\n                    zero_node = graph.make_node(\n                        'Constant', dtype=dtypes.ONNX.FLOAT, value=[0] * 10)\n                index = None\n                if node.attr('data_format') == 'NCHW':\n                    index = graph.make_node(\n                        'Constant', dtype=dtypes.ONNX.INT32,\n                        value=[2, 6, 3, 7])\n                elif node.attr('data_format') == 'NHWC':\n                    index = graph.make_node(\n                        'Constant', dtype=dtypes.ONNX.INT32,\n                        value=[1, 5, 2, 6])\n                elif node.attr('data_format') == 'NCDHW':\n                    index = graph.make_node(\n                        'Constant',\n                        dtype=dtypes.ONNX.INT32,\n                        value=[4, 9, 3, 8, 2, 7])\n                elif node.attr('data_format') == 'NDHWC':\n                    index = graph.make_node(\n                        'Constant',\n                        dtype=dtypes.ONNX.INT32,\n                        value=[3, 8, 2, 7, 1, 6])\n\n                float_paddle_node = graph.make_node(\n                    'ScatterElements',\n                    inputs=[zero_node, index, casted_padding_node])\n                paddle_node = graph.make_node(\n                    'Cast', inputs=[float_paddle_node], to=dtypes.ONNX.INT64)\n                pads_node = paddle_node\n\n        value = None\n        if node.attr('pad_value') is not None:\n            value = node.attr('pad_value')\n        elif node.attr('value') is not None:\n            value = node.attr('value')\n        value_node = graph.make_node(\n            'Constant',\n            attrs={\n                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                'value': value\n            })\n\n        graph.make_node(\n            'Pad',\n            inputs=node.input('X') + [pads_node, value_node],\n            outputs=node.output('Out'),\n            mode=mode)",
  "def convert_padding(cls, node, **kw):\n        x_shape = node.input_shape('X', 0)\n        paddings = node.attr('paddings')\n        if paddings == []:\n            return None\n        onnx_paddings = None\n        if node.attr('data_format') == 'NCHW':\n            onnx_paddings = [\n                0, 0, paddings[0], paddings[2], 0, 0, paddings[1], paddings[3]\n            ]\n        elif node.attr('data_format') == 'NHWC':\n            onnx_paddings = [\n                0, paddings[0], paddings[2], 0, 0, paddings[1], paddings[3], 0\n            ]\n        elif node.attr('data_format') == 'NCDHW':\n            onnx_paddings = [\n                0, 0, paddings[4], paddings[2], paddings[0], 0, 0, paddings[5],\n                paddings[3], paddings[1]\n            ]\n        elif node.attr('data_format') == 'NDHWC':\n            onnx_paddings = [\n                0, paddings[4], paddings[2], paddings[0], 0, 0, paddings[5],\n                paddings[3], paddings[1], 0\n            ]\n        return onnx_paddings",
  "def opset_7(cls, graph, node, **kw):\n        shape_input_list = node.input('ShapeTensorList')\n        shape_input = None\n        if len(shape_input_list) == 0:\n            shape_input = node.input('ShapeTensor')\n        else:\n            shape_input = graph.make_node(\n                \"Concat\", inputs=node.input('ShapeTensorList'), axis=0)\n        if shape_input is None or len(shape_input) == 0:\n            graph.make_node(\n                'RandomNormal',\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],\n                outputs=node.output('Out'),\n                shape=node.attr('shape'),\n                seed=float(node.attr('seed')),\n                mean=node.attr('mean'),\n                scale=node.attr('std'))\n        else:\n            cast_input_shape = graph.make_node(\n                'Cast', inputs=shape_input, to=dtypes.ONNX.INT64)\n            zero_like_node = graph.make_node(\n                'ConstantOfShape',\n                inputs=cast_input_shape,\n                dims=[1],\n                dtype=dtypes.ONNX.FLOAT,\n                value=[0])\n            graph.make_node(\n                'RandomNormalLike',\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],\n                outputs=node.output('Out'),\n                inputs=zero_like_node,\n                seed=float(node.attr('seed')),\n                mean=node.attr('mean'),\n                scale=node.attr('std'))",
  "def opset_1(cls, graph, node, **kw):\n        graph.make_node(\n            'RandomUniformLike',\n            inputs=node.input('Input'),\n            outputs=node.output('Out'),\n            high=node.attr('max'),\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],\n            low=node.attr('min'),\n            seed=float(node.attr('seed')), )",
  "def opset_7(cls, graph, node, **kw):\n        shape_input_list = node.input('ShapeTensorList')\n        shape_input = None\n        if len(shape_input_list) == 0:\n            shape_input = node.input('ShapeTensor')\n        else:\n            shape_input = graph.make_node(\n                \"Concat\", inputs=node.input('ShapeTensorList'), axis=0)\n        if shape_input is None or len(shape_input) == 0:\n            graph.make_node(\n                'RandomUniform',\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],\n                outputs=node.output('Out'),\n                shape=node.attr('shape'),\n                seed=float(node.attr('seed')),\n                low=node.attr('min'),\n                high=node.attr('max'))\n        else:\n            cast_input_shape = graph.make_node(\n                'Cast', inputs=shape_input, to=dtypes.ONNX.INT64)\n            zero_like_node = graph.make_node(\n                'ConstantOfShape',\n                inputs=cast_input_shape,\n                dtype=dtypes.ONNX.FLOAT,\n                value=[0])\n            graph.make_node(\n                'RandomUniformLike',\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.attr('dtype')],\n                outputs=node.output('Out'),\n                inputs=zero_like_node,\n                seed=float(node.attr('seed')),\n                low=node.attr('min'),\n                high=node.attr('max'))",
  "def opset_9(cls, graph, node, **kw):\n        inputs = [node.input('X')[0]]\n        resize_type = kw['mapper_dict'][node.type]\n        cls.waringInfo(graph, node, resize_type)\n        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:\n            output_node = cls.compute_outsize_node(\n                graph, node, return_scale=True)\n        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:\n            output_node = cls.compute_scale_node(graph, node)\n        else:\n            output_node = cls.compute_attrs_node(graph, node, return_scale=True)\n\n        inputs = inputs + output_node\n        op = kw['opset_op_dict'][graph.opset_version]\n        graph.make_node(\n            op, inputs=inputs, outputs=node.output('Out'), mode=resize_type)",
  "def opset_11(cls, graph, node, **kw):\n        inputs = [node.input('X')[0]]\n        resize_type = kw['mapper_dict'][node.type]\n        cls.waringInfo(graph, node, resize_type)\n        if node.attr('align_corners'):\n            coordinate_transformation_mode = 'align_corners'\n        elif node.attr('align_mode') == 1 and resize_type is not 'cubic':\n            coordinate_transformation_mode = 'asymmetric'\n        elif resize_type == 'nearest':\n            coordinate_transformation_mode = 'asymmetric'\n        else:\n            coordinate_transformation_mode = 'half_pixel'\n        roi_node = graph.make_node(\n            'Constant',\n            attrs={\n                'dtype': dtypes.ONNX.FLOAT,\n                'value': [1, 1, 1, 1, 1, 1, 1, 1]\n            })\n\n        inputs.append(roi_node)\n        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:\n            output_node = cls.compute_outsize_node(graph, node)\n        elif 'Scale' in node.inputs and len(node.input('Scale')) > 0:\n            output_node = cls.compute_scale_node(graph, node)\n        else:\n            output_node = cls.compute_attrs_node(graph, node)\n        inputs = inputs + output_node\n        attrs = {\n            'mode': resize_type,\n            'coordinate_transformation_mode': coordinate_transformation_mode\n        }\n        if resize_type == 'nearest' and coordinate_transformation_mode == 'asymmetric':\n            attrs['nearest_mode'] = 'floor'\n        graph.make_node(\n            'Resize', inputs=inputs, outputs=node.output('Out'), attrs=attrs)",
  "def compute_outsize_node(cls, graph, node, return_scale=False):\n        dtype = dtypes.ONNX.INT64\n        if return_scale:\n            dtype = dtypes.ONNX.FLOAT\n        input_shape_node = graph.make_node('Shape', inputs=node.input('X'))\n        if dtype != dtypes.ONNX.INT64:\n            input_shape_node = graph.make_node(\n                'Cast', inputs=[input_shape_node], to=dtype)\n        shape_pre_node = mapper_helper.slice_helper(\n            graph, input_shape_node, axes=[], starts=[0], ends=[2])\n\n        out_size = [node.attr('out_d'), node.attr('out_h'), node.attr('out_w')]\n        out_size = [val for val in out_size if val > 0]\n        use_tensor = False\n        if len(node.input('OutSize')) > 0 or len(node.input('SizeTensor')) > 0:\n            use_tensor = True\n        if len(out_size) > 0 and not use_tensor:\n            out_size_node = graph.make_node(\n                'Constant', attrs={'dtype': dtype,\n                                   'value': out_size})\n        else:\n            out_size_node, _ = mapper_helper.get_node_attr_value(\n                graph, node, None, 'OutSize', 'SizeTensor', dtype=dtype)\n        out_size_node = graph.make_node(\n            'Concat', inputs=[shape_pre_node, out_size_node], axis=0)\n\n        if return_scale:\n            scale_node = graph.make_node(\n                'Div', inputs=[out_size_node, input_shape_node])\n            return [scale_node]\n\n        scale_empty_node = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.FLOAT,\n                               'value': []})\n        return [scale_empty_node, out_size_node]",
  "def compute_scale_node(cls, graph, node):\n        cast_scale = graph.make_node(\n            'Cast', inputs=node.input('Scale'), to=dtypes.ONNX.FLOAT)\n        inputs_cocat = []\n        const_node = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.FLOAT,\n                               'value': [1, 1]})\n        inputs_cocat.append(const_node)\n        scale = node.attr('scale')\n        if isinstance(scale, (float, int)):\n            cast_scale = [cast_scale] * (len(node.input_shape('X', 0)) - 2)\n            inputs_cocat = inputs_cocat + cast_scale\n        else:\n            inputs_cocat = inputs_cocat + [cast_scale]\n        scale_node = graph.make_node('Concat', inputs=inputs_cocat, axis=0)\n        return [scale_node]",
  "def compute_attrs_node(cls, graph, node, return_scale=False):\n        out_size = [node.attr('out_d'), node.attr('out_h'), node.attr('out_w')]\n        scale = node.attr('scale')\n        if isinstance(scale, (float, int)):\n            scale = [scale] * (len(node.input_shape('X', 0)) - 2)\n\n        out_size = [val for val in out_size if val > 0]\n        if len(out_size) > 0:\n            output_node = cls.compute_outsize_node(\n                graph, node, return_scale=return_scale)\n            return output_node\n\n        assert len(scale) > 0, Exception(\"scale size should > 0!\")\n        scale_node = graph.make_node(\n            'Constant',\n            attrs={'dtype': dtypes.ONNX.FLOAT,\n                   'value': [1, 1] + scale})\n        return [scale_node]",
  "def waringInfo(cls, graph, node, resize_type):\n        assert node.attrs['data_layout'] == 'NCHW', \\\n            \"The conv data layout should be 'NCHW' , but received data format \" \\\n            \"is %s.\" % node.attrs['data_format']\n\n        if graph.opset_version < 11:\n            if node.attr('align_corners') or resize_type in [\"cubic\"]:\n                raise Exception(\n                    \"When align_corners is true or resize_type is 'cubic', the case isn't supported in onnx(opset<=10), \"\n                    \"Try converting with opset_version>= 11 \")\n            if node.attr('align_mode') == 0 and resize_type in [\n                    \"bilinear\", \"linear\", \"trilinear\"\n            ]:\n                raise Exception(\n                    \"When align_mode == 0 and resize_type is 'bilinear' or 'linear or 'trilinear', the case isn't \"\n                    \"supported in onnx(opset<=10), Try converting with opset_version>= 11 \"\n                )",
  "def opset_11(cls, graph, node, **kw):\n        upscale_factor = node.attr('upscale_factor')\n\n        node = graph.make_node(\n            'DepthToSpace',\n            inputs=node.input('X'),\n            outputs=node.output('Out'),\n            blocksize=upscale_factor,\n            mode='CRD')",
  "def opset_11(cls, graph, node, **kw):\n        ids = node.input('Ids', 0)\n        input_dtype = dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('Ids', 0)]\n        if input_dtype != dtypes.ONNX.INT64:\n            ids = graph.make_node('Cast', inputs=[ids], to=dtypes.ONNX.INT64)\n\n        shape = graph.make_node(\n            'Constant',\n            value=[node.input_shape('Ids', 0)[0], 1],\n            dtype=dtypes.ONNX.INT64)\n        reshape_index = graph.make_node('Reshape', inputs=[ids, shape])\n        if not node.attr('overwrite'):\n            raise Exception(\"overwrite = False not support yet.\")\n        else:\n            graph.make_node(\n                'ScatterND',\n                inputs=[\n                    node.input('X', 0), reshape_index, node.input('Updates', 0)\n                ],\n                outputs=node.output('Out'))",
  "def opset_11(cls, graph, node, **kw):\n        shape = graph.make_node('Shape', inputs=node.input('X', 0))\n        zero_like_node = graph.make_node(\n            'ConstantOfShape',\n            inputs=[shape],\n            dims=[1],\n            dtype=dtypes.ONNX.FLOAT,\n            value=[0])\n        add_node = graph.make_node(\n            'ScatterND',\n            inputs=[\n                zero_like_node, node.input('Index', 0), node.input('Updates', 0)\n            ], )\n        graph.make_node(\n            'Add',\n            inputs=[node.input('X', 0), add_node],\n            outputs=node.output('Out'))",
  "def opset_8(cls, graph, node, **kw):\n        tensors = [t for t in list(node.input('X'))]\n        tensors_shape = [graph.make_node('Shape', inputs=t) for t in tensors]\n        out_shape = graph.make_node('Concat', inputs=tensors_shape, axis=0)\n        out = []\n        for i, t in enumerate(tensors):\n            shape_i = [\n                graph.make_node(\n                    'Constant',\n                    attrs={'dtype': dtypes.ONNX.INT64,\n                           'value': [1]})\n            ] * len(tensors)\n            shape_i[i] = tensors_shape[i]\n            t_reshaped = graph.make_node(\n                'Reshape',\n                inputs=[t, graph.make_node(\n                    'Concat', inputs=shape_i, axis=0)])\n            out.append(\n                graph.make_node(\n                    'Expand',\n                    inputs=[t_reshaped, out_shape],\n                    outputs=node.output('Out')[i]))",
  "def opset_7(cls, graph, node, **kw):\n        inputs = node.input('X')\n        x_dtype = node.input_dtype('X', 0)\n        if x_dtype == paddle.bool or x_dtype == paddle.float64:\n            inputs = [\n                graph.make_node(\n                    \"Cast\", inputs=inputs, to=dtypes.ONNX.FLOAT)\n            ]\n        axes = node.attr(\"axis\")\n        if not isinstance(axes, list):\n            axes = [axes]\n        input_shape = node.input_shape('X', 0)\n\n        for i, axis in enumerate(axes):\n            if axis < 0:\n                axes[i] += len(input_shape)\n            assert input_shape[\n                axis] > 0, \"The dimension in axis of input must be fixed for flip operator, but now the input shape({}) in axis({}) is unknow.\".format(\n                    input_shape, axis)\n\n        temp_input = inputs[0]\n        for i, axis in enumerate(axes):\n            if input_shape[axis] == 1:\n                if i != len(axes) - 1:\n                    continue\n                else:\n                    if x_dtype == paddle.bool or x_dtype == paddle.float64:\n                        graph.make_node(\n                            \"Cast\",\n                            inputs=[temp_input],\n                            outputs=node.output(\"Out\"),\n                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n                    else:\n                        graph.make_node(\n                            \"Identity\",\n                            inputs=[temp_input],\n                            outputs=node.output(\"Out\"))\n            else:\n                splits = graph.make_node(\n                    \"Split\",\n                    inputs=[temp_input],\n                    outputs=input_shape[axis],\n                    axis=axis,\n                    split=[1] * input_shape[axis])\n                reversed_splits = splits[::-1]\n                if i != len(axes) - 1:\n                    temp_input = graph.make_node(\n                        \"Concat\", inputs=reversed_splits, axis=axis)\n                else:\n                    if x_dtype == paddle.bool or x_dtype == paddle.float64:\n                        out = graph.make_node(\n                            \"Concat\", inputs=reversed_splits, axis=axis)\n                        graph.make_node(\n                            \"Cast\",\n                            inputs=[out],\n                            outputs=node.output(\"Out\"),\n                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n                    else:\n                        graph.make_node(\n                            \"Concat\",\n                            inputs=reversed_splits,\n                            outputs=node.output(\"Out\"),\n                            axis=axis)",
  "def opset_13(cls, graph, node, **kw):\n        inputs = node.input('X')\n        x_dtype = node.input_dtype('X', 0)\n        if x_dtype == paddle.bool or x_dtype == paddle.float64:\n            inputs = [\n                graph.make_node(\n                    \"Cast\", inputs=inputs, to=dtypes.ONNX.FLOAT)\n            ]\n        axes = node.attr(\"axis\")\n        if not isinstance(axes, list):\n            axes = [axes]\n        input_shape = node.input_shape('X', 0)\n\n        for i, axis in enumerate(axes):\n            if axis < 0:\n                axes[i] += len(input_shape)\n            assert input_shape[\n                axis] > 0, \"The dimension in axis of input must be fixed for flip operator, but now the input shape({}) in axis({}) is unknow.\".format(\n                    input_shape, axis)\n\n        temp_input = inputs[0]\n        for i, axis in enumerate(axes):\n            if input_shape[axis] == 1:\n                if i != len(axes) - 1:\n                    continue\n                else:\n                    if x_dtype == paddle.bool or x_dtype == paddle.float64:\n                        graph.make_node(\n                            \"Cast\",\n                            inputs=[temp_input],\n                            outputs=node.output(\"Out\"),\n                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n                    else:\n                        graph.make_node(\n                            \"Identity\",\n                            inputs=[temp_input],\n                            outputs=node.output(\"Out\"))\n            else:\n                split = graph.make_node(\n                    'Constant',\n                    attrs={\n                        'dtype': dtypes.ONNX.INT64,\n                        'value': [1] * input_shape[axis]\n                    })\n                splits = graph.make_node(\n                    \"Split\",\n                    inputs=[temp_input, split],\n                    outputs=input_shape[axis],\n                    axis=axis)\n                reversed_splits = splits[::-1]\n                if i != len(axes) - 1:\n                    temp_input = graph.make_node(\n                        \"Concat\", inputs=reversed_splits, axis=axis)\n                else:\n                    if x_dtype == paddle.bool or x_dtype == paddle.float64:\n                        out = graph.make_node(\n                            \"Concat\", inputs=reversed_splits, axis=axis)\n                        graph.make_node(\n                            \"Cast\",\n                            inputs=[out],\n                            outputs=node.output(\"Out\"),\n                            to=dtypes.DTYPE_PADDLE_ONNX_MAP[x_dtype])\n                    else:\n                        graph.make_node(\n                            \"Concat\",\n                            inputs=reversed_splits,\n                            outputs=node.output(\"Out\"),\n                            axis=axis)",
  "def get_max_support_version(versions, opset_version):\n    max_version = -1\n    for vs in sorted(versions):\n        if vs <= opset_version:\n            max_version = vs\n    return max_version",
  "def register_op_mapper(paddle_op, mapper_obj):\n    paddle_op_list = []\n\n    if isinstance(paddle_op, six.string_types):\n        paddle_op_list.append(paddle_op)\n    elif isinstance(paddle_op, list):\n        paddle_op_list = paddle_op\n    else:\n        raise ValueError('paddle_op must be List or string, but got type {}.'.\n                         format(type(paddle_op)))\n\n    if not isinstance(mapper_obj, six.class_types):\n        raise ValueError('mapper_obj must be Class, but got type {}.'.format(\n            type(mapper_obj)))\n\n    valid_register_func = 0\n    for k, v in inspect.getmembers(mapper_obj, inspect.ismethod):\n        if k.startswith(\"opset_\"):\n            version = int(k.replace(\"opset_\", \"\"))\n            if version > 13 or version < 1:\n                raise Exception(\n                    'the specific method of operator mapper must be named opset_[number](1<=number<=13), such as opset_9, but got {}.'.\n                    format(k))\n            valid_register_func += 1\n\n    if valid_register_func == 0:\n        raise Exception(\n            'the specific method of operator mapper must be classmethod, which named opset_[number](1<=number<=13), such as opset_9, but none achieved.'\n        )\n\n    mapper = OpMapper(paddle_op_list)\n    mapper(mapper_obj)",
  "class OpMapper(object):\n    OPSETS = {}\n    REGISTER_CUSTOM_PADDLE_OP = {}\n\n    def __init__(self, paddle_op, **kwargs):\n        if not isinstance(paddle_op, list):\n            paddle_op = [paddle_op]\n        self.paddle_op = paddle_op\n        self.kwargs = kwargs\n\n    def __call__(self, cls):\n        for k, v in inspect.getmembers(cls, inspect.ismethod):\n            if k.startswith(\"opset_\"):\n                version = int(k.replace(\"opset_\", \"\"))\n                for op in self.paddle_op:\n                    if op not in OpMapper.OPSETS:\n                        OpMapper.OPSETS[op] = {}\n                    opset_dict = OpMapper.OPSETS[op]\n                    opset_dict[version] = (v, self.kwargs)\n\n    @staticmethod\n    def mapping(graph, node, operator_export_type=\"ONNX\"):\n        try:\n            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:\n                if operator_export_type in [\"PaddleFallback\"]:\n                    opsets = OpMapper.OPSETS[node.type]\n                    versions = list(opsets.keys())\n                    convert_version = get_max_support_version(\n                        versions, graph.opset_version)\n                    mapper_func, kw = opsets[convert_version]\n                    mapper_func(graph, node, **kw)\n                else:\n                    custom_paddle_op = OpMapper.REGISTER_CUSTOM_PADDLE_OP[\n                        node.type](node)\n                    custom_paddle_graph, output_results = custom_paddle_op.get_paddle_graph(\n                    )\n                    OpMapper.check_support_status(custom_paddle_graph.node_map,\n                                                  graph.opset_version)\n                    graph.build_op_nodes(custom_paddle_graph.node_map)\n\n                    node_output_results = dict()\n                    for k in node.output_names:\n                        custom_outs = output_results[k]\n                        node_outs = node.output(k)\n                        assert len(custom_outs) == len(\n                            node_outs\n                        ), \"Length of custom implementation operator's outputs is not same with the length of original operator's outputs.\"\n                        for i in range(len(custom_outs)):\n                            graph.make_node(\n                                \"Identity\",\n                                inputs=[custom_outs[i]],\n                                outputs=[node_outs[i]])\n            else:\n                opsets = OpMapper.OPSETS[node.type]\n                versions = list(opsets.keys())\n                convert_version = get_max_support_version(versions,\n                                                          graph.opset_version)\n                mapper_func, kw = opsets[convert_version]\n                mapper_func(graph, node, **kw)\n        except Exception as e:\n            raise Exception(\n                \"Error happened when mapping node ['{}'] to onnx, which op_type is '{}' with inputs: {} and outputs: {}, specific error: \".\n                format(node.layer_name, node.type, node.inputs,\n                       node.outputs) + str(e))\n\n    @staticmethod\n    def get_recommend_opset_version(node_map, opset_version):\n        recommend_opset_version = OpMapper.check_support_status(\n            node_map, opset_version, True)\n        for name, node in list(node_map.items()):\n            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:  #\u5982\u679c\u662fcustom\u7684op\uff0c\u83b7\u53d6custom\u7684\u63a8\u8350op\n                custom_paddle_op = OpMapper.REGISTER_CUSTOM_PADDLE_OP[\n                    node.type](node)\n                custom_paddle_graph, output_results = custom_paddle_op.get_paddle_graph(\n                )\n                custom_recommend_opset_version = OpMapper.check_support_status(\n                    custom_paddle_graph.node_map, opset_version, True)\n                recommend_opset_version = max(recommend_opset_version,\n                                              custom_recommend_opset_version)\n        if opset_version != recommend_opset_version:\n            warning_info = \"\\n======================\\n\"\n            warning_info += \"\\nFor a successful conversion, set the recommended opset version : {}\\n\".format(\n                recommend_opset_version)\n            warning_info += \"\\n======================\\n\"\n            logging.warning(warning_info)\n        return recommend_opset_version\n\n    @staticmethod\n    def check_support_status(node_map, opset_version, for_check=False):\n        op_mapping_status = {\n            OP_MAPPING_NO_REGISTER: [],\n            OP_MAPPING_NO_VERSION: [],\n        }\n        for name, node in list(node_map.items()):\n            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:\n                continue\n            if node.type not in OpMapper.OPSETS:\n                op_mapping_status[OP_MAPPING_NO_REGISTER].append(node)\n            else:\n                opsets = OpMapper.OPSETS[node.type]\n                versions = list(opsets.keys())\n                convert_version = get_max_support_version(versions,\n                                                          opset_version)\n                if convert_version == -1:\n                    op_mapping_status[OP_MAPPING_NO_VERSION].append(node)\n\n        if len(op_mapping_status[OP_MAPPING_NO_REGISTER]) > 0:\n            unsupported_op_types = set([\n                node.type for node in op_mapping_status[OP_MAPPING_NO_REGISTER]\n            ])\n            error_info = \"\\nThere's {} ops are not supported yet\\n\".format(\n                len(unsupported_op_types))\n            for op_type in unsupported_op_types:\n                error_info += \"=========== {} ===========\\n\".format(op_type)\n            raise NotImplementedError(error_info)\n\n        if len(op_mapping_status[OP_MAPPING_NO_VERSION]) > 0:\n            unsupported_op_types = set([\n                node.type for node in op_mapping_status[OP_MAPPING_NO_VERSION]\n            ])\n\n            recommend_opset_version = -1\n            for op_type in unsupported_op_types:\n                opsets = OpMapper.OPSETS[op_type]\n                if min(opsets.keys()) > recommend_opset_version:\n                    recommend_opset_version = min(opsets.keys())\n            warning_info = \"\\nThere are {} ops that are not supported in opset version {}, please set opset version >= {}.\\n\".format(\n                len(unsupported_op_types), opset_version,\n                recommend_opset_version)\n\n            for op_type in unsupported_op_types:\n                warning_info += \"=========== {} ===========\\n\".format(op_type)\n            if for_check:\n                logging.warning(warning_info)\n                return recommend_opset_version\n            raise NotImplementedError(warning_info)\n        return opset_version",
  "class CustomPaddleOp(object):\n    CREATE_TIMES = {}\n\n    def __init__(self, node):\n        self.main_program = paddle.static.Program()\n        self.startup_program = paddle.static.Program()\n        self.inputs = self.create_place_holder(node)\n        self.node = node\n\n    def generate_scope_name(self, node):\n        if node.type in CustomPaddleOp.CREATE_TIMES:\n            CustomPaddleOp.CREATE_TIMES[node.type] += 1\n        else:\n            CustomPaddleOp.CREATE_TIMES[node.type] = 1\n        scope_prefix = node.type + str(CustomPaddleOp.CREATE_TIMES[node.type] -\n                                       1) + '_'\n        return scope_prefix\n\n    def create_place_holder(self, node):\n        place_holders = {}\n        with paddle.static.program_guard(self.main_program,\n                                         self.startup_program):\n            for arg_name, idxs in node.inputs.items():\n                place_holders[arg_name] = []\n                for idx in range(len(idxs)):\n                    shape = node.input_shape(arg_name, idx)\n                    dtype = node.input_dtype(arg_name, idx)\n                    name = node.input(arg_name, idx)\n                    data = paddle.static.data(\n                        name=name, shape=shape, dtype=dtype)\n                    place_holders[arg_name].append(data)\n        return place_holders\n\n    def input(self, name, idx=None):\n        if name not in self.inputs:\n            return None\n        if idx is None:\n            return self.inputs[name]\n        if len(self.inputs[name]) <= idx:\n            return None\n        return self.inputs[name][idx]\n\n    def get_paddle_graph(self):\n        scope_prefix = self.generate_scope_name(self.node)\n        scope = paddle.static.Scope()\n        with paddle.static.scope_guard(scope):\n            with paddle.static.program_guard(self.main_program,\n                                             self.startup_program):\n                with paddle.utils.unique_name.guard(scope_prefix):\n                    res = self.forward()\n                    feed_var_names = [\n                        var.name for vars in self.inputs.values()\n                        for var in vars\n                    ]\n                    fetch_vars = [var for vars in res.values() for var in vars]\n                    inference_program = graph_helper.get_program(\n                        self.main_program, feed_var_names, fetch_vars)\n                    paddle_graph = PaddleGraph.build_from_program(\n                        inference_program,\n                        feed_var_names,\n                        fetch_vars,\n                        scope=scope)\n\n        output_results = dict()\n        for arg_name, outs in res.items():\n            output_results[arg_name] = [out.name for out in outs]\n        return paddle_graph, output_results",
  "def register_custom_paddle_op(paddle_op, custom_op):\n    paddle_op_list = []\n\n    if isinstance(paddle_op, six.string_types):\n        paddle_op_list.append(paddle_op)\n    elif isinstance(paddle_op, list):\n        paddle_op_list = paddle_op\n    else:\n        raise ValueError(\"paddle_op' must be List or string, but got type {}.\".\n                         format(type(paddle_op)))\n\n    if not isinstance(custom_op, six.class_types):\n        raise ValueError(\"'custom_op' must be Class, but got type {}.\".format(\n            type(custom_op)))\n\n    forward = getattr(custom_op, \"forward\", None)\n    if not callable(forward):\n        raise Exception(\n            \"Custom paddle operators must be implemented in function named 'forward'.\"\n        )\n\n    for op in paddle_op_list:\n        if op not in OpMapper.REGISTER_CUSTOM_PADDLE_OP:\n            OpMapper.REGISTER_CUSTOM_PADDLE_OP[op] = custom_op",
  "def __init__(self, paddle_op, **kwargs):\n        if not isinstance(paddle_op, list):\n            paddle_op = [paddle_op]\n        self.paddle_op = paddle_op\n        self.kwargs = kwargs",
  "def __call__(self, cls):\n        for k, v in inspect.getmembers(cls, inspect.ismethod):\n            if k.startswith(\"opset_\"):\n                version = int(k.replace(\"opset_\", \"\"))\n                for op in self.paddle_op:\n                    if op not in OpMapper.OPSETS:\n                        OpMapper.OPSETS[op] = {}\n                    opset_dict = OpMapper.OPSETS[op]\n                    opset_dict[version] = (v, self.kwargs)",
  "def mapping(graph, node, operator_export_type=\"ONNX\"):\n        try:\n            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:\n                if operator_export_type in [\"PaddleFallback\"]:\n                    opsets = OpMapper.OPSETS[node.type]\n                    versions = list(opsets.keys())\n                    convert_version = get_max_support_version(\n                        versions, graph.opset_version)\n                    mapper_func, kw = opsets[convert_version]\n                    mapper_func(graph, node, **kw)\n                else:\n                    custom_paddle_op = OpMapper.REGISTER_CUSTOM_PADDLE_OP[\n                        node.type](node)\n                    custom_paddle_graph, output_results = custom_paddle_op.get_paddle_graph(\n                    )\n                    OpMapper.check_support_status(custom_paddle_graph.node_map,\n                                                  graph.opset_version)\n                    graph.build_op_nodes(custom_paddle_graph.node_map)\n\n                    node_output_results = dict()\n                    for k in node.output_names:\n                        custom_outs = output_results[k]\n                        node_outs = node.output(k)\n                        assert len(custom_outs) == len(\n                            node_outs\n                        ), \"Length of custom implementation operator's outputs is not same with the length of original operator's outputs.\"\n                        for i in range(len(custom_outs)):\n                            graph.make_node(\n                                \"Identity\",\n                                inputs=[custom_outs[i]],\n                                outputs=[node_outs[i]])\n            else:\n                opsets = OpMapper.OPSETS[node.type]\n                versions = list(opsets.keys())\n                convert_version = get_max_support_version(versions,\n                                                          graph.opset_version)\n                mapper_func, kw = opsets[convert_version]\n                mapper_func(graph, node, **kw)\n        except Exception as e:\n            raise Exception(\n                \"Error happened when mapping node ['{}'] to onnx, which op_type is '{}' with inputs: {} and outputs: {}, specific error: \".\n                format(node.layer_name, node.type, node.inputs,\n                       node.outputs) + str(e))",
  "def get_recommend_opset_version(node_map, opset_version):\n        recommend_opset_version = OpMapper.check_support_status(\n            node_map, opset_version, True)\n        for name, node in list(node_map.items()):\n            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:  #\u5982\u679c\u662fcustom\u7684op\uff0c\u83b7\u53d6custom\u7684\u63a8\u8350op\n                custom_paddle_op = OpMapper.REGISTER_CUSTOM_PADDLE_OP[\n                    node.type](node)\n                custom_paddle_graph, output_results = custom_paddle_op.get_paddle_graph(\n                )\n                custom_recommend_opset_version = OpMapper.check_support_status(\n                    custom_paddle_graph.node_map, opset_version, True)\n                recommend_opset_version = max(recommend_opset_version,\n                                              custom_recommend_opset_version)\n        if opset_version != recommend_opset_version:\n            warning_info = \"\\n======================\\n\"\n            warning_info += \"\\nFor a successful conversion, set the recommended opset version : {}\\n\".format(\n                recommend_opset_version)\n            warning_info += \"\\n======================\\n\"\n            logging.warning(warning_info)\n        return recommend_opset_version",
  "def check_support_status(node_map, opset_version, for_check=False):\n        op_mapping_status = {\n            OP_MAPPING_NO_REGISTER: [],\n            OP_MAPPING_NO_VERSION: [],\n        }\n        for name, node in list(node_map.items()):\n            if node.type in OpMapper.REGISTER_CUSTOM_PADDLE_OP:\n                continue\n            if node.type not in OpMapper.OPSETS:\n                op_mapping_status[OP_MAPPING_NO_REGISTER].append(node)\n            else:\n                opsets = OpMapper.OPSETS[node.type]\n                versions = list(opsets.keys())\n                convert_version = get_max_support_version(versions,\n                                                          opset_version)\n                if convert_version == -1:\n                    op_mapping_status[OP_MAPPING_NO_VERSION].append(node)\n\n        if len(op_mapping_status[OP_MAPPING_NO_REGISTER]) > 0:\n            unsupported_op_types = set([\n                node.type for node in op_mapping_status[OP_MAPPING_NO_REGISTER]\n            ])\n            error_info = \"\\nThere's {} ops are not supported yet\\n\".format(\n                len(unsupported_op_types))\n            for op_type in unsupported_op_types:\n                error_info += \"=========== {} ===========\\n\".format(op_type)\n            raise NotImplementedError(error_info)\n\n        if len(op_mapping_status[OP_MAPPING_NO_VERSION]) > 0:\n            unsupported_op_types = set([\n                node.type for node in op_mapping_status[OP_MAPPING_NO_VERSION]\n            ])\n\n            recommend_opset_version = -1\n            for op_type in unsupported_op_types:\n                opsets = OpMapper.OPSETS[op_type]\n                if min(opsets.keys()) > recommend_opset_version:\n                    recommend_opset_version = min(opsets.keys())\n            warning_info = \"\\nThere are {} ops that are not supported in opset version {}, please set opset version >= {}.\\n\".format(\n                len(unsupported_op_types), opset_version,\n                recommend_opset_version)\n\n            for op_type in unsupported_op_types:\n                warning_info += \"=========== {} ===========\\n\".format(op_type)\n            if for_check:\n                logging.warning(warning_info)\n                return recommend_opset_version\n            raise NotImplementedError(warning_info)\n        return opset_version",
  "def __init__(self, node):\n        self.main_program = paddle.static.Program()\n        self.startup_program = paddle.static.Program()\n        self.inputs = self.create_place_holder(node)\n        self.node = node",
  "def generate_scope_name(self, node):\n        if node.type in CustomPaddleOp.CREATE_TIMES:\n            CustomPaddleOp.CREATE_TIMES[node.type] += 1\n        else:\n            CustomPaddleOp.CREATE_TIMES[node.type] = 1\n        scope_prefix = node.type + str(CustomPaddleOp.CREATE_TIMES[node.type] -\n                                       1) + '_'\n        return scope_prefix",
  "def create_place_holder(self, node):\n        place_holders = {}\n        with paddle.static.program_guard(self.main_program,\n                                         self.startup_program):\n            for arg_name, idxs in node.inputs.items():\n                place_holders[arg_name] = []\n                for idx in range(len(idxs)):\n                    shape = node.input_shape(arg_name, idx)\n                    dtype = node.input_dtype(arg_name, idx)\n                    name = node.input(arg_name, idx)\n                    data = paddle.static.data(\n                        name=name, shape=shape, dtype=dtype)\n                    place_holders[arg_name].append(data)\n        return place_holders",
  "def input(self, name, idx=None):\n        if name not in self.inputs:\n            return None\n        if idx is None:\n            return self.inputs[name]\n        if len(self.inputs[name]) <= idx:\n            return None\n        return self.inputs[name][idx]",
  "def get_paddle_graph(self):\n        scope_prefix = self.generate_scope_name(self.node)\n        scope = paddle.static.Scope()\n        with paddle.static.scope_guard(scope):\n            with paddle.static.program_guard(self.main_program,\n                                             self.startup_program):\n                with paddle.utils.unique_name.guard(scope_prefix):\n                    res = self.forward()\n                    feed_var_names = [\n                        var.name for vars in self.inputs.values()\n                        for var in vars\n                    ]\n                    fetch_vars = [var for vars in res.values() for var in vars]\n                    inference_program = graph_helper.get_program(\n                        self.main_program, feed_var_names, fetch_vars)\n                    paddle_graph = PaddleGraph.build_from_program(\n                        inference_program,\n                        feed_var_names,\n                        fetch_vars,\n                        scope=scope)\n\n        output_results = dict()\n        for arg_name, outs in res.items():\n            output_results[arg_name] = [out.name for out in outs]\n        return paddle_graph, output_results",
  "class YOLOBox():\n    support_opset_verison_range = (9, 12)\n\n    node_pred_box_x1_decode = None\n    node_pred_box_y1_decode = None\n    node_pred_box_x2_decode = None\n    node_pred_box_y2_decode = None\n    node_pred_box_x2_sub_w = None\n    node_pred_box_y2_sub_h = None\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        model_name = node.output('Boxes', 0)\n        input_shape = node.input_shape('X', 0)\n        mapper_helper.is_static_shape(input_shape)\n        image_size = node.input('ImgSize')\n        input_height = input_shape[2]\n        input_width = input_shape[3]\n        class_num = node.attr('class_num')\n        anchors = node.attr('anchors')\n        num_anchors = int(len(anchors)) // 2\n        scale_x_y = node.attr('scale_x_y')\n        downsample_ratio = node.attr('downsample_ratio')\n        input_size = input_height * downsample_ratio\n        conf_thresh = node.attr('conf_thresh')\n        conf_thresh_mat = [conf_thresh\n                           ] * num_anchors * input_height * input_width\n\n        cls.score_shape = [\n            1, input_height * input_width * int(num_anchors), class_num\n        ]\n\n        im_outputs = []\n\n        x_shape = [1, num_anchors, 5 + class_num, input_height, input_width]\n        node_x_shape = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                               'value': x_shape})\n\n        node_x_reshape = graph.make_node(\n            'Reshape', inputs=[node.input('X')[0], node_x_shape])\n        node_x_transpose = graph.make_node(\n            'Transpose', inputs=[node_x_reshape], perm=[0, 1, 3, 4, 2])\n\n        range_x = []\n        range_y = []\n        for i in range(0, input_width):\n            range_x.append(i)\n        for j in range(0, input_height):\n            range_y.append(j)\n\n        node_range_x = graph.make_node(\n            'Constant',\n            attrs={\n                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                'value': range_x,\n            })\n\n        node_range_y = graph.make_node(\n            'Constant',\n            inputs=[],\n            attrs={\n                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                'value': range_y,\n            })\n\n        range_x_new_shape = [1, input_width]\n        range_y_new_shape = [input_height, 1]\n\n        node_range_x_new_shape = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.ONNX.INT64,\n            value=range_x_new_shape)\n        node_range_y_new_shape = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.ONNX.INT64,\n            value=range_y_new_shape)\n\n        node_range_x_reshape = graph.make_node(\n            'Reshape', inputs=[node_range_x, node_range_x_new_shape])\n        node_range_y_reshape = graph.make_node(\n            'Reshape', inputs=[node_range_y, node_range_y_new_shape])\n\n        node_grid_x = graph.make_node(\n            \"Tile\", inputs=[node_range_x_reshape, node_range_y_new_shape])\n\n        node_grid_y = graph.make_node(\n            \"Tile\", inputs=[node_range_y_reshape, node_range_x_new_shape])\n\n        node_box_x = model_name + \"@box_x\"\n        node_box_y = model_name + \"@box_y\"\n        node_box_w = model_name + \"@box_w\"\n        node_box_h = model_name + \"@box_h\"\n        node_conf = model_name + \"@conf\"\n        node_prob = model_name + \"@prob\"\n        output = [\n            node_box_x, node_box_y, node_box_w, node_box_h, node_conf, node_prob\n        ]\n        node_split_input = mapper_helper.split_helper(\n            graph, [node_x_transpose],\n            output,\n            -1, [1, 1, 1, 1, 1, class_num],\n            dtype=node.input_dtype('X', 0))\n\n        node_box_x_sigmoid = graph.make_node(\"Sigmoid\", inputs=[node_box_x])\n\n        node_box_y_sigmoid = graph.make_node(\"Sigmoid\", inputs=[node_box_y])\n\n        if scale_x_y is not None:\n            bias_x_y = -0.5 * (scale_x_y - 1.0)\n            scale_x_y_node = graph.make_node(\n                'Constant',\n                attrs={\n                    'dtype':\n                    dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                    'value': scale_x_y\n                })\n\n            bias_x_y_node = graph.make_node(\n                'Constant',\n                attrs={\n                    'dtype':\n                    dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                    'value': bias_x_y\n                })\n            node_box_x_sigmoid = graph.make_node(\n                \"Mul\", inputs=[node_box_x_sigmoid, scale_x_y_node])\n            node_box_x_sigmoid = graph.make_node(\n                \"Add\", inputs=[node_box_x_sigmoid, bias_x_y_node])\n            node_box_y_sigmoid = graph.make_node(\n                \"Mul\", inputs=[node_box_y_sigmoid, scale_x_y_node])\n            node_box_y_sigmoid = graph.make_node(\n                \"Add\", inputs=[node_box_y_sigmoid, bias_x_y_node])\n        node_box_x_squeeze = mapper_helper.squeeze_helper(\n            graph, node_box_x_sigmoid, [4])\n\n        node_box_y_squeeze = mapper_helper.squeeze_helper(\n            graph, node_box_y_sigmoid, [4])\n\n        node_box_x_add_grid = graph.make_node(\n            \"Add\", inputs=[node_grid_x, node_box_x_squeeze])\n\n        node_box_y_add_grid = graph.make_node(\n            \"Add\", inputs=[node_grid_y, node_box_y_squeeze])\n\n        node_input_h = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=[input_height])\n\n        node_input_w = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=[input_width])\n\n        node_box_x_encode = graph.make_node(\n            'Div', inputs=[node_box_x_add_grid, node_input_w])\n\n        node_box_y_encode = graph.make_node(\n            'Div', inputs=[node_box_y_add_grid, node_input_h])\n\n        node_anchor_tensor = graph.make_node(\n            \"Constant\",\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=anchors)\n\n        anchor_shape = [int(num_anchors), 2]\n        node_anchor_shape = graph.make_node(\n            \"Constant\", inputs=[], dtype=dtypes.ONNX.INT64, value=anchor_shape)\n\n        node_anchor_tensor_reshape = graph.make_node(\n            \"Reshape\", inputs=[node_anchor_tensor, node_anchor_shape])\n\n        node_input_size = graph.make_node(\n            \"Constant\",\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=[input_size])\n\n        node_anchors_div_input_size = graph.make_node(\n            \"Div\", inputs=[node_anchor_tensor_reshape, node_input_size])\n\n        node_anchor_w = model_name + \"@anchor_w\"\n        node_anchor_h = model_name + \"@anchor_h\"\n\n        node_anchor_split = mapper_helper.split_helper(\n            graph,\n            inputs=node_anchors_div_input_size,\n            axis=1,\n            split=[1, 1],\n            outputs=[node_anchor_w, node_anchor_h],\n            dtype=node.input_dtype('X', 0))\n\n        new_anchor_shape = [1, int(num_anchors), 1, 1]\n        node_new_anchor_shape = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.ONNX.INT64,\n            value=new_anchor_shape)\n\n        node_anchor_w_reshape = graph.make_node(\n            'Reshape', inputs=[node_anchor_w, node_new_anchor_shape])\n\n        node_anchor_h_reshape = graph.make_node(\n            'Reshape', inputs=[node_anchor_h, node_new_anchor_shape])\n\n        node_box_w_squeeze = mapper_helper.squeeze_helper(graph, node_box_w,\n                                                          [4])\n        node_box_h_squeeze = mapper_helper.squeeze_helper(graph, node_box_h,\n                                                          [4])\n\n        node_box_w_exp = graph.make_node(\"Exp\", inputs=[node_box_w_squeeze])\n        node_box_h_exp = graph.make_node(\"Exp\", inputs=[node_box_h_squeeze])\n\n        node_box_w_encode = graph.make_node(\n            'Mul', inputs=[node_box_w_exp, node_anchor_w_reshape])\n\n        node_box_h_encode = graph.make_node(\n            'Mul', inputs=[node_box_h_exp, node_anchor_h_reshape])\n\n        node_conf_sigmoid = graph.make_node('Sigmoid', inputs=[node_conf])\n\n        node_conf_thresh = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=conf_thresh_mat)\n\n        conf_shape = [1, int(num_anchors), input_height, input_width, 1]\n        node_conf_shape = graph.make_node(\n            'Constant', inputs=[], dtype=dtypes.ONNX.INT64, value=conf_shape)\n\n        node_conf_thresh_reshape = graph.make_node(\n            'Reshape', inputs=[node_conf_thresh, node_conf_shape])\n\n        node_conf_sub = graph.make_node(\n            'Sub', inputs=[node_conf_sigmoid, node_conf_thresh_reshape])\n\n        node_conf_clip = mapper_helper.clip_helper(graph, node, node_conf_sub,\n                                                   float(MAX_FLOAT32), 0.0)\n\n        node_zeros = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=[0])\n\n        node_conf_clip_bool = graph.make_node(\n            'Greater', inputs=[node_conf_clip, node_zeros])\n\n        node_conf_clip_cast = graph.make_node(\n            'Cast',\n            inputs=[node_conf_clip_bool],\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n\n        node_conf_set_zero = graph.make_node(\n            'Mul', inputs=[node_conf_sigmoid, node_conf_clip_cast])\n\n        node_prob_sigmoid = graph.make_node('Sigmoid', inputs=[node_prob])\n\n        new_shape = [1, int(num_anchors), input_height, input_width, 1]\n        node_new_shape = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.ONNX.INT64,\n            dims=[len(new_shape)],\n            value=new_shape)\n\n        node_conf_new_shape = graph.make_node(\n            'Reshape', inputs=[node_conf_set_zero, node_new_shape])\n\n        cls.node_score = graph.make_node(\n            'Mul', inputs=[node_prob_sigmoid, node_conf_new_shape])\n\n        node_conf_bool = graph.make_node(\n            'Greater', inputs=[node_conf_new_shape, node_zeros])\n\n        node_box_x_new_shape = graph.make_node(\n            'Reshape', inputs=[node_box_x_encode, node_new_shape])\n\n        node_box_y_new_shape = graph.make_node(\n            'Reshape', inputs=[node_box_y_encode, node_new_shape])\n\n        node_box_w_new_shape = graph.make_node(\n            'Reshape', inputs=[node_box_w_encode, node_new_shape])\n\n        node_box_h_new_shape = graph.make_node(\n            'Reshape', inputs=[node_box_h_encode, node_new_shape])\n\n        node_pred_box = graph.make_node(\n            'Concat',\n            inputs=[node_box_x_new_shape, node_box_y_new_shape, \\\n                   node_box_w_new_shape, node_box_h_new_shape],\n            axis=4)\n\n        node_conf_cast = graph.make_node(\n            'Cast',\n            inputs=[node_conf_bool],\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n\n        node_pred_box_mul_conf = graph.make_node(\n            'Mul', inputs=[node_pred_box, node_conf_cast])\n\n        box_shape = [1, int(num_anchors) * input_height * input_width, 4]\n        node_box_shape = graph.make_node(\n            'Constant', inputs=[], dtype=dtypes.ONNX.INT64, value=box_shape)\n\n        node_pred_box_new_shape = graph.make_node(\n            'Reshape', inputs=[node_pred_box_mul_conf, node_box_shape])\n\n        node_pred_box_x = model_name + \"@_pred_box_x\"\n        node_pred_box_y = model_name + \"@_pred_box_y\"\n        node_pred_box_w = model_name + \"@_pred_box_w\"\n        node_pred_box_h = model_name + \"@_pred_box_h\"\n        if node.input_dtype('X', 0) == paddle.float64:\n            node_pred_box_new_shape = graph.make_node(\n                'Cast', inputs=[node_pred_box_new_shape], to=TensorProto.FLOAT)\n        node_pred_box_split = mapper_helper.split_helper(\n            graph,\n            inputs=node_pred_box_new_shape,\n            axis=2,\n            split=[1, 1, 1, 1],\n            outputs=[\n                node_pred_box_x, node_pred_box_y, node_pred_box_w,\n                node_pred_box_h\n            ],\n            dtype=node.input_dtype('X', 0))\n\n        if node.input_dtype('X', 0) == paddle.float64:\n            node_pred_box_x = graph.make_node(\n                'Cast',\n                inputs=[node_pred_box_x],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n            node_pred_box_y = graph.make_node(\n                'Cast',\n                inputs=[node_pred_box_y],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n            node_pred_box_w = graph.make_node(\n                'Cast',\n                inputs=[node_pred_box_w],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n            node_pred_box_h = graph.make_node(\n                'Cast',\n                inputs=[node_pred_box_h],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n        node_number_two = graph.make_node(\n            \"Constant\",\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=[2])\n\n        node_half_w = graph.make_node(\n            \"Div\", inputs=[node_pred_box_w, node_number_two])\n\n        node_half_h = graph.make_node(\n            \"Div\", inputs=[node_pred_box_h, node_number_two])\n\n        node_pred_box_x1 = graph.make_node(\n            'Sub', inputs=[node_pred_box_x, node_half_w])\n\n        node_pred_box_y1 = graph.make_node(\n            'Sub', inputs=[node_pred_box_y, node_half_h])\n\n        node_pred_box_x2 = graph.make_node(\n            'Add', inputs=[node_pred_box_x, node_half_w])\n\n        node_pred_box_y2 = graph.make_node(\n            'Add', inputs=[node_pred_box_y, node_half_h])\n\n        node_sqeeze_image_size = mapper_helper.squeeze_helper(\n            graph, image_size[0], [0])\n\n        node_img_height = model_name + \"@img_height\"\n        node_img_width = model_name + \"@img_width\"\n        node_image_size_split = mapper_helper.split_helper(\n            graph, [node_sqeeze_image_size], [node_img_height, node_img_width],\n            -1, [1, 1],\n            dtype=node.input_dtype('X', 0))\n\n        node_img_width_cast = graph.make_node(\n            'Cast',\n            inputs=[node_img_width],\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n\n        node_img_height_cast = graph.make_node(\n            'Cast',\n            inputs=[node_img_height],\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n\n        cls.node_pred_box_x1_decode = graph.make_node(\n            'Mul',\n            inputs=[node_pred_box_x1, node_img_width_cast])  #boxes[box_idx]\n\n        cls.node_pred_box_y1_decode = graph.make_node(\n            'Mul', inputs=[node_pred_box_y1,\n                           node_img_height_cast])  #boxes[box_idx + 1]\n\n        cls.node_pred_box_x2_decode = graph.make_node(\n            'Mul',\n            inputs=[node_pred_box_x2, node_img_width_cast])  #boxes[box_idx + 2]\n\n        cls.node_pred_box_y2_decode = graph.make_node(\n            'Mul', inputs=[node_pred_box_y2,\n                           node_img_height_cast])  #boxes[box_idx + 3]\n\n        if node.attr('clip_bbox'):\n            node_number_one = graph.make_node(\n                'Constant',\n                inputs=[],\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                value=[1])\n\n            node_new_img_height = graph.make_node(\n                'Sub', inputs=[node_img_height_cast, node_number_one])\n\n            node_new_img_width = graph.make_node(\n                'Sub', inputs=[node_img_width_cast, node_number_one])\n\n            cls.node_pred_box_x2_sub_w = graph.make_node(\n                'Sub',\n                inputs=[cls.node_pred_box_x2_decode, node_new_img_width])\n\n            cls.node_pred_box_y2_sub_h = graph.make_node(\n                'Sub',\n                inputs=[cls.node_pred_box_y2_decode, node_new_img_height])\n\n            node_pred_box_x1_clip = mapper_helper.clip_helper(\n                graph, node, cls.node_pred_box_x1_decode,\n                float(MAX_FLOAT32), 0.0)\n            node_pred_box_y1_clip = mapper_helper.clip_helper(\n                graph, node, cls.node_pred_box_y1_decode,\n                float(MAX_FLOAT32), 0.0)\n            node_pred_box_x2_clip = mapper_helper.clip_helper(\n                graph, node, cls.node_pred_box_x2_sub_w,\n                float(MAX_FLOAT32), 0.0)\n            node_pred_box_y2_clip = mapper_helper.clip_helper(\n                graph, node, cls.node_pred_box_y2_sub_h,\n                float(MAX_FLOAT32), 0.0)\n            node_pred_box_x2_res = graph.make_node(\n                'Sub',\n                inputs=[cls.node_pred_box_x2_decode, node_pred_box_x2_clip])\n\n            node_pred_box_y2_res = graph.make_node(\n                'Sub',\n                inputs=[cls.node_pred_box_y2_decode, node_pred_box_y2_clip])\n\n            node_pred_box_result = graph.make_node(\n                'Concat',\n                inputs=[\n                    node_pred_box_x1_clip, node_pred_box_y1_clip,\n                    node_pred_box_x2_res, node_pred_box_y2_res\n                ],\n                outputs=node.output('Boxes'),\n                axis=-1)\n        else:\n            node_pred_box_result = graph.make_node(\n                'Concat',\n                inputs=[\n                    cls.node_pred_box_x1_decode, cls.node_pred_box_y1_decode,\n                    cls.node_pred_box_x2_decode, cls.node_pred_box_y2_decode\n                ],\n                outputs=node.output('Boxes'),\n                axis=-1)\n        node_score_shape = graph.make_node(\n            \"Constant\",\n            inputs=[],\n            dtype=dtypes.ONNX.INT64,\n            value=cls.score_shape)\n\n        node_score_new_shape = graph.make_node(\n            'Reshape',\n            inputs=[cls.node_score, node_score_shape],\n            outputs=node.output('Scores'))",
  "def opset_9(cls, graph, node, **kw):\n        model_name = node.output('Boxes', 0)\n        input_shape = node.input_shape('X', 0)\n        mapper_helper.is_static_shape(input_shape)\n        image_size = node.input('ImgSize')\n        input_height = input_shape[2]\n        input_width = input_shape[3]\n        class_num = node.attr('class_num')\n        anchors = node.attr('anchors')\n        num_anchors = int(len(anchors)) // 2\n        scale_x_y = node.attr('scale_x_y')\n        downsample_ratio = node.attr('downsample_ratio')\n        input_size = input_height * downsample_ratio\n        conf_thresh = node.attr('conf_thresh')\n        conf_thresh_mat = [conf_thresh\n                           ] * num_anchors * input_height * input_width\n\n        cls.score_shape = [\n            1, input_height * input_width * int(num_anchors), class_num\n        ]\n\n        im_outputs = []\n\n        x_shape = [1, num_anchors, 5 + class_num, input_height, input_width]\n        node_x_shape = graph.make_node(\n            'Constant', attrs={'dtype': dtypes.ONNX.INT64,\n                               'value': x_shape})\n\n        node_x_reshape = graph.make_node(\n            'Reshape', inputs=[node.input('X')[0], node_x_shape])\n        node_x_transpose = graph.make_node(\n            'Transpose', inputs=[node_x_reshape], perm=[0, 1, 3, 4, 2])\n\n        range_x = []\n        range_y = []\n        for i in range(0, input_width):\n            range_x.append(i)\n        for j in range(0, input_height):\n            range_y.append(j)\n\n        node_range_x = graph.make_node(\n            'Constant',\n            attrs={\n                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                'value': range_x,\n            })\n\n        node_range_y = graph.make_node(\n            'Constant',\n            inputs=[],\n            attrs={\n                'dtype': dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                'value': range_y,\n            })\n\n        range_x_new_shape = [1, input_width]\n        range_y_new_shape = [input_height, 1]\n\n        node_range_x_new_shape = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.ONNX.INT64,\n            value=range_x_new_shape)\n        node_range_y_new_shape = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.ONNX.INT64,\n            value=range_y_new_shape)\n\n        node_range_x_reshape = graph.make_node(\n            'Reshape', inputs=[node_range_x, node_range_x_new_shape])\n        node_range_y_reshape = graph.make_node(\n            'Reshape', inputs=[node_range_y, node_range_y_new_shape])\n\n        node_grid_x = graph.make_node(\n            \"Tile\", inputs=[node_range_x_reshape, node_range_y_new_shape])\n\n        node_grid_y = graph.make_node(\n            \"Tile\", inputs=[node_range_y_reshape, node_range_x_new_shape])\n\n        node_box_x = model_name + \"@box_x\"\n        node_box_y = model_name + \"@box_y\"\n        node_box_w = model_name + \"@box_w\"\n        node_box_h = model_name + \"@box_h\"\n        node_conf = model_name + \"@conf\"\n        node_prob = model_name + \"@prob\"\n        output = [\n            node_box_x, node_box_y, node_box_w, node_box_h, node_conf, node_prob\n        ]\n        node_split_input = mapper_helper.split_helper(\n            graph, [node_x_transpose],\n            output,\n            -1, [1, 1, 1, 1, 1, class_num],\n            dtype=node.input_dtype('X', 0))\n\n        node_box_x_sigmoid = graph.make_node(\"Sigmoid\", inputs=[node_box_x])\n\n        node_box_y_sigmoid = graph.make_node(\"Sigmoid\", inputs=[node_box_y])\n\n        if scale_x_y is not None:\n            bias_x_y = -0.5 * (scale_x_y - 1.0)\n            scale_x_y_node = graph.make_node(\n                'Constant',\n                attrs={\n                    'dtype':\n                    dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                    'value': scale_x_y\n                })\n\n            bias_x_y_node = graph.make_node(\n                'Constant',\n                attrs={\n                    'dtype':\n                    dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                    'value': bias_x_y\n                })\n            node_box_x_sigmoid = graph.make_node(\n                \"Mul\", inputs=[node_box_x_sigmoid, scale_x_y_node])\n            node_box_x_sigmoid = graph.make_node(\n                \"Add\", inputs=[node_box_x_sigmoid, bias_x_y_node])\n            node_box_y_sigmoid = graph.make_node(\n                \"Mul\", inputs=[node_box_y_sigmoid, scale_x_y_node])\n            node_box_y_sigmoid = graph.make_node(\n                \"Add\", inputs=[node_box_y_sigmoid, bias_x_y_node])\n        node_box_x_squeeze = mapper_helper.squeeze_helper(\n            graph, node_box_x_sigmoid, [4])\n\n        node_box_y_squeeze = mapper_helper.squeeze_helper(\n            graph, node_box_y_sigmoid, [4])\n\n        node_box_x_add_grid = graph.make_node(\n            \"Add\", inputs=[node_grid_x, node_box_x_squeeze])\n\n        node_box_y_add_grid = graph.make_node(\n            \"Add\", inputs=[node_grid_y, node_box_y_squeeze])\n\n        node_input_h = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=[input_height])\n\n        node_input_w = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=[input_width])\n\n        node_box_x_encode = graph.make_node(\n            'Div', inputs=[node_box_x_add_grid, node_input_w])\n\n        node_box_y_encode = graph.make_node(\n            'Div', inputs=[node_box_y_add_grid, node_input_h])\n\n        node_anchor_tensor = graph.make_node(\n            \"Constant\",\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=anchors)\n\n        anchor_shape = [int(num_anchors), 2]\n        node_anchor_shape = graph.make_node(\n            \"Constant\", inputs=[], dtype=dtypes.ONNX.INT64, value=anchor_shape)\n\n        node_anchor_tensor_reshape = graph.make_node(\n            \"Reshape\", inputs=[node_anchor_tensor, node_anchor_shape])\n\n        node_input_size = graph.make_node(\n            \"Constant\",\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=[input_size])\n\n        node_anchors_div_input_size = graph.make_node(\n            \"Div\", inputs=[node_anchor_tensor_reshape, node_input_size])\n\n        node_anchor_w = model_name + \"@anchor_w\"\n        node_anchor_h = model_name + \"@anchor_h\"\n\n        node_anchor_split = mapper_helper.split_helper(\n            graph,\n            inputs=node_anchors_div_input_size,\n            axis=1,\n            split=[1, 1],\n            outputs=[node_anchor_w, node_anchor_h],\n            dtype=node.input_dtype('X', 0))\n\n        new_anchor_shape = [1, int(num_anchors), 1, 1]\n        node_new_anchor_shape = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.ONNX.INT64,\n            value=new_anchor_shape)\n\n        node_anchor_w_reshape = graph.make_node(\n            'Reshape', inputs=[node_anchor_w, node_new_anchor_shape])\n\n        node_anchor_h_reshape = graph.make_node(\n            'Reshape', inputs=[node_anchor_h, node_new_anchor_shape])\n\n        node_box_w_squeeze = mapper_helper.squeeze_helper(graph, node_box_w,\n                                                          [4])\n        node_box_h_squeeze = mapper_helper.squeeze_helper(graph, node_box_h,\n                                                          [4])\n\n        node_box_w_exp = graph.make_node(\"Exp\", inputs=[node_box_w_squeeze])\n        node_box_h_exp = graph.make_node(\"Exp\", inputs=[node_box_h_squeeze])\n\n        node_box_w_encode = graph.make_node(\n            'Mul', inputs=[node_box_w_exp, node_anchor_w_reshape])\n\n        node_box_h_encode = graph.make_node(\n            'Mul', inputs=[node_box_h_exp, node_anchor_h_reshape])\n\n        node_conf_sigmoid = graph.make_node('Sigmoid', inputs=[node_conf])\n\n        node_conf_thresh = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=conf_thresh_mat)\n\n        conf_shape = [1, int(num_anchors), input_height, input_width, 1]\n        node_conf_shape = graph.make_node(\n            'Constant', inputs=[], dtype=dtypes.ONNX.INT64, value=conf_shape)\n\n        node_conf_thresh_reshape = graph.make_node(\n            'Reshape', inputs=[node_conf_thresh, node_conf_shape])\n\n        node_conf_sub = graph.make_node(\n            'Sub', inputs=[node_conf_sigmoid, node_conf_thresh_reshape])\n\n        node_conf_clip = mapper_helper.clip_helper(graph, node, node_conf_sub,\n                                                   float(MAX_FLOAT32), 0.0)\n\n        node_zeros = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=[0])\n\n        node_conf_clip_bool = graph.make_node(\n            'Greater', inputs=[node_conf_clip, node_zeros])\n\n        node_conf_clip_cast = graph.make_node(\n            'Cast',\n            inputs=[node_conf_clip_bool],\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n\n        node_conf_set_zero = graph.make_node(\n            'Mul', inputs=[node_conf_sigmoid, node_conf_clip_cast])\n\n        node_prob_sigmoid = graph.make_node('Sigmoid', inputs=[node_prob])\n\n        new_shape = [1, int(num_anchors), input_height, input_width, 1]\n        node_new_shape = graph.make_node(\n            'Constant',\n            inputs=[],\n            dtype=dtypes.ONNX.INT64,\n            dims=[len(new_shape)],\n            value=new_shape)\n\n        node_conf_new_shape = graph.make_node(\n            'Reshape', inputs=[node_conf_set_zero, node_new_shape])\n\n        cls.node_score = graph.make_node(\n            'Mul', inputs=[node_prob_sigmoid, node_conf_new_shape])\n\n        node_conf_bool = graph.make_node(\n            'Greater', inputs=[node_conf_new_shape, node_zeros])\n\n        node_box_x_new_shape = graph.make_node(\n            'Reshape', inputs=[node_box_x_encode, node_new_shape])\n\n        node_box_y_new_shape = graph.make_node(\n            'Reshape', inputs=[node_box_y_encode, node_new_shape])\n\n        node_box_w_new_shape = graph.make_node(\n            'Reshape', inputs=[node_box_w_encode, node_new_shape])\n\n        node_box_h_new_shape = graph.make_node(\n            'Reshape', inputs=[node_box_h_encode, node_new_shape])\n\n        node_pred_box = graph.make_node(\n            'Concat',\n            inputs=[node_box_x_new_shape, node_box_y_new_shape, \\\n                   node_box_w_new_shape, node_box_h_new_shape],\n            axis=4)\n\n        node_conf_cast = graph.make_node(\n            'Cast',\n            inputs=[node_conf_bool],\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n\n        node_pred_box_mul_conf = graph.make_node(\n            'Mul', inputs=[node_pred_box, node_conf_cast])\n\n        box_shape = [1, int(num_anchors) * input_height * input_width, 4]\n        node_box_shape = graph.make_node(\n            'Constant', inputs=[], dtype=dtypes.ONNX.INT64, value=box_shape)\n\n        node_pred_box_new_shape = graph.make_node(\n            'Reshape', inputs=[node_pred_box_mul_conf, node_box_shape])\n\n        node_pred_box_x = model_name + \"@_pred_box_x\"\n        node_pred_box_y = model_name + \"@_pred_box_y\"\n        node_pred_box_w = model_name + \"@_pred_box_w\"\n        node_pred_box_h = model_name + \"@_pred_box_h\"\n        if node.input_dtype('X', 0) == paddle.float64:\n            node_pred_box_new_shape = graph.make_node(\n                'Cast', inputs=[node_pred_box_new_shape], to=TensorProto.FLOAT)\n        node_pred_box_split = mapper_helper.split_helper(\n            graph,\n            inputs=node_pred_box_new_shape,\n            axis=2,\n            split=[1, 1, 1, 1],\n            outputs=[\n                node_pred_box_x, node_pred_box_y, node_pred_box_w,\n                node_pred_box_h\n            ],\n            dtype=node.input_dtype('X', 0))\n\n        if node.input_dtype('X', 0) == paddle.float64:\n            node_pred_box_x = graph.make_node(\n                'Cast',\n                inputs=[node_pred_box_x],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n            node_pred_box_y = graph.make_node(\n                'Cast',\n                inputs=[node_pred_box_y],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n            node_pred_box_w = graph.make_node(\n                'Cast',\n                inputs=[node_pred_box_w],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n            node_pred_box_h = graph.make_node(\n                'Cast',\n                inputs=[node_pred_box_h],\n                to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n        node_number_two = graph.make_node(\n            \"Constant\",\n            inputs=[],\n            dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n            value=[2])\n\n        node_half_w = graph.make_node(\n            \"Div\", inputs=[node_pred_box_w, node_number_two])\n\n        node_half_h = graph.make_node(\n            \"Div\", inputs=[node_pred_box_h, node_number_two])\n\n        node_pred_box_x1 = graph.make_node(\n            'Sub', inputs=[node_pred_box_x, node_half_w])\n\n        node_pred_box_y1 = graph.make_node(\n            'Sub', inputs=[node_pred_box_y, node_half_h])\n\n        node_pred_box_x2 = graph.make_node(\n            'Add', inputs=[node_pred_box_x, node_half_w])\n\n        node_pred_box_y2 = graph.make_node(\n            'Add', inputs=[node_pred_box_y, node_half_h])\n\n        node_sqeeze_image_size = mapper_helper.squeeze_helper(\n            graph, image_size[0], [0])\n\n        node_img_height = model_name + \"@img_height\"\n        node_img_width = model_name + \"@img_width\"\n        node_image_size_split = mapper_helper.split_helper(\n            graph, [node_sqeeze_image_size], [node_img_height, node_img_width],\n            -1, [1, 1],\n            dtype=node.input_dtype('X', 0))\n\n        node_img_width_cast = graph.make_node(\n            'Cast',\n            inputs=[node_img_width],\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n\n        node_img_height_cast = graph.make_node(\n            'Cast',\n            inputs=[node_img_height],\n            to=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)])\n\n        cls.node_pred_box_x1_decode = graph.make_node(\n            'Mul',\n            inputs=[node_pred_box_x1, node_img_width_cast])  #boxes[box_idx]\n\n        cls.node_pred_box_y1_decode = graph.make_node(\n            'Mul', inputs=[node_pred_box_y1,\n                           node_img_height_cast])  #boxes[box_idx + 1]\n\n        cls.node_pred_box_x2_decode = graph.make_node(\n            'Mul',\n            inputs=[node_pred_box_x2, node_img_width_cast])  #boxes[box_idx + 2]\n\n        cls.node_pred_box_y2_decode = graph.make_node(\n            'Mul', inputs=[node_pred_box_y2,\n                           node_img_height_cast])  #boxes[box_idx + 3]\n\n        if node.attr('clip_bbox'):\n            node_number_one = graph.make_node(\n                'Constant',\n                inputs=[],\n                dtype=dtypes.DTYPE_PADDLE_ONNX_MAP[node.input_dtype('X', 0)],\n                value=[1])\n\n            node_new_img_height = graph.make_node(\n                'Sub', inputs=[node_img_height_cast, node_number_one])\n\n            node_new_img_width = graph.make_node(\n                'Sub', inputs=[node_img_width_cast, node_number_one])\n\n            cls.node_pred_box_x2_sub_w = graph.make_node(\n                'Sub',\n                inputs=[cls.node_pred_box_x2_decode, node_new_img_width])\n\n            cls.node_pred_box_y2_sub_h = graph.make_node(\n                'Sub',\n                inputs=[cls.node_pred_box_y2_decode, node_new_img_height])\n\n            node_pred_box_x1_clip = mapper_helper.clip_helper(\n                graph, node, cls.node_pred_box_x1_decode,\n                float(MAX_FLOAT32), 0.0)\n            node_pred_box_y1_clip = mapper_helper.clip_helper(\n                graph, node, cls.node_pred_box_y1_decode,\n                float(MAX_FLOAT32), 0.0)\n            node_pred_box_x2_clip = mapper_helper.clip_helper(\n                graph, node, cls.node_pred_box_x2_sub_w,\n                float(MAX_FLOAT32), 0.0)\n            node_pred_box_y2_clip = mapper_helper.clip_helper(\n                graph, node, cls.node_pred_box_y2_sub_h,\n                float(MAX_FLOAT32), 0.0)\n            node_pred_box_x2_res = graph.make_node(\n                'Sub',\n                inputs=[cls.node_pred_box_x2_decode, node_pred_box_x2_clip])\n\n            node_pred_box_y2_res = graph.make_node(\n                'Sub',\n                inputs=[cls.node_pred_box_y2_decode, node_pred_box_y2_clip])\n\n            node_pred_box_result = graph.make_node(\n                'Concat',\n                inputs=[\n                    node_pred_box_x1_clip, node_pred_box_y1_clip,\n                    node_pred_box_x2_res, node_pred_box_y2_res\n                ],\n                outputs=node.output('Boxes'),\n                axis=-1)\n        else:\n            node_pred_box_result = graph.make_node(\n                'Concat',\n                inputs=[\n                    cls.node_pred_box_x1_decode, cls.node_pred_box_y1_decode,\n                    cls.node_pred_box_x2_decode, cls.node_pred_box_y2_decode\n                ],\n                outputs=node.output('Boxes'),\n                axis=-1)\n        node_score_shape = graph.make_node(\n            \"Constant\",\n            inputs=[],\n            dtype=dtypes.ONNX.INT64,\n            value=cls.score_shape)\n\n        node_score_new_shape = graph.make_node(\n            'Reshape',\n            inputs=[cls.node_score, node_score_shape],\n            outputs=node.output('Scores'))",
  "class MultiClassNMS():\n    support_opset_verision_range = (10, 16)\n    \"\"\"\n    Convert the paddle multiclass_nms to onnx op.\n    This op is get the select boxes from origin boxes.\n    \"\"\"\n\n    @classmethod\n    def opset_10(cls, graph, node, **kw):\n        if node.input_shape(\"BBoxes\", 0)[0] != 1:\n            logging.warning(\n                \"Due to the operator:{}, the converted ONNX model will only supports input[batch_size] == 1.\".\n                format(node.type))\n        scores = node.input('Scores', 0)\n        bboxes = node.input('BBoxes', 0)\n        num_class = node.input_shape('Scores', 0)[1]\n        if len(node.input_shape('Scores', 0)) == 2:\n            # inputs: scores & bboxes is lod tensor\n            scores = graph.make_node('Transpose', inputs=[scores], perm=[1, 0])\n            scores = mapper_helper.unsqueeze_helper(graph, scores, [0])\n            if graph.opset_version < 13:\n                scores_list = graph.make_node(\n                    'Split',\n                    inputs=scores,\n                    outputs=num_class,\n                    axis=1,\n                    split=[1] * num_class)\n            else:\n                split_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[1] * num_class)\n                scores_list = graph.make_node(\n                    \"Split\",\n                    inputs=[scores] + [split_const],\n                    outputs=num_class,\n                    axis=1)\n\n            bboxes = graph.make_node('Transpose', inputs=bboxes, perm=[1, 0, 2])\n            if graph.opset_version < 13:\n                bboxes_list = graph.make_node(\n                    'Split',\n                    inputs=bboxes,\n                    outputs=num_class,\n                    axis=0,\n                    split=[1] * num_class)\n            else:\n                split_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[1] * num_class)\n                bboxes_list = graph.make_node(\n                    \"Split\",\n                    inputs=[bboxes] + [split_const],\n                    outputs=num_class,\n                    axis=0)\n            bbox_ids = []\n            if not isinstance(scores_list, list):\n                scores_list = [scores_list]\n            if not isinstance(bboxes_list, list):\n                bboxes_list = [bboxes_list]\n            for i in range(num_class):\n                bbox_id = cls.nms(graph,\n                                  node,\n                                  scores_list[i],\n                                  bboxes_list[i],\n                                  class_id=i)\n                bbox_ids.append(bbox_id)\n            bbox_ids = graph.make_node('Concat', inputs=bbox_ids, axis=0)\n            const_shape = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=[1, -1, 4])\n            bboxes = graph.make_node('Reshape', inputs=[bboxes, const_shape])\n            cls.keep_top_k(\n                graph, node, bbox_ids, scores, bboxes, is_lod_input=True)\n        else:\n            bbox_ids = cls.nms(graph, node, scores, bboxes)\n            cls.keep_top_k(graph, node, bbox_ids, scores, bboxes)\n\n    @classmethod\n    def nms(cls, graph, node, scores, bboxes, class_id=None):\n        normalized = node.attr('normalized')\n        nms_top_k = node.attr('nms_top_k')\n        if node.type == 'matrix_nms':\n            iou_threshold = 0.5\n            logging.warning(\n                \"Operator:{} is not supported completely, so we use traditional\"\n                \" NMS (nms_theshold={}) to instead it, which introduce some difference.\".\n                format(node.type, str(iou_threshold)))\n        else:\n            iou_threshold = node.attr('nms_threshold')\n        if nms_top_k == -1:\n            nms_top_k = 100000\n\n        #convert the paddle attribute to onnx tensor\n        score_threshold = graph.make_node(\n            'Constant',\n            dtype=dtypes.ONNX.FLOAT,\n            value=[float(node.attr('score_threshold'))])\n        iou_threshold = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.FLOAT, value=[float(iou_threshold)])\n        nms_top_k = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[np.int64(nms_top_k)])\n\n        # the paddle data format is x1,y1,x2,y2\n        kwargs = {'center_point_box': 0}\n\n        if normalized:\n            select_bbox_indices = graph.make_node(\n                'NonMaxSuppression',\n                inputs=[\n                    bboxes, scores, nms_top_k, iou_threshold, score_threshold\n                ])\n        elif not normalized:\n            value_one = graph.make_node(\n                'Constant', dims=[1], dtype=dtypes.ONNX.FLOAT, value=1.0)\n            if graph.opset_version < 13:\n                new_bboxes = graph.make_node(\n                    'Split',\n                    inputs=[bboxes],\n                    outputs=4,\n                    axis=2,\n                    split=[1, 1, 1, 1])\n            else:\n                split_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[1, 1, 1, 1])\n                new_bboxes = graph.make_node(\n                    \"Split\", inputs=[bboxes] + [split_const], outputs=4, axis=2)\n            new_xmax = graph.make_node('Add', inputs=[new_bboxes[2], value_one])\n            new_ymax = graph.make_node('Add', inputs=[new_bboxes[3], value_one])\n            new_bboxes = graph.make_node(\n                'Concat',\n                inputs=[new_bboxes[0], new_bboxes[1], new_xmax, new_ymax],\n                axis=2)\n            select_bbox_indices = graph.make_node(\n                'NonMaxSuppression',\n                inputs=[\n                    new_bboxes, scores, nms_top_k, iou_threshold,\n                    score_threshold\n                ])\n\n        if class_id is not None and class_id != 0:\n            class_id = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=[0, class_id, 0])\n            class_id = mapper_helper.unsqueeze_helper(graph, class_id, [0])\n            select_bbox_indices = graph.make_node(\n                'Add', inputs=[select_bbox_indices, class_id])\n\n        return select_bbox_indices\n\n    @classmethod\n    def keep_top_k(cls,\n                   graph,\n                   node,\n                   select_bbox_indices,\n                   scores,\n                   bboxes,\n                   is_lod_input=False):\n        # step 1 nodes select the nms class\n        # create some const value to use\n        background = node.attr('background_label')\n        const_values = []\n        for value in [0, 1, 2, -1]:\n            const_value = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=[value])\n            const_values.append(const_value)\n\n        # In this code block, we will deocde the raw score data, reshape N * C * M to 1 * N*C*M\n        # and the same time, decode the select indices to 1 * D, gather the select_indices\n        class_id = graph.make_node(\n            'Gather', inputs=[select_bbox_indices, const_values[1]], axis=1)\n\n        squeezed_class_id = mapper_helper.squeeze_helper(graph, class_id, [1])\n\n        bbox_id = graph.make_node(\n            'Gather', inputs=[select_bbox_indices, const_values[2]], axis=1)\n\n        if background == 0:\n            nonzero = graph.make_node('NonZero', inputs=[squeezed_class_id])\n        else:\n            filter_cls_id = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT32, value=[background])\n            cast = graph.make_node(\n                'Cast', inputs=[squeezed_class_id], to=dtypes.ONNX.INT32)\n            filter_index = graph.make_node('Sub', inputs=[cast, filter_cls_id])\n            nonzero = graph.make_node('NonZero', inputs=[filter_index])\n\n        class_id = graph.make_node('Gather', inputs=[class_id, nonzero], axis=0)\n        class_id = graph.make_node(\n            'Cast', inputs=[class_id], to=dtypes.ONNX.INT64)\n\n        bbox_id = graph.make_node('Gather', inputs=[bbox_id, nonzero], axis=0)\n        bbox_id = graph.make_node(\n            'Cast', inputs=[bbox_id], to=dtypes.ONNX.INT64)\n\n        # get the shape of scores\n        shape_scores = graph.make_node('Shape', inputs=scores)\n\n        # gather the index: 2 shape of scores\n        class_num = graph.make_node(\n            'Gather', inputs=[shape_scores, const_values[2]], axis=0)\n\n        # reshape scores N * C * M to (N*C*M) * 1\n        scores = graph.make_node('Reshape', inputs=[scores, const_values[-1]])\n\n        # mul class * M\n        mul_classnum_boxnum = graph.make_node(\n            'Mul', inputs=[class_id, class_num])\n\n        # add class * M * index\n        add_class_indices = graph.make_node(\n            'Add', inputs=[mul_classnum_boxnum, bbox_id])\n\n        # Squeeze the indices to 1 dim\n        score_indices = mapper_helper.squeeze_helper(graph, add_class_indices,\n                                                     [0, 2])\n\n        # gather the data from flatten scores\n        scores = graph.make_node(\n            'Gather', inputs=[scores, score_indices], axis=0)\n\n        keep_top_k = node.attr('keep_top_k')\n        keep_top_k = graph.make_node(\n            'Constant',\n            dtype=dtypes.ONNX.INT64,\n            dims=[1, 1],\n            value=[node.attr('keep_top_k')])\n\n        # get min(topK, num_select)\n        shape_select_num = graph.make_node('Shape', inputs=[scores])\n        const_zero = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[0])\n        gather_select_num = graph.make_node(\n            'Gather', inputs=[shape_select_num, const_zero], axis=0)\n        unsqueeze_select_num = mapper_helper.unsqueeze_helper(\n            graph, gather_select_num, [0])\n\n        concat_topK_select_num = graph.make_node(\n            'Concat', inputs=[unsqueeze_select_num, keep_top_k], axis=0)\n        cast_concat_topK_select_num = graph.make_node(\n            'Cast', inputs=[concat_topK_select_num], to=6)\n        keep_top_k = graph.make_node(\n            'ReduceMin', inputs=[cast_concat_topK_select_num], keepdims=0)\n        # unsqueeze the indices to 1D tensor\n        keep_top_k = mapper_helper.unsqueeze_helper(graph, keep_top_k, [0])\n\n        # cast the indices to INT64\n        keep_top_k = graph.make_node('Cast', inputs=[keep_top_k], to=7)\n\n        # select topk scores  indices\n        keep_topk_scores, keep_topk_indices = graph.make_node(\n            'TopK', inputs=[scores, keep_top_k], outputs=2)\n\n        # gather topk label, scores, boxes\n        gather_topk_scores = graph.make_node(\n            'Gather', inputs=[scores, keep_topk_indices], axis=0)\n\n        gather_topk_class = graph.make_node(\n            'Gather', inputs=[class_id, keep_topk_indices], axis=1)\n\n        # gather the boxes need to gather the boxes id, then get boxes\n        if is_lod_input:\n            gather_topk_boxes_id = graph.make_node(\n                'Gather', [add_class_indices, keep_topk_indices], axis=1)\n        else:\n            gather_topk_boxes_id = graph.make_node(\n                'Gather', [bbox_id, keep_topk_indices], axis=1)\n\n        # squeeze the gather_topk_boxes_id to 1 dim\n        squeeze_topk_boxes_id = mapper_helper.squeeze_helper(\n            graph, gather_topk_boxes_id, [0, 2])\n\n        gather_select_boxes = graph.make_node(\n            'Gather', inputs=[bboxes, squeeze_topk_boxes_id], axis=1)\n\n        # concat the final result\n        # before concat need to cast the class to float\n        cast_topk_class = graph.make_node(\n            'Cast', inputs=[gather_topk_class], to=1)\n\n        unsqueeze_topk_scores = mapper_helper.unsqueeze_helper(\n            graph, gather_topk_scores, [0, 2])\n\n        inputs_concat_final_results = [\n            cast_topk_class, unsqueeze_topk_scores, gather_select_boxes\n        ]\n\n        sort_by_socre_results = graph.make_node(\n            'Concat', inputs=inputs_concat_final_results, axis=2)\n\n        # sort by class_id\n        squeeze_cast_topk_class = mapper_helper.squeeze_helper(\n            graph, cast_topk_class, [0, 2])\n\n        neg_squeeze_cast_topk_class = graph.make_node(\n            'Neg', inputs=[squeeze_cast_topk_class])\n\n        data, indices = graph.make_node(\n            'TopK', inputs=[neg_squeeze_cast_topk_class, keep_top_k], outputs=2)\n\n        concat_final_results = graph.make_node(\n            'Gather', inputs=[sort_by_socre_results, indices], axis=1)\n\n        concat_final_results = mapper_helper.squeeze_helper(\n            graph, concat_final_results, [0], node.output('Out'))\n\n        if node.type in ['multiclass_nms2', 'matrix_nms', 'multiclass_nms3']:\n            final_indices = mapper_helper.squeeze_helper(graph, bbox_id, [0],\n                                                         node.output('Index'))\n            if node.type in ['matrix_nms', 'multiclass_nms3']:\n                select_bboxes_shape = graph.make_node('Shape', inputs=[indices])\n                select_bboxes_shape1 = graph.make_node(\n                    'Cast', inputs=[select_bboxes_shape], to=dtypes.ONNX.INT32)\n                indices = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[0])\n                rois_num = None\n                if 'NmsRoisNum' in node.outputs:\n                    rois_num = node.output('NmsRoisNum')\n                elif 'RoisNum' in node.outputs:\n                    rois_num = node.output('RoisNum')\n                if rois_num is not None:\n                    graph.make_node(\n                        \"Gather\",\n                        inputs=[select_bboxes_shape1, indices],\n                        outputs=rois_num)",
  "def opset_10(cls, graph, node, **kw):\n        if node.input_shape(\"BBoxes\", 0)[0] != 1:\n            logging.warning(\n                \"Due to the operator:{}, the converted ONNX model will only supports input[batch_size] == 1.\".\n                format(node.type))\n        scores = node.input('Scores', 0)\n        bboxes = node.input('BBoxes', 0)\n        num_class = node.input_shape('Scores', 0)[1]\n        if len(node.input_shape('Scores', 0)) == 2:\n            # inputs: scores & bboxes is lod tensor\n            scores = graph.make_node('Transpose', inputs=[scores], perm=[1, 0])\n            scores = mapper_helper.unsqueeze_helper(graph, scores, [0])\n            if graph.opset_version < 13:\n                scores_list = graph.make_node(\n                    'Split',\n                    inputs=scores,\n                    outputs=num_class,\n                    axis=1,\n                    split=[1] * num_class)\n            else:\n                split_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[1] * num_class)\n                scores_list = graph.make_node(\n                    \"Split\",\n                    inputs=[scores] + [split_const],\n                    outputs=num_class,\n                    axis=1)\n\n            bboxes = graph.make_node('Transpose', inputs=bboxes, perm=[1, 0, 2])\n            if graph.opset_version < 13:\n                bboxes_list = graph.make_node(\n                    'Split',\n                    inputs=bboxes,\n                    outputs=num_class,\n                    axis=0,\n                    split=[1] * num_class)\n            else:\n                split_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[1] * num_class)\n                bboxes_list = graph.make_node(\n                    \"Split\",\n                    inputs=[bboxes] + [split_const],\n                    outputs=num_class,\n                    axis=0)\n            bbox_ids = []\n            if not isinstance(scores_list, list):\n                scores_list = [scores_list]\n            if not isinstance(bboxes_list, list):\n                bboxes_list = [bboxes_list]\n            for i in range(num_class):\n                bbox_id = cls.nms(graph,\n                                  node,\n                                  scores_list[i],\n                                  bboxes_list[i],\n                                  class_id=i)\n                bbox_ids.append(bbox_id)\n            bbox_ids = graph.make_node('Concat', inputs=bbox_ids, axis=0)\n            const_shape = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=[1, -1, 4])\n            bboxes = graph.make_node('Reshape', inputs=[bboxes, const_shape])\n            cls.keep_top_k(\n                graph, node, bbox_ids, scores, bboxes, is_lod_input=True)\n        else:\n            bbox_ids = cls.nms(graph, node, scores, bboxes)\n            cls.keep_top_k(graph, node, bbox_ids, scores, bboxes)",
  "def nms(cls, graph, node, scores, bboxes, class_id=None):\n        normalized = node.attr('normalized')\n        nms_top_k = node.attr('nms_top_k')\n        if node.type == 'matrix_nms':\n            iou_threshold = 0.5\n            logging.warning(\n                \"Operator:{} is not supported completely, so we use traditional\"\n                \" NMS (nms_theshold={}) to instead it, which introduce some difference.\".\n                format(node.type, str(iou_threshold)))\n        else:\n            iou_threshold = node.attr('nms_threshold')\n        if nms_top_k == -1:\n            nms_top_k = 100000\n\n        #convert the paddle attribute to onnx tensor\n        score_threshold = graph.make_node(\n            'Constant',\n            dtype=dtypes.ONNX.FLOAT,\n            value=[float(node.attr('score_threshold'))])\n        iou_threshold = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.FLOAT, value=[float(iou_threshold)])\n        nms_top_k = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[np.int64(nms_top_k)])\n\n        # the paddle data format is x1,y1,x2,y2\n        kwargs = {'center_point_box': 0}\n\n        if normalized:\n            select_bbox_indices = graph.make_node(\n                'NonMaxSuppression',\n                inputs=[\n                    bboxes, scores, nms_top_k, iou_threshold, score_threshold\n                ])\n        elif not normalized:\n            value_one = graph.make_node(\n                'Constant', dims=[1], dtype=dtypes.ONNX.FLOAT, value=1.0)\n            if graph.opset_version < 13:\n                new_bboxes = graph.make_node(\n                    'Split',\n                    inputs=[bboxes],\n                    outputs=4,\n                    axis=2,\n                    split=[1, 1, 1, 1])\n            else:\n                split_const = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[1, 1, 1, 1])\n                new_bboxes = graph.make_node(\n                    \"Split\", inputs=[bboxes] + [split_const], outputs=4, axis=2)\n            new_xmax = graph.make_node('Add', inputs=[new_bboxes[2], value_one])\n            new_ymax = graph.make_node('Add', inputs=[new_bboxes[3], value_one])\n            new_bboxes = graph.make_node(\n                'Concat',\n                inputs=[new_bboxes[0], new_bboxes[1], new_xmax, new_ymax],\n                axis=2)\n            select_bbox_indices = graph.make_node(\n                'NonMaxSuppression',\n                inputs=[\n                    new_bboxes, scores, nms_top_k, iou_threshold,\n                    score_threshold\n                ])\n\n        if class_id is not None and class_id != 0:\n            class_id = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=[0, class_id, 0])\n            class_id = mapper_helper.unsqueeze_helper(graph, class_id, [0])\n            select_bbox_indices = graph.make_node(\n                'Add', inputs=[select_bbox_indices, class_id])\n\n        return select_bbox_indices",
  "def keep_top_k(cls,\n                   graph,\n                   node,\n                   select_bbox_indices,\n                   scores,\n                   bboxes,\n                   is_lod_input=False):\n        # step 1 nodes select the nms class\n        # create some const value to use\n        background = node.attr('background_label')\n        const_values = []\n        for value in [0, 1, 2, -1]:\n            const_value = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT64, value=[value])\n            const_values.append(const_value)\n\n        # In this code block, we will deocde the raw score data, reshape N * C * M to 1 * N*C*M\n        # and the same time, decode the select indices to 1 * D, gather the select_indices\n        class_id = graph.make_node(\n            'Gather', inputs=[select_bbox_indices, const_values[1]], axis=1)\n\n        squeezed_class_id = mapper_helper.squeeze_helper(graph, class_id, [1])\n\n        bbox_id = graph.make_node(\n            'Gather', inputs=[select_bbox_indices, const_values[2]], axis=1)\n\n        if background == 0:\n            nonzero = graph.make_node('NonZero', inputs=[squeezed_class_id])\n        else:\n            filter_cls_id = graph.make_node(\n                'Constant', dtype=dtypes.ONNX.INT32, value=[background])\n            cast = graph.make_node(\n                'Cast', inputs=[squeezed_class_id], to=dtypes.ONNX.INT32)\n            filter_index = graph.make_node('Sub', inputs=[cast, filter_cls_id])\n            nonzero = graph.make_node('NonZero', inputs=[filter_index])\n\n        class_id = graph.make_node('Gather', inputs=[class_id, nonzero], axis=0)\n        class_id = graph.make_node(\n            'Cast', inputs=[class_id], to=dtypes.ONNX.INT64)\n\n        bbox_id = graph.make_node('Gather', inputs=[bbox_id, nonzero], axis=0)\n        bbox_id = graph.make_node(\n            'Cast', inputs=[bbox_id], to=dtypes.ONNX.INT64)\n\n        # get the shape of scores\n        shape_scores = graph.make_node('Shape', inputs=scores)\n\n        # gather the index: 2 shape of scores\n        class_num = graph.make_node(\n            'Gather', inputs=[shape_scores, const_values[2]], axis=0)\n\n        # reshape scores N * C * M to (N*C*M) * 1\n        scores = graph.make_node('Reshape', inputs=[scores, const_values[-1]])\n\n        # mul class * M\n        mul_classnum_boxnum = graph.make_node(\n            'Mul', inputs=[class_id, class_num])\n\n        # add class * M * index\n        add_class_indices = graph.make_node(\n            'Add', inputs=[mul_classnum_boxnum, bbox_id])\n\n        # Squeeze the indices to 1 dim\n        score_indices = mapper_helper.squeeze_helper(graph, add_class_indices,\n                                                     [0, 2])\n\n        # gather the data from flatten scores\n        scores = graph.make_node(\n            'Gather', inputs=[scores, score_indices], axis=0)\n\n        keep_top_k = node.attr('keep_top_k')\n        keep_top_k = graph.make_node(\n            'Constant',\n            dtype=dtypes.ONNX.INT64,\n            dims=[1, 1],\n            value=[node.attr('keep_top_k')])\n\n        # get min(topK, num_select)\n        shape_select_num = graph.make_node('Shape', inputs=[scores])\n        const_zero = graph.make_node(\n            'Constant', dtype=dtypes.ONNX.INT64, value=[0])\n        gather_select_num = graph.make_node(\n            'Gather', inputs=[shape_select_num, const_zero], axis=0)\n        unsqueeze_select_num = mapper_helper.unsqueeze_helper(\n            graph, gather_select_num, [0])\n\n        concat_topK_select_num = graph.make_node(\n            'Concat', inputs=[unsqueeze_select_num, keep_top_k], axis=0)\n        cast_concat_topK_select_num = graph.make_node(\n            'Cast', inputs=[concat_topK_select_num], to=6)\n        keep_top_k = graph.make_node(\n            'ReduceMin', inputs=[cast_concat_topK_select_num], keepdims=0)\n        # unsqueeze the indices to 1D tensor\n        keep_top_k = mapper_helper.unsqueeze_helper(graph, keep_top_k, [0])\n\n        # cast the indices to INT64\n        keep_top_k = graph.make_node('Cast', inputs=[keep_top_k], to=7)\n\n        # select topk scores  indices\n        keep_topk_scores, keep_topk_indices = graph.make_node(\n            'TopK', inputs=[scores, keep_top_k], outputs=2)\n\n        # gather topk label, scores, boxes\n        gather_topk_scores = graph.make_node(\n            'Gather', inputs=[scores, keep_topk_indices], axis=0)\n\n        gather_topk_class = graph.make_node(\n            'Gather', inputs=[class_id, keep_topk_indices], axis=1)\n\n        # gather the boxes need to gather the boxes id, then get boxes\n        if is_lod_input:\n            gather_topk_boxes_id = graph.make_node(\n                'Gather', [add_class_indices, keep_topk_indices], axis=1)\n        else:\n            gather_topk_boxes_id = graph.make_node(\n                'Gather', [bbox_id, keep_topk_indices], axis=1)\n\n        # squeeze the gather_topk_boxes_id to 1 dim\n        squeeze_topk_boxes_id = mapper_helper.squeeze_helper(\n            graph, gather_topk_boxes_id, [0, 2])\n\n        gather_select_boxes = graph.make_node(\n            'Gather', inputs=[bboxes, squeeze_topk_boxes_id], axis=1)\n\n        # concat the final result\n        # before concat need to cast the class to float\n        cast_topk_class = graph.make_node(\n            'Cast', inputs=[gather_topk_class], to=1)\n\n        unsqueeze_topk_scores = mapper_helper.unsqueeze_helper(\n            graph, gather_topk_scores, [0, 2])\n\n        inputs_concat_final_results = [\n            cast_topk_class, unsqueeze_topk_scores, gather_select_boxes\n        ]\n\n        sort_by_socre_results = graph.make_node(\n            'Concat', inputs=inputs_concat_final_results, axis=2)\n\n        # sort by class_id\n        squeeze_cast_topk_class = mapper_helper.squeeze_helper(\n            graph, cast_topk_class, [0, 2])\n\n        neg_squeeze_cast_topk_class = graph.make_node(\n            'Neg', inputs=[squeeze_cast_topk_class])\n\n        data, indices = graph.make_node(\n            'TopK', inputs=[neg_squeeze_cast_topk_class, keep_top_k], outputs=2)\n\n        concat_final_results = graph.make_node(\n            'Gather', inputs=[sort_by_socre_results, indices], axis=1)\n\n        concat_final_results = mapper_helper.squeeze_helper(\n            graph, concat_final_results, [0], node.output('Out'))\n\n        if node.type in ['multiclass_nms2', 'matrix_nms', 'multiclass_nms3']:\n            final_indices = mapper_helper.squeeze_helper(graph, bbox_id, [0],\n                                                         node.output('Index'))\n            if node.type in ['matrix_nms', 'multiclass_nms3']:\n                select_bboxes_shape = graph.make_node('Shape', inputs=[indices])\n                select_bboxes_shape1 = graph.make_node(\n                    'Cast', inputs=[select_bboxes_shape], to=dtypes.ONNX.INT32)\n                indices = graph.make_node(\n                    'Constant', dtype=dtypes.ONNX.INT64, value=[0])\n                rois_num = None\n                if 'NmsRoisNum' in node.outputs:\n                    rois_num = node.output('NmsRoisNum')\n                elif 'RoisNum' in node.outputs:\n                    rois_num = node.output('RoisNum')\n                if rois_num is not None:\n                    graph.make_node(\n                        \"Gather\",\n                        inputs=[select_bboxes_shape1, indices],\n                        outputs=rois_num)",
  "def expand_aspect_rations(input_aspect_ratior, flip):\n    expsilon = 1e-6\n    output_ratios = [1.0]\n    for input_ratio in input_aspect_ratior:\n        already_exis = False\n        for output_ratio in output_ratios:\n            if abs(input_ratio - output_ratio) < expsilon:\n                already_exis = True\n                break\n        if already_exis == False:\n            output_ratios.append(input_ratio)\n            if flip:\n                output_ratios.append(1.0 / input_ratio)\n    return output_ratios",
  "class PriorBox():\n    \"\"\"\n    In this function, use the attribute to get the prior box, because we do not use\n    the image data and feature map, wo could the python code to create the varaible,\n    and to create the onnx tensor as output.\n    \"\"\"\n    support_opset_verison_range = (1, 12)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        flip = bool(node.attr('flip'))\n        clip = bool(node.attr('clip'))\n        min_max_aspect_ratios_order = bool(\n            node.attr('min_max_aspect_ratios_order'))\n        min_sizes = [float(size) for size in node.attr('min_sizes')]\n        max_sizes = [float(size) for size in node.attr('max_sizes')]\n        if isinstance(node.attr('aspect_ratios'), list):\n            aspect_ratios = [\n                float(ratio) for ratio in node.attr('aspect_ratios')\n            ]\n        else:\n            aspect_ratios = [float(node.attr('aspect_ratios'))]\n        variances = [float(var) for var in node.attr('variances')]\n        # set min_max_aspect_ratios_order = false\n        output_ratios = expand_aspect_rations(aspect_ratios, flip)\n\n        step_w = float(node.attr('step_w'))\n        step_h = float(node.attr('step_h'))\n        offset = float(node.attr('offset'))\n\n        input_shape = node.input_shape('Input', 0)\n        image_shape = node.input_shape('Image', 0)\n\n        img_width = image_shape[3]\n        img_height = image_shape[2]\n        feature_width = input_shape[3]\n        feature_height = input_shape[2]\n        assert img_width > 0 and img_height > 0, require_fixed_shape(\n            cls.__name__)\n\n        step_width = 1.0\n        step_height = 1.0\n\n        if step_w == 0.0 or step_h == 0.0:\n            step_w = float(img_width / feature_width)\n            step_h = float(img_height / feature_height)\n\n        num_priors = len(output_ratios) * len(min_sizes)\n        if len(max_sizes) > 0:\n            num_priors += len(max_sizes)\n        out_dim = (feature_height, feature_width, num_priors, 4)\n        out_boxes = np.zeros(out_dim).astype('float32')\n        out_var = np.zeros(out_dim).astype('float32')\n\n        idx = 0\n        for h in range(feature_height):\n            for w in range(feature_width):\n                c_x = (w + offset) * step_w\n                c_y = (h + offset) * step_h\n                idx = 0\n                for s in range(len(min_sizes)):\n                    min_size = min_sizes[s]\n                    if not min_max_aspect_ratios_order:\n                        # rest of priors\n                        for r in range(len(output_ratios)):\n                            ar = output_ratios[r]\n                            c_w = min_size * math.sqrt(ar) / 2\n                            c_h = (min_size / math.sqrt(ar)) / 2\n                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,\n                                                       (c_y - c_h) / img_height,\n                                                       (c_x + c_w) / img_width,\n                                                       (c_y + c_h) / img_height]\n                            idx += 1\n\n                        if len(max_sizes) > 0:\n                            max_size = max_sizes[s]\n                            # second prior: aspect_ratio = 1,\n                            c_w = c_h = math.sqrt(min_size * max_size) / 2\n                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,\n                                                       (c_y - c_h) / img_height,\n                                                       (c_x + c_w) / img_width,\n                                                       (c_y + c_h) / img_height]\n                            idx += 1\n                    else:\n                        c_w = c_h = min_size / 2.\n                        out_boxes[h, w, idx, :] = [\n                            (c_x - c_w) / img_width, (c_y - c_h) / img_height,\n                            (c_x + c_w) / img_width, (c_y + c_h) / img_height\n                        ]\n                        idx += 1\n                        if len(max_sizes) > 0:\n                            max_size = max_sizes[s]\n                            # second prior: aspect_ratio = 1,\n                            c_w = c_h = math.sqrt(min_size * max_size) / 2\n                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,\n                                                       (c_y - c_h) / img_height,\n                                                       (c_x + c_w) / img_width,\n                                                       (c_y + c_h) / img_height]\n                            idx += 1\n\n                        # rest of priors\n                        for r in range(len(output_ratios)):\n                            ar = output_ratios[r]\n                            if abs(ar - 1.) < 1e-6:\n                                continue\n                            c_w = min_size * math.sqrt(ar) / 2\n                            c_h = (min_size / math.sqrt(ar)) / 2\n                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,\n                                                       (c_y - c_h) / img_height,\n                                                       (c_x + c_w) / img_width,\n                                                       (c_y + c_h) / img_height]\n                            idx += 1\n\n        if clip:\n            out_boxes = np.clip(out_boxes, 0.0, 1.0)\n        # set the variance.\n        out_var = np.tile(variances,\n                          (feature_height, feature_width, num_priors, 1))\n\n        #make node that\n\n        node_boxes = graph.make_node(\n            'Constant',\n            inputs=[],\n            outputs=node.output('Boxes'),\n            dtype=dtypes.ONNX.FLOAT,\n            dims=out_boxes.shape,\n            value=out_boxes.flatten().tolist())\n\n        node_vars = graph.make_node(\n            'Constant',\n            inputs=[],\n            outputs=node.output('Variances'),\n            dtype=dtypes.ONNX.FLOAT,\n            dims=out_var.shape,\n            value=out_var.flatten().tolist())",
  "def opset_9(cls, graph, node, **kw):\n        flip = bool(node.attr('flip'))\n        clip = bool(node.attr('clip'))\n        min_max_aspect_ratios_order = bool(\n            node.attr('min_max_aspect_ratios_order'))\n        min_sizes = [float(size) for size in node.attr('min_sizes')]\n        max_sizes = [float(size) for size in node.attr('max_sizes')]\n        if isinstance(node.attr('aspect_ratios'), list):\n            aspect_ratios = [\n                float(ratio) for ratio in node.attr('aspect_ratios')\n            ]\n        else:\n            aspect_ratios = [float(node.attr('aspect_ratios'))]\n        variances = [float(var) for var in node.attr('variances')]\n        # set min_max_aspect_ratios_order = false\n        output_ratios = expand_aspect_rations(aspect_ratios, flip)\n\n        step_w = float(node.attr('step_w'))\n        step_h = float(node.attr('step_h'))\n        offset = float(node.attr('offset'))\n\n        input_shape = node.input_shape('Input', 0)\n        image_shape = node.input_shape('Image', 0)\n\n        img_width = image_shape[3]\n        img_height = image_shape[2]\n        feature_width = input_shape[3]\n        feature_height = input_shape[2]\n        assert img_width > 0 and img_height > 0, require_fixed_shape(\n            cls.__name__)\n\n        step_width = 1.0\n        step_height = 1.0\n\n        if step_w == 0.0 or step_h == 0.0:\n            step_w = float(img_width / feature_width)\n            step_h = float(img_height / feature_height)\n\n        num_priors = len(output_ratios) * len(min_sizes)\n        if len(max_sizes) > 0:\n            num_priors += len(max_sizes)\n        out_dim = (feature_height, feature_width, num_priors, 4)\n        out_boxes = np.zeros(out_dim).astype('float32')\n        out_var = np.zeros(out_dim).astype('float32')\n\n        idx = 0\n        for h in range(feature_height):\n            for w in range(feature_width):\n                c_x = (w + offset) * step_w\n                c_y = (h + offset) * step_h\n                idx = 0\n                for s in range(len(min_sizes)):\n                    min_size = min_sizes[s]\n                    if not min_max_aspect_ratios_order:\n                        # rest of priors\n                        for r in range(len(output_ratios)):\n                            ar = output_ratios[r]\n                            c_w = min_size * math.sqrt(ar) / 2\n                            c_h = (min_size / math.sqrt(ar)) / 2\n                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,\n                                                       (c_y - c_h) / img_height,\n                                                       (c_x + c_w) / img_width,\n                                                       (c_y + c_h) / img_height]\n                            idx += 1\n\n                        if len(max_sizes) > 0:\n                            max_size = max_sizes[s]\n                            # second prior: aspect_ratio = 1,\n                            c_w = c_h = math.sqrt(min_size * max_size) / 2\n                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,\n                                                       (c_y - c_h) / img_height,\n                                                       (c_x + c_w) / img_width,\n                                                       (c_y + c_h) / img_height]\n                            idx += 1\n                    else:\n                        c_w = c_h = min_size / 2.\n                        out_boxes[h, w, idx, :] = [\n                            (c_x - c_w) / img_width, (c_y - c_h) / img_height,\n                            (c_x + c_w) / img_width, (c_y + c_h) / img_height\n                        ]\n                        idx += 1\n                        if len(max_sizes) > 0:\n                            max_size = max_sizes[s]\n                            # second prior: aspect_ratio = 1,\n                            c_w = c_h = math.sqrt(min_size * max_size) / 2\n                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,\n                                                       (c_y - c_h) / img_height,\n                                                       (c_x + c_w) / img_width,\n                                                       (c_y + c_h) / img_height]\n                            idx += 1\n\n                        # rest of priors\n                        for r in range(len(output_ratios)):\n                            ar = output_ratios[r]\n                            if abs(ar - 1.) < 1e-6:\n                                continue\n                            c_w = min_size * math.sqrt(ar) / 2\n                            c_h = (min_size / math.sqrt(ar)) / 2\n                            out_boxes[h, w, idx, :] = [(c_x - c_w) / img_width,\n                                                       (c_y - c_h) / img_height,\n                                                       (c_x + c_w) / img_width,\n                                                       (c_y + c_h) / img_height]\n                            idx += 1\n\n        if clip:\n            out_boxes = np.clip(out_boxes, 0.0, 1.0)\n        # set the variance.\n        out_var = np.tile(variances,\n                          (feature_height, feature_width, num_priors, 1))\n\n        #make node that\n\n        node_boxes = graph.make_node(\n            'Constant',\n            inputs=[],\n            outputs=node.output('Boxes'),\n            dtype=dtypes.ONNX.FLOAT,\n            dims=out_boxes.shape,\n            value=out_boxes.flatten().tolist())\n\n        node_vars = graph.make_node(\n            'Constant',\n            inputs=[],\n            outputs=node.output('Variances'),\n            dtype=dtypes.ONNX.FLOAT,\n            dims=out_var.shape,\n            value=out_var.flatten().tolist())",
  "class DensityPriorBox():\n    \"\"\"\n    In this function, use the attribute to get the prior box, because we do not use\n    the image data and feature map, wo could the python code to create the varaible,\n    and to create the onnx tensor as output.\n    \"\"\"\n    support_opset_verison_range = (1, 12)\n\n    @classmethod\n    def opset_9(cls, graph, node, **kw):\n        clip = bool(node.attr('clip'))\n        densities = node.attr('densities')\n        fixed_ratios = node.attr('fixed_ratios')\n        fixed_sizes = node.attr('fixed_sizes')\n        flatten_to_2d = bool(node.attr('flatten_to_2d'))\n        offset = node.attr('offset')\n        step_h = node.attr('step_h')\n        step_w = node.attr('step_w')\n        variances = node.attr('variances')\n\n        input_shape = node.input_shape('Input', 0)\n        image_shape = node.input_shape('Image', 0)\n\n        img_width = image_shape[3]\n        img_height = image_shape[2]\n        feature_width = input_shape[3]\n        feature_height = input_shape[2]\n\n        assert img_width > 0 and img_height > 0, require_fixed_shape(\n            cls.__name__)\n\n        if step_w == 0.0 or step_h == 0.0:\n            step_w = float(img_width / feature_width)\n            step_h = float(img_height / feature_height)\n\n        num_priors = 0\n        if len(fixed_sizes) > 0 and len(densities) > 0:\n            for density in densities:\n                if len(fixed_ratios) > 0:\n                    num_priors += len(fixed_ratios) * (pow(density, 2))\n\n        out_dim = (feature_height, feature_width, num_priors, 4)\n        out_boxes = np.zeros(out_dim).astype('float32')\n        out_var = np.zeros(out_dim).astype('float32')\n        step_average = int((step_w + step_h) * 0.5)\n\n        for h in range(feature_height):\n            for w in range(feature_width):\n                c_x = (w + offset) * step_w\n                c_y = (h + offset) * step_h\n                idx = 0\n\n                for density, fixed_size in zip(densities, fixed_sizes):\n                    if (len(fixed_ratios) > 0):\n                        for ar in fixed_ratios:\n                            shift = int(step_average / density)\n                            box_width_ratio = fixed_size * math.sqrt(ar)\n                            box_height_ratio = fixed_size / math.sqrt(ar)\n                            for di in range(density):\n                                for dj in range(density):\n                                    c_x_temp = c_x - step_average / 2.0 + shift / 2.0 + dj * shift\n                                    c_y_temp = c_y - step_average / 2.0 + shift / 2.0 + di * shift\n                                    out_boxes[h, w, idx, :] = [\n                                        max((c_x_temp - box_width_ratio / 2.0) /\n                                            img_width, 0),\n                                        max((c_y_temp - box_height_ratio / 2.0)\n                                            / img_height, 0),\n                                        min((c_x_temp + box_width_ratio / 2.0) /\n                                            img_width, 1),\n                                        min((c_y_temp + box_height_ratio / 2.0)\n                                            / img_height, 1)\n                                    ]\n                                    idx += 1\n\n        if clip:\n            out_boxes = np.clip(out_boxes, 0.0, 1.0)\n        # set the variance.\n        out_var = np.tile(variances,\n                          (feature_height, feature_width, num_priors, 1))\n\n        if flatten_to_2d:\n            out_boxes = out_boxes.reshape((-1, 4))\n            out_var = out_var.reshape((-1, 4))\n\n        #make node that\n\n        node_boxes = graph.make_node(\n            'Constant',\n            inputs=[],\n            outputs=node.output('Boxes'),\n            dtype=dtypes.ONNX.FLOAT,\n            dims=out_boxes.shape,\n            value=out_boxes.flatten().tolist())\n\n        node_vars = graph.make_node(\n            'Constant',\n            inputs=[],\n            outputs=node.output('Variances'),\n            dtype=dtypes.ONNX.FLOAT,\n            dims=out_var.shape,\n            value=out_var.flatten().tolist())",
  "def opset_9(cls, graph, node, **kw):\n        clip = bool(node.attr('clip'))\n        densities = node.attr('densities')\n        fixed_ratios = node.attr('fixed_ratios')\n        fixed_sizes = node.attr('fixed_sizes')\n        flatten_to_2d = bool(node.attr('flatten_to_2d'))\n        offset = node.attr('offset')\n        step_h = node.attr('step_h')\n        step_w = node.attr('step_w')\n        variances = node.attr('variances')\n\n        input_shape = node.input_shape('Input', 0)\n        image_shape = node.input_shape('Image', 0)\n\n        img_width = image_shape[3]\n        img_height = image_shape[2]\n        feature_width = input_shape[3]\n        feature_height = input_shape[2]\n\n        assert img_width > 0 and img_height > 0, require_fixed_shape(\n            cls.__name__)\n\n        if step_w == 0.0 or step_h == 0.0:\n            step_w = float(img_width / feature_width)\n            step_h = float(img_height / feature_height)\n\n        num_priors = 0\n        if len(fixed_sizes) > 0 and len(densities) > 0:\n            for density in densities:\n                if len(fixed_ratios) > 0:\n                    num_priors += len(fixed_ratios) * (pow(density, 2))\n\n        out_dim = (feature_height, feature_width, num_priors, 4)\n        out_boxes = np.zeros(out_dim).astype('float32')\n        out_var = np.zeros(out_dim).astype('float32')\n        step_average = int((step_w + step_h) * 0.5)\n\n        for h in range(feature_height):\n            for w in range(feature_width):\n                c_x = (w + offset) * step_w\n                c_y = (h + offset) * step_h\n                idx = 0\n\n                for density, fixed_size in zip(densities, fixed_sizes):\n                    if (len(fixed_ratios) > 0):\n                        for ar in fixed_ratios:\n                            shift = int(step_average / density)\n                            box_width_ratio = fixed_size * math.sqrt(ar)\n                            box_height_ratio = fixed_size / math.sqrt(ar)\n                            for di in range(density):\n                                for dj in range(density):\n                                    c_x_temp = c_x - step_average / 2.0 + shift / 2.0 + dj * shift\n                                    c_y_temp = c_y - step_average / 2.0 + shift / 2.0 + di * shift\n                                    out_boxes[h, w, idx, :] = [\n                                        max((c_x_temp - box_width_ratio / 2.0) /\n                                            img_width, 0),\n                                        max((c_y_temp - box_height_ratio / 2.0)\n                                            / img_height, 0),\n                                        min((c_x_temp + box_width_ratio / 2.0) /\n                                            img_width, 1),\n                                        min((c_y_temp + box_height_ratio / 2.0)\n                                            / img_height, 1)\n                                    ]\n                                    idx += 1\n\n        if clip:\n            out_boxes = np.clip(out_boxes, 0.0, 1.0)\n        # set the variance.\n        out_var = np.tile(variances,\n                          (feature_height, feature_width, num_priors, 1))\n\n        if flatten_to_2d:\n            out_boxes = out_boxes.reshape((-1, 4))\n            out_var = out_var.reshape((-1, 4))\n\n        #make node that\n\n        node_boxes = graph.make_node(\n            'Constant',\n            inputs=[],\n            outputs=node.output('Boxes'),\n            dtype=dtypes.ONNX.FLOAT,\n            dims=out_boxes.shape,\n            value=out_boxes.flatten().tolist())\n\n        node_vars = graph.make_node(\n            'Constant',\n            inputs=[],\n            outputs=node.output('Variances'),\n            dtype=dtypes.ONNX.FLOAT,\n            dims=out_var.shape,\n            value=out_var.flatten().tolist())",
  "class BoxCoder():\n    \"\"\"\n    we use the decode the prior box to target box,\n    we just use the decode mode to transform this op.\n    \"\"\"\n    support_opset_verison_range = (7, 12)\n\n    @classmethod\n    def opset_7(cls, graph, node, **kw):\n        input_names = node.input_names\n\n        t_size = node.input_shape('TargetBox', 0)\n        p_size = node.input_shape('PriorBox', 0)\n\n        # get the outout_name\n        result_name = node.output('OutputBox', 0)\n        # n is size of batch, m is boxes num of targe_boxes\n        n = t_size[0]\n        m = t_size[0]\n\n        axis = int(node.attr('axis'))\n\n        #norm\n        norm = bool(node.attr('box_normalized'))\n\n        name_slice_x1 = node.output('OutputBox')[0] + \"@x1\"\n        name_slice_y1 = node.output('OutputBox')[0] + \"@y1\"\n        name_slice_x2 = node.output('OutputBox')[0] + \"@x2\"\n        name_slice_y2 = node.output('OutputBox')[0] + \"@y2\"\n\n        #make onnx tensor to save the intermeidate reslut\n        name_slice_indices = [\n            [node.output('OutputBox')[0] + \"@slice_\" + str(i)]\n            for i in range(1, 3)\n        ]\n        node_slice_indices = [None for i in range(1, 3)]\n\n        # create the range(0, 4) const data to slice\n        for i in range(1, 3):\n            tmp_node = graph.make_node(\n                'Constant',\n                inputs=[],\n                outputs=name_slice_indices[i - 1],\n                dtype=dtypes.ONNX.FLOAT,\n                dims=(),\n                value=[i])\n        # make node split data\n        name_box_split = [\n            name_slice_x1, name_slice_y1, name_slice_x2, name_slice_y2\n        ]\n        split_shape = list(p_size)\n        split_shape[-1] = 1\n\n        node_split_prior_node = graph.make_node(\n            'Split',\n            inputs=node.input('PriorBox'),\n            outputs=name_box_split,\n            axis=1)\n\n        # make node get centor node for decode\n        final_outputs_vars = []\n        if not norm:\n            name_centor_w_tmp = [node.output('OutputBox')[0] + \"@centor_w_tmp\"]\n            name_centor_h_tmp = [node.output('OutputBox')[0] + \"@centor_h_tmp\"]\n            node_centor_w_tmp = None\n            node_centor_h_tmp = None\n            name_centor_tmp_list = [name_centor_w_tmp, name_centor_h_tmp]\n            node_centor_tmp_list = [node_centor_w_tmp, node_centor_h_tmp]\n\n            count = 2\n            for (name, op_node) in zip(name_centor_tmp_list,\n                                       node_centor_tmp_list):\n                tmp_node = graph.make_node('Add',\n                       inputs=[node.output('OutputBox')[0] + \"@slice_\" + str(1)]\\\n                           + [name_box_split[count]],\n                       outputs=name)\n                count = count + 1\n        if not norm:\n            inputs_sub = [[name_centor_w_tmp[0], name_box_split[0]],\n                          [name_centor_h_tmp[0], name_box_split[1]]]\n        else:\n            inputs_sub = [[name_box_split[2], name_box_split[0]],\n                          [name_box_split[3], name_box_split[1]]]\n        outputs_sub = [result_name + \"@pb_w\", result_name + \"@pb_h\"]\n        for i in range(0, 2):\n            tmp_node = graph.make_node(\n                'Sub', inputs=inputs_sub[i], outputs=[outputs_sub[i]])\n        # according to prior_box height and weight to get centor x, y\n        name_half_value = [result_name + \"@half_value\"]\n        node_half_value = graph.make_node(\n            'Constant',\n            inputs=[],\n            outputs=name_half_value,\n            dtype=dtypes.ONNX.FLOAT,\n            dims=(),\n            value=[0.5])\n        outputs_half_wh = [[result_name + \"@pb_w_half\"],\n                           [result_name + \"@pb_h_half\"]]\n        inputs_half_wh = [[result_name + \"@pb_w\", name_half_value[0]],\n                          [result_name + \"@pb_h\", name_half_value[0]]]\n\n        for i in range(0, 2):\n            tmp_node = graph.make_node(\n                'Mul', inputs=inputs_half_wh[i], outputs=outputs_half_wh[i])\n\n        inputs_centor_xy = [[outputs_half_wh[0][0], name_slice_x1],\n                            [outputs_half_wh[1][0], name_slice_y1]]\n\n        outputs_centor_xy = [[result_name + \"@pb_x\"], [result_name + \"@pb_y\"]]\n\n        # final calc the centor x ,y\n        for i in range(0, 2):\n            tmp_node = graph.make_node(\n                'Add', inputs=inputs_centor_xy[i], outputs=outputs_centor_xy[i])\n        # reshape the data\n        shape = (1, split_shape[0]) if axis == 0 else (split_shape[0], 1)\n\n        # need to reshape the data\n        inputs_transpose_pb = [\n            [result_name + \"@pb_w\"],\n            [result_name + \"@pb_h\"],\n            [result_name + \"@pb_x\"],\n            [result_name + \"@pb_y\"],\n        ]\n        outputs_transpose_pb = [\n            [result_name + \"@pb_w_transpose\"],\n            [result_name + \"@pb_h_transpose\"],\n            [result_name + \"@pb_x_transpose\"],\n            [result_name + \"@pb_y_transpose\"],\n        ]\n        if axis == 0:\n            name_reshape_pb = [result_name + \"@pb_transpose\"]\n            # reshape the data\n            for i in range(0, 4):\n                tmp_node = graph.make_node(\n                    'Transpose',\n                    inputs=inputs_transpose_pb[i],\n                    outputs=outputs_transpose_pb[i])\n        # decoder the box according to the target_box and variacne\n        name_variance_raw = [result_name + \"@variance_raw\"]\n        name_variance_unsqueeze = [result_name + \"@variance_unsqueeze\"]\n        shape = []\n        # make node to extend the data\n        var_split_axis = 0\n        var_split_inputs_name = []\n        if 'PriorBoxVar' in input_names and len(node.input('PriorBoxVar')) > 0:\n            if axis == 1:\n                raise Exception(\n                    \"The op box_coder has variable do not support aixs broadcast\"\n                )\n            axes = []\n            var_split_inputs_name = [result_name + \"@variance_split\"]\n            tmp_node = graph.make_node(\n                'Transpose',\n                inputs=node.input('PriorBoxVar'),\n                outputs=var_split_inputs_name)\n            var_split_axis = 0\n        else:\n            variances = [1.0, 1.0, 1.0, 1.0]\n            if 'variance' in node.attrs and len(node.attr('variance')) > 0:\n                variances = [float(var) for var in node.attr('variance')]\n            node_variance_create = graph.make_node(\n                'Constant',\n                inputs=[],\n                outputs=name_variance_raw,\n                dtype=dtypes.ONNX.FLOAT,\n                dims=[len(variances)],\n                value=variances)\n            var_split_axis = 0\n            var_split_inputs_name = name_variance_raw\n\n        # decode the result\n        outputs_split_variance = [\n            result_name + \"@variance_split\" + str(i) for i in range(0, 4)\n        ]\n        outputs_split_targebox = [\n            result_name + \"@targebox_split\" + str(i) for i in range(0, 4)\n        ]\n        node_split_var = graph.make_node(\n            'Split',\n            inputs=var_split_inputs_name,\n            outputs=outputs_split_variance,\n            axis=var_split_axis)\n        node_split_target = graph.make_node(\n            'Split',\n            inputs=node.input('TargetBox'),\n            outputs=outputs_split_targebox,\n            axis=2)\n\n        outputs_squeeze_targebox = [\n            result_name + \"@targebox_squeeze\" + str(i) for i in range(0, 4)\n        ]\n        for (input_name, output_name) in zip(outputs_split_targebox,\n                                             outputs_squeeze_targebox):\n            tmp_node = mapper_helper.squeeze_helper(graph, input_name, [2],\n                                                    [output_name])\n\n        output_shape_step1 = list(t_size)[:-1]\n\n        inputs_tb_step1 = [\n            [outputs_squeeze_targebox[0], outputs_split_variance[0]],\n            [outputs_squeeze_targebox[1], outputs_split_variance[1]],\n            [outputs_squeeze_targebox[2], outputs_split_variance[2]],\n            [outputs_squeeze_targebox[3], outputs_split_variance[3]]\n        ]\n        outputs_tb_step1 = [[result_name + \"@decode_x_step1\"],\n                            [result_name + \"@decode_y_step1\"],\n                            [result_name + \"@decode_w_step1\"],\n                            [result_name + \"@decode_h_step1\"]]\n\n        for input_step1, output_step_1 in zip(inputs_tb_step1,\n                                              outputs_tb_step1):\n            tmp_node = graph.make_node(\n                'Mul', inputs=input_step1, outputs=output_step_1)\n        if axis == 0:\n            inputs_tbxy_step2 = [[\n                outputs_tb_step1[0][0], outputs_transpose_pb[0][0]\n            ], [outputs_tb_step1[1][0], outputs_transpose_pb[1][0]]]\n        else:\n            inputs_tbxy_step2 = [[\n                outputs_tb_step1[0][0], inputs_transpose_pb[0][0]\n            ], [outputs_tb_step1[1][0], inputs_transpose_pb[1][0]]]\n\n        outputs_tbxy_step2 = [[result_name + \"@decode_x_step2\"],\n                              [result_name + \"@decode_y_step2\"]]\n\n        for input_step2, output_step_2 in zip(inputs_tbxy_step2,\n                                              outputs_tbxy_step2):\n            tmp_node = graph.make_node(\n                'Mul', inputs=input_step2, outputs=output_step_2)\n        if axis == 0:\n            inputs_tbxy_step3 = [[\n                outputs_tbxy_step2[0][0], outputs_transpose_pb[2][0]\n            ], [outputs_tbxy_step2[1][0], outputs_transpose_pb[3][0]]]\n        else:\n            inputs_tbxy_step3 = [[\n                outputs_tbxy_step2[0][0], inputs_transpose_pb[2][0]\n            ], [outputs_tbxy_step2[1][0], inputs_transpose_pb[3][0]]]\n\n        outputs_tbxy_step3 = [[result_name + \"@decode_x_step3\"],\n                              [result_name + \"@decode_y_step3\"]]\n\n        for input_step3, output_step_3 in zip(inputs_tbxy_step3,\n                                              outputs_tbxy_step3):\n            tmp_node = graph.make_node(\n                'Add', inputs=input_step3, outputs=output_step_3)\n\n        # deal with width & height\n        inputs_tbwh_step2 = [outputs_tb_step1[2], outputs_tb_step1[3]]\n        outputs_tbwh_step2 = [[result_name + \"@decode_w_step2\"],\n                              [result_name + \"@decode_h_step2\"]]\n\n        for input_name, output_name in zip(inputs_tbwh_step2,\n                                           outputs_tbwh_step2):\n            tmp_node = graph.make_node(\n                'Exp', inputs=input_name, outputs=output_name)\n\n        if axis == 0:\n            inputs_tbwh_step3 = [[\n                outputs_tbwh_step2[0][0], outputs_transpose_pb[0][0]\n            ], [outputs_tbwh_step2[1][0], outputs_transpose_pb[1][0]]]\n        else:\n            inputs_tbwh_step3 = [[\n                outputs_tbwh_step2[0][0], inputs_transpose_pb[0][0]\n            ], [outputs_tbwh_step2[1][0], inputs_transpose_pb[1][0]]]\n\n        outputs_tbwh_step3 = [[result_name + \"@decode_w_step3\"],\n                              [result_name + \"@decode_h_step3\"]]\n\n        for input_name, output_name in zip(inputs_tbwh_step3,\n                                           outputs_tbwh_step3):\n            tmp_node = graph.make_node(\n                'Mul', inputs=input_name, outputs=output_name)\n\n        # final step to calc the result, and concat the result to output\n        # return the output box, [(x1, y1), (x2, y2)]\n\n        inputs_half_tbwh_step4 = [[\n            outputs_tbwh_step3[0][0], result_name + \"@slice_2\"\n        ], [outputs_tbwh_step3[1][0], result_name + \"@slice_2\"]]\n\n        outputs_half_tbwh_step4 = [[result_name + \"@decode_half_w_step4\"],\n                                   [result_name + \"@decode_half_h_step4\"]]\n        for inputs_name, outputs_name in zip(inputs_half_tbwh_step4,\n                                             outputs_half_tbwh_step4):\n            tmp_node = graph.make_node(\n                'Div', inputs=inputs_name, outputs=outputs_name)\n        inputs_output_point1 = [[\n            outputs_tbxy_step3[0][0], outputs_half_tbwh_step4[0][0]\n        ], [outputs_tbxy_step3[1][0], outputs_half_tbwh_step4[1][0]]]\n\n        outputs_output_point1 = [[result_name + \"@ouput_x1\"],\n                                 [result_name + \"@output_y1\"]]\n        for input_name, output_name in zip(inputs_output_point1,\n                                           outputs_output_point1):\n            tmp_node = graph.make_node(\n                'Sub', inputs=input_name, outputs=output_name)\n\n        inputs_output_point2 = [[\n            outputs_tbxy_step3[0][0], outputs_half_tbwh_step4[0][0]\n        ], [outputs_tbxy_step3[1][0], outputs_half_tbwh_step4[1][0]]]\n\n        outputs_output_point2 = [[result_name + \"@ouput_x2\"],\n                                 [result_name + \"@output_y2\"]]\n\n        for input_name, output_name in zip(inputs_output_point2,\n                                           outputs_output_point2):\n            tmp_node = graph.make_node(\n                'Add', inputs=input_name, outputs=output_name)\n        if not norm:\n            inputs_unnorm_point2 = [[\n                outputs_output_point2[0][0], result_name + \"@slice_1\"\n            ], [outputs_output_point2[1][0], result_name + \"@slice_1\"]]\n            outputs_unnorm_point2 = [[result_name + \"@ouput_unnorm_x2\"],\n                                     [result_name + \"@ouput_unnorm_y2\"]]\n\n            for input_name, output_name in zip(inputs_unnorm_point2,\n                                               outputs_unnorm_point2):\n                tmp_node = graph.make_node(\n                    'Sub', inputs=input_name, outputs=output_name)\n            outputs_output_point2 = outputs_unnorm_point2\n\n        outputs_output_point1.extend(outputs_output_point2)\n        ouputs_points_unsqueeze = [[result_name + \"@points_unsqueeze_x1\"],\n                                   [result_name + \"points_unsqueeze_y1\"],\n                                   [result_name + \"points_unsqueeze_x2\"],\n                                   [result_name + \"points_unsqueeze_y2\"]]\n\n        for input_name, output_name in zip(outputs_output_point1,\n                                           ouputs_points_unsqueeze):\n            tmp_node = mapper_helper.unsqueeze_helper(\n                graph, input_name, [len(output_shape_step1)], output_name)\n        outputs_points_unsqueeze_list = [\n            output[0] for output in ouputs_points_unsqueeze\n        ]\n        node_point_final = graph.make_node(\n            'Concat',\n            inputs=outputs_points_unsqueeze_list,\n            outputs=node.output('OutputBox'),\n            axis=len(output_shape_step1))",
  "def opset_7(cls, graph, node, **kw):\n        input_names = node.input_names\n\n        t_size = node.input_shape('TargetBox', 0)\n        p_size = node.input_shape('PriorBox', 0)\n\n        # get the outout_name\n        result_name = node.output('OutputBox', 0)\n        # n is size of batch, m is boxes num of targe_boxes\n        n = t_size[0]\n        m = t_size[0]\n\n        axis = int(node.attr('axis'))\n\n        #norm\n        norm = bool(node.attr('box_normalized'))\n\n        name_slice_x1 = node.output('OutputBox')[0] + \"@x1\"\n        name_slice_y1 = node.output('OutputBox')[0] + \"@y1\"\n        name_slice_x2 = node.output('OutputBox')[0] + \"@x2\"\n        name_slice_y2 = node.output('OutputBox')[0] + \"@y2\"\n\n        #make onnx tensor to save the intermeidate reslut\n        name_slice_indices = [\n            [node.output('OutputBox')[0] + \"@slice_\" + str(i)]\n            for i in range(1, 3)\n        ]\n        node_slice_indices = [None for i in range(1, 3)]\n\n        # create the range(0, 4) const data to slice\n        for i in range(1, 3):\n            tmp_node = graph.make_node(\n                'Constant',\n                inputs=[],\n                outputs=name_slice_indices[i - 1],\n                dtype=dtypes.ONNX.FLOAT,\n                dims=(),\n                value=[i])\n        # make node split data\n        name_box_split = [\n            name_slice_x1, name_slice_y1, name_slice_x2, name_slice_y2\n        ]\n        split_shape = list(p_size)\n        split_shape[-1] = 1\n\n        node_split_prior_node = graph.make_node(\n            'Split',\n            inputs=node.input('PriorBox'),\n            outputs=name_box_split,\n            axis=1)\n\n        # make node get centor node for decode\n        final_outputs_vars = []\n        if not norm:\n            name_centor_w_tmp = [node.output('OutputBox')[0] + \"@centor_w_tmp\"]\n            name_centor_h_tmp = [node.output('OutputBox')[0] + \"@centor_h_tmp\"]\n            node_centor_w_tmp = None\n            node_centor_h_tmp = None\n            name_centor_tmp_list = [name_centor_w_tmp, name_centor_h_tmp]\n            node_centor_tmp_list = [node_centor_w_tmp, node_centor_h_tmp]\n\n            count = 2\n            for (name, op_node) in zip(name_centor_tmp_list,\n                                       node_centor_tmp_list):\n                tmp_node = graph.make_node('Add',\n                       inputs=[node.output('OutputBox')[0] + \"@slice_\" + str(1)]\\\n                           + [name_box_split[count]],\n                       outputs=name)\n                count = count + 1\n        if not norm:\n            inputs_sub = [[name_centor_w_tmp[0], name_box_split[0]],\n                          [name_centor_h_tmp[0], name_box_split[1]]]\n        else:\n            inputs_sub = [[name_box_split[2], name_box_split[0]],\n                          [name_box_split[3], name_box_split[1]]]\n        outputs_sub = [result_name + \"@pb_w\", result_name + \"@pb_h\"]\n        for i in range(0, 2):\n            tmp_node = graph.make_node(\n                'Sub', inputs=inputs_sub[i], outputs=[outputs_sub[i]])\n        # according to prior_box height and weight to get centor x, y\n        name_half_value = [result_name + \"@half_value\"]\n        node_half_value = graph.make_node(\n            'Constant',\n            inputs=[],\n            outputs=name_half_value,\n            dtype=dtypes.ONNX.FLOAT,\n            dims=(),\n            value=[0.5])\n        outputs_half_wh = [[result_name + \"@pb_w_half\"],\n                           [result_name + \"@pb_h_half\"]]\n        inputs_half_wh = [[result_name + \"@pb_w\", name_half_value[0]],\n                          [result_name + \"@pb_h\", name_half_value[0]]]\n\n        for i in range(0, 2):\n            tmp_node = graph.make_node(\n                'Mul', inputs=inputs_half_wh[i], outputs=outputs_half_wh[i])\n\n        inputs_centor_xy = [[outputs_half_wh[0][0], name_slice_x1],\n                            [outputs_half_wh[1][0], name_slice_y1]]\n\n        outputs_centor_xy = [[result_name + \"@pb_x\"], [result_name + \"@pb_y\"]]\n\n        # final calc the centor x ,y\n        for i in range(0, 2):\n            tmp_node = graph.make_node(\n                'Add', inputs=inputs_centor_xy[i], outputs=outputs_centor_xy[i])\n        # reshape the data\n        shape = (1, split_shape[0]) if axis == 0 else (split_shape[0], 1)\n\n        # need to reshape the data\n        inputs_transpose_pb = [\n            [result_name + \"@pb_w\"],\n            [result_name + \"@pb_h\"],\n            [result_name + \"@pb_x\"],\n            [result_name + \"@pb_y\"],\n        ]\n        outputs_transpose_pb = [\n            [result_name + \"@pb_w_transpose\"],\n            [result_name + \"@pb_h_transpose\"],\n            [result_name + \"@pb_x_transpose\"],\n            [result_name + \"@pb_y_transpose\"],\n        ]\n        if axis == 0:\n            name_reshape_pb = [result_name + \"@pb_transpose\"]\n            # reshape the data\n            for i in range(0, 4):\n                tmp_node = graph.make_node(\n                    'Transpose',\n                    inputs=inputs_transpose_pb[i],\n                    outputs=outputs_transpose_pb[i])\n        # decoder the box according to the target_box and variacne\n        name_variance_raw = [result_name + \"@variance_raw\"]\n        name_variance_unsqueeze = [result_name + \"@variance_unsqueeze\"]\n        shape = []\n        # make node to extend the data\n        var_split_axis = 0\n        var_split_inputs_name = []\n        if 'PriorBoxVar' in input_names and len(node.input('PriorBoxVar')) > 0:\n            if axis == 1:\n                raise Exception(\n                    \"The op box_coder has variable do not support aixs broadcast\"\n                )\n            axes = []\n            var_split_inputs_name = [result_name + \"@variance_split\"]\n            tmp_node = graph.make_node(\n                'Transpose',\n                inputs=node.input('PriorBoxVar'),\n                outputs=var_split_inputs_name)\n            var_split_axis = 0\n        else:\n            variances = [1.0, 1.0, 1.0, 1.0]\n            if 'variance' in node.attrs and len(node.attr('variance')) > 0:\n                variances = [float(var) for var in node.attr('variance')]\n            node_variance_create = graph.make_node(\n                'Constant',\n                inputs=[],\n                outputs=name_variance_raw,\n                dtype=dtypes.ONNX.FLOAT,\n                dims=[len(variances)],\n                value=variances)\n            var_split_axis = 0\n            var_split_inputs_name = name_variance_raw\n\n        # decode the result\n        outputs_split_variance = [\n            result_name + \"@variance_split\" + str(i) for i in range(0, 4)\n        ]\n        outputs_split_targebox = [\n            result_name + \"@targebox_split\" + str(i) for i in range(0, 4)\n        ]\n        node_split_var = graph.make_node(\n            'Split',\n            inputs=var_split_inputs_name,\n            outputs=outputs_split_variance,\n            axis=var_split_axis)\n        node_split_target = graph.make_node(\n            'Split',\n            inputs=node.input('TargetBox'),\n            outputs=outputs_split_targebox,\n            axis=2)\n\n        outputs_squeeze_targebox = [\n            result_name + \"@targebox_squeeze\" + str(i) for i in range(0, 4)\n        ]\n        for (input_name, output_name) in zip(outputs_split_targebox,\n                                             outputs_squeeze_targebox):\n            tmp_node = mapper_helper.squeeze_helper(graph, input_name, [2],\n                                                    [output_name])\n\n        output_shape_step1 = list(t_size)[:-1]\n\n        inputs_tb_step1 = [\n            [outputs_squeeze_targebox[0], outputs_split_variance[0]],\n            [outputs_squeeze_targebox[1], outputs_split_variance[1]],\n            [outputs_squeeze_targebox[2], outputs_split_variance[2]],\n            [outputs_squeeze_targebox[3], outputs_split_variance[3]]\n        ]\n        outputs_tb_step1 = [[result_name + \"@decode_x_step1\"],\n                            [result_name + \"@decode_y_step1\"],\n                            [result_name + \"@decode_w_step1\"],\n                            [result_name + \"@decode_h_step1\"]]\n\n        for input_step1, output_step_1 in zip(inputs_tb_step1,\n                                              outputs_tb_step1):\n            tmp_node = graph.make_node(\n                'Mul', inputs=input_step1, outputs=output_step_1)\n        if axis == 0:\n            inputs_tbxy_step2 = [[\n                outputs_tb_step1[0][0], outputs_transpose_pb[0][0]\n            ], [outputs_tb_step1[1][0], outputs_transpose_pb[1][0]]]\n        else:\n            inputs_tbxy_step2 = [[\n                outputs_tb_step1[0][0], inputs_transpose_pb[0][0]\n            ], [outputs_tb_step1[1][0], inputs_transpose_pb[1][0]]]\n\n        outputs_tbxy_step2 = [[result_name + \"@decode_x_step2\"],\n                              [result_name + \"@decode_y_step2\"]]\n\n        for input_step2, output_step_2 in zip(inputs_tbxy_step2,\n                                              outputs_tbxy_step2):\n            tmp_node = graph.make_node(\n                'Mul', inputs=input_step2, outputs=output_step_2)\n        if axis == 0:\n            inputs_tbxy_step3 = [[\n                outputs_tbxy_step2[0][0], outputs_transpose_pb[2][0]\n            ], [outputs_tbxy_step2[1][0], outputs_transpose_pb[3][0]]]\n        else:\n            inputs_tbxy_step3 = [[\n                outputs_tbxy_step2[0][0], inputs_transpose_pb[2][0]\n            ], [outputs_tbxy_step2[1][0], inputs_transpose_pb[3][0]]]\n\n        outputs_tbxy_step3 = [[result_name + \"@decode_x_step3\"],\n                              [result_name + \"@decode_y_step3\"]]\n\n        for input_step3, output_step_3 in zip(inputs_tbxy_step3,\n                                              outputs_tbxy_step3):\n            tmp_node = graph.make_node(\n                'Add', inputs=input_step3, outputs=output_step_3)\n\n        # deal with width & height\n        inputs_tbwh_step2 = [outputs_tb_step1[2], outputs_tb_step1[3]]\n        outputs_tbwh_step2 = [[result_name + \"@decode_w_step2\"],\n                              [result_name + \"@decode_h_step2\"]]\n\n        for input_name, output_name in zip(inputs_tbwh_step2,\n                                           outputs_tbwh_step2):\n            tmp_node = graph.make_node(\n                'Exp', inputs=input_name, outputs=output_name)\n\n        if axis == 0:\n            inputs_tbwh_step3 = [[\n                outputs_tbwh_step2[0][0], outputs_transpose_pb[0][0]\n            ], [outputs_tbwh_step2[1][0], outputs_transpose_pb[1][0]]]\n        else:\n            inputs_tbwh_step3 = [[\n                outputs_tbwh_step2[0][0], inputs_transpose_pb[0][0]\n            ], [outputs_tbwh_step2[1][0], inputs_transpose_pb[1][0]]]\n\n        outputs_tbwh_step3 = [[result_name + \"@decode_w_step3\"],\n                              [result_name + \"@decode_h_step3\"]]\n\n        for input_name, output_name in zip(inputs_tbwh_step3,\n                                           outputs_tbwh_step3):\n            tmp_node = graph.make_node(\n                'Mul', inputs=input_name, outputs=output_name)\n\n        # final step to calc the result, and concat the result to output\n        # return the output box, [(x1, y1), (x2, y2)]\n\n        inputs_half_tbwh_step4 = [[\n            outputs_tbwh_step3[0][0], result_name + \"@slice_2\"\n        ], [outputs_tbwh_step3[1][0], result_name + \"@slice_2\"]]\n\n        outputs_half_tbwh_step4 = [[result_name + \"@decode_half_w_step4\"],\n                                   [result_name + \"@decode_half_h_step4\"]]\n        for inputs_name, outputs_name in zip(inputs_half_tbwh_step4,\n                                             outputs_half_tbwh_step4):\n            tmp_node = graph.make_node(\n                'Div', inputs=inputs_name, outputs=outputs_name)\n        inputs_output_point1 = [[\n            outputs_tbxy_step3[0][0], outputs_half_tbwh_step4[0][0]\n        ], [outputs_tbxy_step3[1][0], outputs_half_tbwh_step4[1][0]]]\n\n        outputs_output_point1 = [[result_name + \"@ouput_x1\"],\n                                 [result_name + \"@output_y1\"]]\n        for input_name, output_name in zip(inputs_output_point1,\n                                           outputs_output_point1):\n            tmp_node = graph.make_node(\n                'Sub', inputs=input_name, outputs=output_name)\n\n        inputs_output_point2 = [[\n            outputs_tbxy_step3[0][0], outputs_half_tbwh_step4[0][0]\n        ], [outputs_tbxy_step3[1][0], outputs_half_tbwh_step4[1][0]]]\n\n        outputs_output_point2 = [[result_name + \"@ouput_x2\"],\n                                 [result_name + \"@output_y2\"]]\n\n        for input_name, output_name in zip(inputs_output_point2,\n                                           outputs_output_point2):\n            tmp_node = graph.make_node(\n                'Add', inputs=input_name, outputs=output_name)\n        if not norm:\n            inputs_unnorm_point2 = [[\n                outputs_output_point2[0][0], result_name + \"@slice_1\"\n            ], [outputs_output_point2[1][0], result_name + \"@slice_1\"]]\n            outputs_unnorm_point2 = [[result_name + \"@ouput_unnorm_x2\"],\n                                     [result_name + \"@ouput_unnorm_y2\"]]\n\n            for input_name, output_name in zip(inputs_unnorm_point2,\n                                               outputs_unnorm_point2):\n                tmp_node = graph.make_node(\n                    'Sub', inputs=input_name, outputs=output_name)\n            outputs_output_point2 = outputs_unnorm_point2\n\n        outputs_output_point1.extend(outputs_output_point2)\n        ouputs_points_unsqueeze = [[result_name + \"@points_unsqueeze_x1\"],\n                                   [result_name + \"points_unsqueeze_y1\"],\n                                   [result_name + \"points_unsqueeze_x2\"],\n                                   [result_name + \"points_unsqueeze_y2\"]]\n\n        for input_name, output_name in zip(outputs_output_point1,\n                                           ouputs_points_unsqueeze):\n            tmp_node = mapper_helper.unsqueeze_helper(\n                graph, input_name, [len(output_shape_step1)], output_name)\n        outputs_points_unsqueeze_list = [\n            output[0] for output in ouputs_points_unsqueeze\n        ]\n        node_point_final = graph.make_node(\n            'Concat',\n            inputs=outputs_points_unsqueeze_list,\n            outputs=node.output('OutputBox'),\n            axis=len(output_shape_step1))",
  "class Im2Sequence():\n    support_opset_verison_range = (1, 12)\n\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        n, c, h, w = node.input_shape('X', 0)\n        assert h > 0 and w > 0, \"Only supported fixed input shape for im2sequence operator.\"\n        stride_h, stride_w = node.attr('strides')\n        paddings = node.attr('paddings')\n        assert node.attr(\n            'out_stride'\n        ) != 1, \"Only out_stride==1 is supported for im2sequence operator.\"\n        h = h + paddings[0] + paddings[1]\n        w = w + paddings[1] + paddings[2]\n        kernel_h, kernel_w = node.attr('kernels')\n        out_h = 1 + (h - kernel_h + stride_h - 1) // stride_h\n        out_w = 1 + (w - kernel_w + stride_w - 1) // stride_w\n        h_steps = list()\n        for i in range(out_h):\n            h_steps.append([i * stride_h, i * stride_h + kernel_h])\n        w_steps = list()\n        for i in range(out_w):\n            w_steps.append([i * stride_w, i * stride_w + kernel_w])\n\n        slice_node_blocks = list()\n        for i in range(out_h):\n            for j in range(out_w):\n                starts_node = graph.make_node(\n                    'Constant',\n                    dtype=dtypes.ONNX.INT64,\n                    dims=[4],\n                    value=[0, 0, h_steps[i][0], w_steps[j][0]])\n                ends_node = graph.make_node(\n                    'Constant',\n                    dtype=dtypes.ONNX.INT64,\n                    dims=[4],\n                    value=[999999, 999999, h_steps[i][1], w_steps[j][1]])\n                nodes.extend([starts_node, ends_node])\n\n                slice_block_node = graph.make_node(\n                    'Slice',\n                    inputs=[node.input('X', 0), starts_node, ends_node])\n                flatten_block_node = graph.make_node(\n                    \"Flatten\", inputs=[slice_block_node], axis=0)\n                nodes.extend([slice_block_node, flatten_block_node])\n        concat_block_node = graph.make_node(\n            \"Concat\",\n            inputs=slice_node_blocks,\n            outputs=node.output('Out'),\n            axis=0)\n        logging.info(\"==========Importance Notice===========\")\n        logging.info(\n            \"Since im2sequence operator is used in your paddlepaddle model, the translated onnx model only support input data with batch_size=1.\"\n        )\n        logging.info(\"======================================\")",
  "def opset_1(cls, graph, node, **kw):\n        n, c, h, w = node.input_shape('X', 0)\n        assert h > 0 and w > 0, \"Only supported fixed input shape for im2sequence operator.\"\n        stride_h, stride_w = node.attr('strides')\n        paddings = node.attr('paddings')\n        assert node.attr(\n            'out_stride'\n        ) != 1, \"Only out_stride==1 is supported for im2sequence operator.\"\n        h = h + paddings[0] + paddings[1]\n        w = w + paddings[1] + paddings[2]\n        kernel_h, kernel_w = node.attr('kernels')\n        out_h = 1 + (h - kernel_h + stride_h - 1) // stride_h\n        out_w = 1 + (w - kernel_w + stride_w - 1) // stride_w\n        h_steps = list()\n        for i in range(out_h):\n            h_steps.append([i * stride_h, i * stride_h + kernel_h])\n        w_steps = list()\n        for i in range(out_w):\n            w_steps.append([i * stride_w, i * stride_w + kernel_w])\n\n        slice_node_blocks = list()\n        for i in range(out_h):\n            for j in range(out_w):\n                starts_node = graph.make_node(\n                    'Constant',\n                    dtype=dtypes.ONNX.INT64,\n                    dims=[4],\n                    value=[0, 0, h_steps[i][0], w_steps[j][0]])\n                ends_node = graph.make_node(\n                    'Constant',\n                    dtype=dtypes.ONNX.INT64,\n                    dims=[4],\n                    value=[999999, 999999, h_steps[i][1], w_steps[j][1]])\n                nodes.extend([starts_node, ends_node])\n\n                slice_block_node = graph.make_node(\n                    'Slice',\n                    inputs=[node.input('X', 0), starts_node, ends_node])\n                flatten_block_node = graph.make_node(\n                    \"Flatten\", inputs=[slice_block_node], axis=0)\n                nodes.extend([slice_block_node, flatten_block_node])\n        concat_block_node = graph.make_node(\n            \"Concat\",\n            inputs=slice_node_blocks,\n            outputs=node.output('Out'),\n            axis=0)\n        logging.info(\"==========Importance Notice===========\")\n        logging.info(\n            \"Since im2sequence operator is used in your paddlepaddle model, the translated onnx model only support input data with batch_size=1.\"\n        )\n        logging.info(\"======================================\")",
  "class BoxClip(CustomPaddleOp):\n    def __init__(self, node, **kw):\n        super(BoxClip, self).__init__(node)\n\n    def forward(self):\n        input = self.input('Input', 0)\n        im_info = self.input('ImInfo', 0)\n        im_info = paddle.reshape(im_info, shape=[3])\n        h, w, s = paddle.tensor.split(im_info, axis=0, num_or_sections=3)\n        tensor_one = paddle.full(shape=[1], dtype='float32', fill_value=1.0)\n        tensor_zero = paddle.full(shape=[1], dtype='float32', fill_value=0.0)\n        h = paddle.subtract(h, tensor_one)\n        w = paddle.subtract(w, tensor_one)\n        xmin, ymin, xmax, ymax = paddle.tensor.split(\n            input, axis=-1, num_or_sections=4)\n        xmin = paddle.maximum(paddle.minimum(xmin, w), tensor_zero)\n        ymin = paddle.maximum(paddle.minimum(ymin, h), tensor_zero)\n        xmax = paddle.maximum(paddle.minimum(xmax, w), tensor_zero)\n        ymax = paddle.maximum(paddle.minimum(ymax, h), tensor_zero)\n        cliped_box = paddle.concat([xmin, ymin, xmax, ymax], axis=-1)\n\n        return {'Output': [cliped_box]}",
  "class Boxclip:\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'box_clip',\n            inputs=node.input('Input')+node.input('ImInfo'),\n            outputs=node.output('Output'),\n            domain = 'custom')",
  "def __init__(self, node, **kw):\n        super(BoxClip, self).__init__(node)",
  "def forward(self):\n        input = self.input('Input', 0)\n        im_info = self.input('ImInfo', 0)\n        im_info = paddle.reshape(im_info, shape=[3])\n        h, w, s = paddle.tensor.split(im_info, axis=0, num_or_sections=3)\n        tensor_one = paddle.full(shape=[1], dtype='float32', fill_value=1.0)\n        tensor_zero = paddle.full(shape=[1], dtype='float32', fill_value=0.0)\n        h = paddle.subtract(h, tensor_one)\n        w = paddle.subtract(w, tensor_one)\n        xmin, ymin, xmax, ymax = paddle.tensor.split(\n            input, axis=-1, num_or_sections=4)\n        xmin = paddle.maximum(paddle.minimum(xmin, w), tensor_zero)\n        ymin = paddle.maximum(paddle.minimum(ymin, h), tensor_zero)\n        xmax = paddle.maximum(paddle.minimum(xmax, w), tensor_zero)\n        ymax = paddle.maximum(paddle.minimum(ymax, h), tensor_zero)\n        cliped_box = paddle.concat([xmin, ymin, xmax, ymax], axis=-1)\n\n        return {'Output': [cliped_box]}",
  "def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'box_clip',\n            inputs=node.input('Input')+node.input('ImInfo'),\n            outputs=node.output('Output'),\n            domain = 'custom')",
  "class AnchorGenerator(CustomPaddleOp):\n    def __init__(self, node, **kw):\n        super(AnchorGenerator, self).__init__(node)\n        #self.x_shape = node.input_shape('Input', 0)\n        self.anchor_sizes = node.attr('anchor_sizes')\n        self.aspect_ratios = node.attr('aspect_ratios')\n        self.offset = node.attr('offset')\n        self.strides = node.attr('stride')\n        self.variances = node.attr('variances')\n        self.shapes = self.compute_shapes()\n\n    def compute_shapes(self):\n        shapes = list()\n        for r in range(len(self.aspect_ratios)):\n            ar = self.aspect_ratios[r]\n            for s in range(len(self.anchor_sizes)):\n                anchor_size = self.anchor_sizes[s]\n                area = self.strides[0] * self.strides[1]\n                area_ratios = area / ar\n                base_w = np.floor(np.sqrt(area_ratios) + 0.5)\n                base_h = np.floor(base_w * ar + 0.5)\n                scale_w = anchor_size / self.strides[0]\n                scale_h = anchor_size / self.strides[1]\n                w = scale_w * base_w\n                h = scale_h * base_h\n                shapes.append([\n                    -0.5 * (w - 1), -0.5 * (h - 1), 0.5 * (w - 1), 0.5 * (h - 1)\n                ])\n        return shapes\n\n    def forward(self):\n        input_feature = self.input('Input', 0)\n        input_shape = paddle.shape(input_feature)\n        n, c, h, w = paddle.tensor.split(input_shape, num_or_sections=4)\n        x_ctr = paddle.arange(start=0, end=w, step=1, dtype=input_feature.dtype)\n        y_ctr = paddle.arange(start=0, end=h, step=1, dtype=input_feature.dtype)\n        x_ctr = x_ctr * self.strides[0] + self.offset * (self.strides[0] - 1)\n        y_ctr = y_ctr * self.strides[1] + self.offset * (self.strides[1] - 1)\n        tensor_one = paddle.ones(shape=[1], dtype='int64')\n        tensor_len_shape = paddle.full(\n            shape=[1], fill_value=len(self.shapes), dtype='int64')\n        x_ctr = paddle.reshape(x_ctr, shape=(1, -1))\n        y_ctr = paddle.reshape(y_ctr, shape=(1, -1))\n        x_ctr = paddle.tile(x_ctr, repeat_times=(h, tensor_one))\n        y_ctr = paddle.tile(y_ctr, repeat_times=(w, tensor_one))\n        y_ctr = paddle.transpose(y_ctr, perm=[1, 0])\n        centers = paddle.stack([x_ctr, y_ctr], axis=-1)\n        centers = paddle.tensor.unsqueeze(centers, axis=[2])\n        centers = paddle.tile(centers, repeat_times=(1, 1, len(self.shapes), 2))\n        shape_tensor = paddle.assign(np.array(self.shapes).astype('float32'))\n        anchors = centers + shape_tensor\n        variance_tensor = paddle.assign(\n            np.asarray(self.variances).astype('float32'))\n        vars = paddle.reshape(variance_tensor, shape=[1, 1, 1, -1])\n        vars = paddle.tile(\n            vars, repeat_times=(h, w, tensor_len_shape, tensor_one))\n        return {'Anchors': [anchors], 'Variances': [vars]}",
  "class Anchors_generator:\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'anchor_generator',\n            inputs=node.input('Input'),\n            outputs=node.output('Anchors') + node.output('Variances'),\n            anchor_sizes = node.attr('anchor_sizes'),\n            aspect_ratios = node.attr('aspect_ratios'),\n            offset = node.attr('offset'),\n            strides = node.attr('stride'),\n            variances = node.attr('variances'),\n            domain = 'custom')",
  "def __init__(self, node, **kw):\n        super(AnchorGenerator, self).__init__(node)\n        #self.x_shape = node.input_shape('Input', 0)\n        self.anchor_sizes = node.attr('anchor_sizes')\n        self.aspect_ratios = node.attr('aspect_ratios')\n        self.offset = node.attr('offset')\n        self.strides = node.attr('stride')\n        self.variances = node.attr('variances')\n        self.shapes = self.compute_shapes()",
  "def compute_shapes(self):\n        shapes = list()\n        for r in range(len(self.aspect_ratios)):\n            ar = self.aspect_ratios[r]\n            for s in range(len(self.anchor_sizes)):\n                anchor_size = self.anchor_sizes[s]\n                area = self.strides[0] * self.strides[1]\n                area_ratios = area / ar\n                base_w = np.floor(np.sqrt(area_ratios) + 0.5)\n                base_h = np.floor(base_w * ar + 0.5)\n                scale_w = anchor_size / self.strides[0]\n                scale_h = anchor_size / self.strides[1]\n                w = scale_w * base_w\n                h = scale_h * base_h\n                shapes.append([\n                    -0.5 * (w - 1), -0.5 * (h - 1), 0.5 * (w - 1), 0.5 * (h - 1)\n                ])\n        return shapes",
  "def forward(self):\n        input_feature = self.input('Input', 0)\n        input_shape = paddle.shape(input_feature)\n        n, c, h, w = paddle.tensor.split(input_shape, num_or_sections=4)\n        x_ctr = paddle.arange(start=0, end=w, step=1, dtype=input_feature.dtype)\n        y_ctr = paddle.arange(start=0, end=h, step=1, dtype=input_feature.dtype)\n        x_ctr = x_ctr * self.strides[0] + self.offset * (self.strides[0] - 1)\n        y_ctr = y_ctr * self.strides[1] + self.offset * (self.strides[1] - 1)\n        tensor_one = paddle.ones(shape=[1], dtype='int64')\n        tensor_len_shape = paddle.full(\n            shape=[1], fill_value=len(self.shapes), dtype='int64')\n        x_ctr = paddle.reshape(x_ctr, shape=(1, -1))\n        y_ctr = paddle.reshape(y_ctr, shape=(1, -1))\n        x_ctr = paddle.tile(x_ctr, repeat_times=(h, tensor_one))\n        y_ctr = paddle.tile(y_ctr, repeat_times=(w, tensor_one))\n        y_ctr = paddle.transpose(y_ctr, perm=[1, 0])\n        centers = paddle.stack([x_ctr, y_ctr], axis=-1)\n        centers = paddle.tensor.unsqueeze(centers, axis=[2])\n        centers = paddle.tile(centers, repeat_times=(1, 1, len(self.shapes), 2))\n        shape_tensor = paddle.assign(np.array(self.shapes).astype('float32'))\n        anchors = centers + shape_tensor\n        variance_tensor = paddle.assign(\n            np.asarray(self.variances).astype('float32'))\n        vars = paddle.reshape(variance_tensor, shape=[1, 1, 1, -1])\n        vars = paddle.tile(\n            vars, repeat_times=(h, w, tensor_len_shape, tensor_one))\n        return {'Anchors': [anchors], 'Variances': [vars]}",
  "def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'anchor_generator',\n            inputs=node.input('Input'),\n            outputs=node.output('Anchors') + node.output('Variances'),\n            anchor_sizes = node.attr('anchor_sizes'),\n            aspect_ratios = node.attr('aspect_ratios'),\n            offset = node.attr('offset'),\n            strides = node.attr('stride'),\n            variances = node.attr('variances'),\n            domain = 'custom')",
  "class GridSampler(CustomPaddleOp):\n    def __init__(self, node, **kw):\n        super(GridSampler, self).__init__(node)\n        self.padding_mode = node.attr('padding_mode')\n        self.mode = node.attr('mode')\n        self.align_corners = node.attr('align_corners')\n\n    def paddle_bilinear_grid_sample(self, im, grid, align_corners=False):\n        # this code reference: https://mmcv.readthedocs.io/en/latest/_modules/mmcv/ops/point_sample.html\n        im_shape = paddle.shape(im)\n        n, c, h, w = paddle.split(im_shape, num_or_sections=4)\n        grid_shape = paddle.shape(grid)\n        gn, gh, gw, _ = paddle.split(grid_shape, num_or_sections=4)\n\n        # n, c, h, w = im.shape\n        # gn, gh, gw, _ = grid.shape\n        # assert n == gn\n\n        x = grid[:, :, :, 0]\n        y = grid[:, :, :, 1]\n\n        if align_corners:\n            x = ((x + 1) / 2) * (w - 1)\n            y = ((y + 1) / 2) * (h - 1)\n        else:\n            x = ((x + 1) * w - 1) / 2\n            y = ((y + 1) * h - 1) / 2\n\n        x = paddle.reshape(x, [n, -1])\n        y = paddle.reshape(y, [n, -1])\n\n        x0 = paddle.floor(x).astype('int64')\n        y0 = paddle.floor(y).astype('int64')\n        x1 = x0 + 1\n        y1 = y0 + 1\n\n        x1_cast = x1.astype(grid.dtype)\n        x0_cast = x0.astype(grid.dtype)\n        y1_cast = y1.astype(grid.dtype)\n        y0_cast = y0.astype(grid.dtype)\n        wa = paddle.unsqueeze(((x1_cast - x) * (y1_cast - y)), 1)\n        wb = paddle.unsqueeze(((x1_cast - x) * (y - y0_cast)), 1)\n        wc = paddle.unsqueeze(((x - x0_cast) * (y1_cast - y)), 1)\n        wd = paddle.unsqueeze(((x - x0_cast) * (y - y0_cast)), 1)\n\n        # Apply default for grid_sample function zero padding\n        im_padded = paddle.nn.functional.pad(im,\n                                             pad=[1, 1, 1, 1],\n                                             mode='constant',\n                                             value=0)\n        if im_padded.dtype != im.dtype:\n            im_padded = paddle.cast(im_padded, im.dtype)\n        padded_h = h + 2\n        padded_w = w + 2\n        # save points positions after padding\n        x0, x1, y0, y1 = x0 + 1, x1 + 1, y0 + 1, y1 + 1\n\n        # Clip coordinates to padded image size\n        tensor_zero = paddle.full(shape=[1], dtype='int64', fill_value=0.0)\n        tensor_padded_w = paddle.full(\n            shape=[1], dtype='int64', fill_value=padded_w - 1)\n        tensor_padded_h = paddle.full(\n            shape=[1], dtype='int64', fill_value=padded_h - 1)\n        x0 = paddle.where(x0 < 0, tensor_zero, x0)\n        x0 = paddle.where(x0 > padded_w - 1, tensor_padded_w, x0)\n        x1 = paddle.where(x1 < 0, tensor_zero, x1)\n        x1 = paddle.where(x1 > padded_w - 1, tensor_padded_w, x1)\n        y0 = paddle.where(y0 < 0, tensor_zero, y0)\n        y0 = paddle.where(y0 > padded_h - 1, tensor_padded_h, y0)\n        y1 = paddle.where(y1 < 0, tensor_zero, y1)\n        y1 = paddle.where(y1 > padded_h - 1, tensor_padded_h, y1)\n        im_padded = paddle.reshape(im_padded, [n, c, -1])\n\n        x0_y0 = paddle.expand(\n            paddle.unsqueeze((x0 + y0 * padded_w), 1), [-1, c, -1])\n        x0_y1 = paddle.expand(\n            paddle.unsqueeze((x0 + y1 * padded_w), 1), [-1, c, -1])\n        x1_y0 = paddle.expand(\n            paddle.unsqueeze((x1 + y0 * padded_w), 1), [-1, c, -1])\n        x1_y1 = paddle.expand(\n            paddle.unsqueeze((x1 + y1 * padded_w), 1), [-1, c, -1])\n\n        Ia = self.paddle_gather(im_padded, 2, x0_y0)\n        Ib = self.paddle_gather(im_padded, 2, x0_y1)\n        Ic = self.paddle_gather(im_padded, 2, x1_y0)\n        Id = self.paddle_gather(im_padded, 2, x1_y1)\n\n        return paddle.reshape((Ia * wa + Ib * wb + Ic * wc + Id * wd),\n                              [n, c, gh, gw])\n\n    def paddle_gather(self, x, dim, index):\n        # index_shape = index.shape\n        index_shape = paddle.shape(index)\n        x_shape = paddle.shape(x)\n        index_flatten = index.flatten()\n        if dim < 0:\n            dim = len(x.shape) + dim\n        nd_index = []\n        for k in range(len(x.shape)):\n            if k == dim:\n                nd_index.append(index_flatten)\n            else:\n                reshape_shape = [1] * len(x.shape)\n                x_shape_k = x_shape[k]\n                # x_shape_k = x.shape[k]\n                reshape_shape[k] = x_shape_k\n                x_arange = paddle.arange(x_shape_k, dtype=index.dtype)\n                x_arange = x_arange.reshape(reshape_shape)\n                dim_index = paddle.expand(x_arange, index_shape).flatten()\n                nd_index.append(dim_index)\n        ind2 = paddle.transpose(paddle.stack(nd_index), [1, 0]).astype(\"int64\")\n        paddle_out = paddle.gather_nd(x, ind2).reshape(index_shape)\n        return paddle_out\n\n    def forward(self):\n        input = self.input('X', 0)\n        grid = self.input('Grid', 0)\n        if self.mode != 'bilinear' or self.padding_mode != 'zeros':\n            raise Exception(\n                \"grid_sample only is supported with mode should be 'bilinear' and padding_mode should be 'zeros'\"\n            )\n        res = self.paddle_bilinear_grid_sample(\n            input, grid, align_corners=self.align_corners)\n        return {'Output': [res]}",
  "def __init__(self, node, **kw):\n        super(GridSampler, self).__init__(node)\n        self.padding_mode = node.attr('padding_mode')\n        self.mode = node.attr('mode')\n        self.align_corners = node.attr('align_corners')",
  "def paddle_bilinear_grid_sample(self, im, grid, align_corners=False):\n        # this code reference: https://mmcv.readthedocs.io/en/latest/_modules/mmcv/ops/point_sample.html\n        im_shape = paddle.shape(im)\n        n, c, h, w = paddle.split(im_shape, num_or_sections=4)\n        grid_shape = paddle.shape(grid)\n        gn, gh, gw, _ = paddle.split(grid_shape, num_or_sections=4)\n\n        # n, c, h, w = im.shape\n        # gn, gh, gw, _ = grid.shape\n        # assert n == gn\n\n        x = grid[:, :, :, 0]\n        y = grid[:, :, :, 1]\n\n        if align_corners:\n            x = ((x + 1) / 2) * (w - 1)\n            y = ((y + 1) / 2) * (h - 1)\n        else:\n            x = ((x + 1) * w - 1) / 2\n            y = ((y + 1) * h - 1) / 2\n\n        x = paddle.reshape(x, [n, -1])\n        y = paddle.reshape(y, [n, -1])\n\n        x0 = paddle.floor(x).astype('int64')\n        y0 = paddle.floor(y).astype('int64')\n        x1 = x0 + 1\n        y1 = y0 + 1\n\n        x1_cast = x1.astype(grid.dtype)\n        x0_cast = x0.astype(grid.dtype)\n        y1_cast = y1.astype(grid.dtype)\n        y0_cast = y0.astype(grid.dtype)\n        wa = paddle.unsqueeze(((x1_cast - x) * (y1_cast - y)), 1)\n        wb = paddle.unsqueeze(((x1_cast - x) * (y - y0_cast)), 1)\n        wc = paddle.unsqueeze(((x - x0_cast) * (y1_cast - y)), 1)\n        wd = paddle.unsqueeze(((x - x0_cast) * (y - y0_cast)), 1)\n\n        # Apply default for grid_sample function zero padding\n        im_padded = paddle.nn.functional.pad(im,\n                                             pad=[1, 1, 1, 1],\n                                             mode='constant',\n                                             value=0)\n        if im_padded.dtype != im.dtype:\n            im_padded = paddle.cast(im_padded, im.dtype)\n        padded_h = h + 2\n        padded_w = w + 2\n        # save points positions after padding\n        x0, x1, y0, y1 = x0 + 1, x1 + 1, y0 + 1, y1 + 1\n\n        # Clip coordinates to padded image size\n        tensor_zero = paddle.full(shape=[1], dtype='int64', fill_value=0.0)\n        tensor_padded_w = paddle.full(\n            shape=[1], dtype='int64', fill_value=padded_w - 1)\n        tensor_padded_h = paddle.full(\n            shape=[1], dtype='int64', fill_value=padded_h - 1)\n        x0 = paddle.where(x0 < 0, tensor_zero, x0)\n        x0 = paddle.where(x0 > padded_w - 1, tensor_padded_w, x0)\n        x1 = paddle.where(x1 < 0, tensor_zero, x1)\n        x1 = paddle.where(x1 > padded_w - 1, tensor_padded_w, x1)\n        y0 = paddle.where(y0 < 0, tensor_zero, y0)\n        y0 = paddle.where(y0 > padded_h - 1, tensor_padded_h, y0)\n        y1 = paddle.where(y1 < 0, tensor_zero, y1)\n        y1 = paddle.where(y1 > padded_h - 1, tensor_padded_h, y1)\n        im_padded = paddle.reshape(im_padded, [n, c, -1])\n\n        x0_y0 = paddle.expand(\n            paddle.unsqueeze((x0 + y0 * padded_w), 1), [-1, c, -1])\n        x0_y1 = paddle.expand(\n            paddle.unsqueeze((x0 + y1 * padded_w), 1), [-1, c, -1])\n        x1_y0 = paddle.expand(\n            paddle.unsqueeze((x1 + y0 * padded_w), 1), [-1, c, -1])\n        x1_y1 = paddle.expand(\n            paddle.unsqueeze((x1 + y1 * padded_w), 1), [-1, c, -1])\n\n        Ia = self.paddle_gather(im_padded, 2, x0_y0)\n        Ib = self.paddle_gather(im_padded, 2, x0_y1)\n        Ic = self.paddle_gather(im_padded, 2, x1_y0)\n        Id = self.paddle_gather(im_padded, 2, x1_y1)\n\n        return paddle.reshape((Ia * wa + Ib * wb + Ic * wc + Id * wd),\n                              [n, c, gh, gw])",
  "def paddle_gather(self, x, dim, index):\n        # index_shape = index.shape\n        index_shape = paddle.shape(index)\n        x_shape = paddle.shape(x)\n        index_flatten = index.flatten()\n        if dim < 0:\n            dim = len(x.shape) + dim\n        nd_index = []\n        for k in range(len(x.shape)):\n            if k == dim:\n                nd_index.append(index_flatten)\n            else:\n                reshape_shape = [1] * len(x.shape)\n                x_shape_k = x_shape[k]\n                # x_shape_k = x.shape[k]\n                reshape_shape[k] = x_shape_k\n                x_arange = paddle.arange(x_shape_k, dtype=index.dtype)\n                x_arange = x_arange.reshape(reshape_shape)\n                dim_index = paddle.expand(x_arange, index_shape).flatten()\n                nd_index.append(dim_index)\n        ind2 = paddle.transpose(paddle.stack(nd_index), [1, 0]).astype(\"int64\")\n        paddle_out = paddle.gather_nd(x, ind2).reshape(index_shape)\n        return paddle_out",
  "def forward(self):\n        input = self.input('X', 0)\n        grid = self.input('Grid', 0)\n        if self.mode != 'bilinear' or self.padding_mode != 'zeros':\n            raise Exception(\n                \"grid_sample only is supported with mode should be 'bilinear' and padding_mode should be 'zeros'\"\n            )\n        res = self.paddle_bilinear_grid_sample(\n            input, grid, align_corners=self.align_corners)\n        return {'Output': [res]}",
  "class DistributeFpnProposals(CustomPaddleOp):\n    def __init__(self, node, **kw):\n        super(DistributeFpnProposals, self).__init__(node)\n        self.max_level = node.attr('max_level')\n        self.min_level = node.attr('min_level')\n        self.refer_level = node.attr('refer_level')\n        self.refer_scale = node.attr('refer_scale')\n        self.pixel_offset = node.attr('pixel_offset')\n\n    def bbox_area(self, boxes):\n        offset = 1 if self.pixel_offset else 0\n        xmin, ymin, xmax, ymax = paddle.tensor.split(\n            boxes, axis=1, num_or_sections=4)\n        width = xmax - xmin + offset\n        height = ymax - ymin + offset\n        areas = width * height\n        return areas\n\n    def forward(self):\n        fpn_rois = self.input('FpnRois', 0)\n        areas = self.bbox_area(fpn_rois)\n        scale = paddle.sqrt(areas)\n        num_level = self.max_level - self.min_level + 1\n        target_level = paddle.log(scale / self.refer_scale + 1e-06) / np.log(2)\n        target_level = paddle.floor(self.refer_level + target_level)\n        target_level = paddle.clip(\n            target_level, min=self.min_level, max=self.max_level)\n\n        rois = list()\n        rois_idx_order = list()\n        rois_num_per_level = list()\n\n        for level in range(self.min_level, self.max_level + 1):\n            level_tensor = paddle.full_like(target_level, fill_value=level)\n            res = paddle.equal(target_level, level_tensor)\n            res = paddle.squeeze(res, axis=1)\n            res = paddle.cast(res, dtype='int32')\n            index = paddle.nonzero(res)\n            roi = paddle.gather(fpn_rois, index, axis=0)\n            rois.append(roi)\n            rois_idx_order.append(index)\n            rois_num_per_level.append(paddle.shape(roi)[0])\n        rois_idx_order = paddle.concat(rois_idx_order, axis=0)\n        size = paddle.shape(rois_idx_order)[0]\n        _, rois_idx_restore = paddle.topk(\n            rois_idx_order, axis=0, sorted=True, largest=False, k=size)\n\n        rois_idx_restore = paddle.cast(rois_idx_restore, dtype='int32')\n        if len(self.input('RoisNum')) > 0:\n            # trick: to keep rois num\n            rois_num_per_level[0] += self.input('RoisNum', 0) * 0\n            return {\n                'MultiFpnRois': rois,\n                'RestoreIndex': [rois_idx_restore],\n                'MultiLevelRoIsNum': rois_num_per_level\n            }\n        else:\n            return {'MultiFpnRois': rois, 'RestoreIndex': [rois_idx_restore]}",
  "class Distributefpnproposals:\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'distribute_fpn_proposals',\n            inputs=node.input('FpnRois'),\n            outputs=node.output('MultiFpnRois') + node.output('RestoreIndex'),\n            max_level=node.attr('max_level'),\n            min_level=node.attr('min_level'),\n            refer_level=node.attr('refer_level'),\n            refer_scale=node.attr('refer_scale'),\n            domain='custom')",
  "def __init__(self, node, **kw):\n        super(DistributeFpnProposals, self).__init__(node)\n        self.max_level = node.attr('max_level')\n        self.min_level = node.attr('min_level')\n        self.refer_level = node.attr('refer_level')\n        self.refer_scale = node.attr('refer_scale')\n        self.pixel_offset = node.attr('pixel_offset')",
  "def bbox_area(self, boxes):\n        offset = 1 if self.pixel_offset else 0\n        xmin, ymin, xmax, ymax = paddle.tensor.split(\n            boxes, axis=1, num_or_sections=4)\n        width = xmax - xmin + offset\n        height = ymax - ymin + offset\n        areas = width * height\n        return areas",
  "def forward(self):\n        fpn_rois = self.input('FpnRois', 0)\n        areas = self.bbox_area(fpn_rois)\n        scale = paddle.sqrt(areas)\n        num_level = self.max_level - self.min_level + 1\n        target_level = paddle.log(scale / self.refer_scale + 1e-06) / np.log(2)\n        target_level = paddle.floor(self.refer_level + target_level)\n        target_level = paddle.clip(\n            target_level, min=self.min_level, max=self.max_level)\n\n        rois = list()\n        rois_idx_order = list()\n        rois_num_per_level = list()\n\n        for level in range(self.min_level, self.max_level + 1):\n            level_tensor = paddle.full_like(target_level, fill_value=level)\n            res = paddle.equal(target_level, level_tensor)\n            res = paddle.squeeze(res, axis=1)\n            res = paddle.cast(res, dtype='int32')\n            index = paddle.nonzero(res)\n            roi = paddle.gather(fpn_rois, index, axis=0)\n            rois.append(roi)\n            rois_idx_order.append(index)\n            rois_num_per_level.append(paddle.shape(roi)[0])\n        rois_idx_order = paddle.concat(rois_idx_order, axis=0)\n        size = paddle.shape(rois_idx_order)[0]\n        _, rois_idx_restore = paddle.topk(\n            rois_idx_order, axis=0, sorted=True, largest=False, k=size)\n\n        rois_idx_restore = paddle.cast(rois_idx_restore, dtype='int32')\n        if len(self.input('RoisNum')) > 0:\n            # trick: to keep rois num\n            rois_num_per_level[0] += self.input('RoisNum', 0) * 0\n            return {\n                'MultiFpnRois': rois,\n                'RestoreIndex': [rois_idx_restore],\n                'MultiLevelRoIsNum': rois_num_per_level\n            }\n        else:\n            return {'MultiFpnRois': rois, 'RestoreIndex': [rois_idx_restore]}",
  "def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'distribute_fpn_proposals',\n            inputs=node.input('FpnRois'),\n            outputs=node.output('MultiFpnRois') + node.output('RestoreIndex'),\n            max_level=node.attr('max_level'),\n            min_level=node.attr('min_level'),\n            refer_level=node.attr('refer_level'),\n            refer_scale=node.attr('refer_scale'),\n            domain='custom')",
  "class GenerateProposals(CustomPaddleOp):\n    def __init__(self, node, **kw):\n        paddle.enable_static()\n        super(GenerateProposals, self).__init__(node)\n        self.eta = node.attr('eta')\n        self.min_size = node.attr('min_size')\n        self.nms_thresh = node.attr('nms_thresh')\n        self.post_nms_topN = node.attr('post_nms_topN')\n        self.pre_nms_topN = node.attr('pre_nms_topN')\n        self.type = node.type\n        if self.type == 'generate_proposals_v2':\n            self.pixel_offset = node.attr('pixel_offset')\n        else:\n            self.pixel_offset = True\n\n    def filter_boxes(self, boxes, im_w, im_h, im_s, min_size):\n        min_size = max(min_size, 1.0)\n        xmin, ymin, xmax, ymax = paddle.tensor.split(\n            boxes, axis=1, num_or_sections=4)\n        x_ctr = (xmax + xmin) / 2 + 0.5\n        y_ctr = (ymax + ymin) / 2 + 0.5\n        ws = (xmax - xmin) / im_s + 1\n        hs = (ymax - ymin) / im_s + 1\n\n        min_size = np.asarray([min_size], dtype='float32')\n        min_size = paddle.assign(min_size)\n        valid_flag_ws = paddle.greater_equal(ws, min_size)\n        valid_flag_hs = paddle.greater_equal(hs, min_size)\n        valid_flag_x = paddle.less_equal(x_ctr, im_w)\n        valid_flag_y = paddle.less_equal(y_ctr, im_h)\n        valid_flag = paddle.logical_and(valid_flag_ws, valid_flag_hs)\n        valid_flag = paddle.logical_and(valid_flag, valid_flag_x)\n        valid_flag = paddle.logical_and(valid_flag, valid_flag_y)\n        valid_flag = paddle.squeeze(valid_flag, axis=1)\n        valid_inds = paddle.nonzero(valid_flag)\n\n        return valid_inds\n\n    def filter_boxes_v2(self, boxes, im_w, im_h, min_size, pixel_offset=True):\n        min_size = max(min_size, 1.0)\n        xmin, ymin, xmax, ymax = paddle.tensor.split(\n            boxes, axis=1, num_or_sections=4)\n\n        offset = 1 if pixel_offset else 0\n        ws = (xmax - xmin) + offset\n        hs = (ymax - ymin) + offset\n\n        min_size = np.asarray([min_size], dtype='float32')\n        min_size = paddle.assign(min_size)\n        valid_flag_ws = paddle.greater_equal(ws, min_size)\n        valid_flag_hs = paddle.greater_equal(hs, min_size)\n        valid_flag = paddle.logical_and(valid_flag_ws, valid_flag_hs)\n        if pixel_offset:\n            x_ctr = xmin + ws / 2\n            y_ctr = ymin + hs / 2\n            valid_flag_x = paddle.less_equal(x_ctr, im_w)\n            valid_flag_y = paddle.less_equal(y_ctr, im_h)\n            valid_flag = paddle.logical_and(valid_flag, valid_flag_x)\n            valid_flag = paddle.logical_and(valid_flag, valid_flag_y)\n\n        valid_flag = paddle.squeeze(valid_flag, axis=1)\n        valid_inds = paddle.nonzero(valid_flag)\n        return valid_inds\n\n    def clip_tiled_boxes(self, im_w, im_h, input_boxes, pixel_offset=True):\n        offset = 1 if pixel_offset else 0\n        xmin, ymin, xmax, ymax = paddle.tensor.split(\n            input_boxes, axis=1, num_or_sections=4)\n        xmin = paddle.clip(xmin, max=im_w - offset, min=0)\n        ymin = paddle.clip(ymin, max=im_h - offset, min=0)\n        xmax = paddle.clip(xmax, max=im_w - offset, min=0)\n        ymax = paddle.clip(ymax, max=im_h - offset, min=0)\n        input_boxes = paddle.concat([xmin, ymin, xmax, ymax], axis=1)\n        return input_boxes\n\n    def box_encode(self, anchors, bbox_deltas, variances, pixel_offset=True):\n        offset = 1 if pixel_offset else 0\n        anchor_xmin, anchor_ymin, anchor_xmax, anchor_ymax = paddle.tensor.split(\n            anchors, axis=1, num_or_sections=4)\n        anchor_width = anchor_xmax - anchor_xmin + offset\n        anchor_height = anchor_ymax - anchor_ymin + offset\n        anchor_center_x = anchor_xmin + 0.5 * anchor_width\n        anchor_center_y = anchor_ymin + 0.5 * anchor_height\n        var_center_x, var_center_y, var_width, var_height = paddle.tensor.split(\n            variances, axis=1, num_or_sections=4)\n        delta_center_x, delta_center_y, delta_width, delta_height = paddle.tensor.split(\n            bbox_deltas, axis=1, num_or_sections=4)\n\n        bbox_center_x = var_center_x * delta_center_x * anchor_width + anchor_center_x\n        bbox_center_y = var_center_y * delta_center_y * anchor_height + anchor_center_y\n        bbox_width = paddle.exp(\n            paddle.clip(\n                var_width * delta_width, max=BBOX_CLIP_DEFAULT)) * anchor_width\n        bbox_height = paddle.exp(\n            paddle.clip(\n                var_height * delta_height,\n                max=BBOX_CLIP_DEFAULT)) * anchor_height\n\n        proposal_xmin = bbox_center_x - bbox_width / 2\n        proposal_ymin = bbox_center_y - bbox_height / 2\n        proposal_xmax = bbox_center_x + bbox_width / 2 - offset\n        proposal_ymax = bbox_center_y + bbox_height / 2 - offset\n        proposal = paddle.concat(\n            [proposal_xmin, proposal_ymin, proposal_xmax, proposal_ymax],\n            axis=1)\n        return proposal\n\n    def proposal_for_single_sample(self, anchors, bbox_deltas, im_info, scores,\n                                   variances):\n        proposal_num = paddle.shape(scores)[0]\n        pre_nms_top_n_tensor = paddle.assign(\n            np.asarray(\n                [self.pre_nms_topN], dtype='int32'))\n        k_candidate = paddle.concat([proposal_num, pre_nms_top_n_tensor])\n        k = paddle.min(k_candidate)\n        scores, index = paddle.topk(scores, k=k, axis=0)\n        bbox_deltas = paddle.gather(bbox_deltas, index, axis=0)\n        anchors = paddle.gather(anchors, index, axis=0)\n        variances = paddle.gather(variances, index, axis=0)\n\n        proposal = self.box_encode(anchors, bbox_deltas, variances,\n                                   self.pixel_offset)\n        if self.type == \"generate_proposals_v2\":\n            im_h, im_w = paddle.tensor.split(im_info, axis=1, num_or_sections=2)\n        else:\n            im_h, im_w, im_s = paddle.tensor.split(\n                im_info, axis=1, num_or_sections=3)\n        proposal = self.clip_tiled_boxes(im_w, im_h, proposal,\n                                         self.pixel_offset)\n\n        if self.type == \"generate_proposals_v2\":\n            keep = self.filter_boxes_v2(proposal, im_w, im_h, self.min_size,\n                                        self.pixel_offset)\n        else:\n            keep = self.filter_boxes(proposal, im_w, im_h, im_s, self.min_size)\n\n        tail_proposal = paddle.zeros(shape=[1, 4], dtype=proposal.dtype)\n        proposal_num = paddle.shape(proposal)[0]\n        tail_keep = paddle.reshape(proposal_num, shape=[1, 1])\n        tail_keep = paddle.cast(tail_keep, dtype=keep.dtype)\n        tail_scores = paddle.zeros(shape=[1, 1], dtype=scores.dtype)\n        # proposal = paddle.concat([proposal, tail_proposal])\n        # keep = paddle.concat([keep, tail_keep])\n        # scores = paddle.concat([scores, tail_scores])\n\n        bbox_sel = paddle.gather(proposal, keep, axis=0)\n        scores_sel = paddle.gather(scores, keep, axis=0)\n        proposal = paddle.unsqueeze(bbox_sel, axis=0)\n        scores = paddle.transpose(scores_sel, perm=[1, 0])\n        scores = paddle.unsqueeze(scores, axis=0)\n        out = layers.multiclass_nms(\n            proposal,\n            scores,\n            background_label=-1,\n            nms_top_k=self.pre_nms_topN,\n            score_threshold=-10000.,\n            keep_top_k=self.post_nms_topN,\n            nms_threshold=self.nms_thresh,\n            normalized=False if self.pixel_offset else True,\n            nms_eta=self.eta)\n        label, scores, proposal = paddle.tensor.split(\n            out, axis=1, num_or_sections=[1, 1, 4])\n        return scores, proposal\n\n    def forward(self):\n        anchors = self.input('Anchors', 0)\n        bboxdeltas = self.input('BboxDeltas', 0)\n        if self.type == 'generate_proposals_v2':\n            iminfo = self.input('ImShape', 0)\n        else:\n            iminfo = self.input('ImInfo', 0)\n        scores = self.input('Scores', 0)\n        variances = self.input('Variances', 0)\n\n        bboxdeltas = paddle.transpose(bboxdeltas, perm=[0, 2, 3, 1])\n        bboxdeltas = paddle.reshape(bboxdeltas, [-1, 4])\n        scores = paddle.transpose(scores, perm=[0, 2, 3, 1])\n        scores = paddle.reshape(scores, [-1, 1])\n        anchors = paddle.reshape(anchors, [-1, 4])\n        variances = paddle.reshape(variances, [-1, 4])\n\n        new_scores, proposals = self.proposal_for_single_sample(\n            anchors, bboxdeltas, iminfo, scores, variances)\n        if len(self.node.outputs) == 3:\n            rois_num = paddle.shape(new_scores)[0]\n            return {\n                'RpnRoiProbs': [new_scores],\n                'RpnRois': [proposals],\n                'RpnRoisNum': [rois_num]\n            }\n        else:\n            return {'RpnRoiProbs': [new_scores], 'RpnRois': [proposals]}",
  "def __init__(self, node, **kw):\n        paddle.enable_static()\n        super(GenerateProposals, self).__init__(node)\n        self.eta = node.attr('eta')\n        self.min_size = node.attr('min_size')\n        self.nms_thresh = node.attr('nms_thresh')\n        self.post_nms_topN = node.attr('post_nms_topN')\n        self.pre_nms_topN = node.attr('pre_nms_topN')\n        self.type = node.type\n        if self.type == 'generate_proposals_v2':\n            self.pixel_offset = node.attr('pixel_offset')\n        else:\n            self.pixel_offset = True",
  "def filter_boxes(self, boxes, im_w, im_h, im_s, min_size):\n        min_size = max(min_size, 1.0)\n        xmin, ymin, xmax, ymax = paddle.tensor.split(\n            boxes, axis=1, num_or_sections=4)\n        x_ctr = (xmax + xmin) / 2 + 0.5\n        y_ctr = (ymax + ymin) / 2 + 0.5\n        ws = (xmax - xmin) / im_s + 1\n        hs = (ymax - ymin) / im_s + 1\n\n        min_size = np.asarray([min_size], dtype='float32')\n        min_size = paddle.assign(min_size)\n        valid_flag_ws = paddle.greater_equal(ws, min_size)\n        valid_flag_hs = paddle.greater_equal(hs, min_size)\n        valid_flag_x = paddle.less_equal(x_ctr, im_w)\n        valid_flag_y = paddle.less_equal(y_ctr, im_h)\n        valid_flag = paddle.logical_and(valid_flag_ws, valid_flag_hs)\n        valid_flag = paddle.logical_and(valid_flag, valid_flag_x)\n        valid_flag = paddle.logical_and(valid_flag, valid_flag_y)\n        valid_flag = paddle.squeeze(valid_flag, axis=1)\n        valid_inds = paddle.nonzero(valid_flag)\n\n        return valid_inds",
  "def filter_boxes_v2(self, boxes, im_w, im_h, min_size, pixel_offset=True):\n        min_size = max(min_size, 1.0)\n        xmin, ymin, xmax, ymax = paddle.tensor.split(\n            boxes, axis=1, num_or_sections=4)\n\n        offset = 1 if pixel_offset else 0\n        ws = (xmax - xmin) + offset\n        hs = (ymax - ymin) + offset\n\n        min_size = np.asarray([min_size], dtype='float32')\n        min_size = paddle.assign(min_size)\n        valid_flag_ws = paddle.greater_equal(ws, min_size)\n        valid_flag_hs = paddle.greater_equal(hs, min_size)\n        valid_flag = paddle.logical_and(valid_flag_ws, valid_flag_hs)\n        if pixel_offset:\n            x_ctr = xmin + ws / 2\n            y_ctr = ymin + hs / 2\n            valid_flag_x = paddle.less_equal(x_ctr, im_w)\n            valid_flag_y = paddle.less_equal(y_ctr, im_h)\n            valid_flag = paddle.logical_and(valid_flag, valid_flag_x)\n            valid_flag = paddle.logical_and(valid_flag, valid_flag_y)\n\n        valid_flag = paddle.squeeze(valid_flag, axis=1)\n        valid_inds = paddle.nonzero(valid_flag)\n        return valid_inds",
  "def clip_tiled_boxes(self, im_w, im_h, input_boxes, pixel_offset=True):\n        offset = 1 if pixel_offset else 0\n        xmin, ymin, xmax, ymax = paddle.tensor.split(\n            input_boxes, axis=1, num_or_sections=4)\n        xmin = paddle.clip(xmin, max=im_w - offset, min=0)\n        ymin = paddle.clip(ymin, max=im_h - offset, min=0)\n        xmax = paddle.clip(xmax, max=im_w - offset, min=0)\n        ymax = paddle.clip(ymax, max=im_h - offset, min=0)\n        input_boxes = paddle.concat([xmin, ymin, xmax, ymax], axis=1)\n        return input_boxes",
  "def box_encode(self, anchors, bbox_deltas, variances, pixel_offset=True):\n        offset = 1 if pixel_offset else 0\n        anchor_xmin, anchor_ymin, anchor_xmax, anchor_ymax = paddle.tensor.split(\n            anchors, axis=1, num_or_sections=4)\n        anchor_width = anchor_xmax - anchor_xmin + offset\n        anchor_height = anchor_ymax - anchor_ymin + offset\n        anchor_center_x = anchor_xmin + 0.5 * anchor_width\n        anchor_center_y = anchor_ymin + 0.5 * anchor_height\n        var_center_x, var_center_y, var_width, var_height = paddle.tensor.split(\n            variances, axis=1, num_or_sections=4)\n        delta_center_x, delta_center_y, delta_width, delta_height = paddle.tensor.split(\n            bbox_deltas, axis=1, num_or_sections=4)\n\n        bbox_center_x = var_center_x * delta_center_x * anchor_width + anchor_center_x\n        bbox_center_y = var_center_y * delta_center_y * anchor_height + anchor_center_y\n        bbox_width = paddle.exp(\n            paddle.clip(\n                var_width * delta_width, max=BBOX_CLIP_DEFAULT)) * anchor_width\n        bbox_height = paddle.exp(\n            paddle.clip(\n                var_height * delta_height,\n                max=BBOX_CLIP_DEFAULT)) * anchor_height\n\n        proposal_xmin = bbox_center_x - bbox_width / 2\n        proposal_ymin = bbox_center_y - bbox_height / 2\n        proposal_xmax = bbox_center_x + bbox_width / 2 - offset\n        proposal_ymax = bbox_center_y + bbox_height / 2 - offset\n        proposal = paddle.concat(\n            [proposal_xmin, proposal_ymin, proposal_xmax, proposal_ymax],\n            axis=1)\n        return proposal",
  "def proposal_for_single_sample(self, anchors, bbox_deltas, im_info, scores,\n                                   variances):\n        proposal_num = paddle.shape(scores)[0]\n        pre_nms_top_n_tensor = paddle.assign(\n            np.asarray(\n                [self.pre_nms_topN], dtype='int32'))\n        k_candidate = paddle.concat([proposal_num, pre_nms_top_n_tensor])\n        k = paddle.min(k_candidate)\n        scores, index = paddle.topk(scores, k=k, axis=0)\n        bbox_deltas = paddle.gather(bbox_deltas, index, axis=0)\n        anchors = paddle.gather(anchors, index, axis=0)\n        variances = paddle.gather(variances, index, axis=0)\n\n        proposal = self.box_encode(anchors, bbox_deltas, variances,\n                                   self.pixel_offset)\n        if self.type == \"generate_proposals_v2\":\n            im_h, im_w = paddle.tensor.split(im_info, axis=1, num_or_sections=2)\n        else:\n            im_h, im_w, im_s = paddle.tensor.split(\n                im_info, axis=1, num_or_sections=3)\n        proposal = self.clip_tiled_boxes(im_w, im_h, proposal,\n                                         self.pixel_offset)\n\n        if self.type == \"generate_proposals_v2\":\n            keep = self.filter_boxes_v2(proposal, im_w, im_h, self.min_size,\n                                        self.pixel_offset)\n        else:\n            keep = self.filter_boxes(proposal, im_w, im_h, im_s, self.min_size)\n\n        tail_proposal = paddle.zeros(shape=[1, 4], dtype=proposal.dtype)\n        proposal_num = paddle.shape(proposal)[0]\n        tail_keep = paddle.reshape(proposal_num, shape=[1, 1])\n        tail_keep = paddle.cast(tail_keep, dtype=keep.dtype)\n        tail_scores = paddle.zeros(shape=[1, 1], dtype=scores.dtype)\n        # proposal = paddle.concat([proposal, tail_proposal])\n        # keep = paddle.concat([keep, tail_keep])\n        # scores = paddle.concat([scores, tail_scores])\n\n        bbox_sel = paddle.gather(proposal, keep, axis=0)\n        scores_sel = paddle.gather(scores, keep, axis=0)\n        proposal = paddle.unsqueeze(bbox_sel, axis=0)\n        scores = paddle.transpose(scores_sel, perm=[1, 0])\n        scores = paddle.unsqueeze(scores, axis=0)\n        out = layers.multiclass_nms(\n            proposal,\n            scores,\n            background_label=-1,\n            nms_top_k=self.pre_nms_topN,\n            score_threshold=-10000.,\n            keep_top_k=self.post_nms_topN,\n            nms_threshold=self.nms_thresh,\n            normalized=False if self.pixel_offset else True,\n            nms_eta=self.eta)\n        label, scores, proposal = paddle.tensor.split(\n            out, axis=1, num_or_sections=[1, 1, 4])\n        return scores, proposal",
  "def forward(self):\n        anchors = self.input('Anchors', 0)\n        bboxdeltas = self.input('BboxDeltas', 0)\n        if self.type == 'generate_proposals_v2':\n            iminfo = self.input('ImShape', 0)\n        else:\n            iminfo = self.input('ImInfo', 0)\n        scores = self.input('Scores', 0)\n        variances = self.input('Variances', 0)\n\n        bboxdeltas = paddle.transpose(bboxdeltas, perm=[0, 2, 3, 1])\n        bboxdeltas = paddle.reshape(bboxdeltas, [-1, 4])\n        scores = paddle.transpose(scores, perm=[0, 2, 3, 1])\n        scores = paddle.reshape(scores, [-1, 1])\n        anchors = paddle.reshape(anchors, [-1, 4])\n        variances = paddle.reshape(variances, [-1, 4])\n\n        new_scores, proposals = self.proposal_for_single_sample(\n            anchors, bboxdeltas, iminfo, scores, variances)\n        if len(self.node.outputs) == 3:\n            rois_num = paddle.shape(new_scores)[0]\n            return {\n                'RpnRoiProbs': [new_scores],\n                'RpnRois': [proposals],\n                'RpnRoisNum': [rois_num]\n            }\n        else:\n            return {'RpnRoiProbs': [new_scores], 'RpnRois': [proposals]}",
  "class CollectFpnProposals(CustomPaddleOp):\n    def __init__(self, node, **kw):\n        super(CollectFpnProposals, self).__init__(node)\n        self.post_nms_top_n = node.attr('post_nms_topN')\n\n    def forward(self):\n        multi_level_rois = self.input('MultiLevelRois')\n        multi_level_scores = self.input('MultiLevelScores')\n        multi_level_rois = paddle.concat(multi_level_rois, axis=0)\n        multi_level_scores = paddle.concat(multi_level_scores, axis=0)\n        proposal_num = paddle.shape(multi_level_scores)[0]\n        post_nms_top_n_tensor = paddle.assign(\n            np.array([self.post_nms_top_n]).astype('int32'))\n        k_candidate = paddle.concat([proposal_num, post_nms_top_n_tensor])\n        k = paddle.min(k_candidate)\n        scores, index = paddle.topk(multi_level_scores, k=k, axis=0)\n        rois = paddle.gather(multi_level_rois, index, axis=0)\n        return {\"FpnRois\": [rois]}",
  "class Collectfpnproposals:\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'collect_fpn_proposals',\n            inputs=node.input('MultiLevelRois')+ node.input('MultiLevelScores'),\n            outputs=node.output('FpnRois'),\n            post_nms_top_n = node.attr('post_nms_topN'),\n            domain = 'custom')",
  "def __init__(self, node, **kw):\n        super(CollectFpnProposals, self).__init__(node)\n        self.post_nms_top_n = node.attr('post_nms_topN')",
  "def forward(self):\n        multi_level_rois = self.input('MultiLevelRois')\n        multi_level_scores = self.input('MultiLevelScores')\n        multi_level_rois = paddle.concat(multi_level_rois, axis=0)\n        multi_level_scores = paddle.concat(multi_level_scores, axis=0)\n        proposal_num = paddle.shape(multi_level_scores)[0]\n        post_nms_top_n_tensor = paddle.assign(\n            np.array([self.post_nms_top_n]).astype('int32'))\n        k_candidate = paddle.concat([proposal_num, post_nms_top_n_tensor])\n        k = paddle.min(k_candidate)\n        scores, index = paddle.topk(multi_level_scores, k=k, axis=0)\n        rois = paddle.gather(multi_level_rois, index, axis=0)\n        return {\"FpnRois\": [rois]}",
  "def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'collect_fpn_proposals',\n            inputs=node.input('MultiLevelRois')+ node.input('MultiLevelScores'),\n            outputs=node.output('FpnRois'),\n            post_nms_top_n = node.attr('post_nms_topN'),\n            domain = 'custom')",
  "class DeformConv2d(CustomPaddleOp):\n    def check_attribute(self, node):\n        utils.compare_attr_between_dims(\n            node.attr('strides'), (0, 1), 'strides', 'equal')\n        utils.compare_attr_between_dims(\n            node.attr('paddings'), (0, 1), 'paddings', 'equal')\n        utils.compare_attr_between_dims(\n            node.input_shape('Offset', 0), (2, 3), 'Offset', 'equal')\n        utils.compare_attr(\n            node.attr('deformable_groups'), 1, 'deformable_groups', 'equal')\n\n    def __init__(self, node, **kw):\n        super(DeformConv2d, self).__init__(node)\n        self.check_attribute(node)\n        self.in_channel = node.input_shape('Input', 0)[1]\n        self.offset_channel = node.input_shape('Offset', 0)[1]\n        self.stride = node.attr('strides')[0]\n        self.padding = node.attr('paddings')\n        if len(self.padding) == 2:\n            self.padding += self.padding\n        self.groups = node.attr('groups')\n        self.dilation = node.attr('dilations')[0]\n        self.padded_x_h = node.input_shape('Input', 0)[2]\n        self.padded_x_w = node.input_shape('Input', 0)[3]\n        if self.padded_x_h > 0:\n            self.padded_x_h = self.padded_x_h + self.padding[0] + self.padding[1]\n        if self.padded_x_w > 0:\n            self.padded_x_w = self.padded_x_w + self.padding[2] + self.padding[3]\n\n        self.kernel_size = node.input_shape('Filter', 0)[2]\n        self.N = self.kernel_size**2\n        self.num_filters = node.input_shape('Filter', 0)[0]\n\n    def forward(self):\n        input = self.input('Input', 0)\n        weight = self.input('Filter', 0)\n        mask = self.input('Mask', 0)\n        offset = self.input('Offset', 0)\n\n        input = layers.pad2d(input, self.padding)\n        input_shape = paddle.shape(input)\n        if self.padded_x_h < 0 or self.padded_x_w < 0:\n            self.padded_x_h = input_shape[2]\n            self.padded_x_w = input_shape[3]\n\n        offset_x = paddle.strided_slice(\n            offset,\n            axes=[1],\n            starts=[0],\n            ends=[self.offset_channel],\n            strides=[2])\n        offset_y = paddle.strided_slice(\n            offset,\n            axes=[1],\n            starts=[1],\n            ends=[self.offset_channel],\n            strides=[2])\n        offset = paddle.concat([offset_x, offset_y], axis=1)\n        offset_shape = paddle.shape(offset)\n        offset_h = offset_shape[2]\n        offset_w = offset_shape[3]\n\n        coordinate = self.get_offset_coordinate(offset, 'float32', offset_shape)\n\n        coordinate = coordinate.transpose((0, 2, 3, 1))\n        coord_lt, coord_rb, coord_lb, coord_rt = self.get_bilinear_corner_coordinate(\n            coordinate, self.padded_x_h, self.padded_x_w)\n\n        # clip coordinate\n        coordinate = paddle.concat(\n            [\n                paddle.clip(coordinate[:, :, :, :self.N], 0,\n                            self.padded_x_h - 1),\n                paddle.clip(coordinate[:, :, :, self.N:], 0,\n                            self.padded_x_w - 1)\n            ],\n            axis=-1)\n\n        cof_lt, cof_rb, cof_lb, cof_rt = self.get_bilinear_coefficient(\n            coord_lt, coord_rb, coord_lb, coord_rt, coordinate)\n\n        feature_lt = self.get_feature_by_coordinate(input, coord_lt, offset_h,\n                                                    offset_w, self.padded_x_w)\n        feature_rb = self.get_feature_by_coordinate(input, coord_rb, offset_h,\n                                                    offset_w, self.padded_x_w)\n        feature_lb = self.get_feature_by_coordinate(input, coord_lb, offset_h,\n                                                    offset_w, self.padded_x_w)\n        feature_rt = self.get_feature_by_coordinate(input, coord_rt, offset_h,\n                                                    offset_w, self.padded_x_w)\n\n        feature_after_deformation = paddle.unsqueeze(cof_lt, 1) * feature_lt + \\\n                   paddle.unsqueeze(cof_rb, 1) * feature_rb + \\\n                   paddle.unsqueeze(cof_lb, 1) * feature_lb + \\\n                   paddle.unsqueeze(cof_rt, 1) * feature_rt\n\n        # modulation\n        if mask is not None:\n            mask = paddle.transpose(mask, (0, 2, 3, 1))\n            mask = paddle.unsqueeze(mask, 1)\n            mask = paddle.tile(mask, [1, self.in_channel, 1, 1, 1])\n            feature_after_deformation *= mask\n\n        feature_after_deformation = self.reshape_feature(\n            feature_after_deformation, offset_h, offset_w)\n\n        out = paddle.nn.functional.conv2d(\n            feature_after_deformation,\n            weight,\n            stride=self.kernel_size,\n            groups=self.groups)\n\n        return {'Output': [out]}\n\n    def get_offset_coordinate(self, offset, dtype, offset_shape):\n        kernel_grid_origin_x = paddle.arange(\n            0,\n            self.kernel_size + (self.kernel_size - 1) * (self.dilation - 1),\n            step=self.dilation,\n            dtype=dtype)\n        kernel_grid_origin_x = kernel_grid_origin_x.unsqueeze(1)\n        kernel_grid_origin_x = paddle.tile(kernel_grid_origin_x,\n                                           [1, self.kernel_size])\n        kernel_grid_origin_y = paddle.arange(\n            0,\n            self.kernel_size + (self.kernel_size - 1) * (self.dilation - 1),\n            step=self.dilation,\n            dtype=dtype)\n        kernel_grid_origin_y = kernel_grid_origin_y.unsqueeze(0)\n        kernel_grid_origin_y = paddle.tile(kernel_grid_origin_y,\n                                           [self.kernel_size, 1])\n        kernel_grid_origin_x = paddle.reshape(kernel_grid_origin_x, [-1])\n        kernel_grid_origin_y = paddle.reshape(kernel_grid_origin_y, [-1])\n        kernel_grid_origin = paddle.concat(\n            [kernel_grid_origin_x, kernel_grid_origin_y], -1)\n        kernel_grid_origin = paddle.reshape(kernel_grid_origin,\n                                            (1, 2 * self.N, 1, 1))\n\n        kernel_offset_x = paddle.arange(\n            0, offset_shape[2] * self.stride, step=self.stride, dtype=dtype)\n        kernel_offset_x = kernel_offset_x.unsqueeze(1)\n        kernel_offset_x = paddle.expand(kernel_offset_x, offset_shape[2:])\n        kernel_offset_y = paddle.arange(\n            0, offset_shape[3] * self.stride, step=self.stride, dtype=dtype)\n        kernel_offset_y = kernel_offset_y.unsqueeze(0)\n        kernel_offset_y = paddle.expand(kernel_offset_y, offset_shape[2:])\n        kernel_offset_x = kernel_offset_x.unsqueeze([0, 1])\n        kernel_offset_x = paddle.tile(kernel_offset_x, (1, self.N, 1, 1))\n        kernel_offset_y = kernel_offset_y.unsqueeze([0, 1])\n        kernel_offset_y = paddle.tile(kernel_offset_y, (1, self.N, 1, 1))\n\n        kernel_offset = paddle.concat([kernel_offset_x, kernel_offset_y], 1)\n        offset = offset + paddle.cast(kernel_offset, 'float32') + paddle.cast(\n            kernel_grid_origin, 'float32')\n\n        return offset\n\n    def get_bilinear_corner_coordinate(self, coord, padded_h, padded_w):\n        coord_lt = coord.floor()\n        coord_rb = coord_lt + 1\n        coord_lt = paddle.cast(\n            paddle.concat(\n                [\n                    paddle.clip(coord_lt[:, :, :, :self.N], 0, padded_h - 1),\n                    paddle.clip(coord_lt[:, :, :, self.N:], 0, padded_w - 1)\n                ],\n                axis=-1),\n            dtype='int64')\n        coord_rb = paddle.cast(\n            paddle.concat(\n                [\n                    paddle.clip(coord_rb[:, :, :, :self.N], 0, padded_h - 1),\n                    paddle.clip(coord_rb[:, :, :, self.N:], 0, padded_w - 1)\n                ],\n                axis=-1),\n            dtype='int64')\n        coord_lb = paddle.concat(\n            [coord_lt[:, :, :, :self.N], coord_rb[:, :, :, self.N:]], axis=-1)\n        coord_rt = paddle.concat(\n            [coord_rb[:, :, :, :self.N], coord_lt[:, :, :, self.N:]], axis=-1)\n\n        return coord_lt, coord_rb, coord_lb, coord_rt\n\n    def get_bilinear_coefficient(self, coord_lt, coord_rb, coord_lb, coord_rt,\n                                 p):\n        cof_lt = (1 + (paddle.cast(\n            coord_lt[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])\n                  ) * (1 + paddle.cast(\n                      coord_lt[:, :, :, self.N:], dtype='float32') -\n                       p[:, :, :, self.N:])\n        cof_rb = (1 - (paddle.cast(\n            coord_rb[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])\n                  ) * (1 - (paddle.cast(\n                      coord_rb[:, :, :, self.N:], dtype='float32') -\n                            p[:, :, :, self.N:]))\n        cof_lb = (1 + (paddle.cast(\n            coord_lb[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])\n                  ) * (1 - (paddle.cast(\n                      coord_lb[:, :, :, self.N:], dtype='float32') -\n                            p[:, :, :, self.N:]))\n        cof_rt = (1 - (paddle.cast(\n            coord_rt[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])\n                  ) * (1 + paddle.cast(\n                      coord_rt[:, :, :, self.N:], dtype='float32') -\n                       p[:, :, :, self.N:])\n\n        return cof_lt, cof_rb, cof_lb, cof_rt\n\n    def get_feature_by_coordinate(self, x, coord, offset_h, offset_w,\n                                  padded_x_w):\n        x = paddle.reshape(x, [0, 0, -1])\n        index = paddle.cast(\n            coord[:, :, :, :self.N] * padded_x_w,\n            dtype='int64') + coord[:, :, :, self.N:]  # offset_x*w + offset_y\n        index = paddle.unsqueeze(index, 1)\n        index = paddle.tile(index, [1, self.in_channel, 1, 1, 1])\n        index = paddle.reshape(index, (0, 0, -1))\n        x_range = list(range(3))\n        dim = 2\n        x_range[0] = dim\n        x_range[dim] = 0\n        x_swaped = paddle.transpose(x, perm=x_range)\n        index_range = list(range(3))\n        index_range[0] = dim\n        index_range[dim] = 0\n        index_swaped = paddle.transpose(index, perm=index_range)\n        x_shape = layers.shape(x_swaped)\n        index_shape = layers.shape(index_swaped)\n        prod = paddle.prod(x_shape[1:], keepdim=True)\n        x_swaped_flattend = paddle.reshape(x_swaped, [-1])\n        index_swaped_flattend = paddle.reshape(index_swaped, [-1])\n        index_swaped_flattend *= prod\n        bias = paddle.arange(start=0, end=prod, step=1, dtype='float32')\n        bias = paddle.tile(bias, index_shape[0])\n        index_swaped_flattend += bias\n        gathered = paddle.gather(x_swaped_flattend, index_swaped_flattend)\n        gathered = paddle.reshape(gathered, layers.shape(index_swaped))\n        x_offset = paddle.transpose(gathered, perm=x_range)\n        x_offset = paddle.reshape(\n            x_offset, (-1, self.in_channel, offset_h, offset_w, self.N))\n        return x_offset\n\n    def reshape_feature(self, x_offset, offset_h, offset_w):\n        x_offset = paddle.concat(\n            [\n                paddle.reshape(x_offset[:, :, :, :, s:s + self.kernel_size], (\n                    -1, self.in_channel, offset_h, offset_w * self.kernel_size))\n                for s in range(0, self.N, self.kernel_size)\n            ],\n            axis=-1)\n        x_offset = paddle.reshape(x_offset, (-1, self.in_channel,\n                                             offset_h * self.kernel_size,\n                                             offset_w * self.kernel_size))\n        return x_offset",
  "class Deformconv2d:\n    @classmethod\n    def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'deformable_conv',\n            inputs=node.input('Input')+node.input('Filter')+node.input('Mask')+node.input('Offset'),\n            outputs=node.output('Output'),\n            stride = node.attr('strides'),\n            padding = node.attr('paddings'),\n            groups = node.attr('groups'),\n            dilation = node.attr('dilations'),\n            deformable_groups = node.attr('deformable_groups'),\n            domain = 'custom')",
  "def check_attribute(self, node):\n        utils.compare_attr_between_dims(\n            node.attr('strides'), (0, 1), 'strides', 'equal')\n        utils.compare_attr_between_dims(\n            node.attr('paddings'), (0, 1), 'paddings', 'equal')\n        utils.compare_attr_between_dims(\n            node.input_shape('Offset', 0), (2, 3), 'Offset', 'equal')\n        utils.compare_attr(\n            node.attr('deformable_groups'), 1, 'deformable_groups', 'equal')",
  "def __init__(self, node, **kw):\n        super(DeformConv2d, self).__init__(node)\n        self.check_attribute(node)\n        self.in_channel = node.input_shape('Input', 0)[1]\n        self.offset_channel = node.input_shape('Offset', 0)[1]\n        self.stride = node.attr('strides')[0]\n        self.padding = node.attr('paddings')\n        if len(self.padding) == 2:\n            self.padding += self.padding\n        self.groups = node.attr('groups')\n        self.dilation = node.attr('dilations')[0]\n        self.padded_x_h = node.input_shape('Input', 0)[2]\n        self.padded_x_w = node.input_shape('Input', 0)[3]\n        if self.padded_x_h > 0:\n            self.padded_x_h = self.padded_x_h + self.padding[0] + self.padding[1]\n        if self.padded_x_w > 0:\n            self.padded_x_w = self.padded_x_w + self.padding[2] + self.padding[3]\n\n        self.kernel_size = node.input_shape('Filter', 0)[2]\n        self.N = self.kernel_size**2\n        self.num_filters = node.input_shape('Filter', 0)[0]",
  "def forward(self):\n        input = self.input('Input', 0)\n        weight = self.input('Filter', 0)\n        mask = self.input('Mask', 0)\n        offset = self.input('Offset', 0)\n\n        input = layers.pad2d(input, self.padding)\n        input_shape = paddle.shape(input)\n        if self.padded_x_h < 0 or self.padded_x_w < 0:\n            self.padded_x_h = input_shape[2]\n            self.padded_x_w = input_shape[3]\n\n        offset_x = paddle.strided_slice(\n            offset,\n            axes=[1],\n            starts=[0],\n            ends=[self.offset_channel],\n            strides=[2])\n        offset_y = paddle.strided_slice(\n            offset,\n            axes=[1],\n            starts=[1],\n            ends=[self.offset_channel],\n            strides=[2])\n        offset = paddle.concat([offset_x, offset_y], axis=1)\n        offset_shape = paddle.shape(offset)\n        offset_h = offset_shape[2]\n        offset_w = offset_shape[3]\n\n        coordinate = self.get_offset_coordinate(offset, 'float32', offset_shape)\n\n        coordinate = coordinate.transpose((0, 2, 3, 1))\n        coord_lt, coord_rb, coord_lb, coord_rt = self.get_bilinear_corner_coordinate(\n            coordinate, self.padded_x_h, self.padded_x_w)\n\n        # clip coordinate\n        coordinate = paddle.concat(\n            [\n                paddle.clip(coordinate[:, :, :, :self.N], 0,\n                            self.padded_x_h - 1),\n                paddle.clip(coordinate[:, :, :, self.N:], 0,\n                            self.padded_x_w - 1)\n            ],\n            axis=-1)\n\n        cof_lt, cof_rb, cof_lb, cof_rt = self.get_bilinear_coefficient(\n            coord_lt, coord_rb, coord_lb, coord_rt, coordinate)\n\n        feature_lt = self.get_feature_by_coordinate(input, coord_lt, offset_h,\n                                                    offset_w, self.padded_x_w)\n        feature_rb = self.get_feature_by_coordinate(input, coord_rb, offset_h,\n                                                    offset_w, self.padded_x_w)\n        feature_lb = self.get_feature_by_coordinate(input, coord_lb, offset_h,\n                                                    offset_w, self.padded_x_w)\n        feature_rt = self.get_feature_by_coordinate(input, coord_rt, offset_h,\n                                                    offset_w, self.padded_x_w)\n\n        feature_after_deformation = paddle.unsqueeze(cof_lt, 1) * feature_lt + \\\n                   paddle.unsqueeze(cof_rb, 1) * feature_rb + \\\n                   paddle.unsqueeze(cof_lb, 1) * feature_lb + \\\n                   paddle.unsqueeze(cof_rt, 1) * feature_rt\n\n        # modulation\n        if mask is not None:\n            mask = paddle.transpose(mask, (0, 2, 3, 1))\n            mask = paddle.unsqueeze(mask, 1)\n            mask = paddle.tile(mask, [1, self.in_channel, 1, 1, 1])\n            feature_after_deformation *= mask\n\n        feature_after_deformation = self.reshape_feature(\n            feature_after_deformation, offset_h, offset_w)\n\n        out = paddle.nn.functional.conv2d(\n            feature_after_deformation,\n            weight,\n            stride=self.kernel_size,\n            groups=self.groups)\n\n        return {'Output': [out]}",
  "def get_offset_coordinate(self, offset, dtype, offset_shape):\n        kernel_grid_origin_x = paddle.arange(\n            0,\n            self.kernel_size + (self.kernel_size - 1) * (self.dilation - 1),\n            step=self.dilation,\n            dtype=dtype)\n        kernel_grid_origin_x = kernel_grid_origin_x.unsqueeze(1)\n        kernel_grid_origin_x = paddle.tile(kernel_grid_origin_x,\n                                           [1, self.kernel_size])\n        kernel_grid_origin_y = paddle.arange(\n            0,\n            self.kernel_size + (self.kernel_size - 1) * (self.dilation - 1),\n            step=self.dilation,\n            dtype=dtype)\n        kernel_grid_origin_y = kernel_grid_origin_y.unsqueeze(0)\n        kernel_grid_origin_y = paddle.tile(kernel_grid_origin_y,\n                                           [self.kernel_size, 1])\n        kernel_grid_origin_x = paddle.reshape(kernel_grid_origin_x, [-1])\n        kernel_grid_origin_y = paddle.reshape(kernel_grid_origin_y, [-1])\n        kernel_grid_origin = paddle.concat(\n            [kernel_grid_origin_x, kernel_grid_origin_y], -1)\n        kernel_grid_origin = paddle.reshape(kernel_grid_origin,\n                                            (1, 2 * self.N, 1, 1))\n\n        kernel_offset_x = paddle.arange(\n            0, offset_shape[2] * self.stride, step=self.stride, dtype=dtype)\n        kernel_offset_x = kernel_offset_x.unsqueeze(1)\n        kernel_offset_x = paddle.expand(kernel_offset_x, offset_shape[2:])\n        kernel_offset_y = paddle.arange(\n            0, offset_shape[3] * self.stride, step=self.stride, dtype=dtype)\n        kernel_offset_y = kernel_offset_y.unsqueeze(0)\n        kernel_offset_y = paddle.expand(kernel_offset_y, offset_shape[2:])\n        kernel_offset_x = kernel_offset_x.unsqueeze([0, 1])\n        kernel_offset_x = paddle.tile(kernel_offset_x, (1, self.N, 1, 1))\n        kernel_offset_y = kernel_offset_y.unsqueeze([0, 1])\n        kernel_offset_y = paddle.tile(kernel_offset_y, (1, self.N, 1, 1))\n\n        kernel_offset = paddle.concat([kernel_offset_x, kernel_offset_y], 1)\n        offset = offset + paddle.cast(kernel_offset, 'float32') + paddle.cast(\n            kernel_grid_origin, 'float32')\n\n        return offset",
  "def get_bilinear_corner_coordinate(self, coord, padded_h, padded_w):\n        coord_lt = coord.floor()\n        coord_rb = coord_lt + 1\n        coord_lt = paddle.cast(\n            paddle.concat(\n                [\n                    paddle.clip(coord_lt[:, :, :, :self.N], 0, padded_h - 1),\n                    paddle.clip(coord_lt[:, :, :, self.N:], 0, padded_w - 1)\n                ],\n                axis=-1),\n            dtype='int64')\n        coord_rb = paddle.cast(\n            paddle.concat(\n                [\n                    paddle.clip(coord_rb[:, :, :, :self.N], 0, padded_h - 1),\n                    paddle.clip(coord_rb[:, :, :, self.N:], 0, padded_w - 1)\n                ],\n                axis=-1),\n            dtype='int64')\n        coord_lb = paddle.concat(\n            [coord_lt[:, :, :, :self.N], coord_rb[:, :, :, self.N:]], axis=-1)\n        coord_rt = paddle.concat(\n            [coord_rb[:, :, :, :self.N], coord_lt[:, :, :, self.N:]], axis=-1)\n\n        return coord_lt, coord_rb, coord_lb, coord_rt",
  "def get_bilinear_coefficient(self, coord_lt, coord_rb, coord_lb, coord_rt,\n                                 p):\n        cof_lt = (1 + (paddle.cast(\n            coord_lt[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])\n                  ) * (1 + paddle.cast(\n                      coord_lt[:, :, :, self.N:], dtype='float32') -\n                       p[:, :, :, self.N:])\n        cof_rb = (1 - (paddle.cast(\n            coord_rb[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])\n                  ) * (1 - (paddle.cast(\n                      coord_rb[:, :, :, self.N:], dtype='float32') -\n                            p[:, :, :, self.N:]))\n        cof_lb = (1 + (paddle.cast(\n            coord_lb[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])\n                  ) * (1 - (paddle.cast(\n                      coord_lb[:, :, :, self.N:], dtype='float32') -\n                            p[:, :, :, self.N:]))\n        cof_rt = (1 - (paddle.cast(\n            coord_rt[:, :, :, :self.N], dtype='float32') - p[:, :, :, :self.N])\n                  ) * (1 + paddle.cast(\n                      coord_rt[:, :, :, self.N:], dtype='float32') -\n                       p[:, :, :, self.N:])\n\n        return cof_lt, cof_rb, cof_lb, cof_rt",
  "def get_feature_by_coordinate(self, x, coord, offset_h, offset_w,\n                                  padded_x_w):\n        x = paddle.reshape(x, [0, 0, -1])\n        index = paddle.cast(\n            coord[:, :, :, :self.N] * padded_x_w,\n            dtype='int64') + coord[:, :, :, self.N:]  # offset_x*w + offset_y\n        index = paddle.unsqueeze(index, 1)\n        index = paddle.tile(index, [1, self.in_channel, 1, 1, 1])\n        index = paddle.reshape(index, (0, 0, -1))\n        x_range = list(range(3))\n        dim = 2\n        x_range[0] = dim\n        x_range[dim] = 0\n        x_swaped = paddle.transpose(x, perm=x_range)\n        index_range = list(range(3))\n        index_range[0] = dim\n        index_range[dim] = 0\n        index_swaped = paddle.transpose(index, perm=index_range)\n        x_shape = layers.shape(x_swaped)\n        index_shape = layers.shape(index_swaped)\n        prod = paddle.prod(x_shape[1:], keepdim=True)\n        x_swaped_flattend = paddle.reshape(x_swaped, [-1])\n        index_swaped_flattend = paddle.reshape(index_swaped, [-1])\n        index_swaped_flattend *= prod\n        bias = paddle.arange(start=0, end=prod, step=1, dtype='float32')\n        bias = paddle.tile(bias, index_shape[0])\n        index_swaped_flattend += bias\n        gathered = paddle.gather(x_swaped_flattend, index_swaped_flattend)\n        gathered = paddle.reshape(gathered, layers.shape(index_swaped))\n        x_offset = paddle.transpose(gathered, perm=x_range)\n        x_offset = paddle.reshape(\n            x_offset, (-1, self.in_channel, offset_h, offset_w, self.N))\n        return x_offset",
  "def reshape_feature(self, x_offset, offset_h, offset_w):\n        x_offset = paddle.concat(\n            [\n                paddle.reshape(x_offset[:, :, :, :, s:s + self.kernel_size], (\n                    -1, self.in_channel, offset_h, offset_w * self.kernel_size))\n                for s in range(0, self.N, self.kernel_size)\n            ],\n            axis=-1)\n        x_offset = paddle.reshape(x_offset, (-1, self.in_channel,\n                                             offset_h * self.kernel_size,\n                                             offset_w * self.kernel_size))\n        return x_offset",
  "def opset_1(cls, graph, node, **kw):\n        node = graph.make_node(\n            'deformable_conv',\n            inputs=node.input('Input')+node.input('Filter')+node.input('Mask')+node.input('Offset'),\n            outputs=node.output('Output'),\n            stride = node.attr('strides'),\n            padding = node.attr('paddings'),\n            groups = node.attr('groups'),\n            dilation = node.attr('dilations'),\n            deformable_groups = node.attr('deformable_groups'),\n            domain = 'custom')",
  "class _MatrixEntryIterator(object):\n    def __init__(self, rows, cols, rowMajor):\n        self.rows = rows\n        self.cols = cols\n        self.currentRow = 0\n        self.currentCol = 0\n        self.rowMajor = rowMajor\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        return self.__next__()  # Python 2.x compatibility\n\n    def __next__(self):\n        row = self.currentRow\n        col = self.currentCol\n        if self.rowMajor == 0:\n            if self.currentCol >= self.cols:\n                raise StopIteration\n\n            self.currentRow = self.currentRow + 1\n            if self.currentRow >= self.rows:\n                self.currentRow = 0\n                self.currentCol = self.currentCol + 1\n        else:\n            if self.currentRow >= self.rows:\n                raise StopIteration\n\n            self.currentCol = self.currentCol + 1\n            if self.currentCol >= self.cols:\n                self.currentCol = 0\n                self.currentRow = self.currentRow + 1\n\n        return (row, col)",
  "class EigenMatrixPrinter:\n    \"Print Eigen Matrix or Array of some kind\"\n\n    def __init__(self, variety, val):\n        \"Extract all the necessary information\"\n\n        # Save the variety (presumably \"Matrix\" or \"Array\") for later usage\n        self.variety = variety\n\n        # The gdb extension does not support value template arguments - need to extract them by hand\n        type = val.type\n        if type.code == gdb.TYPE_CODE_REF:\n            type = type.target()\n        self.type = type.unqualified().strip_typedefs()\n        tag = self.type.tag\n        regex = re.compile('\\<.*\\>')\n        m = regex.findall(tag)[0][1:-1]\n        template_params = m.split(',')\n        template_params = [x.replace(\" \", \"\") for x in template_params]\n\n        if template_params[1] == '-0x00000000000000001' or template_params[\n                1] == '-0x000000001' or template_params[1] == '-1':\n            self.rows = val['m_storage']['m_rows']\n        else:\n            self.rows = int(template_params[1])\n\n        if template_params[2] == '-0x00000000000000001' or template_params[\n                2] == '-0x000000001' or template_params[2] == '-1':\n            self.cols = val['m_storage']['m_cols']\n        else:\n            self.cols = int(template_params[2])\n\n        self.options = 0  # default value\n        if len(template_params) > 3:\n            self.options = template_params[3]\n\n        self.rowMajor = (int(self.options) & 0x1)\n\n        self.innerType = self.type.template_argument(0)\n\n        self.val = val\n\n        # Fixed size matrices have a struct as their storage, so we need to walk through this\n        self.data = self.val['m_storage']['m_data']\n        if self.data.type.code == gdb.TYPE_CODE_STRUCT:\n            self.data = self.data['array']\n            self.data = self.data.cast(self.innerType.pointer())\n\n    class _iterator(_MatrixEntryIterator):\n        def __init__(self, rows, cols, dataPtr, rowMajor):\n            super(EigenMatrixPrinter._iterator, self).__init__(rows, cols,\n                                                               rowMajor)\n\n            self.dataPtr = dataPtr\n\n        def __next__(self):\n\n            row, col = super(EigenMatrixPrinter._iterator, self).__next__()\n\n            item = self.dataPtr.dereference()\n            self.dataPtr = self.dataPtr + 1\n            if (self.cols == 1):  #if it's a column vector\n                return ('[%d]' % (row, ), item)\n            elif (self.rows == 1):  #if it's a row vector\n                return ('[%d]' % (col, ), item)\n            return ('[%d,%d]' % (row, col), item)\n\n    def children(self):\n\n        return self._iterator(self.rows, self.cols, self.data, self.rowMajor)\n\n    def to_string(self):\n        return \"Eigen::%s<%s,%d,%d,%s> (data ptr: %s)\" % (\n            self.variety, self.innerType, self.rows, self.cols, \"RowMajor\"\n            if self.rowMajor else \"ColMajor\", self.data)",
  "class EigenSparseMatrixPrinter:\n    \"Print an Eigen SparseMatrix\"\n\n    def __init__(self, val):\n        \"Extract all the necessary information\"\n\n        type = val.type\n        if type.code == gdb.TYPE_CODE_REF:\n            type = type.target()\n        self.type = type.unqualified().strip_typedefs()\n        tag = self.type.tag\n        regex = re.compile('\\<.*\\>')\n        m = regex.findall(tag)[0][1:-1]\n        template_params = m.split(',')\n        template_params = [x.replace(\" \", \"\") for x in template_params]\n\n        self.options = 0\n        if len(template_params) > 1:\n            self.options = template_params[1]\n\n        self.rowMajor = (int(self.options) & 0x1)\n\n        self.innerType = self.type.template_argument(0)\n\n        self.val = val\n\n        self.data = self.val['m_data']\n        self.data = self.data.cast(self.innerType.pointer())\n\n    class _iterator(_MatrixEntryIterator):\n        def __init__(self, rows, cols, val, rowMajor):\n            super(EigenSparseMatrixPrinter._iterator, self).__init__(\n                rows, cols, rowMajor)\n\n            self.val = val\n\n        def __next__(self):\n\n            row, col = super(EigenSparseMatrixPrinter._iterator,\n                             self).__next__()\n\n            # repeat calculations from SparseMatrix.h:\n            outer = row if self.rowMajor else col\n            inner = col if self.rowMajor else row\n            start = self.val['m_outerIndex'][outer]\n            end = ((start + self.val['m_innerNonZeros'][outer])\n                   if self.val['m_innerNonZeros'] else\n                   self.val['m_outerIndex'][outer + 1])\n\n            # and from CompressedStorage.h:\n            data = self.val['m_data']\n            if start >= end:\n                item = 0\n            elif (end > start) and (inner == data['m_indices'][end - 1]):\n                item = data['m_values'][end - 1]\n            else:\n                # create Python index list from the target range within m_indices\n                indices = [\n                    data['m_indices'][x]\n                    for x in range(int(start), int(end) - 1)\n                ]\n                # find the index with binary search\n                idx = int(start) + bisect_left(indices, inner)\n                if ((idx < end) and (data['m_indices'][idx] == inner)):\n                    item = data['m_values'][idx]\n                else:\n                    item = 0\n\n            return ('[%d,%d]' % (row, col), item)\n\n    def children(self):\n        if self.data:\n            return self._iterator(self.rows(),\n                                  self.cols(), self.val, self.rowMajor)\n\n        return iter([])  # empty matrix, for now\n\n    def rows(self):\n        return self.val['m_outerSize'] if self.rowMajor else self.val[\n            'm_innerSize']\n\n    def cols(self):\n        return self.val['m_innerSize'] if self.rowMajor else self.val[\n            'm_outerSize']\n\n    def to_string(self):\n\n        if self.data:\n            status = (\"not compressed\"\n                      if self.val['m_innerNonZeros'] else \"compressed\")\n        else:\n            status = \"empty\"\n        dimensions = \"%d x %d\" % (self.rows(), self.cols())\n        layout = \"row\" if self.rowMajor else \"column\"\n\n        return \"Eigen::SparseMatrix<%s>, %s, %s major, %s\" % (\n            self.innerType, dimensions, layout, status)",
  "class EigenQuaternionPrinter:\n    \"Print an Eigen Quaternion\"\n\n    def __init__(self, val):\n        \"Extract all the necessary information\"\n        # The gdb extension does not support value template arguments - need to extract them by hand\n        type = val.type\n        if type.code == gdb.TYPE_CODE_REF:\n            type = type.target()\n        self.type = type.unqualified().strip_typedefs()\n        self.innerType = self.type.template_argument(0)\n        self.val = val\n\n        # Quaternions have a struct as their storage, so we need to walk through this\n        self.data = self.val['m_coeffs']['m_storage']['m_data']['array']\n        self.data = self.data.cast(self.innerType.pointer())\n\n    class _iterator:\n        def __init__(self, dataPtr):\n            self.dataPtr = dataPtr\n            self.currentElement = 0\n            self.elementNames = ['x', 'y', 'z', 'w']\n\n        def __iter__(self):\n            return self\n\n        def next(self):\n            return self.__next__()  # Python 2.x compatibility\n\n        def __next__(self):\n            element = self.currentElement\n\n            if self.currentElement >= 4:  #there are 4 elements in a quanternion\n                raise StopIteration\n\n            self.currentElement = self.currentElement + 1\n\n            item = self.dataPtr.dereference()\n            self.dataPtr = self.dataPtr + 1\n            return ('[%s]' % (self.elementNames[element], ), item)\n\n    def children(self):\n\n        return self._iterator(self.data)\n\n    def to_string(self):\n        return \"Eigen::Quaternion<%s> (data ptr: %s)\" % (self.innerType,\n                                                         self.data)",
  "def build_eigen_dictionary():\n    pretty_printers_dict[re.compile(\n        '^Eigen::Quaternion<.*>$')] = lambda val: EigenQuaternionPrinter(val)\n    pretty_printers_dict[re.compile(\n        '^Eigen::Matrix<.*>$')] = lambda val: EigenMatrixPrinter(\"Matrix\", val)\n    pretty_printers_dict[\n        re.compile('^Eigen::SparseMatrix<.*>$'\n                   )] = lambda val: EigenSparseMatrixPrinter(val)\n    pretty_printers_dict[re.compile(\n        '^Eigen::Array<.*>$')] = lambda val: EigenMatrixPrinter(\"Array\", val)",
  "def register_eigen_printers(obj):\n    \"Register eigen pretty-printers with objfile Obj\"\n\n    if obj == None:\n        obj = gdb\n    obj.pretty_printers.append(lookup_function)",
  "def lookup_function(val):\n    \"Look-up and return a pretty-printer that can print va.\"\n\n    type = val.type\n\n    if type.code == gdb.TYPE_CODE_REF:\n        type = type.target()\n\n    type = type.unqualified().strip_typedefs()\n\n    typename = type.tag\n    if typename == None:\n        return None\n\n    for function in pretty_printers_dict:\n        if function.search(typename):\n            return pretty_printers_dict[function](val)\n\n    return None",
  "def __init__(self, rows, cols, rowMajor):\n        self.rows = rows\n        self.cols = cols\n        self.currentRow = 0\n        self.currentCol = 0\n        self.rowMajor = rowMajor",
  "def __iter__(self):\n        return self",
  "def next(self):\n        return self.__next__()",
  "def __next__(self):\n        row = self.currentRow\n        col = self.currentCol\n        if self.rowMajor == 0:\n            if self.currentCol >= self.cols:\n                raise StopIteration\n\n            self.currentRow = self.currentRow + 1\n            if self.currentRow >= self.rows:\n                self.currentRow = 0\n                self.currentCol = self.currentCol + 1\n        else:\n            if self.currentRow >= self.rows:\n                raise StopIteration\n\n            self.currentCol = self.currentCol + 1\n            if self.currentCol >= self.cols:\n                self.currentCol = 0\n                self.currentRow = self.currentRow + 1\n\n        return (row, col)",
  "def __init__(self, variety, val):\n        \"Extract all the necessary information\"\n\n        # Save the variety (presumably \"Matrix\" or \"Array\") for later usage\n        self.variety = variety\n\n        # The gdb extension does not support value template arguments - need to extract them by hand\n        type = val.type\n        if type.code == gdb.TYPE_CODE_REF:\n            type = type.target()\n        self.type = type.unqualified().strip_typedefs()\n        tag = self.type.tag\n        regex = re.compile('\\<.*\\>')\n        m = regex.findall(tag)[0][1:-1]\n        template_params = m.split(',')\n        template_params = [x.replace(\" \", \"\") for x in template_params]\n\n        if template_params[1] == '-0x00000000000000001' or template_params[\n                1] == '-0x000000001' or template_params[1] == '-1':\n            self.rows = val['m_storage']['m_rows']\n        else:\n            self.rows = int(template_params[1])\n\n        if template_params[2] == '-0x00000000000000001' or template_params[\n                2] == '-0x000000001' or template_params[2] == '-1':\n            self.cols = val['m_storage']['m_cols']\n        else:\n            self.cols = int(template_params[2])\n\n        self.options = 0  # default value\n        if len(template_params) > 3:\n            self.options = template_params[3]\n\n        self.rowMajor = (int(self.options) & 0x1)\n\n        self.innerType = self.type.template_argument(0)\n\n        self.val = val\n\n        # Fixed size matrices have a struct as their storage, so we need to walk through this\n        self.data = self.val['m_storage']['m_data']\n        if self.data.type.code == gdb.TYPE_CODE_STRUCT:\n            self.data = self.data['array']\n            self.data = self.data.cast(self.innerType.pointer())",
  "class _iterator(_MatrixEntryIterator):\n        def __init__(self, rows, cols, dataPtr, rowMajor):\n            super(EigenMatrixPrinter._iterator, self).__init__(rows, cols,\n                                                               rowMajor)\n\n            self.dataPtr = dataPtr\n\n        def __next__(self):\n\n            row, col = super(EigenMatrixPrinter._iterator, self).__next__()\n\n            item = self.dataPtr.dereference()\n            self.dataPtr = self.dataPtr + 1\n            if (self.cols == 1):  #if it's a column vector\n                return ('[%d]' % (row, ), item)\n            elif (self.rows == 1):  #if it's a row vector\n                return ('[%d]' % (col, ), item)\n            return ('[%d,%d]' % (row, col), item)",
  "def children(self):\n\n        return self._iterator(self.rows, self.cols, self.data, self.rowMajor)",
  "def to_string(self):\n        return \"Eigen::%s<%s,%d,%d,%s> (data ptr: %s)\" % (\n            self.variety, self.innerType, self.rows, self.cols, \"RowMajor\"\n            if self.rowMajor else \"ColMajor\", self.data)",
  "def __init__(self, val):\n        \"Extract all the necessary information\"\n\n        type = val.type\n        if type.code == gdb.TYPE_CODE_REF:\n            type = type.target()\n        self.type = type.unqualified().strip_typedefs()\n        tag = self.type.tag\n        regex = re.compile('\\<.*\\>')\n        m = regex.findall(tag)[0][1:-1]\n        template_params = m.split(',')\n        template_params = [x.replace(\" \", \"\") for x in template_params]\n\n        self.options = 0\n        if len(template_params) > 1:\n            self.options = template_params[1]\n\n        self.rowMajor = (int(self.options) & 0x1)\n\n        self.innerType = self.type.template_argument(0)\n\n        self.val = val\n\n        self.data = self.val['m_data']\n        self.data = self.data.cast(self.innerType.pointer())",
  "class _iterator(_MatrixEntryIterator):\n        def __init__(self, rows, cols, val, rowMajor):\n            super(EigenSparseMatrixPrinter._iterator, self).__init__(\n                rows, cols, rowMajor)\n\n            self.val = val\n\n        def __next__(self):\n\n            row, col = super(EigenSparseMatrixPrinter._iterator,\n                             self).__next__()\n\n            # repeat calculations from SparseMatrix.h:\n            outer = row if self.rowMajor else col\n            inner = col if self.rowMajor else row\n            start = self.val['m_outerIndex'][outer]\n            end = ((start + self.val['m_innerNonZeros'][outer])\n                   if self.val['m_innerNonZeros'] else\n                   self.val['m_outerIndex'][outer + 1])\n\n            # and from CompressedStorage.h:\n            data = self.val['m_data']\n            if start >= end:\n                item = 0\n            elif (end > start) and (inner == data['m_indices'][end - 1]):\n                item = data['m_values'][end - 1]\n            else:\n                # create Python index list from the target range within m_indices\n                indices = [\n                    data['m_indices'][x]\n                    for x in range(int(start), int(end) - 1)\n                ]\n                # find the index with binary search\n                idx = int(start) + bisect_left(indices, inner)\n                if ((idx < end) and (data['m_indices'][idx] == inner)):\n                    item = data['m_values'][idx]\n                else:\n                    item = 0\n\n            return ('[%d,%d]' % (row, col), item)",
  "def children(self):\n        if self.data:\n            return self._iterator(self.rows(),\n                                  self.cols(), self.val, self.rowMajor)\n\n        return iter([])",
  "def rows(self):\n        return self.val['m_outerSize'] if self.rowMajor else self.val[\n            'm_innerSize']",
  "def cols(self):\n        return self.val['m_innerSize'] if self.rowMajor else self.val[\n            'm_outerSize']",
  "def to_string(self):\n\n        if self.data:\n            status = (\"not compressed\"\n                      if self.val['m_innerNonZeros'] else \"compressed\")\n        else:\n            status = \"empty\"\n        dimensions = \"%d x %d\" % (self.rows(), self.cols())\n        layout = \"row\" if self.rowMajor else \"column\"\n\n        return \"Eigen::SparseMatrix<%s>, %s, %s major, %s\" % (\n            self.innerType, dimensions, layout, status)",
  "def __init__(self, val):\n        \"Extract all the necessary information\"\n        # The gdb extension does not support value template arguments - need to extract them by hand\n        type = val.type\n        if type.code == gdb.TYPE_CODE_REF:\n            type = type.target()\n        self.type = type.unqualified().strip_typedefs()\n        self.innerType = self.type.template_argument(0)\n        self.val = val\n\n        # Quaternions have a struct as their storage, so we need to walk through this\n        self.data = self.val['m_coeffs']['m_storage']['m_data']['array']\n        self.data = self.data.cast(self.innerType.pointer())",
  "class _iterator:\n        def __init__(self, dataPtr):\n            self.dataPtr = dataPtr\n            self.currentElement = 0\n            self.elementNames = ['x', 'y', 'z', 'w']\n\n        def __iter__(self):\n            return self\n\n        def next(self):\n            return self.__next__()  # Python 2.x compatibility\n\n        def __next__(self):\n            element = self.currentElement\n\n            if self.currentElement >= 4:  #there are 4 elements in a quanternion\n                raise StopIteration\n\n            self.currentElement = self.currentElement + 1\n\n            item = self.dataPtr.dereference()\n            self.dataPtr = self.dataPtr + 1\n            return ('[%s]' % (self.elementNames[element], ), item)",
  "def children(self):\n\n        return self._iterator(self.data)",
  "def to_string(self):\n        return \"Eigen::Quaternion<%s> (data ptr: %s)\" % (self.innerType,\n                                                         self.data)",
  "def __init__(self, rows, cols, dataPtr, rowMajor):\n            super(EigenMatrixPrinter._iterator, self).__init__(rows, cols,\n                                                               rowMajor)\n\n            self.dataPtr = dataPtr",
  "def __next__(self):\n\n            row, col = super(EigenMatrixPrinter._iterator, self).__next__()\n\n            item = self.dataPtr.dereference()\n            self.dataPtr = self.dataPtr + 1\n            if (self.cols == 1):  #if it's a column vector\n                return ('[%d]' % (row, ), item)\n            elif (self.rows == 1):  #if it's a row vector\n                return ('[%d]' % (col, ), item)\n            return ('[%d,%d]' % (row, col), item)",
  "def __init__(self, rows, cols, val, rowMajor):\n            super(EigenSparseMatrixPrinter._iterator, self).__init__(\n                rows, cols, rowMajor)\n\n            self.val = val",
  "def __next__(self):\n\n            row, col = super(EigenSparseMatrixPrinter._iterator,\n                             self).__next__()\n\n            # repeat calculations from SparseMatrix.h:\n            outer = row if self.rowMajor else col\n            inner = col if self.rowMajor else row\n            start = self.val['m_outerIndex'][outer]\n            end = ((start + self.val['m_innerNonZeros'][outer])\n                   if self.val['m_innerNonZeros'] else\n                   self.val['m_outerIndex'][outer + 1])\n\n            # and from CompressedStorage.h:\n            data = self.val['m_data']\n            if start >= end:\n                item = 0\n            elif (end > start) and (inner == data['m_indices'][end - 1]):\n                item = data['m_values'][end - 1]\n            else:\n                # create Python index list from the target range within m_indices\n                indices = [\n                    data['m_indices'][x]\n                    for x in range(int(start), int(end) - 1)\n                ]\n                # find the index with binary search\n                idx = int(start) + bisect_left(indices, inner)\n                if ((idx < end) and (data['m_indices'][idx] == inner)):\n                    item = data['m_values'][idx]\n                else:\n                    item = 0\n\n            return ('[%d,%d]' % (row, col), item)",
  "def __init__(self, dataPtr):\n            self.dataPtr = dataPtr\n            self.currentElement = 0\n            self.elementNames = ['x', 'y', 'z', 'w']",
  "def __iter__(self):\n            return self",
  "def next(self):\n            return self.__next__()",
  "def __next__(self):\n            element = self.currentElement\n\n            if self.currentElement >= 4:  #there are 4 elements in a quanternion\n                raise StopIteration\n\n            self.currentElement = self.currentElement + 1\n\n            item = self.dataPtr.dereference()\n            self.dataPtr = self.dataPtr + 1\n            return ('[%s]' % (self.elementNames[element], ), item)",
  "def cd(path):\n    if not os.path.isabs(path):\n        raise RuntimeError('Can only cd to absolute path, got: {}'.format(path))\n    orig_path = os.getcwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(orig_path)",
  "class ONNXCommand(setuptools.Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass",
  "class create_version(ONNXCommand):\n    def run(self):\n        with open(os.path.join(SRC_DIR, 'version.py'), 'w') as f:\n            f.write(dedent('''\\\n            # This file is generated by setup.py. DO NOT EDIT!\n\n            from __future__ import absolute_import\n            from __future__ import division\n            from __future__ import print_function\n            from __future__ import unicode_literals\n\n            version = '{version}'\n            git_version = '{git_version}'\n            '''.format(**dict(VersionInfo._asdict()))))",
  "class cmake_build(setuptools.Command):\n    \"\"\"\n    Compiles everything when `python setupmnm.py build` is run using cmake.\n\n    Custom args can be passed to cmake by specifying the `CMAKE_ARGS`\n    environment variable.\n\n    The number of CPUs used by `make` can be specified by passing `-j<ncpus>`\n    to `setup.py build`.  By default all CPUs are used.\n    \"\"\"\n    user_options = [\n        (str('jobs='), str('j'), str('Specifies the number of jobs to use with make'))\n    ]\n\n    built = False\n\n    def initialize_options(self):\n        self.jobs = None\n\n    def finalize_options(self):\n        if sys.version_info[0] >= 3:\n            self.set_undefined_options('build', ('parallel', 'jobs'))\n        if self.jobs is None and os.getenv(\"MAX_JOBS\") is not None:\n            self.jobs = os.getenv(\"MAX_JOBS\")\n        self.jobs = multiprocessing.cpu_count() if self.jobs is None else int(self.jobs)\n\n    def run(self):\n        if cmake_build.built:\n            return\n        cmake_build.built = True\n        if not os.path.exists(CMAKE_BUILD_DIR):\n            os.makedirs(CMAKE_BUILD_DIR)\n\n        with cd(CMAKE_BUILD_DIR):\n            build_type = 'Release'\n            # configure\n            cmake_args = [\n                CMAKE,\n                '-DPYTHON_INCLUDE_DIR={}'.format(sysconfig.get_python_inc()),\n                '-DPYTHON_EXECUTABLE={}'.format(sys.executable),\n                '-DBUILD_ONNX_PYTHON=ON',\n                '-DONNX_USE_LITE_PROTO=ON',\n                '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON',\n                '-DONNX_NAMESPACE={}'.format(ONNX_NAMESPACE),\n                '-DPY_EXT_SUFFIX={}'.format(\n                    sysconfig.get_config_var('EXT_SUFFIX') or ''),\n            ]\n            if COVERAGE:\n                cmake_args.append('-DONNX_COVERAGE=ON')\n            if COVERAGE or DEBUG:\n                # in order to get accurate coverage information, the\n                # build needs to turn off optimizations\n                build_type = 'Debug'\n            cmake_args.append('-DCMAKE_BUILD_TYPE=%s' % build_type)\n            if WINDOWS:\n                cmake_args.extend([\n                    # we need to link with libpython on windows, so\n                    # passing python version to window in order to\n                    # find python in cmake\n                    '-DPY_VERSION={}'.format('{0}.{1}'.format(* \\\n                                                              sys.version_info[:2])),\n                ])\n                if USE_MSVC_STATIC_RUNTIME:\n                    cmake_args.append('-DONNX_USE_MSVC_STATIC_RUNTIME=ON')\n                if platform.architecture()[0] == '64bit':\n                    cmake_args.extend(['-A', 'x64', '-T', 'host=x64'])\n                else:\n                    cmake_args.extend(['-A', 'Win32', '-T', 'host=x86'])\n            if ONNX_ML:\n                cmake_args.append('-DONNX_ML=1')\n            if ONNX_VERIFY_PROTO3:\n                cmake_args.append('-DONNX_VERIFY_PROTO3=1')\n            if ONNX_BUILD_TESTS:\n                cmake_args.append('-DONNX_BUILD_TESTS=ON')\n            if 'CMAKE_ARGS' in os.environ:\n                extra_cmake_args = shlex.split(os.environ['CMAKE_ARGS'])\n                # prevent crossfire with downstream scripts\n                del os.environ['CMAKE_ARGS']\n                log.info('Extra cmake args: {}'.format(extra_cmake_args))\n                cmake_args.extend(extra_cmake_args)\n            cmake_args.append(TOP_DIR)\n            subprocess.check_call(cmake_args)\n\n            build_args = [CMAKE, '--build', os.curdir]\n            if WINDOWS:\n                build_args.extend(['--config', build_type])\n                build_args.extend(['--', '/maxcpucount:{}'.format(self.jobs)])\n            else:\n                build_args.extend(['--', '-j', str(self.jobs)])\n            subprocess.check_call(build_args)",
  "class build_py(setuptools.command.build_py.build_py):\n    def run(self):\n        self.run_command('create_version')\n        self.run_command('cmake_build')\n\n        generated_python_files = \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, 'onnxoptimizer', '*.py')) + \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, 'onnxoptimizer', '*.pyi'))\n\n        for src in generated_python_files:\n            dst = os.path.join(\n                TOP_DIR, os.path.relpath(src, CMAKE_BUILD_DIR))\n            self.copy_file(src, dst)\n\n        return setuptools.command.build_py.build_py.run(self)",
  "class develop(setuptools.command.develop.develop):\n    def run(self):\n        self.run_command('build_py')\n        setuptools.command.develop.develop.run(self)",
  "class build_ext(setuptools.command.build_ext.build_ext):\n    def run(self):\n        self.run_command('cmake_build')\n        setuptools.command.build_ext.build_ext.run(self)\n\n    def build_extensions(self):\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = os.path.basename(self.get_ext_filename(fullname))\n\n            lib_path = CMAKE_BUILD_DIR\n            if os.name == 'nt':\n                debug_lib_dir = os.path.join(lib_path, \"Debug\")\n                release_lib_dir = os.path.join(lib_path, \"Release\")\n                if os.path.exists(debug_lib_dir):\n                    lib_path = debug_lib_dir\n                elif os.path.exists(release_lib_dir):\n                    lib_path = release_lib_dir\n            src = os.path.join(lib_path, filename)\n            dst = os.path.join(os.path.realpath(\n                self.build_lib), \"onnxoptimizer\", filename)\n            self.copy_file(src, dst)",
  "class mypy_type_check(ONNXCommand):\n    description = 'Run MyPy type checker'\n\n    def run(self):\n        \"\"\"Run command.\"\"\"\n        onnx_script = os.path.realpath(os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), \"tools/mypy-onnx.py\"))\n        returncode = subprocess.call([sys.executable, onnx_script])\n        sys.exit(returncode)",
  "def initialize_options(self):\n        pass",
  "def finalize_options(self):\n        pass",
  "def run(self):\n        with open(os.path.join(SRC_DIR, 'version.py'), 'w') as f:\n            f.write(dedent('''\\\n            # This file is generated by setup.py. DO NOT EDIT!\n\n            from __future__ import absolute_import\n            from __future__ import division\n            from __future__ import print_function\n            from __future__ import unicode_literals\n\n            version = '{version}'\n            git_version = '{git_version}'\n            '''.format(**dict(VersionInfo._asdict()))))",
  "def initialize_options(self):\n        self.jobs = None",
  "def finalize_options(self):\n        if sys.version_info[0] >= 3:\n            self.set_undefined_options('build', ('parallel', 'jobs'))\n        if self.jobs is None and os.getenv(\"MAX_JOBS\") is not None:\n            self.jobs = os.getenv(\"MAX_JOBS\")\n        self.jobs = multiprocessing.cpu_count() if self.jobs is None else int(self.jobs)",
  "def run(self):\n        if cmake_build.built:\n            return\n        cmake_build.built = True\n        if not os.path.exists(CMAKE_BUILD_DIR):\n            os.makedirs(CMAKE_BUILD_DIR)\n\n        with cd(CMAKE_BUILD_DIR):\n            build_type = 'Release'\n            # configure\n            cmake_args = [\n                CMAKE,\n                '-DPYTHON_INCLUDE_DIR={}'.format(sysconfig.get_python_inc()),\n                '-DPYTHON_EXECUTABLE={}'.format(sys.executable),\n                '-DBUILD_ONNX_PYTHON=ON',\n                '-DONNX_USE_LITE_PROTO=ON',\n                '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON',\n                '-DONNX_NAMESPACE={}'.format(ONNX_NAMESPACE),\n                '-DPY_EXT_SUFFIX={}'.format(\n                    sysconfig.get_config_var('EXT_SUFFIX') or ''),\n            ]\n            if COVERAGE:\n                cmake_args.append('-DONNX_COVERAGE=ON')\n            if COVERAGE or DEBUG:\n                # in order to get accurate coverage information, the\n                # build needs to turn off optimizations\n                build_type = 'Debug'\n            cmake_args.append('-DCMAKE_BUILD_TYPE=%s' % build_type)\n            if WINDOWS:\n                cmake_args.extend([\n                    # we need to link with libpython on windows, so\n                    # passing python version to window in order to\n                    # find python in cmake\n                    '-DPY_VERSION={}'.format('{0}.{1}'.format(* \\\n                                                              sys.version_info[:2])),\n                ])\n                if USE_MSVC_STATIC_RUNTIME:\n                    cmake_args.append('-DONNX_USE_MSVC_STATIC_RUNTIME=ON')\n                if platform.architecture()[0] == '64bit':\n                    cmake_args.extend(['-A', 'x64', '-T', 'host=x64'])\n                else:\n                    cmake_args.extend(['-A', 'Win32', '-T', 'host=x86'])\n            if ONNX_ML:\n                cmake_args.append('-DONNX_ML=1')\n            if ONNX_VERIFY_PROTO3:\n                cmake_args.append('-DONNX_VERIFY_PROTO3=1')\n            if ONNX_BUILD_TESTS:\n                cmake_args.append('-DONNX_BUILD_TESTS=ON')\n            if 'CMAKE_ARGS' in os.environ:\n                extra_cmake_args = shlex.split(os.environ['CMAKE_ARGS'])\n                # prevent crossfire with downstream scripts\n                del os.environ['CMAKE_ARGS']\n                log.info('Extra cmake args: {}'.format(extra_cmake_args))\n                cmake_args.extend(extra_cmake_args)\n            cmake_args.append(TOP_DIR)\n            subprocess.check_call(cmake_args)\n\n            build_args = [CMAKE, '--build', os.curdir]\n            if WINDOWS:\n                build_args.extend(['--config', build_type])\n                build_args.extend(['--', '/maxcpucount:{}'.format(self.jobs)])\n            else:\n                build_args.extend(['--', '-j', str(self.jobs)])\n            subprocess.check_call(build_args)",
  "def run(self):\n        self.run_command('create_version')\n        self.run_command('cmake_build')\n\n        generated_python_files = \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, 'onnxoptimizer', '*.py')) + \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, 'onnxoptimizer', '*.pyi'))\n\n        for src in generated_python_files:\n            dst = os.path.join(\n                TOP_DIR, os.path.relpath(src, CMAKE_BUILD_DIR))\n            self.copy_file(src, dst)\n\n        return setuptools.command.build_py.build_py.run(self)",
  "def run(self):\n        self.run_command('build_py')\n        setuptools.command.develop.develop.run(self)",
  "def run(self):\n        self.run_command('cmake_build')\n        setuptools.command.build_ext.build_ext.run(self)",
  "def build_extensions(self):\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = os.path.basename(self.get_ext_filename(fullname))\n\n            lib_path = CMAKE_BUILD_DIR\n            if os.name == 'nt':\n                debug_lib_dir = os.path.join(lib_path, \"Debug\")\n                release_lib_dir = os.path.join(lib_path, \"Release\")\n                if os.path.exists(debug_lib_dir):\n                    lib_path = debug_lib_dir\n                elif os.path.exists(release_lib_dir):\n                    lib_path = release_lib_dir\n            src = os.path.join(lib_path, filename)\n            dst = os.path.join(os.path.realpath(\n                self.build_lib), \"onnxoptimizer\", filename)\n            self.copy_file(src, dst)",
  "def run(self):\n        \"\"\"Run command.\"\"\"\n        onnx_script = os.path.realpath(os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), \"tools/mypy-onnx.py\"))\n        returncode = subprocess.call([sys.executable, onnx_script])\n        sys.exit(returncode)",
  "def optimize(model, passes=None, fixed_point=False):  # type: (ModelProto, Optional[Sequence[Text]], bool) -> ModelProto\n    \"\"\"Apply the optimization on the serialized ModelProto.\n\n    Arguments:\n        input (ModelProto): model\n        names (list of string): list of optimization names\n\n    Return:\n        return (ModelProto) optimized model\n    \"\"\"\n\n    if passes is None:\n        print('WARNING: defualt optimization passes will be enlarged to all fuse and elimination passes in the next version')\n        passes = ['eliminate_nop_transpose',\n                  'eliminate_nop_pad',\n                  'fuse_consecutive_transposes',\n                  'fuse_transpose_into_gemm']\n    if not isinstance(model, ModelProto):\n        raise ValueError(\n            'Optimizer only accepts ModelProto, incorrect type: {}'.format(type(model)))\n\n    model_str = model.SerializeToString()\n    if fixed_point:\n        optimized_model_str = C.optimize_fixedpoint(model_str, passes)\n    else:\n        optimized_model_str = C.optimize(model_str, passes)\n\n    return onnx.load_from_string(optimized_model_str)",
  "def main():  # type: () -> None\n    try:\n        root_folder = os.path.realpath(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n        os.chdir(root_folder)\n\n        subprocess.check_call([\"mypy\", \".\"])\n        subprocess.check_call([\"mypy\", \"--py2\", \".\"])\n\n        exit(0)\n    except subprocess.CalledProcessError:\n        # Catch this exception because we don't want it to output a backtrace that would clutter the mypy output\n        exit(1)",
  "def lint(session: nox.Session) -> None:\n    \"\"\"\n    Lint the codebase (except for clang-format/tidy).\n    \"\"\"\n    session.install(\"pre-commit\")\n    session.run(\"pre-commit\", \"run\", \"-a\")",
  "def tests(session: nox.Session) -> None:\n    \"\"\"\n    Run the tests (requires a compiler).\n    \"\"\"\n    tmpdir = session.create_tmp()\n    session.install(\"cmake\")\n    session.install(\"-r\", \"tests/requirements.txt\")\n    session.run(\n        \"cmake\",\n        \"-S.\",\n        f\"-B{tmpdir}\",\n        \"-DPYBIND11_WERROR=ON\",\n        \"-DDOWNLOAD_CATCH=ON\",\n        \"-DDOWNLOAD_EIGEN=ON\",\n        *session.posargs, )\n    session.run(\"cmake\", \"--build\", tmpdir)\n    session.run(\"cmake\", \"--build\", tmpdir, \"--config=Release\", \"--target\",\n                \"check\")",
  "def tests_packaging(session: nox.Session) -> None:\n    \"\"\"\n    Run the packaging tests.\n    \"\"\"\n\n    session.install(\"-r\", \"tests/requirements.txt\", \"--prefer-binary\")\n    session.run(\"pytest\", \"tests/extra_python_package\")",
  "def docs(session: nox.Session) -> None:\n    \"\"\"\n    Build the docs. Pass \"serve\" to serve.\n    \"\"\"\n\n    session.install(\"-r\", \"docs/requirements.txt\")\n    session.chdir(\"docs\")\n\n    if \"pdf\" in session.posargs:\n        session.run(\"sphinx-build\", \"-b\", \"latexpdf\", \".\", \"_build\")\n        return\n\n    session.run(\"sphinx-build\", \"-b\", \"html\", \".\", \"_build\")\n\n    if \"serve\" in session.posargs:\n        session.log(\n            \"Launching docs at http://localhost:8000/ - use Ctrl-C to quit\")\n        session.run(\"python\", \"-m\", \"http.server\", \"8000\", \"-d\", \"_build/html\")\n    elif session.posargs:\n        session.error(\"Unsupported argument to docs\")",
  "def make_changelog(session: nox.Session) -> None:\n    \"\"\"\n    Inspect the closed issues and make entries for a changelog.\n    \"\"\"\n    session.install(\"ghapi\", \"rich\")\n    session.run(\"python\", \"tools/make_changelog.py\")",
  "def build(session: nox.Session) -> None:\n    \"\"\"\n    Build SDists and wheels.\n    \"\"\"\n\n    session.install(\"build\")\n    session.log(\"Building normal files\")\n    session.run(\"python\", \"-m\", \"build\", *session.posargs)\n    session.log(\"Building pybind11-global files (PYBIND11_GLOBAL_SDIST=1)\")\n    session.run(\"python\",\n                \"-m\",\n                \"build\",\n                *session.posargs,\n                env={\"PYBIND11_GLOBAL_SDIST\": \"1\"})",
  "def build_expected_version_hex(matches: Dict[str, str]) -> str:\n    patch_level_serial = matches[\"PATCH\"]\n    serial = None\n    major = int(matches[\"MAJOR\"])\n    minor = int(matches[\"MINOR\"])\n    flds = patch_level_serial.split(\".\")\n    if flds:\n        patch = int(flds[0])\n        if len(flds) == 1:\n            level = \"0\"\n            serial = 0\n        elif len(flds) == 2:\n            level_serial = flds[1]\n            for level in (\"a\", \"b\", \"c\", \"dev\"):\n                if level_serial.startswith(level):\n                    serial = int(level_serial[len(level):])\n                    break\n    if serial is None:\n        msg = f'Invalid PYBIND11_VERSION_PATCH: \"{patch_level_serial}\"'\n        raise RuntimeError(msg)\n    version_hex_str = f\"{major:02x}{minor:02x}{patch:02x}{level[:1]}{serial:x}\"\n    return f\"0x{version_hex_str.upper()}\"",
  "def get_and_replace(filename: Path, binary: bool=False,\n                    **opts: str) -> Union[bytes, str]:\n    if binary:\n        contents = filename.read_bytes()\n        return string.Template(contents.decode()).substitute(opts).encode()\n\n    return string.Template(filename.read_text()).substitute(opts)",
  "class SDist(setuptools.command.sdist.sdist):  # type: ignore[misc]\n    def make_release_tree(self, base_dir: str, files: List[str]) -> None:\n        super().make_release_tree(base_dir, files)\n\n        for to, src in to_src:\n            txt = get_and_replace(\n                src, binary=True, version=version, extra_cmd=\"\")\n\n            dest = Path(base_dir) / to\n\n            # This is normally linked, so unlink before writing!\n            dest.unlink()\n            dest.write_bytes(txt)",
  "def remove_output(*sources: str) -> Iterator[None]:\n    try:\n        yield\n    finally:\n        for src in sources:\n            shutil.rmtree(src)",
  "def make_release_tree(self, base_dir: str, files: List[str]) -> None:\n        super().make_release_tree(base_dir, files)\n\n        for to, src in to_src:\n            txt = get_and_replace(\n                src, binary=True, version=version, extra_cmd=\"\")\n\n            dest = Path(base_dir) / to\n\n            # This is normally linked, so unlink before writing!\n            dest.unlink()\n            dest.write_bytes(txt)",
  "class Pybind11Extension(_Extension):  # type: ignore[misc]\n    \"\"\"\n    Build a C++11+ Extension module with pybind11. This automatically adds the\n    recommended flags when you init the extension and assumes C++ sources - you\n    can further modify the options yourself.\n\n    The customizations are:\n\n    * ``/EHsc`` and ``/bigobj`` on Windows\n    * ``stdlib=libc++`` on macOS\n    * ``visibility=hidden`` and ``-g0`` on Unix\n\n    Finally, you can set ``cxx_std`` via constructor or afterwards to enable\n    flags for C++ std, and a few extra helper flags related to the C++ standard\n    level. It is _highly_ recommended you either set this, or use the provided\n    ``build_ext``, which will search for the highest supported extension for\n    you if the ``cxx_std`` property is not set. Do not set the ``cxx_std``\n    property more than once, as flags are added when you set it. Set the\n    property to None to disable the addition of C++ standard flags.\n\n    If you want to add pybind11 headers manually, for example for an exact\n    git checkout, then set ``include_pybind11=False``.\n    \"\"\"\n\n    # flags are prepended, so that they can be further overridden, e.g. by\n    # ``extra_compile_args=[\"-g\"]``.\n\n    def _add_cflags(self, flags: List[str]) -> None:\n        self.extra_compile_args[:0] = flags\n\n    def _add_ldflags(self, flags: List[str]) -> None:\n        self.extra_link_args[:0] = flags\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n\n        self._cxx_level = 0\n        cxx_std = kwargs.pop(\"cxx_std\", 0)\n\n        if \"language\" not in kwargs:\n            kwargs[\"language\"] = \"c++\"\n\n        include_pybind11 = kwargs.pop(\"include_pybind11\", True)\n\n        super().__init__(*args, **kwargs)\n\n        # Include the installed package pybind11 headers\n        if include_pybind11:\n            # If using setup_requires, this fails the first time - that's okay\n            try:\n                import pybind11\n\n                pyinc = pybind11.get_include()\n\n                if pyinc not in self.include_dirs:\n                    self.include_dirs.append(pyinc)\n            except ModuleNotFoundError:\n                pass\n\n        self.cxx_std = cxx_std\n\n        cflags = []\n        ldflags = []\n        if WIN:\n            cflags += [\"/EHsc\", \"/bigobj\"]\n        else:\n            cflags += [\"-fvisibility=hidden\"]\n            env_cflags = os.environ.get(\"CFLAGS\", \"\")\n            env_cppflags = os.environ.get(\"CPPFLAGS\", \"\")\n            c_cpp_flags = shlex.split(env_cflags) + shlex.split(env_cppflags)\n            if not any(opt.startswith(\"-g\") for opt in c_cpp_flags):\n                cflags += [\"-g0\"]\n            if MACOS:\n                cflags += [\"-stdlib=libc++\"]\n                ldflags += [\"-stdlib=libc++\"]\n        self._add_cflags(cflags)\n        self._add_ldflags(ldflags)\n\n    @property\n    def cxx_std(self) -> int:\n        \"\"\"\n        The CXX standard level. If set, will add the required flags. If left at\n        0, it will trigger an automatic search when pybind11's build_ext is\n        used. If None, will have no effect.  Besides just the flags, this may\n        add a macos-min 10.9 or 10.14 flag if MACOSX_DEPLOYMENT_TARGET is\n        unset.\n        \"\"\"\n        return self._cxx_level\n\n    @cxx_std.setter\n    def cxx_std(self, level: int) -> None:\n\n        if self._cxx_level:\n            warnings.warn(\n                \"You cannot safely change the cxx_level after setting it!\")\n\n        # MSVC 2015 Update 3 and later only have 14 (and later 17) modes, so\n        # force a valid flag here.\n        if WIN and level == 11:\n            level = 14\n\n        self._cxx_level = level\n\n        if not level:\n            return\n\n        cflags = [STD_TMPL.format(level)]\n        ldflags = []\n\n        if MACOS and \"MACOSX_DEPLOYMENT_TARGET\" not in os.environ:\n            # C++17 requires a higher min version of macOS. An earlier version\n            # (10.12 or 10.13) can be set manually via environment variable if\n            # you are careful in your feature usage, but 10.14 is the safest\n            # setting for general use. However, never set higher than the\n            # current macOS version!\n            current_macos = tuple(\n                int(x) for x in platform.mac_ver()[0].split(\".\")[:2])\n            desired_macos = (10, 9) if level < 17 else (10, 14)\n            macos_string = \".\".join(\n                str(x) for x in min(current_macos, desired_macos))\n            macosx_min = f\"-mmacosx-version-min={macos_string}\"\n            cflags += [macosx_min]\n            ldflags += [macosx_min]\n\n        self._add_cflags(cflags)\n        self._add_ldflags(ldflags)",
  "def tmp_chdir() -> Iterator[str]:\n    \"Prepare and enter a temporary directory, cleanup when done\"\n\n    # Threadsafe\n    with tmp_chdir_lock:\n        olddir = os.getcwd()\n        try:\n            tmpdir = tempfile.mkdtemp()\n            os.chdir(tmpdir)\n            yield tmpdir\n        finally:\n            os.chdir(olddir)\n            shutil.rmtree(tmpdir)",
  "def has_flag(compiler: Any, flag: str) -> bool:\n    \"\"\"\n    Return the flag if a flag name is supported on the\n    specified compiler, otherwise None (can be used as a boolean).\n    If multiple flags are passed, return the first that matches.\n    \"\"\"\n\n    with tmp_chdir():\n        fname = Path(\"flagcheck.cpp\")\n        # Don't trigger -Wunused-parameter.\n        fname.write_text(\n            \"int main (int, char **) { return 0; }\", encoding=\"utf-8\")\n\n        try:\n            compiler.compile([str(fname)], extra_postargs=[flag])\n        except distutils.errors.CompileError:\n            return False\n        return True",
  "def auto_cpp_level(compiler: Any) -> Union[str, int]:\n    \"\"\"\n    Return the max supported C++ std level (17, 14, or 11). Returns latest on Windows.\n    \"\"\"\n\n    if WIN:\n        return \"latest\"\n\n    levels = [17, 14, 11]\n\n    for level in levels:\n        if has_flag(compiler, STD_TMPL.format(level)):\n            return level\n\n    msg = \"Unsupported compiler -- at least C++11 support is needed!\"\n    raise RuntimeError(msg)",
  "class build_ext(_build_ext):  # type: ignore[misc] # noqa: N801\n    \"\"\"\n    Customized build_ext that allows an auto-search for the highest supported\n    C++ level for Pybind11Extension. This is only needed for the auto-search\n    for now, and is completely optional otherwise.\n    \"\"\"\n\n    def build_extensions(self) -> None:\n        \"\"\"\n        Build extensions, injecting C++ std for Pybind11Extension if needed.\n        \"\"\"\n\n        for ext in self.extensions:\n            if hasattr(ext, \"_cxx_level\") and ext._cxx_level == 0:\n                ext.cxx_std = auto_cpp_level(self.compiler)\n\n        super().build_extensions()",
  "def intree_extensions(\n        paths: Iterable[str],\n        package_dir: Optional[Dict[str, str]]=None) -> List[Pybind11Extension]:\n    \"\"\"\n    Generate Pybind11Extensions from source files directly located in a Python\n    source tree.\n\n    ``package_dir`` behaves as in ``setuptools.setup``.  If unset, the Python\n    package root parent is determined as the first parent directory that does\n    not contain an ``__init__.py`` file.\n    \"\"\"\n    exts = []\n\n    if package_dir is None:\n        for path in paths:\n            parent, _ = os.path.split(path)\n            while os.path.exists(os.path.join(parent, \"__init__.py\")):\n                parent, _ = os.path.split(parent)\n            relname, _ = os.path.splitext(os.path.relpath(path, parent))\n            qualified_name = relname.replace(os.path.sep, \".\")\n            exts.append(Pybind11Extension(qualified_name, [path]))\n        return exts\n\n    for path in paths:\n        for prefix, parent in package_dir.items():\n            if path.startswith(parent):\n                relname, _ = os.path.splitext(os.path.relpath(path, parent))\n                qualified_name = relname.replace(os.path.sep, \".\")\n                if prefix:\n                    qualified_name = prefix + \".\" + qualified_name\n                exts.append(Pybind11Extension(qualified_name, [path]))\n                break\n        else:\n            msg = (\n                f\"path {path} is not a child of any of the directories listed \"\n                f\"in 'package_dir' ({package_dir})\")\n            raise ValueError(msg)\n\n    return exts",
  "def naive_recompile(obj: str, src: str) -> bool:\n    \"\"\"\n    This will recompile only if the source file changes. It does not check\n    header files, so a more advanced function or Ccache is better if you have\n    editable header files in your package.\n    \"\"\"\n    return os.stat(obj).st_mtime < os.stat(src).st_mtime",
  "def no_recompile(obg: str, src: str) -> bool:  # pylint: disable=unused-argument\n    \"\"\"\n    This is the safest but slowest choice (and is the default) - will always\n    recompile sources.\n    \"\"\"\n    return True",
  "class ParallelCompile:\n    \"\"\"\n    Make a parallel compile function. Inspired by\n    numpy.distutils.ccompiler.CCompiler.compile and cppimport.\n\n    This takes several arguments that allow you to customize the compile\n    function created:\n\n    envvar:\n        Set an environment variable to control the compilation threads, like\n        NPY_NUM_BUILD_JOBS\n    default:\n        0 will automatically multithread, or 1 will only multithread if the\n        envvar is set.\n    max:\n        The limit for automatic multithreading if non-zero\n    needs_recompile:\n        A function of (obj, src) that returns True when recompile is needed.  No\n        effect in isolated mode; use ccache instead, see\n        https://github.com/matplotlib/matplotlib/issues/1507/\n\n    To use::\n\n        ParallelCompile(\"NPY_NUM_BUILD_JOBS\").install()\n\n    or::\n\n        with ParallelCompile(\"NPY_NUM_BUILD_JOBS\"):\n            setup(...)\n\n    By default, this assumes all files need to be recompiled. A smarter\n    function can be provided via needs_recompile.  If the output has not yet\n    been generated, the compile will always run, and this function is not\n    called.\n    \"\"\"\n\n    __slots__ = (\"envvar\", \"default\", \"max\", \"_old\", \"needs_recompile\")\n\n    def __init__(\n            self,\n            envvar: Optional[str]=None,\n            default: int=0,\n            max: int=0,  # pylint: disable=redefined-builtin\n            needs_recompile: Callable[[str, str], bool]=no_recompile,\n    ) -> None:\n        self.envvar = envvar\n        self.default = default\n        self.max = max\n        self.needs_recompile = needs_recompile\n        self._old: List[CCompilerMethod] = []\n\n    def function(self) -> CCompilerMethod:\n        \"\"\"\n        Builds a function object usable as distutils.ccompiler.CCompiler.compile.\n        \"\"\"\n\n        def compile_function(\n                compiler: distutils.ccompiler.CCompiler,\n                sources: List[str],\n                output_dir: Optional[str]=None,\n                macros: Optional[Union[Tuple[str], Tuple[str, Optional[\n                    str]]]]=None,\n                include_dirs: Optional[List[str]]=None,\n                debug: bool=False,\n                extra_preargs: Optional[List[str]]=None,\n                extra_postargs: Optional[List[str]]=None,\n                depends: Optional[List[str]]=None, ) -> Any:\n\n            # These lines are directly from distutils.ccompiler.CCompiler\n            macros, objects, extra_postargs, pp_opts, build = compiler._setup_compile(  # type: ignore[attr-defined]\n                output_dir, macros, include_dirs, sources, depends,\n                extra_postargs)\n            cc_args = compiler._get_cc_args(\n                pp_opts, debug, extra_preargs)  # type: ignore[attr-defined]\n\n            # The number of threads; start with default.\n            threads = self.default\n\n            # Determine the number of compilation threads, unless set by an environment variable.\n            if self.envvar is not None:\n                threads = int(os.environ.get(self.envvar, self.default))\n\n            def _single_compile(obj: Any) -> None:\n                try:\n                    src, ext = build[obj]\n                except KeyError:\n                    return\n\n                if not os.path.exists(obj) or self.needs_recompile(obj, src):\n                    compiler._compile(obj, src, ext, cc_args, extra_postargs,\n                                      pp_opts)  # type: ignore[attr-defined]\n\n            try:\n                # Importing .synchronize checks for platforms that have some multiprocessing\n                # capabilities but lack semaphores, such as AWS Lambda and Android Termux.\n                import multiprocessing.synchronize\n                from multiprocessing.pool import ThreadPool\n            except ImportError:\n                threads = 1\n\n            if threads == 0:\n                try:\n                    threads = multiprocessing.cpu_count()\n                    threads = self.max if self.max and self.max < threads else threads\n                except NotImplementedError:\n                    threads = 1\n\n            if threads > 1:\n                with ThreadPool(threads) as pool:\n                    for _ in pool.imap_unordered(_single_compile, objects):\n                        pass\n            else:\n                for ob in objects:\n                    _single_compile(ob)\n\n            return objects\n\n        return compile_function\n\n    def install(self: S) -> S:\n        \"\"\"\n        Installs the compile function into distutils.ccompiler.CCompiler.compile.\n        \"\"\"\n        distutils.ccompiler.CCompiler.compile = self.function(\n        )  # type: ignore[assignment]\n        return self\n\n    def __enter__(self: S) -> S:\n        self._old.append(distutils.ccompiler.CCompiler.compile)\n        return self.install()\n\n    def __exit__(self, *args: Any) -> None:\n        distutils.ccompiler.CCompiler.compile = self._old.pop(\n        )",
  "def _add_cflags(self, flags: List[str]) -> None:\n        self.extra_compile_args[:0] = flags",
  "def _add_ldflags(self, flags: List[str]) -> None:\n        self.extra_link_args[:0] = flags",
  "def __init__(self, *args: Any, **kwargs: Any) -> None:\n\n        self._cxx_level = 0\n        cxx_std = kwargs.pop(\"cxx_std\", 0)\n\n        if \"language\" not in kwargs:\n            kwargs[\"language\"] = \"c++\"\n\n        include_pybind11 = kwargs.pop(\"include_pybind11\", True)\n\n        super().__init__(*args, **kwargs)\n\n        # Include the installed package pybind11 headers\n        if include_pybind11:\n            # If using setup_requires, this fails the first time - that's okay\n            try:\n                import pybind11\n\n                pyinc = pybind11.get_include()\n\n                if pyinc not in self.include_dirs:\n                    self.include_dirs.append(pyinc)\n            except ModuleNotFoundError:\n                pass\n\n        self.cxx_std = cxx_std\n\n        cflags = []\n        ldflags = []\n        if WIN:\n            cflags += [\"/EHsc\", \"/bigobj\"]\n        else:\n            cflags += [\"-fvisibility=hidden\"]\n            env_cflags = os.environ.get(\"CFLAGS\", \"\")\n            env_cppflags = os.environ.get(\"CPPFLAGS\", \"\")\n            c_cpp_flags = shlex.split(env_cflags) + shlex.split(env_cppflags)\n            if not any(opt.startswith(\"-g\") for opt in c_cpp_flags):\n                cflags += [\"-g0\"]\n            if MACOS:\n                cflags += [\"-stdlib=libc++\"]\n                ldflags += [\"-stdlib=libc++\"]\n        self._add_cflags(cflags)\n        self._add_ldflags(ldflags)",
  "def cxx_std(self) -> int:\n        \"\"\"\n        The CXX standard level. If set, will add the required flags. If left at\n        0, it will trigger an automatic search when pybind11's build_ext is\n        used. If None, will have no effect.  Besides just the flags, this may\n        add a macos-min 10.9 or 10.14 flag if MACOSX_DEPLOYMENT_TARGET is\n        unset.\n        \"\"\"\n        return self._cxx_level",
  "def cxx_std(self, level: int) -> None:\n\n        if self._cxx_level:\n            warnings.warn(\n                \"You cannot safely change the cxx_level after setting it!\")\n\n        # MSVC 2015 Update 3 and later only have 14 (and later 17) modes, so\n        # force a valid flag here.\n        if WIN and level == 11:\n            level = 14\n\n        self._cxx_level = level\n\n        if not level:\n            return\n\n        cflags = [STD_TMPL.format(level)]\n        ldflags = []\n\n        if MACOS and \"MACOSX_DEPLOYMENT_TARGET\" not in os.environ:\n            # C++17 requires a higher min version of macOS. An earlier version\n            # (10.12 or 10.13) can be set manually via environment variable if\n            # you are careful in your feature usage, but 10.14 is the safest\n            # setting for general use. However, never set higher than the\n            # current macOS version!\n            current_macos = tuple(\n                int(x) for x in platform.mac_ver()[0].split(\".\")[:2])\n            desired_macos = (10, 9) if level < 17 else (10, 14)\n            macos_string = \".\".join(\n                str(x) for x in min(current_macos, desired_macos))\n            macosx_min = f\"-mmacosx-version-min={macos_string}\"\n            cflags += [macosx_min]\n            ldflags += [macosx_min]\n\n        self._add_cflags(cflags)\n        self._add_ldflags(ldflags)",
  "def build_extensions(self) -> None:\n        \"\"\"\n        Build extensions, injecting C++ std for Pybind11Extension if needed.\n        \"\"\"\n\n        for ext in self.extensions:\n            if hasattr(ext, \"_cxx_level\") and ext._cxx_level == 0:\n                ext.cxx_std = auto_cpp_level(self.compiler)\n\n        super().build_extensions()",
  "def __init__(\n            self,\n            envvar: Optional[str]=None,\n            default: int=0,\n            max: int=0,  # pylint: disable=redefined-builtin\n            needs_recompile: Callable[[str, str], bool]=no_recompile,\n    ) -> None:\n        self.envvar = envvar\n        self.default = default\n        self.max = max\n        self.needs_recompile = needs_recompile\n        self._old: List[CCompilerMethod] = []",
  "def function(self) -> CCompilerMethod:\n        \"\"\"\n        Builds a function object usable as distutils.ccompiler.CCompiler.compile.\n        \"\"\"\n\n        def compile_function(\n                compiler: distutils.ccompiler.CCompiler,\n                sources: List[str],\n                output_dir: Optional[str]=None,\n                macros: Optional[Union[Tuple[str], Tuple[str, Optional[\n                    str]]]]=None,\n                include_dirs: Optional[List[str]]=None,\n                debug: bool=False,\n                extra_preargs: Optional[List[str]]=None,\n                extra_postargs: Optional[List[str]]=None,\n                depends: Optional[List[str]]=None, ) -> Any:\n\n            # These lines are directly from distutils.ccompiler.CCompiler\n            macros, objects, extra_postargs, pp_opts, build = compiler._setup_compile(  # type: ignore[attr-defined]\n                output_dir, macros, include_dirs, sources, depends,\n                extra_postargs)\n            cc_args = compiler._get_cc_args(\n                pp_opts, debug, extra_preargs)  # type: ignore[attr-defined]\n\n            # The number of threads; start with default.\n            threads = self.default\n\n            # Determine the number of compilation threads, unless set by an environment variable.\n            if self.envvar is not None:\n                threads = int(os.environ.get(self.envvar, self.default))\n\n            def _single_compile(obj: Any) -> None:\n                try:\n                    src, ext = build[obj]\n                except KeyError:\n                    return\n\n                if not os.path.exists(obj) or self.needs_recompile(obj, src):\n                    compiler._compile(obj, src, ext, cc_args, extra_postargs,\n                                      pp_opts)  # type: ignore[attr-defined]\n\n            try:\n                # Importing .synchronize checks for platforms that have some multiprocessing\n                # capabilities but lack semaphores, such as AWS Lambda and Android Termux.\n                import multiprocessing.synchronize\n                from multiprocessing.pool import ThreadPool\n            except ImportError:\n                threads = 1\n\n            if threads == 0:\n                try:\n                    threads = multiprocessing.cpu_count()\n                    threads = self.max if self.max and self.max < threads else threads\n                except NotImplementedError:\n                    threads = 1\n\n            if threads > 1:\n                with ThreadPool(threads) as pool:\n                    for _ in pool.imap_unordered(_single_compile, objects):\n                        pass\n            else:\n                for ob in objects:\n                    _single_compile(ob)\n\n            return objects\n\n        return compile_function",
  "def install(self: S) -> S:\n        \"\"\"\n        Installs the compile function into distutils.ccompiler.CCompiler.compile.\n        \"\"\"\n        distutils.ccompiler.CCompiler.compile = self.function(\n        )  # type: ignore[assignment]\n        return self",
  "def __enter__(self: S) -> S:\n        self._old.append(distutils.ccompiler.CCompiler.compile)\n        return self.install()",
  "def __exit__(self, *args: Any) -> None:\n        distutils.ccompiler.CCompiler.compile = self._old.pop(\n        )",
  "def compile_function(\n                compiler: distutils.ccompiler.CCompiler,\n                sources: List[str],\n                output_dir: Optional[str]=None,\n                macros: Optional[Union[Tuple[str], Tuple[str, Optional[\n                    str]]]]=None,\n                include_dirs: Optional[List[str]]=None,\n                debug: bool=False,\n                extra_preargs: Optional[List[str]]=None,\n                extra_postargs: Optional[List[str]]=None,\n                depends: Optional[List[str]]=None, ) -> Any:\n\n            # These lines are directly from distutils.ccompiler.CCompiler\n            macros, objects, extra_postargs, pp_opts, build = compiler._setup_compile(  # type: ignore[attr-defined]\n                output_dir, macros, include_dirs, sources, depends,\n                extra_postargs)\n            cc_args = compiler._get_cc_args(\n                pp_opts, debug, extra_preargs)  # type: ignore[attr-defined]\n\n            # The number of threads; start with default.\n            threads = self.default\n\n            # Determine the number of compilation threads, unless set by an environment variable.\n            if self.envvar is not None:\n                threads = int(os.environ.get(self.envvar, self.default))\n\n            def _single_compile(obj: Any) -> None:\n                try:\n                    src, ext = build[obj]\n                except KeyError:\n                    return\n\n                if not os.path.exists(obj) or self.needs_recompile(obj, src):\n                    compiler._compile(obj, src, ext, cc_args, extra_postargs,\n                                      pp_opts)  # type: ignore[attr-defined]\n\n            try:\n                # Importing .synchronize checks for platforms that have some multiprocessing\n                # capabilities but lack semaphores, such as AWS Lambda and Android Termux.\n                import multiprocessing.synchronize\n                from multiprocessing.pool import ThreadPool\n            except ImportError:\n                threads = 1\n\n            if threads == 0:\n                try:\n                    threads = multiprocessing.cpu_count()\n                    threads = self.max if self.max and self.max < threads else threads\n                except NotImplementedError:\n                    threads = 1\n\n            if threads > 1:\n                with ThreadPool(threads) as pool:\n                    for _ in pool.imap_unordered(_single_compile, objects):\n                        pass\n            else:\n                for ob in objects:\n                    _single_compile(ob)\n\n            return objects",
  "def _single_compile(obj: Any) -> None:\n                try:\n                    src, ext = build[obj]\n                except KeyError:\n                    return\n\n                if not os.path.exists(obj) or self.needs_recompile(obj, src):\n                    compiler._compile(obj, src, ext, cc_args, extra_postargs,\n                                      pp_opts)",
  "def print_includes() -> None:\n    dirs = [\n        sysconfig.get_path(\"include\"),\n        sysconfig.get_path(\"platinclude\"),\n        get_include(),\n    ]\n\n    # Make unique but preserve order\n    unique_dirs = []\n    for d in dirs:\n        if d and d not in unique_dirs:\n            unique_dirs.append(d)\n\n    print(\" \".join(\"-I\" + d for d in unique_dirs))",
  "def main() -> None:\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--includes\",\n        action=\"store_true\",\n        help=\"Include flags for both pybind11 and Python headers.\", )\n    parser.add_argument(\n        \"--cmakedir\",\n        action=\"store_true\",\n        help=\"Print the CMake module directory, ideal for setting -Dpybind11_ROOT in CMake.\",\n    )\n    args = parser.parse_args()\n    if not sys.argv[1:]:\n        parser.print_help()\n    if args.includes:\n        print_includes()\n    if args.cmakedir:\n        print(get_cmake_dir())",
  "def get_include(user: bool=False) -> str:  # pylint: disable=unused-argument\n    \"\"\"\n    Return the path to the pybind11 include directory. The historical \"user\"\n    argument is unused, and may be removed.\n    \"\"\"\n    installed_path = os.path.join(DIR, \"include\")\n    source_path = os.path.join(os.path.dirname(DIR), \"include\")\n    return installed_path if os.path.exists(installed_path) else source_path",
  "def get_cmake_dir() -> str:\n    \"\"\"\n    Return the path to the pybind11 CMake module directory.\n    \"\"\"\n    cmake_installed_path = os.path.join(DIR, \"share\", \"cmake\", \"pybind11\")\n    if os.path.exists(cmake_installed_path):\n        return cmake_installed_path\n\n    msg = \"pybind11 not installed, installation required to access the CMake files\"\n    raise ImportError(msg)",
  "def _to_int(s: str) -> Union[int, str]:\n    try:\n        return int(s)\n    except ValueError:\n        return s",
  "def test_to_from_numpy_zero_copy():\n    \"\"\"Test the copy free conversion of numpy array via DLPack.\"\"\"\n    np_ary = np.random.normal(size=[10, 10])\n    np_ary_big = np.random.normal(size=[12, 10])\n    dlpack_capsule = from_numpy(np_ary_big)\n    reconstructed_ary = to_numpy(dlpack_capsule)\n    del dlpack_capsule\n    np_ary_big[1:11] = np_ary\n    del np_ary_big\n    np.testing.assert_equal(actual=reconstructed_ary[1:11], desired=np_ary)",
  "def test_to_from_numpy_memory():\n    \"\"\"Test that DLPack capsule keeps the source array alive\"\"\"\n    source_array = np.random.normal(size=[10, 10])\n    np_array_ref = source_array.copy()\n    dlpack_capsule = from_numpy(source_array)\n    del source_array\n    reconstructed_array = to_numpy(dlpack_capsule)\n    del dlpack_capsule\n    np.testing.assert_equal(actual=reconstructed_array, desired=np_array_ref)",
  "def test_from_numpy():\n    \"\"\"Test the copy free conversion of numpy to a tvm ndarray.\"\"\"\n    np_array = np.random.normal(size=[10, 10])\n    np_array_ref = np_array.copy()\n    tvm_array = tvm.nd.from_dlpack(from_numpy(np_array))\n    del np_array\n    np.testing.assert_equal(actual=tvm_array.numpy(), desired=np_array_ref)\n    del tvm_array",
  "def test_to_numpy():\n    \"\"\"Test the copy free conversion of a tvm ndarray to a numpy array\"\"\"\n    tvm_array = tvm.nd.array(np.random.normal(size=[10, 10]))\n    np_array_ref = tvm_array.numpy()\n    np_array = to_numpy(tvm_array.__dlpack__())\n    del tvm_array\n    np.testing.assert_equal(actual=np_array, desired=np_array_ref)\n    del np_array",
  "def _array_interface_from_dl_tensor(dlt):\n    \"\"\"Constructs NumPy's array_interface dictionary\n    from `dlpack.DLTensor` descriptor.\"\"\"\n    assert isinstance(dlt, DLTensor)\n    shape = tuple(dlt.shape[dim] for dim in range(dlt.ndim))\n    itemsize = dlt.dtype.lanes * dlt.dtype.bits // 8\n    if dlt.strides:\n        strides = tuple(dlt.strides[dim] * itemsize for dim in range(dlt.ndim))\n    else:\n        # Array is compact, make it numpy compatible.\n        strides = []\n        for i, s in enumerate(shape):\n            cumulative = 1\n            for e in range(i + 1, dlt.ndim):\n                cumulative *= shape[e]\n            strides.append(cumulative * itemsize)\n        strides = tuple(strides)\n    typestr = \"|\" + str(dlt.dtype.type_code)[0] + str(itemsize)\n    return dict(\n        version=3,\n        shape=shape,\n        strides=strides,\n        data=(dlt.data, True),\n        offset=dlt.byte_offset,\n        typestr=typestr, )",
  "class _Holder:\n    \"\"\"A wrapper that combines a pycapsule and array_interface for consumption by  numpy.\n\n    Parameters\n    ----------\n    array_interface : dict\n        A description of the underlying memory.\n\n    pycapsule : PyCapsule\n        A wrapper around the dlpack tensor that will be converted to numpy.\n    \"\"\"\n\n    def __init__(self, array_interface, pycapsule) -> None:\n        self.__array_interface__ = array_interface\n        self._pycapsule = pycapsule",
  "def to_numpy(pycapsule) -> np.ndarray:\n    \"\"\"Convert a dlpack tensor into a numpy array without copying.\n\n    Parameters\n    ----------\n    pycapsule : PyCapsule\n        A pycapsule wrapping a dlpack tensor that will be converted.\n\n    Returns\n    -------\n    np_array : np.ndarray\n        A new numpy array that uses the same underlying memory as the input\n        pycapsule.\n    \"\"\"\n    assert ctypes.pythonapi.PyCapsule_IsValid(pycapsule, _c_str_dltensor)\n    dl_managed_tensor = ctypes.pythonapi.PyCapsule_GetPointer(pycapsule,\n                                                              _c_str_dltensor)\n    dl_managed_tensor_ptr = ctypes.cast(dl_managed_tensor,\n                                        ctypes.POINTER(DLManagedTensor))\n    dl_managed_tensor = dl_managed_tensor_ptr.contents\n    holder = _Holder(\n        _array_interface_from_dl_tensor(dl_managed_tensor.dl_tensor),\n        pycapsule)\n    return np.ctypeslib.as_array(holder)",
  "def __init__(self, array_interface, pycapsule) -> None:\n        self.__array_interface__ = array_interface\n        self._pycapsule = pycapsule",
  "class DLDeviceType(ctypes.c_int):\n    \"\"\"The enum that encodes the type of the device where\n    DLTensor memory is allocated.\n    \"\"\"\n    kDLCPU = 1\n    kDLCUDA = 2\n    kDLCUDAHost = 3\n    kDLOpenCL = 4\n    kDLVulkan = 7\n    kDLMetal = 8\n    kDLVPI = 9\n    kDLROCM = 10\n    kDLROCMHost = 11\n    kDLCUDAManaged = 13\n    kDLOneAPI = 14\n\n    def __str__(self):\n        return {\n            self.kDLCPU: \"CPU\",\n            self.kDLCUDA: \"CUDA\",\n            self.kDLCUDAHost: \"CUDAHost\",\n            self.kDLOpenCL: \"OpenCL\",\n            self.kDLVulkan: \"Vulkan\",\n            self.kDLMetal: \"Metal\",\n            self.kDLVPI: \"VPI\",\n            self.kDLROCM: \"ROCM\",\n            self.kDLROCMHost: \"ROMCHost\",\n            self.kDLCUDAManaged: \"CUDAManaged\",\n            self.kDLOneAPI: \"oneAPI\",\n        }[self.value]",
  "class DLDevice(ctypes.Structure):\n    \"\"\"Represents the device where DLTensor memory is allocated.\n    The device is represented by the pair of fields:\n       device_type: DLDeviceType\n       device_id: c_int\n    \"\"\"\n    _fields_ = [\n        (\"device_type\", DLDeviceType),\n        (\"device_id\", ctypes.c_int),\n    ]",
  "class DLDataTypeCode(ctypes.c_uint8):\n    \"\"\"An integer that encodes the category of DLTensor elements' data type.\"\"\"\n    kDLInt = 0\n    kDLUInt = 1\n    kDLFloat = 2\n    kDLOpaquePointer = 3\n    kDLBfloat = 4\n    kDLComplex = 5\n\n    def __str__(self):\n        return {\n            self.kDLInt: \"int\",\n            self.kDLUInt: \"uint\",\n            self.kDLFloat: \"float\",\n            self.kDLBfloat: \"bfloat\",\n            self.kDLComplex: \"complex\",\n            self.kDLOpaquePointer: \"void_p\"\n        }[self.value]",
  "class DLDataType(ctypes.Structure):\n    \"\"\"Descriptor of data type for elements of DLTensor.\n    The data type is described by a triple, `DLDataType.type_code`,\n    `DLDataType.bits`, and `DLDataType.lanes`.\n\n    The element is understood as packed `lanes` repetitions of\n    elements from `type_code` data-category of width `bits`.\n    \"\"\"\n    _fields_ = [\n        (\"type_code\", DLDataTypeCode),\n        (\"bits\", ctypes.c_uint8),\n        (\"lanes\", ctypes.c_uint16),\n    ]\n    TYPE_MAP = {\n        \"bool\": (DLDataTypeCode.kDLUInt, 1, 1),\n        \"int8\": (DLDataTypeCode.kDLInt, 8, 1),\n        \"int16\": (DLDataTypeCode.kDLInt, 16, 1),\n        \"int32\": (DLDataTypeCode.kDLInt, 32, 1),\n        \"int64\": (DLDataTypeCode.kDLInt, 64, 1),\n        \"uint8\": (DLDataTypeCode.kDLUInt, 8, 1),\n        \"uint16\": (DLDataTypeCode.kDLUInt, 16, 1),\n        \"uint32\": (DLDataTypeCode.kDLUInt, 32, 1),\n        \"uint64\": (DLDataTypeCode.kDLUInt, 64, 1),\n        \"float16\": (DLDataTypeCode.kDLFloat, 16, 1),\n        \"float32\": (DLDataTypeCode.kDLFloat, 32, 1),\n        \"float64\": (DLDataTypeCode.kDLFloat, 64, 1),\n        \"complex64\": (DLDataTypeCode.kDLComplex, 64, 1),\n        \"complex128\": (DLDataTypeCode.kDLComplex, 128, 1)\n    }",
  "class DLTensor(ctypes.Structure):\n    \"\"\"Structure describing strided layout of DLTensor.\n    Fields are:\n       data:  void pointer\n       device: DLDevice\n       ndim: number of indices needed to reference an\n             element of the tensor\n       dtype: data type descriptor\n       shape: tuple with lengths of the corresponding\n              tensor dimensions\n       strides: tuple of numbers of array elements to\n                step in each dimension when traversing\n                the tensor\n       byte_offset: data + byte_offset gives the address of\n                tensor element with index (0,) * ndim\n    \"\"\"\n    _fields_ = [\n        (\"data\", ctypes.c_void_p),\n        (\"device\", DLDevice),\n        (\"ndim\", ctypes.c_int),\n        (\"dtype\", DLDataType),\n        (\"shape\", ctypes.POINTER(ctypes.c_int64)),\n        (\"strides\", ctypes.POINTER(ctypes.c_int64)),\n        (\"byte_offset\", ctypes.c_uint64),\n    ]",
  "class DLManagedTensor(ctypes.Structure):\n    \"\"\"Structure storing the pointer to the tensor descriptor,\n    deleter callable for the tensor descriptor, and pointer to\n    some additional data. These are stored in fields `dl_tensor`,\n    `deleter`, and `manager_ctx`.\"\"\"\n    _fields_ = [\n        (\"dl_tensor\", DLTensor),\n        (\"manager_ctx\", ctypes.c_void_p),\n        (\"deleter\", ctypes.CFUNCTYPE(None, ctypes.c_void_p)),\n    ]",
  "def __str__(self):\n        return {\n            self.kDLCPU: \"CPU\",\n            self.kDLCUDA: \"CUDA\",\n            self.kDLCUDAHost: \"CUDAHost\",\n            self.kDLOpenCL: \"OpenCL\",\n            self.kDLVulkan: \"Vulkan\",\n            self.kDLMetal: \"Metal\",\n            self.kDLVPI: \"VPI\",\n            self.kDLROCM: \"ROCM\",\n            self.kDLROCMHost: \"ROMCHost\",\n            self.kDLCUDAManaged: \"CUDAManaged\",\n            self.kDLOneAPI: \"oneAPI\",\n        }[self.value]",
  "def __str__(self):\n        return {\n            self.kDLInt: \"int\",\n            self.kDLUInt: \"uint\",\n            self.kDLFloat: \"float\",\n            self.kDLBfloat: \"bfloat\",\n            self.kDLComplex: \"complex\",\n            self.kDLOpaquePointer: \"void_p\"\n        }[self.value]",
  "class _Holder:\n    \"\"\"A wrapper around a numpy array to keep track of references to the underlying memory.\n\n    Parameters\n    ----------\n    np_array : np.ndarray\n        The numpy array that will be converted to a DLPack tensor and must be managed.\n    \"\"\"\n\n    def __init__(self, np_array: np.ndarray) -> None:\n        self.np_array = np_array\n        self.data = np_array.ctypes.data_as(ctypes.c_void_p)\n        self.shape = np_array.ctypes.shape_as(ctypes.c_int64)\n        self.strides = np_array.ctypes.strides_as(ctypes.c_int64)\n        for i in range(np_array.ndim):\n            self.strides[i] //= np_array.itemsize\n\n    def _as_manager_ctx(self) -> ctypes.c_void_p:\n        py_obj = ctypes.py_object(self)\n        py_obj_ptr = ctypes.pointer(py_obj)\n        ctypes.pythonapi.Py_IncRef(py_obj)\n        ctypes.pythonapi.Py_IncRef(ctypes.py_object(py_obj_ptr))\n        return ctypes.cast(py_obj_ptr, ctypes.c_void_p)",
  "def _numpy_array_deleter(handle: ctypes.c_void_p) -> None:\n    \"\"\"A function to deallocate the memory of a numpy array.\"\"\"\n    dl_managed_tensor = DLManagedTensor.from_address(handle)\n    py_obj_ptr = ctypes.cast(dl_managed_tensor.manager_ctx,\n                             ctypes.POINTER(ctypes.py_object))\n    py_obj = py_obj_ptr.contents\n    ctypes.pythonapi.Py_DecRef(py_obj)\n    ctypes.pythonapi.Py_DecRef(ctypes.py_object(py_obj_ptr))\n    ctypes.pythonapi.PyMem_RawFree(handle)",
  "def _numpy_pycapsule_deleter(handle: ctypes.c_void_p) -> None:\n    \"\"\"A function to deallocate a pycapsule that wraps a numpy array.\"\"\"\n    pycapsule: ctypes.py_object = ctypes.cast(handle, ctypes.py_object)\n    if ctypes.pythonapi.PyCapsule_IsValid(pycapsule, _c_str_dltensor):\n        dl_managed_tensor = ctypes.pythonapi.PyCapsule_GetPointer(\n            pycapsule, _c_str_dltensor)\n        _numpy_array_deleter(dl_managed_tensor)\n        ctypes.pythonapi.PyCapsule_SetDestructor(pycapsule, None)",
  "def from_numpy(np_array: np.ndarray):\n    \"\"\"Convert a numpy array to another type of dlpack compatible array.\n\n    Parameters\n    ----------\n    np_array : np.ndarray\n        The source numpy array that will be converted.\n\n    Returns\n    -------\n    pycapsule : PyCapsule\n        A pycapsule containing a DLManagedTensor that can be converted\n        to other array formats without copying the underlying memory.\n    \"\"\"\n    holder = _Holder(np_array)\n    size = ctypes.c_size_t(ctypes.sizeof(DLManagedTensor))\n    dl_managed_tensor = DLManagedTensor.from_address(\n        ctypes.pythonapi.PyMem_RawMalloc(size))\n    dl_managed_tensor.dl_tensor.data = holder.data\n    dl_managed_tensor.dl_tensor.device = DLDevice(1, 0)\n    dl_managed_tensor.dl_tensor.ndim = np_array.ndim\n    dl_managed_tensor.dl_tensor.dtype = DLDataType.TYPE_MAP[str(\n        np_array.dtype)]\n    dl_managed_tensor.dl_tensor.shape = holder.shape\n    dl_managed_tensor.dl_tensor.strides = holder.strides\n    dl_managed_tensor.dl_tensor.byte_offset = 0\n    dl_managed_tensor.manager_ctx = holder._as_manager_ctx()\n    dl_managed_tensor.deleter = _numpy_array_deleter\n    pycapsule = ctypes.pythonapi.PyCapsule_New(\n        ctypes.byref(dl_managed_tensor),\n        _c_str_dltensor,\n        _numpy_pycapsule_deleter, )\n    return pycapsule",
  "def __init__(self, np_array: np.ndarray) -> None:\n        self.np_array = np_array\n        self.data = np_array.ctypes.data_as(ctypes.c_void_p)\n        self.shape = np_array.ctypes.shape_as(ctypes.c_int64)\n        self.strides = np_array.ctypes.strides_as(ctypes.c_int64)\n        for i in range(np_array.ndim):\n            self.strides[i] //= np_array.itemsize",
  "def _as_manager_ctx(self) -> ctypes.c_void_p:\n        py_obj = ctypes.py_object(self)\n        py_obj_ptr = ctypes.pointer(py_obj)\n        ctypes.pythonapi.Py_IncRef(py_obj)\n        ctypes.pythonapi.Py_IncRef(ctypes.py_object(py_obj_ptr))\n        return ctypes.cast(py_obj_ptr, ctypes.c_void_p)",
  "def cd(path):\n    if not os.path.isabs(path):\n        raise RuntimeError(\"Can only cd to absolute path, got: {}\".format(path))\n    orig_path = os.getcwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(orig_path)",
  "class ONNXCommand(setuptools.Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass",
  "class create_version(ONNXCommand):\n    def run(self):\n        with open(os.path.join(SRC_DIR, \"version.py\"), \"w\") as f:\n            f.write(dedent(\"\"\"\\\n            # This file is generated by setup.py. DO NOT EDIT!\n\n\n            version = \"{version}\"\n            git_version = \"{git_version}\"\n            \"\"\".format(**dict(VersionInfo._asdict()))))",
  "class cmake_build(setuptools.Command):\n    \"\"\"\n    Compiles everything when `python setup.py build` is run using cmake.\n\n    Custom args can be passed to cmake by specifying the `CMAKE_ARGS`\n    environment variable.\n\n    The number of CPUs used by `make` can be specified by passing `-j<ncpus>`\n    to `setup.py build`.  By default all CPUs are used.\n    \"\"\"\n    user_options = [\n        (str(\"jobs=\"), str(\"j\"), str(\"Specifies the number of jobs to use with make\"))\n    ]\n\n    built = False\n\n    def initialize_options(self):\n        self.jobs = None\n\n    def finalize_options(self):\n        self.set_undefined_options(\"build\", (\"parallel\", \"jobs\"))\n        if self.jobs is None and os.getenv(\"MAX_JOBS\") is not None:\n            self.jobs = os.getenv(\"MAX_JOBS\")\n        self.jobs = multiprocessing.cpu_count() if self.jobs is None else int(self.jobs)\n\n    def run(self):\n        if cmake_build.built:\n            return\n        cmake_build.built = True\n        if not os.path.exists(CMAKE_BUILD_DIR):\n            os.makedirs(CMAKE_BUILD_DIR)\n\n        with cd(CMAKE_BUILD_DIR):\n            build_type = \"Release\"\n            # configure\n            cmake_args = [\n                CMAKE,\n                \"-DPYTHON_INCLUDE_DIR={}\".format(sysconfig.get_python_inc()),\n                \"-DPYTHON_EXECUTABLE={}\".format(sys.executable),\n                \"-DBUILD_ONNX_PYTHON=ON\",\n                \"-DCMAKE_EXPORT_COMPILE_COMMANDS=ON\",\n                \"-DONNX_NAMESPACE={}\".format(ONNX_NAMESPACE),\n                \"-DPY_EXT_SUFFIX={}\".format(sysconfig.get_config_var(\"EXT_SUFFIX\") or ''),\n            ]\n            if COVERAGE:\n                cmake_args.append(\"-DONNX_COVERAGE=ON\")\n            if COVERAGE or DEBUG:\n                # in order to get accurate coverage information, the\n                # build needs to turn off optimizations\n                build_type = \"Debug\"\n            cmake_args.append(\"-DCMAKE_BUILD_TYPE=%s\" % build_type)\n            if WINDOWS:\n                cmake_args.extend([\n                    # we need to link with libpython on windows, so\n                    # passing python version to window in order to\n                    # find python in cmake\n                    \"-DPY_VERSION={}\".format(\"{0}.{1}\".format(*sys.version_info[:2])),\n                ])\n                if USE_MSVC_STATIC_RUNTIME:\n                    cmake_args.append(\"-DONNX_USE_MSVC_STATIC_RUNTIME=ON\")\n                if platform.architecture()[0] == \"64bit\":\n                    cmake_args.extend([\"-A\", \"x64\", \"-T\", \"host=x64\"])\n                else:\n                    cmake_args.extend([\"-A\", \"Win32\", \"-T\", \"host=x86\"])\n            if ONNX_ML:\n                cmake_args.append(\"-DONNX_ML=1\")\n            if ONNX_VERIFY_PROTO3:\n                cmake_args.append(\"-DONNX_VERIFY_PROTO3=1\")\n            if ONNX_BUILD_TESTS:\n                cmake_args.append(\"-DONNX_BUILD_TESTS=ON\")\n            if ONNX_DISABLE_EXCEPTIONS:\n                cmake_args.append(\"-DONNX_DISABLE_EXCEPTIONS=ON\")\n            if \"CMAKE_ARGS\" in os.environ:\n                extra_cmake_args = shlex.split(os.environ[\"CMAKE_ARGS\"])\n                # prevent crossfire with downstream scripts\n                del os.environ[\"CMAKE_ARGS\"]\n                log.info(\"Extra cmake args: {}\".format(extra_cmake_args))\n                cmake_args.extend(extra_cmake_args)\n            cmake_args.append(TOP_DIR)\n            log.info(\"Using cmake args: {}\".format(cmake_args))\n            if \"-DONNX_DISABLE_EXCEPTIONS=ON\" in cmake_args:\n                raise RuntimeError(\"-DONNX_DISABLE_EXCEPTIONS=ON option is only available for c++ builds. Python binding require exceptions to be enabled.\")\n            subprocess.check_call(cmake_args)\n\n            build_args = [CMAKE, \"--build\", os.curdir]\n            if WINDOWS:\n                build_args.extend([\"--config\", build_type])\n                build_args.extend([\"--\", \"/maxcpucount:{}\".format(self.jobs)])\n            else:\n                build_args.extend([\"--\", \"-j\", str(self.jobs)])\n            subprocess.check_call(build_args)",
  "class build_py(setuptools.command.build_py.build_py):\n    def run(self):\n        self.run_command(\"create_version\")\n        self.run_command(\"cmake_build\")\n\n        generated_python_files = \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, \"onnx\", \"*.py\")) + \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, \"onnx\", \"*.pyi\"))\n\n        for src in generated_python_files:\n            dst = os.path.join(\n                TOP_DIR, os.path.relpath(src, CMAKE_BUILD_DIR))\n            self.copy_file(src, dst)\n\n        return setuptools.command.build_py.build_py.run(self)",
  "class develop(setuptools.command.develop.develop):\n    def run(self):\n        self.run_command(\"build_py\")\n        setuptools.command.develop.develop.run(self)",
  "class build_ext(setuptools.command.build_ext.build_ext):\n    def run(self):\n        self.run_command(\"cmake_build\")\n        setuptools.command.build_ext.build_ext.run(self)\n\n    def build_extensions(self):\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = os.path.basename(self.get_ext_filename(fullname))\n\n            lib_path = CMAKE_BUILD_DIR\n            if os.name == \"nt\":\n                debug_lib_dir = os.path.join(lib_path, \"Debug\")\n                release_lib_dir = os.path.join(lib_path, \"Release\")\n                if os.path.exists(debug_lib_dir):\n                    lib_path = debug_lib_dir\n                elif os.path.exists(release_lib_dir):\n                    lib_path = release_lib_dir\n            src = os.path.join(lib_path, filename)\n            dst = os.path.join(os.path.realpath(self.build_lib), \"onnx\", filename)\n            self.copy_file(src, dst)",
  "class mypy_type_check(ONNXCommand):\n    description = \"Run MyPy type checker\"\n\n    def run(self):\n        \"\"\"Run command.\"\"\"\n        onnx_script = os.path.realpath(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"tools/mypy-onnx.py\"))\n        returncode = subprocess.call([sys.executable, onnx_script])\n        sys.exit(returncode)",
  "def initialize_options(self):\n        pass",
  "def finalize_options(self):\n        pass",
  "def run(self):\n        with open(os.path.join(SRC_DIR, \"version.py\"), \"w\") as f:\n            f.write(dedent(\"\"\"\\\n            # This file is generated by setup.py. DO NOT EDIT!\n\n\n            version = \"{version}\"\n            git_version = \"{git_version}\"\n            \"\"\".format(**dict(VersionInfo._asdict()))))",
  "def initialize_options(self):\n        self.jobs = None",
  "def finalize_options(self):\n        self.set_undefined_options(\"build\", (\"parallel\", \"jobs\"))\n        if self.jobs is None and os.getenv(\"MAX_JOBS\") is not None:\n            self.jobs = os.getenv(\"MAX_JOBS\")\n        self.jobs = multiprocessing.cpu_count() if self.jobs is None else int(self.jobs)",
  "def run(self):\n        if cmake_build.built:\n            return\n        cmake_build.built = True\n        if not os.path.exists(CMAKE_BUILD_DIR):\n            os.makedirs(CMAKE_BUILD_DIR)\n\n        with cd(CMAKE_BUILD_DIR):\n            build_type = \"Release\"\n            # configure\n            cmake_args = [\n                CMAKE,\n                \"-DPYTHON_INCLUDE_DIR={}\".format(sysconfig.get_python_inc()),\n                \"-DPYTHON_EXECUTABLE={}\".format(sys.executable),\n                \"-DBUILD_ONNX_PYTHON=ON\",\n                \"-DCMAKE_EXPORT_COMPILE_COMMANDS=ON\",\n                \"-DONNX_NAMESPACE={}\".format(ONNX_NAMESPACE),\n                \"-DPY_EXT_SUFFIX={}\".format(sysconfig.get_config_var(\"EXT_SUFFIX\") or ''),\n            ]\n            if COVERAGE:\n                cmake_args.append(\"-DONNX_COVERAGE=ON\")\n            if COVERAGE or DEBUG:\n                # in order to get accurate coverage information, the\n                # build needs to turn off optimizations\n                build_type = \"Debug\"\n            cmake_args.append(\"-DCMAKE_BUILD_TYPE=%s\" % build_type)\n            if WINDOWS:\n                cmake_args.extend([\n                    # we need to link with libpython on windows, so\n                    # passing python version to window in order to\n                    # find python in cmake\n                    \"-DPY_VERSION={}\".format(\"{0}.{1}\".format(*sys.version_info[:2])),\n                ])\n                if USE_MSVC_STATIC_RUNTIME:\n                    cmake_args.append(\"-DONNX_USE_MSVC_STATIC_RUNTIME=ON\")\n                if platform.architecture()[0] == \"64bit\":\n                    cmake_args.extend([\"-A\", \"x64\", \"-T\", \"host=x64\"])\n                else:\n                    cmake_args.extend([\"-A\", \"Win32\", \"-T\", \"host=x86\"])\n            if ONNX_ML:\n                cmake_args.append(\"-DONNX_ML=1\")\n            if ONNX_VERIFY_PROTO3:\n                cmake_args.append(\"-DONNX_VERIFY_PROTO3=1\")\n            if ONNX_BUILD_TESTS:\n                cmake_args.append(\"-DONNX_BUILD_TESTS=ON\")\n            if ONNX_DISABLE_EXCEPTIONS:\n                cmake_args.append(\"-DONNX_DISABLE_EXCEPTIONS=ON\")\n            if \"CMAKE_ARGS\" in os.environ:\n                extra_cmake_args = shlex.split(os.environ[\"CMAKE_ARGS\"])\n                # prevent crossfire with downstream scripts\n                del os.environ[\"CMAKE_ARGS\"]\n                log.info(\"Extra cmake args: {}\".format(extra_cmake_args))\n                cmake_args.extend(extra_cmake_args)\n            cmake_args.append(TOP_DIR)\n            log.info(\"Using cmake args: {}\".format(cmake_args))\n            if \"-DONNX_DISABLE_EXCEPTIONS=ON\" in cmake_args:\n                raise RuntimeError(\"-DONNX_DISABLE_EXCEPTIONS=ON option is only available for c++ builds. Python binding require exceptions to be enabled.\")\n            subprocess.check_call(cmake_args)\n\n            build_args = [CMAKE, \"--build\", os.curdir]\n            if WINDOWS:\n                build_args.extend([\"--config\", build_type])\n                build_args.extend([\"--\", \"/maxcpucount:{}\".format(self.jobs)])\n            else:\n                build_args.extend([\"--\", \"-j\", str(self.jobs)])\n            subprocess.check_call(build_args)",
  "def run(self):\n        self.run_command(\"create_version\")\n        self.run_command(\"cmake_build\")\n\n        generated_python_files = \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, \"onnx\", \"*.py\")) + \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, \"onnx\", \"*.pyi\"))\n\n        for src in generated_python_files:\n            dst = os.path.join(\n                TOP_DIR, os.path.relpath(src, CMAKE_BUILD_DIR))\n            self.copy_file(src, dst)\n\n        return setuptools.command.build_py.build_py.run(self)",
  "def run(self):\n        self.run_command(\"build_py\")\n        setuptools.command.develop.develop.run(self)",
  "def run(self):\n        self.run_command(\"cmake_build\")\n        setuptools.command.build_ext.build_ext.run(self)",
  "def build_extensions(self):\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = os.path.basename(self.get_ext_filename(fullname))\n\n            lib_path = CMAKE_BUILD_DIR\n            if os.name == \"nt\":\n                debug_lib_dir = os.path.join(lib_path, \"Debug\")\n                release_lib_dir = os.path.join(lib_path, \"Release\")\n                if os.path.exists(debug_lib_dir):\n                    lib_path = debug_lib_dir\n                elif os.path.exists(release_lib_dir):\n                    lib_path = release_lib_dir\n            src = os.path.join(lib_path, filename)\n            dst = os.path.join(os.path.realpath(self.build_lib), \"onnx\", filename)\n            self.copy_file(src, dst)",
  "def run(self):\n        \"\"\"Run command.\"\"\"\n        onnx_script = os.path.realpath(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"tools/mypy-onnx.py\"))\n        returncode = subprocess.call([sys.executable, onnx_script])\n        sys.exit(returncode)",
  "def run_lfs_install():\n    result = subprocess.run(['git', 'lfs', 'install'], cwd=cwd_path, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    print('Git LFS install completed with return code= {}'.format(result.returncode))",
  "def pull_lfs_file(file_name):\n    result = subprocess.run(['git', 'lfs', 'pull', '--include', file_name, '--exclude', '\\'\\''], cwd=cwd_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    print('LFS pull completed with return code= {}'.format(result.returncode))\n    print(result)",
  "def run_lfs_prune():\n    result = subprocess.run(['git', 'lfs', 'prune'], cwd=cwd_path, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    print('LFS prune completed with return code= {}'.format(result.returncode))",
  "def main():\n    parser = argparse.ArgumentParser(description='Test settings')\n    # default: test all models in the repo\n    # if test_dir is specified, only test files under that specified path\n    parser.add_argument('--test_dir', required=False, default='', type=str,\n                        help='Directory path for testing. e.g., text, vision')\n    args = parser.parse_args()\n    parent_dir = []\n    # if not set, go throught each directory\n    if not args.test_dir:\n        for file in os.listdir():\n            if os.path.isdir(file):\n                parent_dir.append(file)\n    else:\n        parent_dir.append(args.test_dir)\n    model_list = []\n    for directory in parent_dir:\n        for root, _, files in os.walk(directory):\n            for file in files:\n                if file.endswith('.onnx'):\n                    onnx_model_path = os.path.join(root, file)\n                    model_list.append(onnx_model_path)\n                    print(onnx_model_path)\n    # run lfs install before starting the tests\n    run_lfs_install()\n\n    print('=== Running ONNX Checker on {} models ==='.format(len(model_list)))\n    # run checker on each model\n    failed_models = []\n    failed_messages = []\n    skip_models = []\n    for model_path in model_list:\n        start = time.time()\n        model_name = model_path.split('/')[-1]\n        # if the model_path exists in the skip list, simply skip it\n        if model_path.replace('\\\\', '/') in config.SKIP_CHECKER_MODELS:\n            print('Skip model: {}'.format(model_path))\n            skip_models.append(model_path)\n            continue\n        print('-----------------Testing: {}-----------------'.format(model_name))\n        try:\n            pull_lfs_file(model_path)\n            model = onnx.load(model_path)\n            # stricter onnx.checker with onnx.shape_inference\n            onnx.checker.check_model(model, True)\n            # remove the model to save space in CIs\n            if os.path.exists(model_path):\n                os.remove(model_path)\n            # clean git lfs cache\n            run_lfs_prune()\n            print('[PASS]: {} is checked by onnx. '.format(model_name))\n\n        except Exception as e:\n            print('[FAIL]: {}'.format(e))\n            failed_models.append(model_path)\n            failed_messages.append((model_name, e))\n        end = time.time()\n        print('--------------Time used: {} secs-------------'.format(end - start))\n        # enable gc collection to prevent MemoryError by loading too many large models\n        gc.collect()\n\n    if len(failed_models) == 0:\n        print('{} models have been checked. {} models were skipped.'.format(len(model_list), len(skip_models)))\n    else:\n        print('In all {} models, {} models failed, {} models were skipped'.format(len(model_list), len(failed_models), len(skip_models)))\n        for model, error in failed_messages:\n            print('{} failed because: {}'.format(model, error))\n        sys.exit(1)",
  "def main():\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    directory = os.path.join(script_dir, '../onnx/backend/test/data/node')\n    count = failed_count = 0\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.onnx'):\n                test_dir_name = os.path.basename(os.path.normpath(root))\n                onnx_model_path = os.path.join(root, file)\n                try:\n                    model = onnx.load(onnx_model_path)\n                    # check model by ONNX checker\n                    inferred_model = onnx.shape_inference.infer_shapes(model, check_type=True, strict_mode=True)\n                    onnx.checker.check_model(inferred_model)\n\n                except Exception as e:\n                    failed_count += 1\n                    print(\"{} failed: {}\".format(test_dir_name, e))\n                count += 1\n    print('-----------------------------')\n    if failed_count == 0:\n        print(\"{} backend models passed.\".format(count))\n    else:\n        print(\"{} failed in {} backend models.\".format(failed_count, count))\n        sys.exit(1)",
  "class Descriptors(object):\n\n    def __init__(self, request: plugin.CodeGeneratorRequest) -> None:\n        files = {f.name: f for f in request.proto_file}\n        to_generate = {n: files[n] for n in request.file_to_generate}\n        self.files: Dict[Text, d.FileDescriptorProto] = files\n        self.to_generate: Dict[Text, d.FileDescriptorProto] = to_generate\n        self.messages: Dict[Text, d.DescriptorProto] = {}\n        self.message_to_fd: Dict[Text, d.FileDescriptorProto] = {}\n\n        def _add_enums(enums: d.EnumDescriptorProto, prefix: Text, fd: d.FileDescriptorProto) -> None:\n            for enum in enums:\n                self.message_to_fd[prefix + enum.name] = fd\n\n        def _add_messages(messages: d.DescriptorProto, prefix: Text, fd: d.FileDescriptorProto) -> None:\n            for message in messages:\n                self.messages[prefix + message.name] = message\n                self.message_to_fd[prefix + message.name] = fd\n                sub_prefix = prefix + message.name + \".\"\n                _add_messages(message.nested_type, sub_prefix, fd)\n                _add_enums(message.enum_type, sub_prefix, fd)\n\n        for fd in request.proto_file:\n            start_prefix = \".\" + fd.package + \".\"\n            _add_messages(fd.message_type, start_prefix, fd)\n            _add_enums(fd.enum_type, start_prefix, fd)",
  "class PkgWriter(object):\n    \"\"\"Writes a single pyi file\"\"\"\n\n    def __init__(self, fd: d.FileDescriptorProto, descriptors: Descriptors) -> None:\n        self.fd = fd\n        self.descriptors = descriptors\n        self.lines: List[Text] = []\n        self.indent = \"\"\n\n        # dictionary of x->y for `from {x} import {y}`\n        self.imports: Dict[Text, Set[Text]] = defaultdict(set)\n        self.locals: Set[Text] = set()\n\n    def _import(self, path: Text, name: Text, import_as: Optional[Text] = None) -> Text:\n        \"\"\"Imports a stdlib path and returns a handle to it\n        eg. self._import(\"typing\", \"Optional\") -> \"Optional\"\n        \"\"\"\n        imp = path.replace('/', '.')\n        if import_as is not None:\n            self.imports[imp].add(\"{} as {}\".format(name, import_as))\n            return import_as\n        else:\n            self.imports[imp].add(name)\n            return name\n\n    def _import_message(self, type_name: d.FieldDescriptorProto) -> Text:\n        \"\"\"Import a referenced message and return a handle\"\"\"\n        name = cast(Text, type_name)\n\n        if name[0] == '.' and name[1].isupper() and name[2].islower():\n            # Message defined in this file\n            return name[1:]\n\n        message_fd = self.descriptors.message_to_fd[name]\n        if message_fd.name == self.fd.name:\n            # message defined in this package\n            split = name.split('.')\n            for i, segment in enumerate(split):\n                if segment and segment[0].isupper() and segment[1].islower():\n                    return \".\".join(split[i:])\n\n        # Not in package. Must import\n        split = name.split(\".\")\n        for i, segment in enumerate(split):\n            if segment and segment[0].isupper() and segment[1].islower():\n                assert message_fd.name.endswith('.proto')\n                import_name = self._import(message_fd.name[:-6].replace('-', '_') + \"_pb2\", segment)\n                remains = \".\".join(split[i + 1:])\n                if not remains:\n                    return import_name\n                raise AssertionError(\"Don't support nested imports yet\")\n                # return new_nested_import(import_name, remains)\n\n        raise AssertionError(\"Could not parse local name \" + name)\n\n    @contextmanager  # type: ignore\n    def _indent(self) -> Generator[None, None, None]:\n        self.indent = self.indent + \"    \"\n        yield\n        self.indent = self.indent[:-4]\n\n    def _write_line(self, line: Text, *args: Text) -> None:\n        self.lines.append(self.indent + line.format(*args))\n\n    def write_enums(self, enums: List[d.EnumDescriptorProto]) -> None:\n        line = self._write_line\n        for enum in enums:\n            line(\"class {}(int):\", enum.name)\n            with self._indent():\n                line(\"@classmethod\")\n                line(\"def Name(cls, number: int) -> str: ...\")\n                line(\"@classmethod\")\n                line(\"def Value(cls, name: str) -> int: ...\")\n                line(\"@classmethod\")\n                line(\"def keys(cls) -> {}[str]: ...\",\n                    self._import(\"typing\", \"List\"))\n                line(\"@classmethod\")\n                line(\"def values(cls) -> {}[int]: ...\",\n                    self._import(\"typing\", \"List\"))\n                line(\"@classmethod\")\n                line(\"def items(cls) -> {}[{}[str, int]]: ...\",\n                    self._import(\"typing\", \"List\"),\n                    self._import(\"typing\", \"Tuple\"))\n\n            for val in enum.value:\n                line(\"{} = {}({}, {})\", val.name, self._import(\"typing\", \"cast\"), enum.name, val.number)\n            line(\"\")\n\n    def write_messages(self, messages: List[d.DescriptorProto], prefix: Text) -> None:\n        line = self._write_line\n        message_class = self._import(\"google.protobuf.message\", \"Message\")\n\n        for desc in messages:\n            self.locals.add(desc.name)\n            qualified_name = prefix + desc.name\n            line(\"class {}({}):\", desc.name, message_class)\n            with self._indent():\n                # Nested enums/messages\n                self.write_enums(desc.enum_type)\n                self.write_messages(desc.nested_type, qualified_name + \".\")\n\n                # Scalar fields\n                for field in [f for f in desc.field if is_scalar(f)]:\n                    if field.label == d.FieldDescriptorProto.LABEL_REPEATED:\n                        container = self._import(\"google.protobuf.internal.containers\", \"RepeatedScalarFieldContainer\")\n                        line(\"{} = ... # type: {}[{}]\", field.name, container, self.python_type(field))\n                    else:\n                        line(\"{} = ... # type: {}\", field.name, self.python_type(field))\n                line(\"\")\n\n                # Getters for non-scalar fields\n                for field in [f for f in desc.field if not is_scalar(f)]:\n                    line(\"@property\")\n                    if field.label == d.FieldDescriptorProto.LABEL_REPEATED:\n                        msg = self.descriptors.messages[field.type_name]\n                        if msg.options.map_entry:\n                            # map generates a special Entry wrapper message\n                            container = self._import(\"typing\", \"MutableMapping\")\n                            line(\"def {}(self) -> {}[{}, {}]: ...\", field.name, container, self.python_type(msg.field[0]), self.python_type(msg.field[1]))\n                        else:\n                            container = self._import(\"google.protobuf.internal.containers\", \"RepeatedCompositeFieldContainer\")\n                            line(\"def {}(self) -> {}[{}]: ...\", field.name, container, self.python_type(field))\n                    else:\n                        line(\"def {}(self) -> {}: ...\", field.name, self.python_type(field))\n                    line(\"\")\n\n                # Constructor\n                line(\"def __init__(self,\")\n                with self._indent():\n                    # Required args\n                    for field in [f for f in desc.field if f.label == d.FieldDescriptorProto.LABEL_REQUIRED]:\n                        line(\"{} : {},\", field.name, self.python_type(field))\n                    for field in [f for f in desc.field if f.label != d.FieldDescriptorProto.LABEL_REQUIRED]:\n                        if field.label == d.FieldDescriptorProto.LABEL_REPEATED:\n                            if field.type_name != '' and self.descriptors.messages[field.type_name].options.map_entry:\n                                msg = self.descriptors.messages[field.type_name]\n                                line(\"{} : {}[{}[{}, {}]] = None,\", field.name, self._import(\"typing\", \"Optional\", \"OptionalType\"),\n                                    self._import(\"typing\", \"Mapping\"), self.python_type(msg.field[0]), self.python_type(msg.field[1]))\n                            else:\n                                line(\"{} : {}[{}[{}]] = None,\", field.name, self._import(\"typing\", \"Optional\", \"OptionalType\"),\n                                  self._import(\"typing\", \"Iterable\"), self.python_type(field))\n                        else:\n                            line(\"{} : {}[{}] = None,\", field.name, self._import(\"typing\", \"Optional\", \"OptionalType\"),\n                              self.python_type(field))\n                    line(\") -> None: ...\")\n\n                # Standard message methods\n                line(\"@classmethod\")\n                line(\"def FromString(cls, s: bytes) -> {}: ...\", qualified_name)\n                line(\"def MergeFrom(self, other_msg: {}) -> None: ...\", message_class)\n                line(\"def CopyFrom(self, other_msg: {}) -> None: ...\", message_class)\n            line(\"\")\n\n    def write_services(self, services: d.ServiceDescriptorProto) -> None:\n        line = self._write_line\n\n        for service in services:\n            # The service definition interface\n            line(\"class {}({}, metaclass={}):\", service.name, self._import(\"google.protobuf.service\", \"Service\"), self._import(\"abc\", \"ABCMeta\"))\n            with self._indent():\n                for method in service.method:\n                    line(\"@{}\", self._import(\"abc\", \"abstractmethod\"))\n                    line(\"def {}(self,\", method.name)\n                    with self._indent():\n                        line(\"rpc_controller: {},\", self._import(\"google.protobuf.service\", \"RpcController\"))\n                        line(\"request: {},\", self._import_message(method.input_type))\n                        line(\"done: {}[{}[[{}], None]],\",\n                          self._import(\"typing\", \"Optional\"),\n                          self._import(\"typing\", \"Callable\"),\n                          self._import_message(method.output_type))\n                    line(\") -> {}[{}]: ...\", self._import(\"concurrent.futures\", \"Future\"), self._import_message(method.output_type))\n\n            # The stub client\n            line(\"class {}({}):\", service.name + \"_Stub\", service.name)\n            with self._indent():\n                line(\"def __init__(self, rpc_channel: {}) -> None: ...\",\n                  self._import(\"google.protobuf.service\", \"RpcChannel\"))\n\n    def python_type(self, field: d.FieldDescriptorProto) -> Text:\n        mapping: Dict[int, Callable[[], Text]] = {\n            d.FieldDescriptorProto.TYPE_DOUBLE: lambda: \"float\",\n            d.FieldDescriptorProto.TYPE_FLOAT: lambda: \"float\",\n\n            d.FieldDescriptorProto.TYPE_INT64: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_UINT64: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_FIXED64: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_SFIXED64: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_SINT64: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_INT32: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_UINT32: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_FIXED32: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_SFIXED32: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_SINT32: lambda: \"int\",\n\n            d.FieldDescriptorProto.TYPE_BOOL: lambda: \"bool\",\n            d.FieldDescriptorProto.TYPE_STRING: lambda: self._import(\"typing\", \"Text\"),\n            d.FieldDescriptorProto.TYPE_BYTES: lambda: \"bytes\",\n\n            d.FieldDescriptorProto.TYPE_ENUM: lambda: self._import_message(field.type_name),\n            d.FieldDescriptorProto.TYPE_MESSAGE: lambda: self._import_message(field.type_name),\n            d.FieldDescriptorProto.TYPE_GROUP: lambda: self._import_message(field.type_name),\n        }\n\n        assert field.type in mapping, \"Unrecognized type: \" + field.type\n        return mapping[field.type]()\n\n    def write(self) -> Text:\n        imports = []\n        for pkg, items in self.imports.items():\n            imports.append(u\"from {} import (\".format(pkg))\n            for item in sorted(items):\n                imports.append(u\"    {},\".format(item))\n            imports.append(u\")\\n\")\n\n        return \"\\n\".join(imports + self.lines)",
  "def is_scalar(fd: d.FileDescriptorProto) -> bool:\n    return not (\n        fd.type == d.FieldDescriptorProto.TYPE_MESSAGE\n        or fd.type == d.FieldDescriptorProto.TYPE_GROUP\n    )",
  "def generate_mypy_stubs(descriptors: Descriptors, response: plugin.CodeGeneratorResponse) -> None:\n    for name, fd in descriptors.to_generate.items():\n        pkg_writer = PkgWriter(fd, descriptors)\n        pkg_writer.write_enums(fd.enum_type)\n        pkg_writer.write_messages(fd.message_type, \"\")\n        pkg_writer.write_services(fd.service)\n\n        assert name == fd.name\n        assert fd.name.endswith('.proto')\n        output = response.file.add()\n        output.name = fd.name[:-6].replace('-', '_') + '_pb2.pyi'\n        output.content = HEADER + pkg_writer.write()\n        print(\"Writing mypy to\", output.name, file=sys.stderr)",
  "def main() -> None:\n    # Read request message from stdin\n    data = sys.stdin.buffer.read()\n\n    # Parse request\n    request = plugin.CodeGeneratorRequest()\n    request.ParseFromString(data)\n\n    # Create response\n    response = plugin.CodeGeneratorResponse()\n\n    # Generate mypy\n    generate_mypy_stubs(Descriptors(request), response)\n\n    # Serialise response message\n    output = response.SerializeToString()\n\n    # Write to stdout\n    sys.stdout.buffer.write(output)",
  "def __init__(self, request: plugin.CodeGeneratorRequest) -> None:\n        files = {f.name: f for f in request.proto_file}\n        to_generate = {n: files[n] for n in request.file_to_generate}\n        self.files: Dict[Text, d.FileDescriptorProto] = files\n        self.to_generate: Dict[Text, d.FileDescriptorProto] = to_generate\n        self.messages: Dict[Text, d.DescriptorProto] = {}\n        self.message_to_fd: Dict[Text, d.FileDescriptorProto] = {}\n\n        def _add_enums(enums: d.EnumDescriptorProto, prefix: Text, fd: d.FileDescriptorProto) -> None:\n            for enum in enums:\n                self.message_to_fd[prefix + enum.name] = fd\n\n        def _add_messages(messages: d.DescriptorProto, prefix: Text, fd: d.FileDescriptorProto) -> None:\n            for message in messages:\n                self.messages[prefix + message.name] = message\n                self.message_to_fd[prefix + message.name] = fd\n                sub_prefix = prefix + message.name + \".\"\n                _add_messages(message.nested_type, sub_prefix, fd)\n                _add_enums(message.enum_type, sub_prefix, fd)\n\n        for fd in request.proto_file:\n            start_prefix = \".\" + fd.package + \".\"\n            _add_messages(fd.message_type, start_prefix, fd)\n            _add_enums(fd.enum_type, start_prefix, fd)",
  "def __init__(self, fd: d.FileDescriptorProto, descriptors: Descriptors) -> None:\n        self.fd = fd\n        self.descriptors = descriptors\n        self.lines: List[Text] = []\n        self.indent = \"\"\n\n        # dictionary of x->y for `from {x} import {y}`\n        self.imports: Dict[Text, Set[Text]] = defaultdict(set)\n        self.locals: Set[Text] = set()",
  "def _import(self, path: Text, name: Text, import_as: Optional[Text] = None) -> Text:\n        \"\"\"Imports a stdlib path and returns a handle to it\n        eg. self._import(\"typing\", \"Optional\") -> \"Optional\"\n        \"\"\"\n        imp = path.replace('/', '.')\n        if import_as is not None:\n            self.imports[imp].add(\"{} as {}\".format(name, import_as))\n            return import_as\n        else:\n            self.imports[imp].add(name)\n            return name",
  "def _import_message(self, type_name: d.FieldDescriptorProto) -> Text:\n        \"\"\"Import a referenced message and return a handle\"\"\"\n        name = cast(Text, type_name)\n\n        if name[0] == '.' and name[1].isupper() and name[2].islower():\n            # Message defined in this file\n            return name[1:]\n\n        message_fd = self.descriptors.message_to_fd[name]\n        if message_fd.name == self.fd.name:\n            # message defined in this package\n            split = name.split('.')\n            for i, segment in enumerate(split):\n                if segment and segment[0].isupper() and segment[1].islower():\n                    return \".\".join(split[i:])\n\n        # Not in package. Must import\n        split = name.split(\".\")\n        for i, segment in enumerate(split):\n            if segment and segment[0].isupper() and segment[1].islower():\n                assert message_fd.name.endswith('.proto')\n                import_name = self._import(message_fd.name[:-6].replace('-', '_') + \"_pb2\", segment)\n                remains = \".\".join(split[i + 1:])\n                if not remains:\n                    return import_name\n                raise AssertionError(\"Don't support nested imports yet\")\n                # return new_nested_import(import_name, remains)\n\n        raise AssertionError(\"Could not parse local name \" + name)",
  "def _indent(self) -> Generator[None, None, None]:\n        self.indent = self.indent + \"    \"\n        yield\n        self.indent = self.indent[:-4]",
  "def _write_line(self, line: Text, *args: Text) -> None:\n        self.lines.append(self.indent + line.format(*args))",
  "def write_enums(self, enums: List[d.EnumDescriptorProto]) -> None:\n        line = self._write_line\n        for enum in enums:\n            line(\"class {}(int):\", enum.name)\n            with self._indent():\n                line(\"@classmethod\")\n                line(\"def Name(cls, number: int) -> str: ...\")\n                line(\"@classmethod\")\n                line(\"def Value(cls, name: str) -> int: ...\")\n                line(\"@classmethod\")\n                line(\"def keys(cls) -> {}[str]: ...\",\n                    self._import(\"typing\", \"List\"))\n                line(\"@classmethod\")\n                line(\"def values(cls) -> {}[int]: ...\",\n                    self._import(\"typing\", \"List\"))\n                line(\"@classmethod\")\n                line(\"def items(cls) -> {}[{}[str, int]]: ...\",\n                    self._import(\"typing\", \"List\"),\n                    self._import(\"typing\", \"Tuple\"))\n\n            for val in enum.value:\n                line(\"{} = {}({}, {})\", val.name, self._import(\"typing\", \"cast\"), enum.name, val.number)\n            line(\"\")",
  "def write_messages(self, messages: List[d.DescriptorProto], prefix: Text) -> None:\n        line = self._write_line\n        message_class = self._import(\"google.protobuf.message\", \"Message\")\n\n        for desc in messages:\n            self.locals.add(desc.name)\n            qualified_name = prefix + desc.name\n            line(\"class {}({}):\", desc.name, message_class)\n            with self._indent():\n                # Nested enums/messages\n                self.write_enums(desc.enum_type)\n                self.write_messages(desc.nested_type, qualified_name + \".\")\n\n                # Scalar fields\n                for field in [f for f in desc.field if is_scalar(f)]:\n                    if field.label == d.FieldDescriptorProto.LABEL_REPEATED:\n                        container = self._import(\"google.protobuf.internal.containers\", \"RepeatedScalarFieldContainer\")\n                        line(\"{} = ... # type: {}[{}]\", field.name, container, self.python_type(field))\n                    else:\n                        line(\"{} = ... # type: {}\", field.name, self.python_type(field))\n                line(\"\")\n\n                # Getters for non-scalar fields\n                for field in [f for f in desc.field if not is_scalar(f)]:\n                    line(\"@property\")\n                    if field.label == d.FieldDescriptorProto.LABEL_REPEATED:\n                        msg = self.descriptors.messages[field.type_name]\n                        if msg.options.map_entry:\n                            # map generates a special Entry wrapper message\n                            container = self._import(\"typing\", \"MutableMapping\")\n                            line(\"def {}(self) -> {}[{}, {}]: ...\", field.name, container, self.python_type(msg.field[0]), self.python_type(msg.field[1]))\n                        else:\n                            container = self._import(\"google.protobuf.internal.containers\", \"RepeatedCompositeFieldContainer\")\n                            line(\"def {}(self) -> {}[{}]: ...\", field.name, container, self.python_type(field))\n                    else:\n                        line(\"def {}(self) -> {}: ...\", field.name, self.python_type(field))\n                    line(\"\")\n\n                # Constructor\n                line(\"def __init__(self,\")\n                with self._indent():\n                    # Required args\n                    for field in [f for f in desc.field if f.label == d.FieldDescriptorProto.LABEL_REQUIRED]:\n                        line(\"{} : {},\", field.name, self.python_type(field))\n                    for field in [f for f in desc.field if f.label != d.FieldDescriptorProto.LABEL_REQUIRED]:\n                        if field.label == d.FieldDescriptorProto.LABEL_REPEATED:\n                            if field.type_name != '' and self.descriptors.messages[field.type_name].options.map_entry:\n                                msg = self.descriptors.messages[field.type_name]\n                                line(\"{} : {}[{}[{}, {}]] = None,\", field.name, self._import(\"typing\", \"Optional\", \"OptionalType\"),\n                                    self._import(\"typing\", \"Mapping\"), self.python_type(msg.field[0]), self.python_type(msg.field[1]))\n                            else:\n                                line(\"{} : {}[{}[{}]] = None,\", field.name, self._import(\"typing\", \"Optional\", \"OptionalType\"),\n                                  self._import(\"typing\", \"Iterable\"), self.python_type(field))\n                        else:\n                            line(\"{} : {}[{}] = None,\", field.name, self._import(\"typing\", \"Optional\", \"OptionalType\"),\n                              self.python_type(field))\n                    line(\") -> None: ...\")\n\n                # Standard message methods\n                line(\"@classmethod\")\n                line(\"def FromString(cls, s: bytes) -> {}: ...\", qualified_name)\n                line(\"def MergeFrom(self, other_msg: {}) -> None: ...\", message_class)\n                line(\"def CopyFrom(self, other_msg: {}) -> None: ...\", message_class)\n            line(\"\")",
  "def write_services(self, services: d.ServiceDescriptorProto) -> None:\n        line = self._write_line\n\n        for service in services:\n            # The service definition interface\n            line(\"class {}({}, metaclass={}):\", service.name, self._import(\"google.protobuf.service\", \"Service\"), self._import(\"abc\", \"ABCMeta\"))\n            with self._indent():\n                for method in service.method:\n                    line(\"@{}\", self._import(\"abc\", \"abstractmethod\"))\n                    line(\"def {}(self,\", method.name)\n                    with self._indent():\n                        line(\"rpc_controller: {},\", self._import(\"google.protobuf.service\", \"RpcController\"))\n                        line(\"request: {},\", self._import_message(method.input_type))\n                        line(\"done: {}[{}[[{}], None]],\",\n                          self._import(\"typing\", \"Optional\"),\n                          self._import(\"typing\", \"Callable\"),\n                          self._import_message(method.output_type))\n                    line(\") -> {}[{}]: ...\", self._import(\"concurrent.futures\", \"Future\"), self._import_message(method.output_type))\n\n            # The stub client\n            line(\"class {}({}):\", service.name + \"_Stub\", service.name)\n            with self._indent():\n                line(\"def __init__(self, rpc_channel: {}) -> None: ...\",\n                  self._import(\"google.protobuf.service\", \"RpcChannel\"))",
  "def python_type(self, field: d.FieldDescriptorProto) -> Text:\n        mapping: Dict[int, Callable[[], Text]] = {\n            d.FieldDescriptorProto.TYPE_DOUBLE: lambda: \"float\",\n            d.FieldDescriptorProto.TYPE_FLOAT: lambda: \"float\",\n\n            d.FieldDescriptorProto.TYPE_INT64: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_UINT64: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_FIXED64: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_SFIXED64: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_SINT64: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_INT32: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_UINT32: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_FIXED32: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_SFIXED32: lambda: \"int\",\n            d.FieldDescriptorProto.TYPE_SINT32: lambda: \"int\",\n\n            d.FieldDescriptorProto.TYPE_BOOL: lambda: \"bool\",\n            d.FieldDescriptorProto.TYPE_STRING: lambda: self._import(\"typing\", \"Text\"),\n            d.FieldDescriptorProto.TYPE_BYTES: lambda: \"bytes\",\n\n            d.FieldDescriptorProto.TYPE_ENUM: lambda: self._import_message(field.type_name),\n            d.FieldDescriptorProto.TYPE_MESSAGE: lambda: self._import_message(field.type_name),\n            d.FieldDescriptorProto.TYPE_GROUP: lambda: self._import_message(field.type_name),\n        }\n\n        assert field.type in mapping, \"Unrecognized type: \" + field.type\n        return mapping[field.type]()",
  "def write(self) -> Text:\n        imports = []\n        for pkg, items in self.imports.items():\n            imports.append(u\"from {} import (\".format(pkg))\n            for item in sorted(items):\n                imports.append(u\"    {},\".format(item))\n            imports.append(u\")\\n\")\n\n        return \"\\n\".join(imports + self.lines)",
  "def _add_enums(enums: d.EnumDescriptorProto, prefix: Text, fd: d.FileDescriptorProto) -> None:\n            for enum in enums:\n                self.message_to_fd[prefix + enum.name] = fd",
  "def _add_messages(messages: d.DescriptorProto, prefix: Text, fd: d.FileDescriptorProto) -> None:\n            for message in messages:\n                self.messages[prefix + message.name] = message\n                self.message_to_fd[prefix + message.name] = fd\n                sub_prefix = prefix + message.name + \".\"\n                _add_messages(message.nested_type, sub_prefix, fd)\n                _add_enums(message.enum_type, sub_prefix, fd)",
  "def parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(os.path.basename(__file__))\n    parser.add_argument('-r', '--root',\n                        default=os.path.dirname(\n                            os.path.dirname(os.path.abspath(__file__))),\n                        help='onnx root directory (default: %(default)s)')\n    parser.add_argument('-o', '--out', required=True,\n                        help='output directory')\n    return parser.parse_args()",
  "def gen_trace_file(root_dir: Text, out_path: Text) -> None:\n    subprocess.check_output([\n        'lcov',\n        '-c',\n        '-d',\n        root_dir,\n        '--no-external',\n        '--path',\n        root_dir,\n        '-o',\n        out_path])\n\n    subprocess.check_output([\n        'lcov',\n        '-r',\n        out_path,\n        os.path.join(root_dir, 'third_party', '*'),\n        '-o',\n        out_path])\n\n    subprocess.check_output([\n        'lcov',\n        '-r',\n        out_path,\n        os.path.join(root_dir, '.setuptools-cmake-build', '*'),\n        '-o',\n        out_path\n    ])",
  "def gen_html_files(root_dir: Text, trace_path: Text, out_dir: Text) -> None:\n    subprocess.check_output([\n        'genhtml',\n        trace_path,\n        '-p',\n        root_dir,\n        '-o',\n        out_dir,\n    ])",
  "def main() -> None:\n    args = parse_args()\n\n    root = os.path.abspath(args.root)\n    out = os.path.abspath(args.out)\n    if not os.path.exists(out):\n        os.makedirs(out)\n\n    trace_path = os.path.join(out, 'onnx-coverage.info')\n    gen_trace_file(root, trace_path)\n\n    html_dir = os.path.join(out, 'html')\n    gen_html_files(root, trace_path, html_dir)\n\n    print('Static HTML files have been generated at:\\n\\t{}'.format(html_dir))",
  "def main() -> None:\n    try:\n        root_folder = os.path.realpath(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n        os.chdir(root_folder)\n        # Use --no-site-packages to prevent mypy catching other typecheck errors which are not related to ONNX itself\n        subprocess.check_call([\"mypy\", \".\", \"--no-site-packages\"])\n\n        exit(0)\n    except subprocess.CalledProcessError:\n        # Catch this exception because we don't want it to output a backtrace that would clutter the mypy output\n        exit(1)",
  "def _create_checker(proto_type: Type[Message]) -> Callable[[FuncType], FuncType]:\n    def decorator(py_func: FuncType) -> FuncType:\n        @functools.wraps(py_func)\n        def checker(proto: Message, ctx: C.CheckerContext = DEFAULT_CONTEXT) -> Any:\n            if not isinstance(proto, proto_type):\n                raise RuntimeError(\n                    'You cannot pass an object that is not of type {}'.format(\n                        proto_type.__name__))\n            return getattr(C, py_func.__name__)(\n                proto.SerializeToString(), ctx)\n        return cast(FuncType, checker)\n    return decorator",
  "def check_value_info(value_info: ValueInfoProto, ctx: C.CheckerContext = DEFAULT_CONTEXT) -> None:\n    pass",
  "def check_tensor(tensor: TensorProto, ctx: C.CheckerContext = DEFAULT_CONTEXT) -> None:\n    pass",
  "def check_attribute(attr: AttributeProto, ctx: C.CheckerContext = DEFAULT_CONTEXT) -> None:\n    pass",
  "def check_node(node: NodeProto, ctx: C.CheckerContext = DEFAULT_CONTEXT) -> None:\n    pass",
  "def check_graph(graph: GraphProto, ctx: C.CheckerContext = DEFAULT_CONTEXT) -> None:\n    pass",
  "def check_sparse_tensor(sparse: SparseTensorProto, ctx: C.CheckerContext = DEFAULT_CONTEXT) -> None:\n    C.check_sparse_tensor(sparse.SerializeToString(), ctx)",
  "def check_model(model: Union[ModelProto, Text, bytes], full_check: bool = False) -> None:\n    \"\"\"Check the consistency of a model. An exception is raised if the test fails.\n\n    Arguments:\n        model (ModelProto): model to check\n        full_check (bool): if True, the function checks shapes can be inferred\n    \"\"\"\n    # If model is a path instead of ModelProto\n    if isinstance(model, str):\n        C.check_model_path(model)\n        if full_check:\n            onnx.shape_inference.infer_shapes_path(model, check_type=True, strict_mode=True)\n    else:\n        protobuf_string = model if isinstance(model, bytes) else model.SerializeToString()\n        # If the protobuf is larger than 2GB,\n        # remind users should use the model path to check\n        if sys.getsizeof(protobuf_string) > MAXIMUM_PROTOBUF:\n            raise ValueError('This protobuf of onnx model is too large (>2GB). Call check_model with model path instead.')\n        C.check_model(protobuf_string)\n        if full_check:\n            onnx.shape_inference.infer_shapes(model, check_type=True, strict_mode=True)",
  "def decorator(py_func: FuncType) -> FuncType:\n        @functools.wraps(py_func)\n        def checker(proto: Message, ctx: C.CheckerContext = DEFAULT_CONTEXT) -> Any:\n            if not isinstance(proto, proto_type):\n                raise RuntimeError(\n                    'You cannot pass an object that is not of type {}'.format(\n                        proto_type.__name__))\n            return getattr(C, py_func.__name__)(\n                proto.SerializeToString(), ctx)\n        return cast(FuncType, checker)",
  "def checker(proto: Message, ctx: C.CheckerContext = DEFAULT_CONTEXT) -> Any:\n            if not isinstance(proto, proto_type):\n                raise RuntimeError(\n                    'You cannot pass an object that is not of type {}'.format(\n                        proto_type.__name__))\n            return getattr(C, py_func.__name__)(\n                proto.SerializeToString(), ctx)",
  "class Extractor:\n    def __init__(self, model: ModelProto) -> None:\n        self.model = onnx.shape_inference.infer_shapes(model)\n        self.graph = self.model.graph\n        self.wmap = self._build_name2obj_dict(self.graph.initializer)\n        self.vimap = self._build_name2obj_dict(self.graph.value_info)\n\n    @staticmethod\n    def _build_name2obj_dict(objs):  # type: ignore\n        return {obj.name: obj for obj in objs}\n\n    def _collect_new_io_core(self, original_io, io_names_to_extract):  # type: ignore\n        original_io_map = self._build_name2obj_dict(original_io)\n        original_io_names = set(original_io_map.keys())\n        s_io_names_to_extract = set(io_names_to_extract)\n        io_names_to_keep = s_io_names_to_extract & original_io_names\n        new_io_names_to_add = s_io_names_to_extract - original_io_names\n\n        new_io_tensors = []\n        for name in io_names_to_keep:\n            new_io_tensors.append(original_io_map[name])\n        for name in new_io_names_to_add:\n            # activation become input or output\n            new_io_tensors.append(self.vimap[name])\n\n        # adjust sequence\n        new_io_tensors_map = self._build_name2obj_dict(new_io_tensors)\n        return [new_io_tensors_map[name] for name in io_names_to_extract]\n\n    def _collect_new_inputs(self, names: List[Text]) -> List[ValueInfoProto]:\n        return self._collect_new_io_core(self.graph.input, names)  # type: ignore\n\n    def _collect_new_outputs(self, names: List[Text]) -> List[ValueInfoProto]:\n        return self._collect_new_io_core(self.graph.output, names)  # type: ignore\n\n    def _dfs_search_reachable_nodes(\n            self,\n            node_output_name: Text,\n            graph_input_names: List[Text],\n            reachable_nodes: List[NodeProto],\n    ) -> None:\n        if node_output_name in graph_input_names:\n            return\n        for node in self.graph.node:\n            if node in reachable_nodes:\n                continue\n            if node_output_name not in node.output:\n                continue\n            reachable_nodes.append(node)\n            for name in node.input:\n                self._dfs_search_reachable_nodes(name, graph_input_names, reachable_nodes)\n\n    def _collect_reachable_nodes(\n            self,\n            input_names: List[Text],\n            output_names: List[Text],\n    ) -> List[NodeProto]:\n        reachable_nodes = list()  # type: ignore\n        for name in output_names:\n            self._dfs_search_reachable_nodes(name, input_names, reachable_nodes)\n        # needs to be topology sorted.\n        nodes = [n for n in self.graph.node if n in reachable_nodes]\n        return nodes\n\n    def _collect_referred_local_functions(\n            self,\n            nodes,  # type: List[NodeProto]\n    ):  # type: (...) -> List[FunctionProto]\n        # a node in a model graph may refer a function.\n        # a function contains nodes, some of which may in turn refer a function.\n        # we need to find functions referred by graph nodes and\n        # by nodes used to define functions.\n        def find_referred_funcs(nodes, referred_local_functions):  # type: ignore\n            new_nodes = []  # type: List[NodeProto]\n            for node in nodes:\n                # check if the node is a function op\n                match_function = next((\n                    f for f in self.model.functions\n                    if f.name == node.op_type and f.domain == node.domain),\n                    None)\n                if match_function and match_function not in referred_local_functions:\n                    referred_local_functions.append(match_function)\n                    new_nodes.extend(match_function.node)\n\n            return new_nodes\n\n        referred_local_functions = []  # type: List[FunctionProto]\n        new_nodes = find_referred_funcs(nodes, referred_local_functions)\n        while new_nodes:\n            new_nodes = find_referred_funcs(new_nodes, referred_local_functions)\n\n        return referred_local_functions\n\n    def _collect_reachable_tensors(\n            self,\n            nodes: List[NodeProto],\n    ) -> Tuple[List[TensorProto], List[ValueInfoProto]]:\n        all_tensors_name = set()\n        for node in nodes:\n            for name in node.input:\n                all_tensors_name.add(name)\n            for name in node.output:\n                all_tensors_name.add(name)\n\n        initializer = [self.wmap[t] for t in self.wmap.keys() if t in all_tensors_name]\n        value_info = [self.vimap[t] for t in self.vimap.keys() if t in all_tensors_name]\n        assert(len(self.graph.sparse_initializer) == 0)\n        assert(len(self.graph.quantization_annotation) == 0)\n        return (initializer, value_info)\n\n    def _make_model(\n            self,\n            nodes: List[NodeProto],\n            inputs: List[ValueInfoProto],\n            outputs: List[ValueInfoProto],\n            initializer: List[TensorProto],\n            value_info: List[ValueInfoProto],\n            local_functions: List[FunctionProto],\n    ) -> ModelProto:\n        name = 'Extracted from {' + self.graph.name + '}'\n        graph = onnx.helper.make_graph(nodes, name, inputs, outputs, initializer=initializer,\n                                      value_info=value_info)\n\n        meta = {\n            'ir_version': self.model.ir_version,\n            'opset_imports': self.model.opset_import,\n            'producer_name': 'onnx.utils.extract_model',\n            'functions': local_functions,\n        }\n        return onnx.helper.make_model(graph, **meta)\n\n    def extract_model(\n            self,\n            input_names: List[Text],\n            output_names: List[Text],\n    ) -> ModelProto:\n        inputs = self._collect_new_inputs(input_names)\n        outputs = self._collect_new_outputs(output_names)\n        nodes = self._collect_reachable_nodes(input_names, output_names)\n        initializer, value_info = self._collect_reachable_tensors(nodes)\n        local_functions = self._collect_referred_local_functions(nodes)\n        model = self._make_model(nodes, inputs, outputs, initializer, value_info, local_functions)\n\n        return model",
  "def extract_model(\n        input_path: Text,\n        output_path: Text,\n        input_names: List[Text],\n        output_names: List[Text],\n        check_model: bool = True,\n) -> None:\n    \"\"\"Extracts sub-model from an ONNX model.\n\n    The sub-model is defined by the names of the input and output tensors *exactly*.\n\n    Note: For control-flow operators, e.g. If and Loop, the _boundary of sub-model_,\n    which is defined by the input and output tensors, should not _cut through_ the\n    subgraph that is connected to the _main graph_ as attributes of these operators.\n\n    Arguments:\n        input_path (string): The path to original ONNX model.\n        output_path (string): The path to save the extracted ONNX model.\n        input_names (list of string): The names of the input tensors that to be extracted.\n        output_names (list of string): The names of the output tensors that to be extracted.\n        check_model (bool): Whether to run model checker on the extracted model.\n    \"\"\"\n    if not os.path.exists(input_path):\n        raise ValueError(\"Invalid input model path: %s\" % input_path)\n    if not output_path:\n        raise ValueError(\"Output model path shall not be empty!\")\n    if not output_names:\n        raise ValueError(\"Output tensor names shall not be empty!\")\n\n    onnx.checker.check_model(input_path)\n    model = onnx.load(input_path)\n\n    e = Extractor(model)\n    extracted = e.extract_model(input_names, output_names)\n\n    onnx.save(extracted, output_path)\n    if check_model:\n        onnx.checker.check_model(output_path)",
  "def __init__(self, model: ModelProto) -> None:\n        self.model = onnx.shape_inference.infer_shapes(model)\n        self.graph = self.model.graph\n        self.wmap = self._build_name2obj_dict(self.graph.initializer)\n        self.vimap = self._build_name2obj_dict(self.graph.value_info)",
  "def _build_name2obj_dict(objs):  # type: ignore\n        return {obj.name: obj for obj in objs}",
  "def _collect_new_io_core(self, original_io, io_names_to_extract):  # type: ignore\n        original_io_map = self._build_name2obj_dict(original_io)\n        original_io_names = set(original_io_map.keys())\n        s_io_names_to_extract = set(io_names_to_extract)\n        io_names_to_keep = s_io_names_to_extract & original_io_names\n        new_io_names_to_add = s_io_names_to_extract - original_io_names\n\n        new_io_tensors = []\n        for name in io_names_to_keep:\n            new_io_tensors.append(original_io_map[name])\n        for name in new_io_names_to_add:\n            # activation become input or output\n            new_io_tensors.append(self.vimap[name])\n\n        # adjust sequence\n        new_io_tensors_map = self._build_name2obj_dict(new_io_tensors)\n        return [new_io_tensors_map[name] for name in io_names_to_extract]",
  "def _collect_new_inputs(self, names: List[Text]) -> List[ValueInfoProto]:\n        return self._collect_new_io_core(self.graph.input, names)",
  "def _collect_new_outputs(self, names: List[Text]) -> List[ValueInfoProto]:\n        return self._collect_new_io_core(self.graph.output, names)",
  "def _dfs_search_reachable_nodes(\n            self,\n            node_output_name: Text,\n            graph_input_names: List[Text],\n            reachable_nodes: List[NodeProto],\n    ) -> None:\n        if node_output_name in graph_input_names:\n            return\n        for node in self.graph.node:\n            if node in reachable_nodes:\n                continue\n            if node_output_name not in node.output:\n                continue\n            reachable_nodes.append(node)\n            for name in node.input:\n                self._dfs_search_reachable_nodes(name, graph_input_names, reachable_nodes)",
  "def _collect_reachable_nodes(\n            self,\n            input_names: List[Text],\n            output_names: List[Text],\n    ) -> List[NodeProto]:\n        reachable_nodes = list()  # type: ignore\n        for name in output_names:\n            self._dfs_search_reachable_nodes(name, input_names, reachable_nodes)\n        # needs to be topology sorted.\n        nodes = [n for n in self.graph.node if n in reachable_nodes]\n        return nodes",
  "def _collect_referred_local_functions(\n            self,\n            nodes,  # type: List[NodeProto]\n    ):  # type: (...) -> List[FunctionProto]\n        # a node in a model graph may refer a function.\n        # a function contains nodes, some of which may in turn refer a function.\n        # we need to find functions referred by graph nodes and\n        # by nodes used to define functions.\n        def find_referred_funcs(nodes, referred_local_functions):  # type: ignore\n            new_nodes = []  # type: List[NodeProto]\n            for node in nodes:\n                # check if the node is a function op\n                match_function = next((\n                    f for f in self.model.functions\n                    if f.name == node.op_type and f.domain == node.domain),\n                    None)\n                if match_function and match_function not in referred_local_functions:\n                    referred_local_functions.append(match_function)\n                    new_nodes.extend(match_function.node)\n\n            return new_nodes\n\n        referred_local_functions = []  # type: List[FunctionProto]\n        new_nodes = find_referred_funcs(nodes, referred_local_functions)\n        while new_nodes:\n            new_nodes = find_referred_funcs(new_nodes, referred_local_functions)\n\n        return referred_local_functions",
  "def _collect_reachable_tensors(\n            self,\n            nodes: List[NodeProto],\n    ) -> Tuple[List[TensorProto], List[ValueInfoProto]]:\n        all_tensors_name = set()\n        for node in nodes:\n            for name in node.input:\n                all_tensors_name.add(name)\n            for name in node.output:\n                all_tensors_name.add(name)\n\n        initializer = [self.wmap[t] for t in self.wmap.keys() if t in all_tensors_name]\n        value_info = [self.vimap[t] for t in self.vimap.keys() if t in all_tensors_name]\n        assert(len(self.graph.sparse_initializer) == 0)\n        assert(len(self.graph.quantization_annotation) == 0)\n        return (initializer, value_info)",
  "def _make_model(\n            self,\n            nodes: List[NodeProto],\n            inputs: List[ValueInfoProto],\n            outputs: List[ValueInfoProto],\n            initializer: List[TensorProto],\n            value_info: List[ValueInfoProto],\n            local_functions: List[FunctionProto],\n    ) -> ModelProto:\n        name = 'Extracted from {' + self.graph.name + '}'\n        graph = onnx.helper.make_graph(nodes, name, inputs, outputs, initializer=initializer,\n                                      value_info=value_info)\n\n        meta = {\n            'ir_version': self.model.ir_version,\n            'opset_imports': self.model.opset_import,\n            'producer_name': 'onnx.utils.extract_model',\n            'functions': local_functions,\n        }\n        return onnx.helper.make_model(graph, **meta)",
  "def extract_model(\n            self,\n            input_names: List[Text],\n            output_names: List[Text],\n    ) -> ModelProto:\n        inputs = self._collect_new_inputs(input_names)\n        outputs = self._collect_new_outputs(output_names)\n        nodes = self._collect_reachable_nodes(input_names, output_names)\n        initializer, value_info = self._collect_reachable_tensors(nodes)\n        local_functions = self._collect_referred_local_functions(nodes)\n        model = self._make_model(nodes, inputs, outputs, initializer, value_info, local_functions)\n\n        return model",
  "def find_referred_funcs(nodes, referred_local_functions):  # type: ignore\n            new_nodes = []  # type: List[NodeProto]\n            for node in nodes:\n                # check if the node is a function op\n                match_function = next((\n                    f for f in self.model.functions\n                    if f.name == node.op_type and f.domain == node.domain),\n                    None)\n                if match_function and match_function not in referred_local_functions:\n                    referred_local_functions.append(match_function)\n                    new_nodes.extend(match_function.node)\n\n            return new_nodes",
  "def check_overlapping_names(\n    g1: GraphProto,\n    g2: GraphProto,\n    io_map: Optional[List[Tuple[Text, Text]]] = None\n) -> List[Tuple[Text, List[Text]]]:\n    \"\"\"Checks whether there are name collisions between two graphs\n\n    Returns a list of tuples where the first element represents the member containing overlapping names\n    (One of: \"node\", \"edge\", \"value_info\", \"initializer\", \"sparse_initializer\"), and the\n    second element contains a list of names that appear in both graphs on that category.\n\n    Optionally, it takes an io_map, representing the output/inputs to be connected. It provided, overlapping\n    present in the io_map argument will be ignored.\n    \"\"\"\n    if type(g1) is not GraphProto:\n        raise ValueError(\"g1 argument is not an ONNX graph\")\n    if type(g2) is not GraphProto:\n        raise ValueError(\"g2 argument is not an ONNX graph\")\n\n    def _overlapping(c1: List[Text], c2: List[Text]) -> List[Text]:\n        return list(set(c1) & set(c2))\n\n    def _edge_names(graph: GraphProto, exclude: Set[Text] = set()) -> List[Text]:\n        edges = []\n        for n in graph.node:\n            for i in n.input:\n                if i != '' and i not in exclude:\n                    edges.append(i)\n            for o in n.output:\n                if o != '' and o not in exclude:\n                    edges.append(o)\n        return edges\n\n    result = []\n\n    if not io_map:\n        io_map = []\n    io_map_inputs = set([elem[1] for elem in io_map])\n\n    # Edges already cover input/output\n    overlap = _overlapping(\n        _edge_names(g1), _edge_names(g2, exclude=io_map_inputs)\n    )\n    if len(overlap) > 0:\n        result.append(('edge', overlap))\n\n    overlap = _overlapping([e.name for e in g1.value_info], [e.name for e in g2.value_info])\n    if len(overlap) > 0:\n        result.append(('value_info', overlap))\n\n    overlap = _overlapping([e.name for e in g1.initializer], [e.name for e in g2.initializer])\n    if len(overlap) > 0:\n        result.append(('initializer', overlap))\n\n    overlap = _overlapping([e.values.name for e in g1.sparse_initializer],\n                           [e.values.name for e in g2.sparse_initializer]) + \\\n              _overlapping([e.indices.name for e in g1.sparse_initializer],\n                           [e.indices.name for e in g2.sparse_initializer])\n    if len(overlap) > 0:\n        result.append(('sparse_initializer', overlap))\n\n    return result",
  "def merge_graphs(\n        g1: GraphProto,\n        g2: GraphProto,\n        io_map: List[Tuple[Text, Text]],\n        inputs: Optional[List[Text]] = None,\n        outputs: Optional[List[Text]] = None,\n        prefix1: Optional[Text] = None,\n        prefix2: Optional[Text] = None,\n        name: Optional[Text] = None,\n        doc_string: Optional[Text] = None,\n) -> GraphProto:\n    \"\"\"Combines two ONNX graphs into a single one.\n\n    The combined graph is defined by connecting the specified set of outputs/inputs. Those inputs/outputs\n    not specified in the io_map argument will remain as inputs/outputs of the combined graph.\n\n    Arguments:\n        g1 (GraphProto): First graph\n        g2 (GraphProto): Second graph\n        io_map (list of pairs of string): The pairs of names [(out0, in0), (out1, in1), ...]\n                                          representing outputs of the first graph and inputs of the second\n                                          to be connected\n        inputs (list of string): Optional list of inputs to be included in the combined graph\n                                 By default, all inputs not present in the ``io_map`` argument will be\n                                 included in the combined model\n        outputs (list of string): Optional list of outputs to be included in the combined graph\n                                  By default, all outputs not present in the ``io_map`` argument will be\n                                  included in the combined model\n        prefix1 (string): Optional prefix to be added to all names in g1\n        prefix2 (string): Optional prefix to be added to all names in g2\n        name (string): Optional name for the combined graph\n                       By default, the name is g1.name and g2.name concatenated with an undescore delimiter\n        doc_string (string): Optional docstring for the combined graph\n                             If not provided, a default docstring with the concatenation of g1 and g2 docstrings is used\n\n    Returns:\n        GraphProto\n    \"\"\"\n    if type(g1) is not GraphProto:\n        raise ValueError(\"g1 argument is not an ONNX graph\")\n    if type(g2) is not GraphProto:\n        raise ValueError(\"g2 argument is not an ONNX graph\")\n\n    # Prefixing names in the graph if requested, adjusting io_map accordingly\n    if prefix1 or prefix2:\n        if prefix1:\n            g1_copy = GraphProto()\n            g1_copy.CopyFrom(g1)\n            g1 = g1_copy\n            g1 = add_prefix_graph(g1, prefix=prefix1)\n        if prefix2:\n            g2_copy = GraphProto()\n            g2_copy.CopyFrom(g2)\n            g2 = g2_copy\n            g2 = add_prefix_graph(g2, prefix=prefix2)\n        io_map = [\n            (prefix1 + io[0] if prefix1 else io[0],\n             prefix2 + io[1] if prefix2 else io[1])\n            for io in io_map]\n\n    io_map_g1_outs = set([io[0] for io in io_map])\n    io_map_g2_ins = set([io[1] for io in io_map])\n    reversed_io_map = {in_name: out_name for out_name, in_name in io_map}\n    g1_outs = set([o.name for o in g1.output])\n    g2_ins = set([i.name for i in g2.input])\n\n    # If necessary extract subgraphs\n    if inputs or outputs:\n        if not inputs:\n            g1_inputs = [i.name for i in g1.input]\n            g2_inputs = [i.name for i in g2.input]\n        else:\n            input_set = set(inputs)\n            g1_inputs = [i.name for i in g1.input if i.name in input_set]\n            g2_inputs = [i.name for i in g2.input if i.name in input_set or i.name in io_map_g2_ins]\n\n        if not outputs:\n            g1_outputs = [o.name for o in g1.input]\n            g2_outputs = [o.name for o in g2.input]\n        else:\n            output_set = set(outputs)\n            g1_outputs = [o.name for o in g1.output if o.name in output_set or o.name in io_map_g1_outs]\n            g2_outputs = [o.name for o in g2.output if o.name in output_set]\n\n        if len(g1_inputs) < len(g1.input) or len(g1_outputs) < len(g1.output):\n            e1 = utils.Extractor(helper.make_model(g1))\n            g1 = e1.extract_model(g1_inputs, g1_outputs).graph\n\n        if len(g2_inputs) < len(g2.input) or len(g2_outputs) < len(g2.output):\n            e2 = utils.Extractor(helper.make_model(g2))\n            g2 = e2.extract_model(g2_inputs, g2_outputs).graph\n\n    # Check that input/output names specified in the io_map argument are valid input/output names\n    for g1_out_name, g2_in_name in io_map:\n        if g1_out_name not in g1_outs:\n            raise ValueError(f\"Output {g1_out_name} is not present in g1\")\n        if g2_in_name not in g2_ins:\n            raise ValueError(f\"Input {g2_in_name} is not present in g2\")\n\n    # Check for name collision\n    overlapping_names = check_overlapping_names(g1, g2, io_map)\n    if len(overlapping_names) > 0:\n        category, names = overlapping_names[0]\n        raise ValueError(\n            \"Cant merge two graphs with overlapping names. \"\n            f\"Found repeated {category} names: \" + \", \".join(names) + \"\\n\"\n            + \"Consider using ``onnx.compose.add_prefix`` to add a prefix to names in one of the graphs.\"\n        )\n\n    g = GraphProto()\n\n    g.node.extend(g1.node)\n    g2_nodes_begin = len(g.node)\n    g.node.extend(g2.node)\n    g2_nodes_end = len(g.node)\n\n    # Connecting outputs of the first graph with the inputs of the second\n    for node_idx in range(g2_nodes_begin, g2_nodes_end):\n        node = g.node[node_idx]\n        for index, name in enumerate(node.input):\n            if name in reversed_io_map:\n                node.input[index] = reversed_io_map[name]\n\n    if inputs:\n        input_set = set(inputs)\n        g.input.extend([i for i in g1.input if i.name in input_set])\n        g.input.extend([i for i in g2.input if i.name in input_set])\n    else:\n        g.input.extend(g1.input)\n        g.input.extend([i for i in g2.input if i.name not in io_map_g2_ins])\n\n    if outputs:\n        output_set = set(outputs)\n        g.output.extend([o for o in g1.output if o.name in output_set])\n        g.output.extend([o for o in g2.output if o.name in output_set])\n    else:\n        g.output.extend([o for o in g1.output if o.name not in io_map_g1_outs])\n        g.output.extend(g2.output)\n\n    g.initializer.extend(g1.initializer)\n    g.initializer.extend(\n        [init for init in g2.initializer if init.name not in io_map_g2_ins])\n\n    g.sparse_initializer.extend(g1.sparse_initializer)\n    g.sparse_initializer.extend(\n        [init for init in g2.sparse_initializer if init.values.name not in io_map_g2_ins])\n\n    g.value_info.extend(g1.value_info)\n    g.value_info.extend([vi for vi in g2.value_info if vi.name not in io_map_g2_ins])\n\n    g.name = name if name is not None else \"_\".join([g1.name, g2.name])\n\n    if doc_string is None:\n        doc_string = f\"Graph combining {g1.name} and {g2.name}\\n\" + \\\n            g1.name + \"\\n\\n\" + g1.doc_string + \"\\n\\n\" + g2.name + \"\\n\\n\" + g2.doc_string\n    g.doc_string = doc_string\n\n    return g",
  "def merge_models(\n        m1: ModelProto,\n        m2: ModelProto,\n        io_map: List[Tuple[Text, Text]],\n        inputs: Optional[List[Text]] = None,\n        outputs: Optional[List[Text]] = None,\n        prefix1: Optional[Text] = None,\n        prefix2: Optional[Text] = None,\n        name: Optional[Text] = None,\n        doc_string: Optional[Text] = None,\n        producer_name: Optional[Text] = 'onnx.compose.merge_models',\n        producer_version: Optional[Text] = \"1.0\",\n        domain: Optional[Text] = \"\",\n        model_version: Optional[int] = 1\n) -> ModelProto:\n    \"\"\"Combines two ONNX models into a single one.\n\n    The combined model is defined by connecting the specified set of outputs/inputs.\n    Those inputs/outputs not specified in the io_map argument will remain as\n    inputs/outputs of the combined model.\n\n    Both models should have the same IR version, and same operator sets imported.\n\n    Arguments:\n        m1 (ModelProto): First model\n        m2 (ModelProto): Second model\n        io_map (list of pairs of string): The pairs of names [(out0, in0), (out1, in1), ...]\n                                          representing outputs of the first graph and inputs of the second\n                                          to be connected\n        inputs (list of string): Optional list of inputs to be included in the combined graph\n                                 By default, all inputs not present in the ``io_map`` argument will be\n                                 included in the combined model\n        outputs (list of string): Optional list of outputs to be included in the combined graph\n                                  By default, all outputs not present in the ``io_map`` argument will be\n                                  included in the combined model\n        prefix1 (string): Optional prefix to be added to all names in m1\n        prefix2 (string): Optional prefix to be added to all names in m2\n        name (string): Optional name for the combined graph\n                       By default, the name is g1.name and g2.name concatenated with an undescore delimiter\n        doc_string (string): Optional docstring for the combined graph\n                             If not provided, a default docstring with the concatenation of g1 and g2 docstrings is used\n        producer_name (string): Optional producer name for the combined model. Default: 'onnx.compose'\n        producer_version (string): Optional producer version for the combined model. Default: \"1.0\"\n        domain (string): Optional domain of the combined model. Default: \"\"\n        model_version (int): Optional version of the graph encoded. Default: 1\n\n    Returns:\n        ModelProto\n    \"\"\"\n    if type(m1) is not ModelProto:\n        raise ValueError(\"m1 argument is not an ONNX model\")\n    if type(m2) is not ModelProto:\n        raise ValueError(\"m2 argument is not an ONNX model\")\n\n    if m1.ir_version != m2.ir_version:\n        raise ValueError(\n            f\"IR version mismatch {m1.ir_version} != {m2.ir_version}.\"\n            \" Both models should have have the same IR version\")\n    ir_version = m1.ir_version\n\n    opset_import_map: MutableMapping[Text, int] = {}\n    opset_imports = \\\n        [entry for entry in m1.opset_import] + \\\n        [entry for entry in m2.opset_import]\n\n    for entry in opset_imports:\n        if entry.domain in opset_import_map:\n            found_version = opset_import_map[entry.domain]\n            if entry.version != found_version:\n                raise ValueError(\n                    \"Can't merge two models with different operator set ids for a given domain. \"\n                    f\"Got: {m1.opset_import} and {m2.opset_import}\")\n        else:\n            opset_import_map[entry.domain] = entry.version\n\n    # Prefixing names in the graph if requested, adjusting io_map accordingly\n    if prefix1 or prefix2:\n        if prefix1:\n            m1_copy = ModelProto()\n            m1_copy.CopyFrom(m1)\n            m1 = m1_copy\n            m1 = add_prefix(m1, prefix=prefix1)\n        if prefix2:\n            m2_copy = ModelProto()\n            m2_copy.CopyFrom(m2)\n            m2 = m2_copy\n            m2 = add_prefix(m2, prefix=prefix2)\n        io_map = [\n            (prefix1 + io[0] if prefix1 else io[0],\n             prefix2 + io[1] if prefix2 else io[1])\n            for io in io_map]\n\n    graph = merge_graphs(m1.graph, m2.graph, io_map,\n                         inputs=inputs, outputs=outputs,\n                         name=name, doc_string=doc_string)\n    model = helper.make_model(graph,\n                              producer_name=producer_name,\n                              producer_version=producer_version,\n                              domain=domain,\n                              model_version=model_version,\n                              opset_imports=opset_imports,\n                              ir_version=ir_version)\n\n    # Merging model metadata props\n    model_props = {}\n    for meta_entry in m1.metadata_props:\n        model_props[meta_entry.key] = meta_entry.value\n    for meta_entry in m2.metadata_props:\n        if meta_entry.key in model_props:\n            value = model_props[meta_entry.key]\n            if value != meta_entry.value:\n                raise ValueError(\n                    \"Can't merge models with different values for the same model metadata property.\"\n                    f\" Found: property = {meta_entry.key}, with values {value} and {meta_entry.value}.\"\n                )\n        else:\n            model_props[meta_entry.key] = meta_entry.value\n    helper.set_model_props(model, model_props)\n\n    # Merging functions\n    function_overlap = list(set([f.name for f in m1.functions]) & set([f.name for f in m2.functions]))\n    if function_overlap:\n        raise ValueError(\n            \"Can't merge models with overlapping local function names.\"\n            \" Found in both graphs: \" + ', '.join(function_overlap)\n        )\n    model.functions.MergeFrom(m1.functions)\n    model.functions.MergeFrom(m2.functions)\n\n    checker.check_model(model)\n    return model",
  "def add_prefix_graph(\n        graph: GraphProto,\n        prefix: Text,\n        rename_nodes: Optional[bool] = True,\n        rename_edges: Optional[bool] = True,\n        rename_inputs: Optional[bool] = True,\n        rename_outputs: Optional[bool] = True,\n        rename_initializers: Optional[bool] = True,\n        rename_value_infos: Optional[bool] = True,\n        inplace: Optional[bool] = False,\n) -> GraphProto:\n    \"\"\"Adds a prefix to names of elements in a graph: nodes, edges, inputs, outputs,\n    initializers, sparse initializer, value infos.\n\n    It can be used as a utility before merging graphs that have overlapping names.\n    Empty names are not prefixed.\n\n    Arguments:\n        graph (GraphProto): Graph\n        prefix (Text): Prefix to be added to each name in the graph\n        rename_nodes (bool): Whether to prefix node names\n        rename_edges (bool): Whether to prefix node edge names\n        rename_inputs (bool): Whether to prefix input names\n        rename_outputs (bool): Whether to prefix output names\n        rename_initializers (bool): Whether to prefix initializer and sparse initializer names\n        rename_value_infos (bool): Whether to prefix value info names\n        inplace (bool): If True, mutates the graph directly.\n                        Otherwise, a copy will be created\n\n    Returns:\n        GraphProto\n    \"\"\"\n    if type(graph) is not GraphProto:\n        raise ValueError(\"graph argument is not an ONNX graph\")\n\n    if not inplace:\n        g = GraphProto()\n        g.CopyFrom(graph)\n    else:\n        g = graph\n\n    def _prefixed(prefix: Text, name: Text) -> Text:\n        return prefix + name if len(name) > 0 else name\n\n    name_map = {}\n    if rename_edges:\n        for n in g.node:\n            for e in n.input:\n                name_map[e] = _prefixed(prefix, e)\n            for e in n.output:\n                name_map[e] = _prefixed(prefix, e)\n    else:\n        if rename_outputs:\n            for entry in g.output:\n                name_map[entry.name] = _prefixed(prefix, entry.name)\n        if rename_inputs:\n            for entry in g.input:\n                name_map[entry.name] = _prefixed(prefix, entry.name)\n\n    if rename_nodes:\n        for n in g.node:\n            n.name = _prefixed(prefix, n.name)\n\n    if rename_initializers:\n        for init in g.initializer:\n            name_map[init.name] = _prefixed(prefix, init.name)\n        for sparse_init in g.sparse_initializer:\n            name_map[sparse_init.values.name] = _prefixed(prefix, sparse_init.values.name)\n            name_map[sparse_init.indices.name] = _prefixed(prefix, sparse_init.indices.name)\n\n    if rename_value_infos:\n        for entry in g.value_info:\n            name_map[entry.name] = _prefixed(prefix, entry.name)\n\n    for n in g.node:\n        for i in range(len(n.output)):\n            if n.output[i] in name_map:\n                n.output[i] = name_map[n.output[i]]\n        for i in range(len(n.input)):\n            if n.input[i] in name_map:\n                n.input[i] = name_map[n.input[i]]\n\n    for in_desc in g.input:\n        if in_desc.name in name_map:\n            in_desc.name = name_map[in_desc.name]\n    for out_desc in g.output:\n        if out_desc.name in name_map:\n            out_desc.name = name_map[out_desc.name]\n\n    for initializer in g.initializer:\n        if initializer.name in name_map:\n            initializer.name = name_map[initializer.name]\n    for sparse_initializer in g.sparse_initializer:\n        if sparse_initializer.values.name in name_map:\n            sparse_initializer.values.name = name_map[sparse_initializer.values.name]\n        if sparse_initializer.indices.name in name_map:\n            sparse_initializer.indices.name = name_map[sparse_initializer.indices.name]\n\n    for value_info in g.value_info:\n        if value_info.name in name_map:\n            value_info.name = name_map[value_info.name]\n\n    return g",
  "def add_prefix(\n        model: ModelProto,\n        prefix: Text,\n        rename_nodes: Optional[bool] = True,\n        rename_edges: Optional[bool] = True,\n        rename_inputs: Optional[bool] = True,\n        rename_outputs: Optional[bool] = True,\n        rename_initializers: Optional[bool] = True,\n        rename_value_infos: Optional[bool] = True,\n        rename_functions: Optional[bool] = True,\n        inplace: Optional[bool] = False,\n) -> ModelProto:\n    \"\"\"Adds a prefix to names of elements in a graph: nodes, edges, inputs, outputs,\n    initializers, sparse initializer, value infos, and local functions.\n\n    It can be used as a utility before merging graphs that have overlapping names.\n    Empty names are not _prefixed.\n\n    Arguments:\n        model (ModelProto): Model\n        prefix (Text): Prefix to be added to each name in the graph\n        rename_nodes (bool): Whether to prefix node names\n        rename_edges (bool): Whether to prefix node edge names\n        rename_inputs (bool): Whether to prefix input names\n        rename_outputs (bool): Whether to prefix output names\n        rename_initializers (bool): Whether to prefix initializer and sparse initializer names\n        rename_value_infos (bool): Whether to prefix value info nanes\n        rename_functions (bool): Whether to prefix local function names\n        inplace (bool): If True, mutates the model directly.\n                        Otherwise, a copy will be created\n\n    Returns:\n        ModelProto\n    \"\"\"\n    if type(model) is not ModelProto:\n        raise ValueError(\"model argument is not an ONNX model\")\n\n    if not inplace:\n        m = ModelProto()\n        m.CopyFrom(model)\n        model = m\n\n    add_prefix_graph(\n        model.graph, prefix,\n        rename_nodes=rename_nodes,\n        rename_edges=rename_edges,\n        rename_inputs=rename_inputs,\n        rename_outputs=rename_outputs,\n        rename_initializers=rename_initializers,\n        rename_value_infos=rename_value_infos,\n        inplace=True  # No need to create a copy, since it's a new model\n    )\n\n    if rename_functions:\n        f_name_map = {}\n        for f in model.functions:\n            new_f_name = prefix + f.name\n            f_name_map[f.name] = new_f_name\n            f.name = new_f_name\n        # Adjust references to local functions in other local function\n        # definitions\n        for f in model.functions:\n            for n in f.node:\n                if n.op_type in f_name_map:\n                    n.op_type = f_name_map[n.op_type]\n        # Adjust references to local functions in the graph\n        for n in model.graph.node:\n            if n.op_type in f_name_map:\n                n.op_type = f_name_map[n.op_type]\n\n    return model",
  "def expand_out_dim_graph(\n        graph: GraphProto,\n        dim_idx: int,\n        inplace: Optional[bool] = False,\n) -> GraphProto:\n    \"\"\"Inserts an extra dimension with extent 1 to each output in the graph.\n\n    Inserts an Unsqueeze node for each output. It can be used as a utility before merging graphs,\n    for example when the second one expects a batch dimension.\n\n    Arguments:\n        graph (GraphProto): Graph\n        dim_idx (int): Index of the dimension to be inserted.\n                       A negative value means counting dimensions from the back.\n        inplace (bool): If True, mutates the model directly.\n                        Otherwise, a copy will be created\n\n    Returns:\n        GraphProto\n    \"\"\"\n    if type(graph) is not GraphProto:\n        raise ValueError(\"graph argument is not an ONNX graph\")\n\n    if not inplace:\n        g = GraphProto()\n        g.CopyFrom(graph)\n    else:\n        g = graph\n\n    orig_out_names = [output.name for output in g.output]\n\n    for n in g.node:\n        for i in range(len(n.output)):\n            if n.output[i] in orig_out_names:\n                n.output[i] = n.output[i] + f'_collapsed_dim_{dim_idx}'\n        for i in range(len(n.input)):\n            if n.input[i] in orig_out_names:\n                n.input[i] = n.input[i] + f'_collapsed_dim_{dim_idx}'\n\n    expand_dim_k = g.name + \"_expand_out_dim_idx\"\n    g.node.append(\n        helper.make_node(\n            'Constant', inputs=[], outputs=[expand_dim_k], name=f\"{expand_dim_k}-constant\",\n            value=helper.make_tensor(name=f\"{expand_dim_k}-value\", data_type=tp.INT64,\n                                     dims=[1, ], vals=[dim_idx, ]))\n    )\n\n    for _ in range(len(g.output)):\n        o = g.output.pop(0)\n        prev_output = o.name + f'_collapsed_dim_{dim_idx}'\n        g.node.append(\n            helper.make_node('Unsqueeze', inputs=[prev_output, expand_dim_k],\n                             outputs=[o.name], name=f\"unsqueeze-{o.name}\")\n        )\n        new_shape = [d.dim_value for d in o.type.tensor_type.shape.dim]\n        new_shape.insert(dim_idx, 1)\n        g.output.append(\n            helper.make_tensor_value_info(o.name, o.type.tensor_type.elem_type, new_shape))\n    return g",
  "def expand_out_dim(\n        model: ModelProto,\n        dim_idx: int,\n        inplace: Optional[bool] = False,\n) -> ModelProto:\n    \"\"\"Inserts an extra dimension with extent 1 to each output in the graph.\n\n    Inserts an Unsqueeze node for each output. It can be used as a utility before merging graphs,\n    for example when the second one expects a batch dimension.\n\n    Arguments:\n        model (ModelProto): Model\n        dim_idx (int): Index of the dimension to be inserted.\n                       A negative value means counting dimensions from the back.\n        inplace (bool): If True, mutates the model directly.\n                        Otherwise, a copy will be created\n\n    Returns:\n        ModelProto\n    \"\"\"\n    if type(model) is not ModelProto:\n        raise ValueError(\"model argument is not an ONNX model\")\n\n    if not inplace:\n        m = ModelProto()\n        m.CopyFrom(model)\n        model = m\n\n    expand_out_dim_graph(\n        model.graph,\n        dim_idx,\n        inplace=True  # No need to create a copy, since it's a new model\n    )\n    return model",
  "def _overlapping(c1: List[Text], c2: List[Text]) -> List[Text]:\n        return list(set(c1) & set(c2))",
  "def _edge_names(graph: GraphProto, exclude: Set[Text] = set()) -> List[Text]:\n        edges = []\n        for n in graph.node:\n            for i in n.input:\n                if i != '' and i not in exclude:\n                    edges.append(i)\n            for o in n.output:\n                if o != '' and o not in exclude:\n                    edges.append(o)\n        return edges",
  "def _prefixed(prefix: Text, name: Text) -> Text:\n        return prefix + name if len(name) > 0 else name",
  "def process_ifs(lines: Iterable[Text], onnx_ml: bool) -> Iterable[Text]:\n    in_if = 0\n    for line in lines:\n        if IF_ONNX_ML_REGEX.match(line):\n            assert 0 == in_if\n            in_if = 1\n        elif ELSE_ONNX_ML_REGEX.match(line):\n            assert 1 == in_if\n            in_if = 2\n        elif ENDIF_ONNX_ML_REGEX.match(line):\n            assert (1 == in_if or 2 == in_if)\n            in_if = 0\n        else:\n            if 0 == in_if:\n                yield line\n            elif (1 == in_if and onnx_ml):\n                yield line\n            elif (2 == in_if and not onnx_ml):\n                yield line",
  "def process_package_name(lines: Iterable[Text], package_name: Text) -> Iterable[Text]:\n    need_rename = (package_name != DEFAULT_PACKAGE_NAME)\n    for line in lines:\n        m = IMPORT_REGEX.match(line) if need_rename else None\n        if m:\n            include_name = m.group(2)\n            ml = ML_REGEX.match(include_name)\n            if ml:\n                include_name = \"{}_{}-ml\".format(ml.group(1), package_name)\n            else:\n                include_name = \"{}_{}\".format(include_name, package_name)\n            yield m.group(1) + 'import \"{}.proto\";'.format(include_name)\n        else:\n            yield PACKAGE_NAME_REGEX.sub(package_name, line)",
  "def convert_to_proto3(lines: Iterable[Text]) -> Iterable[Text]:\n    for line in lines:\n        # Set the syntax specifier\n        m = PROTO_SYNTAX_REGEX.match(line)\n        if m:\n            yield m.group(1) + 'syntax = \"proto3\";'\n            continue\n\n        # Remove optional keywords\n        m = OPTIONAL_REGEX.match(line)\n        if m:\n            yield m.group(1) + m.group(2)\n            continue\n\n        # Rewrite import\n        m = IMPORT_REGEX.match(line)\n        if m:\n            yield m.group(1) + 'import \"{}.proto3\";'.format(m.group(2))\n            continue\n\n        yield line",
  "def gen_proto3_code(protoc_path: Text, proto3_path: Text, include_path: Text, cpp_out: Text, python_out: Text) -> None:\n    print(\"Generate pb3 code using {}\".format(protoc_path))\n    build_args = [protoc_path, proto3_path, '-I', include_path]\n    build_args.extend(['--cpp_out', cpp_out, '--python_out', python_out])\n    subprocess.check_call(build_args)",
  "def translate(source: Text, proto: int, onnx_ml: bool, package_name: Text) -> Text:\n    lines: Iterable[Text] = source.splitlines()\n    lines = process_ifs(lines, onnx_ml=onnx_ml)\n    lines = process_package_name(lines, package_name=package_name)\n    if proto == 3:\n        lines = convert_to_proto3(lines)\n    else:\n        assert proto == 2\n    return \"\\n\".join(lines)",
  "def qualify(f: Text, pardir: Text = os.path.realpath(os.path.dirname(__file__))) -> Text:\n    return os.path.join(pardir, f)",
  "def convert(stem: Text, package_name: Text, output: Text, do_onnx_ml: bool = False, lite: bool = False, protoc_path: Text = '') -> None:\n    proto_in = qualify(\"{}.in.proto\".format(stem))\n    need_rename = (package_name != DEFAULT_PACKAGE_NAME)\n    # Having a separate variable for import_ml ensures that the import statements for the generated\n    # proto files can be set separately from the ONNX_ML environment variable setting.\n    import_ml = do_onnx_ml\n    # We do not want to generate the onnx-data-ml.proto files for onnx-data.in.proto,\n    # as there is no change between onnx-data.proto and the ML version.\n    if 'onnx-data' in proto_in:\n        do_onnx_ml = False\n    if do_onnx_ml:\n        proto_base = \"{}_{}-ml\".format(stem, package_name) if need_rename else \"{}-ml\".format(stem)\n    else:\n        proto_base = \"{}_{}\".format(stem, package_name) if need_rename else \"{}\".format(stem)\n    proto = qualify(\"{}.proto\".format(proto_base), pardir=output)\n    proto3 = qualify(\"{}.proto3\".format(proto_base), pardir=output)\n\n    print(\"Processing {}\".format(proto_in))\n    with io.open(proto_in, 'r') as fin:\n        source = fin.read()\n        print(\"Writing {}\".format(proto))\n        with io.open(proto, 'w', newline='') as fout:\n            fout.write(autogen_header)\n            fout.write(translate(source, proto=2, onnx_ml=import_ml, package_name=package_name))\n            if lite:\n                fout.write(LITE_OPTION)\n        print(\"Writing {}\".format(proto3))\n        with io.open(proto3, 'w', newline='') as fout:\n            fout.write(autogen_header)\n            fout.write(translate(source, proto=3, onnx_ml=import_ml, package_name=package_name))\n            if lite:\n                fout.write(LITE_OPTION)\n        if protoc_path:\n            porto3_dir = os.path.dirname(proto3)\n            base_dir = os.path.dirname(porto3_dir)\n            gen_proto3_code(protoc_path, proto3, base_dir, base_dir, base_dir)\n            pb3_files = glob.glob(os.path.join(porto3_dir, '*.proto3.*'))\n            for pb3_file in pb3_files:\n                print(\"Removing {}\".format(pb3_file))\n                os.remove(pb3_file)\n\n        if need_rename:\n            if do_onnx_ml:\n                proto_header = qualify(\"{}-ml.pb.h\".format(stem), pardir=output)\n            else:\n                proto_header = qualify(\"{}.pb.h\".format(stem), pardir=output)\n            print(\"Writing {}\".format(proto_header))\n            with io.open(proto_header, 'w', newline='') as fout:\n                fout.write(\"#pragma once\\n\")\n                fout.write(\"#include \\\"{}.pb.h\\\"\\n\".format(proto_base))\n\n    # Generate py mapping\n    # \"-\" is invalid in python module name, replaces '-' with '_'\n    pb_py = qualify('{}_pb.py'.format(stem.replace('-', '_')), pardir=output)\n    if need_rename:\n        pb2_py = qualify('{}_pb2.py'.format(proto_base.replace('-', '_')), pardir=output)\n    else:\n        if do_onnx_ml:\n            pb2_py = qualify('{}_ml_pb2.py'.format(stem.replace('-', '_')), pardir=output)\n        else:\n            pb2_py = qualify('{}_pb2.py'.format(stem.replace('-', '_')), pardir=output)\n\n    print('generating {}'.format(pb_py))\n    with open(pb_py, 'w') as f:\n        f.write(str(dedent('''\\\n        # This file is generated by setup.py. DO NOT EDIT!\n\n\n        from .{} import *  # noqa\n        '''.format(os.path.splitext(os.path.basename(pb2_py))[0]))))",
  "def main() -> None:\n    parser = argparse.ArgumentParser(\n        description='Generates .proto file variations from .in.proto')\n    parser.add_argument('-p', '--package', default='onnx',\n                        help='package name in the generated proto files'\n                        ' (default: %(default)s)')\n    parser.add_argument('-m', '--ml', action='store_true', help='ML mode')\n    parser.add_argument('-l', '--lite', action='store_true',\n                        help='generate lite proto to use with protobuf-lite')\n    parser.add_argument('-o', '--output',\n                        default=os.path.realpath(os.path.dirname(__file__)),\n                        help='output directory (default: %(default)s)')\n    parser.add_argument('--protoc_path',\n                        default='',\n                        help='path to protoc for proto3 file validation')\n    parser.add_argument('stems', nargs='*', default=['onnx', 'onnx-operators', 'onnx-data'],\n                        help='list of .in.proto file stems '\n                        '(default: %(default)s)')\n    args = parser.parse_args()\n\n    if not os.path.exists(args.output):\n        os.makedirs(args.output)\n\n    for stem in args.stems:\n        convert(stem,\n                package_name=args.package,\n                output=args.output,\n                do_onnx_ml=args.ml,\n                lite=args.lite,\n                protoc_path=args.protoc_path)",
  "class ModelInfo(object):\n    \"\"\"\n    A class to represent a model's property and metadata in the ONNX Hub.\n    It extracts model name, path, sha, tags, etc. from the passed in raw_model_info dict.\n\n    Attributes:\n        model: The name of the model.\n        model_path: The path to the model, relative to the model zoo (https://github.com/onnx/models/) repo root.\n        metadata: Additional metadata of the model, such as the size of the model, IO ports, etc.\n        model_sha: The SHA256 digest of the model file.\n        tags: A set of tags associated with the model.\n        opset: The opset version of the model.\n    \"\"\"\n\n    def __init__(self, raw_model_info: Dict[str, Any]) -> None:\n        \"\"\"\n        Parameters:\n            raw_model_info: A JSON dict containing the model info.\n        \"\"\"\n        self.model = cast(str, raw_model_info[\"model\"])\n\n        self.model_path = cast(str, raw_model_info[\"model_path\"])\n        self.metadata: Dict[str, Any] = cast(Dict[str, Any], raw_model_info[\"metadata\"])\n        self.model_sha: Optional[str] = None\n        if \"model_sha\" in self.metadata:\n            self.model_sha = cast(str, self.metadata[\"model_sha\"])\n\n        self.tags: Set[str] = set()\n        if \"tags\" in self.metadata:\n            self.tags = set(cast(List[str], self.metadata[\"tags\"]))\n\n        self.opset = cast(int, raw_model_info[\"opset_version\"])\n        self.raw_model_info: Dict[str, Any] = raw_model_info\n\n    def __str__(self) -> str:\n        return \"ModelInfo(model={}, opset={}, path={}, metadata={})\".format(\n            self.model, self.opset, self.model_path, self.metadata\n        )\n\n    def __repr__(self) -> str:\n        return self.__str__()",
  "def set_dir(new_dir: str) -> None:\n    \"\"\"\n    Sets the current ONNX hub cache location\n\n    :param new_dir: location of new model hub cache\n    \"\"\"\n    global _ONNX_HUB_DIR\n    _ONNX_HUB_DIR = new_dir",
  "def get_dir() -> str:\n    \"\"\"\n    Gets the current ONNX hub cache location\n\n    :return: The location of the ONNX hub model cache\n    \"\"\"\n    return _ONNX_HUB_DIR",
  "def _parse_repo_info(repo: str) -> Tuple[str, str, str]:\n    \"\"\"\n    Gets the repo owner, name and ref from a repo specification string.\n    \"\"\"\n    repo_owner = repo.split(\"/\")[0]\n    repo_name = repo.split(\"/\")[1].split(\":\")[0]\n    if \":\" in repo:\n        repo_ref = repo.split(\"/\")[1].split(\":\")[1]\n    else:\n        repo_ref = \"main\"\n    return repo_owner, repo_name, repo_ref",
  "def _verify_repo_ref(repo: str) -> bool:\n    \"\"\"\n    Verifies whether the given model repo can be trusted.\n    A model repo can be trusted if it matches onnx/models:main.\n    \"\"\"\n    repo_owner, repo_name, repo_ref = _parse_repo_info(repo)\n    return (repo_owner == \"onnx\") and (repo_name == \"models\") and (repo_ref == \"main\")",
  "def _get_base_url(repo: str, lfs: bool = False) -> str:\n    \"\"\"\n    Gets the base github url from a repo specification string\n\n    :param repo: The location of the model repo in format \"user/repo[:branch]\".\n        If no branch is found will default to \"main\"\n    :param lfs: whether the url is for downloading lfs models\n    :return: the base github url for downloading\n    \"\"\"\n    repo_owner, repo_name, repo_ref = _parse_repo_info(repo)\n\n    if lfs:\n        return \"https://media.githubusercontent.com/media/{}/{}/{}/\".format(repo_owner, repo_name, repo_ref)\n    else:\n        return \"https://raw.githubusercontent.com/{}/{}/{}/\".format(repo_owner, repo_name, repo_ref)",
  "def _download_file(url: str, file_name: str) -> None:\n    \"\"\"\n    Downloads the file with specifed file_name from the url\n\n    :param url: a url of download link\n    :param file_name: a specified file name for the downloaded file\n    \"\"\"\n    chunk_size = 16384  # 1024 * 16\n    with urlopen(url) as response, open(file_name, 'wb') as f:\n        # Loads processively with chuck_size for huge models\n        while True:\n            chunk = response.read(chunk_size)\n            if not chunk:\n                break\n            f.write(chunk)",
  "def list_models(\n    repo: str = \"onnx/models:main\", model: Optional[str] = None, tags: Optional[List[str]] = None\n) -> List[ModelInfo]:\n    \"\"\"\n    Gets the list of model info consistent with a given name and tags\n\n    :param repo: The location of the model repo in format \"user/repo[:branch]\".\n        If no branch is found will default to \"main\"\n    :param model: The name of the model to search for. If `None`, will return all models with matching tags.\n    :param tags: A list of tags to filter models by. If `None`, will return all models with matching name.\n    :return: list of ModelInfo\n    \"\"\"\n    base_url = _get_base_url(repo)\n    manifest_url = base_url + \"ONNX_HUB_MANIFEST.json\"\n    try:\n        with urlopen(manifest_url) as response:\n            manifest: List[ModelInfo] = [ModelInfo(info) for info in json.load(cast(IO[str], response))]\n    except HTTPError as e:\n        raise AssertionError(\"Could not find manifest at {}\".format(manifest_url), e)\n\n    # Filter by model name first.\n    matching_models = manifest if model is None else [m for m in manifest if m.model.lower() == model.lower()]\n\n    # Filter by tags\n    if tags is None:\n        return matching_models\n    else:\n        canonical_tags = {t.lower() for t in tags}\n        matching_info_list: List[ModelInfo] = []\n        for m in matching_models:\n            model_tags = {t.lower() for t in m.tags}\n            if len(canonical_tags.intersection(model_tags)) > 0:\n                matching_info_list.append(m)\n        return matching_info_list",
  "def get_model_info(model: str, repo: str = \"onnx/models:main\", opset: Optional[int] = None) -> ModelInfo:\n    \"\"\"\n    Gets the model info matching the given name and opset.\n\n    :param model: The name of the onnx model in the manifest. This field is case-sensitive\n    :param repo: The location of the model repo in format \"user/repo[:branch]\".\n        If no branch is found will default to \"main\"\n    :param opset: The opset of the model to get. The default of `None` will return the model with largest opset.\n    :return: ModelInfo\n    \"\"\"\n    matching_models = list_models(repo, model)\n    if not matching_models:\n        raise AssertionError(\"No models found with name {}\".format(model))\n\n    if opset is None:\n        selected_models = sorted(matching_models, key=lambda m: -m.opset)\n    else:\n        selected_models = [m for m in matching_models if m.opset == opset]\n        if len(selected_models) == 0:\n            valid_opsets = [m.opset for m in matching_models]\n            raise AssertionError(\"{} has no version with opset {}. Valid opsets: {}\".format(model, opset, valid_opsets))\n    return selected_models[0]",
  "def load(\n    model: str,\n    repo: str = \"onnx/models:main\",\n    opset: Optional[int] = None,\n    force_reload: bool = False,\n    silent: bool = False,\n) -> Optional[onnx.ModelProto]:\n    \"\"\"\n    Downloads a model by name from the onnx model hub\n\n    :param model: The name of the onnx model in the manifest. This field is case-sensitive\n    :param repo: The location of the model repo in format \"user/repo[:branch]\".\n        If no branch is found will default to \"main\"\n    :param opset: The opset of the model to download. The default of `None` automatically chooses the largest opset\n    :param force_reload: Whether to force the model to re-download even if its already found in the cache\n    :param silent: Whether to suppress the warning message if the repo is not trusted.\n    :return: ModelProto or None\n    \"\"\"\n    selected_model = get_model_info(model, repo, opset)\n    local_model_path_arr = selected_model.model_path.split(\"/\")\n    if selected_model.model_sha is not None:\n        local_model_path_arr[-1] = \"{}_{}\".format(selected_model.model_sha, local_model_path_arr[-1])\n    local_model_path = join(_ONNX_HUB_DIR, os.sep.join(local_model_path_arr))\n\n    if force_reload or not os.path.exists(local_model_path):\n        if not _verify_repo_ref(repo) and not silent:\n            msg = (\n                'The model repo specification \"{}\" is not trusted and may'\n                \" contain security vulnerabilities. Only continue if you trust this repo.\"\n            ).format(repo)\n\n            print(msg, file=sys.stderr)\n            print(\"Continue?[y/n]\")\n            if input().lower() != \"y\":\n                return None\n\n        os.makedirs(os.path.dirname(local_model_path), exist_ok=True)\n        lfs_url = _get_base_url(repo, True)\n        print(\"Downloading {} to local path {}\".format(model, local_model_path))\n        _download_file(lfs_url + selected_model.model_path, local_model_path)\n    else:\n        print(\"Using cached {} model from {}\".format(model, local_model_path))\n\n    with open(local_model_path, \"rb\") as f:\n        model_bytes = f.read()\n\n    if selected_model.model_sha is not None:\n        downloaded_sha = hashlib.sha256(model_bytes).hexdigest()\n        if not downloaded_sha == selected_model.model_sha:\n            raise AssertionError(\n                (\n                    \"The cached model has SHA256 {} while checksum should be {}. \"\n                    + \"The model in the hub may have been updated. Use force_reload to download the model from the model hub.\"\n                ).format(downloaded_sha, selected_model.model_sha)\n            )\n\n    return onnx.load(cast(IO[bytes], BytesIO(model_bytes)))",
  "def __init__(self, raw_model_info: Dict[str, Any]) -> None:\n        \"\"\"\n        Parameters:\n            raw_model_info: A JSON dict containing the model info.\n        \"\"\"\n        self.model = cast(str, raw_model_info[\"model\"])\n\n        self.model_path = cast(str, raw_model_info[\"model_path\"])\n        self.metadata: Dict[str, Any] = cast(Dict[str, Any], raw_model_info[\"metadata\"])\n        self.model_sha: Optional[str] = None\n        if \"model_sha\" in self.metadata:\n            self.model_sha = cast(str, self.metadata[\"model_sha\"])\n\n        self.tags: Set[str] = set()\n        if \"tags\" in self.metadata:\n            self.tags = set(cast(List[str], self.metadata[\"tags\"]))\n\n        self.opset = cast(int, raw_model_info[\"opset_version\"])\n        self.raw_model_info: Dict[str, Any] = raw_model_info",
  "def __str__(self) -> str:\n        return \"ModelInfo(model={}, opset={}, path={}, metadata={})\".format(\n            self.model, self.opset, self.model_path, self.metadata\n        )",
  "def __repr__(self) -> str:\n        return self.__str__()",
  "def combine_pairs_to_complex(fa: Sequence[int]) -> Sequence[np.complex64]:\n    return [complex(fa[i * 2], fa[i * 2 + 1]) for i in range(len(fa) // 2)]",
  "def to_array(tensor: TensorProto, base_dir: Text = \"\") -> np.ndarray:\n    \"\"\"Converts a tensor def object to a numpy array.\n\n    Inputs:\n        tensor: a TensorProto object.\n        base_dir: if external tensor exists, base_dir can help to find the path to it\n    Returns:\n        arr: the converted array.\n    \"\"\"\n    if tensor.HasField(\"segment\"):\n        raise ValueError(\n            \"Currently not supporting loading segments.\")\n    if tensor.data_type == TensorProto.UNDEFINED:\n        raise TypeError(\"The element type in the input tensor is not defined.\")\n\n    tensor_dtype = tensor.data_type\n    np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[tensor_dtype]\n    storage_type = mapping.TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE[tensor_dtype]\n    storage_np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[storage_type]\n    storage_field = mapping.STORAGE_TENSOR_TYPE_TO_FIELD[storage_type]\n    dims = tensor.dims\n\n    if tensor.data_type == TensorProto.STRING:\n        utf8_strings = getattr(tensor, storage_field)\n        ss = list(s.decode('utf-8') for s in utf8_strings)\n        return np.asarray(ss).astype(np_dtype).reshape(dims)\n\n    # Load raw data from external tensor if it exists\n    if uses_external_data(tensor):\n        load_external_data_for_tensor(tensor, base_dir)\n\n    if tensor.HasField(\"raw_data\"):\n        # Raw_bytes support: using frombuffer.\n        if sys.byteorder == 'big':\n            # Convert endian from little to big\n            convert_endian(tensor)\n        return np.frombuffer(\n            tensor.raw_data,\n            dtype=np_dtype).reshape(dims)\n    else:\n        # float16/bfloat16 is stored as int32 (uint16 type); Need view to get the original value\n        if (tensor_dtype == TensorProto.FLOAT16\n                or tensor_dtype == TensorProto.BFLOAT16):\n            return (\n                np.asarray(\n                    tensor.int32_data,\n                    dtype=np.uint16)\n                .reshape(dims)\n                .view(np.float16))\n        data = getattr(tensor, storage_field)\n        if (tensor_dtype == TensorProto.COMPLEX64\n                or tensor_dtype == TensorProto.COMPLEX128):\n            data = combine_pairs_to_complex(data)\n\n        return (\n            np.asarray(\n                data,\n                dtype=storage_np_dtype)\n            .astype(np_dtype)\n            .reshape(dims)\n        )",
  "def from_array(arr: np.ndarray, name: Optional[Text] = None) -> TensorProto:\n    \"\"\"Converts a numpy array to a tensor def.\n\n    Inputs:\n        arr: a numpy array.\n        name: (optional) the name of the tensor.\n    Returns:\n        TensorProto: the converted tensor def.\n    \"\"\"\n    tensor = TensorProto()\n    tensor.dims.extend(arr.shape)\n    if name:\n        tensor.name = name\n\n    if arr.dtype == object:\n        # Special care for strings.\n        tensor.data_type = mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype]\n        # TODO: Introduce full string support.\n        # We flatten the array in case there are 2-D arrays are specified\n        # We throw the error below if we have a 3-D array or some kind of other\n        # object. If you want more complex shapes then follow the below instructions.\n        # Unlike other types where the shape is automatically inferred from\n        # nested arrays of values, the only reliable way now to feed strings\n        # is to put them into a flat array then specify type astype(object)\n        # (otherwise all strings may have different types depending on their length)\n        # and then specify shape .reshape([x, y, z])\n        flat_array = arr.flatten()\n        for e in flat_array:\n            if isinstance(e, str):\n                tensor.string_data.append(e.encode('utf-8'))\n            elif isinstance(e, np.ndarray):\n                for s in e:\n                    if isinstance(s, str):\n                        tensor.string_data.append(s.encode('utf-8'))\n                    elif isinstance(s, bytes):\n                        tensor.string_data.append(s)\n            elif isinstance(e, bytes):\n                tensor.string_data.append(e)\n            else:\n                raise NotImplementedError(\n                    \"Unrecognized object in the object array, expect a string, or array of bytes: \", str(type(e)))\n        return tensor\n\n    # For numerical types, directly use numpy raw bytes.\n    try:\n        dtype = mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype]\n    except KeyError:\n        raise RuntimeError(\n            \"Numpy data type not understood yet: {}\".format(str(arr.dtype)))\n    tensor.data_type = dtype\n    tensor.raw_data = arr.tobytes()  # note: tobytes() is only after 1.9.\n    if sys.byteorder == 'big':\n        # Convert endian from big to little\n        convert_endian(tensor)\n\n    return tensor",
  "def to_list(sequence: SequenceProto) -> List[Any]:\n    \"\"\"Converts a sequence def to a Python list.\n\n    Inputs:\n        sequence: a SequenceProto object.\n    Returns:\n        list: the converted list.\n    \"\"\"\n    lst: List[Any] = []\n    elem_type = sequence.elem_type\n    value_field = mapping.STORAGE_ELEMENT_TYPE_TO_FIELD[elem_type]\n    values = getattr(sequence, value_field)\n    for value in values:\n        if elem_type == SequenceProto.TENSOR or elem_type == SequenceProto.SPARSE_TENSOR:\n            lst.append(to_array(value))\n        elif elem_type == SequenceProto.SEQUENCE:\n            lst.append(to_list(value))\n        elif elem_type == SequenceProto.MAP:\n            lst.append(to_dict(value))\n        else:\n            raise TypeError(\"The element type in the input sequence is not supported.\")\n    return lst",
  "def from_list(lst: List[Any], name: Optional[Text] = None, dtype: Optional[int] = None) -> SequenceProto:\n    \"\"\"Converts a list into a sequence def.\n\n    Inputs:\n        lst: a Python list\n        name: (optional) the name of the sequence.\n        dtype: (optional) type of element in the input list, used for specifying\n                          sequence values when converting an empty list.\n    Returns:\n        SequenceProto: the converted sequence def.\n    \"\"\"\n    sequence = SequenceProto()\n    if name:\n        sequence.name = name\n\n    if dtype:\n        elem_type = dtype\n    elif len(lst) > 0:\n        first_elem = lst[0]\n        if isinstance(first_elem, dict):\n            elem_type = SequenceProto.MAP\n        elif isinstance(first_elem, list):\n            elem_type = SequenceProto.SEQUENCE\n        else:\n            elem_type = SequenceProto.TENSOR\n    else:\n        # if empty input list and no dtype specified\n        # choose sequence of tensors on default\n        elem_type = SequenceProto.TENSOR\n    sequence.elem_type = elem_type\n\n    if (len(lst) > 0) and not all(isinstance(elem, type(lst[0])) for elem in lst):\n        raise TypeError(\"The element type in the input list is not the same \"\n                        \"for all elements and therefore is not supported as a sequence.\")\n\n    if elem_type == SequenceProto.TENSOR:\n        for tensor in lst:\n            sequence.tensor_values.extend([from_array(tensor)])\n    elif elem_type == SequenceProto.SEQUENCE:\n        for seq in lst:\n            sequence.sequence_values.extend([from_list(seq)])\n    elif elem_type == SequenceProto.MAP:\n        for map in lst:\n            sequence.map_values.extend([from_dict(map)])\n    else:\n        raise TypeError(\"The element type in the input list is not a tensor, \"\n                        \"sequence, or map and is not supported.\")\n    return sequence",
  "def to_dict(map: MapProto) -> Dict[Any, Any]:\n    \"\"\"Converts a map def to a Python dictionary.\n\n    Inputs:\n        map: a MapProto object.\n    Returns:\n        dict: the converted dictionary.\n    \"\"\"\n    key_list: List[Any] = []\n    if map.key_type == TensorProto.STRING:\n        key_list = list(map.string_keys)\n    else:\n        key_list = list(map.keys)\n\n    value_list = to_list(map.values)\n    if len(key_list) != len(value_list):\n        raise IndexError(\"Length of keys and values for MapProto (map name: \",\n                        map.name,\n                        \") are not the same.\")\n    dictionary = dict(zip(key_list, value_list))\n    return dictionary",
  "def from_dict(dict: Dict[Any, Any], name: Optional[Text] = None) -> MapProto:\n    \"\"\"Converts a Python dictionary into a map def.\n\n    Inputs:\n        dict: Python dictionary\n        name: (optional) the name of the map.\n    Returns:\n        MapProto: the converted map def.\n    \"\"\"\n    map = MapProto()\n    if name:\n        map.name = name\n    keys = list(dict.keys())\n    raw_key_type = np.array(keys[0]).dtype\n    key_type = mapping.NP_TYPE_TO_TENSOR_TYPE[raw_key_type]\n\n    valid_key_int_types = [TensorProto.INT8, TensorProto.INT16, TensorProto.INT32,\n                           TensorProto.INT64, TensorProto.UINT8, TensorProto.UINT16,\n                           TensorProto.UINT32, TensorProto.UINT64]\n\n    if not all(isinstance(key, raw_key_type) for key in keys):\n        raise TypeError(\"The key type in the input dictionary is not the same \"\n                        \"for all keys and therefore is not valid as a map.\")\n\n    values = list(dict.values())\n    raw_value_type = type(values[0])\n    if not all(isinstance(val, raw_value_type) for val in values):\n        raise TypeError(\"The value type in the input dictionary is not the same \"\n                        \"for all values and therefore is not valid as a map.\")\n\n    value_seq = from_list(values)\n\n    map.key_type = key_type\n    if key_type == TensorProto.STRING:\n        map.string_keys.extend(keys)\n    elif key_type in valid_key_int_types:\n        map.keys.extend(keys)\n    map.values.CopyFrom(value_seq)\n    return map",
  "def to_optional(optional: OptionalProto) -> Optional[Any]:\n    \"\"\"Converts an optional def to a Python optional.\n\n    Inputs:\n        optional: an OptionalProto object.\n    Returns:\n        opt: the converted optional.\n    \"\"\"\n    opt: Optional[Any] = None\n    elem_type = optional.elem_type\n    if elem_type == OptionalProto.UNDEFINED:\n        return opt\n    value_field = mapping.OPTIONAL_ELEMENT_TYPE_TO_FIELD[elem_type]\n    value = getattr(optional, value_field)\n    # TODO: create a map and replace conditional branches\n    if elem_type == OptionalProto.TENSOR or elem_type == OptionalProto.SPARSE_TENSOR:\n        opt = to_array(value)\n    elif elem_type == OptionalProto.SEQUENCE:\n        opt = to_list(value)\n    elif elem_type == OptionalProto.MAP:\n        opt = to_dict(value)\n    elif elem_type == OptionalProto.OPTIONAL:\n        return to_optional(value)\n    else:\n        raise TypeError(\"The element type in the input optional is not supported.\")\n    return opt",
  "def from_optional(\n        opt: Optional[Any],\n        name: Optional[Text] = None,\n        dtype: Optional[int] = None\n) -> OptionalProto:\n    \"\"\"Converts an optional value into a Optional def.\n\n    Inputs:\n        opt: a Python optional\n        name: (optional) the name of the optional.\n        dtype: (optional) type of element in the input, used for specifying\n                          optional values when converting empty none. dtype must\n                          be a valid OptionalProto.DataType value\n    Returns:\n        optional: the converted optional def.\n    \"\"\"\n    # TODO: create a map and replace conditional branches\n    optional = OptionalProto()\n    if name:\n        optional.name = name\n\n    if dtype:\n        # dtype must be a valid OptionalProto.DataType\n        valid_dtypes = [v for v in OptionalProto.DataType.values()]\n        assert dtype in valid_dtypes\n        elem_type = dtype\n    elif isinstance(opt, dict):\n        elem_type = OptionalProto.MAP\n    elif isinstance(opt, list):\n        elem_type = OptionalProto.SEQUENCE\n    elif opt is None:\n        elem_type = OptionalProto.UNDEFINED\n    else:\n        elem_type = OptionalProto.TENSOR\n\n    optional.elem_type = elem_type\n\n    if opt is not None:\n        if elem_type == OptionalProto.TENSOR:\n            optional.tensor_value.CopyFrom(from_array(opt))\n        elif elem_type == OptionalProto.SEQUENCE:\n            optional.sequence_value.CopyFrom(from_list(opt))\n        elif elem_type == OptionalProto.MAP:\n            optional.map_value.CopyFrom(from_dict(opt))\n        else:\n            raise TypeError(\"The element type in the input is not a tensor, \"\n                            \"sequence, or map and is not supported.\")\n    return optional",
  "def convert_endian(tensor: TensorProto) -> None:\n    \"\"\"\n    Call to convert endianess of raw data in tensor.\n\n    Arguments:\n        tensor (TensorProto): TensorProto to be converted.\n    \"\"\"\n    tensor_dtype = tensor.data_type\n    np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[tensor_dtype]\n    tensor.raw_data = np.frombuffer(tensor.raw_data, dtype=np_dtype).byteswap().tobytes()",
  "class ParseError(Exception):\n    pass",
  "def parse_model(model_text: Text) -> onnx.ModelProto:\n    \"\"\"Parse a string to build a ModelProto.\n\n    Arguments:\n        model_text (string): formatted string\n    Returns:\n        ModelProto\n    \"\"\"\n    (success, msg, model_proto_str) = C.parse_model(model_text)\n    if success:\n        return onnx.load_from_string(model_proto_str)\n    else:\n        raise ParseError(msg)",
  "def parse_graph(graph_text: Text) -> onnx.GraphProto:\n    \"\"\"Parse a string to build a GraphProto.\n\n    Arguments:\n        graph_text (string): formatted string\n    Returns:\n        GraphProto\n    \"\"\"\n    (success, msg, graph_proto_str) = C.parse_graph(graph_text)\n    if success:\n        G = onnx.GraphProto()\n        G.ParseFromString(graph_proto_str)\n        return G\n    else:\n        raise ParseError(msg)",
  "def infer_shapes(model: Union[ModelProto, bytes], check_type: bool = False, strict_mode: bool = False, data_prop: bool = False) -> ModelProto:\n    \"\"\"Apply shape inference to the provided ModelProto.\n\n    Inferred shapes are added to the value_info field of the graph.\n\n    If the inferred values conflict with values already provided in the\n    graph, that means that the provided values are invalid (or there is a\n    bug in shape inference), and the result is unspecified.\n\n    Arguments:\n        model (Union[ModelProto, Text, bytes], bool, bool, bool) -> ModelProto\n        check_type (bool): Checks the type-equality for input and output\n        strict_mode (bool): Stricter shape inference, it will throw errors if any;\n            Otherwise, simply stop if any error\n        data_prop (bool): Enables data propagation for limited operators to perform shape computation\n\n    Returns:\n        (ModelProto) model with inferred shape information\n    \"\"\"\n    if isinstance(model, (ModelProto, bytes)):\n        model_str = model if isinstance(model, bytes) else model.SerializeToString()\n        inferred_model_str = C.infer_shapes(model_str, check_type, strict_mode, data_prop)\n        return onnx.load_from_string(inferred_model_str)\n    elif isinstance(model, str):\n        raise TypeError('infer_shapes only accepts ModelProto or bytes,'\n                        'you can use infer_shapes_path for the model path (String).')\n    else:\n        raise TypeError('infer_shapes only accepts ModelProto or bytes, '\n                         'incorrect type: {}'.format(type(model)))",
  "def infer_shapes_path(model_path: Text, output_path: Text = '', check_type: bool = False, strict_mode: bool = False, data_prop: bool = False) -> None:\n    \"\"\"\n    Take model path for shape_inference same as infer_shape; it support >2GB models\n    Directly output the inferred model to the output_path; Default is the original model path\n    \"\"\"\n    if isinstance(model_path, ModelProto):\n        raise TypeError('infer_shapes_path only accepts model Path (String),'\n                        'you can use infer_shapes for the ModelProto.')\n    # Directly output the inferred model into the specified path, return nothing\n    elif isinstance(model_path, str):\n        # If output_path is not defined, default output_path would be the original model path\n        if output_path == '':\n            output_path = model_path\n        C.infer_shapes_path(model_path, output_path, check_type, strict_mode, data_prop)\n    else:\n        raise TypeError('infer_shapes_path only accepts model path (String), '\n                         'incorrect type: {}'.format(type(model_path)))",
  "def create_op_set_id_version_map(table: VersionTableType) -> VersionMapType:\n    \"\"\"create a map from (opset-domain, opset-version) to ir-version from above table\"\"\"\n    result: VersionMapType = dict()\n\n    def process(release_version: Text, ir_version: int, *args: Any) -> None:\n        for pair in zip(['ai.onnx', 'ai.onnx.ml', 'ai.onnx.training'], args):\n            if (pair not in result):\n                result[pair] = ir_version\n    for row in table:\n        process(*row)\n    return result",
  "def find_min_ir_version_for(opsetidlist: List[OperatorSetIdProto]) -> int:\n    \"\"\"Given list of opset ids, determine minimum IR version required\"\"\"\n    default_min_version = 3\n\n    def find_min(domain: Union[Text, None], version: int) -> int:\n        key = (domain if domain else 'ai.onnx', version)\n        if (key in OP_SET_ID_VERSION_MAP):\n            return OP_SET_ID_VERSION_MAP[key]\n        else:\n            raise ValueError(\"Unsupported opset-version.\")\n    if (opsetidlist):\n        return max([find_min(x.domain, x.version) for x in opsetidlist])\n    return default_min_version",
  "def make_node(\n        op_type: Text,\n        inputs: Sequence[Text],\n        outputs: Sequence[Text],\n        name: Optional[Text] = None,\n        doc_string: Optional[Text] = None,\n        domain: Optional[Text] = None,\n        **kwargs: Any\n) -> NodeProto:\n    \"\"\"Construct a NodeProto.\n\n    Arguments:\n        op_type (string): The name of the operator to construct\n        inputs (list of string): list of input names\n        outputs (list of string): list of output names\n        name (string, default None): optional unique identifier for NodeProto\n        doc_string (string, default None): optional documentation string for NodeProto\n        domain (string, default None): optional domain for NodeProto.\n            If it's None, we will just use default domain (which is empty)\n        **kwargs (dict): the attributes of the node.  The acceptable values\n            are documented in :func:`make_attribute`.\n    Returns:\n        NodeProto\n    \"\"\"\n\n    node = NodeProto()\n    node.op_type = op_type\n    node.input.extend(inputs)\n    node.output.extend(outputs)\n    if name:\n        node.name = name\n    if doc_string:\n        node.doc_string = doc_string\n    if domain is not None:\n        node.domain = domain\n    if kwargs:\n        node.attribute.extend(\n            make_attribute(key, value)\n            for key, value in sorted(kwargs.items())\n            if value is not None)\n    return node",
  "def make_operatorsetid(\n        domain: Text,\n        version: int,\n) -> OperatorSetIdProto:\n    \"\"\"Construct an OperatorSetIdProto.\n\n    Arguments:\n        domain (string): The domain of the operator set id\n        version (integer): Version of operator set id\n    Returns:\n        OperatorSetIdProto\n    \"\"\"\n    operatorsetid = OperatorSetIdProto()\n    operatorsetid.domain = domain\n    operatorsetid.version = version\n    return operatorsetid",
  "def make_graph(\n    nodes: Sequence[NodeProto],\n    name: Text,\n    inputs: Sequence[ValueInfoProto],\n    outputs: Sequence[ValueInfoProto],\n    initializer: Optional[Sequence[TensorProto]] = None,\n    doc_string: Optional[Text] = None,\n    value_info: Sequence[ValueInfoProto] = [],\n    sparse_initializer: Optional[Sequence[SparseTensorProto]] = None,\n) -> GraphProto:\n    \"\"\"Construct a GraphProto\n\n    Arguments:\n        nodes: list of NodeProto\n        name (string): graph name\n        inputs: list of ValueInfoProto\n        outputs: list of ValueInfoProto\n        initializer: list of TensorProto\n        doc_string (string): graph documentation\n        value_info: list of ValueInfoProto\n        sparse_initializer: list of SparseTensorProto\n    Returns:\n        GraphProto\n    \"\"\"\n    if initializer is None:\n        initializer = []\n    if sparse_initializer is None:\n        sparse_initializer = []\n    if value_info is None:\n        value_info = []\n    graph = GraphProto()\n    graph.node.extend(nodes)\n    graph.name = name\n    graph.input.extend(inputs)\n    graph.output.extend(outputs)\n    graph.initializer.extend(initializer)\n    graph.sparse_initializer.extend(sparse_initializer)\n    graph.value_info.extend(value_info)\n    if doc_string:\n        graph.doc_string = doc_string\n    return graph",
  "def make_opsetid(domain: Text, version: int) -> OperatorSetIdProto:\n    \"\"\"Construct an OperatorSetIdProto.\n\n    Arguments:\n        domain (string): The domain of the operator set id\n        version (integer): Version of operator set id\n    Returns:\n        OperatorSetIdProto\n    \"\"\"\n    opsetid = OperatorSetIdProto()\n    opsetid.domain = domain\n    opsetid.version = version\n    return opsetid",
  "def make_function(\n    domain: Text,\n    fname: Text,\n    inputs: Sequence[Text],\n    outputs: Sequence[Text],\n    nodes: Sequence[NodeProto],\n    opset_imports: Sequence[OperatorSetIdProto],\n    attributes: Optional[Sequence[Text]] = [],\n    doc_string: Optional[Text] = None\n) -> FunctionProto:\n    f = FunctionProto()\n    f.domain = domain\n    f.name = fname\n    f.input.extend(inputs)\n    f.output.extend(outputs)\n    f.node.extend(nodes)\n    f.opset_import.extend(opset_imports)\n    f.attribute.extend(attributes)\n    if doc_string:\n        f.doc_string = doc_string\n    return f",
  "def make_model(graph: GraphProto, **kwargs: Any) -> ModelProto:\n    \"\"\"Construct a ModelProto\n\n    Arguments:\n        graph (GraphProto): *make_graph* returns\n        **kwargs: any attribute to add to the returned instance\n    Returns:\n        ModelProto\n    \"\"\"\n    model = ModelProto()\n    # Touch model.ir_version so it is stored as the version from which it is\n    # generated.\n    model.ir_version = IR_VERSION\n    model.graph.CopyFrom(graph)\n\n    opset_imports: Optional[Sequence[OperatorSetIdProto]] = None\n    opset_imports = kwargs.pop('opset_imports', None)  # type: ignore\n    if opset_imports is not None:\n        model.opset_import.extend(opset_imports)\n    else:\n        # Default import\n        imp = model.opset_import.add()\n        imp.version = defs.onnx_opset_version()\n\n    functions: Optional[Sequence[FunctionProto]] = None\n    functions = kwargs.pop('functions', None)  # type: ignore\n    if functions is not None:\n        model.functions.extend(functions)\n\n    for k, v in kwargs.items():\n        # TODO: Does this work with repeated fields?\n        setattr(model, k, v)\n    return model",
  "def make_model_gen_version(graph: GraphProto, **kwargs: Any) -> ModelProto:\n    ir_version_field = str('ir_version')\n    if (ir_version_field not in kwargs):\n        opset_imports_field = str('opset_imports')\n        imports = (kwargs[opset_imports_field] if opset_imports_field in kwargs else [])\n        kwargs[ir_version_field] = find_min_ir_version_for(imports)\n    return make_model(graph, **kwargs)",
  "def set_model_props(model: ModelProto, dict_value: Dict[Text, Text]) -> None:\n    del model.metadata_props[:]\n    for (k, v) in dict_value.items():\n        entry = model.metadata_props.add()\n        entry.key = k\n        entry.value = v",
  "def split_complex_to_pairs(ca: Sequence[np.complex64]) -> Sequence[int]:\n    return [(ca[i // 2].real if (i % 2 == 0) else ca[i // 2].imag)\n            for i in range(len(ca) * 2)]",
  "def make_tensor(\n        name: Text,\n        data_type: int,\n        dims: Sequence[int],\n        vals: Any,\n        raw: bool = False\n) -> TensorProto:\n    '''\n    Make a TensorProto with specified arguments.  If raw is False, this\n    function will choose the corresponding proto field to store the\n    values based on data_type. If raw is True, use \"raw_data\" proto\n    field to store the values, and values should be of type bytes in\n    this case.\n\n    Arguments:\n        name (string): tensor name\n        data_type (int): a value such as onnx.TensorProto.FLOAT\n        dims (List[int]): shape\n        vals: values\n        raw (bool): if True, vals contains the seralized content of the tensor,\n            otherwise, vals should be a list of values of the type defined by *data_type*\n\n    Returns:\n        TensorProto\n    '''\n    tensor = TensorProto()\n    tensor.data_type = data_type\n    tensor.name = name\n\n    if data_type == TensorProto.STRING:\n        assert not raw, \"Can not use raw_data to store string type\"\n\n    # Check number of vals specified equals tensor size\n    expected_size = 1 if (not raw) else (mapping.TENSOR_TYPE_TO_NP_TYPE[data_type].itemsize)\n    # Flatten a numpy array if its rank > 1\n    if type(vals) is np.ndarray and len(vals.shape) > 1:\n        vals = vals.flatten()\n    for d in dims:\n        expected_size = expected_size * d\n\n    if len(vals) != expected_size:\n        raise ValueError(\"Number of values does not match tensor's size. Expected {}, but it is {}. \"\n            .format(expected_size, len(vals)))\n\n    if raw:\n        tensor.raw_data = vals\n    else:\n        if (data_type == TensorProto.COMPLEX64\n                or data_type == TensorProto.COMPLEX128):\n            vals = split_complex_to_pairs(vals)\n        # floa16/bfloat16 are stored as uint16\n        elif (data_type == TensorProto.FLOAT16\n                or data_type == TensorProto.BFLOAT16):\n            vals = np.array(vals).astype(np.float16).view(dtype=np.uint16).flatten().tolist()\n        field = mapping.STORAGE_TENSOR_TYPE_TO_FIELD[\n            mapping.TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE[data_type]]\n        getattr(tensor, field).extend(vals)\n    tensor.dims.extend(dims)\n    return tensor",
  "def make_sparse_tensor(\n    values: TensorProto,\n    indices: TensorProto,\n    dims: Sequence[int]\n) -> SparseTensorProto:\n    \"\"\"Construct a SparseTensorProto\n\n    Arguments:\n        values (TensorProto): the values\n        indices (TensorProto): the indices\n        dims: the shape\n\n    Returns:\n        SparseTensorProto\n    \"\"\"\n    sparse = SparseTensorProto()\n    sparse.values.CopyFrom(values)\n    sparse.indices.CopyFrom(indices)\n    sparse.dims.extend(dims)\n    return sparse",
  "def make_sequence(\n        name: Text,\n        elem_type: SequenceProto.DataType,\n        values: Sequence[Any],\n) -> SequenceProto:\n    '''\n    Make a Sequence with specified value arguments.\n    '''\n    sequence = SequenceProto()\n    sequence.name = name\n    sequence.elem_type = elem_type\n    values_field = mapping.STORAGE_ELEMENT_TYPE_TO_FIELD[elem_type]\n    getattr(sequence, values_field).extend(values)\n    return sequence",
  "def make_map(\n        name: Text,\n        key_type: int,\n        keys: List[Any],\n        values: SequenceProto\n) -> MapProto:\n    '''\n    Make a Map with specified key-value pair arguments.\n\n    Criteria for conversion:\n    - Keys and Values must have the same number of elements\n    - Every key in keys must be of the same type\n    - Every value in values must be of the same type\n    '''\n    map = MapProto()\n    valid_key_int_types = [TensorProto.INT8, TensorProto.INT16, TensorProto.INT32,\n                           TensorProto.INT64, TensorProto.UINT8, TensorProto.UINT16,\n                           TensorProto.UINT32, TensorProto.UINT64]\n    map.name = name\n    map.key_type = key_type\n    if key_type == TensorProto.STRING:\n        map.string_keys.extend(keys)\n    elif key_type in valid_key_int_types:\n        map.keys.extend(keys)\n    map.values.CopyFrom(values)\n    return map",
  "def make_optional(\n        name: Text,\n        elem_type: OptionalProto.DataType,\n        value: Optional[Any],\n) -> OptionalProto:\n    '''\n    Make an Optional with specified value arguments.\n    '''\n    optional = OptionalProto()\n    optional.name = name\n    optional.elem_type = elem_type\n    if elem_type != 0:\n        values_field = mapping.OPTIONAL_ELEMENT_TYPE_TO_FIELD[elem_type]\n        getattr(optional, values_field).CopyFrom(value)\n    return optional",
  "def _to_bytes_or_false(val: Union[Text, bytes]) -> Union[bytes, bool]:\n    \"\"\"An internal graph to convert the input to a bytes or to False.\n\n    The criteria for conversion is as follows and should be python 2 and 3\n    compatible:\n    - If val is py2 str or py3 bytes: return bytes\n    - If val is py2 unicode or py3 str: return val.decode('utf-8')\n    - Otherwise, return False\n    \"\"\"\n    if isinstance(val, bytes):\n        return val\n    try:\n        return val.encode('utf-8')\n    except AttributeError:\n        return False",
  "def make_attribute(\n        key: Text,\n        value: Any,\n        doc_string: Optional[Text] = None\n) -> AttributeProto:\n    \"\"\"Makes an AttributeProto based on the value type.\"\"\"\n    attr = AttributeProto()\n    attr.name = key\n    if doc_string:\n        attr.doc_string = doc_string\n\n    is_iterable = isinstance(value, collections.abc.Iterable)\n    bytes_or_false = _to_bytes_or_false(value)\n    # First, singular cases\n    # float\n    if isinstance(value, float):\n        attr.f = value\n        attr.type = AttributeProto.FLOAT\n    # integer\n    elif isinstance(value, numbers.Integral):\n        attr.i = cast(int, value)\n        attr.type = AttributeProto.INT\n    # string\n    elif bytes_or_false is not False:\n        assert isinstance(bytes_or_false, bytes)\n        attr.s = bytes_or_false\n        attr.type = AttributeProto.STRING\n    elif isinstance(value, TensorProto):\n        attr.t.CopyFrom(value)\n        attr.type = AttributeProto.TENSOR\n    elif isinstance(value, SparseTensorProto):\n        attr.sparse_tensor.CopyFrom(value)\n        attr.type = AttributeProto.SPARSE_TENSOR\n    elif isinstance(value, GraphProto):\n        attr.g.CopyFrom(value)\n        attr.type = AttributeProto.GRAPH\n    elif isinstance(value, TypeProto):\n        attr.tp.CopyFrom(value)\n        attr.type = AttributeProto.TYPE_PROTO\n    # third, iterable cases\n    elif is_iterable:\n        byte_array = [_to_bytes_or_false(v) for v in value]\n        if all(isinstance(v, numbers.Integral) for v in value):\n            # Turn np.int32/64 into Python built-in int.\n            attr.ints.extend(int(v) for v in value)\n            attr.type = AttributeProto.INTS\n        elif all(isinstance(v, numbers.Real) for v in value):\n            # Since ints and floats are members of Real, this allows a mix of ints and floats\n            # (and converts the ints to floats).\n            attr.floats.extend(float(v) for v in value)\n            attr.type = AttributeProto.FLOATS\n        elif all(map(lambda bytes_or_false: bytes_or_false is not False, byte_array)):\n            attr.strings.extend(cast(List[bytes], byte_array))\n            attr.type = AttributeProto.STRINGS\n        elif all(isinstance(v, TensorProto) for v in value):\n            attr.tensors.extend(value)\n            attr.type = AttributeProto.TENSORS\n        elif all(isinstance(v, SparseTensorProto) for v in value):\n            attr.sparse_tensors.extend(value)\n            attr.type = AttributeProto.SPARSE_TENSORS\n        elif all(isinstance(v, GraphProto) for v in value):\n            attr.graphs.extend(value)\n            attr.type = AttributeProto.GRAPHS\n        elif all(isinstance(tp, TypeProto) for tp in value):\n            attr.type_protos.extend(value)\n            attr.type = AttributeProto.TYPE_PROTOS\n        else:\n            raise ValueError(\n                \"You passed in an iterable attribute but I cannot figure out \"\n                \"its applicable type.\")\n    else:\n        raise TypeError(\n            'value \"{}\" is not valid attribute data type.'.format(value))\n    return attr",
  "def get_attribute_value(attr: AttributeProto) -> Any:\n    if attr.type == AttributeProto.FLOAT:\n        return attr.f\n    if attr.type == AttributeProto.INT:\n        return attr.i\n    if attr.type == AttributeProto.STRING:\n        return attr.s\n    if attr.type == AttributeProto.TENSOR:\n        return attr.t\n    if attr.type == AttributeProto.SPARSE_TENSOR:\n        return attr.sparse_tensor\n    if attr.type == AttributeProto.GRAPH:\n        return attr.g\n    if attr.type == AttributeProto.TYPE_PROTO:\n        return attr.tp\n    if attr.type == AttributeProto.FLOATS:\n        return list(attr.floats)\n    if attr.type == AttributeProto.INTS:\n        return list(attr.ints)\n    if attr.type == AttributeProto.STRINGS:\n        return list(attr.strings)\n    if attr.type == AttributeProto.TENSORS:\n        return list(attr.tensors)\n    if attr.type == AttributeProto.SPARSE_TENSORS:\n        return list(attr.sparse_tensors)\n    if attr.type == AttributeProto.GRAPHS:\n        return list(attr.graphs)\n    if attr.type == AttributeProto.TYPE_PROTOS:\n        return list(attr.type_protos)\n    raise ValueError(\"Unsupported ONNX attribute: {}\".format(attr))",
  "def make_empty_tensor_value_info(name: Text) -> ValueInfoProto:\n    value_info_proto = ValueInfoProto()\n    value_info_proto.name = name\n    return value_info_proto",
  "def make_tensor_type_proto(\n        elem_type: int,\n        shape: Optional[Sequence[Union[Text, int, None]]],\n        shape_denotation: Optional[List[Text]] = None,\n) -> TypeProto:\n    \"\"\"Makes a Tensor TypeProto based on the data type and shape.\"\"\"\n\n    type_proto = TypeProto()\n    tensor_type_proto = type_proto.tensor_type\n    tensor_type_proto.elem_type = elem_type\n    tensor_shape_proto = tensor_type_proto.shape\n\n    if shape is not None:\n        # You might think this is a no-op (extending a normal Python\n        # list by [] certainly is), but protobuf lists work a little\n        # differently; if a field is never set, it is omitted from the\n        # resulting protobuf; a list that is explicitly set to be\n        # empty will get an (empty) entry in the protobuf. This\n        # difference is visible to our consumers, so make sure we emit\n        # an empty shape!\n        tensor_shape_proto.dim.extend([])\n\n        if shape_denotation:\n            if len(shape_denotation) != len(shape):\n                raise ValueError(\n                    'Invalid shape_denotation. '\n                    'Must be of the same length as shape.')\n\n        for i, d in enumerate(shape):\n            dim = tensor_shape_proto.dim.add()\n            if d is None:\n                pass\n            elif isinstance(d, int):\n                dim.dim_value = d\n            elif isinstance(d, str):\n                dim.dim_param = d\n            else:\n                raise ValueError(\n                    'Invalid item in shape: {}. '\n                    'Needs to be of int or str.'.format(d))\n\n            if shape_denotation:\n                dim.denotation = shape_denotation[i]\n\n    return type_proto",
  "def make_tensor_value_info(\n        name: Text,\n        elem_type: int,\n        shape: Optional[Sequence[Union[Text, int, None]]],\n        doc_string: Text = \"\",\n        shape_denotation: Optional[List[Text]] = None,\n) -> ValueInfoProto:\n    \"\"\"Makes a ValueInfoProto based on the data type and shape.\"\"\"\n    value_info_proto = ValueInfoProto()\n    value_info_proto.name = name\n    if doc_string:\n        value_info_proto.doc_string = doc_string\n\n    tensor_type_proto = make_tensor_type_proto(elem_type, shape, shape_denotation)\n    value_info_proto.type.CopyFrom(tensor_type_proto)\n    return value_info_proto",
  "def make_sparse_tensor_type_proto(\n        elem_type: int,\n        shape: Optional[Sequence[Union[Text, int, None]]],\n        shape_denotation: Optional[List[Text]] = None,\n) -> TypeProto:\n    \"\"\"Makes a SparseTensor TypeProto based on the data type and shape.\"\"\"\n\n    type_proto = TypeProto()\n    sparse_tensor_type_proto = type_proto.sparse_tensor_type\n    sparse_tensor_type_proto.elem_type = elem_type\n    sparse_tensor_shape_proto = sparse_tensor_type_proto.shape\n\n    if shape is not None:\n        # You might think this is a no-op (extending a normal Python\n        # list by [] certainly is), but protobuf lists work a little\n        # differently; if a field is never set, it is omitted from the\n        # resulting protobuf; a list that is explicitly set to be\n        # empty will get an (empty) entry in the protobuf. This\n        # difference is visible to our consumers, so make sure we emit\n        # an empty shape!\n        sparse_tensor_shape_proto.dim.extend([])\n\n        if shape_denotation:\n            if len(shape_denotation) != len(shape):\n                raise ValueError(\n                    'Invalid shape_denotation. '\n                    'Must be of the same length as shape.')\n\n        for i, d in enumerate(shape):\n            dim = sparse_tensor_shape_proto.dim.add()\n            if d is None:\n                pass\n            elif isinstance(d, int):\n                dim.dim_value = d\n            elif isinstance(d, str):\n                dim.dim_param = d\n            else:\n                raise ValueError(\n                    'Invalid item in shape: {}. '\n                    'Needs to be of int or text.'.format(d))\n\n            if shape_denotation:\n                dim.denotation = shape_denotation[i]\n\n    return type_proto",
  "def make_sparse_tensor_value_info(\n        name: Text,\n        elem_type: int,\n        shape: Optional[Sequence[Union[Text, int, None]]],\n        doc_string: Text = \"\",\n        shape_denotation: Optional[List[Text]] = None,\n) -> ValueInfoProto:\n    \"\"\"Makes a SparseTensor ValueInfoProto based on the data type and shape.\"\"\"\n    value_info_proto = ValueInfoProto()\n    value_info_proto.name = name\n    if doc_string:\n        value_info_proto.doc_string = doc_string\n\n    sparse_tensor_type_proto = make_sparse_tensor_type_proto(elem_type, shape, shape_denotation)\n    value_info_proto.type.sparse_tensor_type.CopyFrom(sparse_tensor_type_proto.sparse_tensor_type)\n    return value_info_proto",
  "def make_sequence_type_proto(\n        inner_type_proto: TypeProto,\n) -> TypeProto:\n    \"\"\"Makes a sequence TypeProto.\"\"\"\n    type_proto = TypeProto()\n    type_proto.sequence_type.elem_type.CopyFrom(inner_type_proto)\n    return type_proto",
  "def make_optional_type_proto(\n        inner_type_proto: TypeProto,\n) -> TypeProto:\n    \"\"\"Makes an optional TypeProto.\"\"\"\n    type_proto = TypeProto()\n    type_proto.optional_type.elem_type.CopyFrom(inner_type_proto)\n    return type_proto",
  "def make_value_info(\n        name: Text,\n        type_proto: TypeProto,\n        doc_string: Text = \"\",\n) -> ValueInfoProto:\n    \"\"\"Makes a ValueInfoProto with the given type_proto.\"\"\"\n    value_info_proto = ValueInfoProto()\n    value_info_proto.name = name\n    if doc_string:\n        value_info_proto.doc_string = doc_string\n\n    value_info_proto.type.CopyFrom(type_proto)\n    return value_info_proto",
  "def _sanitize_str(s: Union[Text, bytes]) -> Text:\n    if isinstance(s, str):\n        sanitized = s\n    elif isinstance(s, bytes):\n        sanitized = s.decode('utf-8', errors='ignore')\n    else:\n        sanitized = str(s)\n    if len(sanitized) < 64:\n        return sanitized\n    return sanitized[:64] + '...<+len=%d>' % (len(sanitized) - 64)",
  "def make_tensor_sequence_value_info(\n        name: Text,\n        elem_type: int,\n        shape: Optional[Sequence[Union[Text, int, None]]],\n        doc_string: Text = \"\",\n        elem_shape_denotation: Optional[List[Text]] = None,\n) -> ValueInfoProto:\n    \"\"\"Makes a Sequence[Tensors] ValueInfoProto based on the data type and shape.\"\"\"\n    value_info_proto = ValueInfoProto()\n    value_info_proto.name = name\n    if doc_string:\n        value_info_proto.doc_string = doc_string\n\n    tensor_type_proto = make_tensor_type_proto(elem_type, shape, elem_shape_denotation)\n    sequence_type_proto = make_sequence_type_proto(tensor_type_proto)\n    value_info_proto.type.sequence_type.CopyFrom(sequence_type_proto.sequence_type)\n\n    return value_info_proto",
  "def printable_attribute(attr: AttributeProto, subgraphs: bool = False) -> Union[Text, Tuple[Text, List[GraphProto]]]:\n    content = []\n    content.append(attr.name)\n    content.append(\"=\")\n\n    def str_float(f: float) -> Text:\n        # NB: Different Python versions print different numbers of trailing\n        # decimals, specifying this explicitly keeps it consistent for all\n        # versions\n        return '{:.15g}'.format(f)\n\n    def str_int(i: int) -> Text:\n        return str(i)\n\n    _T = TypeVar('_T')  # noqa\n\n    def str_list(str_elem: Callable[[_T], Text], xs: Sequence[_T]) -> Text:\n        return '[' + ', '.join(map(str_elem, xs)) + ']'\n\n    # for now, this logic should continue to work as long as we are running on a proto3\n    # implementation. If/when we switch to proto3, we will need to use attr.type\n\n    # To support printing subgraphs, if we find a graph attribute, print out\n    # its name here and pass the graph itself up to the caller for later\n    # printing.\n    graphs = []\n    if attr.HasField(\"f\"):\n        content.append(str_float(attr.f))\n    elif attr.HasField(\"i\"):\n        content.append(str_int(attr.i))\n    elif attr.HasField(\"s\"):\n        # TODO: Bit nervous about Python 2 / Python 3 determinism implications\n        content.append(repr(_sanitize_str(attr.s)))\n    elif attr.HasField(\"t\"):\n        if len(attr.t.dims) > 0:\n            content.append(\"<Tensor>\")\n        else:\n            # special case to print scalars\n            field = STORAGE_TENSOR_TYPE_TO_FIELD[attr.t.data_type]\n            content.append('<Scalar Tensor {}>'.format(str(getattr(attr.t, field))))\n    elif attr.HasField(\"g\"):\n        content.append(\"<graph {}>\".format(attr.g.name))\n        graphs.append(attr.g)\n    elif attr.HasField(\"tp\"):\n        content.append(\"<Type Proto {}>\".format(attr.tp))\n    elif attr.floats:\n        content.append(str_list(str_float, attr.floats))\n    elif attr.ints:\n        content.append(str_list(str_int, attr.ints))\n    elif attr.strings:\n        # TODO: Bit nervous about Python 2 / Python 3 determinism implications\n        content.append(str(list(map(_sanitize_str, attr.strings))))\n    elif attr.tensors:\n        content.append(\"[<Tensor>, ...]\")\n    elif attr.type_protos:\n        content.append('[')\n        for i, tp in enumerate(attr.type_protos):\n            comma = ',' if i != len(attr.type_protos) - 1 else ''\n            content.append('<Type Proto {}>{}'.format(tp, comma))\n        content.append(']')\n    elif attr.graphs:\n        content.append('[')\n        for i, g in enumerate(attr.graphs):\n            comma = ',' if i != len(attr.graphs) - 1 else ''\n            content.append('<graph {}>{}'.format(g.name, comma))\n        content.append(']')\n        graphs.extend(attr.graphs)\n    else:\n        content.append(\"<Unknown>\")\n    if subgraphs:\n        return ' '.join(content), graphs\n    else:\n        return ' '.join(content)",
  "def printable_dim(dim: TensorShapeProto.Dimension) -> Text:\n    which = dim.WhichOneof('value')\n    assert which is not None\n    return str(getattr(dim, which))",
  "def printable_type(t: TypeProto) -> Text:\n    if t.WhichOneof('value') == \"tensor_type\":\n        s = TensorProto.DataType.Name(t.tensor_type.elem_type)\n        if t.tensor_type.HasField('shape'):\n            if len(t.tensor_type.shape.dim):\n                s += str(', ' + 'x'.join(map(printable_dim, t.tensor_type.shape.dim)))\n            else:\n                s += str(', scalar')\n        return s\n    if t.WhichOneof('value') is None:\n        return \"\"\n    return 'Unknown type {}'.format(t.WhichOneof('value'))",
  "def printable_value_info(v: ValueInfoProto) -> Text:\n    s = '%{}'.format(v.name)\n    if v.type:\n        s = '{}[{}]'.format(s, printable_type(v.type))\n    return s",
  "def printable_tensor_proto(t: TensorProto) -> Text:\n    s = '%{}['.format(t.name)\n    s += TensorProto.DataType.Name(t.data_type)\n    if t.dims is not None:\n        if len(t.dims):\n            s += str(', ' + 'x'.join(map(str, t.dims)))\n        else:\n            s += str(', scalar')\n    s += ']'\n    return s",
  "def printable_node(node: NodeProto, prefix: Text = '', subgraphs: bool = False) -> Union[Text, Tuple[Text, List[GraphProto]]]:\n    content = []\n    if len(node.output):\n        content.append(\n            ', '.join(['%{}'.format(name) for name in node.output]))\n        content.append('=')\n    # To deal with nested graphs\n    graphs: List[GraphProto] = []\n    printed_attrs = []\n    for attr in node.attribute:\n        if subgraphs:\n            printed_attr_subgraphs = printable_attribute(attr, subgraphs)\n            assert isinstance(printed_attr_subgraphs[1], list)\n            graphs.extend(printed_attr_subgraphs[1])\n            printed_attrs.append(printed_attr_subgraphs[0])\n        else:\n            printed = printable_attribute(attr)\n            assert isinstance(printed, Text)\n            printed_attrs.append(printed)\n    printed_attributes = ', '.join(sorted(printed_attrs))\n    printed_inputs = ', '.join(['%{}'.format(name) for name in node.input])\n    if node.attribute:\n        content.append(\"{}[{}]({})\".format(node.op_type, printed_attributes, printed_inputs))\n    else:\n        content.append(\"{}({})\".format(node.op_type, printed_inputs))\n    if subgraphs:\n        return prefix + ' '.join(content), graphs\n    else:\n        return prefix + ' '.join(content)",
  "def printable_graph(graph: GraphProto, prefix: Text = '') -> Text:\n    \"\"\"\n    Display a GraphProto as a string.\n\n    Arguments:\n        graph (GraphProto): the graph to display\n        prefix (string): prefix of every line\n\n    Returns:\n        string\n    \"\"\"\n    content = []\n    indent = prefix + '  '\n    # header\n    header = ['graph', graph.name]\n    initializers = {t.name for t in graph.initializer}\n    if len(graph.input):\n        header.append(\"(\")\n        in_strs = []  # required inputs\n        in_with_init_strs = []  # optional inputs with initializer providing default value\n        for inp in graph.input:\n            if inp.name not in initializers:\n                in_strs.append(printable_value_info(inp))\n            else:\n                in_with_init_strs.append(printable_value_info(inp))\n        if in_strs:\n            content.append(prefix + ' '.join(header))\n            header = []\n            for line in in_strs:\n                content.append(prefix + '  ' + line)\n        header.append(\")\")\n\n        if in_with_init_strs:\n            header.append(\"optional inputs with matching initializers (\")\n            content.append(prefix + ' '.join(header))\n            header = []\n            for line in in_with_init_strs:\n                content.append(prefix + '  ' + line)\n            header.append(\")\")\n\n        # from IR 4 onwards an initializer is not required to have a matching graph input\n        # so output the name, type and shape of those as well\n        if len(in_with_init_strs) < len(initializers):\n            graph_inputs = {i.name for i in graph.input}\n            init_strs = [printable_tensor_proto(i) for i in graph.initializer\n                         if i.name not in graph_inputs]\n            header.append(\"initializers (\")\n            content.append(prefix + ' '.join(header))\n            header = []\n            for line in init_strs:\n                content.append(prefix + '  ' + line)\n            header.append(\")\")\n\n    header.append('{')\n    content.append(prefix + ' '.join(header))\n    graphs: List[GraphProto] = []\n    # body\n    for node in graph.node:\n        contents_subgraphs = printable_node(node, indent, subgraphs=True)\n        assert isinstance(contents_subgraphs[1], list)\n        content.append(contents_subgraphs[0])\n        graphs.extend(contents_subgraphs[1])\n    # tail\n    tail = ['return']\n    if len(graph.output):\n        tail.append(\n            ', '.join(['%{}'.format(out.name) for out in graph.output]))\n    content.append(indent + ' '.join(tail))\n    # closing bracket\n    content.append(prefix + '}')\n    for g in graphs:\n        content.append('\\n' + printable_graph(g))\n    return '\\n'.join(content)",
  "def strip_doc_string(proto: google.protobuf.message.Message) -> None:\n    \"\"\"\n    Empties `doc_string` field on any nested protobuf messages\n    \"\"\"\n    assert isinstance(proto, google.protobuf.message.Message)\n    for descriptor in proto.DESCRIPTOR.fields:\n        if descriptor.name == 'doc_string':\n            proto.ClearField(descriptor.name)\n        elif descriptor.type == descriptor.TYPE_MESSAGE:\n            if descriptor.label == descriptor.LABEL_REPEATED:\n                for x in getattr(proto, descriptor.name):\n                    strip_doc_string(x)\n            elif proto.HasField(descriptor.name):\n                strip_doc_string(getattr(proto, descriptor.name))",
  "def make_training_info(algorithm: GraphProto, algorithm_bindings: AssignmentBindingType, initialization: Optional[GraphProto], initialization_bindings: Optional[AssignmentBindingType]) -> TrainingInfoProto:\n    training_info = TrainingInfoProto()\n    training_info.algorithm.CopyFrom(algorithm)\n    for k, v in algorithm_bindings:\n        binding = training_info.update_binding.add()\n        binding.key = k\n        binding.value = v\n\n    if initialization:\n        training_info.initialization.CopyFrom(initialization)\n    if initialization_bindings:\n        for k, v in initialization_bindings:\n            binding = training_info.initialization_binding.add()\n            binding.key = k\n            binding.value = v\n\n    return training_info",
  "def make_sequence_value_info(\n        name: Text,\n        elem_type: int,\n        shape: Optional[Sequence[Union[Text, int, None]]],\n        doc_string: Text = \"\",\n        elem_shape_denotation: Optional[List[Text]] = None,\n) -> ValueInfoProto:\n    \"\"\"Makes a Sequence[Tensors] ValueInfoProto based on the data type and shape.\"\"\"\n    warnings.warn(str(\"`onnx.helper.make_sequence_value_info` is a deprecated alias for `onnx.helper.make_tensor_sequence_value_info`. To silence this warning, please use `make_tensor_sequence_value_info` for `TensorProto` sequences. Deprecated in ONNX v1.10.0, `onnx.helper.make_sequence_value_info alias` will be removed in an upcoming release.\"), DeprecationWarning, stacklevel=2)\n    return make_tensor_sequence_value_info(name, elem_type, shape, doc_string, elem_shape_denotation)",
  "def process(release_version: Text, ir_version: int, *args: Any) -> None:\n        for pair in zip(['ai.onnx', 'ai.onnx.ml', 'ai.onnx.training'], args):\n            if (pair not in result):\n                result[pair] = ir_version",
  "def find_min(domain: Union[Text, None], version: int) -> int:\n        key = (domain if domain else 'ai.onnx', version)\n        if (key in OP_SET_ID_VERSION_MAP):\n            return OP_SET_ID_VERSION_MAP[key]\n        else:\n            raise ValueError(\"Unsupported opset-version.\")",
  "def str_float(f: float) -> Text:\n        # NB: Different Python versions print different numbers of trailing\n        # decimals, specifying this explicitly keeps it consistent for all\n        # versions\n        return '{:.15g}'.format(f)",
  "def str_int(i: int) -> Text:\n        return str(i)",
  "def str_list(str_elem: Callable[[_T], Text], xs: Sequence[_T]) -> Text:\n        return '[' + ', '.join(map(str_elem, xs)) + ']'",
  "def _load_bytes(f: Union[IO[bytes], Text]) -> bytes:\n    if hasattr(f, 'read') and callable(cast(IO[bytes], f).read):\n        s = cast(IO[bytes], f).read()\n    else:\n        with open(cast(Text, f), 'rb') as readable:\n            s = readable.read()\n    return s",
  "def _save_bytes(str: bytes, f: Union[IO[bytes], Text]) -> None:\n    if hasattr(f, 'write') and callable(cast(IO[bytes], f).write):\n        cast(IO[bytes], f).write(str)\n    else:\n        with open(cast(Text, f), 'wb') as writable:\n            writable.write(str)",
  "def _get_file_path(f: Union[IO[bytes], Text]) -> Optional[Text]:\n    if isinstance(f, str):\n        return os.path.abspath(f)\n    if hasattr(f, 'name'):\n        return os.path.abspath(f.name)\n    return None",
  "def _serialize(proto: Union[bytes, google.protobuf.message.Message]) -> bytes:\n    '''\n    Serialize a in-memory proto to bytes\n\n    Arguments:\n        proto: a in-memory proto, such as a ModelProto, TensorProto, etc\n\n    Returns:\n        Serialized proto in bytes\n    '''\n    if isinstance(proto, bytes):\n        return proto\n    elif hasattr(proto, 'SerializeToString') and callable(proto.SerializeToString):\n        result = proto.SerializeToString()\n        return result\n    else:\n        raise TypeError('No SerializeToString method is detected. '\n                         'neither proto is a str.\\ntype is {}'.format(type(proto)))",
  "def _deserialize(s: bytes, proto: _Proto) -> _Proto:\n    '''\n    Parse bytes into a in-memory proto\n\n    Arguments:\n        s: bytes containing serialized proto\n        proto: a in-memory proto object\n\n    Returns:\n        The proto instance filled in by s\n    '''\n    if not isinstance(s, bytes):\n        raise ValueError('Parameter s must be bytes, but got type: {}'.format(type(s)))\n\n    if not (hasattr(proto, 'ParseFromString') and callable(proto.ParseFromString)):\n        raise ValueError('No ParseFromString method is detected. '\n                         '\\ntype is {}'.format(type(proto)))\n\n    decoded = cast(Optional[int], proto.ParseFromString(s))\n    if decoded is not None and decoded != len(s):\n        raise google.protobuf.message.DecodeError(\n            \"Protobuf decoding consumed too few bytes: {} out of {}\".format(\n                decoded, len(s)))\n    return proto",
  "def load_model(f: Union[IO[bytes], Text], format: Optional[Any] = None, load_external_data: bool = True) -> ModelProto:\n    '''\n    Loads a serialized ModelProto into memory\n    load_external_data is true if the external data under the same directory of the model and load the external data\n    If not, users need to call load_external_data_for_model with directory to load\n\n    Arguments:\n        f: can be a file-like object (has \"read\" function) or a string containing a file name\n        format: for future use\n\n    Returns:\n        Loaded in-memory ModelProto\n    '''\n    s = _load_bytes(f)\n    model = load_model_from_string(s, format=format)\n\n    if load_external_data:\n        model_filepath = _get_file_path(f)\n        if model_filepath:\n            base_dir = os.path.dirname(model_filepath)\n            load_external_data_for_model(model, base_dir)\n\n    return model",
  "def load_tensor(f: Union[IO[bytes], Text], format: Optional[Any] = None) -> TensorProto:\n    '''\n    Loads a serialized TensorProto into memory\n\n    Arguments:\n        f: can be a file-like object (has \"read\" function) or a string containing a file name\n        format: for future use\n\n    Returns:\n        Loaded in-memory TensorProto\n    '''\n    s = _load_bytes(f)\n    return load_tensor_from_string(s, format=format)",
  "def load_model_from_string(s: bytes, format: Optional[Any] = None) -> ModelProto:\n    '''\n    Loads a binary string (bytes) that contains serialized ModelProto\n\n    Arguments:\n        s: a string, which contains serialized ModelProto\n        format: for future use\n\n    Returns:\n        Loaded in-memory ModelProto\n    '''\n    return _deserialize(s, ModelProto())",
  "def load_tensor_from_string(s: bytes, format: Optional[Any] = None) -> TensorProto:\n    '''\n    Loads a binary string (bytes) that contains serialized TensorProto\n\n    Arguments:\n        s: a string, which contains serialized TensorProto\n        format: for future use\n\n    Returns:\n        Loaded in-memory TensorProto\n    '''\n    return _deserialize(s, TensorProto())",
  "def save_model(proto: Union[ModelProto, bytes], f: Union[IO[bytes], Text], format: Optional[Any] = None, save_as_external_data: bool = False, all_tensors_to_one_file: bool = True, location: Optional[Text] = None, size_threshold: int = 1024, convert_attribute: bool = False) -> None:\n    '''\n    Saves the ModelProto to the specified path and optionally, serialize tensors with raw data as external data before saving.\n\n    Arguments:\n        proto: should be a in-memory ModelProto\n        f: can be a file-like object (has \"write\" function) or a string containing a file name format for future use\n        all_tensors_to_one_file: If true, save all tensors to one external file specified by location.\n            If false, save each tensor to a file named with the tensor name.\n        location: specify the external file that all tensors to save to.\n            If not specified, will use the model name.\n        size_threshold: Threshold for size of data. Only when tensor's data is >= the size_threshold it will be converted\n            to external data. To convert every tensor with raw data to external data set size_threshold=0.\n        convert_attribute: If true, convert all tensors to external data\n            If false, convert only non-attribute tensors to external data\n    '''\n    if isinstance(proto, bytes):\n        proto = _deserialize(proto, ModelProto())\n\n    if save_as_external_data:\n        convert_model_to_external_data(proto, all_tensors_to_one_file, location, size_threshold, convert_attribute)\n\n    model_filepath = _get_file_path(f)\n    if model_filepath:\n        basepath = os.path.dirname(model_filepath)\n        proto = write_external_data_tensors(proto, basepath)\n\n    s = _serialize(proto)\n    _save_bytes(s, f)",
  "def save_tensor(proto: TensorProto, f: Union[IO[bytes], Text]) -> None:\n    '''\n    Saves the TensorProto to the specified path.\n\n    Arguments:\n        proto: should be a in-memory TensorProto\n        f: can be a file-like object (has \"write\" function) or a string containing a file name\n        format: for future use\n    '''\n    s = _serialize(proto)\n    _save_bytes(s, f)",
  "def convert_version(model: ModelProto, target_version: int) -> ModelProto:\n    \"\"\"Apply the version conversion on the serialized ModelProto.\n\n    Arguments:\n        input (ModelProto): model\n        target_version (int): target opset version\n\n    Returns:\n        return (ModelProto) converted model\n\n    Raises Exceptions:\n        RuntimeError when some necessary conversion is not supported\n\n    Supported adapters:\n        - Add from Opset 7 to Opset 6\n        - Add from Opset 6 to Opset 5\n        - Add from Opset 6 to Opset 7\n        - Add from Opset 5 to Opset 6\n        - Mul from Opset 6 to Opset 7\n        - Mul from Opset 7 to Opset 6\n        - Mul from Opset 6 to Opset 5\n        - Mul from Opset 5 to Opset 6\n        - Gemm from Opset 7 to Opset 6\n        - Gemm from Opset 6 to Opset 5\n        - Gemm from Opset 6 to Opset 7\n        - Gemm from Opset 5 to Opset 6\n        - Relu from Opset 6 to Opset 5\n        - Relu from Opset 5 to Opset 6\n        - BatchNorm from Opset 7 to Opset 6\n        - BatchNorm from Opset 6 to Opset 7\n        - BatchNorm from Opset 6 to Opset 5\n        - BatchNorm from Opset 5 to Opset 6\n        - Concat from Opset 4 to Opset 3\n        - Concat from Opset 3 to Opset 4\n        - Reshape from Opset 5 to Opset 4\n        - Reshape from Opset 4 to Opset 5\n        - Sum from Opset 7 to Opset 8\n        - Sum from Opset 8 to Opset 7\n        - Sum from Opset 6 to Opset 5\n        - Sum from Opset 5 to Opset 6\n        - MaxPool from Opset 8 to Opset 7\n        - MaxPool from Opset 7 to Opset 8\n        - AveragePool from Opset 7 to Opset 6\n        - AveragePool from Opset 6 to Opset 7\n        - Dropout from Opset 7 to Opset 6\n        - Dropout from Opset 6 to Opset 5\n        - Dropout from Opset 6 to Opset 7\n        - Dropout from Opset 5 to Opset 6\n        - RNN from Opset 13 to Opset 14\n        - RNN from Opset 14 to Opset 13\n        - GRU from Opset 13 to Opset 14\n        - GRU from Opset 14 to Opset 13\n        - LSTM from Opset 13 to Opset 14\n        - LSTM from Opset 14 to Opset 13\n\n    Unsupported adapters:\n        - Min from Opset 8 to Opset 7\n        - Min from Opset 7 to Opset 8\n        - Min from Opset 6 to Opset 5\n        - Min from Opset 5 to Opset 6\n        - Mean from Opset 8 to Opset 7\n        - Mean from Opset 7 to Opset 8\n        - Mean from Opset 6 to Opset 5\n        - Mean from Opset 5 to Opset 6\n        - Max from Opset 8 to Opset 7\n        - Max from Opset 7 to Opset 8\n        - Max from Opset 6 to Opset 5\n        - Max from Opset 5 to Opset 6\n        - Xor from Opset 6 to Opset 7\n        - Xor from Opset 7 to Opset 6\n        - Upsample from Opset 6 to Opset 7\n        - Upsample from Opset 7 to Opset 6\n        - Sub from Opset 6 to Opset 7\n        - Sub from Opset 7 to Opset 6\n        - Sub from Opset 6 to Opset 5\n        - Sub from Opset 5 to Opset 6\n        - RNN from Opset 6 to Opset 7\n        - RNN from Opset 7 to Opset 6\n        - Pow from Opset 6 to Opset 7\n        - Pow from Opset 7 to Opset 6\n        - PRelu from Opset 6 to Opset 7\n        - PRelu from Opset 7 to Opset 6\n        - PRelu from Opset 6 to Opset 5\n        - PRelu from Opset 5 to Opset 6\n        - Or from Opset 6 to Opset 7\n        - Or from Opset 7 to Opset 6\n        - Less from Opset 6 to Opset 7\n        - Less from Opset 7 to Opset 6\n        - LSTM from Opset 6 to Opset 7\n        - LSTM from Opset 7 to Opset 6\n        - Greater from Opset 6 to Opset 7\n        - Greater from Opset 7 to Opset 6\n        - GRU from Opset 6 to Opset 7\n        - GRU from Opset 7 to Opset 6\n        - GRU from Opset 3 to Opset 2\n        - GRU from Opset 2 to Opset 3\n        - Equal from Opset 6 to Opset 7\n        - Equal from Opset 7 to Opset 6\n        - Div from Opset 6 to Opset 7\n        - Div from Opset 7 to Opset 6\n        - Div from Opset 6 to Opset 5\n        - Div from Opset 5 to Opset 6\n        - And from Opset 6 to Opset 7\n        - And from Opset 7 to Opset 6\n        - And from Opset 6 to Opset 5\n        - And from Opset 5 to Opset 6\n        - Tile from Opset 6 to Opset 5\n        - Tile from Opset 5 to Opset 6\n        - Sqrt from Opset 6 to Opset 5\n        - Sqrt from Opset 5 to Opset 6\n        - Sigmoid from opset 6 to opset 5\n        - Sigmoid from opset 5 to opset 6\n        - Selu from opset 6 to opset 5\n        - Selu from opset 5 to opset 6\n        - Reciprocal from opset 6 to opset 5\n        - Reciprocal from opset 5 to opset 6\n        - Neg from opset 6 to opset 5\n        - Neg from opset 5 to opset 6\n        - Log from opset 6 to opset 5\n        - Log from opset 5 to opset 6\n        - LeakyRelu from opset 6 to opset 5\n        - LeakyRelu from opset 5 to opset 6\n        - InstanceNormalization from opset 6 to opset 5\n        - InstanceNormalization from opset 5 to opset 6\n        - HardSigmoid from opset 6 to opset 5\n        - HardSigmoid from opset 5 to opset 6\n        - Floor from opset 6 to opset 5\n        - Floor from opset 5 to opset 6\n        - Exp from opset 6 to opset 5\n        - Exp from opset 5 to opset 6\n        - Elu from opset 6 to opset 5\n        - Elu from opset 5 to opset 6\n        - Clip from opset 6 to opset 5\n        - Clip from opset 5 to opset 6\n        - Ceil from opset 6 to opset 5\n        - Ceil from opset 5 to opset 6\n        - Cast from opset 6 to opset 5\n        - Cast from opset 5 to opset 6\n        - Abs from opset 6 to opset 5\n        - Abs from opset 5 to opset 6\n        - Split from opset 2 to opset 1\n        - Split from opset 1 to opset 2\n        - Pad from opset 2 to opset 1\n        - Pad from opset 1 to opset 2\n        - LpPool from opset 2 to opset 1\n        - LpPool from opset 1 to opset 2\n        - GlobalLpPool from opset 2 to opset 1\n        - GlobalLpPool from opset 1 to opset 2\n    \"\"\"\n    if not isinstance(model, ModelProto):\n        raise ValueError('VersionConverter only accepts ModelProto as model, incorrect type: {}'.format(type(model)))\n    if not isinstance(target_version, int):\n        raise ValueError('VersionConverter only accepts int as target_version, incorrect type: {}'.format(type(target_version)))\n    model_str = model.SerializeToString()\n    converted_model_str = C.convert_version(model_str, target_version)\n    return load_from_string(converted_model_str)",
  "class ExternalDataInfo(object):\n\n    def __init__(self, tensor: TensorProto) -> None:\n        self.location = ''\n        self.offset = None\n        self.length = None\n        self.checksum = None\n        self.basepath = ''\n\n        for entry in tensor.external_data:\n            setattr(self, entry.key, entry.value)\n\n        if self.offset:\n            self.offset = int(self.offset)\n\n        if self.length:\n            self.length = int(self.length)",
  "def load_external_data_for_tensor(tensor: TensorProto, base_dir: Text) -> None:\n    \"\"\"\n    Loads data from an external file for tensor.\n    Ideally TensorProto should not hold any raw data but if it does it will be ignored.\n\n    Arguments:\n        tensor: a TensorProto object.\n        base_dir: directory that contains the external data.\n    \"\"\"\n    info = ExternalDataInfo(tensor)\n    file_location = _sanitize_path(info.location)\n    external_data_file_path = os.path.join(base_dir, file_location)\n\n    with open(external_data_file_path, 'rb') as data_file:\n\n        if info.offset:\n            data_file.seek(info.offset)\n\n        if info.length:\n            tensor.raw_data = data_file.read(info.length)\n        else:\n            tensor.raw_data = data_file.read()",
  "def load_external_data_for_model(model: ModelProto, base_dir: Text) -> None:\n    \"\"\"\n    Loads external tensors into model\n\n    Arguments:\n        model: ModelProto to load external data to\n        base_dir: directory that contains external data\n    \"\"\"\n    for tensor in _get_all_tensors(model):\n        if uses_external_data(tensor):\n            load_external_data_for_tensor(tensor, base_dir)\n            # After loading raw_data from external_data, change the state of tensors\n            tensor.data_location = TensorProto.DEFAULT\n            # and remove external data\n            del tensor.external_data[:]",
  "def set_external_data(tensor: TensorProto,\n                      location: Text,\n                      offset: Optional[int] = None,\n                      length: Optional[int] = None,\n                      checksum: Optional[Text] = None,\n                      basepath: Optional[Text] = None\n                      ) -> None:\n    if not tensor.HasField(\"raw_data\"):\n        raise ValueError(\"Tensor \" + tensor.name + \"does not have raw_data field. Cannot set external data for this tensor.\")\n\n    del tensor.external_data[:]\n    tensor.data_location = TensorProto.EXTERNAL\n    for (k, v) in {\n        'location': location,\n        'offset': int(offset) if offset is not None else None,\n        'length': int(length) if length is not None else None,\n        'checksum': checksum,\n        'basepath': basepath\n    }.items():\n        if v is not None:\n            entry = tensor.external_data.add()\n            entry.key = k\n            entry.value = str(v)",
  "def convert_model_to_external_data(model: ModelProto, all_tensors_to_one_file: bool = True, location: Optional[Text] = None, size_threshold: int = 1024, convert_attribute: bool = False) -> None:\n    \"\"\"\n    Call to set all tensors with raw data as external data. This call should preceed 'save_model'.\n    'save_model' saves all the tensors data as external data after calling this function.\n\n    Arguments:\n        model (ModelProto): Model to be converted.\n        all_tensors_to_one_file (bool): If true, save all tensors to one external file specified by location.\n            If false, save each tensor to a file named with the tensor name.\n        location: specify the external file that all tensors to save to.\n            If not specified, will use the model name.\n        size_threshold: Threshold for size of data. Only when tensor's data is >= the size_threshold\n            it will be converted to external data. To convert every tensor with raw data to external data set size_threshold=0.\n        convert_attribute (bool): If true, convert all tensors to external data\n                       If false, convert only non-attribute tensors to external data\n    \"\"\"\n    tensors = _get_initializer_tensors(model)\n    if convert_attribute:\n        tensors = _get_all_tensors(model)\n\n    if all_tensors_to_one_file:\n        file_name = Text(uuid.uuid1())\n        if location:\n            file_name = location\n        for tensor in tensors:\n            if tensor.HasField(\"raw_data\") and sys.getsizeof(tensor.raw_data) >= size_threshold:\n                set_external_data(tensor, file_name)\n    else:\n        for tensor in tensors:\n            if tensor.HasField(\"raw_data\") and sys.getsizeof(tensor.raw_data) >= size_threshold:\n                tensor_location = tensor.name\n                if not _is_valid_filename(tensor_location):\n                    tensor_location = Text(uuid.uuid1())\n                set_external_data(tensor, tensor_location)",
  "def convert_model_from_external_data(model: ModelProto) -> None:\n    \"\"\"\n    Call to set all tensors which use external data as embedded data. save_model saves all the tensors data as embedded data after calling this function.\n\n    Arguments:\n        model (ModelProto): Model to be converted.\n    \"\"\"\n    for tensor in _get_all_tensors(model):\n        if uses_external_data(tensor):\n            if not tensor.HasField(\"raw_data\"):\n                raise ValueError(\"raw_data field doesn't exist.\")\n            del tensor.external_data[:]\n            tensor.data_location = TensorProto.DEFAULT",
  "def save_external_data(tensor: TensorProto, base_path: Text) -> None:\n    \"\"\"\n    Writes tensor data to an external file according to information in the `external_data` field.\n\n    Arguments:\n        tensor (TensorProto): Tensor object to be serialized\n        base_path: System path of a folder where tensor data is to be stored\n    \"\"\"\n    info = ExternalDataInfo(tensor)\n    external_data_file_path = os.path.join(base_path, info.location)\n\n    # Retrieve the tensor's data from raw_data or load external file\n    if not tensor.HasField(\"raw_data\"):\n        raise ValueError(\"raw_data field doesn't exist.\")\n\n    # Create file if it doesn't exist\n    if not os.path.isfile(external_data_file_path):\n        open(external_data_file_path, 'ab').close()\n\n    # Open file for reading and writing at random locations ('r+b')\n    with open(external_data_file_path, 'r+b') as data_file:\n        data_file.seek(0, 2)\n        if info.offset is not None:\n            # Pad file to required offset if needed\n            file_size = data_file.tell()\n            if info.offset > file_size:\n                data_file.write(b\"\\0\" * (info.offset - file_size))\n\n            data_file.seek(info.offset)\n        offset = data_file.tell()\n        data_file.write(tensor.raw_data)\n        set_external_data(tensor, info.location, offset, data_file.tell() - offset)",
  "def _get_all_tensors(onnx_model_proto: ModelProto) -> Iterable[TensorProto]:\n    \"\"\"Scan an ONNX model for all tensors and return as an iterator.\"\"\"\n    return chain(_get_initializer_tensors(onnx_model_proto),\n                 _get_attribute_tensors(onnx_model_proto))",
  "def _recursive_attribute_processor(attribute: AttributeProto, func: Callable[[GraphProto], Iterable[TensorProto]]) -> Iterable[TensorProto]:\n    \"\"\"Create an iterator through processing ONNX model attributes with functor.\"\"\"\n    if attribute.type == AttributeProto.GRAPH:\n        yield from func(attribute.g)\n    if attribute.type == AttributeProto.GRAPHS:\n        for graph in attribute.graphs:\n            yield from func(graph)",
  "def _get_initializer_tensors_from_graph(onnx_model_proto_graph: GraphProto) -> Iterable[TensorProto]:\n    \"\"\"Create an iterator of initializer tensors from ONNX model graph.\"\"\"\n    for initializer in onnx_model_proto_graph.initializer:\n        yield initializer\n    for node in onnx_model_proto_graph.node:\n        for attribute in node.attribute:\n            yield from _recursive_attribute_processor(attribute, _get_initializer_tensors_from_graph)",
  "def _get_initializer_tensors(onnx_model_proto: ModelProto) -> Iterable[TensorProto]:\n    \"\"\"Create an iterator of initializer tensors from ONNX model.\"\"\"\n    yield from _get_initializer_tensors_from_graph(onnx_model_proto.graph)",
  "def _get_attribute_tensors_from_graph(onnx_model_proto_graph: GraphProto) -> Iterable[TensorProto]:\n    \"\"\"Create an iterator of tensors from node attributes of an ONNX model graph.\"\"\"\n    for node in onnx_model_proto_graph.node:\n        for attribute in node.attribute:\n            if attribute.HasField(\"t\"):\n                yield attribute.t\n            for tensor in attribute.tensors:\n                yield tensor\n            yield from _recursive_attribute_processor(attribute, _get_attribute_tensors_from_graph)",
  "def _get_attribute_tensors(onnx_model_proto: ModelProto) -> Iterable[TensorProto]:\n    \"\"\"Create an iterator of tensors from node attributes of an ONNX model.\"\"\"\n    yield from _get_attribute_tensors_from_graph(onnx_model_proto.graph)",
  "def _sanitize_path(path: Text) -> Text:\n    \"\"\"Remove path components which would allow traversing up a directory tree from a base path.\n\n    Note: This method is currently very basic and should be expanded.\n    \"\"\"\n    return path.lstrip('/.')",
  "def _is_valid_filename(filename: Text) -> bool:\n    \"\"\"Utility to check whether the provided filename is valid.\"\"\"\n    exp = re.compile(\"^[^<>:;,?\\\"*|/]+$\")\n    match = exp.match(filename)\n    if match:\n        return True\n    else:\n        return False",
  "def uses_external_data(tensor: TensorProto) -> bool:\n    \"\"\"Returns true if the tensor stores data in an external location.\"\"\"\n    return tensor.HasField(\"data_location\") and tensor.data_location == TensorProto.EXTERNAL",
  "def remove_external_data_field(tensor: TensorProto, field_key: Text) -> None:\n    \"\"\"\n    Removes a field from a Tensor's external_data key-value store.\n\n    Modifies tensor object in place.\n\n    Arguments:\n        tensor (TensorProto): Tensor object from which value will be removed\n        field_key (string): The key of the field to be removed\n    \"\"\"\n    for (i, field) in enumerate(tensor.external_data):\n        if field.key == field_key:\n            del tensor.external_data[i]",
  "def write_external_data_tensors(model: ModelProto, filepath: Text) -> ModelProto:\n    \"\"\"\n    Serializes data for all the tensors which have data location set to TensorProto.External.\n\n    Note: This function also strips basepath information from all tensors' external_data fields.\n\n    Arguments:\n        model (ModelProto): Model object which is the source of tensors to serialize.\n        filepath: System path to the directory which should be treated as base path for external data.\n\n    Returns:\n        ModelProto: The modified model object.\n    \"\"\"\n    for tensor in _get_all_tensors(model):\n        # Writing to external data happens in 2 passes:\n        # 1. Tensors with raw data which pass the necessary conditions (size threshold etc) are marked for serialization\n        # 2. The raw data in these tensors is serialized to a file\n        # Thus serialize only if tensor has raw data and it was marked for serialization\n        if uses_external_data(tensor) and tensor.HasField(\"raw_data\"):\n            save_external_data(tensor, filepath)\n            tensor.ClearField(str('raw_data'))\n\n    return model",
  "def __init__(self, tensor: TensorProto) -> None:\n        self.location = ''\n        self.offset = None\n        self.length = None\n        self.checksum = None\n        self.basepath = ''\n\n        for entry in tensor.external_data:\n            setattr(self, entry.key, entry.value)\n\n        if self.offset:\n            self.offset = int(self.offset)\n\n        if self.length:\n            self.length = int(self.length)",
  "class DeviceType(object):\n    _Type = NewType('_Type', int)\n    CPU: _Type = _Type(0)\n    CUDA: _Type = _Type(1)",
  "class Device(object):\n    '''\n    Describes device type and device id\n    syntax: device_type:device_id(optional)\n    example: 'CPU', 'CUDA', 'CUDA:1'\n    '''\n\n    def __init__(self, device: Text) -> None:\n        options = device.split(':')\n        self.type = getattr(DeviceType, options[0])\n        self.device_id = 0\n        if len(options) > 1:\n            self.device_id = int(options[1])",
  "def namedtupledict(typename: Text, field_names: Sequence[Text], *args: Any, **kwargs: Any) -> Type[Tuple[Any, ...]]:\n    field_names_map = {n: i for i, n in enumerate(field_names)}\n    # Some output names are invalid python identifier, e.g. \"0\"\n    kwargs.setdefault(str('rename'), True)\n    data = namedtuple(typename, field_names, *args, **kwargs)  # type: ignore\n\n    def getitem(self: Any, key: Any) -> Any:\n        if isinstance(key, str):\n            key = field_names_map[key]\n        return super(type(self), self).__getitem__(key)  # type: ignore\n    setattr(data, \"__getitem__\", getitem)\n    return data",
  "class BackendRep(object):\n    def run(self, inputs: Any, **kwargs: Any) -> Tuple[Any, ...]:\n        pass",
  "class Backend(object):\n    @classmethod\n    def is_compatible(cls,\n                      model: ModelProto,\n                      device: Text = 'CPU',\n                      **kwargs: Any\n                      ) -> bool:\n        # Return whether the model is compatible with the backend.\n        return True\n\n    @classmethod\n    def prepare(cls,\n                model: ModelProto,\n                device: Text = 'CPU',\n                **kwargs: Any\n                ) -> Optional[BackendRep]:\n        # TODO Remove Optional from return type\n        onnx.checker.check_model(model)\n        return None\n\n    @classmethod\n    def run_model(cls,\n                  model: ModelProto,\n                  inputs: Any,\n                  device: Text = 'CPU',\n                  **kwargs: Any\n                  ) -> Tuple[Any, ...]:\n        backend = cls.prepare(model, device, **kwargs)\n        assert backend is not None\n        return backend.run(inputs)\n\n    @classmethod\n    def run_node(cls,\n                 node: NodeProto,\n                 inputs: Any,\n                 device: Text = 'CPU',\n                 outputs_info: Optional[Sequence[Tuple[numpy.dtype, Tuple[int, ...]]]] = None,\n                 **kwargs: Dict[Text, Any]\n                 ) -> Optional[Tuple[Any, ...]]:\n        '''Simple run one operator and return the results.\n        Args:\n            outputs_info: a list of tuples, which contains the element type and\n            shape of each output. First element of the tuple is the dtype, and\n            the second element is the shape. More use case can be found in\n            https://github.com/onnx/onnx/blob/main/onnx/backend/test/runner/__init__.py\n        '''\n        # TODO Remove Optional from return type\n        if 'opset_version' in kwargs:\n            special_context = c_checker.CheckerContext()\n            special_context.ir_version = IR_VERSION\n            special_context.opset_imports = {'': kwargs['opset_version']}  # type: ignore\n            onnx.checker.check_node(node, special_context)\n        else:\n            onnx.checker.check_node(node)\n        return None\n\n    @classmethod\n    def supports_device(cls, device: Text) -> bool:\n        \"\"\"\n        Checks whether the backend is compiled with particular device support.\n        In particular it's used in the testing suite.\n        \"\"\"\n        return True",
  "def __init__(self, device: Text) -> None:\n        options = device.split(':')\n        self.type = getattr(DeviceType, options[0])\n        self.device_id = 0\n        if len(options) > 1:\n            self.device_id = int(options[1])",
  "def getitem(self: Any, key: Any) -> Any:\n        if isinstance(key, str):\n            key = field_names_map[key]\n        return super(type(self), self).__getitem__(key)",
  "def run(self, inputs: Any, **kwargs: Any) -> Tuple[Any, ...]:\n        pass",
  "def is_compatible(cls,\n                      model: ModelProto,\n                      device: Text = 'CPU',\n                      **kwargs: Any\n                      ) -> bool:\n        # Return whether the model is compatible with the backend.\n        return True",
  "def prepare(cls,\n                model: ModelProto,\n                device: Text = 'CPU',\n                **kwargs: Any\n                ) -> Optional[BackendRep]:\n        # TODO Remove Optional from return type\n        onnx.checker.check_model(model)\n        return None",
  "def run_model(cls,\n                  model: ModelProto,\n                  inputs: Any,\n                  device: Text = 'CPU',\n                  **kwargs: Any\n                  ) -> Tuple[Any, ...]:\n        backend = cls.prepare(model, device, **kwargs)\n        assert backend is not None\n        return backend.run(inputs)",
  "def run_node(cls,\n                 node: NodeProto,\n                 inputs: Any,\n                 device: Text = 'CPU',\n                 outputs_info: Optional[Sequence[Tuple[numpy.dtype, Tuple[int, ...]]]] = None,\n                 **kwargs: Dict[Text, Any]\n                 ) -> Optional[Tuple[Any, ...]]:\n        '''Simple run one operator and return the results.\n        Args:\n            outputs_info: a list of tuples, which contains the element type and\n            shape of each output. First element of the tuple is the dtype, and\n            the second element is the shape. More use case can be found in\n            https://github.com/onnx/onnx/blob/main/onnx/backend/test/runner/__init__.py\n        '''\n        # TODO Remove Optional from return type\n        if 'opset_version' in kwargs:\n            special_context = c_checker.CheckerContext()\n            special_context.ir_version = IR_VERSION\n            special_context.opset_imports = {'': kwargs['opset_version']}  # type: ignore\n            onnx.checker.check_node(node, special_context)\n        else:\n            onnx.checker.check_node(node)\n        return None",
  "def supports_device(cls, device: Text) -> bool:\n        \"\"\"\n        Checks whether the backend is compiled with particular device support.\n        In particular it's used in the testing suite.\n        \"\"\"\n        return True",
  "def abs(input: np.ndarray) -> np.ndarray:\n    return np.abs(input)",
  "def collect_sample_implementations() -> Dict[Text, Text]:\n    dict: Dict[Text, Text] = {}\n    _recursive_scan(sys.modules[__name__], dict)\n    return dict",
  "def _recursive_scan(package: ModuleType, dict: Dict[Text, Text]) -> None:\n    pkg_dir = package.__path__  # type: ignore\n    module_location = package.__name__\n    for _module_loader, name, ispkg in pkgutil.iter_modules(pkg_dir):  # type: ignore\n        module_name = \"{}.{}\".format(module_location, name)  # Module/package\n        module = importlib.import_module(module_name)\n        dict[name] = inspect.getsource(module)\n        if ispkg:\n            _recursive_scan(module, dict)",
  "def check_model() -> None:\n    parser = argparse.ArgumentParser('check-model')\n    parser.add_argument('model_pb', type=argparse.FileType('rb'))\n    args = parser.parse_args()\n\n    model = load(args.model_pb)\n    checker.check_model(model)",
  "def check_node() -> None:\n    parser = argparse.ArgumentParser('check-node')\n    parser.add_argument('node_pb', type=argparse.FileType('rb'))\n    args = parser.parse_args()\n\n    node = NodeProto()\n    node.ParseFromString(args.node_pb.read())\n    checker.check_node(node)",
  "def _escape_label(name: Text) -> Text:\n    # json.dumps is poor man's escaping\n    return json.dumps(name)",
  "def _form_and_sanitize_docstring(s: Text) -> Text:\n    url = 'javascript:alert('\n    url += _escape_label(s).replace('\"', '\\'').replace('<', '').replace('>', '')\n    url += ')'\n    return url",
  "def GetOpNodeProducer(embed_docstring: bool = False, **kwargs: Any) -> _NodeProducer:\n    def ReallyGetOpNode(op: NodeProto, op_id: int) -> pydot.Node:\n        if op.name:\n            node_name = '%s/%s (op#%d)' % (op.name, op.op_type, op_id)\n        else:\n            node_name = '%s (op#%d)' % (op.op_type, op_id)\n        for i, input in enumerate(op.input):\n            node_name += '\\n input' + str(i) + ' ' + input\n        for i, output in enumerate(op.output):\n            node_name += '\\n output' + str(i) + ' ' + output\n        node = pydot.Node(node_name, **kwargs)\n        if embed_docstring:\n            url = _form_and_sanitize_docstring(op.doc_string)\n            node.set_URL(url)\n        return node\n    return ReallyGetOpNode",
  "def GetPydotGraph(\n    graph: GraphProto,\n    name: Optional[Text] = None,\n    rankdir: Text = 'LR',\n    node_producer: Optional[_NodeProducer] = None,\n    embed_docstring: bool = False,\n) -> pydot.Dot:\n    if node_producer is None:\n        node_producer = GetOpNodeProducer(embed_docstring=embed_docstring, **OP_STYLE)\n    pydot_graph = pydot.Dot(name, rankdir=rankdir)\n    pydot_nodes: Dict[Text, pydot.Node] = {}\n    pydot_node_counts: Dict[Text, int] = defaultdict(int)\n    for op_id, op in enumerate(graph.node):\n        op_node = node_producer(op, op_id)\n        pydot_graph.add_node(op_node)\n        for input_name in op.input:\n            if input_name not in pydot_nodes:\n                input_node = pydot.Node(\n                    _escape_label(\n                        input_name + str(pydot_node_counts[input_name])),\n                    label=_escape_label(input_name),\n                    **BLOB_STYLE\n                )\n                pydot_nodes[input_name] = input_node\n            else:\n                input_node = pydot_nodes[input_name]\n            pydot_graph.add_node(input_node)\n            pydot_graph.add_edge(pydot.Edge(input_node, op_node))\n        for output_name in op.output:\n            if output_name in pydot_nodes:\n                pydot_node_counts[output_name] += 1\n            output_node = pydot.Node(\n                _escape_label(\n                    output_name + str(pydot_node_counts[output_name])),\n                label=_escape_label(output_name),\n                **BLOB_STYLE\n            )\n            pydot_nodes[output_name] = output_node\n            pydot_graph.add_node(output_node)\n            pydot_graph.add_edge(pydot.Edge(op_node, output_node))\n    return pydot_graph",
  "def main() -> None:\n    parser = argparse.ArgumentParser(description=\"ONNX net drawer\")\n    parser.add_argument(\n        \"--input\",\n        type=Text, required=True,\n        help=\"The input protobuf file.\",\n    )\n    parser.add_argument(\n        \"--output\",\n        type=Text, required=True,\n        help=\"The output protobuf file.\",\n    )\n    parser.add_argument(\n        \"--rankdir\", type=Text, default='LR',\n        help=\"The rank direction of the pydot graph.\",\n    )\n    parser.add_argument(\n        \"--embed_docstring\", action=\"store_true\",\n        help=\"Embed docstring as javascript alert. Useful for SVG format.\",\n    )\n    args = parser.parse_args()\n    model = ModelProto()\n    with open(args.input, 'rb') as fid:\n        content = fid.read()\n        model.ParseFromString(content)\n    pydot_graph = GetPydotGraph(\n        model.graph,\n        name=model.graph.name,\n        rankdir=args.rankdir,\n        node_producer=GetOpNodeProducer(\n            embed_docstring=args.embed_docstring,\n            **OP_STYLE\n        ),\n    )\n    pydot_graph.write_dot(args.output)",
  "def ReallyGetOpNode(op: NodeProto, op_id: int) -> pydot.Node:\n        if op.name:\n            node_name = '%s/%s (op#%d)' % (op.name, op.op_type, op_id)\n        else:\n            node_name = '%s (op#%d)' % (op.op_type, op_id)\n        for i, input in enumerate(op.input):\n            node_name += '\\n input' + str(i) + ' ' + input\n        for i, output in enumerate(op.output):\n            node_name += '\\n output' + str(i) + ' ' + output\n        node = pydot.Node(node_name, **kwargs)\n        if embed_docstring:\n            url = _form_and_sanitize_docstring(op.doc_string)\n            node.set_URL(url)\n        return node",
  "def update_inputs_outputs_dims(model: ModelProto, input_dims: Dict[Text, List[Any]], output_dims: Dict[Text, List[Any]]) -> ModelProto:\n    \"\"\"\n        This function updates the dimension sizes of the model's inputs and outputs to the values\n        provided in input_dims and output_dims. if the dim value provided is negative, a unique dim_param\n        will be set for that dimension.\n\n        Example. if we have the following shape for inputs and outputs:\n                shape(input_1) = ('b', 3, 'w', 'h')\n                shape(input_2) = ('b', 4)\n                and shape(output)  = ('b', 'd', 5)\n\n            The parameters can be provided as:\n                input_dims = {\n                    \"input_1\": ['b', 3, 'w', 'h'],\n                    \"input_2\": ['b', 4],\n                }\n                output_dims = {\n                    \"output\": ['b', -1, 5]\n                }\n\n            Putting it together:\n                model = onnx.load('model.onnx')\n                updated_model = update_inputs_outputs_dims(model, input_dims, output_dims)\n                onnx.save(updated_model, 'model.onnx')\n    \"\"\"\n    dim_param_set: Set[Text] = set()\n\n    def init_dim_param_set(dim_param_set: Set[Text], value_infos: List[ValueInfoProto]) -> None:\n        for info in value_infos:\n            shape = info.type.tensor_type.shape\n            for dim in shape.dim:\n                if dim.HasField('dim_param'):\n                    dim_param_set.add(dim.dim_param)  # type: ignore\n\n    init_dim_param_set(dim_param_set, model.graph.input)  # type: ignore\n    init_dim_param_set(dim_param_set, model.graph.output)  # type: ignore\n    init_dim_param_set(dim_param_set, model.graph.value_info)  # type: ignore\n\n    def update_dim(tensor: ValueInfoProto, dim: Any, j: int, name: Text) -> None:\n        dim_proto = tensor.type.tensor_type.shape.dim[j]\n        if isinstance(dim, int):\n            if dim >= 0:\n                if dim_proto.HasField('dim_value') and dim_proto.dim_value != dim:\n                    raise ValueError('Unable to set dimension value to {} for axis {} of {}. Contradicts existing dimension value {}.'\n                        .format(dim, j, name, dim_proto.dim_value))\n                dim_proto.dim_value = dim\n            else:\n                generated_dim_param = name + '_' + str(j)\n                if generated_dim_param in dim_param_set:\n                    raise ValueError('Unable to generate unique dim_param for axis {} of {}. Please manually provide a dim_param value.'\n                        .format(j, name))\n                dim_proto.dim_param = generated_dim_param\n        elif isinstance(dim, str):\n            dim_proto.dim_param = dim\n        else:\n            raise ValueError('Only int or str is accepted as dimension value, incorrect type: {}'.format(type(dim)))\n\n    for input in model.graph.input:\n        input_name = input.name\n        input_dim_arr = input_dims[input_name]\n        for j, dim in enumerate(input_dim_arr):\n            update_dim(input, dim, j, input_name)\n\n    for output in model.graph.output:\n        output_name = output.name\n        output_dim_arr = output_dims[output_name]\n        for j, dim in enumerate(output_dim_arr):\n            update_dim(output, dim, j, output_name)\n\n    onnx.checker.check_model(model)\n    return model",
  "def init_dim_param_set(dim_param_set: Set[Text], value_infos: List[ValueInfoProto]) -> None:\n        for info in value_infos:\n            shape = info.type.tensor_type.shape\n            for dim in shape.dim:\n                if dim.HasField('dim_param'):\n                    dim_param_set.add(dim.dim_param)",
  "def update_dim(tensor: ValueInfoProto, dim: Any, j: int, name: Text) -> None:\n        dim_proto = tensor.type.tensor_type.shape.dim[j]\n        if isinstance(dim, int):\n            if dim >= 0:\n                if dim_proto.HasField('dim_value') and dim_proto.dim_value != dim:\n                    raise ValueError('Unable to set dimension value to {} for axis {} of {}. Contradicts existing dimension value {}.'\n                        .format(dim, j, name, dim_proto.dim_value))\n                dim_proto.dim_value = dim\n            else:\n                generated_dim_param = name + '_' + str(j)\n                if generated_dim_param in dim_param_set:\n                    raise ValueError('Unable to generate unique dim_param for axis {} of {}. Please manually provide a dim_param value.'\n                        .format(j, name))\n                dim_proto.dim_param = generated_dim_param\n        elif isinstance(dim, str):\n            dim_proto.dim_param = dim\n        else:\n            raise ValueError('Only int or str is accepted as dimension value, incorrect type: {}'.format(type(dim)))",
  "def display_number(v: int) -> Text:\n    if defs.OpSchema.is_infinite(v):\n        return '&#8734;'\n    return Text(v)",
  "def should_render_domain(domain: Text) -> bool:\n    if domain == ONNX_ML_DOMAIN and not ONNX_ML:\n        return False\n    if ONNX_ML and domain != ONNX_ML_DOMAIN:\n        return False\n    return True",
  "def format_name_with_domain(domain: Text, schema_name: Text) -> Text:\n    if domain:\n        return '{}.{}'.format(domain, schema_name)\n    return schema_name",
  "def format_versions(versions: Sequence[OpSchema]) -> Text:\n    return '{}'.format(', '.join(display_version_link(format_name_with_domain(v.domain, v.name),\n                                               v.since_version) for v in versions[::-1]))",
  "def display_attr_type(v: OpSchema.AttrType) -> Text:\n    assert isinstance(v, OpSchema.AttrType)\n    s = Text(v)\n    s = s[s.rfind('.') + 1:].lower()\n    if s[-1] == 's':\n        s = 'list of ' + s\n    return s",
  "def display_domain(domain: Text) -> Text:\n    if domain:\n        return \"the '{}' operator set\".format(domain)\n    return \"the default ONNX operator set\"",
  "def display_domain_short(domain: Text) -> Text:\n    if domain:\n        return domain\n    return 'ai.onnx (default)'",
  "def display_version_link(name: Text, version: int) -> Text:\n    changelog_md = 'Changelog' + ext\n    name_with_ver = '{}-{}'.format(name, version)\n    return '<a href=\"{}#{}\">{}</a>'.format(changelog_md, name_with_ver, version)",
  "def generate_formal_parameter_tags(formal_parameter: OpSchema.FormalParameter) -> Text:\n    tags: List[Text] = []\n    if OpSchema.FormalParameterOption.Optional == formal_parameter.option:\n        tags = [\"optional\"]\n    elif OpSchema.FormalParameterOption.Variadic == formal_parameter.option:\n        if formal_parameter.isHomogeneous:\n            tags = [\"variadic\"]\n        else:\n            tags = [\"variadic\", \"heterogeneous\"]\n    differentiable: OpSchema.DifferentiationCategory = OpSchema.DifferentiationCategory.Differentiable\n    non_differentiable: OpSchema.DifferentiationCategory = OpSchema.DifferentiationCategory.NonDifferentiable\n    if differentiable == formal_parameter.differentiationCategory:\n        tags.append('differentiable')\n    elif non_differentiable == formal_parameter.differentiationCategory:\n        tags.append('non-differentiable')\n\n    return '' if len(tags) == 0 else ' (' + ', '.join(tags) + ')'",
  "def display_schema(schema: OpSchema, versions: Sequence[OpSchema]) -> Text:\n    s = ''\n\n    # doc\n    if schema.doc:\n        s += '\\n'\n        s += '\\n'.join(('  ' + line).rstrip()\n                       for line in schema.doc.lstrip().splitlines())\n        s += '\\n'\n\n    # since version\n    s += '\\n#### Version\\n'\n    if schema.support_level == OpSchema.SupportType.EXPERIMENTAL:\n        s += '\\nNo versioning maintained for experimental ops.'\n    else:\n        s += '\\nThis version of the operator has been ' + ('deprecated' if schema.deprecated else 'available') + ' since version {}'.format(schema.since_version)\n        s += ' of {}.\\n'.format(display_domain(schema.domain))\n        if len(versions) > 1:\n            # TODO: link to the Changelog.md\n            s += '\\nOther versions of this operator: {}\\n'.format(\n                ', '.join(display_version_link(format_name_with_domain(v.domain, v.name),\n                                               v.since_version) for v in versions[:-1]))\n\n    # If this schema is deprecated, don't display any of the following sections\n    if schema.deprecated:\n        return s\n\n    # attributes\n    if schema.attributes:\n        s += '\\n#### Attributes\\n\\n'\n        s += '<dl>\\n'\n        for _, attr in sorted(schema.attributes.items()):\n            # option holds either required or default value\n            opt = ''\n            if attr.required:\n                opt = 'required'\n            elif attr.default_value.name:\n                default_value = helper.get_attribute_value(attr.default_value)\n\n                def format_value(value: Any) -> Text:\n                    if isinstance(value, float):\n                        formatted = str(np.round(value, 5))\n                        # use default formatting, unless too long.\n                        if (len(formatted) > 10):\n                            formatted = str(\"({:e})\".format(value))\n                        return formatted\n                    elif isinstance(value, (bytes, bytearray)):\n                        return str(value.decode('utf-8'))\n                    return str(value)\n\n                if isinstance(default_value, list):\n                    default_value = [format_value(val) for val in default_value]\n                else:\n                    default_value = format_value(default_value)\n                opt = 'default is {}'.format(default_value)\n\n            s += '<dt><tt>{}</tt> : {}{}</dt>\\n'.format(\n                attr.name,\n                display_attr_type(attr.type),\n                ' ({})'.format(opt) if opt else '')\n            s += '<dd>{}</dd>\\n'.format(attr.description)\n        s += '</dl>\\n'\n\n    # inputs\n    s += '\\n#### Inputs'\n    if schema.min_input != schema.max_input:\n        s += ' ({} - {})'.format(display_number(schema.min_input),\n                                 display_number(schema.max_input))\n    s += '\\n\\n'\n    if schema.inputs:\n        s += '<dl>\\n'\n        for input in schema.inputs:\n            option_str = generate_formal_parameter_tags(input)\n            s += '<dt><tt>{}</tt>{} : {}</dt>\\n'.format(input.name, option_str, input.typeStr)\n            s += '<dd>{}</dd>\\n'.format(input.description)\n        s += '</dl>\\n'\n\n    # outputs\n    s += '\\n#### Outputs'\n    if schema.min_output != schema.max_output:\n        s += ' ({} - {})'.format(display_number(schema.min_output),\n                                 display_number(schema.max_output))\n    s += '\\n\\n'\n\n    if schema.outputs:\n        s += '<dl>\\n'\n        for output in schema.outputs:\n            option_str = generate_formal_parameter_tags(output)\n            s += '<dt><tt>{}</tt>{} : {}</dt>\\n'.format(output.name, option_str, output.typeStr)\n            s += '<dd>{}</dd>\\n'.format(output.description)\n        s += '</dl>\\n'\n\n    # type constraints\n    s += '\\n#### Type Constraints'\n    s += '\\n\\n'\n    if schema.type_constraints:\n        s += '<dl>\\n'\n        for type_constraint in schema.type_constraints:\n            allowedTypes = type_constraint.allowed_type_strs\n            if (len(allowedTypes) > 0):\n                allowedTypeStr = allowedTypes[0]\n            for allowedType in allowedTypes[1:]:\n                allowedTypeStr += ', ' + allowedType\n            s += '<dt><tt>{}</tt> : {}</dt>\\n'.format(\n                type_constraint.type_param_str, allowedTypeStr)\n            s += '<dd>{}</dd>\\n'.format(type_constraint.description)\n        s += '</dl>\\n'\n\n    # Function Body\n    # TODO: this should be refactored to show the function body graph's picture (DAG).\n    #if schema.has_function or schema.has_context_dependent_function:  # type: ignore\n    #    s += '\\n#### Function\\n'\n    #    s += '\\nThe Function can be represented as a function.\\n'\n\n    return s",
  "def support_level_str(level: OpSchema.SupportType) -> Text:\n    return \\\n        \"<sub>experimental</sub> \" if level == OpSchema.SupportType.EXPERIMENTAL else \"\"",
  "class Args(NamedTuple):\n    output: str\n    changelog: str",
  "def main(args: Args) -> None:\n    with io.open(args.changelog, 'w', newline='') as fout:\n        fout.write('<!--- SPDX-License-Identifier: Apache-2.0 -->\\n')\n        fout.write('## Operator Changelog\\n')\n        fout.write(\n            \"*This file is automatically generated from the\\n\"\n            \"            [def files](/onnx/defs) via [this script](/onnx/defs/gen_doc.py).\\n\"\n            \"            Do not modify directly and instead edit operator definitions.*\\n\"\n            \"\\n\"\n            \"For an operator input/output's differentiability, it can be differentiable,\\n\"\n            \"            non-differentiable, or undefined. If a variable's differentiability\\n\"\n            \"            is not specified, that variable has undefined differentiability.\\n\")\n\n        # domain -> version -> [schema]\n        dv_index: Dict[Text, Dict[int, List[OpSchema]]] = defaultdict(lambda: defaultdict(list))\n        for schema in defs.get_all_schemas_with_history():\n            dv_index[schema.domain][schema.since_version].append(schema)\n\n        fout.write('\\n')\n\n        for domain, versionmap in sorted(dv_index.items()):\n            if not should_render_domain(domain):\n                continue\n\n            s = '# {}\\n'.format(display_domain_short(domain))\n\n            for version, unsorted_schemas in sorted(versionmap.items()):\n                s += '## Version {} of {}\\n'.format(version, display_domain(domain))\n                for schema in sorted(unsorted_schemas, key=lambda s: s.name):\n                    name_with_ver = '{}-{}'.format(format_name_with_domain(domain, schema.name),\n                                                   schema.since_version)\n                    s += ('### <a name=\"{}\"></a>**{}**' + (' (deprecated)' if schema.deprecated else '') + '</a>\\n').format(name_with_ver, name_with_ver)\n                    s += display_schema(schema, [schema])\n                    s += '\\n'\n\n            fout.write(s)\n\n    with io.open(args.output, 'w', newline='', encoding=\"utf-8\") as fout:\n        fout.write('<!--- SPDX-License-Identifier: Apache-2.0 -->\\n')\n        fout.write('## Operator Schemas\\n')\n        fout.write(\n            \"*This file is automatically generated from the\\n\"\n            \"            [def files](/onnx/defs) via [this script](/onnx/defs/gen_doc.py).\\n\"\n            \"            Do not modify directly and instead edit operator definitions.*\\n\"\n            \"\\n\"\n            \"For an operator input/output's differentiability, it can be differentiable,\\n\"\n            \"            non-differentiable, or undefined. If a variable's differentiability\\n\"\n            \"            is not specified, that variable has undefined differentiability.\\n\")\n\n        # domain -> support level -> name -> [schema]\n        index: Dict[Text, Dict[int, Dict[Text, List[OpSchema]]]] = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n        for schema in defs.get_all_schemas_with_history():\n            index[schema.domain][int(schema.support_level)][schema.name].append(schema)\n\n        fout.write('\\n')\n\n        # Preprocess the Operator Schemas\n        # [(domain, [(support_level, [(schema name, current schema, all versions schemas)])])]\n        operator_schemas: List[Tuple[Text, List[Tuple[int, List[Tuple[Text, OpSchema, List[OpSchema]]]]]]] = list()\n        existing_ops: Set[Text] = set()\n        for domain, _supportmap in sorted(index.items()):\n            if not should_render_domain(domain):\n                continue\n\n            processed_supportmap = list()\n            for _support, _namemap in sorted(_supportmap.items()):\n                processed_namemap = list()\n                for n, unsorted_versions in sorted(_namemap.items()):\n                    versions = sorted(unsorted_versions, key=lambda s: s.since_version)\n                    schema = versions[-1]\n                    if schema.name in existing_ops:\n                        continue\n                    existing_ops.add(schema.name)\n                    processed_namemap.append((n, schema, versions))\n                processed_supportmap.append((_support, processed_namemap))\n            operator_schemas.append((domain, processed_supportmap))\n\n        # Table of contents\n        for domain, supportmap in operator_schemas:\n            s = '### {}\\n'.format(display_domain_short(domain))\n            fout.write(s)\n\n            fout.write('|**Operator**|**Since version**|\\n')\n            fout.write('|-|-|\\n')\n\n            function_ops = list()\n            for _, namemap in supportmap:\n                for n, schema, versions in namemap:\n                    if schema.has_function or schema.has_context_dependent_function:  # type: ignore\n                        function_ops.append((n, schema, versions))\n                        continue\n                    s = '|{}<a href=\"#{}\">{}</a>{}|{}|\\n'.format(\n                        support_level_str(schema.support_level),\n                        format_name_with_domain(domain, n),\n                        format_name_with_domain(domain, n),\n                        ' (deprecated)' if schema.deprecated else '',\n                        format_versions(versions))\n                    fout.write(s)\n            if len(function_ops):\n                fout.write('|**Function**|**Since version**|\\n')\n                for n, schema, versions in function_ops:\n                    s = '|{}<a href=\"#{}\">{}</a>|{}|\\n'.format(\n                        support_level_str(schema.support_level),\n                        format_name_with_domain(domain, n),\n                        format_name_with_domain(domain, n),\n                        format_versions(versions))\n                    fout.write(s)\n\n            fout.write('\\n')\n\n        fout.write('\\n')\n\n        for domain, supportmap in operator_schemas:\n            s = '## {}\\n'.format(display_domain_short(domain))\n            fout.write(s)\n\n            for _, namemap in supportmap:\n                for op_type, schema, versions in namemap:\n                    # op_type\n                    s = ('### {}<a name=\"{}\"></a><a name=\"{}\">**{}**' + (' (deprecated)' if schema.deprecated else '') + '</a>\\n').format(\n                        support_level_str(schema.support_level),\n                        format_name_with_domain(domain, op_type),\n                        format_name_with_domain(domain, op_type.lower()),\n                        format_name_with_domain(domain, op_type))\n\n                    s += display_schema(schema, versions)\n\n                    s += '\\n\\n'\n\n                    if op_type in SNIPPETS:\n                        s += '#### Examples\\n\\n'\n                        for summary, code in sorted(SNIPPETS[op_type]):\n                            s += '<details>\\n'\n                            s += '<summary>{}</summary>\\n\\n'.format(summary)\n                            s += '```python\\n{}\\n```\\n\\n'.format(code)\n                            s += '</details>\\n'\n                            s += '\\n\\n'\n                    if op_type.lower() in SAMPLE_IMPLEMENTATIONS:\n                        s += '#### Sample Implementation\\n\\n'\n                        s += '<details>\\n'\n                        s += '<summary>{}</summary>\\n\\n'.format(op_type)\n                        s += '```python\\n{}\\n```\\n\\n'.format(SAMPLE_IMPLEMENTATIONS[op_type.lower()])\n                        s += '</details>\\n'\n                        s += '\\n\\n'\n\n                    fout.write(s)",
  "def format_value(value: Any) -> Text:\n                    if isinstance(value, float):\n                        formatted = str(np.round(value, 5))\n                        # use default formatting, unless too long.\n                        if (len(formatted) > 10):\n                            formatted = str(\"({:e})\".format(value))\n                        return formatted\n                    elif isinstance(value, (bytes, bytearray)):\n                        return str(value.decode('utf-8'))\n                    return str(value)",
  "def main() -> None:\n    # domain -> support level -> name -> [schema]\n    with_inference = []\n    without_inference = []\n    for schema in defs.get_all_schemas():\n        domain, name, has_inference = schema.domain, schema.name, schema.has_type_and_shape_inference_function\n        elem = (domain, name)\n        if has_inference:\n            with_inference.append(elem)\n        else:\n            without_inference.append(elem)\n    print(len(with_inference), 'operators have a type/shape inference function.')\n    print(len(without_inference), 'do not. These are:')\n    for domain, name in sorted(without_inference):\n        print(domain, name)",
  "def onnx_opset_version() -> int:\n    return C.schema_version_map()[ONNX_DOMAIN][1]",
  "def _Function_proto(self):  # type: ignore\n    func_proto = FunctionProto()\n    func_proto.ParseFromString(self._function_body)\n    return func_proto",
  "def _Attribute_default_value(self):  # type: ignore\n    attr = AttributeProto()\n    attr.ParseFromString(self._default_value)\n    return attr",
  "def get_function_ops() -> List[OpSchema]:\n    schemas = C.get_all_schemas()\n    return [schema for schema in schemas if schema.has_function or schema.has_context_dependent_function]",
  "def argsparser():\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        'tools',\n        choices=['compress', 'convert', 'simple_serving', 'paddle2coreml'])\n    ## argumentments for auto compression\n    parser.add_argument(\n        '--config_path',\n        type=str,\n        default=None,\n        help=\"path of compression strategy config.\")\n    parser.add_argument(\n        '--method',\n        type=str,\n        default=None,\n        help=\"choose PTQ or QAT as quantization method\")\n    parser.add_argument(\n        '--save_dir',\n        type=str,\n        default='./output',\n        help=\"directory to save model.\")\n    parser.add_argument(\n        '--devices',\n        type=str,\n        default='gpu',\n        help=\"which device used to compress.\")\n    ## arguments for other x2paddle\n    parser.add_argument(\n        '--framework',\n        type=str,\n        default=None,\n        help=\"define which deeplearning framework(tensorflow/caffe/onnx)\")\n    parser.add_argument(\n        '--model',\n        type=str,\n        default=None,\n        help=\"define model file path for tensorflow or onnx\")\n    parser.add_argument(\n        \"--prototxt\",\n        \"-p\",\n        type=str,\n        default=None,\n        help=\"prototxt file of caffe model\")\n    parser.add_argument(\n        \"--weight\",\n        \"-w\",\n        type=str,\n        default=None,\n        help=\"weight file of caffe model\")\n    parser.add_argument(\n        \"--caffe_proto\",\n        \"-c\",\n        type=str,\n        default=None,\n        help=\"optional: the .py file compiled by caffe proto file of caffe model\"\n    )\n    parser.add_argument(\n       \"--input_shape_dict\",\n       \"-isd\",\n       type=str,\n       default=None,\n       help=\"define input shapes, e.g --input_shape_dict=\\\"{'image':[1, 3, 608, 608]}\\\" or\" \\\n       \"--input_shape_dict=\\\"{'image':[1, 3, 608, 608], 'im_shape': [1, 2], 'scale_factor': [1, 2]}\\\"\")\n    parser.add_argument(\n        \"--enable_code_optim\",\n        \"-co\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"Turn on code optimization\")\n    ## arguments for simple serving\n    parser.add_argument(\n        \"--app\",\n        type=str,\n        default=\"server:app\",\n        help=\"Simple serving app string\")\n    parser.add_argument(\n        \"--host\",\n        type=str,\n        default=\"127.0.0.1\",\n        help=\"Simple serving host IP address\")\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Simple serving host port\")\n    ## arguments for paddle2coreml\n    parser.add_argument(\n        \"--p2c_paddle_model_dir\",\n        type=str,\n        default=None,\n        help=\"define paddle model path\")\n    parser.add_argument(\n        \"--p2c_coreml_model_dir\",\n        type=str,\n        default=None,\n        help=\"define generated coreml model path\")\n    parser.add_argument(\n        \"--p2c_coreml_model_name\",\n        type=str,\n        default=\"coreml_model\",\n        help=\"define generated coreml model name\")\n    parser.add_argument(\n        \"--p2c_input_names\", type=str, default=None, help=\"define input names\")\n    parser.add_argument(\n        \"--p2c_input_dtypes\",\n        type=str,\n        default=\"float32\",\n        help=\"define input dtypes\")\n    parser.add_argument(\n        \"--p2c_input_shapes\",\n        type=str,\n        default=None,\n        help=\"define input shapes\")\n    parser.add_argument(\n        \"--p2c_output_names\",\n        type=str,\n        default=None,\n        help=\"define output names\")\n    ## arguments for other tools\n    return parser",
  "def main():\n    args = argsparser().parse_args()\n    if args.tools == \"compress\":\n        from .auto_compression.fd_auto_compress.fd_auto_compress import auto_compress\n        print(\"Welcome to use FastDeploy Auto Compression Toolkit!\")\n        auto_compress(args)\n    if args.tools == \"convert\":\n        try:\n            import platform\n            import logging\n            v0, v1, v2 = platform.python_version().split('.')\n            if not (int(v0) >= 3 and int(v1) >= 5):\n                logging.info(\"[ERROR] python>=3.5 is required\")\n                return\n            import paddle\n            v0, v1, v2 = paddle.__version__.split('.')\n            logging.info(\"paddle.__version__ = {}\".format(paddle.__version__))\n            if v0 == '0' and v1 == '0' and v2 == '0':\n                logging.info(\n                    \"[WARNING] You are use develop version of paddlepaddle\")\n            elif int(v0) != 2 or int(v1) < 0:\n                logging.info(\"[ERROR] paddlepaddle>=2.0.0 is required\")\n                return\n            from x2paddle.convert import tf2paddle, caffe2paddle, onnx2paddle\n            if args.framework == \"tensorflow\":\n                assert args.model is not None, \"--model should be defined while convert tensorflow model\"\n                tf2paddle(args.model, args.save_dir)\n            elif args.framework == \"caffe\":\n                assert args.prototxt is not None and args.weight is not None, \"--prototxt and --weight should be defined while convert caffe model\"\n                caffe2paddle(args.prototxt, args.weight, args.save_dir,\n                             args.caffe_proto)\n            elif args.framework == \"onnx\":\n                assert args.model is not None, \"--model should be defined while convert onnx model\"\n                onnx2paddle(\n                    args.model,\n                    args.save_dir,\n                    input_shape_dict=args.input_shape_dict)\n            else:\n                raise Exception(\n                    \"--framework only support tensorflow/caffe/onnx now\")\n        except ImportError:\n            print(\n                \"Model convert failed! Please check if you have installed it!\")\n    if args.tools == \"simple_serving\":\n        custom_logging_config = {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"formatters\": {\n                \"default\": {\n                    \"()\": \"uvicorn.logging.DefaultFormatter\",\n                    \"fmt\": \"%(asctime)s %(levelprefix)s %(message)s\",\n                    'datefmt': '%Y-%m-%d %H:%M:%S',\n                    \"use_colors\": None,\n                },\n            },\n            \"handlers\": {\n                \"default\": {\n                    \"formatter\": \"default\",\n                    \"class\": \"logging.StreamHandler\",\n                    \"stream\": \"ext://sys.stderr\",\n                },\n                'null': {\n                    \"formatter\": \"default\",\n                    \"class\": 'logging.NullHandler'\n                }\n            },\n            \"loggers\": {\n                \"\": {\n                    \"handlers\": [\"null\"],\n                    \"level\": \"DEBUG\"\n                },\n                \"uvicorn.error\": {\n                    \"handlers\": [\"default\"],\n                    \"level\": \"DEBUG\"\n                }\n            },\n        }\n        uvicorn.run(args.app,\n                    host=args.host,\n                    port=args.port,\n                    app_dir='.',\n                    log_config=custom_logging_config)\n    if args.tools == \"paddle2coreml\":\n        if any([\n                args.p2c_paddle_model_dir is None,\n                args.p2c_coreml_model_dir is None,\n                args.p2c_input_names is None, args.p2c_input_shapes is None,\n                args.p2c_output_names is None\n        ]):\n            raise Exception(\n                \"paddle2coreml need to define --p2c_paddle_model_dir, --p2c_coreml_model_dir, --p2c_input_names, --p2c_input_shapes, --p2c_output_names\"\n            )\n        import coremltools as ct\n        import os\n        import numpy as np\n\n        def type_to_np_dtype(dtype):\n            if dtype == 'float32':\n                return np.float32\n            elif dtype == 'float64':\n                return np.float64\n            elif dtype == 'int32':\n                return np.int32\n            elif dtype == 'int64':\n                return np.int64\n            elif dtype == 'uint8':\n                return np.uint8\n            elif dtype == 'uint16':\n                return np.uint16\n            elif dtype == 'uint32':\n                return np.uint32\n            elif dtype == 'uint64':\n                return np.uint64\n            elif dtype == 'int8':\n                return np.int8\n            elif dtype == 'int16':\n                return np.int16\n            else:\n                raise Exception(\"Unsupported dtype: {}\".format(dtype))\n\n        input_names = args.p2c_input_names.split(' ')\n        input_shapes = [[int(i) for i in shape.split(',')]\n                        for shape in args.p2c_input_shapes.split(' ')]\n        input_dtypes = map(type_to_np_dtype, args.p2c_input_dtypes.split(' '))\n        output_names = args.p2c_output_names.split(' ')\n        sample_input = [\n            ct.TensorType(\n                name=k,\n                shape=s,\n                dtype=d, )\n            for k, s, d in zip(input_names, input_shapes, input_dtypes)\n        ]\n\n        coreml_model = ct.convert(\n            args.p2c_paddle_model_dir,\n            convert_to=\"mlprogram\",\n            minimum_deployment_target=ct.target.macOS13,\n            inputs=sample_input,\n            outputs=[ct.TensorType(name=name) for name in output_names], )\n        coreml_model.save(\n            os.path.join(args.p2c_coreml_model_dir,\n                         args.p2c_coreml_model_name))",
  "def type_to_np_dtype(dtype):\n            if dtype == 'float32':\n                return np.float32\n            elif dtype == 'float64':\n                return np.float64\n            elif dtype == 'int32':\n                return np.int32\n            elif dtype == 'int64':\n                return np.int64\n            elif dtype == 'uint8':\n                return np.uint8\n            elif dtype == 'uint16':\n                return np.uint16\n            elif dtype == 'uint32':\n                return np.uint32\n            elif dtype == 'uint64':\n                return np.uint64\n            elif dtype == 'int8':\n                return np.int8\n            elif dtype == 'int16':\n                return np.int16\n            else:\n                raise Exception(\"Unsupported dtype: {}\".format(dtype))",
  "def generate_scale(im, target_shape):\n    origin_shape = im.shape[:2]\n    im_size_min = np.min(origin_shape)\n    im_size_max = np.max(origin_shape)\n    target_size_min = np.min(target_shape)\n    target_size_max = np.max(target_shape)\n    im_scale = float(target_size_min) / float(im_size_min)\n    if np.round(im_scale * im_size_max) > target_size_max:\n        im_scale = float(target_size_max) / float(im_size_max)\n    im_scale_x = im_scale\n    im_scale_y = im_scale\n\n    return im_scale_y, im_scale_x",
  "def yolo_image_preprocess(img, target_shape=[640, 640]):\n    # Resize image\n    im_scale_y, im_scale_x = generate_scale(img, target_shape)\n    img = cv2.resize(\n        img,\n        None,\n        None,\n        fx=im_scale_x,\n        fy=im_scale_y,\n        interpolation=cv2.INTER_LINEAR)\n    # Pad\n    im_h, im_w = img.shape[:2]\n    h, w = target_shape[:]\n    if h != im_h or w != im_w:\n        canvas = np.ones((h, w, 3), dtype=np.float32)\n        canvas *= np.array([114.0, 114.0, 114.0], dtype=np.float32)\n        canvas[0:im_h, 0:im_w, :] = img.astype(np.float32)\n        img = canvas\n    img = np.transpose(img / 255, [2, 0, 1])\n\n    return img.astype(np.float32)",
  "def cls_resize_short(img, target_size):\n\n    img_h, img_w = img.shape[:2]\n    percent = float(target_size) / min(img_w, img_h)\n    w = int(round(img_w * percent))\n    h = int(round(img_h * percent))\n\n    return cv2.resize(img, (w, h), interpolation=cv2.INTER_LINEAR)",
  "def crop_image(img, target_size, center):\n\n    height, width = img.shape[:2]\n    size = target_size\n\n    if center == True:\n        w_start = (width - size) // 2\n        h_start = (height - size) // 2\n    else:\n        w_start = np.random.randint(0, width - size + 1)\n        h_start = np.random.randint(0, height - size + 1)\n    w_end = w_start + size\n    h_end = h_start + size\n\n    return img[h_start:h_end, w_start:w_end, :]",
  "def cls_image_preprocess(img):\n\n    # resize\n    img = cls_resize_short(img, target_size=256)\n    # crop\n    img = crop_image(img, target_size=224, center=True)\n\n    #ToCHWImage & Normalize\n    img = np.transpose(img / 255, [2, 0, 1])\n\n    img_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n    img_std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n    img -= img_mean\n    img /= img_std\n\n    return img.astype(np.float32)",
  "def ppdet_resize_no_keepratio(img, target_shape=[640, 640]):\n    im_shape = img.shape\n\n    resize_h, resize_w = target_shape\n    im_scale_y = resize_h / im_shape[0]\n    im_scale_x = resize_w / im_shape[1]\n\n    scale_factor = np.asarray([im_scale_y, im_scale_x], dtype=np.float32)\n    return cv2.resize(\n        img, None, None, fx=im_scale_x, fy=im_scale_y,\n        interpolation=2), scale_factor",
  "def ppyoloe_withNMS_image_preprocess(img):\n\n    img, scale_factor = ppdet_resize_no_keepratio(img, target_shape=[640, 640])\n\n    img = np.transpose(img / 255, [2, 0, 1])\n\n    img_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n    img_std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n    img -= img_mean\n    img /= img_std\n\n    return img.astype(np.float32), scale_factor",
  "def ppyoloe_plus_withNMS_image_preprocess(img):\n\n    img, scale_factor = ppdet_resize_no_keepratio(img, target_shape=[640, 640])\n\n    img = np.transpose(img / 255, [2, 0, 1])\n\n    return img.astype(np.float32), scale_factor",
  "def ppseg_cityscapes_ptq_preprocess(img):\n\n    #ToCHWImage & Normalize\n    img = np.transpose(img / 255.0, [2, 0, 1])\n\n    img_mean = np.array([0.5, 0.5, 0.5]).reshape((3, 1, 1))\n    img_std = np.array([0.5, 0.5, 0.5]).reshape((3, 1, 1))\n    img -= img_mean\n    img /= img_std\n\n    return img.astype(np.float32)",
  "def ResizeStepScaling(img,\n                      min_scale_factor=0.75,\n                      max_scale_factor=1.25,\n                      scale_step_size=0.25):\n    # refer form ppseg\n    if min_scale_factor == max_scale_factor:\n        scale_factor = min_scale_factor\n    elif scale_step_size == 0:\n        scale_factor = np.random.uniform(min_scale_factor, max_scale_factor)\n    else:\n        num_steps = int((max_scale_factor - min_scale_factor) / scale_step_size\n                        + 1)\n        scale_factors = np.linspace(min_scale_factor, max_scale_factor,\n                                    num_steps).tolist()\n        np.random.shuffle(scale_factors)\n        scale_factor = scale_factors[0]\n\n    w = int(round(scale_factor * img.shape[1]))\n    h = int(round(scale_factor * img.shape[0]))\n\n    img = cv2.resize(img, (w, h), interpolation=cv2.INTER_LINEAR)\n\n    return img",
  "def RandomPaddingCrop(img,\n                      crop_size=(512, 512),\n                      im_padding_value=(127.5, 127.5, 127.5),\n                      label_padding_value=255):\n\n    if isinstance(crop_size, list) or isinstance(crop_size, tuple):\n        if len(crop_size) != 2:\n            raise ValueError(\n                'Type of `crop_size` is list or tuple. It should include 2 elements, but it is {}'\n                .format(crop_size))\n    else:\n        raise TypeError(\n            \"The type of `crop_size` is invalid. It should be list or tuple, but it is {}\"\n            .format(type(crop_size)))\n\n    if isinstance(crop_size, int):\n        crop_width = crop_size\n        crop_height = crop_size\n    else:\n        crop_width = crop_size[0]\n        crop_height = crop_size[1]\n\n    img_height = img.shape[0]\n    img_width = img.shape[1]\n\n    if img_height == crop_height and img_width == crop_width:\n        return img\n    else:\n        pad_height = max(crop_height - img_height, 0)\n        pad_width = max(crop_width - img_width, 0)\n        if (pad_height > 0 or pad_width > 0):\n            img = cv2.copyMakeBorder(\n                img,\n                0,\n                pad_height,\n                0,\n                pad_width,\n                cv2.BORDER_CONSTANT,\n                value=im_padding_value)\n\n            img_height = img.shape[0]\n            img_width = img.shape[1]\n\n        if crop_height > 0 and crop_width > 0:\n            h_off = np.random.randint(img_height - crop_height + 1)\n            w_off = np.random.randint(img_width - crop_width + 1)\n\n            img = img[h_off:(crop_height + h_off), w_off:(w_off + crop_width\n                                                          ), :]\n\n        return img",
  "def RandomHorizontalFlip(img, prob=0.5):\n    if random.random() < prob:\n\n        if len(img.shape) == 3:\n            img = img[:, ::-1, :]\n        elif len(img.shape) == 2:\n            img = img[:, ::-1]\n\n        return img\n    else:\n        return img",
  "def brightness(im, brightness_lower, brightness_upper):\n    brightness_delta = np.random.uniform(brightness_lower, brightness_upper)\n    im = ImageEnhance.Brightness(im).enhance(brightness_delta)\n    return im",
  "def contrast(im, contrast_lower, contrast_upper):\n    contrast_delta = np.random.uniform(contrast_lower, contrast_upper)\n    im = ImageEnhance.Contrast(im).enhance(contrast_delta)\n    return im",
  "def saturation(im, saturation_lower, saturation_upper):\n    saturation_delta = np.random.uniform(saturation_lower, saturation_upper)\n    im = ImageEnhance.Color(im).enhance(saturation_delta)\n    return im",
  "def hue(im, hue_lower, hue_upper):\n    hue_delta = np.random.uniform(hue_lower, hue_upper)\n    im = np.array(im.convert('HSV'))\n    im[:, :, 0] = im[:, :, 0] + hue_delta\n    im = Image.fromarray(im, mode='HSV').convert('RGB')\n    return im",
  "def sharpness(im, sharpness_lower, sharpness_upper):\n    sharpness_delta = np.random.uniform(sharpness_lower, sharpness_upper)\n    im = ImageEnhance.Sharpness(im).enhance(sharpness_delta)\n    return im",
  "def RandomDistort(img,\n                  brightness_range=0.5,\n                  brightness_prob=0.5,\n                  contrast_range=0.5,\n                  contrast_prob=0.5,\n                  saturation_range=0.5,\n                  saturation_prob=0.5,\n                  hue_range=18,\n                  hue_prob=0.5,\n                  sharpness_range=0.5,\n                  sharpness_prob=0):\n\n    brightness_lower = 1 - brightness_range\n    brightness_upper = 1 + brightness_range\n    contrast_lower = 1 - contrast_range\n    contrast_upper = 1 + contrast_range\n    saturation_lower = 1 - saturation_range\n    saturation_upper = 1 + saturation_range\n    hue_lower = -hue_range\n    hue_upper = hue_range\n    sharpness_lower = 1 - sharpness_range\n    sharpness_upper = 1 + sharpness_range\n    ops = [brightness, contrast, saturation, hue, sharpness]\n    random.shuffle(ops)\n    params_dict = {\n        'brightness': {\n            'brightness_lower': brightness_lower,\n            'brightness_upper': brightness_upper\n        },\n        'contrast': {\n            'contrast_lower': contrast_lower,\n            'contrast_upper': contrast_upper\n        },\n        'saturation': {\n            'saturation_lower': saturation_lower,\n            'saturation_upper': saturation_upper\n        },\n        'hue': {\n            'hue_lower': hue_lower,\n            'hue_upper': hue_upper\n        },\n        'sharpness': {\n            'sharpness_lower': sharpness_lower,\n            'sharpness_upper': sharpness_upper,\n        }\n    }\n    prob_dict = {\n        'brightness': brightness_prob,\n        'contrast': contrast_prob,\n        'saturation': saturation_prob,\n        'hue': hue_prob,\n        'sharpness': sharpness_prob\n    }\n\n    img = img.astype('uint8')\n    img = Image.fromarray(img)\n\n    for id in range(len(ops)):\n        params = params_dict[ops[id].__name__]\n        prob = prob_dict[ops[id].__name__]\n        params['im'] = img\n        if np.random.uniform(0, 1) < prob:\n            img = ops[id](**params)\n    img = np.asarray(img).astype('float32')\n    return img",
  "def ppseg_cityscapes_qat_preprocess(img):\n\n    min_scale_factor = 0.5\n    max_scale_factor = 2.0\n    scale_step_size = 0.25\n\n    crop_size = (1024, 512)\n\n    brightness_range = 0.5\n    contrast_range = 0.5\n    saturation_range = 0.5\n\n    img = ResizeStepScaling(\n        img, min_scale_factor=0.5, max_scale_factor=2.0, scale_step_size=0.25)\n    img = RandomPaddingCrop(img, crop_size=(1024, 512))\n    img = RandomHorizontalFlip(img)\n    img = RandomDistort(\n        img, brightness_range=0.5, contrast_range=0.5, saturation_range=0.5)\n\n    img = np.transpose(img / 255.0, [2, 0, 1])\n    img_mean = np.array([0.5, 0.5, 0.5]).reshape((3, 1, 1))\n    img_std = np.array([0.5, 0.5, 0.5]).reshape((3, 1, 1))\n    img -= img_mean\n    img /= img_std\n    return img.astype(np.float32)",
  "def argsparser():\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        '--config_path',\n        type=str,\n        default=None,\n        help=\"path of compression strategy config.\",\n        required=True)\n    parser.add_argument(\n        '--method',\n        type=str,\n        default=None,\n        help=\"choose PTQ or QAT as quantization method\",\n        required=True)\n    parser.add_argument(\n        '--save_dir',\n        type=str,\n        default='output',\n        help=\"directory to save compressed model.\")\n    parser.add_argument(\n        '--devices',\n        type=str,\n        default='gpu',\n        help=\"which device used to compress.\")\n\n    return parser",
  "def reader_wrapper(reader, input_list):\n\n    if isinstance(input_list, list) and len(input_list) == 1:\n        input_name = input_list[0]\n\n        def gen():\n            in_dict = {}\n            for i, data in enumerate(reader()):\n                imgs = np.array(data[0])\n                in_dict[input_name] = imgs\n                yield in_dict\n\n        return gen\n\n    if isinstance(input_list, list) and len(input_list) > 1:\n\n        def gen():\n            for idx, data in enumerate(reader()):\n                in_dict = {}\n                for i in range(len(input_list)):\n                    intput_name = input_list[i]\n                    feed_data = np.array(data[0][i])\n                    in_dict[intput_name] = feed_data\n\n                yield in_dict\n\n        return gen",
  "def auto_compress(FLAGS):\n\n    #FLAGS needs parse\n    time_s = time.time()\n    paddle.enable_static()\n\n    assert FLAGS.devices in ['cpu', 'gpu', 'xpu', 'npu']\n    paddle.set_device(FLAGS.devices)\n    global global_config\n\n    if FLAGS.method == 'QAT':\n\n        all_config = load_config(FLAGS.config_path)\n        assert \"Global\" in all_config, f\"Key 'Global' not found in config file. \\n{all_config}\"\n        global_config = all_config[\"Global\"]\n        input_list = global_config['input_list']\n\n        assert os.path.exists(global_config[\n            'qat_image_path']), \"image_path does not exist!\"\n        paddle.vision.image.set_image_backend('cv2')\n        # transform could be customized.\n        train_dataset = paddle.vision.datasets.ImageFolder(\n            global_config['qat_image_path'],\n            transform=eval(global_config['qat_preprocess']))\n        train_loader = paddle.io.DataLoader(\n            train_dataset,\n            batch_size=global_config['qat_batch_size'],\n            shuffle=True,\n            drop_last=True,\n            num_workers=0)\n        train_loader = reader_wrapper(train_loader, input_list=input_list)\n        eval_func = None\n\n        # ACT compression\n        ac = AutoCompression(\n            model_dir=global_config['model_dir'],\n            model_filename=global_config['model_filename'],\n            params_filename=global_config['params_filename'],\n            train_dataloader=train_loader,\n            save_dir=FLAGS.save_dir,\n            config=all_config,\n            eval_callback=eval_func)\n        ac.compress()\n\n    # PTQ compression\n    if FLAGS.method == 'PTQ':\n\n        # Read Global config and prepare dataset\n        all_config = load_config(FLAGS.config_path)\n        assert \"Global\" in all_config, f\"Key 'Global' not found in config file. \\n{all_config}\"\n        global_config = all_config[\"Global\"]\n        input_list = global_config['input_list']\n\n        assert os.path.exists(global_config[\n            'ptq_image_path']), \"image_path does not exist!\"\n\n        paddle.vision.image.set_image_backend('cv2')\n        # transform could be customized.\n        val_dataset = paddle.vision.datasets.ImageFolder(\n            global_config['ptq_image_path'],\n            transform=eval(global_config['ptq_preprocess']))\n        val_loader = paddle.io.DataLoader(\n            val_dataset,\n            batch_size=1,\n            shuffle=True,\n            drop_last=True,\n            num_workers=0)\n        val_loader = reader_wrapper(val_loader, input_list=input_list)\n\n        # Read PTQ config\n        assert \"PTQ\" in all_config, f\"Key 'PTQ' not found in config file. \\n{all_config}\"\n        ptq_config = all_config[\"PTQ\"]\n\n        # Inititalize the executor\n        place = paddle.CUDAPlace(\n            0) if FLAGS.devices == 'gpu' else paddle.CPUPlace()\n        exe = paddle.static.Executor(place)\n\n        # Read ONNX or PADDLE format model\n        if global_config['format'] == 'onnx':\n            load_onnx_model(global_config[\"model_dir\"])\n            inference_model_path = global_config[\"model_dir\"].rstrip().rstrip(\n                '.onnx') + '_infer'\n        else:\n            inference_model_path = global_config[\"model_dir\"].rstrip('/')\n\n        quant_post_static(\n            executor=exe,\n            model_dir=inference_model_path,\n            quantize_model_path=FLAGS.save_dir,\n            data_loader=val_loader,\n            model_filename=global_config[\"model_filename\"],\n            params_filename=global_config[\"params_filename\"],\n            batch_size=32,\n            batch_nums=10,\n            algo=ptq_config['calibration_method'],\n            hist_percent=0.999,\n            is_full_quantize=False,\n            bias_correction=False,\n            onnx_format=True,\n            skip_tensor_list=ptq_config['skip_tensor_list']\n            if 'skip_tensor_list' in ptq_config else None)\n\n    time_total = time.time() - time_s\n    print(\"Finish Compression, total time used is : \", time_total, \"seconds.\")",
  "def gen():\n            in_dict = {}\n            for i, data in enumerate(reader()):\n                imgs = np.array(data[0])\n                in_dict[input_name] = imgs\n                yield in_dict",
  "def gen():\n            for idx, data in enumerate(reader()):\n                in_dict = {}\n                for i in range(len(input_list)):\n                    intput_name = input_list[i]\n                    feed_data = np.array(data[0][i])\n                    in_dict[intput_name] = feed_data\n\n                yield in_dict",
  "def get_config():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_path\", default=\"./picodet_l_320_coco_lcnet/model\")\n    parser.add_argument(\n        \"--shape_dict\",\n        default={\"image\": [1, 3, 320, 320],\n                 \"scale_factor\": [1, 2]})\n    parser.add_argument(\"--tvm_save_name\", default=\"tvm_model\")\n    parser.add_argument(\"--tvm_save_path\", default=\"./tvm_save\")\n    args = parser.parse_args()\n    return args",
  "def read_model(model_path):\n    return paddle.jit.load(model_path)",
  "def paddle_to_tvm(paddle_model,\n                  shape_dict,\n                  tvm_save_name=\"tvm_model\",\n                  tvm_save_path=\"./tvm_save\"):\n    if isinstance(shape_dict, str):\n        shape_dict = eval(shape_dict)\n    mod, params = relay.frontend.from_paddle(paddle_model, shape_dict)\n    # \u8fd9\u91cc\u9996\u5148\u5728PC\u7684CPU\u4e0a\u8fdb\u884c\u6d4b\u8bd5 \u6240\u4ee5\u4f7f\u7528LLVM\u8fdb\u884c\u5bfc\u51fa\n    target = tvm.target.Target(\"llvm\", host=\"llvm\")\n    dev = tvm.cpu(0)\n    # \u8fd9\u91cc\u5229\u7528TVM\u6784\u5efa\u51fa\u4f18\u5316\u540e\u6a21\u578b\u7684\u4fe1\u606f\n    with tvm.transform.PassContext(opt_level=2):\n        base_lib = relay.build_module.build(mod, target, params=params)\n        if not os.path.exists(tvm_save_path):\n            os.mkdir(tvm_save_path)\n        lib_save_path = os.path.join(tvm_save_path, tvm_save_name + \".so\")\n        base_lib.export_library(lib_save_path)\n        param_save_path = os.path.join(tvm_save_path,\n                                       tvm_save_name + \".params\")\n        with open(param_save_path, 'wb') as fo:\n            fo.write(relay.save_param_dict(base_lib.get_params()))\n        module = graph_executor.GraphModule(base_lib['default'](dev))\n        module.load_params(relay.save_param_dict(base_lib.get_params()))\n        print(\"export success\")",
  "def get_config():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--verbose\", default=True, help=\"rknntoolkit verbose\")\n    parser.add_argument(\"--config_path\")\n    parser.add_argument(\"--target_platform\")\n    args = parser.parse_args()\n    return args",
  "def  load_example_input_datas():\n    \"\"\"fake data\"\"\"\n    data_list = []\n    input_1 = torch.randn(1, 3, 224, 224, dtype=torch.float32).cuda()\n    data_list.append(input_1)\n    return data_list",
  "def cmake_build():\n    \"\"\"execute cmake build, to make the shared lib `libporos.so` \"\"\"\n    cwd = os.getcwd()\n    if spawn.find_executable('cmake') is None:\n        sys.stderr.write(\"CMake is required to build this package.\\n\")\n        sys.exit(-1)\n    _source_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    _build_dir = os.path.join(_source_dir, 'build')\n    _prefix = get_python_lib()\n    try:\n        cmake_configure_command = [\n            'cmake',\n            '-H{0}'.format(_source_dir),\n            '-B{0}'.format(_build_dir),\n            '-DCMAKE_INSTALL_PREFIX={0}'.format(_prefix),\n        ]\n        _generator = os.getenv('CMAKE_GENERATOR')\n        if _generator is not None:\n            cmake_configure_command.append('-G{0}'.format(_generator))\n        spawn.spawn(cmake_configure_command)\n        spawn.spawn(\n            ['cmake', '--build', _build_dir, '-j', str(THREAD_NUM)])\n        os.chdir(cwd)\n    except spawn.DistutilsExecError:\n        sys.stderr.write(\"Error while building with CMake\\n\")\n        sys.exit(-1)",
  "class CleanCommand(Command):\n    \"\"\"Custom clean command to tidy up the project root.\"\"\"\n    PY_CLEAN_FILES = [\n        './build', './dist', './poros/__pycache__', './poros/lib', './*.pyc', './*.tgz', './*.egg-info'\n    ]\n    description = \"Command to tidy up the project root\"\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        for path_spec in self.PY_CLEAN_FILES:\n            # Make paths absolute and relative to this path\n            abs_paths = glob.glob(os.path.normpath(os.path.join(CURRENT_PATH, path_spec)))\n            for path in [str(p) for p in abs_paths]:\n                if not path.startswith(CURRENT_PATH):\n                    # Die if path in CLEAN_FILES is absolute + outside this directory\n                    raise ValueError(\"%s is not a path inside %s\" % (path, CURRENT_PATH))\n                print('Removing %s' % os.path.relpath(path))\n                shutil.rmtree(path)",
  "def initialize_options(self):\n        pass",
  "def finalize_options(self):\n        pass",
  "def run(self):\n        for path_spec in self.PY_CLEAN_FILES:\n            # Make paths absolute and relative to this path\n            abs_paths = glob.glob(os.path.normpath(os.path.join(CURRENT_PATH, path_spec)))\n            for path in [str(p) for p in abs_paths]:\n                if not path.startswith(CURRENT_PATH):\n                    # Die if path in CLEAN_FILES is absolute + outside this directory\n                    raise ValueError(\"%s is not a path inside %s\" % (path, CURRENT_PATH))\n                print('Removing %s' % os.path.relpath(path))\n                shutil.rmtree(path)",
  "def load_example_model():\n    \"\"\"load model\u793a\u4f8b\uff0c\u6b63\u5e38load model\u5373\u53ef\"\"\"\n    import torchvision.models as models\n    std_resnet = models.resnet50(pretrained=True)\n    std_resnet.cuda()\n    std_resnet.eval()\n    return std_resnet",
  "def  load_example_input_datas():\n    \"\"\"\u52a0\u8f7d\u9884\u70ed\u6570\u636e\"\"\"\n    data_list = []\n    #max size\n    input_1 = np.ones((3, 3, 96, 320), np.float32)\n    input_tensor = torch.from_numpy(input_1).cuda()\n    data_list.append(input_tensor)\n\n    #min size\n    input_2 = np.ones((1, 3, 96, 320), np.float32)\n    input_tensor2 = torch.from_numpy(input_2).cuda()\n    data_list.append(input_tensor2)\n    \n    #opt size\n    input_3 = np.ones((1, 3, 96, 320), np.float32)\n    input_tensor3 = torch.from_numpy(input_3).cuda()\n    data_list.append(input_tensor3)\n\n    return data_list",
  "def make_to_tuple(prewarm_input):\n    \"\"\"\n    wrap a single torch.Tensor input to tuple\n    \"\"\"\n    if isinstance(prewarm_input, torch.Tensor):\n        return (prewarm_input,)\n    # done primarily so that weird iterables fail here and not pybind11 code\n    if not isinstance(prewarm_input, tuple):\n        return tuple(prewarm_input)\n    return prewarm_input",
  "def convert_prewarm_inputs(prewarm_inputs):\n    # type: (Any) -> poros._C.PreWarmDatas\n    \"\"\"\n    convert prewarm-data to c10:ivalues list that can handle in poros.\n    we can accept 3 kinds of prewarm_inputs:\n        one input that has a single tensor [torch.Tensor]\n        one input that has multiple variables [tuple]\n        more than one input, that each input has a single tensor [List of torch.Tensor]\n        more that one input, that each input has multiple variables [List of tuple]\n    \"\"\"\n    wraped_prewarm_inputs = []\n    if isinstance(prewarm_inputs, torch.Tensor):\n        wraped_prewarm_inputs.append(make_to_tuple(prewarm_inputs))\n    elif isinstance(prewarm_inputs, tuple):\n        wraped_prewarm_inputs.append(prewarm_inputs)\n    elif isinstance(prewarm_inputs, list):\n        for member in prewarm_inputs:\n            if isinstance(member, torch.Tensor):\n                wraped_prewarm_inputs.append(make_to_tuple(member))\n            elif isinstance(member, tuple):\n                wraped_prewarm_inputs.append(member)\n            else:\n                raise TypeError(\"prewarm_inputs for poros should be torch.Tensor or wraped as tuple, fix it\")\n    else:\n        raise TypeError(\"prewarm_inputs for poros should be torch.Tensor or wraped as tuple or inputs-lists, fix it\")\n    return wraped_prewarm_inputs",
  "def convert_poros_option(poros_option):\n    # type: Dict[str, Any] -> poros._C.PorosOptions\n    \"\"\"\n    converter key-value poros_option to PorosOptions that can handle in poros\n    \"\"\"\n    option = poros._C.PorosOptions()\n    if poros_option is None:\n        #default situation. if user do not set the poros_option\n        return option\n    elif isinstance(poros_option, PorosOptions):\n        return poros_option.to_internal()\n    else:\n        raise TypeError(\"poros_option for poros should be PorosOptions or a attribute dict fix it\")",
  "class DynamicOptions(object):\n    \"\"\"\n    dynamic settings for poros\n    \"\"\"\n    def __init__(self):\n        \"\"\"set defalut dynamic options\"\"\"\n        self.is_dynamic = False\n        self.min_shapes = []\n        self.opt_shapes = []\n        self.max_shapes = []\n\n    def set_dynamic_options(self, min, opt, max):\n        \"\"\"situation when give three inputs is given\"\"\"\n        option_list = [min, opt, max]\n        for item in option_list:\n            if not (isinstance(item, list)):\n                raise TypeError(\"dynamic_option for poros should be IntList, fix it\")\n        option_list.sort()\n        self.min_shapes = option_list[0]\n        self.opt_shapes = option_list[1]\n        self.max_shapes = option_list[2]\n\n    def set_dynamic_option(self, opt):\n        \"\"\"situation when only one input is given\"\"\"\n        if not isinstance(opt, list):\n            raise TypeError(\"dynamic_option for poros should be IntList, fix it\")\n        else:\n            self.min_shapes = opt\n            self.opt_shapes = opt\n            self.max_shapes = opt\n            self.is_dynamic = False\n    \n    def get_dynamic_options(self):\n        \"\"\"get dynamic options\"\"\"\n        return [self.min_shapes, self.opt_shapes, self.max_shapes]\n\n    def to_internal(self):\n        \"\"\"\n        change DynamicOptions in python env to DynamicShapeOptions in c++ env\n        \"\"\"\n        option = poros._C.DynamicShapeOptions()\n        assert isinstance(self.is_dynamic, bool)\n        option.is_dynamic = self.is_dynamic\n\n        assert isinstance(self.min_shapes, list)\n        option.min_shapes = self.min_shapes\n        assert isinstance(self.opt_shapes, list)\n        option.opt_shapes = self.opt_shapes\n        assert isinstance(self.max_shapes, list)\n        option.max_shapes = self.max_shapes\n        return option",
  "class PorosOptions(object):\n    \"\"\"\n    options for poros\n    \"\"\"\n    available_devices = [\"GPU\", \"CPU\", \"XPU\"]\n    available_debug_mode = [True, False]\n    def __init__(self):\n        self.device = \"GPU\"\n        self.debug = False\n        self.use_fp16 = False\n        self.max_workspace_size = 1 << 30\n        self.is_dynamic = False\n        self.long_to_int = True\n        self.device_id = -1\n        self.unconst_ops_thres = -1\n        self.use_nvidia_tf32 = True\n        self.preprocess_mode = 0\n        self.unsupport_op_list = []\n\n    def to_internal(self):\n        \"\"\"\n        change PorosOptions in python env to PorosOptions in c++ env\n        \"\"\"\n        option = poros._C.PorosOptions()\n        option.device = _parse_device(self.device)\n        assert isinstance(self.debug, bool)\n        option.debug = self.debug\n        assert isinstance(self.use_fp16, bool)\n        option.use_fp16 = self.use_fp16\n        assert type(self.max_workspace_size) is int\n        option.max_workspace_size = self.max_workspace_size\n        assert isinstance(self.is_dynamic, bool)\n        option.is_dynamic = self.is_dynamic\n        assert isinstance(self.long_to_int, bool)\n        option.long_to_int = self.long_to_int\n        assert type(self.device_id) is int\n        option.device_id = self.device_id\n        assert type(self.unconst_ops_thres) is int\n        option.unconst_ops_thres = self.unconst_ops_thres\n        assert type(self.use_nvidia_tf32) is bool\n        option.use_nvidia_tf32 = self.use_nvidia_tf32\n        assert type(self.preprocess_mode) is int\n        option.preprocess_mode = self.preprocess_mode\n        assert type(self.unsupport_op_list) is list\n        option.unsupport_op_list = self.unsupport_op_list\n\n        return option\n\n    def set_device(self, device):\n        \"\"\"set device\"\"\"\n        if device not in PorosOptions.available_devices:\n            raise TypeError(\"device for poros invalid, only %s supported, fix it\" % (PorosOptions.available_devices))\n        self.device = device\n    \n    def set_debug(self, debug):\n        \"\"\"set debug\"\"\"\n        if debug not in PorosOptions.available_debug_mode:\n            raise TypeError(\"device for poros invalid, only %s supported, fix it\" % (PorosOptions.available_debug_mode))\n        self.debug = debug",
  "class PorosModule(RecursiveScriptModule):\n    \"\"\"\n    The core data structure of poros. \n    \"\"\"\n    def __init__(self, cpp_module):\n        super(PorosModule, self).__init__(cpp_module)\n        # self.options = PorosOptions()\n        # if option is not None and isinstance(option, PorosOptions):\n        #     self.options = option\n            \n    @staticmethod\n    def _construct(cpp_module, init_fn):\n        \"\"\"\n        Construct a PorosModule that's ready for use. \n        Args:\n            cpp_module:  The C++ Module that will hold the actual state of\n                            this PorosModule instance.\n            init_fn:  Lambda that initializes the PorosModule passed to it.\n        \"\"\"\n        script_module = PorosModule(cpp_module)\n        init_fn(script_module)\n\n        # Finalize the ScriptModule: replace the nn.Module state with our\n        # custom implementations and flip the _initializing bit.\n        PorosModule._finalize_scriptmodule(script_module)\n        return script_module\n    \n    @property\n    def supported_engine(self):\n        \"\"\"supported engine\"\"\"\n        return [\"tensorrt\"]",
  "def __init__(self):\n        \"\"\"set defalut dynamic options\"\"\"\n        self.is_dynamic = False\n        self.min_shapes = []\n        self.opt_shapes = []\n        self.max_shapes = []",
  "def set_dynamic_options(self, min, opt, max):\n        \"\"\"situation when give three inputs is given\"\"\"\n        option_list = [min, opt, max]\n        for item in option_list:\n            if not (isinstance(item, list)):\n                raise TypeError(\"dynamic_option for poros should be IntList, fix it\")\n        option_list.sort()\n        self.min_shapes = option_list[0]\n        self.opt_shapes = option_list[1]\n        self.max_shapes = option_list[2]",
  "def set_dynamic_option(self, opt):\n        \"\"\"situation when only one input is given\"\"\"\n        if not isinstance(opt, list):\n            raise TypeError(\"dynamic_option for poros should be IntList, fix it\")\n        else:\n            self.min_shapes = opt\n            self.opt_shapes = opt\n            self.max_shapes = opt\n            self.is_dynamic = False",
  "def get_dynamic_options(self):\n        \"\"\"get dynamic options\"\"\"\n        return [self.min_shapes, self.opt_shapes, self.max_shapes]",
  "def to_internal(self):\n        \"\"\"\n        change DynamicOptions in python env to DynamicShapeOptions in c++ env\n        \"\"\"\n        option = poros._C.DynamicShapeOptions()\n        assert isinstance(self.is_dynamic, bool)\n        option.is_dynamic = self.is_dynamic\n\n        assert isinstance(self.min_shapes, list)\n        option.min_shapes = self.min_shapes\n        assert isinstance(self.opt_shapes, list)\n        option.opt_shapes = self.opt_shapes\n        assert isinstance(self.max_shapes, list)\n        option.max_shapes = self.max_shapes\n        return option",
  "def __init__(self):\n        self.device = \"GPU\"\n        self.debug = False\n        self.use_fp16 = False\n        self.max_workspace_size = 1 << 30\n        self.is_dynamic = False\n        self.long_to_int = True\n        self.device_id = -1\n        self.unconst_ops_thres = -1\n        self.use_nvidia_tf32 = True\n        self.preprocess_mode = 0\n        self.unsupport_op_list = []",
  "def to_internal(self):\n        \"\"\"\n        change PorosOptions in python env to PorosOptions in c++ env\n        \"\"\"\n        option = poros._C.PorosOptions()\n        option.device = _parse_device(self.device)\n        assert isinstance(self.debug, bool)\n        option.debug = self.debug\n        assert isinstance(self.use_fp16, bool)\n        option.use_fp16 = self.use_fp16\n        assert type(self.max_workspace_size) is int\n        option.max_workspace_size = self.max_workspace_size\n        assert isinstance(self.is_dynamic, bool)\n        option.is_dynamic = self.is_dynamic\n        assert isinstance(self.long_to_int, bool)\n        option.long_to_int = self.long_to_int\n        assert type(self.device_id) is int\n        option.device_id = self.device_id\n        assert type(self.unconst_ops_thres) is int\n        option.unconst_ops_thres = self.unconst_ops_thres\n        assert type(self.use_nvidia_tf32) is bool\n        option.use_nvidia_tf32 = self.use_nvidia_tf32\n        assert type(self.preprocess_mode) is int\n        option.preprocess_mode = self.preprocess_mode\n        assert type(self.unsupport_op_list) is list\n        option.unsupport_op_list = self.unsupport_op_list\n\n        return option",
  "def set_device(self, device):\n        \"\"\"set device\"\"\"\n        if device not in PorosOptions.available_devices:\n            raise TypeError(\"device for poros invalid, only %s supported, fix it\" % (PorosOptions.available_devices))\n        self.device = device",
  "def set_debug(self, debug):\n        \"\"\"set debug\"\"\"\n        if debug not in PorosOptions.available_debug_mode:\n            raise TypeError(\"device for poros invalid, only %s supported, fix it\" % (PorosOptions.available_debug_mode))\n        self.debug = debug",
  "def __init__(self, cpp_module):\n        super(PorosModule, self).__init__(cpp_module)",
  "def _construct(cpp_module, init_fn):\n        \"\"\"\n        Construct a PorosModule that's ready for use. \n        Args:\n            cpp_module:  The C++ Module that will hold the actual state of\n                            this PorosModule instance.\n            init_fn:  Lambda that initializes the PorosModule passed to it.\n        \"\"\"\n        script_module = PorosModule(cpp_module)\n        init_fn(script_module)\n\n        # Finalize the ScriptModule: replace the nn.Module state with our\n        # custom implementations and flip the _initializing bit.\n        PorosModule._finalize_scriptmodule(script_module)\n        return script_module",
  "def supported_engine(self):\n        \"\"\"supported engine\"\"\"\n        return [\"tensorrt\"]",
  "def _register_with_torch():\n    poros_dir = os.path.dirname(__file__)\n    torch.ops.load_library(poros_dir + '/lib/libporos.so')",
  "def _parse_device(device):\n    # type: Any -> poros._C.Device\n    \"\"\"\n    converter device info to Device struct that can handle in poros\n    \"\"\"\n    if isinstance(device, poros._C.Device):\n        return device\n    elif isinstance(device, str):\n        if device == \"GPU\" or device == \"gpu\":\n            return poros._C.Device.GPU\n        elif device == \"CPU\" or device == \"cpu\":\n            return poros._C.Device.CPU\n        elif device == \"XPU\" or device == \"xpu\":\n            return poros._C.Device.XPU\n        else:\n            ValueError(\"Got a device type unknown (type: \" + str(device) + \")\")\n    else:\n        raise TypeError(\"Device specification must be of type string or poros.Device, but got: \" +\n                str(type(device)))",
  "def wrap_cpp_module(cpp_module):\n    \"\"\"\n    Wrap torch._C.ScriptModule to porosModule, recursively for all submodules\n    \"\"\"\n    def init_fn(script_module):\n        \"\"\"init_fn\"\"\"\n        for name, cpp_module in torch._C.ModuleDict(script_module._c).items():\n            setattr(script_module, name, wrap_cpp_module(cpp_module))\n        script_module._concrete_type = torch._C.ConcreteModuleType.from_jit_type(script_module._c._type())\n\n        for idx, fn in enumerate(script_module._c._get_forward_pre_hooks()):\n            script_module._forward_pre_hooks[idx] = fn\n        for idx, fn in enumerate(script_module._c._get_forward_hooks()):\n            script_module._forward_hooks[idx] = fn\n\n    return PorosModule._construct(cpp_module, init_fn)",
  "def load(filename, poros_options):\n    \"\"\"\n    Args:\n        filename( str): poros model save path\n        poros_options(PorosOptions / Dict of settings): compile settings for poros\n    Returns:\n        PorosModule: Compiled Module of poros, \n                    when run it will partially execute via inlined engine (which is TensorRT)\n    \"\"\"\n    compiled_cpp_mod = poros._C.load(filename, convert_poros_option(poros_options))\n    compiled_module = wrap_cpp_module(compiled_cpp_mod)\n    return compiled_module",
  "def save(module, filename):\n    \"\"\"\n    Args:\n        module\uff08PorosModule\uff09: poros module\n        filename( str): poros model save path\n    \"\"\"\n    assert type(module).__name__ == \"PorosModule\", \"The type of module must be PorosModule\"\n    assert type(filename).__name__ == \"str\", \"The type of filename must be str\"\n    module.save(filename)",
  "def compile(module, prewarm_inputs, poros_options):\n    \"\"\"\n    Compile a TorchScriptModule/nn.Module to porosModule\n    Converts specifically the forward method of the original Module\n    Args:\n        module (torch.nn.Module / torch.jit.ScriptModule): Source module\n        input (list of tensor input): prewarmed data.\n        poros_options(PorosOptions): compile settings for poros\n    Returns:\n        PorosModule: Compiled Module of poros, \n                    when run it will partially execute via inlined engine (which is TensorRT)\n    \"\"\"  \n    if poros_options.device == \"GPU\":  \n        assert \"cuda\" in str(list(module.state_dict().values())[0].device), \\\n            \"If the poros_options.device is GPU, the module.device should also is GPU\"\n\n    sp_model = None\n    if isinstance(module, torch.jit.ScriptModule):\n        sp_model = module\n    else:\n        if poros_options.preprocess_mode == 0:\n            sp_model = torch.jit.script(module, optimize=None, _frames_up=0, _rcb=None)\n        elif poros_options.preprocess_mode == 1:\n            sp_model = torch.jit.trace(module, prewarm_inputs[0])\n        else:\n            raise ValueError(\n                \"preprocess_mode value err: The range of preprocess_mode is [0,1]\")\n    \n    if sp_model is None:\n        raise TypeError(\n            \"can't trans to poros module currently\")\n\n    wraped_inputs = convert_prewarm_inputs(prewarm_inputs)\n\n    compiled_cpp_mod = poros._C.compile_graph(sp_model._c, wraped_inputs, convert_poros_option(poros_options))\n    compiled_module = wrap_cpp_module(compiled_cpp_mod)\n    return compiled_module",
  "def init_fn(script_module):\n        \"\"\"init_fn\"\"\"\n        for name, cpp_module in torch._C.ModuleDict(script_module._c).items():\n            setattr(script_module, name, wrap_cpp_module(cpp_module))\n        script_module._concrete_type = torch._C.ConcreteModuleType.from_jit_type(script_module._c._type())\n\n        for idx, fn in enumerate(script_module._c._get_forward_pre_hooks()):\n            script_module._forward_pre_hooks[idx] = fn\n        for idx, fn in enumerate(script_module._c._get_forward_hooks()):\n            script_module._forward_hooks[idx] = fn",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model\", required=True, help=\"Path of PaddleClas model.\")\n    parser.add_argument(\n        \"--image\", type=str, required=False, help=\"Path of test image file.\")\n    parser.add_argument(\n        \"--cpu_num_thread\",\n        type=int,\n        default=8,\n        help=\"default number of cpu thread.\")\n    parser.add_argument(\n        \"--device_id\", type=int, default=0, help=\"device(gpu) id\")\n    parser.add_argument(\n        \"--profile_mode\",\n        type=str,\n        default=\"runtime\",\n        help=\"runtime or end2end.\")\n    parser.add_argument(\n        \"--repeat\",\n        required=True,\n        type=int,\n        default=1000,\n        help=\"number of repeats for profiling.\")\n    parser.add_argument(\n        \"--warmup\",\n        required=True,\n        type=int,\n        default=50,\n        help=\"number of warmup for profiling.\")\n    parser.add_argument(\n        \"--device\",\n        default=\"cpu\",\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"default\",\n        help=\"inference backend, default, ort, ov, trt, paddle, paddle_trt.\")\n    parser.add_argument(\n        \"--enable_trt_fp16\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable fp16 in trt backend\")\n    parser.add_argument(\n        \"--enable_collect_memory_info\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable collect memory info\")\n    parser.add_argument(\n        \"--include_h2d_d2h\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether run profiling with h2d and d2h\")\n    args = parser.parse_args()\n    return args",
  "def build_option(args):\n    option = fd.RuntimeOption()\n    device = args.device\n    backend = args.backend\n    enable_trt_fp16 = args.enable_trt_fp16\n    if args.profile_mode == \"runtime\":\n        option.enable_profiling(args.include_h2d_d2h, args.repeat, args.warmup)\n    option.set_cpu_thread_num(args.cpu_num_thread)\n    if device == \"gpu\":\n        option.use_gpu()\n        if backend == \"ort\":\n            option.use_ort_backend()\n        elif backend == \"paddle\":\n            option.use_paddle_backend()\n        elif backend == \"ov\":\n            option.use_openvino_backend()\n            option.set_openvino_device(name=\"GPU\")\n            # change name and shape for models\n            option.set_openvino_shape_info({\"x\": [1, 3, 224, 224]})\n        elif backend in [\"trt\", \"paddle_trt\"]:\n            option.use_trt_backend()\n            if backend == \"paddle_trt\":\n                option.use_paddle_infer_backend()\n                option.paddle_infer_option.enable_trt = True\n                # Set max_batch_size 1 for best performance\n                option.trt_option.max_batch_size = 1\n            if enable_trt_fp16:\n                option.enable_trt_fp16()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with GPU, only support default/ort/paddle/trt/paddle_trt now, {} is not supported.\".\n                format(backend))\n    elif device == \"cpu\":\n        if backend == \"ort\":\n            option.use_ort_backend()\n        elif backend == \"ov\":\n            option.use_openvino_backend()\n        elif backend == \"paddle\":\n            option.use_paddle_backend()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with CPU, only support default/ort/ov/paddle now, {} is not supported.\".\n                format(backend))\n    else:\n        raise Exception(\n            \"Only support device CPU/GPU now, {} is not supported.\".format(\n                device))\n\n    return option",
  "class StatBase(object):\n    \"\"\"StatBase\"\"\"\n    nvidia_smi_path = \"nvidia-smi\"\n    gpu_keys = ('index', 'uuid', 'name', 'timestamp', 'memory.total',\n                'memory.free', 'memory.used', 'utilization.gpu',\n                'utilization.memory')\n    nu_opt = ',nounits'\n    cpu_keys = ('cpu.util', 'memory.util', 'memory.used')",
  "class Monitor(StatBase):\n    \"\"\"Monitor\"\"\"\n\n    def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()\n\n    def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()\n\n    def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result\n\n    def output(self):\n        return self.result\n\n    def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return",
  "def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()",
  "def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()",
  "def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result",
  "def output(self):\n        return self.result",
  "def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model\", required=True, help=\"Path of Yolo onnx model.\")\n    parser.add_argument(\n        \"--image\", type=str, required=False, help=\"Path of test image file.\")\n    parser.add_argument(\n        \"--cpu_num_thread\",\n        type=int,\n        default=8,\n        help=\"default number of cpu thread.\")\n    parser.add_argument(\n        \"--device_id\", type=int, default=0, help=\"device(gpu) id\")\n    parser.add_argument(\n        \"--iter_num\",\n        required=True,\n        type=int,\n        default=300,\n        help=\"number of iterations for computing performance.\")\n    parser.add_argument(\n        \"--device\",\n        default=\"cpu\",\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"default\",\n        help=\"inference backend, default, ort, ov, trt, paddle, paddle_trt.\")\n    parser.add_argument(\n        \"--enable_trt_fp16\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable fp16 in trt backend\")\n    parser.add_argument(\n        \"--enable_collect_memory_info\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable collect memory info\")\n    args = parser.parse_args()\n    return args",
  "def build_option(args):\n    option = fd.RuntimeOption()\n    device = args.device\n    backend = args.backend\n    enable_trt_fp16 = args.enable_trt_fp16\n    option.set_cpu_thread_num(args.cpu_num_thread)\n    if device == \"gpu\":\n        option.use_gpu()\n        if backend == \"ort\":\n            option.use_ort_backend()\n        elif backend == \"paddle\":\n            option.use_paddle_backend()\n        elif backend == \"ov\":\n            option.use_openvino_backend()\n            option.set_openvino_device(name=\"GPU\")\n            # change name and shape for models\n            option.set_openvino_shape_info({\"images\": [1, 3, 640, 640]})\n        elif backend in [\"trt\", \"paddle_trt\"]:\n            option.use_trt_backend()\n            if backend == \"paddle_trt\":\n                option.use_paddle_infer_backend()\n                option.paddle_infer_option.enable_trt = True\n            if enable_trt_fp16:\n                option.enable_trt_fp16()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with GPU, only support default/ort/paddle/trt/paddle_trt now, {} is not supported.\".\n                format(backend))\n    elif device == \"cpu\":\n        if backend == \"ort\":\n            option.use_ort_backend()\n        elif backend == \"ov\":\n            option.use_openvino_backend()\n        elif backend == \"paddle\":\n            option.use_paddle_backend()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with CPU, only support default/ort/ov/paddle now, {} is not supported.\".\n                format(backend))\n    else:\n        raise Exception(\n            \"Only support device CPU/GPU now, {} is not supported.\".format(\n                device))\n\n    return option",
  "class StatBase(object):\n    \"\"\"StatBase\"\"\"\n    nvidia_smi_path = \"nvidia-smi\"\n    gpu_keys = ('index', 'uuid', 'name', 'timestamp', 'memory.total',\n                'memory.free', 'memory.used', 'utilization.gpu',\n                'utilization.memory')\n    nu_opt = ',nounits'\n    cpu_keys = ('cpu.util', 'memory.util', 'memory.used')",
  "class Monitor(StatBase):\n    \"\"\"Monitor\"\"\"\n\n    def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()\n\n    def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()\n\n    def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result\n\n    def output(self):\n        return self.result\n\n    def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return",
  "def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()",
  "def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()",
  "def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result",
  "def output(self):\n        return self.result",
  "def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model\", required=True, help=\"Path of PaddleSeg model.\")\n    parser.add_argument(\n        \"--image\", type=str, required=False, help=\"Path of test image file.\")\n    parser.add_argument(\n        \"--cpu_num_thread\",\n        type=int,\n        default=8,\n        help=\"default number of cpu thread.\")\n    parser.add_argument(\n        \"--device_id\", type=int, default=0, help=\"device(gpu) id\")\n    parser.add_argument(\n        \"--iter_num\",\n        required=True,\n        type=int,\n        default=300,\n        help=\"number of iterations for computing performance.\")\n    parser.add_argument(\n        \"--device\",\n        default=\"cpu\",\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"default\",\n        help=\"inference backend, default, ort, ov, trt, paddle, paddle_trt.\")\n    parser.add_argument(\n        \"--enable_trt_fp16\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable fp16 in trt backend\")\n    parser.add_argument(\n        \"--enable_collect_memory_info\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable collect memory info\")\n    args = parser.parse_args()\n    return args",
  "def build_option(args):\n    option = fd.RuntimeOption()\n    device = args.device\n    backend = args.backend\n    enable_trt_fp16 = args.enable_trt_fp16\n    option.set_cpu_thread_num(args.cpu_num_thread)\n    if device == \"gpu\":\n        option.use_gpu()\n        if backend == \"ort\":\n            option.use_ort_backend()\n        elif backend == \"paddle\":\n            option.use_paddle_backend()\n        elif backend == \"ov\":\n            option.use_openvino_backend()\n            option.set_openvino_device(name=\"GPU\")  # use gpu\n            # change name and shape for models\n            option.set_openvino_shape_info({\"x\": [1, 3, 512, 512]})\n        elif backend in [\"trt\", \"paddle_trt\"]:\n            option.use_trt_backend()\n            if \"Deeplabv3_ResNet101\" in args.model or \"FCN_HRNet_W18\" in args.model or \"Unet_cityscapes\" in args.model or \"PP_LiteSeg_B_STDC2_cityscapes\" in args.model:\n                option.trt_option.set_shape(\"x\", [1, 3, 1024, 2048],\n                                            [1, 3, 1024, 2048],\n                                            [1, 3, 1024, 2048])\n            elif \"Portrait_PP_HumanSegV2_Lite_256x144\" in args.model:\n                option.trt_option.set_shape(\n                    \"x\", [1, 3, 144, 256], [1, 3, 144, 256], [1, 3, 144, 256])\n            elif \"PP_HumanSegV1_Server\" in args.model:\n                option.trt_option.set_shape(\n                    \"x\", [1, 3, 512, 512], [1, 3, 512, 512], [1, 3, 512, 512])\n            else:\n                option.trt_option.set_shape(\n                    \"x\", [1, 3, 192, 192], [1, 3, 192, 192], [1, 3, 192, 192])\n            if backend == \"paddle_trt\":\n                option.paddle_infer_option.collect_trt_shape = True\n                option.use_paddle_infer_backend()\n                option.paddle_infer_option.enable_trt = True\n            if enable_trt_fp16:\n                option.enable_trt_fp16()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with GPU, only support default/ort/paddle/trt/paddle_trt now, {} is not supported.\".\n                format(backend))\n    elif device == \"cpu\":\n        if backend == \"ort\":\n            option.use_ort_backend()\n        elif backend == \"ov\":\n            option.use_openvino_backend()\n        elif backend == \"paddle\":\n            option.use_paddle_backend()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with CPU, only support default/ort/ov/paddle now, {} is not supported.\".\n                format(backend))\n    else:\n        raise Exception(\n            \"Only support device CPU/GPU now, {} is not supported.\".format(\n                device))\n\n    return option",
  "class StatBase(object):\n    \"\"\"StatBase\"\"\"\n    nvidia_smi_path = \"nvidia-smi\"\n    gpu_keys = ('index', 'uuid', 'name', 'timestamp', 'memory.total',\n                'memory.free', 'memory.used', 'utilization.gpu',\n                'utilization.memory')\n    nu_opt = ',nounits'\n    cpu_keys = ('cpu.util', 'memory.util', 'memory.used')",
  "class Monitor(StatBase):\n    \"\"\"Monitor\"\"\"\n\n    def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()\n\n    def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()\n\n    def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result\n\n    def output(self):\n        return self.result\n\n    def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return",
  "def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()",
  "def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()",
  "def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result",
  "def output(self):\n        return self.result",
  "def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_dir\",\n        required=True,\n        help=\"The directory of model and tokenizer.\")\n    parser.add_argument(\n        \"--data_path\", required=True, help=\"The path of uie data.\")\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='cpu',\n        choices=['gpu', 'cpu'],\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default='paddle',\n        choices=['ort', 'paddle', 'trt', 'paddle_trt', 'ov'],\n        help=\"The inference runtime backend.\")\n    parser.add_argument(\n        \"--device_id\", type=int, default=0, help=\"device(gpu) id\")\n    parser.add_argument(\n        \"--batch_size\", type=int, default=1, help=\"The batch size of data.\")\n    parser.add_argument(\n        \"--max_length\",\n        type=int,\n        default=128,\n        help=\"The max length of sequence.\")\n    parser.add_argument(\n        \"--cpu_num_threads\",\n        type=int,\n        default=8,\n        help=\"The number of threads when inferring on cpu.\")\n    parser.add_argument(\n        \"--enable_trt_fp16\",\n        type=distutils.util.strtobool,\n        default=False,\n        help=\"whether enable fp16 in trt backend\")\n    parser.add_argument(\n        \"--epoch\", type=int, default=1, help=\"The epoch of test\")\n    parser.add_argument(\n        \"--enable_collect_memory_info\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable collect memory info\")\n    return parser.parse_args()",
  "def build_option(args):\n    option = fd.RuntimeOption()\n    if args.device == 'cpu':\n        option.use_cpu()\n        option.set_cpu_thread_num(args.cpu_num_threads)\n    else:\n        option.use_gpu(args.device_id)\n    if args.backend == 'paddle':\n        option.use_paddle_backend()\n    elif args.backend == 'ort':\n        option.use_ort_backend()\n    elif args.backend == 'ov':\n        option.use_openvino_backend()\n    else:\n        option.use_trt_backend()\n        if args.backend == 'paddle_trt':\n            option.paddle_infer_option.collect_trt_shape = True\n            option.use_paddle_infer_backend()\n            option.paddle_infer_option.enable_trt = True\n        trt_file = os.path.join(args.model_dir, \"infer.trt\")\n        option.trt_option.set_shape(\n            'input_ids',\n            min_shape=[1, 1],\n            opt_shape=[args.batch_size, args.max_length // 2],\n            max_shape=[args.batch_size, args.max_length])\n        option.trt_option.set_shape(\n            'token_type_ids',\n            min_shape=[1, 1],\n            opt_shape=[args.batch_size, args.max_length // 2],\n            max_shape=[args.batch_size, args.max_length])\n        option.trt_option.set_shape(\n            'pos_ids',\n            min_shape=[1, 1],\n            opt_shape=[args.batch_size, args.max_length // 2],\n            max_shape=[args.batch_size, args.max_length])\n        option.trt_option.set_shape(\n            'att_mask',\n            min_shape=[1, 1],\n            opt_shape=[args.batch_size, args.max_length // 2],\n            max_shape=[args.batch_size, args.max_length])\n        if args.enable_trt_fp16:\n            option.enable_trt_fp16()\n            trt_file = trt_file + \".fp16\"\n        option.set_trt_cache_file(trt_file)\n    return option",
  "class StatBase(object):\n    \"\"\"StatBase\"\"\"\n    nvidia_smi_path = \"nvidia-smi\"\n    gpu_keys = ('index', 'uuid', 'name', 'timestamp', 'memory.total',\n                'memory.free', 'memory.used', 'utilization.gpu',\n                'utilization.memory')\n    nu_opt = ',nounits'\n    cpu_keys = ('cpu.util', 'memory.util', 'memory.used')",
  "class Monitor(StatBase):\n    \"\"\"Monitor\"\"\"\n\n    def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()\n\n    def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()\n\n    def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result\n\n    def output(self):\n        return self.result\n\n    def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return",
  "def get_dataset(data_path, max_seq_len=512):\n    json_lines = []\n    with open(data_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            json_line = json.loads(line)\n            content = json_line['content'].strip()\n            prompt = json_line['prompt']\n            # Model Input is aslike: [CLS] Prompt [SEP] Content [SEP]\n            # It include three summary tokens.\n            if max_seq_len <= len(prompt) + 3:\n                raise ValueError(\n                    \"The value of max_seq_len is too small, please set a larger value\"\n                )\n            json_lines.append(json_line)\n\n    return json_lines",
  "def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()",
  "def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()",
  "def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result",
  "def output(self):\n        return self.result",
  "def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_dir\",\n        required=True,\n        help=\"The directory of model and tokenizer.\")\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='gpu',\n        choices=['gpu', 'cpu'],\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default='pp',\n        choices=['ort', 'pp', 'trt', 'pp-trt'],\n        help=\"The inference runtime backend.\")\n    parser.add_argument(\n        \"--device_id\", type=int, default=0, help=\"device(gpu) id\")\n    parser.add_argument(\n        \"--batch_size\", type=int, default=32, help=\"The batch size of data.\")\n    parser.add_argument(\n        \"--max_length\",\n        type=int,\n        default=128,\n        help=\"The max length of sequence.\")\n    parser.add_argument(\n        \"--log_interval\",\n        type=int,\n        default=10,\n        help=\"The interval of logging.\")\n    parser.add_argument(\n        \"--cpu_num_threads\",\n        type=int,\n        default=1,\n        help=\"The number of threads when inferring on cpu.\")\n    parser.add_argument(\n        \"--use_fp16\",\n        type=distutils.util.strtobool,\n        default=False,\n        help=\"Use FP16 mode\")\n    parser.add_argument(\n        \"--use_fast\",\n        type=distutils.util.strtobool,\n        default=True,\n        help=\"Whether to use fast_tokenizer to accelarate the tokenization.\")\n    return parser.parse_args()",
  "def create_fd_runtime(args):\n    option = fd.RuntimeOption()\n    model_path = os.path.join(args.model_dir, \"infer.pdmodel\")\n    params_path = os.path.join(args.model_dir, \"infer.pdiparams\")\n    option.set_model_path(model_path, params_path)\n    if args.device == 'cpu':\n        option.use_cpu()\n        option.set_cpu_thread_num(args.cpu_num_threads)\n    else:\n        option.use_gpu(args.device_id)\n    if args.backend == 'pp':\n        option.use_paddle_backend()\n    elif args.backend == 'ort':\n        option.use_ort_backend()\n    else:\n        option.use_trt_backend()\n        if args.backend == 'pp-trt':\n            option.enable_paddle_to_trt()\n            option.enable_paddle_trt_collect_shape()\n        trt_file = os.path.join(args.model_dir, \"infer.trt\")\n        option.set_trt_input_shape(\n            'input_ids',\n            min_shape=[1, args.max_length],\n            opt_shape=[args.batch_size, args.max_length],\n            max_shape=[args.batch_size, args.max_length])\n        option.set_trt_input_shape(\n            'token_type_ids',\n            min_shape=[1, args.max_length],\n            opt_shape=[args.batch_size, args.max_length],\n            max_shape=[args.batch_size, args.max_length])\n        if args.use_fp16:\n            option.enable_trt_fp16()\n            trt_file = trt_file + \".fp16\"\n        option.set_trt_cache_file(trt_file)\n    return fd.Runtime(option)",
  "def convert_examples_to_data(dataset, batch_size):\n    texts, text_pairs, labels = [], [], []\n    batch_text, batch_text_pair, batch_label = [], [], []\n\n    for i, item in enumerate(dataset):\n        batch_text.append(item['sentence1'])\n        batch_text_pair.append(item['sentence2'])\n        batch_label.append(item['label'])\n        if (i + 1) % batch_size == 0:\n            texts.append(batch_text)\n            text_pairs.append(batch_text_pair)\n            labels.append(batch_label)\n            batch_text, batch_text_pair, batch_label = [], [], []\n    return texts, text_pairs, labels",
  "def postprocess(logits):\n    max_value = np.max(logits, axis=1, keepdims=True)\n    exp_data = np.exp(logits - max_value)\n    probs = exp_data / np.sum(exp_data, axis=1, keepdims=True)\n    out_dict = {\n        \"label\": probs.argmax(axis=-1),\n        \"confidence\": probs.max(axis=-1)\n    }\n    return out_dict",
  "def get_statistics_table(tokenizer_time_costs, runtime_time_costs,\n                         postprocess_time_costs):\n    x = PrettyTable()\n    x.field_names = [\n        \"Stage\", \"Mean latency\", \"P50 latency\", \"P90 latency\", \"P95 latency\"\n    ]\n    x.add_row([\n        \"Tokenization\", f\"{np.mean(tokenizer_time_costs):.4f}\",\n        f\"{np.percentile(tokenizer_time_costs, 50):.4f}\",\n        f\"{np.percentile(tokenizer_time_costs, 90):.4f}\",\n        f\"{np.percentile(tokenizer_time_costs, 95):.4f}\"\n    ])\n    x.add_row([\n        \"Runtime\", f\"{np.mean(runtime_time_costs):.4f}\",\n        f\"{np.percentile(runtime_time_costs, 50):.4f}\",\n        f\"{np.percentile(runtime_time_costs, 90):.4f}\",\n        f\"{np.percentile(runtime_time_costs, 95):.4f}\"\n    ])\n    x.add_row([\n        \"Postprocessing\", f\"{np.mean(postprocess_time_costs):.4f}\",\n        f\"{np.percentile(postprocess_time_costs, 50):.4f}\",\n        f\"{np.percentile(postprocess_time_costs, 90):.4f}\",\n        f\"{np.percentile(postprocess_time_costs, 95):.4f}\"\n    ])\n    return x",
  "def get_current_memory_mb(gpu_id=None):\n    pid = os.getpid()\n    p = psutil.Process(pid)\n    info = p.memory_full_info()\n    cpu_mem = info.uss / 1024. / 1024.\n    gpu_mem = 0\n    if gpu_id is not None:\n        pynvml.nvmlInit()\n        handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_id)\n        meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n        gpu_mem = meminfo.used / 1024. / 1024.\n    return cpu_mem, gpu_mem",
  "def get_current_gputil(gpu_id):\n    GPUs = GPUtil.getGPUs()\n    gpu_load = GPUs[gpu_id].load\n    return gpu_load",
  "def sample_gpuutil(gpu_id, gpu_utilization=[]):\n    while True:\n        gpu_utilization.append(get_current_gputil(gpu_id))\n        time.sleep(0.01)",
  "def show_statistics(tokenizer_time_costs,\n                    runtime_time_costs,\n                    postprocess_time_costs,\n                    correct_num,\n                    total_num,\n                    cpu_mem,\n                    gpu_mem,\n                    gpu_util,\n                    prefix=\"\"):\n    print(\n        f\"{prefix}Acc =  {correct_num/total_num*100:.2f} ({correct_num} /{total_num}).\"\n        f\" CPU memory: {np.mean(cpu_mem):.2f} MB, GPU memory: {np.mean(gpu_mem):.2f} MB,\"\n        f\" GPU utilization {np.max(gpu_util) * 100:.2f}%.\")\n    print(\n        get_statistics_table(tokenizer_time_costs, runtime_time_costs,\n                             postprocess_time_costs))",
  "def run_inference(warmup_steps=None):\n        tokenizer_time_costs = []\n        runtime_time_costs = []\n        postprocess_time_costs = []\n        cpu_mem = []\n        gpu_mem = []\n\n        total_num = 0\n        correct_num = 0\n\n        manager = multiprocessing.Manager()\n        gpu_util = manager.list()\n        p = multiprocessing.Process(\n            target=sample_gpuutil, args=(gpu_id, gpu_util))\n        p.start()\n        for i, (text, text_pair,\n                label) in enumerate(zip(texts, text_pairs, labels)):\n            # Start the process to sample gpu utilization\n            start = time.time()\n            encoded_inputs = tokenizer(\n                text=text,\n                text_pair=text_pair,\n                max_length=args.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='np')\n            tokenizer_time_costs += [(time.time() - start) * 1000]\n\n            start = time.time()\n            input_map = {\n                input_ids_name: encoded_inputs[\"input_ids\"].astype('int64'),\n                token_type_ids_name:\n                encoded_inputs[\"token_type_ids\"].astype('int64'),\n            }\n            results = runtime.infer(input_map)\n            runtime_time_costs += [(time.time() - start) * 1000]\n\n            start = time.time()\n            output = postprocess(results[0])\n            postprocess_time_costs += [(time.time() - start) * 1000]\n\n            cm, gm = get_current_memory_mb(gpu_id)\n            cpu_mem.append(cm)\n            gpu_mem.append(gm)\n\n            total_num += len(label)\n            correct_num += (label == output[\"label\"]).sum()\n            if warmup_steps is not None and i >= warmup_steps:\n                break\n            if (i + 1) % args.log_interval == 0:\n                show_statistics(tokenizer_time_costs, runtime_time_costs,\n                                postprocess_time_costs, correct_num, total_num,\n                                cpu_mem, gpu_mem, gpu_util,\n                                f\"Step {i + 1: 6d}: \")\n        show_statistics(tokenizer_time_costs, runtime_time_costs,\n                        postprocess_time_costs, correct_num, total_num,\n                        cpu_mem, gpu_mem, gpu_util, f\"Final statistics: \")\n        p.terminate()",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model\", required=True, help=\"Path of PaddleClas model.\")\n    parser.add_argument(\n        \"--image\", type=str, required=False, help=\"Path of test image file.\")\n    parser.add_argument(\n        \"--cpu_num_thread\",\n        type=int,\n        default=8,\n        help=\"default number of cpu thread.\")\n    parser.add_argument(\n        \"--device_id\", type=int, default=0, help=\"device(gpu) id\")\n    parser.add_argument(\n        \"--profile_mode\",\n        type=str,\n        default=\"runtime\",\n        help=\"runtime or end2end.\")\n    parser.add_argument(\n        \"--repeat\",\n        required=True,\n        type=int,\n        default=1000,\n        help=\"number of repeats for profiling.\")\n    parser.add_argument(\n        \"--warmup\",\n        required=True,\n        type=int,\n        default=50,\n        help=\"number of warmup for profiling.\")\n    parser.add_argument(\n        \"--device\",\n        default=\"cpu\",\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"default\",\n        help=\"inference backend, default, ort, ov, trt, paddle, paddle_trt.\")\n    parser.add_argument(\n        \"--enable_trt_fp16\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable fp16 in trt backend\")\n    parser.add_argument(\n        \"--enable_lite_fp16\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable fp16 in Paddle Lite backend\")\n    parser.add_argument(\n        \"--enable_collect_memory_info\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable collect memory info\")\n    parser.add_argument(\n        \"--include_h2d_d2h\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether run profiling with h2d and d2h\")\n    args = parser.parse_args()\n    return args",
  "def build_option(args):\n    option = fd.RuntimeOption()\n    device = args.device\n    backend = args.backend\n    enable_trt_fp16 = args.enable_trt_fp16\n    enable_lite_fp16 = args.enable_lite_fp16\n    if args.profile_mode == \"runtime\":\n        option.enable_profiling(args.include_h2d_d2h, args.repeat, args.warmup)\n    option.set_cpu_thread_num(args.cpu_num_thread)\n    if device == \"gpu\":\n        option.use_gpu()\n        if backend == \"ort\":\n            option.use_ort_backend()\n        elif backend == \"paddle\":\n            option.use_paddle_backend()\n        elif backend == \"ov\":\n            option.use_openvino_backend()\n            # Using GPU and CPU heterogeneous execution mode\n            option.set_openvino_device(\"HETERO:GPU,CPU\")\n            # change name and shape for models\n            option.set_openvino_shape_info({\n                \"image\": [1, 3, 320, 320],\n                \"scale_factor\": [1, 2]\n            })\n            # Set CPU up operator\n            option.set_openvino_cpu_operators([\"MulticlassNms\"])\n        elif backend in [\"trt\", \"paddle_trt\"]:\n            option.use_trt_backend()\n            if backend == \"paddle_trt\":\n                option.use_paddle_infer_backend()\n                option.paddle_infer_option.enable_trt = True\n            if enable_trt_fp16:\n                option.enable_trt_fp16()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with GPU, only support default/ort/paddle/trt/paddle_trt now, {} is not supported.\".\n                format(backend))\n    elif device == \"cpu\":\n        if backend == \"ort\":\n            option.use_ort_backend()\n        elif backend == \"ov\":\n            option.use_openvino_backend()\n        elif backend == \"paddle\":\n            option.use_paddle_backend()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with CPU, only support default/ort/ov/paddle now, {} is not supported.\".\n                format(backend))\n    elif device == \"kunlunxin\":\n        option.use_kunlunxin()\n        if backend == \"lite\":\n            option.use_lite_backend()\n        elif backend == \"ort\":\n            option.use_ort_backend()\n        elif backend == \"paddle\":\n            option.use_paddle_backend()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with CPU, only support default/ort/lite/paddle now, {} is not supported.\".\n                format(backend))\n    elif device == \"ascend\":\n        option.use_ascend()\n        if backend == \"lite\":\n            option.use_lite_backend()\n            if enable_lite_fp16:\n                option.enable_lite_fp16()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with CPU, only support default/lite now, {} is not supported.\".\n                format(backend))\n    else:\n        raise Exception(\n            \"Only support device CPU/GPU/Kunlunxin/Ascend now, {} is not supported.\".\n            format(device))\n\n    return option",
  "class StatBase(object):\n    \"\"\"StatBase\"\"\"\n    nvidia_smi_path = \"nvidia-smi\"\n    gpu_keys = ('index', 'uuid', 'name', 'timestamp', 'memory.total',\n                'memory.free', 'memory.used', 'utilization.gpu',\n                'utilization.memory')\n    nu_opt = ',nounits'\n    cpu_keys = ('cpu.util', 'memory.util', 'memory.used')",
  "class Monitor(StatBase):\n    \"\"\"Monitor\"\"\"\n\n    def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()\n\n    def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()\n\n    def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result\n\n    def output(self):\n        return self.result\n\n    def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return",
  "def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()",
  "def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()",
  "def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result",
  "def output(self):\n        return self.result",
  "def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return",
  "def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_dir\", required=True, help=\"Model dir of PPOCR.\")\n    parser.add_argument(\n        \"--det_model\", required=True, help=\"Path of Detection model of PPOCR.\")\n    parser.add_argument(\n        \"--cls_model\",\n        required=True,\n        help=\"Path of Classification model of PPOCR.\")\n    parser.add_argument(\n        \"--rec_model\",\n        required=True,\n        help=\"Path of Recognization model of PPOCR.\")\n    parser.add_argument(\n        \"--rec_label_file\",\n        required=True,\n        help=\"Path of Recognization label file of PPOCR.\")\n    parser.add_argument(\n        \"--image\", type=str, required=False, help=\"Path of test image file.\")\n    parser.add_argument(\n        \"--cpu_num_thread\",\n        type=int,\n        default=8,\n        help=\"default number of cpu thread.\")\n    parser.add_argument(\n        \"--device_id\", type=int, default=0, help=\"device(gpu) id\")\n    parser.add_argument(\n        \"--iter_num\",\n        required=True,\n        type=int,\n        default=300,\n        help=\"number of iterations for computing performance.\")\n    parser.add_argument(\n        \"--device\",\n        default=\"cpu\",\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"default\",\n        help=\"inference backend, default, ort, ov, trt, paddle, paddle_trt.\")\n    parser.add_argument(\n        \"--enable_trt_fp16\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable fp16 in trt backend\")\n    parser.add_argument(\n        \"--enable_collect_memory_info\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable collect memory info\")\n    args = parser.parse_args()\n    return args",
  "def build_option(args):\n    option = fd.RuntimeOption()\n    device = args.device\n    backend = args.backend\n    enable_trt_fp16 = args.enable_trt_fp16\n    option.set_cpu_thread_num(args.cpu_num_thread)\n    if device == \"gpu\":\n        option.use_gpu()\n        if backend == \"ort\":\n            option.use_ort_backend()\n        elif backend == \"paddle\":\n            option.use_paddle_backend()\n        elif backend in [\"trt\", \"paddle_trt\"]:\n            option.use_trt_backend()\n            if backend == \"paddle_trt\":\n                option.paddle_infer_option.collect_trt_shape = True\n                option.use_paddle_infer_backend()\n                option.paddle_infer_option.enable_trt = True\n            if enable_trt_fp16:\n                option.enable_trt_fp16()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with GPU, only support default/ort/paddle/trt/paddle_trt now, {} is not supported.\".\n                format(backend))\n    elif device == \"cpu\":\n        if backend == \"ort\":\n            option.use_ort_backend()\n        elif backend == \"ov\":\n            option.use_openvino_backend()\n        elif backend == \"paddle\":\n            option.use_paddle_backend()\n        elif backend == \"default\":\n            return option\n        else:\n            raise Exception(\n                \"While inference with CPU, only support default/ort/ov/paddle now, {} is not supported.\".\n                format(backend))\n    else:\n        raise Exception(\n            \"Only support device CPU/GPU now, {} is not supported.\".format(\n                device))\n\n    return option",
  "class StatBase(object):\n    \"\"\"StatBase\"\"\"\n    nvidia_smi_path = \"nvidia-smi\"\n    gpu_keys = ('index', 'uuid', 'name', 'timestamp', 'memory.total',\n                'memory.free', 'memory.used', 'utilization.gpu',\n                'utilization.memory')\n    nu_opt = ',nounits'\n    cpu_keys = ('cpu.util', 'memory.util', 'memory.used')",
  "class Monitor(StatBase):\n    \"\"\"Monitor\"\"\"\n\n    def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()\n\n    def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()\n\n    def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result\n\n    def output(self):\n        return self.result\n\n    def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return",
  "def __init__(self, use_gpu=False, gpu_id=0, interval=0.1):\n        self.result = {}\n        self.gpu_id = gpu_id\n        self.use_gpu = use_gpu\n        self.interval = interval\n        self.cpu_stat_q = multiprocessing.Queue()",
  "def start(self):\n        cmd = '%s --id=%s --query-gpu=%s --format=csv,noheader%s -lms 50' % (\n            StatBase.nvidia_smi_path, self.gpu_id, ','.join(StatBase.gpu_keys),\n            StatBase.nu_opt)\n        if self.use_gpu:\n            self.gpu_stat_worker = subprocess.Popen(\n                cmd,\n                stderr=subprocess.STDOUT,\n                stdout=subprocess.PIPE,\n                shell=True,\n                close_fds=True,\n                preexec_fn=os.setsid)\n        # cpu stat\n        pid = os.getpid()\n        self.cpu_stat_worker = multiprocessing.Process(\n            target=self.cpu_stat_func,\n            args=(self.cpu_stat_q, pid, self.interval))\n        self.cpu_stat_worker.start()",
  "def stop(self):\n        try:\n            if self.use_gpu:\n                os.killpg(self.gpu_stat_worker.pid, signal.SIGUSR1)\n            # os.killpg(p.pid, signal.SIGTERM)\n            self.cpu_stat_worker.terminate()\n            self.cpu_stat_worker.join(timeout=0.01)\n        except Exception as e:\n            print(e)\n            return\n\n        # gpu\n        if self.use_gpu:\n            lines = self.gpu_stat_worker.stdout.readlines()\n            lines = [\n                line.strip().decode(\"utf-8\") for line in lines\n                if line.strip() != ''\n            ]\n            gpu_info_list = [{\n                k: v\n                for k, v in zip(StatBase.gpu_keys, line.split(', '))\n            } for line in lines]\n            if len(gpu_info_list) == 0:\n                return\n            result = gpu_info_list[0]\n            for item in gpu_info_list:\n                for k in item.keys():\n                    if k not in [\"name\", \"uuid\", \"timestamp\"]:\n                        result[k] = max(int(result[k]), int(item[k]))\n                    else:\n                        result[k] = max(result[k], item[k])\n            self.result['gpu'] = result\n\n        # cpu\n        cpu_result = {}\n        if self.cpu_stat_q.qsize() > 0:\n            cpu_result = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n        while not self.cpu_stat_q.empty():\n            item = {\n                k: v\n                for k, v in zip(StatBase.cpu_keys, self.cpu_stat_q.get())\n            }\n            for k in StatBase.cpu_keys:\n                cpu_result[k] = max(cpu_result[k], item[k])\n        cpu_result['name'] = cpuinfo.get_cpu_info()['brand_raw']\n        self.result['cpu'] = cpu_result",
  "def output(self):\n        return self.result",
  "def cpu_stat_func(self, q, pid, interval=0.0):\n        \"\"\"cpu stat function\"\"\"\n        stat_info = psutil.Process(pid)\n        while True:\n            # pid = os.getpid()\n            cpu_util, mem_util, mem_use = stat_info.cpu_percent(\n            ), stat_info.memory_percent(), round(stat_info.memory_info().rss /\n                                                 1024.0 / 1024.0, 4)\n            q.put([cpu_util, mem_util, mem_use])\n            time.sleep(interval)\n        return"
]