[
  "#!/usr/bin/env python\n# encoding: utf-8\n# __author__ = 'Demon'\nfrom seedemu.layers import Base, Routing, Ebgp, PeerRelationship, Ibgp, Ospf\nfrom seedemu.compiler import Docker\nfrom seedemu.services import DomainNameCachingService\nfrom seedemu.core import Emulator, Binding, Filter, Node\nfrom typing import List\n\nsim = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nibgp = Ibgp()\nospf = Ospf()\nldns = DomainNameCachingService()\n\n\ndef make_stub_as(asn: int, exchange: str):\n    stub_as = base.createAutonomousSystem(asn)\n    host = stub_as.createHost('host0')\n    host1 = stub_as.createHost('host1')\n    host2 = stub_as.createHost('host2')\n    host3 = stub_as.createHost('host3')\n    host4 = stub_as.createHost('host4')\n    host5 = stub_as.createHost('host5')\n    ldns_host = stub_as.createHost('ldns') #used for local dns service\n\n    router = stub_as.createRouter('router0')\n    net = stub_as.createNetwork('net0')\n\n    \n    router.joinNetwork('net0')\n    host.joinNetwork('net0')\n    host1.joinNetwork('net0')\n    host2.joinNetwork('net0')\n    host3.joinNetwork('net0')\n    host4.joinNetwork('net0')\n    host5.joinNetwork('net0')\n    ldns_host.joinNetwork('net0')\n\n    router.joinNetwork(exchange)\n\n##############Install local DNS###############################################\nldns.install('local-dns-150').setConfigureResolvconf(True)\nldns.install('local-dns-151').setConfigureResolvconf(True)\nldns.install('local-dns-152').setConfigureResolvconf(True)\nldns.install('local-dns-153').setConfigureResolvconf(True)\nldns.install('local-dns-154').setConfigureResolvconf(True)\nldns.install('local-dns-160').setConfigureResolvconf(True)\nldns.install('local-dns-161').setConfigureResolvconf(True)\n\n#Add bindings for local dns:\nsim.addBinding(Binding('local-dns-150', filter = Filter(asn=150, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-151', filter = Filter(asn=151, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-152', filter = Filter(asn=152, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-153', filter = Filter(asn=153, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-154', filter = Filter(asn=154, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-160', filter = Filter(asn=160, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-161', filter = Filter(asn=161, nodeName=\"ldns\")))\n\n##############################################################################\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\nbase.createInternetExchange(102)\n\nmake_stub_as(150, 'ix100')\nmake_stub_as(151, 'ix100')\n\nmake_stub_as(152, 'ix101')\nmake_stub_as(153, 'ix101')\nmake_stub_as(154, 'ix101')\n\nmake_stub_as(160, 'ix102')\nmake_stub_as(161, 'ix102')\n\n###############################################################################\n\nas2 = base.createAutonomousSystem(2)\n\nas2_100 = as2.createRouter('r0')\nas2_101 = as2.createRouter('r1')\nas2_102 = as2.createRouter('r2')\n\nas2_100.joinNetwork('ix100')\nas2_101.joinNetwork('ix101')\nas2_102.joinNetwork('ix102')\n\nas2_net_100_101 = as2.createNetwork('n01')\nas2_net_101_102 = as2.createNetwork('n12')\nas2_net_102_100 = as2.createNetwork('n20')\n\n\n\n\n\nas2_100.joinNetwork('n01')\nas2_101.joinNetwork('n01')\n\nas2_101.joinNetwork('n12')\nas2_102.joinNetwork('n12')\n\nas2_102.joinNetwork('n20')\nas2_100.joinNetwork('n20')\n\n###############################################################################\n\nas3 = base.createAutonomousSystem(3)\n\nas3_101 = as3.createRouter('r1')\nas3_102 = as3.createRouter('r2')\n\nas3_101.joinNetwork('ix101')\nas3_102.joinNetwork('ix102')\n\nas3_net_101_102 = as3.createNetwork('n12')\n\n\n\nas3_101.joinNetwork('n12')\nas3_102.joinNetwork('n12')\n\n###############################################################################\n\nebgp.addPrivatePeering(100, 2, 150, PeerRelationship.Provider)\nebgp.addPrivatePeering(100, 150, 151, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(101, 2, 3, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 154, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 154, PeerRelationship.Provider)\n\n\nebgp.addPrivatePeering(102, 2, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 161, PeerRelationship.Provider)\n\n###############################################################################\n\n\nsim.addLayer(base)\nsim.addLayer(routing)\nsim.addLayer(ebgp)\nsim.addLayer(ibgp)\nsim.addLayer(ospf)\nsim.addLayer(ldns)\n\nsim.dump('base-component.bin')",
  "#!/usr/bin/env python3\n# \n\nimport argparse\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nrc('text', usetex=True)\n\n# 装饰子：指明被装饰函数输入的是笛卡尔坐标点\nfrom fealpy.decorator import cartesian\n\n# 网格工厂：生成常用的简单区域上的网格\nfrom fealpy.mesh import MeshFactory as MF\nfrom fealpy.mesh import HalfEdgeMesh2d\n\n# 均匀剖分的时间离散\nfrom fealpy.timeintegratoralg import UniformTimeLine\n\n# 热传导 pde 模型\nfrom fealpy.pde.heatequation_model_2d import ExpExpData\n\n# Lagrange 有限元空间\nfrom fealpy.functionspace import LagrangeFiniteElementSpace\n\n# Dirichlet 边界条件\nfrom fealpy.boundarycondition import DirichletBC \nfrom fealpy.tools.show import showmultirate\n\n# solver\nfrom scipy.sparse.linalg import spsolve\n\n#拷贝对象\nimport copy\n\n## 参数解析\nparser = argparse.ArgumentParser(description=\n        \"\"\"\n        三角形网格自适应有限元方法求解热传导方程\n        \"\"\")\n\nparser.add_argument('--ns',\n        default=10, type=int,\n        help='空间各个方向剖分段数， 默认剖分 10 段.')\n\nparser.add_argument('--nt',\n        default=100, type=int,\n        help='时间剖分段数，默认剖分 100 段.')\n\nparser.add_argument('--tol',\n        default=0.05, type=float,\n        help='自适应加密停止阈值，默认设定为 0.05.')\n\nparser.add_argument('--rtheta',\n        default=0.7, type=float,\n        help='自适应加密参数，默认设定为 0.7.')\n\nparser.add_argument('--ctheta',\n        default=0.3, type=float,\n        help='自适应粗化参数，默认设定为 0.3.')\n\nargs = parser.parse_args()\n\nns = args.ns\nnt = args.nt\ntol = args.tol\n\nrtheta = args.rtheta \nctheta = args.ctheta \n\npde = ExpExpData()\ndomain = pde.domain()\nc = pde.diffusionCoefficient\n\ntmesh = UniformTimeLine(0, 1, nt) # 均匀时间剖分\n\nsmesh = MF.boxmesh2d(domain, nx=ns, ny=ns, meshtype='tri')\nsmesh = HalfEdgeMesh2d.from_mesh(smesh, NV=3) # 三角形网格的单边数据结构\n\nsmesh.add_plot(plt)\nplt.savefig('./test-' + str(0) + '.png')\nplt.close()\ni = 0   \nwhile True:\n\n    # 初始网格的自适应\n    space = LagrangeFiniteElementSpace(smesh, p=1) # 构造线性元空间\n    # 当前时间步的有限元解\n    uh0 = space.interpolation(pde.init_value)\n    eta = space.recovery_estimate(uh0, method='area_harmonic')\n    err = np.sqrt(np.sum(eta**2))\n    if err < tol:\n        break\n    isMarkedCell = smesh.refine_marker(eta, rtheta, method='L2')\n    smesh.refine_triangle_rg(isMarkedCell)\n    i += 1\n    smesh.add_plot(plt)\n    plt.savefig('./test-' + str(i+1) + '.png')\n    plt.close()\n\nspace = LagrangeFiniteElementSpace(smesh, p=1)\nuh0 = space.interpolation(pde.init_value)\n\nfor j in range(0, nt): \n\n    # 下一个的时间层 t1\n    t1 = tmesh.next_time_level()\n    print(\"t1=\", t1)\n\n    while True:\n        # 下一层时间步的有限元解\n        uh1 = space.function()\n        A = c*space.stiff_matrix() # 刚度矩阵\n        M = space.mass_matrix() # 质量矩阵\n        dt = tmesh.current_time_step_length() # 时间步长\n        G = M + dt*A # 隐式迭代矩阵\n\n        # t1 时间层的右端项\n        @cartesian\n        def source(p):\n            return pde.source(p, t1)\n        F = space.source_vector(source)\n        F *= dt\n        F += M@uh0\n\n        # t1 时间层的 Dirichlet 边界条件处理\n        @cartesian\n        def dirichlet(p):\n            return pde.dirichlet(p, t1)\n        bc = DirichletBC(space, dirichlet)\n        GD, F = bc.apply(G, F, uh1)\n        \n        # 代数系统求解\n        uh1[:] = spsolve(GD, F)\n        eta = space.recovery_estimate(uh1, method='area_harmonic')\n        err = np.sqrt(np.sum(eta**2))\n        print('errrefine', err)\n        if err < tol:\n            break\n        else:\n            #加密并插值\n            NN0 = smesh.number_of_nodes()\n            edge = smesh.entity('edge')\n            isMarkedCell = smesh.refine_marker(eta, rtheta, method='L2')\n            smesh.refine_triangle_rg(isMarkedCell)\n            i += 1\n            smesh.add_plot(plt)\n            plt.savefig('./test-'+str(i+1)+'.png')\n            plt.close()\n            space = LagrangeFiniteElementSpace(smesh, p=1)\n            print('refinedof', space.number_of_global_dofs())\n            uh00 = space.function()\n            nn2e = smesh.newnode2edge\n            uh00[:NN0] = uh0\n            uh00[NN0:] = np.average(uh0[edge[nn2e]], axis=-1)\n            uh0 = space.function()\n            uh0[:] = uh00\n    #粗化网格并插值\n    isMarkedCell = smesh.refine_marker(eta, ctheta, 'COARSEN')\n    smesh.coarsen_triangle_rg(isMarkedCell)\n    i += 1\n    smesh.add_plot(plt)\n    plt.savefig('./test-'+str(i+1)+'.png')\n    plt.close()\n    space = LagrangeFiniteElementSpace(smesh, p=1)\n    print('coarsendof', space.number_of_global_dofs())\n    uh2 = space.function()\n    retain = smesh.retainnode\n    uh2[:] = uh1[retain]\n    uh1 = space.function()\n    uh0 = space.function()\n    uh1[:] = uh2\n\n    # t1 时间层的误差\n    @cartesian\n    def solution(p):\n        return pde.solution(p, t1)\n    error = space.integralalg.error(solution, uh1)\n    print(\"error:\", error)\n\n    #画数值解图像\n    if (t1 ==0.01) | (t1 == 0.49) | (t1==0.99):\n        fig = plt.figure()\n        axes = fig.add_subplot(1, 1, 1, projection='3d')\n        uh1.add_plot(axes, cmap='rainbow')\n    uh0[:] = uh1\n    uh1[:] = 0.0\n\n    # 时间步进一层 \n    tmesh.advance()\n\nplt.show()\n",
  "\n#!/usr/bin/env python3\n# \nimport sys\nimport argparse\nimport numpy as np\nfrom fealpy.pde.adi_2d import ADI_2d as PDE\nfrom fealpy.mesh import TriangleMesh\nfrom fealpy.mesh import MeshFactory as mf\nfrom fealpy.decorator import cartesian, barycentric\nfrom numpy.linalg import inv\nimport matplotlib.pyplot as plt\nfrom fealpy.functionspace import FirstKindNedelecFiniteElementSpace2d\nfrom fealpy.functionspace import ScaledMonomialSpace2d\nfrom fealpy.quadrature import  GaussLegendreQuadrature\nfrom scipy.sparse import csr_matrix, coo_matrix\nfrom numpy.linalg import inv\nfrom scipy.sparse import csr_matrix, spdiags, eye, bmat \t\t\nfrom scipy.sparse.linalg import spsolve\nfrom fealpy.tools.show import showmultirate, show_error_table\n\n\n## 参数解析\nparser = argparse.ArgumentParser(description=\n        \"\"\"\n        三角形网格上最低次混合RTN元\n        \"\"\")\n\nparser.add_argument('--nt',\n        default=100, type=int,\n        help='时间剖分段数，默认剖分 100 段.')\n        \nparser.add_argument('--ns',\n        default=5, type=int,\n        help='空间各个方向初始剖分段数， 默认剖分 10 段.')\n        \nparser.add_argument('--nmax',\n        default=5, type=int,\n        help='空间迭代次数， 默认迭代5次.')\n\t\t\t\t\t\n##\nargs = parser.parse_args()\nns = args.ns\nnt = args.nt\nnmax = args.nmax\n##初始网格\nbox = [0, 1, 0, 1]\nmesh = mf.boxmesh2d(box, nx=ns, ny=ns, meshtype='tri')  \n\n##\n###真解\nsigma = 3*np.pi\nepsilon = 1.0\nmu = 1.0\npde = PDE(sigma, epsilon, mu)\n\n\ntau = 1.0e-5\n\"\"\"\nerrorType = ['$|| E - E_h||_{\\Omega,0}$',\n\t\t'$|| U - H_h||_{\\Omega,0}$'\n             ]\n\nerrorMatrix = np.zeros((len(errorType), nmax), dtype=np.float64)\n\"\"\"\nerrorType = ['$|| E - E_h||_{\\Omega,0}$'\n             ]\n\nerrorMatrix = np.zeros((1, nmax), dtype=np.float64)\nNDof = np.zeros(nmax, dtype=np.float64)\n\nfor n in range(nmax):\n\t# 电场初始值\n\tspace = FirstKindNedelecFiniteElementSpace2d(mesh, p=0)\n\tdef init_E_value(p):\n\t\treturn pde.Efield(p, 0.5*tau)\t\n\tEh0 = space.interpolation(init_E_value)\n\tEh1 = space.function()\n\tgdof = space.number_of_global_dofs()\n\tNDof[n] = gdof \n\n\tsmspace = ScaledMonomialSpace2d(mesh, p=0)  #分片常数\n\t# 磁场初始值\n\tdef init_H_value(p):\n\t\treturn pde.Hz(p, tau)    \n\tHh0 = smspace.local_projection(init_H_value)\n\tHh1 = smspace.function()\n\t\n\tdef get_phi_curl_matrix():\n\t\tqf = mesh.integrator(q=9, etype='cell')\n\t\tbcs, ws = qf.get_quadrature_points_and_weights()\n\t\tcellmeasure = mesh.entity_measure('cell')\n\t\tps= mesh.bc_to_point(bcs) #(NQ, NC, GD)\n\t\n\t\tcurlpsi = space.curl_basis(bcs) #(NQ, NC, ldof) and ldof=3\n\t\tgdof = space.number_of_global_dofs()\n\t\tcell2dof = space.cell_to_dof() #(NC, ldof)\n\t\t\n\t\t\n\t\tphi = smspace.basis(ps) #(1,1,1)\n\t\tsmsgdof = smspace.number_of_global_dofs() #(NC,)\n\t\tsmscell2dof = smspace.cell_to_dof() #(NC, Lldof) and Lldof=1\n\t\t\n\t\tM = np.einsum('i, imd, ijk, j->jkd', ws, phi, curlpsi, cellmeasure, optimize=True)\n\t\t#print('M.shape=',M.shape)\n\t\t\n\t\t#(NC,ldof)-->(NC,ldof, 1)=M.shape\n\t\tI = cell2dof[:, :, None]\n\t\t#(NC,Lldof)-->(NC,Lldof,1)-->(NC, ldof, Lldof)=M.shape \n\t\tJ = np.broadcast_to(smscell2dof[:, :, None], shape=M.shape)\n\t\tM = csr_matrix((M.flat, (I.flat, J.flat)), shape=(gdof, smsgdof))\n\t\t\n\t\treturn M\n\t\n\tM_EMat = space.mass_matrix(epsilon)\n\tM_sigMat = space.mass_matrix(sigma)\n\tM_SMat = space.curl_matrix(1.0/mu)\n\tM_HMat = smspace.mass_matrix()\n\t\n\tM_CMat = get_phi_curl_matrix()\n\tTM_CMat = M_CMat.T #转置\n\t\n\tLMat = M_EMat + tau/2*M_sigMat + (tau**2/4)*M_SMat\n\tRMat = M_EMat - tau/2*M_sigMat + (tau**2/4)*M_SMat\n\t\n\tfor i in range(nt):\n\t\t# t1 时间层的计算Eh的右端项\n\t\tx = Hh0.T.flat # 把 Hh 按列展平\n\t\tRtH1 = tau*M_CMat@x\n\t\t\n\t\ty = Eh0.T.flat # 把 Eh 按列展平\n\t\tRtE1 = RMat@y\n\t\t@cartesian\n\t\tdef sol_g(p):\n\t\t\treturn pde.gsource(p, (i+1)*tau) \n\t\tRt1 = space.source_vector(sol_g)\n\t\tRt1 = tau*Rt1\t\n\t\tF1 = RtH1 + RtE1 + Rt1\n\t\t\n\t\t# 下一个时间层的边界条件处理,得到处理完边界之后的总刚和右端项\n\t\t# 下一个时间层的电场的计算\n\t\tedge2dof = space.dof.edge_to_dof()\n\t\tgdof = space.number_of_global_dofs()\n\t\tisDDof = np.zeros(gdof, dtype=np.bool_)\n\t\tindex = mesh.ds.boundary_edge_index()\n\t\tisDDof[edge2dof[index]] = True\n\t\n\t\tbdIdx = np.zeros(LMat.shape[0], dtype=np.int_)\n\t\tbdIdx[isDDof] = 1\n\t\tTbd = spdiags(bdIdx, 0, LMat.shape[0], LMat.shape[0])\n\t\tT = spdiags(1-bdIdx, 0, LMat.shape[0], LMat.shape[0])\t\n\t\tA1 = T@LMat@T + Tbd\n\t\tF1[isDDof] = y[isDDof]\n\t\tEh1[:] = spsolve(A1, F1)\n\t\t\n\t\t\n\t\t# 下一个时间层磁场的计算\n\t\tA2 = mu*M_HMat\n\t\t@cartesian\n\t\tdef sol_fz(p):\n\t\t\treturn pde.fzsource(p, (i+1.5)*tau)\n\t\t\n\t\tRt2 = smspace.source_vector(sol_fz)\t\n\t\tRt2 = tau*Rt2\n\t\ty1 = Eh1.T.flat #下一个时间层的Eh\n\t\tF2 = mu*M_HMat@x - tau*TM_CMat@y1 + Rt2\n\t\tHh1[:] = spsolve(A2, F2)\n\t\n\t\tEh0 = Eh1 \n\t\tHh0 = Hh1\n\t\n\t# 最后一个时间层nmax的电场的真解E\n\t@cartesian\n\tdef solutionE(p):\n\t\treturn pde.Efield(p, (nt + 1.5)*tau)\n\t\t\t\n\terrorMatrix[0, n] = space.integralalg.error(solutionE, Eh0)\t\n\t# 最后一个时间层itmax的磁场的真解Hz\n\t@cartesian\n\tdef solutionH(p):\n\t\treturn pde.Hz(p, (nt + 2)*tau)\n\t\t\n\t#errorMatrix[1, n] = smspace.integralalg.error(solutionH, Hh0)\n\t\n\tif n < nmax - 1:\n\t\tmesh.uniform_refine()\n\t\t\n\t\n\nprint(\"errorMatrix = \", errorMatrix)\n\nshowmultirate(plt, 0, NDof, errorMatrix,  errorType, propsize=20)   \nplt.show()\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n\t\n\t\n\t\n\t\n\t\n\n\n\n\t\n\t\n\t\n\n\n",
  "#!/usr/bin/env python\n\n# Author: Junzi Liu <latrix1247@gmail.com>\n\n'''\nCalculate the effective electronic coupling based on single determinant diabatic\nstates.\n\nHere the diabatic states are calcuated by mom-SCF(HF or DFT). And the direct\nelectronic coupling is obtained using Hartree-Fock formlism and it can use both\nHF and DFT wave functions because of their single determinant form.  Within\nmom-SCF, it is supposed to evaluate the electronic coupling between any two\nstates.\n'''\n\nimport os\nimport numpy\nfrom pyscf import gto\nfrom pyscf import scf\nfrom pyscf import dft\n\n\nmol = gto.Mole()\nmol.verbose = 3\nmol.atom = [\n [\"C\",  ( 0.000000,  0.418626, 0.000000)],\n [\"H\",  (-0.460595,  1.426053, 0.000000)],\n [\"O\",  ( 1.196516,  0.242075, 0.000000)],\n [\"N\",  (-0.936579, -0.568753, 0.000000)],\n [\"H\",  (-0.634414, -1.530889, 0.000000)],\n [\"H\",  (-1.921071, -0.362247, 0.000000)]\n]\nmol.basis = {\"H\": '6-311++g**',\n             \"O\": '6-311++g**',\n             \"N\": '6-311++g**',\n             \"C\": '6-311++g**',\n             }\nmol.build()\n\n# First state calculation with DFT\na = dft.UKS(mol)\na.xc='b3lyp'\n# Store molecular orbital information into chkfile\na.chkfile='nh2cho_s0.chkfile'\na.scf()\nmo0 = a.mo_coeff\nocc0 = a.mo_occ\n\n# Set initial ouoccupation pattern for excited state\nocc0[0][11] = 0.0\nocc0[0][12] = 1.0\n\n# Second state calculation with DFT \nb = dft.UKS(mol)\nb.xc='b3lyp'\n# Store molecular orbital information into another chkfile\nb.chkfile='nh2cho_s1.chkfile'\ndm = b.make_rdm1(mo0, occ0)\n# Use mom method to determine occupation number\nscf.addons.mom_occ_(b, mo0, occ0)\nb.scf(dm)\n\n# Read the MO coefficients and occupation numbers from chkfile.\n# So the calculation of electronic coupling can be carried out \n# standalone use chkfiles.\nmo0 = scf.chkfile.load('nh2cho_s0.chkfile', 'scf/mo_coeff')\nocc0 = scf.chkfile.load('nh2cho_s0.chkfile', 'scf/mo_occ')\nmo1 = scf.chkfile.load('nh2cho_s1.chkfile', 'scf/mo_coeff')\nocc1 = scf.chkfile.load('nh2cho_s1.chkfile', 'scf/mo_occ')\n\nmf = scf.UHF(mol)\n# Calculate overlap between two determiant <I|F>\ns, x = mf.det_ovlp(mo0, mo1, occ0, occ1)\n\n# Construct density matrix \ndm_s0 = mf.make_rdm1(mo0, occ0)\ndm_s1 = mf.make_rdm1(mo1, occ1)\ndm_01 = mf.make_asym_dm(mo0, mo1, occ0, occ1, x)\n\n# One-electron part contrbution\nh1e = mf.get_hcore(mol)\ne1_s0 = numpy.einsum('ji,ji', h1e.conj(), dm_s0[0]+dm_s0[1])\ne1_s1 = numpy.einsum('ji,ji', h1e.conj(), dm_s1[0]+dm_s1[1])\ne1_01 = numpy.einsum('ji,ji', h1e.conj(), dm_01[0]+dm_01[1])\n\n# Two-electron part contrbution. D_{IF} is asymmetric\nvhf_s0 = mf.get_veff(mol, dm_s0)\nvhf_s1 = mf.get_veff(mol, dm_s1)\nvhf_01 = mf.get_veff(mol, dm_01, hermi=0)\n\n# New total energy: <I|H|I>, <F|H|F>, <I|H|F>\ne_s0 = mf.energy_elec(dm_s0, h1e, vhf_s0)\ne_s1 = mf.energy_elec(dm_s1, h1e, vhf_s1)\ne_01 = mf.energy_elec(dm_01, h1e, vhf_01)\n\nprint('The overlap between these two determiants is: %12.8f' % s)\nprint('E_1e(I),  E_JK(I),  E_tot(I):  %15.7f, %13.7f, %15.7f' % (e1_s0, e_s0[1], e_s0[0]))\nprint('E_1e(F),  E_JK(F),  E_tot(I):  %15.7f, %13.7f, %15.7f' % (e1_s1, e_s1[1], e_s1[0]))\nprint('E_1e(IF), E_JK(IF), E_tot(IF): %15.7f, %13.7f, %15.7f' % (e1_01, e_01[1], e_01[0]))\nprint(' <I|H|F> coupling is: %12.7f a.u.' % (e_01[0]*s))\nprint('(0.5*s*H_II+H_FF) is: %12.7f a.u.' % (0.5*s*(e_s0[0]+e_s1[0])))\n\n# Calculate the effective electronic coupling\n# V_{IF} = \\frac{1}{1-S_{IF}^2}\\left| H_{IF} - S_{IF}\\frac{H_{II}+H_{FF}}{2} \\right|\nv01 = s*(e_01[0]-(e_s0[0]+e_s1[0])*0.5)/(1.0 - s*s)\nprint('The effective coupling is: %7.5f eV' % (numpy.abs(v01)*27.211385) )\n\n#remove chkfile if necessary\nos.remove('nh2cho_s0.chkfile')\nos.remove('nh2cho_s1.chkfile')\n",
  "#!/usr/bin/env python\n\nimport logging\n\nimport capytaine as cpt\nfrom capytaine.bem.airy_waves import airy_waves_free_surface_elevation\nfrom capytaine.ui.vtk.animation import Animation\n\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)s:\\t%(message)s\")\n\n# Generate the mesh of a sphere\n\nfull_mesh = cpt.mesh_sphere(radius=3, center=(0, 0, 0), resolution=(20, 20))\nfull_sphere = cpt.FloatingBody(mesh=full_mesh)\nfull_sphere.add_translation_dof(name=\"Heave\")\n\n# Keep only the immersed part of the mesh\nsphere = full_sphere.immersed_part()\n\n# Set up and solve problem\nsolver = cpt.BEMSolver()\n\ndiffraction_problem = cpt.DiffractionProblem(body=sphere, wave_direction=0.0, omega=2.0)\ndiffraction_result = solver.solve(diffraction_problem)\n\nradiation_problem = cpt.RadiationProblem(body=sphere, radiating_dof=\"Heave\", omega=2.0)\nradiation_result = solver.solve(radiation_problem)\n\n# Define a mesh of the free surface and compute the free surface elevation\nfree_surface = cpt.FreeSurface(x_range=(-50, 50), y_range=(-50, 50), nx=150, ny=150)\ndiffraction_elevation_at_faces = solver.compute_free_surface_elevation(free_surface, diffraction_result)\nradiation_elevation_at_faces = solver.compute_free_surface_elevation(free_surface, radiation_result)\n\n# Add incoming waves\ndiffraction_elevation_at_faces = diffraction_elevation_at_faces + airy_waves_free_surface_elevation(free_surface, diffraction_problem)\n\n# Run the animations\nanimation = Animation(loop_duration=diffraction_result.period)\nanimation.add_body(full_sphere, faces_motion=None)\nanimation.add_free_surface(free_surface, faces_elevation=0.5*diffraction_elevation_at_faces)\nanimation.run(camera_position=(-30, -30, 30))  # The camera is oriented towards (0, 0, 0) by default.\n# animation.save(\"path/to/the/video/file.ogv\", camera_position=(-30, -30, 30))\n\nanimation = Animation(loop_duration=radiation_result.period)\nanimation.add_body(full_sphere, faces_motion=full_sphere.dofs[\"Heave\"])\nanimation.add_free_surface(free_surface, faces_elevation=3.0*radiation_elevation_at_faces)\nanimation.run(camera_position=(-30, -30, 30))\n# animation.save(\"path/to/the/video/file.ogv\", camera_position=(-30, -30, 30))\n\n",
  "from seedemu.layers import Base, Routing, Ebgp\nfrom seedemu.services import WebService\nfrom seedemu.compiler import Docker\nfrom seedemu.core import Emulator, Binding, Filter\n\n# Initialize the emulator and layers\nemu = Emulator()\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nweb = WebService()\n\n###############################################################################\n# Create an Internet Exchange\nix100 = base.createInternetExchange(100)\n\n# Set map metadata\nix100_net = ix100.getPeeringLan()\nix100_net.setDisplayName('Seattle Internet Exchange')\nix100_net.setDescription('The largest IX in Seattle.')\n\n###############################################################################\n# Create and set up the AS 150\n\n# Create an autonomous system \nas150 = base.createAutonomousSystem(150)\n\n# Create a network \nnet150 = as150.createNetwork('net0')\n\n# Set map metadata\nnet150.setDisplayName('AS150 Backbone')\nnet150.setDescription('This is the main network of AS150.')\n\n# Create a router and connect it to two networks\nas150_router = as150.createRouter('router0')\nas150_router.joinNetwork('net0')\nas150_router.joinNetwork('ix100')\n\n# Set more map metadata\nas150_router.setDisplayName('AS150 Core Router')\nas150_router.setDescription('The core router of AS150.')\n\n# Create a host called web and connect it to a network\nas150.createHost('web').joinNetwork('net0')\n\n# Create a web service on virtual node, give this node a name\nweb.install('web150')\n\n# Bind the virtual node to a host \nemu.addBinding(Binding('web150', filter = Filter(nodeName = 'web', asn = 150)))\n\n\n###############################################################################\n# Create and set up the AS 151\n# It is similar to what is done to AS 150\n\nas151 = base.createAutonomousSystem(151)\nas151.createNetwork('net0')\n\nas151.createHost('web').joinNetwork('net0')\nweb.install('web151')\nemu.addBinding(Binding('web151', filter = Filter(nodeName = 'web', asn = 151)))\n\nas151_router = as151.createRouter('router0')\nas151_router.joinNetwork('net0')\nas151_router.joinNetwork('ix100')\n\n###############################################################################\n# Create and set up the AS 152\n# It is similar to what is done to AS 150\n\nas152 = base.createAutonomousSystem(152)\nas152.createNetwork('net0')\n\nas152.createHost('web').joinNetwork('net0')\nweb.install('web152')\nemu.addBinding(Binding('web152', filter = Filter(nodeName = 'web', asn = 152)))\n\nas152_router = as152.createRouter('router0')\nas152_router.joinNetwork('net0')\nas152_router.joinNetwork('ix100')\n\n###############################################################################\n# Peering the ASes\n\nebgp.addRsPeer(100, 150)\nebgp.addRsPeer(100, 151)\nebgp.addRsPeer(100, 152)\n\n###############################################################################\n# Rendering \n\nemu.addLayer(base)\nemu.addLayer(routing)\nemu.addLayer(ebgp)\nemu.addLayer(web)\n\nemu.render()\n\n###############################################################################\n# Compilation\n\nemu.compile(Docker(internetMapEnabled = True), './output')\n",
  "#!/usr/bin/env python\n\n\"\"\"\nThe crystalfontz 635 has these characters in ROM:\n\n....X. ...... ......\n...XX. .XXXXX ..XXX.\n..XXX. .XXXXX .XXXXX\n.XXXX. .XXXXX .XXXXX\n..XXX. .XXXXX .XXXXX\n...XX. .XXXXX ..XXX.\n....X. ...... ......\n...... ...... ......\n  0x11   0xd0   0xbb\n\nBy adding the characters in CGRAM below we can use them as part of a\nhorizontal slider control, selected check box and selected radio button\nrespectively.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport sys\n\nimport urwid.lcd_display\n\nCGRAM = \"\"\"\n...... ...... ...... ...... ..X... ...... ...... ......\nXXXXXX XXXXXX XXXXXX XXXXXX X.XX.. .XXXXX ..XXX. .....X\n...... XX.... XXXX.. XXXXXX X.XXX. .X...X .X...X ....XX\n...... XX.... XXXX.. XXXXXX X.XXXX .X...X .X...X .X.XX.\n...... XX.... XXXX.. XXXXXX X.XXX. .X...X .X...X .XXX..\nXXXXXX XXXXXX XXXXXX XXXXXX X.XX.. .XXXXX ..XXX. ..X...\n...... ...... ...... ...... ..X... ...... ...... ......\n...... ...... ...... ...... ...... ...... ...... ......\n\"\"\"\n\ndef program_cgram(screen):\n    \"\"\"\n    Load the character data\n    \"\"\"\n    # convert .'s and X's above into integer data\n    cbuf = [list() for x in range(8)]\n    for row in CGRAM.strip().split('\\n'):\n        rowsegments = row.strip().split()\n        for num, r in enumerate(rowsegments):\n            accum = 0\n            for c in r:\n                accum = (accum << 1) + (c == 'X')\n            cbuf[num].append(accum)\n\n    for num, cdata in enumerate(cbuf):\n        screen.program_cgram(num, cdata)\n\nclass LCDCheckBox(urwid.CheckBox):\n    \"\"\"\n    A check box+label that uses only one character for the check box,\n    including custom CGRAM character\n    \"\"\"\n    states = {\n        True: urwid.SelectableIcon('\\xd0'),\n        False: urwid.SelectableIcon('\\x05'),\n    }\n    reserve_columns = 1\n\nclass LCDRadioButton(urwid.RadioButton):\n    \"\"\"\n    A radio button+label that uses only one character for the radio button,\n    including custom CGRAM character\n    \"\"\"\n    states = {\n        True: urwid.SelectableIcon('\\xbb'),\n        False: urwid.SelectableIcon('\\x06'),\n    }\n    reserve_columns = 1\n\nclass LCDProgressBar(urwid.FlowWidget):\n    \"\"\"\n    The \"progress bar\" used by the horizontal slider for this device,\n    using custom CGRAM characters\n    \"\"\"\n    segments = '\\x00\\x01\\x02\\x03'\n    def __init__(self, range, value):\n        self.range = range\n        self.value = value\n\n    def rows(self, size, focus=False):\n        return 1\n\n    def render(self, size, focus=False):\n        \"\"\"\n        Draw the bar with self.segments where [0] is empty and [-1]\n        is completely full\n        \"\"\"\n        (maxcol,) = size\n        steps = self.get_steps(size)\n        filled = urwid.int_scale(self.value, self.range, steps)\n        full_segments = int(filled / (len(self.segments) - 1))\n        last_char = filled % (len(self.segments) - 1) + 1\n        s = (self.segments[-1] * full_segments +\n            self.segments[last_char] +\n            self.segments[0] * (maxcol -full_segments - 1))\n        return urwid.Text(s).render(size)\n\n    def move_position(self, size, direction):\n        \"\"\"\n        Update and return the value one step +ve or -ve, based on\n        the size of the displayed bar.\n\n        direction -- 1 for +ve, 0 for -ve\n        \"\"\"\n        steps = self.get_steps(size)\n        filled = urwid.int_scale(self.value, self.range, steps)\n        filled += 2 * direction - 1\n        value = urwid.int_scale(filled, steps, self.range)\n        value = max(0, min(self.range - 1, value))\n        if value != self.value:\n            self.value = value\n            self._invalidate()\n        return value\n\n    def get_steps(self, size):\n        \"\"\"\n        Return the number of steps available given size for rendering\n        the bar and number of segments we can draw.\n        \"\"\"\n        (maxcol,) = size\n        return maxcol * (len(self.segments) - 1)\n\n\nclass LCDHorizontalSlider(urwid.WidgetWrap):\n    \"\"\"\n    A slider control using custom CGRAM characters\n    \"\"\"\n    def __init__(self, range, value, callback):\n        self.bar = LCDProgressBar(range, value)\n        cols = urwid.Columns([\n            ('fixed', 1, urwid.SelectableIcon('\\x11')),\n            self.bar,\n            ('fixed', 1, urwid.SelectableIcon('\\x04')),\n            ])\n        super().__init__(cols)\n        self.callback = callback\n\n    def keypress(self, size, key):\n        # move the slider based on which arrow is focused\n        if key == 'enter':\n            # use the correct size for adjusting the bar\n            self.bar.move_position((self._w.column_widths(size)[1],),\n                self._w.get_focus_column() != 0)\n            self.callback(self.bar.value)\n        else:\n            return super().keypress(size, key)\n\n\n\nclass MenuOption(urwid.Button):\n    \"\"\"\n    A menu option, indicated with a single arrow character\n    \"\"\"\n    def __init__(self, label, submenu):\n        super().__init__(\"\")\n        # use a Text widget for label, we want the cursor\n        # on the arrow not the label\n        self._label = urwid.Text(\"\")\n        self.set_label(label)\n\n        self._w = urwid.Columns([\n            ('fixed', 1, urwid.SelectableIcon('\\xdf')),\n            self._label])\n\n        urwid.connect_signal(self, 'click',\n            lambda option: show_menu(submenu))\n\n    def keypress(self, size, key):\n        if key == 'right':\n            key = 'enter'\n        return super().keypress(size, key)\n\n\nclass Menu(urwid.ListBox):\n    def __init__(self, widgets):\n        self.menu_parent = None\n        super().__init__(urwid.SimpleListWalker(widgets))\n\n    def keypress(self, size, key):\n        \"\"\"\n        Go back to the previous menu on cancel button (mapped to esc)\n        \"\"\"\n        key = super().keypress(size, key)\n        if key in ('left', 'esc') and self.menu_parent:\n            show_menu(self.menu_parent)\n        else:\n            return key\n\ndef build_menus():\n    cursor_option_group = []\n    def cursor_option(label, style):\n        \"a radio button that sets the cursor style\"\n        def on_change(b, state):\n            if state: screen.set_cursor_style(style)\n        b = LCDRadioButton(cursor_option_group, label,\n            screen.cursor_style == style)\n        urwid.connect_signal(b, 'change', on_change)\n        return b\n\n    def display_setting(label, range, fn):\n        slider = LCDHorizontalSlider(range, range/2, fn)\n        return urwid.Columns([\n            urwid.Text(label),\n            ('fixed', 10, slider),\n            ])\n\n    def led_custom(index):\n        def exp_scale_led(rg):\n            \"\"\"\n            apply an exponential transformation to values sent so\n            that apparent brightness increases in a natural way.\n            \"\"\"\n            return lambda value: screen.set_led_pin(index, rg,\n                [0, 1, 2, 3, 4, 5, 6, 8, 11, 14, 18,\n                23, 29, 38, 48, 61, 79, 100][value])\n\n        return urwid.Columns([\n            ('fixed', 2, urwid.Text('%dR' % index)),\n            LCDHorizontalSlider(18, 0, exp_scale_led(0)),\n            ('fixed', 2, urwid.Text(' G')),\n            LCDHorizontalSlider(18, 0, exp_scale_led(1)),\n            ])\n\n    menu_structure = [\n        ('Display Settings', [\n            display_setting('Brightness', 101, screen.set_backlight),\n            display_setting('Contrast', 76,\n                lambda x: screen.set_lcd_contrast(x + 75)),\n            ]),\n        ('Cursor Settings', [\n            cursor_option('Block', screen.CURSOR_BLINKING_BLOCK),\n            cursor_option('Underscore', screen.CURSOR_UNDERSCORE),\n            cursor_option('Block + Underscore',\n                screen.CURSOR_BLINKING_BLOCK_UNDERSCORE),\n            cursor_option('Inverting Block',\n                screen.CURSOR_INVERTING_BLINKING_BLOCK),\n            ]),\n        ('LEDs', [\n            led_custom(0),\n            led_custom(1),\n            led_custom(2),\n            led_custom(3),\n            ]),\n        ('About this Demo', [\n            urwid.Text(\"This is a demo of Urwid's CF635Display \"\n                \"module. If you need an interface for a limited \"\n                \"character display device this should serve as a \"\n                \"good example for implementing your own display \"\n                \"module and menu-driven application.\"),\n            ])\n        ]\n\n    def build_submenu(ms):\n        \"\"\"\n        Recursive menu building from structure above\n        \"\"\"\n        options = []\n        submenus = []\n        for opt in ms:\n            # shortform for MenuOptions\n            if type(opt) == tuple:\n                name, sub = opt\n                submenu = build_submenu(sub)\n                opt = MenuOption(name, submenu)\n                submenus.append(submenu)\n            options.append(opt)\n        menu = Menu(options)\n        for s in submenus:\n            s.menu_parent = menu\n        return menu\n    return build_submenu(menu_structure)\n\n\nscreen = urwid.lcd_display.CF635Screen(sys.argv[1])\n# set up our font\nprogram_cgram(screen)\nloop = urwid.MainLoop(build_menus(), screen=screen)\n# FIXME: want screen to know it is in narrow mode, or better yet,\n# do the unicode conversion for us\nurwid.set_encoding('narrow')\n\n\ndef show_menu(menu):\n    loop.widget = menu\n\nloop.run()\n\n",
  "################################################################################\n# Copyright (c) 2021 ContinualAI.                                              #\n# Copyrights licensed under the MIT License.                                   #\n# See the accompanying LICENSE file for terms.                                 #\n#                                                                              #\n# Date: 24-05-2020                                                             #\n# Author(s): Lorenzo Pellegrini                                                #\n# E-mail: contact@continualai.org                                              #\n# Website: avalanche.continualai.org                                           #\n################################################################################\n\n\"\"\"\nThis is a simple example on how to use the Evaluation Plugin.\n\"\"\"\n\nimport argparse\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import SGD\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor, RandomCrop\n\nfrom avalanche.benchmarks import nc_benchmark\nfrom avalanche.benchmarks.datasets.dataset_utils import default_dataset_location\nfrom avalanche.evaluation.metrics import (\n    forgetting_metrics,\n    accuracy_metrics,\n    labels_repartition_metrics,\n    loss_metrics,\n    cpu_usage_metrics,\n    timing_metrics,\n    gpu_usage_metrics,\n    ram_usage_metrics,\n    disk_usage_metrics,\n    MAC_metrics,\n    bwt_metrics,\n    forward_transfer_metrics,\n    class_accuracy_metrics,\n    amca_metrics,\n)\nfrom avalanche.models import SimpleMLP\nfrom avalanche.logging import (\n    InteractiveLogger,\n    TextLogger,\n    CSVLogger,\n    TensorboardLogger,\n)\nfrom avalanche.training.plugins import EvaluationPlugin\nfrom avalanche.training.supervised import Naive\n\n\ndef main(args):\n    # --- CONFIG\n    device = torch.device(\n        f\"cuda:{args.cuda}\" if torch.cuda.is_available() and args.cuda >= 0 else \"cpu\"\n    )\n    # ---------\n\n    # --- TRANSFORMATIONS\n    train_transform = transforms.Compose(\n        [\n            RandomCrop(28, padding=4),\n            ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n        ]\n    )\n    test_transform = transforms.Compose(\n        [ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n    )\n    # ---------\n\n    # --- BENCHMARK CREATION\n    mnist_train = MNIST(\n        root=default_dataset_location(\"mnist\"),\n        train=True,\n        download=True,\n        transform=train_transform,\n    )\n    mnist_test = MNIST(\n        root=default_dataset_location(\"mnist\"),\n        train=False,\n        download=True,\n        transform=test_transform,\n    )\n    benchmark = nc_benchmark(mnist_train, mnist_test, 5, task_labels=False, seed=1234)\n    # ---------\n\n    # MODEL CREATION\n    model = SimpleMLP(num_classes=benchmark.n_classes)\n\n    # DEFINE THE EVALUATION PLUGIN AND LOGGER\n    # The evaluation plugin manages the metrics computation.\n    # It takes as argument a list of metrics and a list of loggers.\n    # The evaluation plugin calls the loggers to serialize the metrics\n    # and save them in persistent memory or print them in the standard output.\n\n    # log to text file\n    text_logger = TextLogger(open(\"log.txt\", \"a\"))\n\n    # print to stdout\n    interactive_logger = InteractiveLogger()\n\n    csv_logger = CSVLogger()\n\n    tb_logger = TensorboardLogger()\n\n    eval_plugin = EvaluationPlugin(\n        accuracy_metrics(\n            minibatch=True,\n            epoch=True,\n            epoch_running=True,\n            experience=True,\n            stream=True,\n        ),\n        loss_metrics(\n            minibatch=True,\n            epoch=True,\n            epoch_running=True,\n            experience=True,\n            stream=True,\n        ),\n        class_accuracy_metrics(\n            epoch=True, stream=True, classes=list(range(benchmark.n_classes))\n        ),\n        amca_metrics(),\n        forgetting_metrics(experience=True, stream=True),\n        bwt_metrics(experience=True, stream=True),\n        forward_transfer_metrics(experience=True, stream=True),\n        cpu_usage_metrics(\n            minibatch=True,\n            epoch=True,\n            epoch_running=True,\n            experience=True,\n            stream=True,\n        ),\n        timing_metrics(\n            minibatch=True,\n            epoch=True,\n            epoch_running=True,\n            experience=True,\n            stream=True,\n        ),\n        ram_usage_metrics(\n            every=0.5, minibatch=True, epoch=True, experience=True, stream=True\n        ),\n        gpu_usage_metrics(\n            args.cuda,\n            every=0.5,\n            minibatch=True,\n            epoch=True,\n            experience=True,\n            stream=True,\n        ),\n        disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        MAC_metrics(minibatch=True, epoch=True, experience=True),\n        labels_repartition_metrics(on_train=True, on_eval=True),\n        loggers=[interactive_logger, text_logger, csv_logger, tb_logger],\n        collect_all=True,\n    )  # collect all metrics (set to True by default)\n\n    # CREATE THE STRATEGY INSTANCE (NAIVE)\n    cl_strategy = Naive(\n        model,\n        SGD(model.parameters(), lr=0.001, momentum=0.9),\n        CrossEntropyLoss(),\n        train_mb_size=500,\n        train_epochs=1,\n        eval_mb_size=100,\n        device=device,\n        evaluator=eval_plugin,\n        eval_every=1,\n    )\n\n    # TRAINING LOOP\n    print(\"Starting experiment...\")\n    results = []\n    for i, experience in enumerate(benchmark.train_stream):\n        print(\"Start of experience: \", experience.current_experience)\n        print(\"Current Classes: \", experience.classes_in_this_experience)\n\n        # train returns a dictionary containing last recorded value\n        # for each metric.\n        res = cl_strategy.train(experience, eval_streams=[benchmark.test_stream])\n        print(\"Training completed\")\n\n        print(\"Computing accuracy on the whole test set\")\n        # test returns a dictionary with the last metric collected during\n        # evaluation on that stream\n        results.append(cl_strategy.eval(benchmark.test_stream))\n\n    print(f\"Test metrics:\\n{results}\")\n\n    # Dict with all the metric curves,\n    # only available when `collect_all` is True.\n    # Each entry is a (x, metric value) tuple.\n    # You can use this dictionary to manipulate the\n    # metrics without avalanche.\n    all_metrics = cl_strategy.evaluator.get_all_metrics()\n    print(f\"Stored metrics: {list(all_metrics.keys())}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--cuda\",\n        type=int,\n        default=0,\n        help=\"Select zero-indexed cuda device. -1 to use CPU.\",\n    )\n    args = parser.parse_args()\n    main(args)\n",
  "import blenderproc as bproc\nimport random\nimport os\nimport numpy as np\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('scene_net_obj_path', help=\"Path to the used scene net `.obj` file, download via scripts/download_scenenet.py\")\nparser.add_argument('scene_texture_path', help=\"Path to the downloaded texture files, you can find them at http://tinyurl.com/zpc9ppb\")\nparser.add_argument('cc_material_path', nargs='?', default=\"resources/cctextures\", help=\"Path to CCTextures folder, see the /scripts for the download script.\")\nparser.add_argument('output_dir', nargs='?', default=\"examples/datasets/scenenet_with_cctextures/output\", help=\"Path to where the final files, will be saved\")\nargs = parser.parse_args()\n\nbproc.init()\n\n# Load the scenenet room and label its objects with category ids based on the nyu mapping\nlabel_mapping = bproc.utility.LabelIdMapping.from_csv(bproc.utility.resolve_resource(os.path.join('id_mappings', 'nyu_idset.csv')))\nobjs = bproc.loader.load_scenenet(args.scene_net_obj_path, args.scene_texture_path, label_mapping)\n\n# Load all recommended cc materials, however don't load their textures yet\ncc_materials = bproc.loader.load_ccmaterials(args.cc_material_path, preload=True)\n\n# Go through all objects\nfor obj in objs:\n    # For each material of the object\n    for i in range(len(obj.get_materials())):\n        # In 40% of all cases\n        if np.random.uniform(0, 1) <= 0.4:\n            # Replace the material with a random one from cc materials\n            obj.set_material(i, random.choice(cc_materials))\n\n# Now load all textures of the materials that were assigned to at least one object\nbproc.loader.load_ccmaterials(args.cc_material_path, fill_used_empty_materials=True)\n\n# In some scenes floors, walls and ceilings are one object that needs to be split first\n# Collect all walls\nwalls = bproc.filter.by_cp(objs, \"category_id\", label_mapping.id_from_label(\"wall\"))\n# Extract floors from the objects\nnew_floors = bproc.object.extract_floor(walls, new_name_for_object=\"floor\", should_skip_if_object_is_already_there=True)\n# Set category id of all new floors\nfor floor in new_floors:\n    floor.set_cp(\"category_id\", label_mapping.id_from_label(\"floor\"))\n# Add new floors to our total set of objects\nobjs += new_floors\n\n# Extract ceilings from the objects\nnew_ceilings = bproc.object.extract_floor(walls, new_name_for_object=\"ceiling\", up_vector_upwards=False, should_skip_if_object_is_already_there=True)\n# Set category id of all new ceiling\nfor ceiling in new_ceilings:\n    ceiling.set_cp(\"category_id\", label_mapping.id_from_label(\"ceiling\"))\n# Add new ceilings to our total set of objects\nobjs += new_ceilings\n\n# Make all lamp objects emit light\nlamps = bproc.filter.by_attr(objs, \"name\", \".*[l|L]amp.*\", regex=True)\nbproc.lighting.light_surface(lamps, emission_strength=15)\n# Also let all ceiling objects emit a bit of light, so the whole room gets more bright\nceilings = bproc.filter.by_attr(objs, \"name\", \".*[c|C]eiling.*\", regex=True)\nbproc.lighting.light_surface(ceilings, emission_strength=2, emission_color=[1,1,1,1])\n\n# Init bvh tree containing all mesh objects\nbvh_tree = bproc.object.create_bvh_tree_multi_objects(objs)\n\n# Find all floors in the scene, so we can sample locations above them\nfloors = bproc.filter.by_cp(objs, \"category_id\", label_mapping.id_from_label(\"floor\"))\nposes = 0\ntries = 0\nwhile tries < 10000 and poses < 5:\n    tries += 1\n    # Sample point above the floor in height of [1.5m, 1.8m]\n    location = bproc.sampler.upper_region(floors, min_height=1.5, max_height=1.8)\n    # Check that there is no object between the sampled point and the floor\n    _, _, _, _, hit_object, _ = bproc.object.scene_ray_cast(location, [0, 0, -1])\n    if hit_object not in floors:\n        continue\n\n    # Sample rotation (fix around X and Y axis)\n    rotation = np.random.uniform([1.2217, 0, 0], [1.2217, 0, 2 * np.pi])\n    cam2world_matrix = bproc.math.build_transformation_mat(location, rotation)\n\n    # Check that there is no obstacle in front of the camera closer than 1m\n    if not bproc.camera.perform_obstacle_in_view_check(cam2world_matrix, {\"min\": 1.0}, bvh_tree):\n        continue\n\n    # Check that the interesting score is not too low\n    if bproc.camera.scene_coverage_score(cam2world_matrix) < 0.1:\n        continue\n\n    # If all checks were passed, add the camera pose\n    bproc.camera.add_camera_pose(cam2world_matrix)\n    poses += 1\n\n# activate normal and depth rendering\nbproc.renderer.enable_normals_output()\nbproc.renderer.enable_depth_output(activate_antialiasing=False)\nbproc.renderer.enable_segmentation_output(map_by=[\"category_id\"])\n\n# render the whole pipeline\ndata = bproc.renderer.render()\n\n# write the data to a .hdf5 container\nbproc.writer.write_hdf5(args.output_dir, data)\n",
  "# ==========================================================================\n#  AIDA Detector description implementation\n# --------------------------------------------------------------------------\n# Copyright (C) Organisation europeenne pour la Recherche nucleaire (CERN)\n# All rights reserved.\n#\n# For the licensing terms see $DD4hepINSTALL/LICENSE.\n# For the list of contributors see $DD4hepINSTALL/doc/CREDITS.\n#\n# ==========================================================================\n#\nfrom __future__ import absolute_import, unicode_literals\nimport logging\n#\nlogging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n#\n#\n\"\"\"\n\n   dd4hep simulation example setup using the python configuration\n\n   @author  M.Frank\n   @version 1.0\n\n\"\"\"\n\n\ndef run():\n  import os\n  import DDG4\n  from DDG4 import OutputLevel as Output\n  from g4units import keV\n\n  args = DDG4.CommandLine()\n  install_dir = os.environ['DD4hepExamplesINSTALL']\n  if args.help:\n    import sys\n    logger.info(\"\"\"\n         python <dir>/Channeling.py -option [-option]\n              -geometry <geometry file name>  File is expected in the examples\n                                              install area:\n                                              \"\"\" + install_dir + \"\"\"\n              -vis                            Enable visualization\n              -macro                          Pass G4 macro file to UI executive\n              -batch                          Run in batch mode for unit testing\n              -events <number>                Run geant4 for specified number of events\n                                              (batch mode only)\n    \"\"\")\n    sys.exit(0)\n\n  kernel = DDG4.Kernel()\n  kernel.loadGeometry(str(\"file:\" + install_dir + \"/examples/DDG4/compact/Channeling.xml\"))\n\n  DDG4.importConstants(kernel.detectorDescription(), debug=False)\n  geant4 = DDG4.Geant4(kernel, tracker='Geant4TrackerCombineAction')\n  geant4.printDetectors()\n  # Configure UI\n  if args.macro:\n    ui = geant4.setupCshUI(macro=args.macro, vis=args.vis)\n  else:\n    ui = geant4.setupCshUI(vis=args.vis)\n\n  if args.batch:\n    ui.Commands = ['/run/beamOn ' + str(args.events), '/ddg4/UI/terminate']\n\n  # Configure field\n  geant4.setupTrackingField(prt=True)\n  # Configure Event actions\n  prt = DDG4.EventAction(kernel, 'Geant4ParticlePrint/ParticlePrint')\n  prt.OutputLevel = Output.DEBUG\n  prt.OutputType = 3  # Print both: table and tree\n  kernel.eventAction().adopt(prt)\n\n  generator_output_level = Output.INFO\n\n  # Configure G4 geometry setup\n  seq, act = geant4.addDetectorConstruction(\"Geant4DetectorGeometryConstruction/ConstructGeo\")\n  act.DebugMaterials = True\n  act.DebugElements = False\n  act.DebugVolumes = True\n  act.DebugShapes = True\n  act.DebugSurfaces = True\n\n  # Setup particle gun\n  gun = geant4.setupGun(\"Gun\", particle='gamma', energy=5 * keV, multiplicity=1)\n  gun.OutputLevel = generator_output_level\n\n  geant4.setupTracker('ChannelingDevice')\n\n  # Now build the physics list:\n  phys = geant4.setupPhysics('QGSP_BERT')\n  ph = DDG4.PhysicsList(kernel, 'Channeling')\n  ph.addPhysicsConstructor(str('Geant4ChannelingPhysics'))\n  ph.enableUI()\n  phys.adopt(ph)\n  phys.dump()\n\n  phys.dump()\n\n  geant4.execute()\n\n\nif __name__ == \"__main__\":\n  run()\n",
  "\"\"\"\n1D FDEM Mu Inversion\n====================\n\n1D inversion of Magnetic Susceptibility from FDEM data assuming a fixed\nelectrical conductivity\n\"\"\"\nfrom SimPEG import (\n    Mesh, Maps, Utils, DataMisfit, Regularization,\n    Optimization, Inversion, InvProblem, Directives\n)\nfrom SimPEG.EM import FDEM\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\ntry:\n    from pymatsolver import PardisoSolver as Solver\nexcept ImportError:\n    from SimPEG import SolverLU as Solver\n\n\ndef run(plotIt=True):\n\n    # Set up cylindrically symmetric mesh\n    cs, ncx, ncz, npad = 10., 15, 25, 13  # padded cylindrical mesh\n    hx = [(cs, ncx), (cs, npad, 1.3)]\n    hz = [(cs, npad, -1.3), (cs, ncz), (cs, npad, 1.3)]\n    mesh = Mesh.CylMesh([hx, 1, hz], '00C')\n\n    # Geologic Parameters model\n    layerz = np.r_[-100., -50.]\n    layer = (mesh.vectorCCz >= layerz[0]) & (mesh.vectorCCz <= layerz[1])\n    active = mesh.vectorCCz < 0.\n\n    # Electrical Conductivity\n    sig_half = 1e-2  # Half-space conductivity\n    sig_air = 1e-8  # Air conductivity\n    sig_layer = 1e-2  # Layer conductivity\n    sigma = np.ones(mesh.nCz)*sig_air\n    sigma[active] = sig_half\n    sigma[layer] = sig_layer\n\n    # mur - relative magnetic permeability\n    mur_half = 1.\n    mur_air = 1.\n    mur_layer = 2.\n    mur = np.ones(mesh.nCz)*mur_air\n    mur[active] = mur_half\n    mur[layer] = mur_layer\n\n    mtrue = mur[active]\n\n    # Maps\n    actMap = Maps.InjectActiveCells(mesh, active, mur_air, nC=mesh.nCz)\n    surj1Dmap = Maps.SurjectVertical1D(mesh)\n    murMap = Maps.MuRelative(mesh)\n\n    # Mapping\n    muMap = murMap * surj1Dmap * actMap\n\n    # ----- FDEM problem & survey -----\n    rxlocs = Utils.ndgrid([np.r_[10.], np.r_[0], np.r_[30.]])\n    bzr = FDEM.Rx.Point_bSecondary(rxlocs, 'z', 'real')\n    # bzi = FDEM.Rx.Point_bSecondary(rxlocs, 'z', 'imag')\n\n    freqs = np.linspace(2000, 10000, 10)  # np.logspace(3, 4, 10)\n    srcLoc = np.array([0., 0., 30.])\n\n    print(\n        'min skin depth = ', 500./np.sqrt(freqs.max() * sig_half),\n        'max skin depth = ', 500./np.sqrt(freqs.min() * sig_half)\n    )\n    print(\n        'max x ', mesh.vectorCCx.max(), 'min z ', mesh.vectorCCz.min(),\n        'max z ', mesh.vectorCCz.max()\n    )\n\n    srcList = [\n        FDEM.Src.MagDipole([bzr], freq, srcLoc, orientation='Z')\n        for freq in freqs\n    ]\n\n    surveyFD = FDEM.Survey(srcList)\n    prbFD = FDEM.Problem3D_b(\n        mesh, sigma=surj1Dmap * sigma, muMap=muMap, Solver=Solver\n    )\n    prbFD.pair(surveyFD)\n    std = 0.03\n    surveyFD.makeSyntheticData(mtrue, std)\n    surveyFD.eps = np.linalg.norm(surveyFD.dtrue)*1e-6\n\n    # FDEM inversion\n    np.random.seed(13472)\n    dmisfit = DataMisfit.l2_DataMisfit(surveyFD)\n    regMesh = Mesh.TensorMesh([mesh.hz[muMap.maps[-1].indActive]])\n    reg = Regularization.Simple(regMesh)\n    opt = Optimization.InexactGaussNewton(maxIterCG=10)\n    invProb = InvProblem.BaseInvProblem(dmisfit, reg, opt)\n\n    # Inversion Directives\n\n    beta = Directives.BetaSchedule(coolingFactor=4, coolingRate=3)\n    betaest = Directives.BetaEstimate_ByEig(beta0_ratio=2.)\n    target = Directives.TargetMisfit()\n    directiveList = [beta, betaest, target]\n\n    inv = Inversion.BaseInversion(invProb, directiveList=directiveList)\n    m0 = mur_half * np.ones(mtrue.size)\n    reg.alpha_s = 2e-2\n    reg.alpha_x = 1.\n    prbFD.counter = opt.counter = Utils.Counter()\n    opt.remember('xc')\n    moptFD = inv.run(m0)\n\n    dpredFD = surveyFD.dpred(moptFD)\n\n    if plotIt:\n        fig, ax = plt.subplots(1, 3, figsize=(10, 6))\n\n        fs = 13  # fontsize\n        matplotlib.rcParams['font.size'] = fs\n\n        # Plot the conductivity model\n        ax[0].semilogx(sigma[active], mesh.vectorCCz[active], 'k-', lw=2)\n        ax[0].set_ylim(-500, 0)\n        ax[0].set_xlim(5e-3, 1e-1)\n\n        ax[0].set_xlabel('Conductivity (S/m)', fontsize=fs)\n        ax[0].set_ylabel('Depth (m)', fontsize=fs)\n        ax[0].grid(\n            which='both', color='k', alpha=0.5, linestyle='-', linewidth=0.2\n        )\n        ax[0].legend(['Conductivity Model'], fontsize=fs, loc=4)\n\n        # Plot the permeability model\n        ax[1].plot(mur[active], mesh.vectorCCz[active], 'k-', lw=2)\n        ax[1].plot(moptFD, mesh.vectorCCz[active], 'b-', lw=2)\n        ax[1].set_ylim(-500, 0)\n        ax[1].set_xlim(0.5, 2.1)\n\n        ax[1].set_xlabel('Relative Permeability', fontsize=fs)\n        ax[1].set_ylabel('Depth (m)', fontsize=fs)\n        ax[1].grid(\n            which='both', color='k', alpha=0.5, linestyle='-', linewidth=0.2\n        )\n        ax[1].legend(['True', 'Predicted'], fontsize=fs, loc=4)\n\n        # plot the data misfits - negative b/c we choose positive to be in the\n        # direction of primary\n\n        ax[2].plot(freqs, -surveyFD.dobs, 'k-', lw=2)\n        # ax[2].plot(freqs, -surveyFD.dobs[1::2], 'k--', lw=2)\n\n        ax[2].loglog(freqs, -dpredFD, 'bo', ms=6)\n        # ax[2].loglog(freqs, -dpredFD[1::2], 'b+', markeredgewidth=2., ms=10)\n\n        # Labels, gridlines, etc\n        ax[2].grid(which='both', alpha=0.5, linestyle='-', linewidth=0.2)\n        ax[2].grid(which='both', alpha=0.5, linestyle='-', linewidth=0.2)\n\n        ax[2].set_xlabel('Frequency (Hz)', fontsize=fs)\n        ax[2].set_ylabel('Vertical magnetic field (-T)', fontsize=fs)\n        ax[2].legend(\n            (\"z-Obs (real)\", \"z-Pred (real)\"),\n            fontsize=fs\n        )\n        ax[2].set_xlim(freqs.max(), freqs.min())\n\n        ax[0].set_title(\"(a) Conductivity Model\", fontsize=fs)\n        ax[1].set_title(\"(b) $\\mu_r$ Model\", fontsize=fs)\n        ax[2].set_title(\"(c) FDEM observed vs. predicted\", fontsize=fs)\n\n        plt.tight_layout(pad=1.5)\n\nif __name__ == '__main__':\n    run(plotIt=True)\n    plt.show()\n",
  "#!/usr/bin/env python\n# encoding: utf-8\n# __author__ = 'Demon'\nfrom seedemu.layers import Base, Routing, Ebgp, PeerRelationship, Ibgp, Ospf\nfrom seedemu.compiler import Docker\nfrom seedemu.core import Emulator\n\nsim = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nibgp = Ibgp()\nospf = Ospf()\n\ndef make_stub_as(asn: int, exchange: str):\n    stub_as = base.createAutonomousSystem(asn)\n    host = stub_as.createHost('host0')\n    host1 = stub_as.createHost('host1')\n    host2 = stub_as.createHost('host2')\n    host3 = stub_as.createHost('host3')\n    host4 = stub_as.createHost('host4')\n    router = stub_as.createRouter('router0')\n\n    net = stub_as.createNetwork('net0')\n\n    \n    router.joinNetwork('net0')\n    host.joinNetwork('net0')\n    host1.joinNetwork('net0')\n    host2.joinNetwork('net0')\n    host3.joinNetwork('net0')\n    host4.joinNetwork('net0')\n\n    router.joinNetwork(exchange)\n\n\n##############################################################################\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\nbase.createInternetExchange(102)\n\nmake_stub_as(150, 'ix100')\nmake_stub_as(151, 'ix100')\n\nmake_stub_as(152, 'ix101')\nmake_stub_as(153, 'ix101')\nmake_stub_as(154, 'ix101')\n\nmake_stub_as(160, 'ix102')\nmake_stub_as(161, 'ix102')\n\n###############################################################################\n\nas2 = base.createAutonomousSystem(2)\n\nas2_100 = as2.createRouter('r0')\nas2_101 = as2.createRouter('r1')\nas2_102 = as2.createRouter('r2')\n\nas2_100.joinNetwork('ix100')\nas2_101.joinNetwork('ix101')\nas2_102.joinNetwork('ix102')\n\nas2_net_100_101 = as2.createNetwork('n01')\nas2_net_101_102 = as2.createNetwork('n12')\nas2_net_102_100 = as2.createNetwork('n20')\n\n\n\n\n\nas2_100.joinNetwork('n01')\nas2_101.joinNetwork('n01')\n\nas2_101.joinNetwork('n12')\nas2_102.joinNetwork('n12')\n\nas2_102.joinNetwork('n20')\nas2_100.joinNetwork('n20')\n\n###############################################################################\n\nas3 = base.createAutonomousSystem(3)\n\nas3_101 = as3.createRouter('r1')\nas3_102 = as3.createRouter('r2')\n\nas3_101.joinNetwork('ix101')\nas3_102.joinNetwork('ix102')\n\nas3_net_101_102 = as3.createNetwork('n12')\n\n\n\nas3_101.joinNetwork('n12')\nas3_102.joinNetwork('n12')\n\n###############################################################################\n\nebgp.addPrivatePeering(100, 2, 150, PeerRelationship.Provider)\nebgp.addPrivatePeering(100, 150, 151, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(101, 2, 3, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 154, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 154, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(102, 2, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 161, PeerRelationship.Provider)\n\n###############################################################################\n\nsim.addLayer(base)\nsim.addLayer(routing)\nsim.addLayer(ebgp)\nsim.addLayer(ibgp)\nsim.addLayer(ospf)\n\nsim.dump('base-component.bin')",
  "#!/usr/bin/env python3\n#X\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom fealpy.mesh import TetrahedronMesh\n\nnode = np.array([\n    [0.0, 0.0, 0.0],\n    [1.0, 0.0, 0.0],\n    [0.0, 1.0, 0.0],\n    [0.0, 0.0, 1.0],\n    [0.0, 1.0, 1.0]], dtype=np.float) # (NN, 3)\n\ncell = np.array([[1, 2, 0, 3], [2, 4, 3, 1]], dtype=np.int) # (NC, 3)\n\nmesh = TetrahedronMesh(node, cell)\n\nNN = mesh.number_of_nodes() # 节点 node 个数\nNE = mesh.number_of_edges() # 边 edge 个数\nNF = mesh.number_of_faces() # 面 face 个数\nNC = mesh.number_of_cells() # 单元 cell 个数\n\nnode = mesh.entity('node') # 节点数组，形状为 (NN,3)，储存节点坐标\nedge = mesh.entity('edge') # 边数组，形状为 (NE, 2), 储存每条边的两个节点的编号\nface = mesh.entity('face') # 面数组，形状为 (NF, 3), 储存构成三角形的三个节点编号\ncell = mesh.entity('cell') # 单元数组，形状为 (NC,4),储存构成四边形的四个节点编号\n\nebc = mesh.entity_barycenter('edge') # (NE,3)，储存各边的重心坐标\nfbc = mesh.entity_barycenter('face') # (NF,3)，储存各面的重心坐标\ncbc = mesh.entity_barycenter('cell') # (NC,3), 储存各单元的重心坐标\n\narea = mesh.entity_measure('cell') # (NC, 1), 每个单元的面积\nface = mesh.entity_measure('face') # (NF, 1), 每个面的面积\neh = mesh.entity_measure('edge') # (NE, 1), 每条边的长度\n\ncell2cell = mesh.ds.cell_to_cell() # (NC, 4)\ncell2face = mesh.ds.cell_to_face() # (NC, 4)\ncell2edge = mesh.ds.cell_to_edge() # (NC, 6)\ncell2node = mesh.ds.cell_to_node() # cell\nprint('cell2cell:\\n', cell2cell)\nprint('cell2face:\\n', cell2face)\nprint('cell2edge:\\n', cell2edge)\nprint('cell2node:\\n', cell2node)\n\nface2cell = mesh.ds.face_to_cell() # (NF, 4)\nface2face = mesh.ds.face_to_face() # (NF, NF)\nface2edge = mesh.ds.face_to_edge() # (NF, 3)\nface2node = mesh.ds.face_to_node() # face\nprint('face2cell:\\n', face2cell)\nprint('face2face:\\n', face2face)\nprint(\"face2edge:\\n\", face2edge)\nprint('face2node:\\n', face2node)\n\nedge2cell = mesh.ds.edge_to_cell() # (NE, NC)\nedge2face = mesh.ds.edge_to_face() # (NE, NF)\nedge2node = mesh.ds.edge_to_node() # edge\nedge2edge = mesh.ds.edge_to_edge() # sparse, (NE, NE)\nprint('edge2cell:\\n',edge2cell)\nprint('edge2face:\\n',edge2face)\nprint(\"edge2edge:\\n\",edge2edge)\nprint('edge2node:\\n',edge2face)\n\n\nnode2cell = mesh.ds.node_to_cell() # sparse, (NN, NC)\nnode2face = mesh.ds.node_to_face() # sparse, (NN, NF)\nnode2edge = mesh.ds.node_to_edge() # sparse, (NN, NE)\nnode2node = mesh.ds.node_to_node() # sparse, (NN, NN)\n\nprint('node2cell:\\n',node2cell)\nprint('node2face:\\n',node2face)\nprint('node2edge:\\n',node2edge)\nprint(\"node2node:\\n\",node2node)\n\n\nisBdNode = mesh.ds.boundary_node_flag() # (NN, ), bool\nisBdEdge = mesh.ds.boundary_edge_flag() # (NE, ), bool\nisBdFace = mesh.ds.boundary_face_flag() # (NC, ), bool\nisBdCell = mesh.ds.boundary_cell_flag() # (NC, ), bool\n\nbdNodeIdx = mesh.ds.boundary_node_index() # \nbdEdgeIdx = mesh.ds.boundary_edge_index() # \nbdFaceIdx = mesh.ds.boundary_face_index() # \nbdCellIdx = mesh.ds.boundary_cell_index() # \n\n\n#mesh.uniform_refine(1)\n\nmesh.print()\n\n\nfig = plt.figure()\naxes = Axes3D(fig)\nmesh.add_plot(axes, alpha=0, showedge=True)\nmesh.find_node(axes,showindex=True,fontsize=40)\nmesh.find_edge(axes, showindex=True,fontsize=40)\nmesh.find_cell(axes, showindex=True,fontsize=40)\nplt.show()\n\n",
  "# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\n\"\"\"\nThis example is about how can simulate the OnlineManager based on rolling tasks.\n\"\"\"\n\nfrom pprint import pprint\nimport fire\nimport qlib\nfrom qlib.model.trainer import DelayTrainerR, DelayTrainerRM, TrainerR, TrainerRM\nfrom qlib.workflow import R\nfrom qlib.workflow.online.manager import OnlineManager\nfrom qlib.workflow.online.strategy import RollingStrategy\nfrom qlib.workflow.task.gen import RollingGen\nfrom qlib.workflow.task.manage import TaskManager\nfrom qlib.tests.config import CSI100_RECORD_LGB_TASK_CONFIG_ONLINE, CSI100_RECORD_XGBOOST_TASK_CONFIG_ONLINE\nimport pandas as pd\nfrom qlib.contrib.evaluate import backtest_daily\nfrom qlib.contrib.evaluate import risk_analysis\nfrom qlib.contrib.strategy import TopkDropoutStrategy\n\n\nclass OnlineSimulationExample:\n    def __init__(\n        self,\n        provider_uri=\"~/.qlib/qlib_data/cn_data\",\n        region=\"cn\",\n        exp_name=\"rolling_exp\",\n        task_url=\"mongodb://10.0.0.4:27017/\",  # not necessary when using TrainerR or DelayTrainerR\n        task_db_name=\"rolling_db\",  # not necessary when using TrainerR or DelayTrainerR\n        task_pool=\"rolling_task\",\n        rolling_step=80,\n        start_time=\"2018-09-10\",\n        end_time=\"2018-10-31\",\n        tasks=None,\n        trainer=\"TrainerR\",\n    ):\n        \"\"\"\n        Init OnlineManagerExample.\n\n        Args:\n            provider_uri (str, optional): the provider uri. Defaults to \"~/.qlib/qlib_data/cn_data\".\n            region (str, optional): the stock region. Defaults to \"cn\".\n            exp_name (str, optional): the experiment name. Defaults to \"rolling_exp\".\n            task_url (str, optional): your MongoDB url. Defaults to \"mongodb://10.0.0.4:27017/\".\n            task_db_name (str, optional): database name. Defaults to \"rolling_db\".\n            task_pool (str, optional): the task pool name (a task pool is a collection in MongoDB). Defaults to \"rolling_task\".\n            rolling_step (int, optional): the step for rolling. Defaults to 80.\n            start_time (str, optional): the start time of simulating. Defaults to \"2018-09-10\".\n            end_time (str, optional): the end time of simulating. Defaults to \"2018-10-31\".\n            tasks (dict or list[dict]): a set of the task config waiting for rolling and training\n        \"\"\"\n        if tasks is None:\n            tasks = [CSI100_RECORD_XGBOOST_TASK_CONFIG_ONLINE, CSI100_RECORD_LGB_TASK_CONFIG_ONLINE]\n        self.exp_name = exp_name\n        self.task_pool = task_pool\n        self.start_time = start_time\n        self.end_time = end_time\n        mongo_conf = {\n            \"task_url\": task_url,\n            \"task_db_name\": task_db_name,\n        }\n        qlib.init(provider_uri=provider_uri, region=region, mongo=mongo_conf)\n        self.rolling_gen = RollingGen(\n            step=rolling_step, rtype=RollingGen.ROLL_SD, ds_extra_mod_func=None\n        )  # The rolling tasks generator, ds_extra_mod_func is None because we just need to simulate to 2018-10-31 and needn't change the handler end time.\n        if trainer == \"TrainerRM\":\n            self.trainer = TrainerRM(self.exp_name, self.task_pool)\n        elif trainer == \"TrainerR\":\n            self.trainer = TrainerR(self.exp_name)\n        else:\n            # TODO: support all the trainers: TrainerR, TrainerRM, DelayTrainerR\n            raise NotImplementedError(f\"This type of input is not supported\")\n        self.rolling_online_manager = OnlineManager(\n            RollingStrategy(exp_name, task_template=tasks, rolling_gen=self.rolling_gen),\n            trainer=self.trainer,\n            begin_time=self.start_time,\n        )\n        self.tasks = tasks\n\n    # Reset all things to the first status, be careful to save important data\n    def reset(self):\n        if isinstance(self.trainer, TrainerRM):\n            TaskManager(self.task_pool).remove()\n        exp = R.get_exp(experiment_name=self.exp_name)\n        for rid in exp.list_recorders():\n            exp.delete_recorder(rid)\n\n    # Run this to run all workflow automatically\n    def main(self):\n        print(\"========== reset ==========\")\n        self.reset()\n        print(\"========== simulate ==========\")\n        self.rolling_online_manager.simulate(end_time=self.end_time)\n        print(\"========== collect results ==========\")\n        print(self.rolling_online_manager.get_collector()())\n        print(\"========== signals ==========\")\n        signals = self.rolling_online_manager.get_signals()\n        print(signals)\n        # Backtesting\n        # - the code is based on this example https://qlib.readthedocs.io/en/latest/component/strategy.html\n        CSI300_BENCH = \"SH000903\"\n        STRATEGY_CONFIG = {\n            \"topk\": 30,\n            \"n_drop\": 3,\n            \"signal\": signals.to_frame(\"score\"),\n        }\n        strategy_obj = TopkDropoutStrategy(**STRATEGY_CONFIG)\n        report_normal, positions_normal = backtest_daily(\n            start_time=signals.index.get_level_values(\"datetime\").min(),\n            end_time=signals.index.get_level_values(\"datetime\").max(),\n            strategy=strategy_obj,\n        )\n        analysis = dict()\n        analysis[\"excess_return_without_cost\"] = risk_analysis(report_normal[\"return\"] - report_normal[\"bench\"])\n        analysis[\"excess_return_with_cost\"] = risk_analysis(\n            report_normal[\"return\"] - report_normal[\"bench\"] - report_normal[\"cost\"]\n        )\n\n        analysis_df = pd.concat(analysis)  # type: pd.DataFrame\n        pprint(analysis_df)\n\n    def worker(self):\n        # train tasks by other progress or machines for multiprocessing\n        # FIXME: only can call after finishing simulation when using DelayTrainerRM, or there will be some exception.\n        print(\"========== worker ==========\")\n        if isinstance(self.trainer, TrainerRM):\n            self.trainer.worker()\n        else:\n            print(f\"{type(self.trainer)} is not supported for worker.\")\n\n\nif __name__ == \"__main__\":\n    ## to run all workflow automatically with your own parameters, use the command below\n    # python online_management_simulate.py main --experiment_name=\"your_exp_name\" --rolling_step=60\n    fire.Fire(OnlineSimulationExample)\n",
  "# A demonstration of basic functions of the Python interface for TACS,\n# this example goes through the process of setting up the using the\n# tacs assembler directly as opposed to using the pyTACS user interface (see analysis.py):\n# loading a mesh, creating elements, evaluating functions, solution, and output\nfrom __future__ import print_function\n\n# Import necessary libraries\nimport numpy as np\nimport os\nfrom mpi4py import MPI\nfrom tacs import TACS, elements, constitutive, functions\n\n# Load structural mesh from BDF file\nbdfFile = os.path.join(os.path.dirname(__file__), \"CRM_box_2nd.bdf\")\ntacs_comm = MPI.COMM_WORLD\nstruct_mesh = TACS.MeshLoader(tacs_comm)\nstruct_mesh.scanBDFFile(bdfFile)\n\n# Set constitutive properties\nrho = 2500.0  # density, kg/m^3\nE = 70e9  # elastic modulus, Pa\nnu = 0.3  # poisson's ratio\nkcorr = 5.0 / 6.0  # shear correction factor\nys = 350e6  # yield stress, Pa\nmin_thickness = 0.002\nmax_thickness = 0.20\nthickness = 0.02\n\n# Loop over components, creating stiffness and element object for each\nnum_components = struct_mesh.getNumComponents()\nfor i in range(num_components):\n    descriptor = struct_mesh.getElementDescript(i)\n    # Setup (isotropic) property and constitutive objects\n    prop = constitutive.MaterialProperties(rho=rho, E=E, nu=nu, ys=ys)\n    # Set one thickness dv for every component\n    stiff = constitutive.IsoShellConstitutive(\n        prop, t=thickness, tMin=min_thickness, tMax=max_thickness, tNum=i\n    )\n\n    element = None\n    transform = None\n    if descriptor in [\"CQUAD\", \"CQUADR\", \"CQUAD4\"]:\n        element = elements.Quad4Shell(transform, stiff)\n    struct_mesh.setElement(i, element)\n\n# Create tacs assembler object from mesh loader\ntacs = struct_mesh.createTACS(6)\n\n# Create the KS Function\nksWeight = 100.0\nfuncs = [functions.KSFailure(tacs, ksWeight=ksWeight)]\n# funcs = [functions.StructuralMass(tacs)]\n# funcs = [functions.Compliance(tacs)]\n\n# Get the design variable values\nx = tacs.createDesignVec()\nx_array = x.getArray()\ntacs.getDesignVars(x)\n\n# Get the node locations\nX = tacs.createNodeVec()\ntacs.getNodes(X)\ntacs.setNodes(X)\n\n# Create the forces\nforces = tacs.createVec()\nforce_array = forces.getArray()\nforce_array[2::6] += 100.0  # uniform load in z direction\ntacs.applyBCs(forces)\n\n# Set up and solve the analysis problem\nres = tacs.createVec()\nans = tacs.createVec()\nu = tacs.createVec()\nmat = tacs.createSchurMat()\npc = TACS.Pc(mat)\nsubspace = 100\nrestarts = 2\ngmres = TACS.KSM(mat, pc, subspace, restarts)\n\n# Assemble the Jacobian and factor\nalpha = 1.0\nbeta = 0.0\ngamma = 0.0\ntacs.zeroVariables()\ntacs.assembleJacobian(alpha, beta, gamma, res, mat)\npc.factor()\n\n# Solve the linear system\ngmres.solve(forces, ans)\ntacs.setVariables(ans)\n\n# Evaluate the function\nfvals1 = tacs.evalFunctions(funcs)\n\n# Solve for the adjoint variables\nadjoint = tacs.createVec()\nres.zeroEntries()\ntacs.addSVSens([funcs[0]], [res])\ngmres.solve(res, adjoint)\n\n# Compute the total derivative w.r.t. material design variables\nfdv_sens = tacs.createDesignVec()\nfdv_sens_array = fdv_sens.getArray()\ntacs.addDVSens([funcs[0]], [fdv_sens])\ntacs.addAdjointResProducts([adjoint], [fdv_sens], -1)\n# Finalize sensitivity arrays across all procs\nfdv_sens.beginSetValues()\nfdv_sens.endSetValues()\n\n# Create a random direction along which to perturb the nodes\npert = tacs.createNodeVec()\nX_array = X.getArray()\npert_array = pert.getArray()\npert_array[0::3] = X_array[1::3]\npert_array[1::3] = X_array[0::3]\npert_array[2::3] = X_array[2::3]\n\n# Compute the total derivative w.r.t. nodal locations\nfXptSens = tacs.createNodeVec()\ntacs.addXptSens([funcs[0]], [fXptSens])\ntacs.addAdjointResXptSensProducts([adjoint], [fXptSens], -1)\n# Finalize sensitivity arrays across all procs\nfXptSens.beginSetValues()\nfXptSens.endSetValues()\n\n# Set the complex step\nxpert = tacs.createDesignVec()\nxpert.setRand()\nxpert_array = xpert.getArray()\nxnew = tacs.createDesignVec()\nxnew.copyValues(x)\nif TACS.dtype is complex:\n    dh = 1e-30\n    xnew.axpy(dh * 1j, xpert)\nelse:\n    dh = 1e-6\n    xnew.axpy(dh, xpert)\n\n# Set the design variables\ntacs.setDesignVars(xnew)\n\n# Compute the perturbed solution\ntacs.zeroVariables()\ntacs.assembleJacobian(alpha, beta, gamma, res, mat)\npc.factor()\ngmres.solve(forces, u)\ntacs.setVariables(u)\n\n# Evaluate the function for perturbed solution\nfvals2 = tacs.evalFunctions(funcs)\n\nif TACS.dtype is complex:\n    fd = fvals2.imag / dh\nelse:\n    fd = (fvals2 - fvals1) / dh\n\nresult = xpert.dot(fdv_sens)\nif tacs_comm.rank == 0:\n    print(\"FD:      \", fd[0])\n    print(\"Result:  \", result)\n    print(\"Rel err: \", (result - fd[0]) / result)\n\n# Reset the old variable values\ntacs.setDesignVars(x)\n\nif TACS.dtype is complex:\n    dh = 1e-30\n    X.axpy(dh * 1j, pert)\nelse:\n    dh = 1e-6\n    X.axpy(dh, pert)\n\n# Set the perturbed node locations\ntacs.setNodes(X)\n\n# Compute the perturbed solution\ntacs.zeroVariables()\ntacs.assembleJacobian(alpha, beta, gamma, res, mat)\npc.factor()\ngmres.solve(forces, u)\ntacs.setVariables(u)\n\n# Evaluate the function again\nfvals2 = tacs.evalFunctions(funcs)\n\nif TACS.dtype is complex:\n    fd = fvals2.imag / dh\nelse:\n    fd = (fvals2 - fvals1) / dh\n\n# Compute the projected derivative\nresult = pert.dot(fXptSens)\n\nif tacs_comm.rank == 0:\n    print(\"FD:      \", fd[0])\n    print(\"Result:  \", result)\n    print(\"Rel err: \", (result - fd[0]) / result)\n\n# Output for visualization\nflag = (\n    TACS.OUTPUT_CONNECTIVITY\n    | TACS.OUTPUT_NODES\n    | TACS.OUTPUT_DISPLACEMENTS\n    | TACS.OUTPUT_STRAINS\n    | TACS.OUTPUT_STRESSES\n    | TACS.OUTPUT_EXTRAS\n    | TACS.OUTPUT_LOADS\n)\nf5 = TACS.ToFH5(tacs, TACS.BEAM_OR_SHELL_ELEMENT, flag)\nf5.writeToFile(\"ucrm.f5\")\n",
  "#!/usr/bin/env python3\n# \nimport sys\nimport argparse\n\nimport numpy as np\nfrom numpy.linalg import inv\nfrom scipy.sparse import spdiags\nfrom scipy.sparse.linalg import spsolve\nimport matplotlib.pyplot as plt\n\nfrom fealpy.mesh import MeshFactory\nfrom fealpy.pde.timeharmonic_2d import CosSinData\nfrom fealpy.functionspace import FirstKindNedelecFiniteElementSpace2d \nfrom fealpy.functionspace import LagrangeFiniteElementSpace\nfrom fealpy.boundarycondition import DirichletBC \n\nfrom fealpy.mesh.adaptive_tools import mark\nfrom fealpy.tools.show import showmultirate\nfrom fealpy.tools.show import show_error_table\n\n\ndef curl_recover(uh):\n\n    mesh = uh.space.mesh\n    space = LagrangeFiniteElementSpace(mesh, p=1)\n    ruh = space.function() # (gdof, 2)\n\n    bc = np.array([1/3, 1/3, 1/3], dtype=mesh.ftype)\n    val = uh.curl_value(bc) #(NC, )\n    w = 1/mesh.entity_measure('cell')\n    val *= w\n\n    NN = mesh.number_of_nodes() \n    NC = mesh.number_of_cells()\n    cell = mesh.entity('cell')\n    w = np.broadcast_to(w.reshape(-1, 1), shape=cell.shape)\n    W = np.zeros(NN, dtype=mesh.ftype)\n    np.add.at(W, cell, w)\n\n    val = np.broadcast_to(val.reshape(-1, 1), shape=cell.shape)\n    np.add.at(ruh, cell, val)\n    ruh /= W\n\n    return ruh\n\ndef spr_edge(mesh, h, edgeVal):\n\n    \"\"\"\n\n    Notes\n    -----\n    mesh: 三角形网格\n    h: 网格节点尺寸\n    edgeVal: 定义在边上的值\n    \"\"\"\n\n    NN = mesh.number_of_nodes() \n    NE = mesh.number_of_edges()\n    NC = mesh.number_of_cells()\n\n    edge = mesh.entity('edge')\n    v = mesh.edge_tangent()/2 # \n    phi = np.ones((NE, 3), dtype=mesh.ftype)\n\n\n    A = np.zeros((NN, 3, 3), dtype=mesh.ftype)\n    b = np.zeros((NN, 3), dtype=mesh.ftype)\n\n    phi[:, 1:] = v/h[edge[:, 0], None] # 边中点相对于 0 号端点的局部坐标 \n    val = phi[:, :, None]*phi[:, None, :]\n    np.add.at(A, (edge[:, 0], np.s_[:], np.s_[:]), val) # A^TA\n    val = phi*edgeVal[:, None]\n    np.add.at(b, (edge[:, 0], np.s_[:]), val)\n\n    phi[:, 1:] = -v/h[edge[:, 1], None] # 边中点相对于 1 号端点的局部坐标 \n    val = phi[:, :, None]*phi[:, None, :]\n    np.add.at(A, (edge[:, 1], np.s_[:], np.s_[:]), val) # A^TA\n    val = phi*edgeVal[:, None]\n    np.add.at(b, (edge[:, 1], np.s_[:]), val)\n    return A, b\n\ndef spr_curl(uh):\n\n    \"\"\"\n    Notes\n    -----\n    给定一个解, 恢复节点处的值\n    \"\"\"\n    mesh = uh.space.mesh\n\n    NN = mesh.number_of_nodes()\n    NE = mesh.number_of_edges()\n    NC = mesh.number_of_cells()\n\n    node = mesh.entity('node')\n    edge = mesh.entity('edge')\n    cell = mesh.entity('cell')\n\n    # 计算数值解在单元重心处的 curl 值\n    bc = np.array([1/3, 1/3, 1/3], dtype=mesh.ftype)\n    cellVal = uh.curl_value(bc) #(NC, )\n\n    # 计算每条边的平均 curl 值\n    edge2cell = mesh.ds.edge_to_cell()\n    edgeVal = (cellVal[edge2cell[:, 0]] + cellVal[edge2cell[:, 1]])/2.0\n\n    # 计算每个节点的最小二乘矩阵\n    h = mesh.node_size()\n    A, b = spr_edge(mesh, h, edgeVal) \n\n    # 处理边界点\n    # 找到每个边界点对应的内部点, 把对应内部点的样本点当做边界节点的样本点\n\n    isBdNode = mesh.ds.boundary_node_flag()\n    idxMap = np.arange(NN, dtype=mesh.itype) # 节点映射数组, 自身到自身的映射\n\n    # 找到一端是边界点, 一端是内部节点的边, 修改节点映射数组\n    flag = isBdNode[edge[:, 0]] & (~isBdNode[edge[:, 1]])\n    idxMap[edge[flag, 0]] = edge[flag, 1]\n    flag = isBdNode[edge[:, 1]] & (~isBdNode[edge[:, 0]])\n    idxMap[edge[flag, 1]] = edge[flag, 0]\n\n    # 找到没有内部节点相邻的角点, 修改节点映射数组\n    isCEdge = edge2cell[:, 0] != edge2cell[:, 1]\n    isCEdge = isCEdge & isBdNode[edge[:, 0]] & isBdNode[edge[:, 1]]\n\n    idxMap[cell[edge2cell[isCEdge, 0], edge2cell[isCEdge, 2]]] = cell[edge2cell[isCEdge, 1], edge2cell[isCEdge, 3]] \n    idxMap[cell[edge2cell[isCEdge, 1], edge2cell[isCEdge, 3]]] = cell[edge2cell[isCEdge, 0], edge2cell[isCEdge, 2]] \n\n    # 计算边界节点对应的最小二乘矩阵和右端\n    # 注意可以直接利用对应内部节点对应的最小二乘矩阵和右端来计算边界点的系统, \n    # 它们有内在的数学关系\n    c = h[idxMap[isBdNode]]/h[isBdNode] \n    xe = (node[idxMap[isBdNode]] - node[isBdNode])/h[isBdNode, None]\n\n    A[isBdNode, 0, 0] = A[idxMap[isBdNode], 0, 0]\n\n    A[isBdNode, 0, 1] = A[idxMap[isBdNode], 0, 0]*xe[:, 0] \n    A[isBdNode, 0, 1]+= A[idxMap[isBdNode], 0, 1]*c\n    A[isBdNode, 1, 0] = A[isBdNode, 0, 1]\n\n    A[isBdNode, 0, 2] = A[idxMap[isBdNode], 0, 0]*xe[:, 1] \n    A[isBdNode, 0, 2]+= A[idxMap[isBdNode], 0, 2]*c\n    A[isBdNode, 2, 0] = A[isBdNode, 0, 2]\n\n    A[isBdNode, 1, 1] = A[idxMap[isBdNode], 0, 0]*xe[:, 0]**2 \n    A[isBdNode, 1, 1]+= A[idxMap[isBdNode], 0, 1]*xe[:, 0]*2*c\n    A[isBdNode, 1, 1]+= A[idxMap[isBdNode], 1, 1]*c**2\n\n    A[isBdNode, 1, 2] = A[idxMap[isBdNode], 0, 0]*xe[:, 0]*xe[:, 1] \n    A[isBdNode, 1, 2]+= A[idxMap[isBdNode], 0, 1]*xe[:, 1]*c\n    A[isBdNode, 1, 2]+= A[idxMap[isBdNode], 0, 2]*xe[:, 0]*c\n    A[isBdNode, 1, 2]+= A[idxMap[isBdNode], 1, 2]*c**2\n    A[isBdNode, 2, 1] = A[isBdNode, 1, 2]\n\n    A[isBdNode, 2, 2] = A[idxMap[isBdNode], 0, 0]*xe[:, 1]**2\n    A[isBdNode, 2, 2]+= A[idxMap[isBdNode], 0, 2]*xe[:, 1]*2*c\n    A[isBdNode, 2, 2]+= A[idxMap[isBdNode], 2, 2]*c**2\n\n    b[isBdNode, 0] = b[idxMap[isBdNode], 0]\n\n    b[isBdNode, 1] = b[idxMap[isBdNode], 0]*xe[:, 0]\n    b[isBdNode, 1]+= b[idxMap[isBdNode], 1]*c\n\n    b[isBdNode, 2] = b[idxMap[isBdNode], 0]*xe[:, 1]\n    b[isBdNode, 2]+= b[idxMap[isBdNode], 2]*c\n\n    A = inv(A)\n    val = (A@b[:, :, None]).reshape(-1, 3)\n\n    space = LagrangeFiniteElementSpace(mesh, p=1)\n    ruh = space.function() # (gdof, 2)\n    ruh[:] = val[:, 0]\n\n    return ruh\n\n\n## 参数解析\nparser = argparse.ArgumentParser(description=\n        \"\"\"\n        这是一个自适应求解时谐方程的程序\n        \"\"\")\n\nparser.add_argument('--degree', \n        default=0, type=int,\n        help='第一类 Nedlec 元的次数, 默认为 0!')\n\nparser.add_argument('--size', \n        default=5, type=int,\n        help='初始网格的 x 和 y 方向剖分段数, 默认为 5 段')\n\nparser.add_argument('--maxit', \n        default=40, type=int,\n        help='自适应迭代次数, 默认自适应迭代 40 次')\n\nparser.add_argument('--theta', \n        default=0.3, type=float,\n        help='自适应迭代的 theta 参数, 默认为  0.3')\n\nparser.print_help()\nargs = parser.parse_args()\nprint('程序参数为:', args)\n\n\n## 开始计算\n\npde = CosSinData()\n\nbox = [-1, 1, -1, 1]\nmesh = MeshFactory.boxmesh2d(box, nx=args.size, ny=args.size, meshtype='tri') \n\n# 去掉第四象限\nmesh.delete_cell(threshold=lambda x: (x[..., 0] > 0) & (x[..., 1] < 0)) \n\nerrorType = ['$|| u - u_h||_{\\Omega,0}$',\n             '$||\\\\nabla\\\\times u - \\\\nabla\\\\times u_h||_{\\Omega, 0}$',\n             '$||\\\\nabla\\\\times u - G(\\\\nabla\\\\times u_h)||_{\\Omega, 0}$',\n             ]\nerrorMatrix = np.zeros((len(errorType), args.maxit), dtype=np.float64)\nNDof = np.zeros(args.maxit, dtype=np.float64)\n\nfor i in range(args.maxit):\n    space = FirstKindNedelecFiniteElementSpace2d(mesh, p=args.degree)\n    bc = DirichletBC(space, pde.dirichlet) \n\n    gdof = space.number_of_global_dofs()\n    NDof[i] = gdof \n\n    uh = space.function()\n    A = space.curl_matrix() - space.mass_matrix()\n    F = space.source_vector(pde.source)\n\n    A, F = bc.apply(A, F, uh)\n\n    uh[:] = spsolve(A, F)\n\n    #ruh = curl_recover(uh)\n    ruh = spr_curl(uh) \n\n    errorMatrix[0, i] = space.integralalg.L2_error(pde.solution, uh)\n    errorMatrix[1, i] = space.integralalg.L2_error(pde.curl, uh.curl_value)\n    errorMatrix[2, i] = space.integralalg.L2_error(pde.curl, ruh)\n    eta = space.integralalg.error(uh.curl_value, ruh, power=2, celltype=True) # 计算单元上的恢复型误差\n\n    if i < args.maxit - 1:\n        isMarkedCell = mark(eta, theta=args.theta)\n        mesh.bisect(isMarkedCell)\n        mesh.add_plot(plt)\n        plt.savefig('./test-' + str(i+1) + '.png')\n        plt.close()\n\n\nshowmultirate(plt, args.maxit-10, NDof, errorMatrix,  errorType, propsize=20)\nplt.show()\n",
  "\"\"\"Get multiplicative sinograms from normalisation and/or attenuation.\n\nUsage:\n  get_multiplicative_sinogram [--help | options]\n\nOptions:\n  -p <path>, --path=<path>      path to data files, defaults to data/examples/PET/mMR\n                                subfolder of SIRF root folder\n  -S <file>, --sino=<file>      template sinogram [default: mMR_template_span11_small.hs]\n  -A <attn>, --attn=<attn>      attenuation image file [default: mu_map.hv]\n  -N <norm>, --norm=<norm>      ECAT8 bin normalisation file [default: norm.n.hdr]\n  -O <outp>, --outp=<outp>      output file [default: multiplicative]\n  -T <file>, --trans=<file>     transform for attn image\n  -t <str>, --trans_type=<str>  transform type (tm, disp, def) [default: tm]\n  --non-interactive           do not show plots\n\"\"\"\n\n# SyneRBI Synergistic Image Reconstruction Framework (SIRF)\n# Copyright 2020 University College London.\n#\n# This is software developed for the Collaborative Computational\n# Project in Synergistic Reconstruction for Biomedical Imaging\n# (formerly CCP PETMR)\n# (http://www.ccpsynerbi.ac.uk/).\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#       http://www.apache.org/licenses/LICENSE-2.0\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\nfrom docopt import docopt\nfrom os import path\nimport sirf.STIR as pet\nimport sirf.Reg as reg\nfrom sirf.Utilities import error, show_3D_array, examples_data_path, existing_filepath\n\n__version__ = '0.1.0'\nargs = docopt(__doc__, version=__version__)\n\n\ndef check_file_exists(filename):\n    \"\"\"Check file exists, else throw error.\"\"\"\n    if not path.isfile(filename):\n        raise error('File not found: %s' % filename)\n\n\n# process command-line options\ndata_path = args['--path']\nif data_path is None:\n    # default to data/examples/PET/mMR\n    # Note: seem to need / even on Windows\n    #data_path = os.path.join(examples_data_path('PET'), 'mMR')\n    data_path = examples_data_path('PET') + '/mMR'\nprint('Finding files in %s' % data_path)\n\n# Sinogram. if sino not found, get the one in the example data\nsino_file = existing_filepath(data_path, args['--sino'])\n\n# Attenuation - image\nattn_im_file = existing_filepath(data_path, args['--attn'])\n\n# Norm - ECAT8\nnorm_e8_file = existing_filepath(data_path, args['--norm'])\n\n# Attn transformation\ntrans = args['--trans']\nif trans:\n    check_file_exists(trans)\ntrans_type = args['--trans_type']\n\n# Output file\noutp_file = args['--outp']\n\n\ndef resample_attn_image(image):\n    \"\"\"Resample the attenuation image.\"\"\"\n    if trans_type == 'tm':\n        transformation = reg.AffineTransformation(trans)\n    elif trans_type == 'disp':\n        transformation = reg.NiftiImageData3DDisplacement(trans)\n    elif trans_type == 'def':\n        transformation = reg.NiftiImageData3DDeformation(trans)\n    else:\n        raise ValueError(\"Unknown transformation type.\")\n\n    resampler = reg.NiftyResampler()\n    resampler.set_reference_image(image)\n    resampler.set_floating_image(image)\n    resampler.set_interpolation_type_to_linear()\n    resampler.set_padding_value(0.0)\n    resampler.add_transformation(transformation)\n    return resampler.forward(image)\n\n\ndef main():\n    \"\"\"Do main.\"\"\"\n    # Acq model and template sino\n    acq_model = pet.AcquisitionModelUsingRayTracingMatrix()\n    acq_data = pet.AcquisitionData(sino_file)\n\n    # If norm is present\n    asm_norm = None\n    if norm_e8_file:\n        # create acquisition sensitivity model from ECAT8 normalisation data\n        asm_norm = pet.AcquisitionSensitivityModel(norm_e8_file)\n\n    # If attenuation is present\n    asm_attn = None\n    if attn_im_file:\n        attn_image = pet.ImageData(attn_im_file)\n        if trans:\n            attn_image = resample_attn_image(attn_image)\n        asm_attn = pet.AcquisitionSensitivityModel(attn_image, acq_model)\n        # temporary fix pending attenuation offset fix in STIR:\n        # converting attenuation into 'bin efficiency'\n        asm_attn.set_up(acq_data)\n        bin_eff = pet.AcquisitionData(acq_data)\n        bin_eff.fill(1.0)\n        print('applying attenuation (please wait, may take a while)...')\n        asm_attn.unnormalise(bin_eff)\n        asm_attn = pet.AcquisitionSensitivityModel(bin_eff)\n\n    # Get ASM dependent on attn and/or norm\n    if asm_norm and asm_attn:\n        print(\"AcquisitionSensitivityModel contains norm and attenuation...\")\n        asm = pet.AcquisitionSensitivityModel(asm_norm, asm_attn)\n    elif asm_norm:\n        print(\"AcquisitionSensitivityModel contains norm...\")\n        asm = asm_norm\n    elif asm_attn:\n        print(\"AcquisitionSensitivityModel contains attenuation...\")\n        asm = asm_attn\n    else:\n        raise ValueError(\"Need norm and/or attn\")\n\n    # only need to project again if normalisation is added\n    # (since attenuation has already been projected)\n    if asm_norm:\n        asm_attn.set_up(acq_data)\n        bin_eff = pet.AcquisitionData(acq_data)\n        bin_eff.fill(1.0)\n        print('getting sinograms for multiplicative factors...')\n        asm.set_up(acq_data)\n        asm.unnormalise(bin_eff)\n\n    print('writing multiplicative sinogram: ' + outp_file)\n    bin_eff.write(outp_file)\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "# ---\n# jupyter:\n#   jupytext:\n#     cell_metadata_filter: tags,-all\n#     notebook_metadata_filter: -jupytext.text_representation.jupytext_version\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#   kernelspec:\n#     display_name: Python 3 (ipykernel)\n#     language: python\n#     name: python3\n# ---\n\n# %% tags=[\"remove-cell\"]\n# bluemira is an integrated inter-disciplinary design tool for future fusion\n# reactors. It incorporates several modules, some of which rely on other\n# codes, to carry out a range of typical conceptual fusion reactor design\n# activities.\n#\n# Copyright (C) 2021-2023 M. Coleman, J. Cook, F. Franza, I.A. Maione, S. McIntosh,\n#                         J. Morris, D. Short\n#\n# bluemira is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# bluemira is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with bluemira; if not, see <https://www.gnu.org/licenses/>.\n\n\"\"\"\nApplication of the dolfin fem 2D magnetostatic to a single coil problem\n\"\"\"\n\n# %% [markdown]\n# # 2-D FEM magnetostatic single coil\n# ## Introduction\n#\n# In this example, we will show how to use the fem_magnetostatic_2D solver to find the\n# magnetic field generated by a simple coil. The coil axis is the z-axis. Solution is\n# calculated on the xz plane.\n#\n# ## Imports\n#\n# Import necessary module definitions.\n\n# %%\nimport os\n\nimport dolfin\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport bluemira.geometry.tools as tools\nimport bluemira.magnetostatics.greens as greens\nfrom bluemira.base.components import Component, PhysicalComponent\nfrom bluemira.base.file import get_bluemira_path\nfrom bluemira.geometry.face import BluemiraFace\nfrom bluemira.magnetostatics.finite_element_2d import (\n    Bz_coil_axis,\n    FemMagnetostatic2d,\n    ScalarSubFunc,\n)\nfrom bluemira.mesh import meshing\nfrom bluemira.mesh.tools import import_mesh, msh_to_xdmf\n\n# %% [markdown]\n#\n# ## Creation of the geometry\n#\n# Definition of coil and enclosure parameters\n\n# %%\nr_enclo = 100\nlcar_enclo = 1.0\n\nrc = 5\ndrc = 0.025\nlcar_coil = 0.05\n\n# %% [markdown]\n#\n# create the coil (rectangular cross section) and set the mesh options\n\n# %%\npoly_coil = tools.make_polygon(\n    [[rc - drc, rc + drc, rc + drc, rc - drc], [0, 0, 0, 0], [-drc, -drc, +drc, +drc]],\n    closed=True,\n    label=\"poly_enclo\",\n)\n\npoly_coil.mesh_options = {\"lcar\": lcar_coil, \"physical_group\": \"poly_coil\"}\ncoil = BluemiraFace(poly_coil)\ncoil.mesh_options = {\"lcar\": lcar_coil, \"physical_group\": \"coil\"}\n\n# %% [markdown]\n#\n# create the enclosure (rectangular cross section) and set the mesh options\n\n# %%\npoly_enclo = tools.make_polygon(\n    [[0, r_enclo, r_enclo, 0], [0, 0, 0, 0], [-r_enclo, -r_enclo, r_enclo, r_enclo]],\n    closed=True,\n    label=\"poly_enclo\",\n)\n\npoly_enclo.mesh_options = {\"lcar\": lcar_enclo, \"physical_group\": \"poly_enclo\"}\nenclosure = BluemiraFace([poly_enclo, poly_coil])\nenclosure.mesh_options = {\"lcar\": lcar_enclo, \"physical_group\": \"enclo\"}\n\n# %% [markdown]\n#\n# create the different components\n\n# %%\nc_universe = Component(name=\"universe\")\nc_enclo = PhysicalComponent(name=\"enclosure\", shape=enclosure, parent=c_universe)\nc_coil = PhysicalComponent(name=\"coil\", shape=coil, parent=c_universe)\n\n# %% [markdown]\n#\n# ## Mesh\n#\n# Create the mesh (by default, mesh is stored in the file Mesh.msh\")\n\n# %%\ndirectory = get_bluemira_path(\"\", subfolder=\"generated_data\")\nmeshfiles = [os.path.join(directory, p) for p in [\"Mesh.geo_unrolled\", \"Mesh.msh\"]]\n\nmeshing.Mesh(meshfile=meshfiles)(c_universe, dim=2)\n\n# %% [markdown]\n#\n# Convert the mesh in xdmf for reading in fenics.\n\n# %%\nmsh_to_xdmf(\"Mesh.msh\", dimensions=(0, 2), directory=directory)\n\nmesh, boundaries, subdomains, labels = import_mesh(\n    \"Mesh\",\n    directory=directory,\n    subdomains=True,\n)\ndolfin.plot(mesh)\nplt.show()\n\n# %% [markdown]\n#\n# ## Setup EM problem\n#\n# Finally, instantiate the em solver\n\n# %%\nem_solver = FemMagnetostatic2d(2)\nem_solver.set_mesh(mesh, boundaries)\n\n# %% [markdown]\n#\n# Define source term (coil current distribution) for the fem problem\n\n# %%\nIc = 1e6\njc = Ic / coil.area\nmarkers = [labels[\"coil\"]]\nfunctions = [jc]\njtot = ScalarSubFunc(functions, markers, subdomains)\n\n# %% [markdown]\n#\n# plot the source term\n#\n# Note: depending on the geometric dimension of the coil, enclosure, and mesh\n# characteristic length, the plot could be not so \"explanatory\".\n\n# %%\nf_space = dolfin.FunctionSpace(mesh, \"DG\", 0)\nf = dolfin.Function(f_space)\nf.interpolate(jtot)\ndolfin.plot(f, title=\"Source term\")\nplt.show()\n\n# %% [markdown]\n#\n# solve the em problem and calculate the magnetic field B\n\n# %%\nem_solver.define_g(jtot)\nem_solver.solve()\nem_solver.calculate_b()\n\n# %% [markdown]\n#\n# Compare the obtained B with both the theoretical value\n#\n# 1) Along the z axis (analytical solution)\n\n# %%\nz_points_axis = np.linspace(0, r_enclo, 200)\nr_points_axis = np.zeros(z_points_axis.shape)\nBz_axis = np.array(\n    [em_solver.B(x) for x in np.array([r_points_axis, z_points_axis]).T]\n).T[1]\nB_teo = np.array([Bz_coil_axis(rc, 0, z, Ic) for z in z_points_axis])\n\nfig, ax = plt.subplots()\nax.plot(z_points_axis, Bz_axis, label=\"B_calc\")\nax.plot(z_points_axis, B_teo, label=\"B_teo\")\nplt.xlabel(\"r (m)\")\nplt.ylabel(\"B (T)\")\nplt.legend()\nplt.show()\n\ndiff = Bz_axis - B_teo\n\nfig, ax = plt.subplots()\nax.plot(z_points_axis, diff, label=\"B_calc - B_teo\")\nplt.xlabel(\"r (m)\")\nplt.ylabel(\"error (T)\")\nplt.legend()\nplt.show()\n\n# %% [markdown]\n#\n# 1) Along a radial path at z_offset (solution from green function)\n\n# %%\nz_offset = 40 * drc\n\npoints_x = np.linspace(0, r_enclo, 200)\npoints_z = np.zeros(z_points_axis.shape) + z_offset\n\ng_psi, g_bx, g_bz = greens.greens_all(rc, 0, points_x, points_z)\ng_psi *= Ic\ng_bx *= Ic\ng_bz *= Ic\nB_fem = np.array([em_solver.B(x) for x in np.array([points_x, points_z]).T])\nBx_fem = B_fem.T[0]\nBz_fem = B_fem.T[1]\n\nfig, ax = plt.subplots()\nax.plot(z_points_axis, Bx_fem, label=\"Bx_fem\")\nax.plot(z_points_axis, g_bx, label=\"Green Bx\")\nplt.xlabel(\"r (m)\")\nplt.ylabel(\"Bx (T)\")\nplt.legend()\nplt.show()\n\nfig, ax = plt.subplots()\nax.plot(z_points_axis, Bz_fem, label=\"Bz_fem\")\nax.plot(z_points_axis, g_bz, label=\"Green Bz\")\nplt.xlabel(\"r (m)\")\nplt.ylabel(\"Bz (T)\")\nplt.legend()\nplt.show()\n\ndiff1 = Bx_fem - g_bx\ndiff2 = Bz_fem - g_bz\n\nfig, ax = plt.subplots()\nax.plot(z_points_axis, diff1, label=\"B_calc - GreenBx\")\nax.plot(z_points_axis, diff2, label=\"B_calc - GreenBz\")\nplt.legend()\nplt.xlabel(\"r (m)\")\nplt.ylabel(\"error (T)\")\nplt.show()\n",
  "#!/usr/bin/env python\n# /*##########################################################################\n#\n# Copyright (c) 2016-2018 European Synchrotron Radiation Facility\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n#\n# ###########################################################################*/\n\"\"\"This script shows a gallery of simple widgets provided by silx.\n\nIt shows the following widgets:\n\n- :class:`~silx.gui.widgets.WaitingPushButton`:\n  A button with a progress-like waiting animated icon.\n\"\"\"\n\n__authors__ = [\"V. Valls\"]\n__license__ = \"MIT\"\n__date__ = \"02/08/2018\"\n\nimport sys\nimport functools\nimport numpy\n\nfrom silx.gui import qt\nfrom silx.gui.colors import Colormap\nfrom silx.gui.widgets.WaitingPushButton import WaitingPushButton\nfrom silx.gui.widgets.ThreadPoolPushButton import ThreadPoolPushButton\nfrom silx.gui.widgets.RangeSlider import RangeSlider\nfrom silx.gui.widgets.LegendIconWidget import LegendIconWidget\nfrom silx.gui.widgets.ElidedLabel import ElidedLabel\n\n\nclass SimpleWidgetExample(qt.QMainWindow):\n    \"\"\"This windows shows simple widget provided by silx.\"\"\"\n\n    def __init__(self):\n        \"\"\"Constructor\"\"\"\n        qt.QMainWindow.__init__(self)\n        self.setWindowTitle(\"Silx simple widget example\")\n\n        main_panel = qt.QWidget(self)\n        main_panel.setLayout(qt.QVBoxLayout())\n\n        layout = main_panel.layout()\n        layout.addWidget(qt.QLabel(\"WaitingPushButton\"))\n        layout.addWidget(self.createWaitingPushButton())\n        layout.addWidget(self.createWaitingPushButton2())\n\n        layout.addWidget(qt.QLabel(\"ThreadPoolPushButton\"))\n        layout.addWidget(self.createThreadPoolPushButton())\n\n        layout.addWidget(qt.QLabel(\"RangeSlider\"))\n        layout.addWidget(self.createRangeSlider())\n        layout.addWidget(self.createRangeSliderWithBackground())\n\n        panel = self.createLegendIconPanel(self)\n        layout.addWidget(qt.QLabel(\"LegendIconWidget\"))\n        layout.addWidget(panel)\n\n        panel = self.createElidedLabelPanel(self)\n        layout.addWidget(qt.QLabel(\"ElidedLabel\"))\n        layout.addWidget(panel)\n\n        self.setCentralWidget(main_panel)\n\n    def createWaitingPushButton(self):\n        widget = WaitingPushButton(text=\"Push me and wait for ever\")\n        widget.clicked.connect(widget.swapWaiting)\n        return widget\n\n    def createWaitingPushButton2(self):\n        widget = WaitingPushButton(text=\"Push me\")\n        widget.setDisabledWhenWaiting(False)\n        widget.clicked.connect(widget.swapWaiting)\n        return widget\n\n    def printResult(self, result):\n        print(result)\n\n    def printError(self, result):\n        print(\"Error\")\n        print(result)\n\n    def printEvent(self, eventName, *args):\n        print(\"Event %s: %s\" % (eventName, args))\n\n    def takesTimeToComputePow(self, a, b):\n        qt.QThread.sleep(2)\n        return a ** b\n\n    def createThreadPoolPushButton(self):\n        widget = ThreadPoolPushButton(text=\"Compute 2^16\")\n        widget.setCallable(self.takesTimeToComputePow, 2, 16)\n        widget.succeeded.connect(self.printResult)\n        widget.failed.connect(self.printError)\n        return widget\n\n    def createRangeSlider(self):\n        widget = RangeSlider(self)\n        widget.setRange(0, 500)\n        widget.setValues(100, 400)\n        widget.sigValueChanged.connect(functools.partial(self.printEvent, \"sigValueChanged\"))\n        widget.sigPositionChanged.connect(functools.partial(self.printEvent, \"sigPositionChanged\"))\n        widget.sigPositionCountChanged.connect(functools.partial(self.printEvent, \"sigPositionCountChanged\"))\n        return widget\n\n    def createRangeSliderWithBackground(self):\n        widget = RangeSlider(self)\n        widget.setRange(0, 500)\n        widget.setValues(100, 400)\n        background = numpy.sin(numpy.arange(250) / 250.0)\n        background[0], background[-1] = background[-1], background[0]\n        colormap = Colormap(\"viridis\")\n        widget.setGroovePixmapFromProfile(background, colormap)\n        return widget\n\n    def createLegendIconPanel(self, parent):\n        panel = qt.QWidget(parent)\n        layout = qt.QVBoxLayout(panel)\n\n        # Empty\n        legend = LegendIconWidget(panel)\n        layout.addWidget(legend)\n\n        # Line\n        legend = LegendIconWidget(panel)\n        legend.setLineStyle(\"-\")\n        legend.setLineColor(\"blue\")\n        legend.setLineWidth(2)\n        layout.addWidget(legend)\n\n        # Symbol\n        legend = LegendIconWidget(panel)\n        legend.setSymbol(\"o\")\n        legend.setSymbolColor(\"red\")\n        layout.addWidget(legend)\n\n        # Line and symbol\n        legend = LegendIconWidget(panel)\n        legend.setLineStyle(\":\")\n        legend.setLineColor(\"green\")\n        legend.setLineWidth(2)\n        legend.setSymbol(\"x\")\n        legend.setSymbolColor(\"violet\")\n        layout.addWidget(legend)\n\n        # Colormap\n        legend = LegendIconWidget(panel)\n        legend.setColormap(\"viridis\")\n        layout.addWidget(legend)\n\n        # Symbol and colormap\n        legend = LegendIconWidget(panel)\n        legend.setSymbol(\"o\")\n        legend.setSymbolColormap(\"viridis\")\n        layout.addWidget(legend)\n\n        # Symbol (without surface) and colormap\n        legend = LegendIconWidget(panel)\n        legend.setSymbol(\"+\")\n        legend.setSymbolColormap(\"plasma\")\n        layout.addWidget(legend)\n\n        # Colormap + Line + Symbol\n        legend = LegendIconWidget(panel)\n        legend.setColormap(\"gray\")\n        legend.setLineStyle(\"-\")\n        legend.setLineColor(\"white\")\n        legend.setLineWidth(3)\n        legend.setSymbol(\".\")\n        legend.setSymbolColormap(\"red\")\n        layout.addWidget(legend)\n\n        return panel\n\n    def createElidedLabelPanel(self, parent):\n        panel = qt.QWidget(parent)\n        layout = qt.QVBoxLayout(panel)\n\n        label = ElidedLabel(parent)\n        label.setText(\"A very long text which is far too long.\")\n        layout.addWidget(label)\n\n        label = ElidedLabel(parent)\n        label.setText(\"A very long text which is far too long.\")\n        label.setElideMode(qt.Qt.ElideMiddle)\n        layout.addWidget(label)\n\n        label = ElidedLabel(parent)\n        label.setText(\"Basically nothing.\")\n        layout.addWidget(label)\n\n        return panel\n\n\ndef main():\n    \"\"\"\n    Main function\n    \"\"\"\n    app = qt.QApplication([])\n    sys.excepthook = qt.exceptionHandler\n    window = SimpleWidgetExample()\n    window.show()\n    result = app.exec()\n    # remove ending warnings relative to QTimer\n    app.deleteLater()\n    sys.excepthook = sys.__excepthook__\n    sys.exit(result)\n\n\nmain()\n",
  "#!/usr/bin/env python3\n#\n# This example shows how to set up a simple CODE-like runaway\n# scenario in DREAM. The simulation uses a constant temperature,\n# density and electric field, and generates a runaway current\n# through the electric field acceleration, demonstrating Dreicer generation.\n#\n# Run as\n#\n#   $ ./basic.py\n#   $ ../../build/iface/dreami dream_settings.h5\n#\n# ###################################################################\n\nimport numpy as np\nimport sys\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nimport DREAM.Settings.Equations.DistributionFunction as DistFunc\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Equations.RunawayElectrons as Runaways\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.CollisionHandler as Collisions\n\nds = DREAMSettings()\n#ds.collisions.collfreq_type = Collisions.COLLFREQ_TYPE_COMPLETELY_SCREENED\nds.collisions.collfreq_type = Collisions.COLLFREQ_TYPE_PARTIALLY_SCREENED\n\n# Physical parameters\nE = 6       # Electric field strength (V/m)\nn = 5e19    # Electron density (m^-3)\nT = 100     # Temperature (eV)\n\n# Grid parameters\npMax = 1    # maximum momentum in units of m_e*c\nNp   = 300  # number of momentum grid points\nNxi  = 20   # number of pitch grid points\ntMax = 1e-3 # simulation time in seconds\nNt   = 20   # number of time steps\n\n# Set E_field\nds.eqsys.E_field.setPrescribedData(E)\n\n# Set temperature\nds.eqsys.T_cold.setPrescribedData(T)\n\n# Set ions\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_PRESCRIBED_FULLY_IONIZED, n=n)\n\n# Disable avalanche generation\nds.eqsys.n_re.setAvalanche(avalanche=Runaways.AVALANCHE_MODE_NEGLECT)\n\n# Hot-tail grid settings\nds.hottailgrid.setNxi(Nxi)\nds.hottailgrid.setNp(Np)\nds.hottailgrid.setPmax(pMax)\n\n# Set initial hot electron Maxwellian\nds.eqsys.f_hot.setInitialProfiles(n0=n, T0=T)\n\n# Set boundary condition type at pMax\n#ds.eqsys.f_hot.setBoundaryCondition(DistFunc.BC_PHI_CONST) # extrapolate flux to boundary\nds.eqsys.f_hot.setBoundaryCondition(DistFunc.BC_F_0) # F=0 outside the boundary\nds.eqsys.f_hot.setSynchrotronMode(DistFunc.SYNCHROTRON_MODE_NEGLECT)\nds.eqsys.f_hot.setAdvectionInterpolationMethod(DistFunc.AD_INTERP_UPWIND)\n\n# Disable runaway grid\nds.runawaygrid.setEnabled(False)\n\n# Set up radial grid\nds.radialgrid.setB0(5)\nds.radialgrid.setMinorRadius(0.22)\nds.radialgrid.setWallRadius(0.22)\nds.radialgrid.setNr(1)\n\n# Set solver type\nds.solver.setType(Solver.LINEAR_IMPLICIT) # semi-implicit time stepping\nds.solver.preconditioner.setEnabled(False)\n\n# include otherquantities to save to output\nds.other.include('fluid','nu_s','nu_D')\n\n# Set time stepper\nds.timestep.setTmax(tMax)\nds.timestep.setNt(Nt)\n\nds.output.setTiming(stdout=True, file=True)\nds.output.setFilename('output.h5')\n\n# Save settings to HDF5 file\nds.save('dream_settings.h5')\n\n",
  "#\n#  ISC License\n#\n#  Copyright (c) 2016, Autonomous Vehicle Systems Lab, University of Colorado at Boulder\n#\n#  Permission to use, copy, modify, and/or distribute this software for any\n#  purpose with or without fee is hereby granted, provided that the above\n#  copyright notice and this permission notice appear in all copies.\n#\n#  THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n#  WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n#  MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n#  ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n#  ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n#  OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n#\n\nr\"\"\"\n\nThis script illustrates how to run a Monte Carlo simulation where the Spice is used within the Python\nsetup.  Note that the Python Spice setup is separate from the BSK c++ Spice module setup.  In this tutorial\na very simple simulation is shown to showcase how to correctly perform Python-based Spice function calls with a\nBasilisk Monte Carlo run.\n\nThe script is found in the folder ``basilisk/examples`` and executed by using::\n\n      python3 scenarioMonteCarloSpice.py\n\nThe simulation sets up a simple spacecraft and associated initial conditions.  Note that the Basilisk spacecraft\nsimulation is setup within the class ``MySimulation``.  Here the the code is added to load Spice kernels within\nPython to pull the Hubble states from Spice.  Thus, this python Spice call is performed within each Monte Carlo\nthread.  In this simple example the Hubble states are then printed to the terminal.\n\nAs this Monte Carlo scenario is setup to run 12 times, by running this script the user should see\nno errors and the Hubble states printed out 12 times.\n\nIn the Controller class `MyController` there is Spice kernel loading code that is commented out.\nIf the kernels are loaded within the controller class then this results in a Spice kernel loading error.\n\nThe user should be careful to load the Spice or use within the Python code within the simulation class.\n\n\n\n\"\"\"\n\n#\n# Basilisk Integrated Test\n#\n# Purpose:  This Monte Carlo example shows how to properly use Spice in such simulations.\n#\n\n\nimport inspect\nimport os\nimport shutil\n\n# @cond DOXYGEN_IGNORE\nfilename = inspect.getframeinfo(inspect.currentframe()).filename\nfileNameString = os.path.basename(os.path.splitext(__file__)[0])\npath = os.path.dirname(os.path.abspath(filename))\n# @endcond\n\nfrom Basilisk import __path__\nbskPath = __path__[0]\n\nfrom Basilisk.utilities import SimulationBaseClass\nfrom Basilisk.utilities import macros\nfrom Basilisk.topLevelModules import pyswice\nfrom Basilisk.utilities.pyswice_spk_utilities import spkRead\n\nfrom Basilisk.simulation import spacecraft\n\nfrom Basilisk.utilities.MonteCarlo.Controller import Controller\n\n\nclass MyController(Controller):\n    def __init__(self):  # Constructor for Monte Carlo simulations\n        Controller.__init__(self)\n\n        # Uncomment the following block to cause this MC scenario to fail\n        # due to an incorrect usage of the pyswice module\n\n        # dataPath = bskPath + \"/supportData/EphemerisData/\"\n        # pyswice.furnsh_c(dataPath + 'naif0011.tls')\n        # pyswice.furnsh_c(dataPath + 'pck00010.tpc')\n        # pyswice.furnsh_c(dataPath + 'de-403-masses.tpc')\n        # pyswice.furnsh_c(dataPath + 'de430.bsp')\n        # pyswice.furnsh_c(dataPath + 'hst_edited.bsp')\n\n\nclass MySimulation(SimulationBaseClass.SimBaseClass):\n    def __init__(self):\n        SimulationBaseClass.SimBaseClass.__init__(self)\n        # Create simulation variable names\n        simTaskName = \"simTask\"\n        simProcessName = \"simProcess\"\n\n\n        self.dynProcess = self.CreateNewProcess(simProcessName)\n\n        self.dynProcess.addTask(self.CreateNewTask(simTaskName, macros.sec2nano(10.)))\n\n        scObject = spacecraft.Spacecraft()\n        self.AddModelToTask(simTaskName, scObject, 1)\n        scObject.hub.r_CN_NInit = [7000000.0, 0.0, 0.0]     # m   - r_CN_N\n        scObject.hub.v_CN_NInit = [0.0, 7500.0, 0.0]        # m/s - v_CN_N\n\n\n        # operate on pyswice\n        dataPath = bskPath + \"/supportData/EphemerisData/\"\n        self.scSpiceName = 'HUBBLE SPACE TELESCOPE'\n        pyswice.furnsh_c(dataPath + 'naif0011.tls')\n        pyswice.furnsh_c(dataPath + 'pck00010.tpc')\n        pyswice.furnsh_c(dataPath + 'de-403-masses.tpc')\n        pyswice.furnsh_c(dataPath + 'de430.bsp')\n        pyswice.furnsh_c(dataPath + 'hst_edited.bsp')\n\n        self.accessSpiceKernel()\n\n        # This is a hack because of a bug in Basilisk... leave this line it keeps\n        # variables from going out of scope after this function returns\n        self.additionalReferences = [scObject]\n\n    def accessSpiceKernel(self):\n        startCalendarTime = '2012 APR 29 15:18:14.907 (UTC)'\n        zeroBase = 'Sun'\n        integFrame = 'j2000'\n        stateOut = spkRead(self.scSpiceName, startCalendarTime, integFrame, zeroBase)\n        print(stateOut)\n\ndef run():\n    \"\"\"\n    This is the main function that is called in this script.  It illustrates possible ways\n    to include the Python Spice library in a simulation that uses Monte Carlo runs.\n    \"\"\"\n\n    # First, the `Controller` class is used in order to define the simulation\n    monteCarlo = MyController()\n    monteCarlo.setSimulationFunction(MySimulation)\n    monteCarlo.setExecutionFunction(executeScenario)\n    monteCarlo.setExecutionCount(12)\n    monteCarlo.setShouldDisperseSeeds(True)\n    monteCarlo.setThreadCount(6)\n    monteCarlo.setVerbose(False)\n\n    dirName = \"montecarlo_test\" + str(os.getpid())\n    monteCarlo.setArchiveDir(dirName)\n\n    # Here is another example where it is allowable to run the python spice routines within a MC simulation setup\n    #\n    # dataPath = bskPath + \"/supportData/EphemerisData/\"\n    # pyswice.furnsh_c(dataPath + 'naif0011.tls')\n    # pyswice.furnsh_c(dataPath + 'pck00010.tpc')\n    # pyswice.furnsh_c(dataPath + 'de-403-masses.tpc')\n    # pyswice.furnsh_c(dataPath + 'de430.bsp')\n    #\n    # startCalendarTime = '2012 AUG 05, 21:35:07.496 (UTC)'\n    # startTimeArray = sim_model.new_doubleArray(1)\n    # pyswice.str2et_c(startCalendarTime, startTimeArray)\n    # sim_model.delete_doubleArray(startTimeArray)\n\n    # After the monteCarlo run is configured, it is executed.\n    failures = monteCarlo.executeSimulations()\n\n    # Now we clean up data from this test\n    shutil.rmtree(dirName)\n\n    return\n\n\ndef executeScenario(sim):\n    sim.ConfigureStopTime(macros.sec2nano(100.))\n    sim.InitializeSimulation()\n\n    # Here is another example where it is allowable to run the python spice routines within a MC simulation setup\n    #\n    # dataPath = bskPath + \"/supportData/EphemerisData/\"\n    # pyswice.furnsh_c(dataPath + 'naif0011.tls')\n    # pyswice.furnsh_c(dataPath + 'pck00010.tpc')\n    # pyswice.furnsh_c(dataPath + 'de-403-masses.tpc')\n    # pyswice.furnsh_c(dataPath + 'de430.bsp')\n\n    sim.ExecuteSimulation()\n\n\nif __name__ == \"__main__\":\n    run()\n",
  "\"\"\"\n=============================================================\nComparison subplots of various index based bandits algorithms\n=============================================================\n\nThis script Compare several bandits agents and as a sub-product also shows\nhow to use subplots in with `plot_writer_data`\n\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rlberry.envs.bandits import BernoulliBandit\nfrom rlberry.manager import ExperimentManager, plot_writer_data\nfrom rlberry.wrappers import WriterWrapper\nfrom rlberry.agents.bandits import (\n    IndexAgent,\n    RandomizedAgent,\n    makeBoundedIMEDIndex,\n    makeBoundedMOSSIndex,\n    makeBoundedNPTSIndex,\n    makeBoundedUCBIndex,\n    makeBoundedUCBVIndex,\n    makeETCIndex,\n    makeEXP3Index,\n)\n\n# Agents definition\n# sphinx_gallery_thumbnail_number = 2\n\n\n# Parameters of the problem\nmeans = np.array([0.6, 0.6, 0.6, 0.9])  # means of the arms\nA = len(means)\nT = 2000  # Horizon\nM = 10  # number of MC simu\n\n# Construction of the experiment\n\nenv_ctor = BernoulliBandit\nenv_kwargs = {\"p\": means}\n\n\nclass UCBAgent(IndexAgent):\n    name = \"UCB\"\n\n    def __init__(self, env, **kwargs):\n        index, _ = makeBoundedUCBIndex()\n        IndexAgent.__init__(self, env, index, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\n\nclass UCBVAgent(IndexAgent):\n    name = \"UCBV\"\n\n    def __init__(self, env, **kwargs):\n        index, params = makeBoundedUCBVIndex()\n        IndexAgent.__init__(self, env, index, tracker_params=params, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\n\nclass ETCAgent(IndexAgent):\n    name = \"ETC\"\n\n    def __init__(self, env, m=20, **kwargs):\n        index, _ = makeETCIndex(A, m)\n        IndexAgent.__init__(self, env, index, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\n\nclass MOSSAgent(IndexAgent):\n    name = \"MOSS\"\n\n    def __init__(self, env, **kwargs):\n        index, _ = makeBoundedMOSSIndex(T, A)\n        IndexAgent.__init__(self, env, index, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\n\nclass IMEDAgent(IndexAgent):\n    name = \"IMED\"\n\n    def __init__(self, env, **kwargs):\n        index, tracker_params = makeBoundedIMEDIndex()\n        IndexAgent.__init__(self, env, index, tracker_params=tracker_params, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\n\nclass NPTSAgent(IndexAgent):\n    name = \"NPTS\"\n\n    def __init__(self, env, **kwargs):\n        index, tracker_params = makeBoundedNPTSIndex()\n        IndexAgent.__init__(self, env, index, tracker_params=tracker_params, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\n\nclass EXP3Agent(RandomizedAgent):\n    name = \"EXP3\"\n\n    def __init__(self, env, **kwargs):\n        prob, tracker_params = makeEXP3Index()\n        RandomizedAgent.__init__(\n            self, env, prob, tracker_params=tracker_params, **kwargs\n        )\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\n\nAgents_class = [\n    ETCAgent,\n    EXP3Agent,\n    IMEDAgent,\n    MOSSAgent,\n    NPTSAgent,\n    UCBAgent,\n    UCBVAgent,\n]\n\nagents = [\n    ExperimentManager(\n        Agent,\n        (env_ctor, env_kwargs),\n        fit_budget=T,\n        n_fit=M,\n        parallelization=\"process\",\n        mp_context=\"fork\",\n    )\n    for Agent in Agents_class\n]\n\n# these parameters should give parallel computing even in notebooks\n\n\n# Agent training\nfor agent in agents:\n    agent.fit()\n\n\n# Compute and plot regret\ndef compute_regret(rewards):\n    return np.cumsum(np.max(means) - rewards)\n\n\n# Compute and plot (pseudo-)regret\ndef compute_pseudo_regret(actions):\n    return np.cumsum(np.max(means) - means[actions.astype(int)])\n\n\noutput = plot_writer_data(\n    agents,\n    tag=\"action\",\n    preprocess_func=compute_pseudo_regret,\n    title=\"Cumulative Pseudo-Regret\",\n    sns_kwargs={\"style\": \"name\"},  # to have varying linestyles\n)\n\noutput = plot_writer_data(\n    agents,\n    tag=\"reward\",\n    preprocess_func=compute_regret,\n    title=\"Cumulative Regret\",\n    sns_kwargs={\"style\": \"name\"},  # to have varying linestyles\n)\n\n\n# Compute and plot number of times each arm was selected\ndef compute_na(actions, a):\n    return np.cumsum(actions == a)\n\n\nfig, axes = plt.subplots(2, 2, sharey=True, figsize=(6, 6))\naxes = axes.ravel()\nfor arm in range(A):\n    output = plot_writer_data(\n        agents,\n        tag=\"action\",\n        preprocess_func=lambda actions: compute_na(actions, arm),\n        title=\"Na for arm \" + str(arm) + \", mean=\" + str(means[arm]),\n        ax=axes[arm],\n        show=False,\n        sns_kwargs={\"style\": \"name\"},  # to have varying linestyles\n    )\nfig.tight_layout()\nplt.show()\n",
  "import numpy as np\nfrom loguru import logger\n\nfrom sc2 import maps\nfrom sc2.bot_ai import BotAI\nfrom sc2.data import Difficulty, Race, Result\nfrom sc2.ids.ability_id import AbilityId\nfrom sc2.ids.buff_id import BuffId\nfrom sc2.ids.unit_typeid import UnitTypeId\nfrom sc2.ids.upgrade_id import UpgradeId\nfrom sc2.main import run_game\nfrom sc2.player import Bot, Computer\nfrom sc2.position import Point2, Point3\nfrom sc2.unit import Unit\nfrom sc2.units import Units\n\n\n# pylint: disable=W0231\nclass ZergRushBot(BotAI):\n\n    def __init__(self):\n        self.on_end_called = False\n\n    async def on_start(self):\n        self.client.game_step = 2\n\n    # pylint: disable=R0912\n    async def on_step(self, iteration):\n        if iteration == 0:\n            await self.chat_send(\"(glhf)\")\n\n        # Draw creep pixelmap for debugging\n        # self.draw_creep_pixelmap()\n\n        # If townhall no longer exists: attack move with all units to enemy start location\n        if not self.townhalls:\n            for unit in self.units.exclude_type({UnitTypeId.EGG, UnitTypeId.LARVA}):\n                unit.attack(self.enemy_start_locations[0])\n            return\n\n        hatch: Unit = self.townhalls[0]\n\n        # Pick a target location\n        target: Point2 = self.enemy_structures.not_flying.random_or(self.enemy_start_locations[0]).position\n\n        # Give all zerglings an attack command\n        for zergling in self.units(UnitTypeId.ZERGLING):\n            zergling.attack(target)\n\n        # Inject hatchery if queen has more than 25 energy\n        for queen in self.units(UnitTypeId.QUEEN):\n            if queen.energy >= 25 and not hatch.has_buff(BuffId.QUEENSPAWNLARVATIMER):\n                queen(AbilityId.EFFECT_INJECTLARVA, hatch)\n\n        # Pull workers out of gas if we have almost enough gas mined, this will stop mining when we reached 100 gas mined\n        if self.vespene >= 88 or self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) > 0:\n            gas_drones: Units = self.workers.filter(lambda w: w.is_carrying_vespene and len(w.orders) < 2)\n            drone: Unit\n            for drone in gas_drones:\n                minerals: Units = self.mineral_field.closer_than(10, hatch)\n                if minerals:\n                    mineral: Unit = minerals.closest_to(drone)\n                    drone.gather(mineral, queue=True)\n\n        # If we have 100 vespene, this will try to research zergling speed once the spawning pool is at 100% completion\n        if self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED\n                                        ) == 0 and self.can_afford(UpgradeId.ZERGLINGMOVEMENTSPEED):\n            spawning_pools_ready: Units = self.structures(UnitTypeId.SPAWNINGPOOL).ready\n            if spawning_pools_ready:\n                self.research(UpgradeId.ZERGLINGMOVEMENTSPEED)\n\n        # If we have less than 2 supply left and no overlord is in the queue: train an overlord\n        if self.supply_left < 2 and self.already_pending(UnitTypeId.OVERLORD) < 1:\n            self.train(UnitTypeId.OVERLORD, 1)\n\n        # While we have less than 88 vespene mined: send drones into extractor one frame at a time\n        if (\n            self.gas_buildings.ready and self.vespene < 88\n            and self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) == 0\n        ):\n            extractor: Unit = self.gas_buildings.first\n            if extractor.surplus_harvesters < 0:\n                self.workers.random.gather(extractor)\n\n        # If we have lost of minerals, make a macro hatchery\n        if self.minerals > 500:\n            for d in range(4, 15):\n                pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                if await self.can_place_single(UnitTypeId.HATCHERY, pos):\n                    self.workers.random.build(UnitTypeId.HATCHERY, pos)\n                    break\n\n        # While we have less than 16 drones, make more drones\n        if self.can_afford(UnitTypeId.DRONE) and self.supply_workers < 16:\n            self.train(UnitTypeId.DRONE)\n\n        # If our spawningpool is completed, start making zerglings\n        if self.structures(UnitTypeId.SPAWNINGPOOL).ready and self.larva and self.can_afford(UnitTypeId.ZERGLING):\n            _amount_trained: int = self.train(UnitTypeId.ZERGLING, self.larva.amount)\n\n        # If we have no extractor, build extractor\n        if (\n            self.gas_buildings.amount + self.already_pending(UnitTypeId.EXTRACTOR) == 0\n            and self.can_afford(UnitTypeId.EXTRACTOR) and self.workers\n        ):\n            drone: Unit = self.workers.random\n            target: Unit = self.vespene_geyser.closest_to(drone)\n            drone.build_gas(target)\n\n        # If we have no spawning pool, try to build spawning pool\n        elif self.structures(UnitTypeId.SPAWNINGPOOL).amount + self.already_pending(UnitTypeId.SPAWNINGPOOL) == 0:\n            if self.can_afford(UnitTypeId.SPAWNINGPOOL):\n                for d in range(4, 15):\n                    pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                    if await self.can_place_single(UnitTypeId.SPAWNINGPOOL, pos):\n                        drone: Unit = self.workers.closest_to(pos)\n                        drone.build(UnitTypeId.SPAWNINGPOOL, pos)\n\n        # If we have no queen, try to build a queen if we have a spawning pool compelted\n        elif (\n            self.units(UnitTypeId.QUEEN).amount + self.already_pending(UnitTypeId.QUEEN) < self.townhalls.amount\n            and self.structures(UnitTypeId.SPAWNINGPOOL).ready\n        ):\n            if self.can_afford(UnitTypeId.QUEEN):\n                self.train(UnitTypeId.QUEEN)\n\n    def draw_creep_pixelmap(self):\n        for (y, x), value in np.ndenumerate(self.state.creep.data_numpy):\n            p = Point2((x, y))\n            h2 = self.get_terrain_z_height(p)\n            pos = Point3((p.x, p.y, h2))\n            # Red if there is no creep\n            color = Point3((255, 0, 0))\n            if value == 1:\n                # Green if there is creep\n                color = Point3((0, 255, 0))\n            self.client.debug_box2_out(pos, half_vertex_length=0.25, color=color)\n\n    async def on_end(self, game_result: Result):\n        self.on_end_called = True\n        logger.info(f\"{self.time_formatted} On end was called\")\n\n\ndef main():\n    run_game(\n        maps.get(\"AcropolisLE\"),\n        [Bot(Race.Zerg, ZergRushBot()), Computer(Race.Terran, Difficulty.Medium)],\n        realtime=False,\n        save_replay_as=\"ZvT.SC2Replay\",\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "from kivy.uix.gridlayout import GridLayout\nfrom kivy.uix.button import Button\nfrom kivy.uix.behaviors import CompoundSelectionBehavior\nfrom kivy.uix.behaviors import FocusBehavior\nfrom kivy.app import runTouchApp\n\n\nclass SelectableGrid(FocusBehavior, CompoundSelectionBehavior, GridLayout):\n\n    def __init__(self, **kwargs):\n        super(SelectableGrid, self).__init__(**kwargs)\n\n        def print_selection(*l):\n            print('selected: ', [x.text for x in self.selected_nodes])\n        self.bind(selected_nodes=print_selection)\n\n    def keyboard_on_key_down(self, window, keycode, text, modifiers):\n        if super(SelectableGrid, self).keyboard_on_key_down(\n                window, keycode, text, modifiers):\n            return True\n        if self.select_with_key_down(window, keycode, text, modifiers):\n            return True\n        return False\n\n    def keyboard_on_key_up(self, window, keycode):\n        if super(SelectableGrid, self).keyboard_on_key_up(window, keycode):\n            return True\n        if self.select_with_key_up(window, keycode):\n            return True\n        return False\n\n    def goto_node(self, key, last_node, last_node_idx):\n        ''' This function is used to go to the node by typing the number\n        of the text of the button.\n        '''\n        node, idx = super(SelectableGrid, self).goto_node(key, last_node,\n                                                          last_node_idx)\n        if node != last_node:\n            return node, idx\n\n        items = list(enumerate(self.get_selectable_nodes()))\n        '''If self.nodes_order_reversed (the default due to using\n        self.children which is reversed), the index is counted from the\n        starts of the selectable nodes, like normal but the nodes are traversed\n        in the reverse order.\n        '''\n        # start searching after the last selected node\n        if not self.nodes_order_reversed:\n            items = items[last_node_idx + 1:] + items[:last_node_idx + 1]\n        else:\n            items = items[:last_node_idx][::-1] + items[last_node_idx:][::-1]\n\n        for i, child in items:\n            if child.text.startswith(key):\n                return child, i\n        return node, idx\n\n    def select_node(self, node):\n        node.background_color = (1, 0, 0, 1)\n        return super(SelectableGrid, self).select_node(node)\n\n    def deselect_node(self, node):\n        node.background_color = (1, 1, 1, 1)\n        super(SelectableGrid, self).deselect_node(node)\n\n    def do_touch(self, instance, touch):\n        if ('button' in touch.profile and touch.button in\n                ('scrollup', 'scrolldown', 'scrollleft', 'scrollright')) or\\\n                instance.collide_point(*touch.pos):\n            self.select_with_touch(instance, touch)\n        else:\n            return False\n        return True\n\n\nroot = SelectableGrid(cols=5, up_count=5, multiselect=True, scroll_count=1)\nfor i in range(40):\n    c = Button(text=str(i))\n    c.bind(on_touch_down=root.do_touch)\n    root.add_widget(c)\n\n\nrunTouchApp(root)\n",
  "import argparse\nimport sys\nimport numpy as np\nimport matplotlib\n\nfrom scipy.sparse import spdiags, bmat\nfrom scipy.sparse.linalg import spsolve\nfrom scipy.sparse import csr_matrix,hstack,vstack,spdiags,bmat\nfrom mumps import DMumpsContext\n\nimport matplotlib.pyplot as plt\nfrom fealpy.functionspace import LagrangeFiniteElementSpace\nfrom fealpy.boundarycondition import DirichletBC \nfrom fealpy.timeintegratoralg import UniformTimeLine\n## Stokes model\nfrom navier_stokes_mold_2d import Poisuille as PDE\n\nfrom fealpy.mesh import TriangleMesh\nfrom fealpy.functionspace import LagrangeFESpace\n\nfrom fealpy.fem import ScalarDiffusionIntegrator, VectorMassIntegrator\nfrom fealpy.fem import VectorDiffusionIntegrator\nfrom fealpy.fem import VectorViscousWorkIntegrator, PressWorkIntegrator\nfrom fealpy.fem import ScalarConvectionIntegrator\nfrom fealpy.fem import BilinearForm, MixedBilinearForm\nfrom fealpy.fem import LinearForm\nfrom fealpy.fem import VectorSourceIntegrator, ScalarSourceIntegrator\nfrom fealpy.fem import DirichletBC\n\n# 参数设置\nparser = argparse.ArgumentParser(description=\n        \"\"\"\n        有限元方法求解NS方程\n        \"\"\")\n\nparser.add_argument('--udegree',\n        default=2, type=int,\n        help='运动有限元空间的次数, 默认为 2 次.')\n\nparser.add_argument('--pdegree',\n        default=1, type=int,\n        help='压力有限元空间的次数, 默认为 1 次.')\n\nparser.add_argument('--nt',\n        default=100, type=int,\n        help='时间剖分段数，默认剖分 5000 段.')\n\nparser.add_argument('--T',\n        default=10, type=float,\n        help='演化终止时间, 默认为 5')\n\nparser.add_argument('--output',\n        default='./', type=str,\n        help='结果输出目录, 默认为 ./')\n\nparser.add_argument('--step',\n        default=10, type=int,\n        help='隔多少步输出一次')\n\nparser.add_argument('--method',\n        default='Netwon', type=str,\n        help='非线性化方法')\n\nargs = parser.parse_args()\nudegree = args.udegree\npdegree = args.pdegree\nnt = args.nt\nT = args.T\noutput = args.output\nstep = args.step\nmethod = args.method\nns = 8\n\nmu= 1\nrho = 1\nudim = 2\ndoforder = 'sdofs'\n\npde = PDE()\nmesh = TriangleMesh.from_unit_square(nx=ns, ny=ns)\nsmesh = TriangleMesh.from_unit_square(nx=ns, ny=ns)\ntmesh = UniformTimeLine(0,T,nt)\ndt = tmesh.dt\nuspace = LagrangeFiniteElementSpace(smesh,p=udegree)\npspace = LagrangeFiniteElementSpace(smesh,p=pdegree)\nnuspace = LagrangeFESpace(mesh,p=2,doforder=doforder)\nnpspace = LagrangeFESpace(mesh,p=1,doforder=doforder)\n\n\nu0 = uspace.function(dim=udim)\nu1 = uspace.function(dim=udim)\n\np1 = pspace.function()\n\nugdof = uspace.number_of_global_dofs()\npgdof = pspace.number_of_global_dofs()\n\nugdof = uspace.number_of_global_dofs()\npgdof = pspace.number_of_global_dofs()\ngdof = pgdof+2*ugdof\n\nVbform0 = BilinearForm(nuspace)\nVbform0.add_domain_integrator(ScalarDiffusionIntegrator())\nVbform0.assembly()\nA = Vbform0.get_matrix()\n\nVbform1 = MixedBilinearForm((npspace,), 2*(nuspace, ))\nVbform1.add_domain_integrator(PressWorkIntegrator()) #TODO: 命名\nVbform1.assembly()\nB = Vbform1.get_matrix()\nB1 = B[:B.shape[0]//2,:]\nB2 = B[B.shape[0]//2:,:]\n\nE = (1/dt)*uspace.mass_matrix()\n\nerrorMatrix = np.zeros((4,nt),dtype=np.float64)\n\nfor i in range(0,3):\n\n    # 下一个时间层t1\n    t1 = tmesh.next_time_level()\n    print(\"t1=\",t1)\n\n    Vbform2 = BilinearForm(nuspace)\n    Vbform2.add_domain_integrator(ScalarConvectionIntegrator(c=u0)) \n    Vbform2.assembly()\n    DD = Vbform2.get_matrix()\n    \n    D1,D2 = uspace.div_matrix(uspace)\n    D = D1 * np.broadcast_to(u0[...,0],D1.shape)+\\\n        D2 * np.broadcast_to(u0[...,1],D1.shape) \n    \n    print(\"asd\",np.abs(D1).sum())\n    print(\"asd\",np.abs(D).sum())\n    print(np.sum(np.abs(D-DD))) \n    M = bmat([[E+A+D,None,-B1],[None,E+A+D,-B2],[-B1.T,-B2.T,None]],format='csr')\n    ''' \n    if method == 'Netwon' :\n        A = bmat([[1/dt*M + mu*S+D1+D2+E1, E2, -C1],\\\n                [E3, 1/dt*M + mu*S +D1+D2+E4, -C2],\\\n                [C1.T, C2.T, None]], format='csr')\n    elif method == 'Ossen':\n        A = bmat([[1/dt*M + mu*S+D1+D2, None, -C1],\\\n                [None, 1/dt*M + mu*S +D1+D2, -C2],\\\n                [C1.T, C2.T, None]], format='csr')\n    elif method == 'Eular':\n        A = bmat([[1/dt*M + mu*S, None, -C1],\\\n                [None, 1/dt*M + mu*S, -C2],\\\n                [C1.T, C2.T, None]], format='csr')\n    '''\n    #右端项\n    F = uspace.source_vector(pde.source,dim=udim) + E@u0\n    FF = np.r_['0', F.T.flat, np.zeros(pgdof)]\n    '''\n    if method == 'Netwon' :\n        b = 1/dt*fb1 + fb2\n        b = np.hstack((b,[0]*pgdof))\n    elif method == 'Ossen':\n        b = 1/dt*fb1\n        b = np.hstack((b,[0]*pgdof))\n    elif method == 'Eular':\n        b =  1/dt*fb1 - fb2\n        b = np.hstack((b,[0]*pgdof))\n    '''\n    u_isBdDof = uspace.is_boundary_dof()\n    #p_isBdDof = np.zeros(pgdof,dtype=np.bool)\n    p_isBdDof = pspace.is_boundary_dof(threshold=pde.is_p_boundary)\n    \n    x = np.zeros(gdof,np.float64)\n    ipoint = uspace.interpolation_points()\n    uso = pde.u_dirichlet(ipoint)\n    x[0:ugdof][u_isBdDof] = uso[:,0][u_isBdDof]\n    x[ugdof:2*ugdof][u_isBdDof] = uso[u_isBdDof][:,1]\n    ipoint = pspace.interpolation_points()\n    pso = pde.p_dirichlet(ipoint)\n    x[-pgdof:][p_isBdDof] = pso[p_isBdDof]\n\n    isBdDof = np.hstack([u_isBdDof, u_isBdDof, p_isBdDof])\n    \n    FF -= M@x\n    bdIdx = np.zeros(gdof, dtype=np.int_)\n    bdIdx[isBdDof] = 1\n    Tbd = spdiags(bdIdx, 0, gdof, gdof)\n    T = spdiags(1-bdIdx, 0, gdof, gdof)\n    M = T@M@T + Tbd\n    FF[isBdDof] = x[isBdDof]\n\n    x[:] = spsolve(M, FF)\n    u1[:, 0] = x[:ugdof]\n    u1[:, 1] = x[ugdof:2*ugdof]\n    p1[:] = x[2*ugdof:]\n    \n    uc1 = pde.velocity(smesh.node)\n    NN = smesh.number_of_nodes()\n    uc2 = u1[:NN]\n    up1 = pde.pressure(smesh.node)\n    up2 = p1[:NN]\n   \n    errorMatrix[0,i] = uspace.integralalg.L2_error(pde.velocity,u1)\n    errorMatrix[1,i] = pspace.integralalg.error(pde.pressure,p1)\n    errorMatrix[2,i] = np.abs(uc1-uc2).max()\n    errorMatrix[3,i] = np.abs(up1-up2).max()\n\n    u0[:] = u1 \n\n    tmesh.advance()\nprint(np.sum(np.abs(u1)))\n'''\nprint(\"uL2:\",errorMatrix[2,-1])\nprint(\"pL2:\",errorMatrix[1,-1])\nprint(\"umax:\",errorMatrix[2,-1])\nprint(\"pmax:\",errorMatrix[3,-1])\nfig1 = plt.figure()\nnode = smesh.node\nx = tuple(node[:,0])\ny = tuple(node[:,1])\nNN = smesh.number_of_nodes()\nu = u1[:NN]\nux = tuple(u[:,0])\nuy = tuple(u[:,1])\n\no = ux\nnorm = matplotlib.colors.Normalize()\ncm = matplotlib.cm.copper\nsm = matplotlib.cm.ScalarMappable(cmap=cm,norm=norm)\nsm.set_array([])\nplt.quiver(x,y,ux,uy,color=cm(norm(o)))\nplt.colorbar(sm)\nplt.show()\n'''\n",
  "#!/usr/bin/env python\nr\"\"\"\nIn this example we solve a FOCUS like Stage II coil optimisation problem: the\ngoal is to find coils that generate a specific target normal field on a given\nsurface.  In this particular case we consider a vacuum field, so the target is\njust zero.\n\nThe objective is given by\n\n    J = (1/2) \\int |B dot n|^2 ds\n        + LENGTH_WEIGHT * (sum CurveLength)\n        + DISTANCE_WEIGHT * MininumDistancePenalty(DISTANCE_THRESHOLD)\n        + CURVATURE_WEIGHT * CurvaturePenalty(CURVATURE_THRESHOLD)\n        + MSC_WEIGHT * MeanSquaredCurvaturePenalty(MSC_THRESHOLD)\n\nif any of the weights are increased, or the thresholds are tightened, the coils\nare more regular and better separated, but the target normal field may not be\nachieved as well. This example demonstrates the adjustment of weights and\npenalties via the use of the `Weight` class.\n\nThe target equilibrium is the QA configuration of arXiv:2108.03711.\n\"\"\"\n\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom scipy.optimize import minimize\n\nfrom simsopt.field import BiotSavart, Current, coils_via_symmetries\nfrom simsopt.geo import (SurfaceRZFourier, curves_to_vtk, create_equally_spaced_curves,\n                         CurveLength, CurveCurveDistance, MeanSquaredCurvature,\n                         LpCurveCurvature, CurveSurfaceDistance)\nfrom simsopt.objectives import Weight, SquaredFlux, QuadraticPenalty\nfrom simsopt.util import in_github_actions\n\n# Number of unique coil shapes, i.e. the number of coils per half field period:\n# (Since the configuration has nfp = 2, multiply by 4 to get the total number of coils.)\nncoils = 4\n\n# Major radius for the initial circular coils:\nR0 = 1.0\n\n# Minor radius for the initial circular coils:\nR1 = 0.5\n\n# Number of Fourier modes describing each Cartesian component of each coil:\norder = 5\n\n# Weight on the curve lengths in the objective function. We use the `Weight`\n# class here to later easily adjust the scalar value and rerun the optimization\n# without having to rebuild the objective.\nLENGTH_WEIGHT = Weight(1e-6)\n\n# Threshold and weight for the coil-to-coil distance penalty in the objective function:\nCC_THRESHOLD = 0.1\nCC_WEIGHT = 1000\n\n# Threshold and weight for the coil-to-surface distance penalty in the objective function:\nCS_THRESHOLD = 0.3\nCS_WEIGHT = 10\n\n# Threshold and weight for the curvature penalty in the objective function:\nCURVATURE_THRESHOLD = 5.\nCURVATURE_WEIGHT = 1e-6\n\n# Threshold and weight for the mean squared curvature penalty in the objective function:\nMSC_THRESHOLD = 5\nMSC_WEIGHT = 1e-6\n\n# Number of iterations to perform:\nMAXITER = 50 if in_github_actions else 400\n\n# File for the desired boundary magnetic surface:\nTEST_DIR = (Path(__file__).parent / \"..\" / \"..\" / \"tests\" / \"test_files\").resolve()\nfilename = TEST_DIR / 'input.LandremanPaul2021_QA'\n\n# Directory for output\nOUT_DIR = \"./output/\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\n#######################################################\n# End of input parameters.\n#######################################################\n\n# Initialize the boundary magnetic surface:\nnphi = 32\nntheta = 32\ns = SurfaceRZFourier.from_vmec_input(filename, range=\"half period\", nphi=nphi, ntheta=ntheta)\n\n# Create the initial coils:\nbase_curves = create_equally_spaced_curves(ncoils, s.nfp, stellsym=True, R0=R0, R1=R1, order=order)\nbase_currents = [Current(1e5) for i in range(ncoils)]\n# Since the target field is zero, one possible solution is just to set all\n# currents to 0. To avoid the minimizer finding that solution, we fix one\n# of the currents:\nbase_currents[0].fix_all()\n\ncoils = coils_via_symmetries(base_curves, base_currents, s.nfp, True)\nbs = BiotSavart(coils)\nbs.set_points(s.gamma().reshape((-1, 3)))\n\ncurves = [c.curve for c in coils]\ncurves_to_vtk(curves, OUT_DIR + \"curves_init\")\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(OUT_DIR + \"surf_init\", extra_data=pointData)\n\n# Define the individual terms objective function:\nJf = SquaredFlux(s, bs)\nJls = [CurveLength(c) for c in base_curves]\nJccdist = CurveCurveDistance(curves, CC_THRESHOLD, num_basecurves=ncoils)\nJcsdist = CurveSurfaceDistance(curves, s, CS_THRESHOLD)\nJcs = [LpCurveCurvature(c, 2, CURVATURE_THRESHOLD) for c in base_curves]\nJmscs = [MeanSquaredCurvature(c) for c in base_curves]\n\n\n# Form the total objective function. To do this, we can exploit the\n# fact that Optimizable objects with J() and dJ() functions can be\n# multiplied by scalars and added:\nJF = Jf \\\n    + LENGTH_WEIGHT * sum(Jls) \\\n    + CC_WEIGHT * Jccdist \\\n    + CS_WEIGHT * Jcsdist \\\n    + CURVATURE_WEIGHT * sum(Jcs) \\\n    + MSC_WEIGHT * sum(QuadraticPenalty(J, MSC_THRESHOLD, \"max\") for J in Jmscs)\n\n# We don't have a general interface in SIMSOPT for optimisation problems that\n# are not in least-squares form, so we write a little wrapper function that we\n# pass directly to scipy.optimize.minimize\n\n\ndef fun(dofs):\n    JF.x = dofs\n    J = JF.J()\n    grad = JF.dJ()\n    jf = Jf.J()\n    BdotN = np.mean(np.abs(np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)))\n    outstr = f\"J={J:.1e}, Jf={jf:.1e}, ⟨B·n⟩={BdotN:.1e}\"\n    cl_string = \", \".join([f\"{J.J():.1f}\" for J in Jls])\n    kap_string = \", \".join(f\"{np.max(c.kappa()):.1f}\" for c in base_curves)\n    msc_string = \", \".join(f\"{J.J():.1f}\" for J in Jmscs)\n    outstr += f\", Len=sum([{cl_string}])={sum(J.J() for J in Jls):.1f}, ϰ=[{kap_string}], ∫ϰ²/L=[{msc_string}]\"\n    outstr += f\", C-C-Sep={Jccdist.shortest_distance():.2f}, C-S-Sep={Jcsdist.shortest_distance():.2f}\"\n    outstr += f\", ║∇J║={np.linalg.norm(grad):.1e}\"\n    print(outstr)\n    return J, grad\n\n\nprint(\"\"\"\n################################################################################\n### Perform a Taylor test ######################################################\n################################################################################\n\"\"\")\nf = fun\ndofs = JF.x\nnp.random.seed(1)\nh = np.random.uniform(size=dofs.shape)\nJ0, dJ0 = f(dofs)\ndJh = sum(dJ0 * h)\nfor eps in [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]:\n    J1, _ = f(dofs + eps*h)\n    J2, _ = f(dofs - eps*h)\n    print(\"err\", (J1-J2)/(2*eps) - dJh)\n\nprint(\"\"\"\n################################################################################\n### Run the optimisation #######################################################\n################################################################################\n\"\"\")\nres = minimize(fun, dofs, jac=True, method='L-BFGS-B', options={'maxiter': MAXITER, 'maxcor': 300}, tol=1e-15)\ncurves_to_vtk(curves, OUT_DIR + f\"curves_opt_short\")\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(OUT_DIR + \"surf_opt_short\", extra_data=pointData)\n\n\n# We now use the result from the optimization as the initial guess for a\n# subsequent optimization with reduced penalty for the coil length. This will\n# result in slightly longer coils but smaller `B·n` on the surface.\ndofs = res.x\nLENGTH_WEIGHT *= 0.1\nres = minimize(fun, dofs, jac=True, method='L-BFGS-B', options={'maxiter': MAXITER, 'maxcor': 300}, tol=1e-15)\ncurves_to_vtk(curves, OUT_DIR + f\"curves_opt_long\")\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(OUT_DIR + \"surf_opt_long\", extra_data=pointData)\n\n# Save the optimized coil shapes and currents so they can be loaded into other scripts for analysis:\nbs.save(OUT_DIR + \"biot_savart_opt.json\")\n",
  "#!/usr/bin/env python3\n# encoding: utf-8\n\nfrom seedemu.layers import Base, Routing, Ebgp, Ibgp, Ospf, PeerRelationship, Dnssec\nfrom seedemu.services import WebService, DomainNameService, DomainNameCachingService\nfrom seedemu.services import CymruIpOriginService, ReverseDomainNameService, BgpLookingGlassService\nfrom seedemu.compiler import Docker, Graphviz\nfrom seedemu.hooks import ResolvConfHook\nfrom seedemu.core import Emulator, Service, Binding, Filter\nfrom seedemu.layers import Router\nfrom seedemu.raps import OpenVpnRemoteAccessProvider\nfrom seedemu.utilities import Makers\n\nfrom typing import List, Tuple, Dict\n\n\n###############################################################################\nemu     = Emulator()\nbase    = Base()\nrouting = Routing()\nebgp    = Ebgp()\nibgp    = Ibgp()\nospf    = Ospf()\nweb     = WebService()\novpn    = OpenVpnRemoteAccessProvider()\n\n\n###############################################################################\n\nix100 = base.createInternetExchange(100)\nix101 = base.createInternetExchange(101)\nix102 = base.createInternetExchange(102)\nix103 = base.createInternetExchange(103)\nix104 = base.createInternetExchange(104)\nix105 = base.createInternetExchange(105)\n\n# Customize names (for visualization purpose)\nix100.getPeeringLan().setDisplayName('NYC-100')\nix101.getPeeringLan().setDisplayName('San Jose-101')\nix102.getPeeringLan().setDisplayName('Chicago-102')\nix103.getPeeringLan().setDisplayName('Miami-103')\nix104.getPeeringLan().setDisplayName('Boston-104')\nix105.getPeeringLan().setDisplayName('Huston-105')\n\n\n###############################################################################\n# Create Transit Autonomous Systems \n\n## Tier 1 ASes\nMakers.makeTransitAs(base, 2, [100, 101, 102, 105], \n       [(100, 101), (101, 102), (100, 105)] \n)\n\nMakers.makeTransitAs(base, 3, [100, 103, 104, 105], \n       [(100, 103), (100, 105), (103, 105), (103, 104)]\n)\n\nMakers.makeTransitAs(base, 4, [100, 102, 104], \n       [(100, 104), (102, 104)]\n)\n\n## Tier 2 ASes\nMakers.makeTransitAs(base, 11, [102, 105], [(102, 105)])\nMakers.makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n###############################################################################\n# Create single-homed stub ASes. \"None\" means create a host only \n\nMakers.makeStubAs(emu, base, 150, 100, [web, None])\nMakers.makeStubAs(emu, base, 151, 100, [web, None])\n\nMakers.makeStubAs(emu, base, 152, 101, [None, None])\nMakers.makeStubAs(emu, base, 153, 101, [web, None, None])\n\nMakers.makeStubAs(emu, base, 154, 102, [None, web])\n\nMakers.makeStubAs(emu, base, 160, 103, [web, None])\nMakers.makeStubAs(emu, base, 161, 103, [web, None])\nMakers.makeStubAs(emu, base, 162, 103, [web, None])\n\nMakers.makeStubAs(emu, base, 163, 104, [web, None])\nMakers.makeStubAs(emu, base, 164, 104, [None, None])\n\nMakers.makeStubAs(emu, base, 170, 105, [web, None])\nMakers.makeStubAs(emu, base, 171, 105, [None])\n\n\n# Add a host with customized IP address to AS-154 \nas154 = base.getAutonomousSystem(154)\nas154.createHost('host_2').joinNetwork('net0', address = '10.154.0.129')\n\n\n# Create real-world AS.\n# AS11872 is the Syracuse University's autonomous system\n\nas11872 = base.createAutonomousSystem(11872)\nas11872.createRealWorldRouter('rw').joinNetwork('ix102', '10.102.0.118')\n\n# Allow outside computer to VPN into AS-152's network\nas152 = base.getAutonomousSystem(152)\nas152.getNetwork('net0').enableRemoteAccess(ovpn)\n\n\n###############################################################################\n# Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n# which means each AS will only export its customers and their own prefixes. \n# We will use this peering relationship to peer all the ASes in an IX.\n# None of them will provide transit service for others. \n\nebgp.addRsPeers(100, [2, 3, 4])\nebgp.addRsPeers(102, [2, 4])\nebgp.addRsPeers(104, [3, 4])\nebgp.addRsPeers(105, [2, 3])\n\n# To buy transit services from another autonomous system, \n# we will use private peering  \n\nebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\nebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\nebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\nebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\nebgp.addPrivatePeerings(102, [2, 4],  [11, 154], PeerRelationship.Provider)\nebgp.addPrivatePeerings(102, [11], [154, 11872], PeerRelationship.Provider)\n\nebgp.addPrivatePeerings(103, [3],  [160, 161, 162], PeerRelationship.Provider)\n\nebgp.addPrivatePeerings(104, [3, 4], [12], PeerRelationship.Provider)\nebgp.addPrivatePeerings(104, [4],  [163], PeerRelationship.Provider)\nebgp.addPrivatePeerings(104, [12], [164], PeerRelationship.Provider)\n\nebgp.addPrivatePeerings(105, [3],  [11, 170], PeerRelationship.Provider)\nebgp.addPrivatePeerings(105, [11], [171], PeerRelationship.Provider)\n\n\n###############################################################################\n\n# Add layers to the emulator\nemu.addLayer(base)\nemu.addLayer(routing)\nemu.addLayer(ebgp)\nemu.addLayer(ibgp)\nemu.addLayer(ospf)\nemu.addLayer(web)\n\n# Save it to a component file, so it can be used by other emulators\nemu.dump('base-component.bin')\n\n# Uncomment the following if you want to generate the final emulation files\nemu.render()\nemu.compile(Docker(), './output')\n\n",
  "#!/usr/bin/env python3\n#\n# This script is intended to illustrate the energy balance by \n# plotting ohmic heating and radiative losses as a function of temperature\n# at equilibrium ionization, similarly to figure 6 in Vallhagen et al JPP 2020. \n# This is achieved by setting a prescribed temperature profile at the values\n# one wants to plot for and run a dynamic simulation until equilibrium has been reached\n# (since equilibrium ionization settings does not seem to work yet).\n#\n# NOTE! Depending on the densities and temperatures one might have to adjust Tmax_restart_eq\n# to be long enough to really reach sufficient equilibration!\n#\n# ###################################################################\n\nimport numpy as np\nimport sys\nimport matplotlib.pyplot as plt\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nfrom DREAM.DREAMOutput import DREAMOutput\nfrom DREAM import runiface\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.CollisionHandler as Collisions\nimport DREAM.Settings.Equations.ElectricField as Efield\nimport DREAM.Settings.Equations.RunawayElectrons as RE\nimport DREAM.Settings.Equations.HotElectronDistribution as FHot\nimport DREAM.Settings.Equations.ColdElectronTemperature as T_cold\n\n\nfrom DREAM.Settings.Equations.ElectricField import ElectricField\nfrom DREAM.Settings.Equations.ColdElectronTemperature import ColdElectronTemperature\n\nds = DREAMSettings()\n\n\n\n# set collision settings\nds.collisions.collfreq_mode = Collisions.COLLFREQ_MODE_FULL\nds.collisions.collfreq_type = Collisions.COLLFREQ_TYPE_PARTIALLY_SCREENED\n#ds.collisions.bremsstrahlung_mode = Collisions.BREMSSTRAHLUNG_MODE_NEGLECT\nds.collisions.bremsstrahlung_mode = Collisions.BREMSSTRAHLUNG_MODE_STOPPING_POWER\n#ds.collisions.lnlambda = Collisions.LNLAMBDA_CONSTANT\nds.collisions.lnlambda = Collisions.LNLAMBDA_ENERGY_DEPENDENT\nds.collisions.pstar_mode = Collisions.PSTAR_MODE_COLLISIONAL\n\n# ds.eqsys.n_re.setEceff(Eceff=RE.COLLQTY_ECEFF_MODE_SIMPLE)\n\n#############################\n# Set simulation parameters #\n#############################\n\n#n_D = 41e20 # deuterium density\n#n_Z = 0.08e20 # Impurity density\n\nn_D = 1e20 # deuterium density\nn_Z = 0.1e20 # Impurity density\n\nJ=1.69e6 # Current density (For caculation of ohmic heating)\n\nB0 = 5.3            # magnetic field strength in Tesla\n\nTmax_init = 1e-11   # simulation time in seconds\nNt_init = 2         # number of time steps\n\nTmax_restart_ioniz = 2e-6\nNt_restart_ioniz = 500\n\nTmax_restart_eq = 30e-3\nNt_restart_eq = 1000\n\nTmax_restart_rad=1e-11\nNt_restart_rad=2\n\nNr = 151             # number of radial grid points\ntimes  = [0]        # times at which parameters are given\nradius = [0, 2]     # span of the radial grid\nradialgrid = np.linspace(radius[0],radius[-1],Nr)\nradius_wall = 2.15  # location of the wall \n\nE_initial = 0.001 # initial electric field in V/m (arbitrary value, does not affect the purpose of this script)\nE_wall = 0.0001        # boundary electric field in V/m\n# NOTE: it does not work to have self-consistent E-field with prescribed BC with E_wall=0, \n# since that leads to Psi_wall=0 constantly, which does not work when you have a relative tolerance\n\nT_initial = np.logspace(np.log10(0.7),np.log10(2e3),Nr)    # initial temperature in eV\n\n# Set up radial grid\nds.radialgrid.setB0(B0)\nds.radialgrid.setMinorRadius(radius[-1])\nds.radialgrid.setNr(Nr)\nds.radialgrid.setWallRadius(radius_wall)\n\n# Set time stepper\nds.timestep.setTmax(Tmax_init)\nds.timestep.setNt(Nt_init)\n\n# Set ions\nZ0=1\nZ=10\n\n# If one wants to start from another initial ionization than fully ionized deuterium and neutral impurities\n# Depending on the temperature range of interest, this can give a faster equilibration\n\"\"\"\nn_D_tmp=np.zeros(2)\nn_D_tmp[0]=0*n_D\nn_D_tmp[1]=1*n_D\nn_D_tmp=n_D_tmp.reshape(-1,1)*np.ones((1,len(radius)))\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_DYNAMIC, n=n_D_tmp,r=np.array(radius))\n\nn_Z_tmp=np.zeros(Z+1)\nn_Z_tmp[Z0]=n_Z\nn_Z_tmp=n_Z_tmp.reshape(-1,1)*np.ones((1,len(radius)))\nds.eqsys.n_i.addIon(name='Ne', Z=Z, iontype=Ions.IONS_DYNAMIC, n=n_Z_tmp,r=np.array(radius))\n\"\"\"\n\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_DYNAMIC_FULLY_IONIZED, n=n_D, opacity_mode=Ions.ION_OPACITY_MODE_GROUND_STATE_OPAQUE)\nds.eqsys.n_i.addIon(name='Ne', Z=Z, iontype=Ions.IONS_DYNAMIC_NEUTRAL, n=n_Z)\n\n# Since this script is intended to illustrate the energy balance at equilibrium ionization,\n# it would be preferable to use these settings but that does not seem to work yet.\n\"\"\"\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_EQUILIBRIUM, n=n_D)\nds.eqsys.n_i.addIon(name='Ne', Z=Z, iontype=Ions.IONS_EQUILIBRIUM, n=n_Z)\n\"\"\"\n\ntemperature = T_initial * np.ones((len(times), len(radialgrid)))\nds.eqsys.T_cold.setPrescribedData(temperature=temperature, times=times, radius=radialgrid)\n\n# Set E_field \nefield = E_initial*np.ones((len(times), len(radius)))\nds.eqsys.E_field.setPrescribedData(efield=efield, times=times, radius=radius)\nds.eqsys.E_field.setBoundaryCondition()\n\n# Disable runaway and hot-tail grid\nds.runawaygrid.setEnabled(False)\nds.hottailgrid.setEnabled(False)\n\n# Use the nonlinear solver\nds.solver.setType(Solver.NONLINEAR)\nds.solver.setLinearSolver(linsolv=Solver.LINEAR_SOLVER_LU)\n\n\nds.other.include('fluid')\n\n# Save settings to HDF5 file\nds.save('init_settings.h5')\nruniface(ds, 'output_init.h5', quiet=False)\n\n#### Ionization #############\nds2 = DREAMSettings(ds)\n\nds2.timestep.setTmax(Tmax_restart_ioniz)\nds2.timestep.setNt(Nt_restart_ioniz)\n\nds2.save('ioniz_restart_settings.h5')\n\nruniface(ds2, 'output_restart_ioniz.h5', quiet=False)\n\n#### Equilibration ############\nds3 = DREAMSettings(ds2)\n\nds3.timestep.setTmax(Tmax_restart_eq)\nds3.timestep.setNt(Nt_restart_eq)\n\nds3.save('eq_restart_settings.h5')\n\nruniface(ds3, 'output_restart_eq.h5', quiet=False)\n\n#### Radiation ################\nds4 = DREAMSettings(ds3)\n\nds4.eqsys.T_cold.setType(ttype=T_cold.TYPE_SELFCONSISTENT)\n\nds4.timestep.setTmax(Tmax_restart_rad)\nds4.timestep.setNt(Nt_restart_rad)\n\nds4.save('rad_restart_settings.h5')\n\nruniface(ds4, 'output_restart_rad.h5', quiet=False)\n\n################ Plot #################\ndo=DREAMOutput(ds4.output.filename)\nsigma=do.other.fluid.conductivity[0,:]\nrad=do.other.fluid.Tcold_radiation[0,:]\nT=do.eqsys.T_cold[0,:]\nplt.loglog(T,J**2/sigma/1e6)\nplt.loglog(T,rad/1e6)\nplt.show()\n",
  "'''User-implemented OSMAPOSL reconstruction\n\nUsage:\n  user_osmaposl [--help | options]\n\nOptions:\n  -f <file>, --file=<file>    raw data file\n                              [default: my_forward_projection.hs]\n  -p <path>, --path=<path>    path to data files, defaults to data/examples/PET\n                              subfolder of SIRF root folder\n  -s <subs>, --subs=<subs>    number of subsets [default: 12]\n  -i <sit>, --subiter=<sit>   number of sub-iterations [default: 2]\n  -e <engn>, --engine=<engn>  reconstruction engine [default: STIR]\n  --non-interactive           do not show plots\n'''\n\n## SyneRBI Synergistic Image Reconstruction Framework (SIRF)\n## Copyright 2015 - 2019 Rutherford Appleton Laboratory STFC\n## Copyright 2015 - 2017 University College London.\n##\n## This is software developed for the Collaborative Computational\n## Project in Synergistic Reconstruction for Biomedical Imaging (formerly CCP PETMR)\n## (http://www.ccpsynerbi.ac.uk/).\n##\n## Licensed under the Apache License, Version 2.0 (the \"License\");\n##   you may not use this file except in compliance with the License.\n##   You may obtain a copy of the License at\n##       http://www.apache.org/licenses/LICENSE-2.0\n##   Unless required by applicable law or agreed to in writing, software\n##   distributed under the License is distributed on an \"AS IS\" BASIS,\n##   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n##   See the License for the specific language governing permissions and\n##   limitations under the License.\n\n__version__ = '0.1.0'\nfrom docopt import docopt\nargs = docopt(__doc__, version=__version__)\n\nfrom sirf.Utilities import show_2D_array\n\n# import engine module\nexec('from sirf.' + args['--engine'] + ' import *')\n\n\n# process command-line options\nnum_subsets = int(args['--subs'])\nnum_subiterations = int(args['--subiter'])\ndata_file = args['--file']\ndata_path = args['--path']\nif data_path is None:\n    data_path = examples_data_path('PET')\nraw_data_file = existing_filepath(data_path, data_file)\nshow_plot = not args['--non-interactive']\n\n\n# user implementation of Ordered Subset Maximum A Posteriori One Step Late\n# reconstruction algorithm\ndef my_osmaposl(image, obj_fun, prior, filter, num_subsets, num_subiterations):\n\n    for sub_iter in range(1, num_subiterations + 1):\n        print('\\n------------- Subiteration %d' % sub_iter) \n\n        # select subset\n        subset = (sub_iter - 1) % num_subsets\n\n        # get sensitivity as ImageData\n        sens_image = obj_fun.get_subset_sensitivity(subset)\n\n        # get backprojection of the ratio of measured to estimated\n        # acquisition data)\n        grad_image = obj_fun.get_backprojection_of_acquisition_ratio\\\n                     (image, subset)\n\n        # get gradient of prior as ImageData\n        prior_grad_image = prior.get_gradient(image)\n\n        # update image data\n        denom = sens_image + prior_grad_image/num_subsets\n        update = grad_image/denom\n        image = image*update\n\n        # apply filter\n        filter.apply(image)\n\n    return image\n\n\ndef main():\n\n    # output goes to files\n    msg_red = MessageRedirector('info.txt', 'warn.txt', 'errr.txt')\n\n    # create acquisition model\n    acq_model = AcquisitionModelUsingRayTracingMatrix()\n\n    # PET acquisition data to be read from the file specified by --file option\n    print('raw data: %s' % raw_data_file)\n    acq_data = AcquisitionData(raw_data_file)\n\n    # create filter that zeroes the image outside a cylinder of the same\n    # diameter as the image xy-section size\n    filter = TruncateToCylinderProcessor()\n\n    # create initial image estimate\n    image_size = (31, 111, 111)\n    voxel_size = (3.375, 3, 3) # voxel sizes are in mm\n    image = ImageData()\n    image.initialise(image_size, voxel_size)\n    image.fill(1.0)\n\n    # create prior\n    prior = QuadraticPrior()\n    prior.set_penalisation_factor(0.5)\n    prior.set_up(image)\n\n    # create objective function\n    obj_fun = make_Poisson_loglikelihood(acq_data)\n    obj_fun.set_acquisition_model(acq_model)\n    obj_fun.set_num_subsets(num_subsets)\n    obj_fun.set_up(image)\n\n    # reconstruct using your own SIRF-based implementation of OSMAPOSL\n    image = my_osmaposl \\\n        (image, obj_fun, prior, filter, num_subsets, num_subiterations)\n\n    if show_plot:\n        # show reconstructed image at z = 20\n        image_array = image.as_array()\n        show_2D_array('Reconstructed image at z = 20', image_array[20,:,:])\n\n#    image.write('my_image.hv')\n\n\n# if anything goes wrong, an exception will be thrown \n# (cf. Error Handling section in the spec)\ntry:\n    main()\n    print('\\n=== done with %s' % __file__)\n\nexcept error as err:\n    # display error information\n    print('%s' % err.value)\n",
  "\"\"\"\n.. _planets_example:\n\n3D Earth and Celestial Bodies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPlot the solar system in PyVista.\n\nThis example is inspired by `planet3D-MATLAB\n<https://github.com/tamaskis/planet3D-MATLAB>`_.\n\n.. note::\n   The purpose of this example is to demonstrate plotting celestial bodies and\n   may lack astronomical precision. There may be inaccuracies in the\n   representation, so please take care when reusing or repurposing this\n   example.\n\n   Please take a look at libraries like `astropy <https://www.astropy.org/>`_\n   if you wish to use Python for astronomical calculations.\n\n\"\"\"\nimport pyvista\nfrom pyvista import examples\n\n###############################################################################\n# Plot the Solar System with Stars in the Background\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# This section relies on calculations in `Visualizing Celestial Bodies in 3D\n# <https://tamaskis.github.io/files/Visualizing_Celestial_Bodies_in_3D.pdf>`_.\n\n\n# Light of the Sun.\nlight = pyvista.Light()\nlight.set_direction_angle(30, -20)\n\n# Load planets\nmercury = examples.planets.load_mercury(radius=2439.0)\nmercury_texture = examples.planets.download_mercury_surface(texture=True)\nvenus = examples.planets.load_venus(radius=6052.0)\nvenus_texture = examples.planets.download_venus_surface(texture=True)\nearth = examples.planets.load_earth(radius=6378.1)\nearth_texture = examples.load_globe_texture()\nmars = examples.planets.load_mars(radius=3397.2)\nmars_texture = examples.planets.download_mars_surface(texture=True)\njupiter = examples.planets.load_jupiter(radius=71492.0)\njupiter_texture = examples.planets.download_jupiter_surface(texture=True)\nsaturn = examples.planets.load_saturn(radius=60268.0)\nsaturn_texture = examples.planets.download_saturn_surface(texture=True)\n# Saturn's rings range from 7000.0 km to 80000.0 km from the surface of the planet\ninner = 60268.0 + 7000.0\nouter = 60268.0 + 80000.0\nsaturn_rings = examples.planets.load_saturn_rings(inner=inner, outer=outer, c_res=50)\nsaturn_rings_texture = examples.planets.download_saturn_rings(texture=True)\nuranus = examples.planets.load_uranus(radius=25559.0)\nuranus_texture = examples.planets.download_uranus_surface(texture=True)\nneptune = examples.planets.load_neptune(radius=24764.0)\nneptune_texture = examples.planets.download_neptune_surface(texture=True)\npluto = examples.planets.load_pluto(radius=1151.0)\npluto_texture = examples.planets.download_pluto_surface(texture=True)\n\n# Move planets to a nice position for the plotter. These numbers are not\n# grounded in reality and are for demonstration purposes only.\nmercury.translate((0.0, 0.0, 0.0), inplace=True)\nvenus.translate((-15000.0, 0.0, 0.0), inplace=True)\nearth.translate((-30000.0, 0.0, 0.0), inplace=True)\nmars.translate((-45000.0, 0.0, 0.0), inplace=True)\njupiter.translate((-150000.0, 0.0, 0.0), inplace=True)\nsaturn.translate((-400000.0, 0.0, 0.0), inplace=True)\nsaturn_rings.translate((-400000.0, 0.0, 0.0), inplace=True)\nuranus.translate((-600000.0, 0.0, 0.0), inplace=True)\nneptune.translate((-700000.0, 0.0, 0.0), inplace=True)\n\n# Add planets to Plotter.\npl = pyvista.Plotter(lighting=\"none\")\ncubemap = examples.download_cubemap_space_16k()\n_ = pl.add_actor(cubemap.to_skybox())\npl.set_environment_texture(cubemap, True)\npl.add_light(light)\npl.add_mesh(mercury, texture=mercury_texture, smooth_shading=True)\npl.add_mesh(venus, texture=venus_texture, smooth_shading=True)\npl.add_mesh(earth, texture=earth_texture, smooth_shading=True)\npl.add_mesh(mars, texture=mars_texture, smooth_shading=True)\npl.add_mesh(jupiter, texture=jupiter_texture, smooth_shading=True)\npl.add_mesh(saturn, texture=saturn_texture, smooth_shading=True)\npl.add_mesh(saturn_rings, texture=saturn_rings_texture, smooth_shading=True)\npl.add_mesh(uranus, texture=uranus_texture, smooth_shading=True)\npl.add_mesh(neptune, texture=neptune_texture, smooth_shading=True)\npl.add_mesh(pluto, texture=pluto_texture, smooth_shading=True)\npl.show()\n\n\n###############################################################################\n# Plot the Planets and their Textures\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Each planet here is in a different subplot. The planet's textures are from\n# `Solar Textures <https://www.solarsystemscope.com/textures/>`_.\n\npl = pyvista.Plotter(shape=(3, 2))\npl.subplot(0, 0)\npl.add_text(\"Mercury\")\npl.add_mesh(examples.planets.download_mercury_surface(), rgb=True)\npl.subplot(0, 1)\npl.add_mesh(mercury, texture=mercury_texture)\npl.subplot(1, 0)\npl.add_text(\"Venus\")\npl.add_mesh(examples.planets.download_venus_surface(atmosphere=True), rgb=True)\npl.subplot(1, 1)\npl.add_mesh(venus, texture=venus_texture)\npl.subplot(2, 0)\npl.add_text(\"Mars\")\npl.add_mesh(examples.planets.download_mars_surface(), rgb=True)\npl.subplot(2, 1)\npl.add_mesh(mars, texture=mars_texture)\npl.show(cpos=\"xy\")\n\n\n###############################################################################\n# Plot the Atmosphere and Surface of Venus\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Here we plot Venus with and without its atmosphere.\n\nvenus = examples.planets.load_venus()\natmosphere_texture = examples.planets.download_venus_surface(atmosphere=True, texture=True)\nsurface_texture = examples.planets.download_venus_surface(atmosphere=False, texture=True)\n\npl = pyvista.Plotter(shape=(1, 2))\npl.subplot(0, 0)\npl.add_text(\"Venus Atmosphere\")\npl.add_mesh(venus, texture=atmosphere_texture, smooth_shading=True)\npl.subplot(0, 1)\npl.add_text(\"Venus Surface\")\npl.add_mesh(venus, texture=surface_texture, smooth_shading=True)\npl.link_views()\npl.show(cpos=\"xy\")\n",
  "\"\"\"Train a transformer tagging model, using Huggingface's Transformers.\"\"\"\n# pip install thinc ml_datasets typer tqdm transformers torch\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple, Callable\nimport torch\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModel\nimport thinc\nfrom thinc.api import PyTorchWrapper, Softmax, chain, with_array, Model, Config\nfrom thinc.api import torch2xp, xp2torch, SequenceCategoricalCrossentropy\nfrom thinc.api import prefer_gpu, use_pytorch_for_gpu_memory\nfrom thinc.types import Floats2d, ArgsKwargs\nimport ml_datasets\nimport tqdm\nimport typer\n\n\nCONFIG = \"\"\"\n[model]\n@layers = \"TransformersTagger.v1\"\nstarter = \"bert-base-multilingual-cased\"\n\n[optimizer]\n@optimizers = \"RAdam.v1\"\nweight_decay = 1e-8\n\n[optimizer.learn_rate]\n@schedules = \"warmup_linear.v1\"\ninitial_rate = 0.01\nwarmup_steps = 3000\ntotal_steps = 6000\n\n[training]\nbatch_size = 128\nwords_per_subbatch = 2000\nn_epoch = 10\n\"\"\"\n\n\ndef main(path: Optional[Path] = None, out_dir: Optional[Path] = None):\n    if prefer_gpu():\n        print(\"Using gpu!\")\n        use_pytorch_for_gpu_memory()\n    # You can edit the CONFIG string within the file, or copy it out to\n    # a separate file and pass in the path.\n    if path is None:\n        config = Config().from_str(CONFIG)\n    else:\n        config = Config().from_disk(path)\n    # resolve constructs objects whenever you have blocks with an @ key.\n    # In the optimizer block we write @optimizers = \"Adam.v1\". This tells Thinc\n    # to use registry.optimizers to fetch the \"Adam.v1\" function. You can\n    # register your own functions as well and build up trees of objects.\n    C = thinc.registry.resolve(config)\n    words_per_subbatch = C[\"training\"][\"words_per_subbatch\"]\n    n_epoch = C[\"training\"][\"n_epoch\"]\n    batch_size = C[\"training\"][\"batch_size\"]\n    model = C[\"model\"]\n    optimizer = C[\"optimizer\"]\n    calculate_loss = SequenceCategoricalCrossentropy()\n\n    (train_X, train_Y), (dev_X, dev_Y) = ml_datasets.ud_ancora_pos_tags()\n    # Convert the outputs to cupy (if we're using that)\n    train_Y = list(map(model.ops.asarray, train_Y))\n    dev_Y = list(map(model.ops.asarray, dev_Y))\n    # Pass in a small batch of data, to fill in missing shapes\n    model.initialize(X=train_X[:5], Y=train_Y[:5])\n    for epoch in range(n_epoch):\n        # Transformers often learn best with large batch sizes -- larger than\n        # fits in GPU memory. But you don't have to backprop the whole batch\n        # at once. Here we consider the \"logical\" batch size (number of examples\n        # per update) separately from the physical batch size.\n        batches = model.ops.multibatch(batch_size, train_X, train_Y, shuffle=True)\n        for outer_batch in tqdm.tqdm(batches, leave=False):\n            # For the physical batch size, what we care about is the number\n            # of words (considering padding too). We also want to sort by\n            # length, for efficiency.\n            for batch in minibatch_by_words(outer_batch, words_per_subbatch):\n                inputs, truths = zip(*batch)\n                guesses, backprop = model(inputs, is_train=True)\n                backprop(calculate_loss.get_grad(guesses, truths))\n            # At the end of the batch, we call the optimizer with the accumulated\n            # gradients, and advance the learning rate schedules.\n            model.finish_update(optimizer)\n            optimizer.step_schedules()\n        # You might want to evaluate more often than once per epoch; that's up\n        # to you.\n        score = evaluate_sequences(model, dev_X, dev_Y, 128)\n        print(epoch, f\"{score:.3f}\")\n        if out_dir:\n            model.to_disk(out_dir / f\"{epoch}.bin\")\n\n\n@dataclass\nclass TokensPlus:\n    \"\"\"Dataclass to hold the output of the Huggingface 'batch_encode_plus' method.\"\"\"\n\n    input_ids: torch.Tensor\n    token_type_ids: torch.Tensor\n    attention_mask: torch.Tensor\n    input_len: List[int]\n    overflowing_tokens: Optional[torch.Tensor] = None\n    num_truncated_tokens: Optional[torch.Tensor] = None\n    special_tokens_mask: Optional[torch.Tensor] = None\n\n\n@thinc.registry.layers(\"TransformersTagger.v1\")\ndef TransformersTagger(\n    starter: str, n_tags: int = 17\n) -> Model[List[List[str]], List[Floats2d]]:\n    return chain(\n        TransformersTokenizer(starter),\n        Transformer(starter),\n        with_array(Softmax(nO=n_tags)),\n    )\n\n\n@thinc.registry.layers(\"transformers_tokenizer.v1\")\ndef TransformersTokenizer(name: str) -> Model[List[List[str]], TokensPlus]:\n    def forward(\n        model, texts: List[List[str]], is_train: bool\n    ) -> Tuple[TokensPlus, Callable]:\n        tokenizer = model.attrs[\"tokenizer\"]\n        token_data = tokenizer.batch_encode_plus(\n            [(text, None) for text in texts],\n            add_special_tokens=True,\n            return_token_type_ids=True,\n            return_attention_masks=True,\n            return_input_lengths=True,\n            return_tensors=\"pt\",\n        )\n        return TokensPlus(**token_data), lambda d_tokens: []\n\n    return Model(\n        \"tokenizer\",\n        forward,\n        attrs={\"tokenizer\": AutoTokenizer.from_pretrained(name)},\n    )\n\n\n@thinc.registry.layers(\"transformers_model.v1\")\ndef Transformer(name: str) -> Model[TokensPlus, List[Floats2d]]:\n    return PyTorchWrapper(\n        AutoModel.from_pretrained(name),\n        convert_inputs=convert_transformer_inputs,\n        convert_outputs=convert_transformer_outputs,\n    )\n\n\ndef convert_transformer_inputs(model, tokens: TokensPlus, is_train):\n    kwargs = {\n        \"input_ids\": tokens.input_ids,\n        \"attention_mask\": tokens.attention_mask,\n        \"token_type_ids\": tokens.token_type_ids,\n    }\n    return ArgsKwargs(args=(), kwargs=kwargs), lambda dX: []\n\n\ndef convert_transformer_outputs(model, inputs_outputs, is_train):\n    layer_inputs, torch_outputs = inputs_outputs\n    torch_tokvecs: torch.Tensor = torch_outputs[0]\n    # Free the memory as soon as we can\n    torch_outputs = None\n    lengths = list(layer_inputs.input_len)\n    tokvecs: List[Floats2d] = model.ops.unpad(torch2xp(torch_tokvecs), lengths)\n    # Remove the BOS and EOS markers.\n    tokvecs = [arr[1:-1] for arr in tokvecs]\n\n    def backprop(d_tokvecs: List[Floats2d]) -> ArgsKwargs:\n        # Restore entries for bos and eos markers.\n        shim = model.shims[0]\n        row = model.ops.alloc2f(1, d_tokvecs[0].shape[1])\n        d_tokvecs = [model.ops.xp.vstack((row, arr, row)) for arr in d_tokvecs]\n        return ArgsKwargs(\n            args=(torch_tokvecs,),\n            kwargs={\n                \"grad_tensors\": xp2torch(model.ops.pad(d_tokvecs, device=shim.device))\n            },\n        )\n\n    return tokvecs, backprop\n\n\ndef evaluate_sequences(\n    model, Xs: List[Floats2d], Ys: List[Floats2d], batch_size: int\n) -> float:\n    correct = 0.0\n    total = 0.0\n    for X, Y in model.ops.multibatch(batch_size, Xs, Ys):\n        Yh = model.predict(X)\n        for yh, y in zip(Yh, Y):\n            correct += (y.argmax(axis=1) == yh.argmax(axis=1)).sum()\n            total += y.shape[0]\n    return float(correct / total)\n\n\ndef minibatch_by_words(pairs, max_words):\n    \"\"\"Group pairs of sequences into minibatches under max_words in size,\n    considering padding. The size of a padded batch is the length of its\n    longest sequence multiplied by the number of elements in the batch.\n    \"\"\"\n    pairs = list(zip(*pairs))\n    pairs.sort(key=lambda xy: len(xy[0]), reverse=True)\n    batch = []\n    for X, Y in pairs:\n        batch.append((X, Y))\n        n_words = max(len(xy[0]) for xy in batch) * len(batch)\n        if n_words >= max_words:\n            # We went *over* the cap, so don't emit the batch with this\n            # example -- move that example into the next one.\n            yield batch[:-1]\n            batch = [(X, Y)]\n    if batch:\n        yield batch\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n",
  "\"\"\"\nHFSS: flex cable CPWG\n---------------------\nThis example shows how you can use PyAEDT to create a flex cable CPWG (coplanar waveguide with ground).\n\"\"\"\n\n###############################################################################\n# Perform required imports\n# ~~~~~~~~~~~~~~~~~~~~~~~~\n# Perform required imports.\n\nimport os\nfrom math import radians, sin, cos, sqrt\nimport pyaedt\n\n###############################################################################\n# Set non-graphical mode\n# ~~~~~~~~~~~~~~~~~~~~~~\n# Set non-graphical mode. \n# You can set ``non_graphical`` either to ``True`` or ``False``.\n\nnon_graphical = False\n\n###############################################################################\n# Launch AEDT\n# ~~~~~~~~~~~\n# Launch AEDT 2023 R2 in graphical mode.\n\nhfss = pyaedt.Hfss(specified_version=\"2023.2\",\n                   solution_type=\"DrivenTerminal\",\n                   new_desktop_session=True,\n                   non_graphical=non_graphical)\nhfss.change_material_override(True)\nhfss.change_automatically_use_causal_materials(True)\nhfss.create_open_region(\"100GHz\")\nhfss.modeler.model_units = \"mil\"\nhfss.mesh.assign_initial_mesh_from_slider(applycurvilinear=True)\n\n###############################################################################\n# Create variables\n# ~~~~~~~~~~~~~~~~\n# Create input variables for creating the flex cable CPWG.\n\ntotal_length = 300\ntheta = 120\nr = 100\nwidth = 3\nheight = 0.1\nspacing = 1.53\ngnd_width = 10\ngnd_thickness = 2\n\nxt = (total_length - r * radians(theta)) / 2\n\n\n###############################################################################\n# Create bend\n# ~~~~~~~~~~~\n# Create the bend. The ``create_bending`` method creates a list of points for\n# the bend based on the curvature radius and extension.\n\ndef create_bending(radius, extension=0):\n    position_list = [(-xt, 0, -radius), (0, 0, -radius)]\n\n    for i in [radians(i) for i in range(theta)] + [radians(theta + 0.000000001)]:\n        position_list.append((radius * sin(i), 0, -radius * cos(i)))\n\n    x1, y1, z1 = position_list[-1]\n    x0, y0, z0 = position_list[-2]\n\n    scale = (xt + extension) / sqrt((x1 - x0) ** 2 + (z1 - z0) ** 2)\n    x, y, z = (x1 - x0) * scale + x0, 0, (z1 - z0) * scale + z0\n\n    position_list[-1] = (x, y, z)\n    return position_list\n\n\n###############################################################################\n# Draw signal line\n# ~~~~~~~~~~~~~~~~\n# Draw a signal line to create a bent signal wire.\n\nposition_list = create_bending(r, 1)\nline = hfss.modeler.create_polyline(\n    position_list=position_list,\n    xsection_type=\"Rectangle\",\n    xsection_width=height,\n    xsection_height=width,\n    matname=\"copper\",\n)\n\n###############################################################################\n# Draw ground line\n# ~~~~~~~~~~~~~~~~\n# Draw a ground line to create two bent ground wires.\n\ngnd_r = [(x, spacing + width / 2 + gnd_width / 2, z) for x, y, z in position_list]\ngnd_l = [(x, -y, z) for x, y, z in gnd_r]\n\ngnd_objs = []\nfor gnd in [gnd_r, gnd_l]:\n    x = hfss.modeler.create_polyline(\n        position_list=gnd, xsection_type=\"Rectangle\", xsection_width=height, xsection_height=gnd_width, matname=\"copper\"\n    )\n    x.color = (255, 0, 0)\n    gnd_objs.append(x)\n\n###############################################################################\n# Draw dielectric\n# ~~~~~~~~~~~~~~~\n# Draw a dielectric to create a dielectric cable.\n\nposition_list = create_bending(r + (height + gnd_thickness) / 2)\n\nfr4 = hfss.modeler.create_polyline(\n    position_list=position_list,\n    xsection_type=\"Rectangle\",\n    xsection_width=gnd_thickness,\n    xsection_height=width + 2 * spacing + 2 * gnd_width,\n    matname=\"FR4_epoxy\",\n)\n\n###############################################################################\n# Create bottom metals\n# ~~~~~~~~~~~~~~~~~~~~\n# Create the bottom metals.\n\nposition_list = create_bending(r + height + gnd_thickness, 1)\n\nbot = hfss.modeler.create_polyline(\n    position_list=position_list,\n    xsection_type=\"Rectangle\",\n    xsection_width=height,\n    xsection_height=width + 2 * spacing + 2 * gnd_width,\n    matname=\"copper\",\n)\n\n###############################################################################\n# Create port interfaces\n# ~~~~~~~~~~~~~~~~~~~~~~\n# Create port interfaces (PEC enclosures).\n\nport_faces = []\nfor face, blockname in zip([fr4.top_face_z, fr4.bottom_face_x], [\"b1\", \"b2\"]):\n    xc, yc, zc = face.center\n    positions = [i.position for i in face.vertices]\n\n    port_sheet_list = [((x - xc) * 10 + xc, (y - yc) + yc, (z - zc) * 10 + zc) for x, y, z in positions]\n    s = hfss.modeler.create_polyline(port_sheet_list, close_surface=True, cover_surface=True)\n    center = [round(i, 6) for i in s.faces[0].center]\n\n    port_block = hfss.modeler.thicken_sheet(s.name, -5)\n    port_block.name = blockname\n    for f in port_block.faces:\n\n        if [round(i, 6) for i in f.center] == center:\n            port_faces.append(f)\n\n    port_block.material_name = \"PEC\"\n\n    for i in [line, bot] + gnd_objs:\n        i.subtract([port_block], True)\n\n    print(port_faces)\n\n###############################################################################\n# Create boundary condition\n# ~~~~~~~~~~~~~~~~~~~~~~~~~\n# Creates a Perfect E boundary condition.\n\nboundary = []\nfor face in [fr4.top_face_y, fr4.bottom_face_y]:\n    s = hfss.modeler.create_object_from_face(face)\n    boundary.append(s)\n    hfss.assign_perfecte_to_sheets(s)\n\n###############################################################################\n# Create ports\n# ~~~~~~~~~~~~\n# Creates ports.\n\nfor s, port_name in zip(port_faces, [\"1\", \"2\"]):\n    reference = [i.name for i in gnd_objs + boundary + [bot]] + [\"b1\", \"b2\"]\n\n    hfss.wave_port(s.id, name=port_name, reference=reference)\n\n###############################################################################\n# Create setup and sweep\n# ~~~~~~~~~~~~~~~~~~~~~~\n# Create the setup and sweep.\n\nsetup = hfss.create_setup(\"setup1\")\nsetup[\"Frequency\"] = \"2GHz\"\nsetup.props[\"MaximumPasses\"] = 10\nsetup.props[\"MinimumConvergedPasses\"] = 2\nhfss.create_linear_count_sweep(\n    setupname=\"setup1\",\n    unit=\"GHz\",\n    freqstart=1e-1,\n    freqstop=4,\n    num_of_freq_points=101,\n    sweepname=\"sweep1\",\n    save_fields=False,\n    sweep_type=\"Interpolating\",\n)\n\n###############################################################################\n# Plot model\n# ~~~~~~~~~~\n# Plot the model.\n\nmy_plot = hfss.plot(show=False, plot_air_objects=False)\nmy_plot.show_axes = False\nmy_plot.show_grid = False\nmy_plot.plot(\n    os.path.join(hfss.working_directory, \"Image.jpg\"),\n)\n###############################################################################\n# Analyze and release\n# ~~~~~~~~~~~~~~~~~~~~\n# Uncomment the ``hfss.analyze`` command if you want to analyze the\n# model and release AEDT.\n\nhfss.release_desktop()\n",
  "\"\"\"\n=========================================================\nComparing different clustering algorithms on toy datasets\n=========================================================\n\nThis example shows characteristics of different\nclustering algorithms on datasets that are \"interesting\"\nbut still in 2D. With the exception of the last dataset,\nthe parameters of each of these dataset-algorithm pairs\nhas been tuned to produce good clustering results. Some\nalgorithms are more sensitive to parameter values than\nothers.\n\nThe last dataset is an example of a 'null' situation for\nclustering: the data is homogeneous, and there is no good\nclustering. For this example, the null dataset uses the\nsame parameters as the dataset in the row above it, which\nrepresents a mismatch in the parameter values and the\ndata structure.\n\nWhile these examples give some intuition about the\nalgorithms, this intuition might not apply to very high\ndimensional data.\n\n\"\"\"\n\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 500\nseed = 30\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(9 * 2 + 3, 13))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    hdbscan = cluster.HDBSCAN(\n        min_samples=params[\"hdbscan_min_samples\"],\n        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n        allow_single_cluster=params[\"allow_single_cluster\"],\n    )\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        (\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" > 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()\n",
  "# -*- coding: utf-8 -*-\n\"\"\"\nThis example demonstrates the use of ColorBarItem, which displays a simple interactive color bar.\n\"\"\"\n## Add path to library (just for examples; you do not need this)\nimport initExample\n\nimport numpy as np\nfrom pyqtgraph.Qt import QtWidgets, mkQApp\nimport pyqtgraph as pg\n\nclass MainWindow(QtWidgets.QMainWindow):\n    \"\"\" example application main window \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(MainWindow, self).__init__(*args, **kwargs)\n        gr_wid = pg.GraphicsLayoutWidget(show=True)\n        self.setCentralWidget(gr_wid)\n        self.setWindowTitle('pyqtgraph example: Interactive color bar')\n        self.resize(800,700)\n        self.show()\n\n        ## Create image items\n        data = np.fromfunction(lambda i, j: (1+0.3*np.sin(i)) * (i)**2 + (j)**2, (100, 100))\n        noisy_data = data * (1 + 0.2 * np.random.random(data.shape) )\n        noisy_transposed = noisy_data.transpose()\n\n        #--- add non-interactive image with integrated color -----------------\n        i1 = pg.ImageItem(image=data)\n        p1 = gr_wid.addPlot(title=\"non-interactive\")\n        p1.addItem( i1 )\n        p1.setMouseEnabled( x=False, y=False)\n        p1.disableAutoRange()\n        p1.hideButtons()\n        p1.setRange(xRange=(0,100), yRange=(0,100), padding=0)\n        for key in ['left','right','top','bottom']:\n            p1.showAxis(key)\n            axis = p1.getAxis(key)\n            axis.setZValue(1)\n            if key in ['top', 'right']: \n                p1.getAxis(key).setStyle( showValues=False )\n\n        cmap = pg.colormap.get('CET-L9')\n        bar = pg.ColorBarItem(\n            interactive=False, values= (0, 30_000), cmap=cmap,\n            label='vertical fixed color bar'\n        )\n        bar.setImageItem( i1, insert_in=p1 )\n\n        #--- add interactive image with integrated horizontal color bar --------------\n        i2 = pg.ImageItem(image=noisy_data)\n        p2 = gr_wid.addPlot(1,0, 1,1, title=\"interactive\")\n        p2.addItem( i2, title='' )\n        # inserted color bar also works with labels on the right.\n        p2.showAxis('right')\n        p2.getAxis('left').setStyle( showValues=False )\n        p2.getAxis('bottom').setLabel('bottom axis label')\n        p2.getAxis('right').setLabel('right axis label')\n\n        cmap = pg.colormap.get('CET-L4')\n        bar = pg.ColorBarItem(\n            values = (0, 30_000),\n            cmap=cmap,\n            label='horizontal color bar',\n            limits = (0, None),\n            rounding=1000,\n            orientation = 'horizontal',\n            pen='#8888FF', hoverPen='#EEEEFF', hoverBrush='#EEEEFF80'\n        )\n        bar.setImageItem( i2, insert_in=p2 )\n\n        #--- multiple images adjusted by a separate color bar ------------------------\n        i3 = pg.ImageItem(image=noisy_data)\n        p3 = gr_wid.addPlot(0,1, 1,1, title=\"shared 1\")\n        p3.addItem( i3 )\n\n        i4 = pg.ImageItem(image=noisy_transposed)\n        p4 = gr_wid.addPlot(1,1, 1,1, title=\"shared 2\")\n        p4.addItem( i4 )\n\n        cmap = pg.colormap.get('CET-L8')\n        bar = pg.ColorBarItem(\n            # values = (-15_000, 15_000),\n            limits = (-30_000, 30_000), # start with full range...\n            rounding=1000,\n            width = 10,\n            cmap=cmap )\n        bar.setImageItem( [i3, i4] )\n        bar.setLevels( low=-5_000, high=15_000) # ... then adjust to retro sunset.\n\n        # manually adjust reserved space at top and bottom to align with plot\n        bar.getAxis('bottom').setHeight(21)\n        bar.getAxis('top').setHeight(31)\n        gr_wid.addItem(bar, 0,2, 2,1) # large bar spanning both rows\n\nmkQApp(\"ColorBarItem Example\")\nmain_window = MainWindow()\n\n## Start Qt event loop\nif __name__ == '__main__':\n    pg.exec()\n",
  "#\n#  Copyright 2019 The FATE Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nimport argparse\n\nfrom pipeline.backend.pipeline import PipeLine\nfrom pipeline.component import DataTransform\nfrom pipeline.component import Evaluation, DataStatistics, HeteroPearson\nfrom pipeline.component import HeteroLR, OneHotEncoder\nfrom pipeline.component import HeteroFeatureBinning\nfrom pipeline.component import HeteroFeatureSelection\nfrom pipeline.component import FeatureScale\nfrom pipeline.component import Intersection\nfrom pipeline.component import Reader\nfrom pipeline.interface import Data\nfrom pipeline.interface import Model\n\nfrom pipeline.utils.tools import load_job_config\n\n\ndef main(config=\"../../config.yaml\", namespace=\"\"):\n    # obtain config\n    if isinstance(config, str):\n        config = load_job_config(config)\n    parties = config.parties\n    guest = parties.guest[0]\n    host = parties.host[0]\n    arbiter = parties.arbiter[0]\n\n    guest_train_data = {\"name\": \"breast_hetero_guest\", \"namespace\": f\"experiment_sid{namespace}\"}\n    host_train_data = {\"name\": \"breast_hetero_host\", \"namespace\": f\"experiment_sid{namespace}\"}\n\n    pipeline = PipeLine().set_initiator(role='guest', party_id=guest).\\\n        set_roles(guest=guest, host=host, arbiter=arbiter)\n\n    reader_0 = Reader(name=\"reader_0\")\n    reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    data_transform_0 = DataTransform(name=\"data_transform_0\", with_match_id=True)\n    data_transform_0.get_party_instance(role='guest', party_id=guest).component_param(with_label=True)\n    data_transform_0.get_party_instance(role='host', party_id=host).component_param(with_label=False)\n\n    intersection_0 = Intersection(name=\"intersection_0\")\n    feature_scale_0 = FeatureScale(name='feature_scale_0', method=\"standard_scale\",\n                                   need_run=True)\n\n    binning_param = {\n        \"method\": \"quantile\",\n        \"compress_thres\": 10000,\n        \"head_size\": 10000,\n        \"error\": 0.001,\n        \"bin_num\": 10,\n        \"bin_indexes\": -1,\n        \"adjustment_factor\": 0.5,\n        \"local_only\": False,\n        \"need_run\": True,\n        \"transform_param\": {\n            \"transform_cols\": -1,\n            \"transform_type\": \"bin_num\"\n        }\n    }\n    hetero_feature_binning_0 = HeteroFeatureBinning(name='hetero_feature_binning_0',\n                                                    **binning_param)\n\n    statistic_0 = DataStatistics(name='statistic_0', statistics=[\"95%\"])\n    pearson_0 = HeteroPearson(name='pearson_0', column_indexes=-1)\n    onehot_0 = OneHotEncoder(name='onehot_0')\n    selection_param = {\n        \"name\": \"hetero_feature_selection_0\",\n        \"select_col_indexes\": -1,\n        \"select_names\": [],\n        \"filter_methods\": [\n            \"manually\",\n            \"unique_value\",\n            \"iv_filter\",\n            \"coefficient_of_variation_value_thres\",\n            \"outlier_cols\"\n        ],\n        \"manually_param\": {\n            \"filter_out_indexes\": [\n                0,\n                1,\n                2\n            ],\n            \"filter_out_names\": [\n                \"x3\"\n            ]\n        },\n        \"unique_param\": {\n            \"eps\": 1e-06\n        },\n        \"iv_param\": {\n            \"metrics\": [\"iv\", \"iv\", \"iv\"],\n            \"filter_type\": [\"threshold\", \"top_k\", \"top_percentile\"],\n            \"threshold\": [0.001, 100, 0.99]\n        },\n        \"variance_coe_param\": {\n            \"value_threshold\": 0.3\n        },\n        \"outlier_param\": {\n            \"percentile\": 0.95,\n            \"upper_threshold\": 2.0\n        }}\n    hetero_feature_selection_0 = HeteroFeatureSelection(**selection_param)\n\n    lr_param = {\n        \"name\": \"hetero_lr_0\",\n        \"penalty\": \"L2\",\n        \"optimizer\": \"rmsprop\",\n        \"tol\": 0.0001,\n        \"alpha\": 0.01,\n        \"max_iter\": 30,\n        \"early_stop\": \"diff\",\n        \"batch_size\": 320,\n        \"learning_rate\": 0.15,\n        \"init_param\": {\n            \"init_method\": \"zeros\"\n        },\n        \"sqn_param\": {\n            \"update_interval_L\": 3,\n            \"memory_M\": 5,\n            \"sample_size\": 5000,\n            \"random_seed\": None\n        },\n        \"cv_param\": {\n            \"n_splits\": 5,\n            \"shuffle\": False,\n            \"random_seed\": 103,\n            \"need_cv\": False\n        }\n    }\n\n    hetero_lr_0 = HeteroLR(**lr_param)\n    evaluation_0 = Evaluation(name='evaluation_0')\n\n    pipeline.add_component(reader_0)\n    pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))\n    pipeline.add_component(intersection_0, data=Data(data=data_transform_0.output.data))\n    pipeline.add_component(feature_scale_0, data=Data(data=intersection_0.output.data))\n    pipeline.add_component(hetero_feature_binning_0, data=Data(data=feature_scale_0.output.data))\n    pipeline.add_component(statistic_0, data=Data(data=feature_scale_0.output.data))\n    pipeline.add_component(pearson_0, data=Data(data=feature_scale_0.output.data))\n\n    pipeline.add_component(hetero_feature_selection_0, data=Data(data=hetero_feature_binning_0.output.data),\n                           model=Model(isometric_model=[hetero_feature_binning_0.output.model,\n                                                        statistic_0.output.model]))\n    pipeline.add_component(onehot_0, data=Data(data=hetero_feature_selection_0.output.data))\n\n    pipeline.add_component(hetero_lr_0, data=Data(train_data=onehot_0.output.data))\n    pipeline.add_component(evaluation_0, data=Data(data=hetero_lr_0.output.data))\n\n    pipeline.compile()\n\n    pipeline.fit()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"PIPELINE DEMO\")\n    parser.add_argument(\"-config\", type=str,\n                        help=\"config file\")\n    args = parser.parse_args()\n    if args.config is not None:\n        main(args.config)\n    else:\n        main()\n",
  "from loguru import logger\n\nfrom sc2 import maps\nfrom sc2.bot_ai import BotAI\nfrom sc2.data import Difficulty, Race\nfrom sc2.ids.ability_id import AbilityId\nfrom sc2.ids.buff_id import BuffId\nfrom sc2.ids.unit_typeid import UnitTypeId\nfrom sc2.ids.upgrade_id import UpgradeId\nfrom sc2.main import run_game\nfrom sc2.player import Bot, Computer\n\n\n# pylint: disable=W0231\nclass WarpGateBot(BotAI):\n\n    def __init__(self):\n        # Initialize inherited class\n        self.proxy_built = False\n\n    async def warp_new_units(self, proxy):\n        for warpgate in self.structures(UnitTypeId.WARPGATE).ready:\n            abilities = await self.get_available_abilities(warpgate)\n            # all the units have the same cooldown anyway so let's just look at ZEALOT\n            if AbilityId.WARPGATETRAIN_STALKER in abilities:\n                pos = proxy.position.to2.random_on_distance(4)\n                placement = await self.find_placement(AbilityId.WARPGATETRAIN_STALKER, pos, placement_step=1)\n                if placement is None:\n                    # return ActionResult.CantFindPlacementLocation\n                    logger.info(\"can't place\")\n                    return\n                warpgate.warp_in(UnitTypeId.STALKER, placement)\n\n    # pylint: disable=R0912\n    async def on_step(self, iteration):\n        await self.distribute_workers()\n\n        if not self.townhalls.ready:\n            # Attack with all workers if we don't have any nexuses left, attack-move on enemy spawn (doesn't work on 4 player map) so that probes auto attack on the way\n            for worker in self.workers:\n                worker.attack(self.enemy_start_locations[0])\n            return\n\n        nexus = self.townhalls.ready.random\n\n        # Build pylon when on low supply\n        if self.supply_left < 2 and self.already_pending(UnitTypeId.PYLON) == 0:\n            # Always check if you can afford something before you build it\n            if self.can_afford(UnitTypeId.PYLON):\n                await self.build(UnitTypeId.PYLON, near=nexus)\n            return\n\n        if self.workers.amount < self.townhalls.amount * 22 and nexus.is_idle:\n            if self.can_afford(UnitTypeId.PROBE):\n                nexus.train(UnitTypeId.PROBE)\n\n        elif self.structures(UnitTypeId.PYLON).amount < 5 and self.already_pending(UnitTypeId.PYLON) == 0:\n            if self.can_afford(UnitTypeId.PYLON):\n                await self.build(UnitTypeId.PYLON, near=nexus.position.towards(self.game_info.map_center, 5))\n\n        proxy = None\n        if self.structures(UnitTypeId.PYLON).ready:\n            proxy = self.structures(UnitTypeId.PYLON).closest_to(self.enemy_start_locations[0])\n            pylon = self.structures(UnitTypeId.PYLON).ready.random\n            if self.structures(UnitTypeId.GATEWAY).ready:\n                # If we have no cyber core, build one\n                if not self.structures(UnitTypeId.CYBERNETICSCORE):\n                    if (\n                        self.can_afford(UnitTypeId.CYBERNETICSCORE)\n                        and self.already_pending(UnitTypeId.CYBERNETICSCORE) == 0\n                    ):\n                        await self.build(UnitTypeId.CYBERNETICSCORE, near=pylon)\n            # Build up to 4 gates\n            if (\n                self.can_afford(UnitTypeId.GATEWAY)\n                and self.structures(UnitTypeId.WARPGATE).amount + self.structures(UnitTypeId.GATEWAY).amount < 4\n            ):\n                await self.build(UnitTypeId.GATEWAY, near=pylon)\n\n        # Build gas\n        for nexus in self.townhalls.ready:\n            vgs = self.vespene_geyser.closer_than(15, nexus)\n            for vg in vgs:\n                if not self.can_afford(UnitTypeId.ASSIMILATOR):\n                    break\n                worker = self.select_build_worker(vg.position)\n                if worker is None:\n                    break\n                if not self.gas_buildings or not self.gas_buildings.closer_than(1, vg):\n                    worker.build_gas(vg)\n                    worker.stop(queue=True)\n\n        # Research warp gate if cybercore is completed\n        if (\n            self.structures(UnitTypeId.CYBERNETICSCORE).ready and self.can_afford(AbilityId.RESEARCH_WARPGATE)\n            and self.already_pending_upgrade(UpgradeId.WARPGATERESEARCH) == 0\n        ):\n            ccore = self.structures(UnitTypeId.CYBERNETICSCORE).ready.first\n            ccore.research(UpgradeId.WARPGATERESEARCH)\n\n        # Morph to warp gate when research is complete\n        for gateway in self.structures(UnitTypeId.GATEWAY).ready.idle:\n            if self.already_pending_upgrade(UpgradeId.WARPGATERESEARCH) == 1:\n                gateway(AbilityId.MORPH_WARPGATE)\n\n        if self.proxy_built and proxy:\n            await self.warp_new_units(proxy)\n\n        # Make stalkers attack either closest enemy unit or enemy spawn location\n        if self.units(UnitTypeId.STALKER).amount > 3:\n            for stalker in self.units(UnitTypeId.STALKER).ready.idle:\n                targets = (self.enemy_units | self.enemy_structures).filter(lambda unit: unit.can_be_attacked)\n                if targets:\n                    target = targets.closest_to(stalker)\n                    stalker.attack(target)\n                else:\n                    stalker.attack(self.enemy_start_locations[0])\n\n        # Build proxy pylon\n        if (\n            self.structures(UnitTypeId.CYBERNETICSCORE).amount >= 1 and not self.proxy_built\n            and self.can_afford(UnitTypeId.PYLON)\n        ):\n            p = self.game_info.map_center.towards(self.enemy_start_locations[0], 20)\n            await self.build(UnitTypeId.PYLON, near=p)\n            self.proxy_built = True\n\n        # Chrono nexus if cybercore is not ready, else chrono cybercore\n        if not self.structures(UnitTypeId.CYBERNETICSCORE).ready:\n            if not nexus.has_buff(BuffId.CHRONOBOOSTENERGYCOST) and not nexus.is_idle:\n                if nexus.energy >= 50:\n                    nexus(AbilityId.EFFECT_CHRONOBOOSTENERGYCOST, nexus)\n        else:\n            ccore = self.structures(UnitTypeId.CYBERNETICSCORE).ready.first\n            if not ccore.has_buff(BuffId.CHRONOBOOSTENERGYCOST) and not ccore.is_idle:\n                if nexus.energy >= 50:\n                    nexus(AbilityId.EFFECT_CHRONOBOOSTENERGYCOST, ccore)\n\n\ndef main():\n    run_game(\n        maps.get(\"(2)CatalystLE\"),\n        [Bot(Race.Protoss, WarpGateBot()), Computer(Race.Protoss, Difficulty.Easy)],\n        realtime=False,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "#\n#  ISC License\n#\n#  Copyright (c) 2016, Autonomous Vehicle Systems Lab, University of Colorado at Boulder\n#\n#  Permission to use, copy, modify, and/or distribute this software for any\n#  purpose with or without fee is hereby granted, provided that the above\n#  copyright notice and this permission notice appear in all copies.\n#\n#  THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n#  WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n#  MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n#  ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n#  ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n#  OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n#\n\n\n\nr\"\"\"\nThis script is a basic demonstration of how to run Monte Carlo simulations. Look at the source code for\nfurther discussion and instructions.\n\n.. note::\n\n    In these Monte Carlo simulations the retained data is stored as the data array with the time\n    information added as the first column.  This is the same retained data format as used\n    with BSK 1.x.\n\n\"\"\"\n\nimport inspect\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfilename = inspect.getframeinfo(inspect.currentframe()).filename\nfileNameString = os.path.basename(os.path.splitext(__file__)[0])\npath = os.path.dirname(os.path.abspath(filename))\n\nfrom Basilisk import __path__\nbskPath = __path__[0]\n\n# import general simulation support files\nimport sys\nfrom Basilisk.utilities.MonteCarlo.Controller import Controller\nfrom Basilisk.utilities.MonteCarlo.RetentionPolicy import RetentionPolicy\nfrom Basilisk.utilities.MonteCarlo.Dispersions import (UniformEulerAngleMRPDispersion, UniformDispersion,\n                                                       NormalVectorCartDispersion)\n\nsys.path.append(path+\"/../BskSim/scenarios/\")\nimport scenario_AttFeedback\n\nsNavTransName = \"sNavTransMsg\"\nattGuidName = \"attGuidMsg\"\n\ndef run(show_plots):\n    \"\"\"This function is called by the py.test environment.\"\"\"\n\n    # A MonteCarlo simulation can be created using the `MonteCarlo` module.\n    # This module is used to execute monte carlo simulations, and access\n    # retained data from previously executed MonteCarlo runs.\n    monteCarlo = Controller()\n    monteCarlo.setSimulationFunction(scenario_AttFeedback.scenario_AttFeedback)  # Required: function that configures the base scenario\n    monteCarlo.setExecutionFunction(scenario_AttFeedback.runScenario)  # Required: function that runs the scenario\n    monteCarlo.setExecutionCount(4)  # Required: Number of MCs to run\n\n    monteCarlo.setArchiveDir(path + \"/scenario_AttFeedbackMC\")  # Optional: If/where to save retained data.\n    monteCarlo.setShouldDisperseSeeds(True)  # Optional: Randomize the seed for each module\n    monteCarlo.setThreadCount(2)  # Optional: Number of processes to spawn MCs on\n    monteCarlo.setVerbose(True)  # Optional: Produce supplemental text output in console describing status\n    monteCarlo.setVarCast('float')  # Optional: Downcast the retained numbers to float32 to save on storage space\n    monteCarlo.setDispMagnitudeFile(True)  # Optional: Produce a .txt file that shows dispersion in std dev units\n\n    # Statistical dispersions can be applied to initial parameters using the MonteCarlo module\n    dispMRPInit = 'TaskList[0].TaskModels[0].hub.sigma_BNInit'\n    dispOmegaInit = 'TaskList[0].TaskModels[0].hub.omega_BN_BInit'\n    dispMass = 'TaskList[0].TaskModels[0].hub.mHub'\n    dispCoMOff = 'TaskList[0].TaskModels[0].hub.r_BcB_B'\n    dispInertia = 'hubref.IHubPntBc_B'\n    dispList = [dispMRPInit, dispOmegaInit, dispMass, dispCoMOff, dispInertia]\n\n    # Add dispersions with their dispersion type\n    monteCarlo.addDispersion(UniformEulerAngleMRPDispersion('TaskList[0].TaskModels[0].hub.sigma_BNInit'))\n    monteCarlo.addDispersion(NormalVectorCartDispersion('TaskList[0].TaskModels[0].hub.omega_BN_BInit', 0.0, 0.75 / 3.0 * np.pi / 180))\n    monteCarlo.addDispersion(UniformDispersion('TaskList[0].TaskModels[0].hub.mHub', ([750.0 - 0.05*750, 750.0 + 0.05*750])))\n    monteCarlo.addDispersion(NormalVectorCartDispersion('TaskList[0].TaskModels[0].hub.r_BcB_B', [0.0, 0.0, 1.0], [0.05 / 3.0, 0.05 / 3.0, 0.1 / 3.0]))\n\n    # A `RetentionPolicy` is used to define what data from the simulation should be retained. A `RetentionPolicy`\n    # is a list of messages and variables to log from each simulation run. It also can have a callback,\n    # used for plotting/processing the retained data.\n    retentionPolicy = RetentionPolicy()\n    samplingTime = int(2E9)\n    retentionPolicy.addMessageLog(sNavTransName, [\"r_BN_N\"])\n    retentionPolicy.addMessageLog(attGuidName, [\"sigma_BR\", \"omega_BR_B\"])\n    retentionPolicy.setDataCallback(displayPlots)\n    monteCarlo.addRetentionPolicy(retentionPolicy)\n\n    failures = monteCarlo.executeSimulations()\n\n    if show_plots:\n        monteCarlo.executeCallbacks()\n        plt.show()\n\n    return\n\ndef displayPlots(data, retentionPolicy):\n    states = data[\"messages\"][attGuidName + \".sigma_BR\"]\n    time = states[:, 0]\n    plt.figure(1)\n    plt.plot(time, states[:,1],\n             time, states[:,2],\n             time, states[:,3])\n\n\n\nif __name__ == \"__main__\":\n    run(True)\n",
  "import blenderproc as bproc\nimport numpy as np\nfrom mathutils import Euler\nimport argparse\nimport os\n\nparser = argparse.ArgumentParser()\nparser.add_argument('house', help=\"Path to the house.json file of the SUNCG scene to load\")\nparser.add_argument('object_path', help='Path to the chair object which will be used to replace others.')\nparser.add_argument('output_dir', nargs='?', default=\"examples/datasets/suncg_with_object_replacer/output\",\n                    help=\"Path to where the final files, will be saved\")\nargs = parser.parse_args()\n\nbproc.init()\n\n# load the objects into the scene\nlabel_mapping = bproc.utility.LabelIdMapping.from_csv(bproc.utility.resolve_resource(os.path.join('id_mappings', 'nyu_idset.csv')))\nobjs = bproc.loader.load_suncg(args.house, label_mapping)\n\n# replace some objects with others\n\nchair_obj = bproc.loader.load_obj(args.object_path)\nif len(chair_obj) != 1:\n    raise Exception(f\"There should only be one chair object not: {len(chair_obj)}\")\nchair_obj = chair_obj[0]\n\n\ndef relative_pose_sampler(obj):\n    # Sample random rotation and apply it to the objects pose\n    obj.blender_obj.rotation_euler.rotate(Euler((0, 0, np.random.uniform(0.0, 6.283185307))))\n\n\nreplace_ratio = 1.0\nbproc.object.replace_objects(\n    objects_to_be_replaced=bproc.filter.by_cp(objs, \"coarse_grained_class\", \"chair\"),\n    objects_to_replace_with=[chair_obj],\n    ignore_collision_with=bproc.filter.by_cp(objs, \"suncg_type\", \"Floor\"),\n    replace_ratio=replace_ratio,\n    copy_properties=True,\n    relative_pose_sampler=relative_pose_sampler\n)\n\n# some objects won't be valid anymore\nobjs = [obj for obj in objs if obj.is_valid()]\n\n# makes Suncg objects emit light\nbproc.lighting.light_suncg_scene()\n\n# Init sampler for sampling locations inside the loaded suncg house\npoint_sampler = bproc.sampler.SuncgPointInRoomSampler(objs)\n# Init bvh tree containing all mesh objects\nbvh_tree = bproc.object.create_bvh_tree_multi_objects([o for o in objs if isinstance(o, bproc.types.MeshObject)])\n\nposes = 0\ntries = 0\nwhile tries < 10000 and poses < 5:\n    # Sample point inside house\n    height = np.random.uniform(0.5, 2)\n    location, _ = point_sampler.sample(height)\n    # Sample rotation (fix around X and Y axis)\n    euler_rotation = np.random.uniform([1.2217, 0, 0], [1.2217, 0, 6.283185307])\n    cam2world_matrix = bproc.math.build_transformation_mat(location, euler_rotation)\n\n    # Check that obstacles are at least 1 meter away from the camera and make sure the view interesting enough\n    if bproc.camera.perform_obstacle_in_view_check(cam2world_matrix, {\"min\": 1.0},\n                                                       bvh_tree) and bproc.camera.scene_coverage_score(\n            cam2world_matrix) > 0.4:\n        bproc.camera.add_camera_pose(cam2world_matrix)\n        poses += 1\n    tries += 1\n\n# activate normal and depth rendering\nbproc.renderer.enable_normals_output()\nbproc.renderer.enable_depth_output(activate_antialiasing=False)\nbproc.material.add_alpha_channel_to_textures(blurry_edges=True)\nbproc.renderer.enable_segmentation_output(map_by=[\"category_id\"])\n\n# render the whole pipeline\ndata = bproc.renderer.render()\n\n# write the data to a .hdf5 container\nbproc.writer.write_hdf5(args.output_dir, data)\n",
  "#!/usr/bin/env python3\n# encoding: utf-8\n\nfrom seedemu import *\nimport os\n\n\n# Create the Emulator \nemu = Emulator()\n\n# Create the base layer\nbase = Base()\n\n###############################################################################\n# Create Internet exchanges \n\nix100 = base.createInternetExchange(100)\nix101 = base.createInternetExchange(101)\nix100.getPeeringLan().setDisplayName('New York-100')\nix101.getPeeringLan().setDisplayName('Chicago-101')\n\n\n###############################################################################\n# Create and set up a transit AS (AS-3)\n\nas3 = base.createAutonomousSystem(3)\n\n# Create 3 internal networks\nas3.createNetwork('net0')\nas3.createNetwork('net1')\nas3.createNetwork('net2')\n\n# Create four routers and link them in a linear structure:\n# ix100 <--> r1 <--> r2 <--> r3 <--> r4 <--> ix101\n# r1 and r2 are BGP routers because they are connected to internet exchanges\nas3.createRouter('r1').joinNetwork('net0').joinNetwork('ix100')\nas3.createRouter('r2').joinNetwork('net0').joinNetwork('net1')\nas3.createRouter('r3').joinNetwork('net1').joinNetwork('net2')\nas3.createRouter('r4').joinNetwork('net2').joinNetwork('ix101')\n\n\n###############################################################################\n# Create and set up the stub AS (AS-151)\n\nas151 = base.createAutonomousSystem(151)\n\n# Create an internal network and a router\nas151.createNetwork('net0')\nas151.createRouter('router0').joinNetwork('net0').joinNetwork('ix100')\n\n# Create two host nodes \nas151.createHost('host0').joinNetwork('net0')\nas151.createHost('host1').joinNetwork('net0', address = '10.151.0.80')\n\n# Install additional software on a host\nhost0 = as151.getHost('host0')\nhost0.addSoftware('telnetd').addSoftware('telnet')\n\n# Run an additional command inside the container \n# The command creates a new account inside the host (also sets its password)\nhost0.addBuildCommand('useradd -m -s /bin/bash seed && echo \"seed:dees\" | chpasswd')\n\n###############################################################################\n# Create and set up the stub AS (AS-152)\n\n\nas152 = base.createAutonomousSystem(152)\nas152.createNetwork('net0')\nas152.createRouter('router0').joinNetwork('net0').joinNetwork('ix101')\nas152.createHost('host0').joinNetwork('net0')\nas152.createHost('host1').joinNetwork('net0')\nas152.createHost('host2').joinNetwork('net0')\n\n# Install additional software on a host\nas152.getHost('host0').addSoftware('telnet')\n\n###############################################################################\n# Create and set up the stub AS (AS-153)\n\n# Use the utility function to create the AS\nMakers.makeStubAs(emu, base, 153, 101, [None, None])\n\n# Further customization\nas153 = base.getAutonomousSystem(153)\nas153.getHost('host_1').addSoftware('telnet')\n\n\n###############################################################################\n# BGP peering\n\n# Create the Ebgp layer\nebgp = Ebgp()\n\n# Make AS-3 the internet service provider for all the stub ASes\nebgp.addPrivatePeering (100, 3,   151, abRelationship = PeerRelationship.Provider)\nebgp.addPrivatePeerings(101, [3], [152, 153], abRelationship = PeerRelationship.Provider)\n\n# Peer AS-152 and AS-153 directly as peers\nebgp.addPrivatePeering(101, 152, 153, abRelationship = PeerRelationship.Peer)\n\n\n###############################################################################\n# Web Service Layer \n\n# Create the WebService layer\nweb = WebService()\n\n# Create web service nodes (virtual nodes)\n# add Class label to the Conatiner (please refer README.md for further information.)\nweb01 = web.install('web01').appendClassName(\"SEEDWeb\")\nweb02 = web.install('web02').appendClassName(\"SyrWeb\")\n\n# Bind the virtual nodes to physical nodes\nemu.addBinding(Binding('web01', filter = Filter(nodeName = 'host0', asn = 151)))\nemu.addBinding(Binding('web02', filter = Filter(nodeName = 'host0', asn = 152)))\n\n###############################################################################\n\nemu.addLayer(base)\nemu.addLayer(ebgp)\nemu.addLayer(web)\n\nemu.addLayer(Routing())\nemu.addLayer(Ibgp())\nemu.addLayer(Ospf())\n\n###############################################################################\n# Save it to a component file, so it can be used by other emulators\n\n# This is optional\nemu.dump('base-component.bin')\n\n\n###############################################################################\n# Rendering: This is where the actual binding happens\n\nemu.render()\n\n# Change the display name for the nodes hosting the web services\nemu.getBindingFor('web01').setDisplayName('Web-1')\nemu.getBindingFor('web02').setDisplayName('Web-2')\n\n\n###############################################################################\n# Compilation\n\ndocker = Docker()\n\n# Use the \"handsonsecurity/seed-ubuntu:small\" custom image from dockerhub\ndocker.addImage(DockerImage('handsonsecurity/seed-ubuntu:small', [], local = False), priority=-1)\ndocker.setImageOverride(as152.getHost('host1'), 'handsonsecurity/seed-ubuntu:small')\n\n# Use the \"seed-ubuntu-large\" custom image from local\ndocker.addImage(DockerImage('seed-ubuntu-large', [], local = True), priority=-1)\ndocker.setImageOverride(as152.getHost('host2'), 'seed-ubuntu-large')\n\n# Generate the Docker files\nemu.compile(docker, './output')\n\n# Copy the base container image to the output folder\n# the base container image should be located under the ouput folder to add it as custom image.\nos.system('cp -r seed-ubuntu-large ./output')\n\n# Generate other type of outputs\n#emu.compile(Graphviz(), './others/graphs')\n#emu.compile(DistributedDocker(), './others/distributed-docker')\n#emu.compile(GcpDistributedDocker(), './others/gcp-distributed-docker')\n\n\n",
  "# This file is part of BurnMan - a thermoelastic and thermodynamic toolkit\n# for the Earth and Planetary Sciences\n# Copyright (C) 2012 - 2015 by the BurnMan team, released under the GNU\n# GPL v2 or later.\n\n\n\"\"\"\n\nexample_composite_seismic_velocities\n------------------------------------\n\nThis example shows how to create different minerals, how to compute seismic\nvelocities, and how to compare them to a seismic reference model.\n\nThere are many different ways in BurnMan to combine minerals into a\ncomposition. Here we present a couple of examples:\n\n1. Two minerals mixed in simple mole fractions. Can be chosen from the BurnMan\n   libraries or from user defined minerals (see example_user_input_material)\n2. Example with three minerals\n3. Using preset solutions\n4. Defining your own solution\n\n\nTo turn a method of mineral creation \"on\" the first if statement above the\nmethod must be set to True, with all others set to False.\n\nNote: These minerals can include a spin transition in (Mg,Fe)O, see\nexample_spintransition.py for explanation of how to implement this\n\n*Uses:*\n\n* :doc:`mineral_database`\n* :class:`burnman.Composite`\n* :class:`burnman.Mineral`\n* :class:`burnman.Solution`\n\n*Demonstrates:*\n\n* Different ways to define a composite\n* Using minerals and solutions\n* Compare computations to seismic models\n\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport burnman\nfrom burnman import minerals\nfrom burnman.classes.solution import Solution\nfrom burnman.classes.solutionmodel import IdealSolution\n\n\nif __name__ == \"__main__\":\n    # To compute seismic velocities and other properties, we need to supply\n    # burnman with a list of minerals (phases) and their molar abundances.\n    # Minerals are classes found in burnman.minerals and are derived from\n    # burnman.minerals.material.\n    # Here are a few ways to define phases and molar_abundances:\n    # Example 1: two simple fixed minerals\n    if True:\n        amount_perovskite = 0.95\n        rock = burnman.Composite(\n            [minerals.SLB_2011.mg_perovskite(), minerals.SLB_2011.periclase()],\n            [amount_perovskite, 1 - amount_perovskite],\n        )\n\n    # Example 2: three materials\n    if False:\n        rock = burnman.Composite(\n            [\n                minerals.SLB_2011.fe_perovskite(),\n                minerals.SLB_2011.periclase(),\n                minerals.SLB_2011.stishovite(),\n            ],\n            [0.7, 0.2, 0.1],\n        )\n\n    # Example 3: Mixing solutions\n    if False:\n        # Defining a rock using a predefined solution from the mineral\n        # library database.\n        preset_solution = minerals.SLB_2011.mg_fe_perovskite()\n        # The line below is optional to see which endmembers\n        # (and in which order) are in the solution\n        # print preset_solution.endmembers\n        # Set molar_fraction of mg_perovskite, fe_perovskite and al_perovskite\n        preset_solution.set_composition([0.9, 0.1, 0.0])\n        rock = burnman.Composite(\n            [preset_solution, minerals.SLB_2011.periclase()], [0.8, 0.2]\n        )\n\n    # Example 4: Defining your own solution\n    if False:\n        # Define a new Solution with mg and fe perovskite endmembers\n        mpv = minerals.SLB_2011.mg_perovskite()\n        fpv = minerals.SLB_2011.fe_perovskite()\n        new_solution = Solution(\n            name=\"New Mg-Fe bridgmanite\",\n            solution_model=IdealSolution(\n                endmembers=[[mpv, \"[Mg]SiO3\"], [fpv, \"[Fe]SiO3\"]]\n            ),\n        )\n\n        # Set molar fraction of endmembers\n        new_solution.set_composition([0.9, 0.1])\n        rock = burnman.Composite(\n            [new_solution, minerals.SLB_2011.periclase()], [0.8, 0.2]\n        )\n\n    # seismic model for comparison:\n    # pick from .prem() .slow() .fast() (see burnman/seismic.py)\n    seismic_model = burnman.seismic.PREM()\n    # set on how many depth slices the computations should be done\n    number_of_points = 20\n    # we will do our computation and comparison at the following depth values:\n    depths = np.linspace(700e3, 2800e3, number_of_points)\n    # alternatively, we could use the values where prem is defined:\n    # depths = seismic_model.internal_depth_list(mindepth=700.e3,\n    # maxdepth=2800.e3)\n    seis_p, seis_rho, seis_vp, seis_vs, seis_vphi = seismic_model.evaluate(\n        [\"pressure\", \"density\", \"v_p\", \"v_s\", \"v_phi\"], depths\n    )\n\n    temperature = burnman.geotherm.brown_shankland(depths)\n\n    print(\"Calculations are done for:\")\n    rock.debug_print()\n\n    mat_rho, mat_vp, mat_vphi, mat_vs, mat_K, mat_G = rock.evaluate(\n        [\"density\", \"v_p\", \"v_phi\", \"v_s\", \"K_S\", \"G\"], seis_p, temperature\n    )\n\n    [vs_err, vphi_err, rho_err] = burnman.utils.math.compare_chifactor(\n        [mat_vs, mat_vphi, mat_rho], [seis_vs, seis_vphi, seis_rho]\n    )\n\n    # PLOTTING\n    # plot vs\n    plt.subplot(2, 2, 1)\n    plt.plot(\n        seis_p / 1.0e9,\n        mat_vs / 1.0e3,\n        color=\"b\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"b\",\n        markersize=4,\n        label=\"computation\",\n    )\n    plt.plot(\n        seis_p / 1.0e9,\n        seis_vs / 1.0e3,\n        color=\"k\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"k\",\n        markersize=4,\n        label=\"reference\",\n    )\n    plt.title(\"Vs (km/s)\")\n    plt.xlim(min(seis_p) / 1.0e9, max(seis_p) / 1.0e9)\n    plt.ylim(5.1, 7.6)\n    plt.legend(loc=\"lower right\")\n    plt.text(40, 7.3, \"misfit= %3.3f\" % vs_err)\n\n    # plot Vphi\n    plt.subplot(2, 2, 2)\n    plt.plot(\n        seis_p / 1.0e9,\n        mat_vphi / 1.0e3,\n        color=\"b\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"b\",\n        markersize=4,\n    )\n    plt.plot(\n        seis_p / 1.0e9,\n        seis_vphi / 1.0e3,\n        color=\"k\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"k\",\n        markersize=4,\n    )\n    plt.title(\"Vphi (km/s)\")\n    plt.xlim(min(seis_p) / 1.0e9, max(seis_p) / 1.0e9)\n    plt.ylim(7, 12)\n    plt.text(40, 11.5, \"misfit= %3.3f\" % vphi_err)\n\n    # plot density\n    plt.subplot(2, 2, 3)\n    plt.plot(\n        seis_p / 1.0e9,\n        mat_rho / 1.0e3,\n        color=\"b\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"b\",\n        markersize=4,\n    )\n    plt.plot(\n        seis_p / 1.0e9,\n        seis_rho / 1.0e3,\n        color=\"k\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"k\",\n        markersize=4,\n    )\n    plt.title(\"density ($\\\\cdot 10^3$ kg/m$^3$)\")\n    plt.xlim(min(seis_p) / 1.0e9, max(seis_p) / 1.0e9)\n    plt.text(40, 4.3, \"misfit= %3.3f\" % rho_err)\n    plt.xlabel(\"Pressure (GPa)\")\n\n    # plot geotherm\n    plt.subplot(2, 2, 4)\n    plt.plot(\n        seis_p / 1e9,\n        temperature,\n        color=\"r\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"r\",\n        markersize=4,\n    )\n    plt.title(\"Geotherm (K)\")\n    plt.xlim(min(seis_p) / 1.0e9, max(seis_p) / 1.0e9)\n    plt.xlabel(\"Pressure (GPa)\")\n\n    plt.savefig(\"output_figures/example_composition.png\")\n    plt.show()\n",
  "# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n\"\"\"\nThis script is an exmaple for how to fuse pruning and distillation.\n\"\"\"\n\nimport pickle\n\nimport torch\n\nfrom examples.compression.models import (\n    build_resnet18,\n    prepare_dataloader,\n    prepare_optimizer,\n    train,\n    training_step,\n    evaluate,\n    device\n)\n\nfrom nni.compression import TorchEvaluator\nfrom nni.compression.base.compressor import Quantizer\nfrom nni.compression.distillation import DynamicLayerwiseDistiller\nfrom nni.compression.pruning import TaylorPruner, AGPPruner\nfrom nni.compression.quantization import QATQuantizer\nfrom nni.compression.utils import auto_set_denpendency_group_ids\nfrom nni.compression.speedup import ModelSpeedup\n\n\nif __name__ == '__main__':\n    # finetuning resnet18 on Cifar10\n    model = build_resnet18()\n    optimizer = prepare_optimizer(model)\n    train(model, optimizer, training_step, lr_scheduler=None, max_steps=None, max_epochs=30)\n    _, test_loader = prepare_dataloader()\n    print('Original model paramater number: ', sum([param.numel() for param in model.parameters()]))\n    print('Original model after 10 epochs finetuning acc: ', evaluate(model, test_loader), '%')\n\n    # build a teacher model\n    teacher_model = build_resnet18()\n    teacher_model.load_state_dict(pickle.loads(pickle.dumps(model.state_dict())))\n\n    # create pruner\n    bn_list = [module_name for module_name, module in model.named_modules() if isinstance(module, torch.nn.BatchNorm2d)]\n    p_config_list = [{\n        'op_types': ['Conv2d'],\n        'sparse_ratio': 0.5\n    }, *[{\n        'op_names': [name],\n        'target_names': ['_output_'],\n        'target_settings': {\n            '_output_': {\n                'align': {\n                    'module_name': name.replace('bn', 'conv') if 'bn' in name else name.replace('downsample.1', 'downsample.0'),\n                    'target_name': 'weight',\n                    'dims': [0],\n                },\n                'granularity': 'per_channel'\n            }\n        }\n    } for name in bn_list]]\n    dummy_input = torch.rand(8, 3, 224, 224).to(device)\n    p_config_list = auto_set_denpendency_group_ids(model, p_config_list, dummy_input)\n\n    optimizer = prepare_optimizer(model)\n    evaluator = TorchEvaluator(train, optimizer, training_step)\n    sub_pruner = TaylorPruner(model, p_config_list, evaluator, training_steps=100)\n    scheduled_pruner = AGPPruner(sub_pruner, interval_steps=100, total_times=30)\n\n    # create quantizer\n    q_config_list = [{\n        'op_types': ['Conv2d'],\n        'quant_dtype': 'int8',\n        'target_names': ['_input_'],\n        'granularity': 'per_channel'\n    }, {\n        'op_types': ['Conv2d'],\n        'quant_dtype': 'int8',\n        'target_names': ['weight'],\n        'granularity': 'out_channel'\n    }, {\n        'op_types': ['BatchNorm2d'],\n        'quant_dtype': 'int8',\n        'target_names': ['_output_'],\n        'granularity': 'per_channel'\n    }]\n\n    quantizer = QATQuantizer.from_compressor(scheduled_pruner, q_config_list, quant_start_step=100)\n\n    # create distiller\n    def teacher_predict(batch, teacher_model):\n        return teacher_model(batch[0])\n\n    d_config_list = [{\n        'op_types': ['Conv2d'],\n        'lambda': 0.1,\n        'apply_method': 'mse',\n    }]\n    distiller = DynamicLayerwiseDistiller.from_compressor(quantizer, d_config_list, teacher_model, teacher_predict, 0.1)\n\n    # max_steps contains (30 iterations 100 steps agp taylor pruning, and 3000 steps finetuning)\n    distiller.compress(max_steps=100 * 60, max_epochs=None)\n    distiller.unwrap_model()\n    distiller.unwrap_teacher_model()\n\n    # speed up model\n    masks = scheduled_pruner.get_masks()\n    speedup = ModelSpeedup(model, dummy_input, masks)\n    model = speedup.speedup_model()\n\n    print('Compressed model paramater number: ', sum([param.numel() for param in model.parameters()]))\n    print('Compressed model without finetuning & qsim acc: ', evaluate(model, test_loader), '%')\n\n    # simulate quantization\n    calibration_config = quantizer.get_calibration_config()\n\n    def trans(calibration_config, speedup: ModelSpeedup):\n        for node, node_info in speedup.node_infos.items():\n            if node.op == 'call_module' and node.target in calibration_config:\n                # assume the module only has one input and one output\n                input_mask = speedup.node_infos[node.args[0]].output_masks\n                param_mask = node_info.param_masks\n                output_mask = node_info.output_masks\n\n                module_cali_config = calibration_config[node.target]\n                if '_input_0' in module_cali_config:\n                    reduce_dims = list(range(len(input_mask.shape)))\n                    reduce_dims.remove(1)\n                    idxs = torch.nonzero(input_mask.sum(reduce_dims), as_tuple=True)[0].cpu()\n                    module_cali_config['_input_0']['scale'] = module_cali_config['_input_0']['scale'].index_select(1, idxs)\n                    module_cali_config['_input_0']['zero_point'] = module_cali_config['_input_0']['zero_point'].index_select(1, idxs)\n                if '_output_0' in module_cali_config:\n                    reduce_dims = list(range(len(output_mask.shape)))\n                    reduce_dims.remove(1)\n                    idxs = torch.nonzero(output_mask.sum(reduce_dims), as_tuple=True)[0].cpu()\n                    module_cali_config['_output_0']['scale'] = module_cali_config['_output_0']['scale'].index_select(1, idxs)\n                    module_cali_config['_output_0']['zero_point'] = module_cali_config['_output_0']['zero_point'].index_select(1, idxs)\n                if 'weight' in module_cali_config:\n                    reduce_dims = list(range(len(param_mask['weight'].shape)))\n                    reduce_dims.remove(0)\n                    idxs = torch.nonzero(param_mask['weight'].sum(reduce_dims), as_tuple=True)[0].cpu()\n                    module_cali_config['weight']['scale'] = module_cali_config['weight']['scale'].index_select(0, idxs)\n                    module_cali_config['weight']['zero_point'] = module_cali_config['weight']['zero_point'].index_select(0, idxs)\n                if 'bias' in module_cali_config:\n                    idxs = torch.nonzero(param_mask['bias'], as_tuple=True)[0].cpu()\n                    module_cali_config['bias']['scale'] = module_cali_config['bias']['scale'].index_select(0, idxs)\n                    module_cali_config['bias']['zero_point'] = module_cali_config['bias']['zero_point'].index_select(0, idxs)\n        return calibration_config\n\n    calibration_config = trans(calibration_config, speedup)\n\n    sim_quantizer = Quantizer(model, q_config_list)\n    sim_quantizer.update_calibration_config(calibration_config)\n\n    print('Compressed model without finetuning acc: ', evaluate(model, test_loader), '%')\n",
  "# Copyright (c) 2015,2016,2017 MetPy Developers.\n# Distributed under the terms of the BSD 3-Clause License.\n# SPDX-License-Identifier: BSD-3-Clause\n\"\"\"\n=================\nAdvanced Sounding\n=================\n\nPlot a sounding using MetPy with more advanced features.\n\nBeyond just plotting data, this uses calculations from `metpy.calc` to find the lifted\ncondensation level (LCL) and the profile of a surface-based parcel. The area between the\nambient profile and the parcel profile is colored as well.\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport metpy.calc as mpcalc\nfrom metpy.cbook import get_test_data\nfrom metpy.plots import add_metpy_logo, SkewT\nfrom metpy.units import units\n\n###########################################\n# Upper air data can be obtained using the siphon package, but for this example we will use\n# some of MetPy's sample data.\n\ncol_names = ['pressure', 'height', 'temperature', 'dewpoint', 'direction', 'speed']\n\ndf = pd.read_fwf(get_test_data('may4_sounding.txt', as_file_obj=False),\n                 skiprows=5, usecols=[0, 1, 2, 3, 6, 7], names=col_names)\n\n# Drop any rows with all NaN values for T, Td, winds\ndf = df.dropna(subset=('temperature', 'dewpoint', 'direction', 'speed'), how='all'\n               ).reset_index(drop=True)\n\n###########################################\n# We will pull the data out of the example dataset into individual variables and\n# assign units.\n\np = df['pressure'].values * units.hPa\nT = df['temperature'].values * units.degC\nTd = df['dewpoint'].values * units.degC\nwind_speed = df['speed'].values * units.knots\nwind_dir = df['direction'].values * units.degrees\nu, v = mpcalc.wind_components(wind_speed, wind_dir)\n\n###########################################\n# Create a new figure. The dimensions here give a good aspect ratio.\n\nfig = plt.figure(figsize=(9, 9))\nadd_metpy_logo(fig, 115, 100)\nskew = SkewT(fig, rotation=45)\n\n# Plot the data using normal plotting functions, in this case using\n# log scaling in Y, as dictated by the typical meteorological plot.\nskew.plot(p, T, 'r')\nskew.plot(p, Td, 'g')\nskew.plot_barbs(p, u, v)\nskew.ax.set_ylim(1000, 100)\nskew.ax.set_xlim(-40, 60)\n\n# Set some better labels than the default\nskew.ax.set_xlabel(f'Temperature ({T.units:~P})')\nskew.ax.set_ylabel(f'Pressure ({p.units:~P})')\n\n# Calculate LCL height and plot as black dot. Because `p`'s first value is\n# ~1000 mb and its last value is ~250 mb, the `0` index is selected for\n# `p`, `T`, and `Td` to lift the parcel from the surface. If `p` was inverted,\n# i.e. start from low value, 250 mb, to a high value, 1000 mb, the `-1` index\n# should be selected.\nlcl_pressure, lcl_temperature = mpcalc.lcl(p[0], T[0], Td[0])\nskew.plot(lcl_pressure, lcl_temperature, 'ko', markerfacecolor='black')\n\n# Calculate full parcel profile and add to plot as black line\nprof = mpcalc.parcel_profile(p, T[0], Td[0]).to('degC')\nskew.plot(p, prof, 'k', linewidth=2)\n\n# Shade areas of CAPE and CIN\nskew.shade_cin(p, T, prof, Td)\nskew.shade_cape(p, T, prof)\n\n# An example of a slanted line at constant T -- in this case the 0\n# isotherm\nskew.ax.axvline(0, color='c', linestyle='--', linewidth=2)\n\n# Add the relevant special lines\nskew.plot_dry_adiabats()\nskew.plot_moist_adiabats()\nskew.plot_mixing_lines()\n\n# Show the plot\nplt.show()\n",
  "# Copyright 2018 DeepMind Technologies Limited. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"An example CRR running on locomotion datasets (mujoco) from D4rl.\"\"\"\n\nfrom absl import app\nfrom absl import flags\nimport acme\nfrom acme import specs\nfrom acme.agents.jax import actor_core as actor_core_lib\nfrom acme.agents.jax import actors\nfrom acme.agents.jax import crr\nfrom acme.datasets import tfds\nfrom acme.examples.offline import helpers as gym_helpers\nfrom acme.jax import variable_utils\nfrom acme.types import Transition\nfrom acme.utils import loggers\nimport haiku as hk\nimport jax\nimport optax\nimport rlds\n\n# Agent flags\nflags.DEFINE_integer('batch_size', 64, 'Batch size.')\nflags.DEFINE_integer('evaluate_every', 20, 'Evaluation period.')\nflags.DEFINE_integer('evaluation_episodes', 10, 'Evaluation episodes.')\nflags.DEFINE_integer(\n    'num_demonstrations', 10,\n    'Number of demonstration episodes to load from the dataset. If None, loads the full dataset.'\n)\nflags.DEFINE_integer('seed', 0, 'Random seed for learner and evaluator.')\n# CQL specific flags.\nflags.DEFINE_float('policy_learning_rate', 3e-5, 'Policy learning rate.')\nflags.DEFINE_float('critic_learning_rate', 3e-4, 'Critic learning rate.')\nflags.DEFINE_float('discount', 0.99, 'Discount.')\nflags.DEFINE_integer('target_update_period', 100, 'Target update periode.')\nflags.DEFINE_integer('grad_updates_per_batch', 1, 'Grad updates per batch.')\nflags.DEFINE_bool(\n    'use_sarsa_target', True,\n    'Compute on-policy target using iterator actions rather than sampled '\n    'actions.'\n)\n# Environment flags.\nflags.DEFINE_string('env_name', 'HalfCheetah-v2',\n                    'Gym mujoco environment name.')\nflags.DEFINE_string(\n    'dataset_name', 'd4rl_mujoco_halfcheetah/v2-medium',\n    'D4rl dataset name. Can be any locomotion dataset from '\n    'https://www.tensorflow.org/datasets/catalog/overview#d4rl.')\n\nFLAGS = flags.FLAGS\n\n\ndef _add_next_action_extras(double_transitions: Transition) -> Transition:\n  return Transition(\n      observation=double_transitions.observation[0],\n      action=double_transitions.action[0],\n      reward=double_transitions.reward[0],\n      discount=double_transitions.discount[0],\n      next_observation=double_transitions.next_observation[0],\n      extras={'next_action': double_transitions.action[1]})\n\n\ndef main(_):\n  key = jax.random.PRNGKey(FLAGS.seed)\n  key_demonstrations, key_learner = jax.random.split(key, 2)\n\n  # Create an environment and grab the spec.\n  environment = gym_helpers.make_environment(task=FLAGS.env_name)\n  environment_spec = specs.make_environment_spec(environment)\n\n  # Get a demonstrations dataset with next_actions extra.\n  transitions = tfds.get_tfds_dataset(\n      FLAGS.dataset_name, FLAGS.num_demonstrations)\n  double_transitions = rlds.transformations.batch(\n      transitions, size=2, shift=1, drop_remainder=True)\n  transitions = double_transitions.map(_add_next_action_extras)\n  demonstrations = tfds.JaxInMemoryRandomSampleIterator(\n      transitions, key=key_demonstrations, batch_size=FLAGS.batch_size)\n\n  # Create the networks to optimize.\n  networks = crr.make_networks(environment_spec)\n\n  # CRR policy loss function.\n  policy_loss_coeff_fn = crr.policy_loss_coeff_advantage_exp\n\n  # Create the learner.\n  learner = crr.CRRLearner(\n      networks=networks,\n      random_key=key_learner,\n      discount=FLAGS.discount,\n      target_update_period=FLAGS.target_update_period,\n      policy_loss_coeff_fn=policy_loss_coeff_fn,\n      iterator=demonstrations,\n      policy_optimizer=optax.adam(FLAGS.policy_learning_rate),\n      critic_optimizer=optax.adam(FLAGS.critic_learning_rate),\n      grad_updates_per_batch=FLAGS.grad_updates_per_batch,\n      use_sarsa_target=FLAGS.use_sarsa_target)\n\n  def evaluator_network(\n      params: hk.Params, key: jax.Array, observation: jax.Array\n  ) -> jax.Array:\n    dist_params = networks.policy_network.apply(params, observation)\n    return networks.sample_eval(dist_params, key)\n\n  actor_core = actor_core_lib.batched_feed_forward_to_actor_core(\n      evaluator_network)\n  variable_client = variable_utils.VariableClient(\n      learner, 'policy', device='cpu')\n  evaluator = actors.GenericActor(\n      actor_core, key, variable_client, backend='cpu')\n\n  eval_loop = acme.EnvironmentLoop(\n      environment=environment,\n      actor=evaluator,\n      logger=loggers.TerminalLogger('evaluation', time_delta=0.))\n\n  # Run the environment loop.\n  while True:\n    for _ in range(FLAGS.evaluate_every):\n      learner.step()\n    eval_loop.run(FLAGS.evaluation_episodes)\n\n\nif __name__ == '__main__':\n  app.run(main)\n",
  "#!/bin/env python\n\"\"\"\nEnsemble verification\n=====================\n\nIn this tutorial we perform a verification of a probabilistic extrapolation nowcast \nusing MeteoSwiss radar data.\n\n\"\"\"\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pprint import pprint\nfrom pysteps import io, nowcasts, rcparams, verification\nfrom pysteps.motion.lucaskanade import dense_lucaskanade\nfrom pysteps.postprocessing import ensemblestats\nfrom pysteps.utils import conversion, dimension, transformation\nfrom pysteps.visualization import plot_precip_field\n\n\n###############################################################################\n# Read precipitation field\n# ------------------------\n#\n# First, we will import the sequence of MeteoSwiss (\"mch\") radar composites.\n# You need the pysteps-data archive downloaded and the pystepsrc file\n# configured with the data_source paths pointing to data folders.\n\n# Selected case\ndate = datetime.strptime(\"201607112100\", \"%Y%m%d%H%M\")\ndata_source = rcparams.data_sources[\"mch\"]\nn_ens_members = 20\nn_leadtimes = 6\nseed = 24\n\n###############################################################################\n# Load the data from the archive\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#\n# The data are upscaled to 2 km resolution to limit the memory usage and thus\n# be able to afford a larger number of ensemble members.\n\nroot_path = data_source[\"root_path\"]\npath_fmt = data_source[\"path_fmt\"]\nfn_pattern = data_source[\"fn_pattern\"]\nfn_ext = data_source[\"fn_ext\"]\nimporter_name = data_source[\"importer\"]\nimporter_kwargs = data_source[\"importer_kwargs\"]\ntimestep = data_source[\"timestep\"]\n\n# Find the radar files in the archive\nfns = io.find_by_date(\n    date, root_path, path_fmt, fn_pattern, fn_ext, timestep, num_prev_files=2\n)\n\n# Read the data from the archive\nimporter = io.get_method(importer_name, \"importer\")\nR, _, metadata = io.read_timeseries(fns, importer, **importer_kwargs)\n\n# Convert to rain rate\nR, metadata = conversion.to_rainrate(R, metadata)\n\n# Upscale data to 2 km\nR, metadata = dimension.aggregate_fields_space(R, metadata, 2000)\n\n# Plot the rainfall field\nplot_precip_field(R[-1, :, :], geodata=metadata)\nplt.show()\n\n# Log-transform the data to unit of dBR, set the threshold to 0.1 mm/h,\n# set the fill value to -15 dBR\nR, metadata = transformation.dB_transform(R, metadata, threshold=0.1, zerovalue=-15.0)\n\n# Set missing values with the fill value\nR[~np.isfinite(R)] = -15.0\n\n# Nicely print the metadata\npprint(metadata)\n\n###############################################################################\n# Forecast\n# --------\n#\n# We use the STEPS approach to produce a ensemble nowcast of precipitation fields.\n\n# Estimate the motion field\nV = dense_lucaskanade(R)\n\n# Perform the ensemble nowcast with STEPS\nnowcast_method = nowcasts.get_method(\"steps\")\nR_f = nowcast_method(\n    R[-3:, :, :],\n    V,\n    n_leadtimes,\n    n_ens_members,\n    n_cascade_levels=6,\n    R_thr=-10.0,\n    kmperpixel=2.0,\n    timestep=timestep,\n    decomp_method=\"fft\",\n    bandpass_filter_method=\"gaussian\",\n    noise_method=\"nonparametric\",\n    vel_pert_method=\"bps\",\n    mask_method=\"incremental\",\n    seed=seed,\n)\n\n# Back-transform to rain rates\nR_f = transformation.dB_transform(R_f, threshold=-10.0, inverse=True)[0]\n\n# Plot some of the realizations\nfig = plt.figure()\nfor i in range(4):\n    ax = fig.add_subplot(221 + i)\n    ax.set_title(\"Member %02d\" % i)\n    plot_precip_field(R_f[i, -1, :, :], geodata=metadata, colorbar=False, axis=\"off\")\nplt.tight_layout()\nplt.show()\n\n###############################################################################\n# Verification\n# ------------\n#\n# Pysteps includes a number of verification metrics to help users to analyze\n# the general characteristics of the nowcasts in terms of consistency and\n# quality (or goodness).\n# Here, we will verify our probabilistic forecasts using the ROC curve,\n# reliability diagrams, and rank histograms, as implemented in the verification\n# module of pysteps.\n\n# Find the files containing the verifying observations\nfns = io.archive.find_by_date(\n    date,\n    root_path,\n    path_fmt,\n    fn_pattern,\n    fn_ext,\n    timestep,\n    0,\n    num_next_files=n_leadtimes,\n)\n\n# Read the observations\nR_o, _, metadata_o = io.read_timeseries(fns, importer, **importer_kwargs)\n\n# Convert to mm/h\nR_o, metadata_o = conversion.to_rainrate(R_o, metadata_o)\n\n# Upscale data to 2 km\nR_o, metadata_o = dimension.aggregate_fields_space(R_o, metadata_o, 2000)\n\n# Compute the verification for the last lead time\n\n# compute the exceedance probability of 0.1 mm/h from the ensemble\nP_f = ensemblestats.excprob(R_f[:, -1, :, :], 0.1, ignore_nan=True)\n\n###############################################################################\n# ROC curve\n# ~~~~~~~~~\n\nroc = verification.ROC_curve_init(0.1, n_prob_thrs=10)\nverification.ROC_curve_accum(roc, P_f, R_o[-1, :, :])\nfig, ax = plt.subplots()\nverification.plot_ROC(roc, ax, opt_prob_thr=True)\nax.set_title(\"ROC curve (+%i min)\" % (n_leadtimes * timestep))\nplt.show()\n\n###############################################################################\n# Reliability diagram\n# ~~~~~~~~~~~~~~~~~~~\n\nreldiag = verification.reldiag_init(0.1)\nverification.reldiag_accum(reldiag, P_f, R_o[-1, :, :])\nfig, ax = plt.subplots()\nverification.plot_reldiag(reldiag, ax)\nax.set_title(\"Reliability diagram (+%i min)\" % (n_leadtimes * timestep))\nplt.show()\n\n###############################################################################\n# Rank histogram\n# ~~~~~~~~~~~~~~\n\nrankhist = verification.rankhist_init(R_f.shape[0], 0.1)\nverification.rankhist_accum(rankhist, R_f[:, -1, :, :], R_o[-1, :, :])\nfig, ax = plt.subplots()\nverification.plot_rankhist(rankhist, ax)\nax.set_title(\"Rank histogram (+%i min)\" % (n_leadtimes * timestep))\nplt.show()\n\n# sphinx_gallery_thumbnail_number = 5\n",
  "\"\"\"\nQ3D Extractor: PCB analysis\n---------------------------\nThis example shows how you can use PyAEDT to create a design in\nQ3D Extractor and run a simulation starting from an EDB Project.\n\"\"\"\n\n###############################################################################\n# Perform required imports\n# ~~~~~~~~~~~~~~~~~~~~~~~~\n# Perform required imports.\nimport os\nimport tempfile\nimport pyaedt\n\n\n###############################################################################\n# Setup project files and path\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Download of needed project file and setup of temporary project directory.\nproject_dir =  pyaedt.generate_unique_folder_name()\naedb_project = pyaedt.downloads.download_file('edb/ANSYS-HSD_V1.aedb',destination=project_dir)\n\nproject_name = pyaedt.generate_unique_name(\"HSD\")\noutput_edb = os.path.join(project_dir, project_name + '.aedb')\noutput_q3d = os.path.join(project_dir, project_name + '_q3d.aedt')\n\n\n###############################################################################\n# Open EDB\n# ~~~~~~~~\n# Open the edb project and created a cutout on the selected nets\n# before exporting to Q3D.\nedb = pyaedt.Edb(aedb_project, edbversion=\"2023.2\")\nedb.cutout([\"CLOCK_I2C_SCL\", \"CLOCK_I2C_SDA\"], [\"GND\"], output_aedb_path=output_edb,\n                              use_pyaedt_extent_computing=True, )\n\n\n###############################################################################\n# Identify pins position\n# ~~~~~~~~~~~~~~~~~~~~~~\n# Identify [x,y] pin locations on the components to define where to assign sources\n# and sinks for Q3D and append Z elevation.\n\npin_u13_scl = [i for i in edb.components[\"U13\"].pins.values() if i.net_name == \"CLOCK_I2C_SCL\"]\npin_u1_scl = [i for i in edb.components[\"U1\"].pins.values() if i.net_name == \"CLOCK_I2C_SCL\"]\npin_u13_sda = [i for i in edb.components[\"U13\"].pins.values() if i.net_name == \"CLOCK_I2C_SDA\"]\npin_u1_sda = [i for i in edb.components[\"U1\"].pins.values() if i.net_name == \"CLOCK_I2C_SDA\"]\n\n\n###############################################################################\n# Append Z Positions\n# ~~~~~~~~~~~~~~~~~~\n# Note: The factor 100 converts from \"meters\" to \"mm\"\n\nlocation_u13_scl = [i * 1000 for i in pin_u13_scl[0].position]\nlocation_u13_scl.append(edb.components[\"U13\"].upper_elevation * 1000)\n\nlocation_u1_scl = [i * 1000 for i in pin_u1_scl[0].position]\nlocation_u1_scl.append(edb.components[\"U1\"].upper_elevation * 1000)\n\nlocation_u13_sda = [i * 1000 for i in pin_u13_sda[0].position]\nlocation_u13_sda.append(edb.components[\"U13\"].upper_elevation * 1000)\n\nlocation_u1_sda = [i * 1000 for i in pin_u1_sda[0].position]\nlocation_u1_sda.append(edb.components[\"U1\"].upper_elevation * 1000)\n\n###############################################################################\n# Save and close Edb\n# ~~~~~~~~~~~~~~~~~~\n# Save, close Edb and open it in Hfss 3D Layout to generate the 3D model.\n\nedb.save_edb()\nedb.close_edb()\n\nh3d = pyaedt.Hfss3dLayout(output_edb, specified_version=\"2023.2\", non_graphical=True, new_desktop_session=True)\n\n###############################################################################\n# Export to Q3D\n# ~~~~~~~~~~~~~\n# Create a dummy setup and export the layout in Q3D.\n# keep_net_name will reassign Q3D nets names from Hfss 3D Layout.\nsetup = h3d.create_setup()\nsetup.export_to_q3d(output_q3d, keep_net_name=True)\nh3d.close_project()\n\n\n\n###############################################################################\n# Open Q3D\n# ~~~~~~~~\n# Launch the newly created q3d project and plot it.\n\nq3d = pyaedt.Q3d(output_q3d)\nq3d.plot(show=False, objects=[\"CLOCK_I2C_SCL\", \"CLOCK_I2C_SDA\"],\n         export_path=os.path.join(q3d.working_directory, \"Q3D.jpg\"), plot_air_objects=False)\n\n###############################################################################\n# Assing Source and Sink\n# ~~~~~~~~~~~~~~~~~~~~~~\n# Use previously calculated position to identify faces and\n# assign sources and sinks on nets.\n\n\n\nf1 = q3d.modeler.get_faceid_from_position(location_u13_scl, obj_name=\"CLOCK_I2C_SCL\")\nq3d.source(f1, net_name=\"CLOCK_I2C_SCL\")\nf1 = q3d.modeler.get_faceid_from_position(location_u13_sda, obj_name=\"CLOCK_I2C_SDA\")\nq3d.source(f1, net_name=\"CLOCK_I2C_SDA\")\nf1 = q3d.modeler.get_faceid_from_position(location_u1_scl, obj_name=\"CLOCK_I2C_SCL\")\nq3d.sink(f1, net_name=\"CLOCK_I2C_SCL\")\nf1 = q3d.modeler.get_faceid_from_position(location_u1_sda, obj_name=\"CLOCK_I2C_SDA\")\nq3d.sink(f1, net_name=\"CLOCK_I2C_SDA\")\n\n###############################################################################\n# Create Setup\n# ~~~~~~~~~~~~\n# Create a setup and a frequency sweep from DC to 2GHz.\n# Analyze project.\n\nsetup = q3d.create_setup()\nsetup.dc_enabled = True\nsetup.capacitance_enabled = False\nsweep = setup.add_sweep()\nsweep.add_subrange(\"LinearStep\", 0, end=2, count=0.05, unit=\"GHz\", save_single_fields=False, clear=True)\nsetup.analyze()\n\n###############################################################################\n# ACL Report\n# ~~~~~~~~~~\n# Compute ACL solutions and plot them.\n\ntraces_acl = q3d.post.available_report_quantities(quantities_category=\"ACL Matrix\")\nsolution = q3d.post.get_solution_data(traces_acl)\nsolution.plot()\n\n###############################################################################\n# ACR Report\n# ~~~~~~~~~~\n# Compute ACR solutions and plot them.\n\ntraces_acr = q3d.post.available_report_quantities(quantities_category=\"ACR Matrix\")\nsolution2 = q3d.post.get_solution_data(traces_acr)\nsolution2.plot()\n\n###############################################################################\n# Close AEDT\n# ~~~~~~~~~~\n# After the simulation completes, you can close AEDT or release it using the\n# ``release_desktop`` method. All methods provide for saving projects before closing.\nq3d.release_desktop()\n",
  "import blenderproc as bproc\nimport argparse\nimport os\nimport numpy as np\n\nparser = argparse.ArgumentParser()\nparser.add_argument('bop_parent_path', help=\"Path to the bop datasets parent directory\")\nparser.add_argument('cc_textures_path', default=\"resources/cctextures\", help=\"Path to downloaded cc textures\")\nparser.add_argument('output_dir', help=\"Path to where the final files will be saved \")\nparser.add_argument('--num_scenes', type=int, default=2000, help=\"How many scenes with 25 images each to generate\")\nargs = parser.parse_args()\n\nbproc.init()\n\n# load bop objects into the scene\ntarget_bop_objs = bproc.loader.load_bop_objs(bop_dataset_path = os.path.join(args.bop_parent_path, 'itodd'), mm2m = True)\n\n# load distractor bop objects\ntless_dist_bop_objs = bproc.loader.load_bop_objs(bop_dataset_path = os.path.join(args.bop_parent_path, 'tless'), model_type = 'cad', mm2m = True)\n\n# load BOP datset intrinsics\nbproc.loader.load_bop_intrinsics(bop_dataset_path = os.path.join(args.bop_parent_path, 'itodd'))\n\n# set shading and hide objects\nfor obj in (target_bop_objs + tless_dist_bop_objs):\n    obj.set_shading_mode('auto')\n    obj.hide(True)\n    \n# create room\nroom_planes = [bproc.object.create_primitive('PLANE', scale=[2, 2, 1]),\n               bproc.object.create_primitive('PLANE', scale=[2, 2, 1], location=[0, -2, 2], rotation=[-1.570796, 0, 0]),\n               bproc.object.create_primitive('PLANE', scale=[2, 2, 1], location=[0, 2, 2], rotation=[1.570796, 0, 0]),\n               bproc.object.create_primitive('PLANE', scale=[2, 2, 1], location=[2, 0, 2], rotation=[0, -1.570796, 0]),\n               bproc.object.create_primitive('PLANE', scale=[2, 2, 1], location=[-2, 0, 2], rotation=[0, 1.570796, 0])]\nfor plane in room_planes:\n    plane.enable_rigidbody(False, collision_shape='BOX', mass=1.0, friction = 100.0, linear_damping = 0.99, angular_damping = 0.99)\n\n# sample light color and strenght from ceiling\nlight_plane = bproc.object.create_primitive('PLANE', scale=[3, 3, 1], location=[0, 0, 10])\nlight_plane.set_name('light_plane')\nlight_plane_material = bproc.material.create('light_material')\n\n# sample point light on shell\nlight_point = bproc.types.Light()\nlight_point.set_energy(20)\n\n# load cc_textures\ncc_textures = bproc.loader.load_ccmaterials(args.cc_textures_path)\n\n# Define a function that samples 6-DoF poses\ndef sample_pose_func(obj: bproc.types.MeshObject):\n    min = np.random.uniform([-0.3, -0.3, 0.0], [-0.2, -0.2, 0.0])\n    max = np.random.uniform([0.2, 0.2, 0.4], [0.3, 0.3, 0.6])\n    obj.set_location(np.random.uniform(min, max))\n    obj.set_rotation_euler(bproc.sampler.uniformSO3())\n    \n# activate depth rendering without antialiasing and set amount of samples for color rendering\nbproc.renderer.enable_depth_output(activate_antialiasing=False)\nbproc.renderer.set_max_amount_of_samples(50)\n\nfor i in range(args.num_scenes):\n\n    # Sample bop objects for a scene\n    sampled_target_bop_objs = list(np.random.choice(target_bop_objs, size=25, replace=False))\n    sampled_distractor_bop_objs = list(np.random.choice(tless_dist_bop_objs, size=5, replace=False))\n\n    # Randomize materials and set physics\n    for obj in (sampled_target_bop_objs + sampled_distractor_bop_objs):        \n        mat = obj.get_materials()[0]\n        if obj.get_cp(\"bop_dataset_name\") in ['itodd', 'tless']:\n            grey_col = np.random.uniform(0.1, 0.7)   \n            mat.set_principled_shader_value(\"Base Color\", [grey_col, grey_col, grey_col, 1])      \n        mat.set_principled_shader_value(\"Roughness\", np.random.uniform(0, 0.5))\n        if obj.get_cp(\"bop_dataset_name\") == 'itodd':  \n            mat.set_principled_shader_value(\"Specular\", np.random.uniform(0.3, 1.0))\n            mat.set_principled_shader_value(\"Metallic\", np.random.uniform(0, 1.0))\n        if obj.get_cp(\"bop_dataset_name\") == 'tless':\n            mat.set_principled_shader_value(\"Metallic\", np.random.uniform(0, 0.5))\n            \n        obj.enable_rigidbody(True, mass=1.0, friction = 100.0, linear_damping = 0.99, angular_damping = 0.99, collision_margin=0.0005)\n        obj.hide(False)\n    \n    # Sample two light sources\n    light_plane_material.make_emissive(emission_strength=np.random.uniform(0.1,0.5), \n                                    emission_color=np.random.uniform([0.5, 0.5, 0.5, 1.0], [1.0, 1.0, 1.0, 1.0]))  \n    light_plane.replace_materials(light_plane_material)\n    light_point.set_color(np.random.uniform([0.5,0.5,0.5],[1,1,1]))\n    location = bproc.sampler.shell(center = [0, 0, 0], radius_min = 0.5, radius_max = 1.5,\n                            elevation_min = 5, elevation_max = 89)\n    light_point.set_location(location)\n\n    # sample CC Texture and assign to room planes\n    random_cc_texture = np.random.choice(cc_textures)\n    for plane in room_planes:\n        plane.replace_materials(random_cc_texture)\n\n\n    # Sample object poses and check collisions \n    bproc.object.sample_poses(objects_to_sample = sampled_target_bop_objs + sampled_distractor_bop_objs,\n                            sample_pose_func = sample_pose_func, \n                            max_tries = 1000)\n            \n    # Physics Positioning\n    bproc.object.simulate_physics_and_fix_final_poses(min_simulation_time=3,\n                                                      max_simulation_time=10,\n                                                      check_object_interval=1,\n                                                      substeps_per_frame = 50,\n                                                      solver_iters=25)\n\n    # BVH tree used for camera obstacle checks\n    bop_bvh_tree = bproc.object.create_bvh_tree_multi_objects(sampled_target_bop_objs + sampled_distractor_bop_objs)\n\n    cam_poses = 0\n    while cam_poses < 25:\n        # Sample location\n        location = bproc.sampler.shell(center = [0, 0, 0],\n                                radius_min = 0.64,\n                                radius_max = 0.78,\n                                elevation_min = 5,\n                                elevation_max = 89)\n        # Determine point of interest in scene as the object closest to the mean of a subset of objects\n        poi = bproc.object.compute_poi(np.random.choice(sampled_target_bop_objs, size=15, replace=False))\n        # Compute rotation based on vector going from location towards poi\n        rotation_matrix = bproc.camera.rotation_from_forward_vec(poi - location, inplane_rot=np.random.uniform(-3.14159, 3.14159))\n        # Add homog cam pose based on location an rotation\n        cam2world_matrix = bproc.math.build_transformation_mat(location, rotation_matrix)\n        \n        # Check that obstacles are at least 0.3 meter away from the camera and make sure the view interesting enough\n        if bproc.camera.perform_obstacle_in_view_check(cam2world_matrix, {\"min\": 0.3}, bop_bvh_tree):\n            # Persist camera pose\n            bproc.camera.add_camera_pose(cam2world_matrix, frame=cam_poses)\n            cam_poses += 1\n\n    # render the whole pipeline\n    data = bproc.renderer.render()\n\n    # Write data in bop format\n    bproc.writer.write_bop(os.path.join(args.output_dir, 'bop_data'),\n                           target_objects = sampled_target_bop_objs,\n                           dataset = 'itodd',\n                           depth_scale = 0.1,\n                           depths = data[\"depth\"],\n                           colors = data[\"colors\"], \n                           color_file_format = \"JPEG\",\n                           ignore_dist_thres = 10)\n    \n    for obj in (sampled_target_bop_objs + sampled_distractor_bop_objs):      \n        obj.disable_rigidbody()\n        obj.hide(True)\n",
  "#!/usr/bin/env python\n#\n# Author: Qiming Sun <osirpt.sun@gmail.com>\n#\n\n'''\nExamples for transform_ci function to transform FCI wave functions with\nrespect to the change of orbital space:\n1. Transform wavefunction wrt orbital rotation/transformation.\n2. Transfer a FCI wave function from a smaller orbital space to the wavefunction\nof a large orbital space.\n'''\n\nimport numpy as np\nimport pyscf\nfrom pyscf import fci, lib\n\n#\n# 1. Transform wavefunction wrt orbital rotation/transformation.\n#\n\nmyhf1 = pyscf.M(atom='H 0 0 0; F 0 0 1.1', basis='6-31g', verbose=0).RHF().run()\ne1, ci1 = fci.FCI(myhf1.mol, myhf1.mo_coeff).kernel()\nprint('FCI energy of mol1', e1)\n\nmyhf2 = pyscf.M(atom='H 0 0 0; F 0 0 1.2', basis='6-31g', verbose=0).RHF().run()\n\ns12 = pyscf.gto.intor_cross('cint1e_ovlp_sph', myhf1.mol, myhf2.mol)\ns12 = myhf1.mo_coeff.T.dot(s12).dot(myhf2.mo_coeff)\nnelec = myhf2.mol.nelectron\nci2 = fci.addons.transform_ci(ci1, nelec, s12)\n\nprint('alpha-string, beta-string,  CI coefficients')\nnorb = myhf2.mo_coeff.shape[1]\nfor c,stra,strb in fci.addons.large_ci(ci2, norb, nelec):\n    print(stra, strb, c)\n\n#\n# 2. Transfer wavefunction from small orbital space to large orbital space\n#\n\nmol = pyscf.M(atom=['H 0 0 %f'%x for x in range(6)], basis='6-31g')\nmf = mol.RHF().run()\nh1 = mf.mo_coeff.T.dot(mf.get_hcore()).dot(mf.mo_coeff)\nh2 = pyscf.lib.einsum('pqrs,pi,qj,rk,sl->ijkl', mf.mol.intor('int2e'),\n                      mf.mo_coeff, mf.mo_coeff, mf.mo_coeff, mf.mo_coeff)\n\nnorb = 6\nnelec = (3, 3)\ncivec = fci.FCI(mol).kernel(h1[:norb,:norb], h2[:norb,:norb,:norb,:norb], norb, nelec,\n                            ecore=mol.energy_nuc())[1]\n\n#\n# Expand to 8 orbitals. It can be done with a 6x8 transformation matrix which\n# maps the old orbitals to the new orbitals.\n#\nu = np.zeros((6, 8))\nfor i in range(6):\n    u[i, i] = 1\ncivec1 = fci.addons.transform_ci(civec, nelec, u)\nprint(civec1.shape)  # == (56, 56)\n\n#\n# Compare to the wavefunction obtained from FCI solver. They should be very\n# closed since the determinants of high excitations are less important for the\n# ground state.\n#\nnorb = 8\nnelec = (3, 3)\ncivec2 = fci.FCI(mol).kernel(h1[:norb,:norb], h2[:norb,:norb,:norb,:norb], norb, nelec,\n                              ecore=mol.energy_nuc())[1]\nprint(np.dot(civec1.ravel(), civec2.ravel()))\n\n#\n# The FCI coefficients are associated with the determinants in strings\n# representations. We can find the determinants' addresses in each space and\n# sort the determinants accordingly. This sorting algorithm is more efficient\n# than the transform_ci function. However, permutation parity have to be\n# handled carefully if orbitals are flipped in the other space. If there is no\n# orbital flipping in the second orbital space, the code below, without\n# the phase due to parity, can be used to transform the FCI wavefunction.\n#\n# Assuming the following orbital mappings between small space and large space:\n#   small space -> large space\n#       0       ->      0\n#       1       ->      1\n#       2       ->      2\n#       3       ->      4\n#       4       ->      5\n#       5       ->      7\n#                       3\n#                       6\n\n#\n# first we get the address of each determinant of CI(6,6) in CI(6,8)\n#\nstrsa = fci.cistring.make_strings([0,1,2,4,5,7], nelec[0])\nstrsb = fci.cistring.make_strings([0,1,2,4,5,7], nelec[1])\naddrsa = fci.cistring.strs2addr(8, nelec[0], strsa)\naddrsb = fci.cistring.strs2addr(8, nelec[1], strsa)\ncivec1 = np.zeros_like(civec2)\ncivec1[addrsa[:,None], addrsb] = civec\n\n#\n# Check against the transform_ci function\n#\nu = np.zeros((6, 8))\nu[0,0] = 1\nu[1,1] = 1\nu[2,2] = 1\nu[3,4] = 1\nu[4,5] = 1\nu[5,7] = 1\ncivec1_ref = fci.addons.transform_ci(civec, nelec, u)\nprint(np.allclose(civec1, civec1_ref))\n\n",
  "\"\"\"\n3D DC inversion of Dipole Dipole array\n======================================\n\nThis is an example for 3D DC Inversion. The model consists of 2 spheres,\none conductive, the other one resistive compared to the background.\n\nWe restrain the inversion to the Core Mesh through the use an Active Cells\nmapping that we combine with an exponetial mapping to invert\nin log conductivity space. Here mapping,  :math:`\\\\mathcal{M}`,\nindicates transformation of our model to a different space:\n\n.. math::\n    \\\\sigma = \\\\mathcal{M}(\\\\mathbf{m})\n\nFollowing example will show you how user can implement a 3D DC inversion.\n\"\"\"\n\nfrom SimPEG import (\n    Mesh, Maps, Utils,\n    DataMisfit, Regularization, Optimization,\n    InvProblem, Directives, Inversion\n)\nfrom SimPEG.EM.Static import DC, Utils as DCUtils\nimport numpy as np\nimport matplotlib.pyplot as plt\ntry:\n    from pymatsolver import Pardiso as Solver\nexcept ImportError:\n    from SimPEG import SolverLU as Solver\n\nnp.random.seed(12345)\n\n# 3D Mesh\n#########\n\n# Cell sizes\ncsx, csy, csz = 1., 1., 0.5\n# Number of core cells in each direction\nncx, ncy, ncz = 41, 31, 21\n# Number of padding cells to add in each direction\nnpad = 7\n# Vectors of cell lengths in each direction with padding\nhx = [(csx, npad, -1.5), (csx, ncx), (csx, npad, 1.5)]\nhy = [(csy, npad, -1.5), (csy, ncy), (csy, npad, 1.5)]\nhz = [(csz, npad, -1.5), (csz, ncz)]\n# Create mesh and center it\nmesh = Mesh.TensorMesh([hx, hy, hz], x0=\"CCN\")\n\n# 2-spheres Model Creation\n##########################\n\n# Spheres parameters\nx0, y0, z0, r0 = -6., 0., -3.5, 3.\nx1, y1, z1, r1 = 6., 0., -3.5, 3.\n\n# ln conductivity\nln_sigback = -5.\nln_sigc = -3.\nln_sigr = -6.\n\n# Define model\n# Background\nmtrue = ln_sigback * np.ones(mesh.nC)\n\n# Conductive sphere\ncsph = (np.sqrt((mesh.gridCC[:, 0] - x0)**2. + (mesh.gridCC[:, 1] - y0)**2. +\n                (mesh.gridCC[:, 2] - z0)**2.)) < r0\nmtrue[csph] = ln_sigc * np.ones_like(mtrue[csph])\n\n# Resistive Sphere\nrsph = (np.sqrt((mesh.gridCC[:, 0] - x1)**2. + (mesh.gridCC[:, 1] - y1)**2. +\n                (mesh.gridCC[:, 2] - z1)**2.)) < r1\nmtrue[rsph] = ln_sigr * np.ones_like(mtrue[rsph])\n\n# Extract Core Mesh\nxmin, xmax = -20., 20.\nymin, ymax = -15., 15.\nzmin, zmax = -10., 0.\nxyzlim = np.r_[[[xmin, xmax], [ymin, ymax], [zmin, zmax]]]\nactind, meshCore = Utils.meshutils.ExtractCoreMesh(xyzlim, mesh)\n\n\n# Function to plot cylinder border\ndef getCylinderPoints(xc, zc, r):\n    xLocOrig1 = np.arange(-r, r + r / 10., r / 10.)\n    xLocOrig2 = np.arange(r, -r - r / 10., -r / 10.)\n    # Top half of cylinder\n    zLoc1 = np.sqrt(-xLocOrig1**2. + r**2.) + zc\n    # Bottom half of cylinder\n    zLoc2 = -np.sqrt(-xLocOrig2**2. + r**2.) + zc\n    # Shift from x = 0 to xc\n    xLoc1 = xLocOrig1 + xc * np.ones_like(xLocOrig1)\n    xLoc2 = xLocOrig2 + xc * np.ones_like(xLocOrig2)\n\n    topHalf = np.vstack([xLoc1, zLoc1]).T\n    topHalf = topHalf[0:-1, :]\n    bottomHalf = np.vstack([xLoc2, zLoc2]).T\n    bottomHalf = bottomHalf[0:-1, :]\n\n    cylinderPoints = np.vstack([topHalf, bottomHalf])\n    cylinderPoints = np.vstack([cylinderPoints, topHalf[0, :]])\n    return cylinderPoints\n\n\n# Setup a synthetic Dipole-Dipole Survey\n# Line 1\nxmin, xmax = -15., 15.\nymin, ymax = 0., 0.\nzmin, zmax = 0, 0\nendl = np.array([[xmin, ymin, zmin], [xmax, ymax, zmax]])\nsurvey1 = DCUtils.gen_DCIPsurvey(endl, \"dipole-dipole\", dim=mesh.dim,\n                                 a=3, b=3, n=8)\n\n# Line 2\nxmin, xmax = -15., 15.\nymin, ymax = 5., 5.\nzmin, zmax = 0, 0\nendl = np.array([[xmin, ymin, zmin], [xmax, ymax, zmax]])\nsurvey2 = DCUtils.gen_DCIPsurvey(endl, \"dipole-dipole\", dim=mesh.dim,\n                                 a=3, b=3, n=8)\n\n# Line 3\nxmin, xmax = -15., 15.\nymin, ymax = -5., -5.\nzmin, zmax = 0, 0\nendl = np.array([[xmin, ymin, zmin], [xmax, ymax, zmax]])\nsurvey3 = DCUtils.gen_DCIPsurvey(endl, \"dipole-dipole\", dim=mesh.dim,\n                                 a=3, b=3, n=8)\n\n# Concatenate lines\nsurvey = DC.Survey(survey1.srcList + survey2.srcList + survey3.srcList)\n\n# Setup Problem with exponential mapping and Active cells only in the core mesh\nexpmap = Maps.ExpMap(mesh)\nmapactive = Maps.InjectActiveCells(mesh=mesh, indActive=actind,\n                                   valInactive=-5.)\nmapping = expmap * mapactive\nproblem = DC.Problem3D_CC(mesh, sigmaMap=mapping)\nproblem.pair(survey)\nproblem.Solver = Solver\n\nsurvey.dpred(mtrue[actind])\nsurvey.makeSyntheticData(mtrue[actind], std=0.05, force=True)\n\n\n# Tikhonov Inversion\n####################\n\n# Initial Model\nm0 = np.median(ln_sigback) * np.ones(mapping.nP)\n# Data Misfit\ndmis = DataMisfit.l2_DataMisfit(survey)\n# Regularization\nregT = Regularization.Simple(mesh, indActive=actind, alpha_s=1e-6,\n                             alpha_x=1., alpha_y=1., alpha_z=1.)\n\n# Optimization Scheme\nopt = Optimization.InexactGaussNewton(maxIter=10)\n\n# Form the problem\nopt.remember('xc')\ninvProb = InvProblem.BaseInvProblem(dmis, regT, opt)\n\n# Directives for Inversions\nbeta = Directives.BetaEstimate_ByEig(beta0_ratio=1e+1)\nTarget = Directives.TargetMisfit()\nbetaSched = Directives.BetaSchedule(coolingFactor=5., coolingRate=2)\n\ninv = Inversion.BaseInversion(invProb, directiveList=[beta, Target,\n                                                      betaSched])\n# Run Inversion\nminv = inv.run(m0)\n\n# Final Plot\n############\n\nfig, ax = plt.subplots(2, 2, figsize=(12, 6))\nax = Utils.mkvc(ax)\n\ncyl0v = getCylinderPoints(x0, z0, r0)\ncyl1v = getCylinderPoints(x1, z1, r1)\n\ncyl0h = getCylinderPoints(x0, y0, r0)\ncyl1h = getCylinderPoints(x1, y1, r1)\n\nclim = [(mtrue[actind]).min(), (mtrue[actind]).max()]\n\ndat = meshCore.plotSlice(((mtrue[actind])), ax=ax[0], normal='Y', clim=clim,\n                         ind=int(ncy / 2))\nax[0].set_title('Ground Truth, Vertical')\nax[0].set_aspect('equal')\n\nmeshCore.plotSlice((minv), ax=ax[1], normal='Y', clim=clim, ind=int(ncy / 2))\nax[1].set_aspect('equal')\nax[1].set_title('Inverted Model, Vertical')\n\nmeshCore.plotSlice(((mtrue[actind])), ax=ax[2], normal='Z', clim=clim,\n                   ind=int(ncz / 2))\nax[2].set_title('Ground Truth, Horizontal')\nax[2].set_aspect('equal')\n\nmeshCore.plotSlice((minv), ax=ax[3], normal='Z', clim=clim, ind=int(ncz / 2))\nax[3].set_title('Inverted Model, Horizontal')\nax[3].set_aspect('equal')\n\nfor i in range(2):\n    ax[i].plot(cyl0v[:, 0], cyl0v[:, 1], 'k--')\n    ax[i].plot(cyl1v[:, 0], cyl1v[:, 1], 'k--')\nfor i in range(2, 4):\n    ax[i].plot(cyl1h[:, 0], cyl1h[:, 1], 'k--')\n    ax[i].plot(cyl0h[:, 0], cyl0h[:, 1], 'k--')\n\nfig.subplots_adjust(right=0.8)\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\ncb = plt.colorbar(dat[0], ax=cbar_ax)\ncb.set_label('ln conductivity')\n\ncbar_ax.axis('off')\n\nplt.show()\n",
  "'''randoms-from-listmode estimation demo\n\nUsage:\n  randoms_from_listmode [--help | options]\n\nThis demo illustrates how to get a good estimate of the randoms from a list mode file.\nIt compares the result with the original delayed coincidences.\n\nThis demo will therefore only work with list mode data where the delayeds are stored.\n\nOptions:\n  -p <path>, --path=<path>     path to data files, defaults to data/examples/PET/mMR\n                               subfolder of SIRF root folder\n  -l <list>, --list=<list>     listmode file [default: list.l.hdr]\n  -o <sino>, --sino=<sino>     sinograms file prefix [default: sinograms]\n  -r <rand>, --rand=<rand>     randoms file [default: randoms]\n  -t <tmpl>, --tmpl=<tmpl>     raw data template [default: mMR_template_span11_small.hs]\n  -i <int>, --interval=<int>   scanning time interval to convert as string '(a,b)'\n                               (no space after comma) [default: (0,100)]\n  -e <engn>, --engine=<engn>   reconstruction engine [default: STIR]\n  -s <stsc>, --storage=<stsc>  acquisition data storage scheme [default: file]\n  --non-interactive            do not show plots\n'''\n\n## SyneRBI Synergistic Image Reconstruction Framework (SIRF)\n## Copyright 2018 - 2019 Rutherford Appleton Laboratory STFC\n## Copyright 2018 University College London.\n##\n## This is software developed for the Collaborative Computational\n## Project in Synergistic Reconstruction for Biomedical Imaging (formerly CCP PETMR)\n## (http://www.ccpsynerbi.ac.uk/).\n##\n## Licensed under the Apache License, Version 2.0 (the \"License\");\n##   you may not use this file except in compliance with the License.\n##   You may obtain a copy of the License at\n##       http://www.apache.org/licenses/LICENSE-2.0\n##   Unless required by applicable law or agreed to in writing, software\n##   distributed under the License is distributed on an \"AS IS\" BASIS,\n##   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n##   See the License for the specific language governing permissions and\n##   limitations under the License.\n\n__version__ = '0.1.0'\nfrom docopt import docopt\nargs = docopt(__doc__, version=__version__)\n\nfrom ast import literal_eval\n\nfrom pUtilities import show_3D_array\n\nimport numpy as np\n\n# import engine module\nexec('from sirf.' + args['--engine'] + ' import *')\n\n\n# process command-line options\ndata_path = args['--path']\nif data_path is None:\n    # default to data/examples/PET/mMR\n    # Note: seem to need / even on Windows\n    #data_path = os.path.join(examples_data_path('PET'), 'mMR')\n    data_path = examples_data_path('PET') + '/mMR'\nlist_file = args['--list']\nsino_file = args['--sino']\nrand_file = args['--rand']\ntmpl_file = args['--tmpl']\nlist_file = existing_filepath(data_path, list_file)\ntmpl_file = existing_filepath(data_path, tmpl_file)\ninterval = literal_eval(args['--interval'])\nstorage = args['--storage']\nshow_plot = not args['--non-interactive']\n\n\ndef main():\n\n    # direct all engine's messages to files\n    msg_red = MessageRedirector('info.txt', 'warn.txt', 'errr.txt')\n\n    # select acquisition data storage scheme\n    AcquisitionData.set_storage_scheme(storage)\n\n    # create listmode-to-sinograms converter object\n    lm2sino = ListmodeToSinograms()\n\n    # set input, output and template files\n    lm2sino.set_input(list_file)\n    lm2sino.set_output_prefix(sino_file)\n    lm2sino.set_template(tmpl_file)\n\n    # set interval\n    lm2sino.set_time_interval(interval[0], interval[1])\n\n    # set flags such that we only get the delayed coincidences\n    lm2sino.flag_on('store_delayeds')\n    lm2sino.flag_off('store_prompts')\n    \n    # set up the converter\n    lm2sino.set_up()\n\n    # convert\n    lm2sino.process()\n\n    # get access to the sinograms\n    delayeds_acq_data = lm2sino.get_output()\n    \n    # estimate the randoms from the delayeds via Maximum Likelihood estimation\n    # This will take at least a few seconds\n    randoms_estimate_acq_data = lm2sino.estimate_randoms();\n    randoms_estimate_acq_data.write(rand_file)\n    \n    # copy the acquisition data into Python arrays\n    delayeds_acq_array = delayeds_acq_data.as_array()\n    randoms_estimate_acq_array = randoms_estimate_acq_data.as_array()\n    acq_dim = delayeds_acq_array.shape\n    print('acquisition data dimensions: %dx%dx%dx%d' % acq_dim)\n    print('The total number of delayed coincidences and estimated randoms have to be very similar.')\n    print('Let us check this:')\n    print('total delayeds: %.1f, total estimated randoms: %.1f' % (delayeds_acq_array.sum(), randoms_estimate_acq_array.sum()))\n    print('Max values should be somewhat similar, but this depends on statistics of course.')\n    print('max delayeds: %f, max estimated randoms: %f' % (delayeds_acq_array.max(), randoms_estimate_acq_array.max()))\n\n    print('A single sinogram (this will look very different for noisy data)')\n    z = acq_dim[1]//2\n    if show_plot:\n        show_3D_array(np.stack((delayeds_acq_array[0,z,:,:], randoms_estimate_acq_array[0,z,:,:])), titles=('raw delayeds', ' estimated randoms'))\n        pylab.show()\n\n\ntry:\n    main()\n    print('\\n=== done with %s' % __file__)\n\nexcept error as err:\n    print('%s' % err.value)\n",
  "# -*- coding: utf-8 -*-\n\"\"\"\nSimple example of subclassing GraphItem.\n\"\"\"\n\nimport initExample ## Add path to library (just for examples; you do not need this)\n\nimport pyqtgraph as pg\nfrom pyqtgraph.Qt import QtCore, QtGui\nimport numpy as np\n\n# Enable antialiasing for prettier plots\npg.setConfigOptions(antialias=True)\n\nw = pg.GraphicsLayoutWidget(show=True)\nw.setWindowTitle('pyqtgraph example: CustomGraphItem')\nv = w.addViewBox()\nv.setAspectLocked()\n\nclass Graph(pg.GraphItem):\n    def __init__(self):\n        self.dragPoint = None\n        self.dragOffset = None\n        self.textItems = []\n        pg.GraphItem.__init__(self)\n        self.scatter.sigClicked.connect(self.clicked)\n        \n    def setData(self, **kwds):\n        self.text = kwds.pop('text', [])\n        self.data = kwds\n        if 'pos' in self.data:\n            npts = self.data['pos'].shape[0]\n            self.data['data'] = np.empty(npts, dtype=[('index', int)])\n            self.data['data']['index'] = np.arange(npts)\n        self.setTexts(self.text)\n        self.updateGraph()\n        \n    def setTexts(self, text):\n        for i in self.textItems:\n            i.scene().removeItem(i)\n        self.textItems = []\n        for t in text:\n            item = pg.TextItem(t)\n            self.textItems.append(item)\n            item.setParentItem(self)\n        \n    def updateGraph(self):\n        pg.GraphItem.setData(self, **self.data)\n        for i,item in enumerate(self.textItems):\n            item.setPos(*self.data['pos'][i])\n        \n        \n    def mouseDragEvent(self, ev):\n        if ev.button() != QtCore.Qt.LeftButton:\n            ev.ignore()\n            return\n        \n        if ev.isStart():\n            # We are already one step into the drag.\n            # Find the point(s) at the mouse cursor when the button was first \n            # pressed:\n            pos = ev.buttonDownPos()\n            pts = self.scatter.pointsAt(pos)\n            if len(pts) == 0:\n                ev.ignore()\n                return\n            self.dragPoint = pts[0]\n            ind = pts[0].data()[0]\n            self.dragOffset = self.data['pos'][ind] - pos\n        elif ev.isFinish():\n            self.dragPoint = None\n            return\n        else:\n            if self.dragPoint is None:\n                ev.ignore()\n                return\n        \n        ind = self.dragPoint.data()[0]\n        self.data['pos'][ind] = ev.pos() + self.dragOffset\n        self.updateGraph()\n        ev.accept()\n        \n    def clicked(self, pts):\n        print(\"clicked: %s\" % pts)\n\n\ng = Graph()\nv.addItem(g)\n\n## Define positions of nodes\npos = np.array([\n    [0,0],\n    [10,0],\n    [0,10],\n    [10,10],\n    [5,5],\n    [15,5]\n    ], dtype=float)\n    \n## Define the set of connections in the graph\nadj = np.array([\n    [0,1],\n    [1,3],\n    [3,2],\n    [2,0],\n    [1,5],\n    [3,5],\n    ])\n    \n## Define the symbol to use for each node (this is optional)\nsymbols = ['o','o','o','o','t','+']\n\n## Define the line style for each connection (this is optional)\nlines = np.array([\n    (255,0,0,255,1),\n    (255,0,255,255,2),\n    (255,0,255,255,3),\n    (255,255,0,255,2),\n    (255,0,0,255,1),\n    (255,255,255,255,4),\n    ], dtype=[('red',np.ubyte),('green',np.ubyte),('blue',np.ubyte),('alpha',np.ubyte),('width',float)])\n\n## Define text to show next to each symbol\ntexts = [\"Point %d\" % i for i in range(6)]\n\n## Update the graph\ng.setData(pos=pos, adj=adj, pen=lines, size=1, symbol=symbols, pxMode=False, text=texts)\n\n\n\n\nif __name__ == '__main__':\n    pg.exec()\n",
  "# Copyright 2018 DeepMind Technologies Limited. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Example running BC on continuous control tasks.\n\nThe network structure consists in a 3-layer MLP with ReLU activation\nand dropout.\n\"\"\"\n\nfrom typing import Callable, Iterator, Tuple\n\nfrom absl import flags\nfrom acme import specs\nfrom acme import types\nfrom acme.agents.jax import actor_core as actor_core_lib\nfrom acme.agents.jax import bc\nfrom acme.datasets import tfds\nimport helpers\nfrom absl import app\nfrom acme.jax import experiments\nfrom acme.jax import types as jax_types\nfrom acme.jax import utils\nfrom acme.utils import lp_utils\nimport dm_env\nimport haiku as hk\nimport launchpad as lp\nimport numpy as np\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_bool(\n    'run_distributed', True, 'Should an agent be executed in a distributed '\n    'way. If False, will run single-threaded.')\n# Agent flags\nflags.DEFINE_string('env_name', 'HalfCheetah-v2', 'What environment to run')\nflags.DEFINE_integer('num_demonstrations', 11,\n                     'Number of demonstration trajectories.')\nflags.DEFINE_integer('num_bc_steps', 100_000, 'Number of bc learning steps.')\nflags.DEFINE_integer('num_steps', 0, 'Number of environment steps.')\nflags.DEFINE_integer('batch_size', 64, 'Batch size.')\nflags.DEFINE_float('learning_rate', 1e-4, 'Optimizer learning rate.')\nflags.DEFINE_float('dropout_rate', 0.1, 'Dropout rate of bc network.')\nflags.DEFINE_integer('num_layers', 3, 'Num layers of bc network.')\nflags.DEFINE_integer('num_units', 256, 'Num units of bc network layers.')\nflags.DEFINE_integer('eval_every', 5000, 'Evaluation period.')\nflags.DEFINE_integer('evaluation_episodes', 10, 'Evaluation episodes.')\nflags.DEFINE_integer('seed', 0, 'Random seed for learner and evaluator.')\n\n\ndef _make_demonstration_dataset_factory(\n    dataset_name: str, num_demonstrations: int,\n    environment_spec: specs.EnvironmentSpec, batch_size: int\n) -> Callable[[jax_types.PRNGKey], Iterator[types.Transition]]:\n  \"\"\"Returns the demonstration dataset factory for the given dataset.\"\"\"\n\n  def demonstration_dataset_factory(\n      random_key: jax_types.PRNGKey) -> Iterator[types.Transition]:\n    \"\"\"Returns an iterator of demonstration samples.\"\"\"\n\n    transitions_iterator = tfds.get_tfds_dataset(\n        dataset_name, num_demonstrations, env_spec=environment_spec)\n    return tfds.JaxInMemoryRandomSampleIterator(\n        transitions_iterator, key=random_key, batch_size=batch_size)\n\n  return demonstration_dataset_factory\n\n\ndef _make_environment_factory(env_name: str) -> jax_types.EnvironmentFactory:\n  \"\"\"Returns the environment factory for the given environment.\"\"\"\n\n  def environment_factory(seed: int) -> dm_env.Environment:\n    del seed\n    return helpers.make_environment(task=env_name)\n\n  return environment_factory\n\n\ndef _make_network_factory(\n    shift: Tuple[np.float64], scale: Tuple[np.float64], num_layers: int,\n    num_units: int,\n    dropout_rate: float) -> Callable[[specs.EnvironmentSpec], bc.BCNetworks]:\n  \"\"\"Returns the factory of networks to be used by the agent.\n\n  Args:\n    shift: Shift of the observations in demonstrations.\n    scale: Scale of the observations in demonstrations.\n    num_layers: Number of layers of the BC network.\n    num_units: Number of units of the BC network.\n    dropout_rate: Dropout rate of the BC network.\n\n  Returns:\n    Network factory.\n  \"\"\"\n\n  def network_factory(spec: specs.EnvironmentSpec) -> bc.BCNetworks:\n    \"\"\"Creates the network used by the agent.\"\"\"\n\n    action_spec = spec.actions\n    num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n    def actor_fn(obs, is_training=False, key=None):\n      obs += shift\n      obs *= scale\n      hidden_layers = [num_units] * num_layers\n      mlp = hk.Sequential([\n          hk.nets.MLP(hidden_layers + [num_dimensions]),\n      ])\n      if is_training:\n        return mlp(obs, dropout_rate=dropout_rate, rng=key)\n      else:\n        return mlp(obs)\n\n    policy = hk.without_apply_rng(hk.transform(actor_fn))\n\n    # Create dummy observations to create network parameters.\n    dummy_obs = utils.zeros_like(spec.observations)\n    dummy_obs = utils.add_batch_dim(dummy_obs)\n\n    policy_network = bc.BCPolicyNetwork(lambda key: policy.init(key, dummy_obs),\n                                        policy.apply)\n\n    return bc.BCNetworks(policy_network=policy_network)\n\n  return network_factory\n\n\ndef build_experiment_config() -> experiments.OfflineExperimentConfig[\n    bc.BCNetworks, actor_core_lib.FeedForwardPolicy, types.Transition]:\n  \"\"\"Returns a config for BC experiments.\"\"\"\n\n  # Create an environment, grab the spec, and use it to create networks.\n  environment = helpers.make_environment(task=FLAGS.env_name)\n  environment_spec = specs.make_environment_spec(environment)\n\n  # Define the demonstrations factory.\n  dataset_name = helpers.get_dataset_name(FLAGS.env_name)\n  demonstration_dataset_factory = _make_demonstration_dataset_factory(\n      dataset_name, FLAGS.num_demonstrations, environment_spec,\n      FLAGS.batch_size)\n\n  # Load the demonstrations to compute the stats.\n  dataset = tfds.get_tfds_dataset(\n      dataset_name, FLAGS.num_demonstrations, env_spec=environment_spec)\n  shift, scale = helpers.get_observation_stats(dataset)\n\n  # Define the network factory.\n  network_factory = _make_network_factory(  # pytype: disable=wrong-arg-types  # numpy-scalars\n      shift=shift,\n      scale=scale,\n      num_layers=FLAGS.num_layers,\n      num_units=FLAGS.num_units,\n      dropout_rate=FLAGS.dropout_rate)\n\n  # Create the BC builder.\n  bc_config = bc.BCConfig(learning_rate=FLAGS.learning_rate)\n  bc_builder = bc.BCBuilder(bc_config, loss_fn=bc.mse())\n\n  environment_factory = _make_environment_factory(FLAGS.env_name)\n\n  return experiments.OfflineExperimentConfig(\n      builder=bc_builder,\n      network_factory=network_factory,\n      demonstration_dataset_factory=demonstration_dataset_factory,\n      environment_factory=environment_factory,\n      max_num_learner_steps=FLAGS.num_bc_steps,\n      seed=FLAGS.seed,\n      environment_spec=environment_spec,\n  )\n\n\ndef main(_):\n  config = build_experiment_config()\n  if FLAGS.run_distributed:\n    program = experiments.make_distributed_offline_experiment(experiment=config)\n    lp.launch(program, xm_resources=lp_utils.make_xm_docker_resources(program))\n  else:\n    experiments.run_offline_experiment(\n        experiment=config,\n        eval_every=FLAGS.eval_every,\n        num_eval_episodes=FLAGS.evaluation_episodes)\n\n\nif __name__ == '__main__':\n  app.run(main)\n",
  "#!/usr/bin/env python\n\n'''\nCCSD with k-point sampling or at an individual k-point\n'''\n\nfrom functools import reduce\nimport numpy\nfrom pyscf.pbc import gto, scf, cc\n\ncell = gto.Cell()\ncell.atom='''\nC 0.000000000000   0.000000000000   0.000000000000\nC 1.685068664391   1.685068664391   1.685068664391\n'''\ncell.basis = 'gth-szv'\ncell.pseudo = 'gth-pade'\ncell.a = '''\n0.000000000, 3.370137329, 3.370137329\n3.370137329, 0.000000000, 3.370137329\n3.370137329, 3.370137329, 0.000000000'''\ncell.unit = 'B'\ncell.verbose = 5\ncell.build()\n\n#\n# KHF and KCCSD with 2x2x2 k-points\n#\nkpts = cell.make_kpts([2,2,2])\nkmf = scf.KRHF(cell)\nkmf.kpts = kpts\nehf = kmf.kernel()\n\nmycc = cc.KCCSD(kmf)\nmycc.kernel()\nprint(\"KRCCSD energy (per unit cell) =\", mycc.e_tot)\n\n#\n# The KHF and KCCSD for single k-point calculation.\n#\nkpts = cell.get_abs_kpts([0.25, 0.25, 0.25])\nkmf = scf.KRHF(cell)\nkmf.kpts = kpts\nehf = kmf.kernel()\n\nmycc = cc.KRCCSD(kmf)\nmycc.kernel()\nprint(\"KRCCSD energy (per unit cell) =\", mycc.e_tot)\n\n\n#\n# The PBC module provides an separated implementation specified for the single\n# k-point calculations.  They are more efficient than the general implementation\n# with k-point sampling.  For gamma point, integrals and orbitals are all real\n# in this implementation.  They can be mixed with other post-HF methods that\n# were provided in the molecular program.\n#\nkpt = cell.get_abs_kpts([0.25, 0.25, 0.25])\nmf = scf.RHF(cell, kpt=kpt)\nehf = mf.kernel()\n\nmycc = cc.RCCSD(mf).run()\nprint(\"RCCSD energy (per unit cell) at k-point =\", mycc.e_tot)\ndm1 = mycc.make_rdm1()\ndm2 = mycc.make_rdm2()\nnmo = mf.mo_coeff.shape[1]\neri_mo = mf.with_df.ao2mo(mf.mo_coeff, kpts=kpt).reshape([nmo]*4)\nh1 = reduce(numpy.dot, (mf.mo_coeff.conj().T, mf.get_hcore(), mf.mo_coeff))\ne_tot = numpy.einsum('ij,ji', h1, dm1) + numpy.einsum('ijkl,jilk', eri_mo, dm2)*.5 + mf.energy_nuc()\nprint(\"RCCSD energy based on CCSD density matrices =\", e_tot.real)\n\n\nmf = scf.addons.convert_to_uhf(mf)\nmycc = cc.UCCSD(mf).run()\nprint(\"UCCSD energy (per unit cell) at k-point =\", mycc.e_tot)\ndm1a, dm1b = mycc.make_rdm1()\ndm2aa, dm2ab, dm2bb = mycc.make_rdm2()\nnmo = dm1a.shape[0]\neri_aa = mf.with_df.ao2mo(mf.mo_coeff[0], kpts=kpt).reshape([nmo]*4)\neri_bb = mf.with_df.ao2mo(mf.mo_coeff[1], kpts=kpt).reshape([nmo]*4)\neri_ab = mf.with_df.ao2mo((mf.mo_coeff[0],mf.mo_coeff[0],mf.mo_coeff[1],mf.mo_coeff[1]), kpts=kpt).reshape([nmo]*4)\nhcore = mf.get_hcore()\nh1a = reduce(numpy.dot, (mf.mo_coeff[0].conj().T, hcore, mf.mo_coeff[0]))\nh1b = reduce(numpy.dot, (mf.mo_coeff[1].conj().T, hcore, mf.mo_coeff[1]))\ne_tot = (numpy.einsum('ij,ji', h1a, dm1a) +\n         numpy.einsum('ij,ji', h1b, dm1b) +\n         numpy.einsum('ijkl,jilk', eri_aa, dm2aa)*.5 +\n         numpy.einsum('ijkl,jilk', eri_ab, dm2ab)    +\n         numpy.einsum('ijkl,jilk', eri_bb, dm2bb)*.5 + mf.energy_nuc())\nprint(\"UCCSD energy based on CCSD density matrices =\", e_tot.real)\n\n\nmf = scf.addons.convert_to_ghf(mf)\nmycc = cc.GCCSD(mf).run()\nprint(\"GCCSD energy (per unit cell) at k-point =\", mycc.e_tot)\ndm1 = mycc.make_rdm1()\ndm2 = mycc.make_rdm2()\nnao = cell.nao_nr()\nnmo = mf.mo_coeff.shape[1]\nmo = mf.mo_coeff[:nao] + mf.mo_coeff[nao:]\neri_mo = mf.with_df.ao2mo(mo, kpts=kpt).reshape([nmo]*4)\norbspin = mf.mo_coeff.orbspin\neri_mo[orbspin[:,None]!=orbspin] = 0\neri_mo[:,:,orbspin[:,None]!=orbspin] = 0\nh1 = reduce(numpy.dot, (mf.mo_coeff.conj().T, mf.get_hcore(), mf.mo_coeff))\ne_tot = numpy.einsum('ij,ji', h1, dm1) + numpy.einsum('ijkl,jilk', eri_mo, dm2)*.5 + mf.energy_nuc()\nprint(\"GCCSD energy based on CCSD density matrices =\", e_tot.real)\n",
  "# -*- coding: utf-8 -*-\n# -----------------------------------------------------------------------------\n# Copyright (c) Vispy Development Team. All Rights Reserved.\n# Distributed under the (new) BSD License. See LICENSE.txt for more info.\n# -----------------------------------------------------------------------------\n# Author:   Almar Klein & Nicolas P .Rougier\n# Date:     04/03/2014\n# Topic:    Fireworks !\n# Keywords: oarticles, gl, sprites\n# -----------------------------------------------------------------------------\n\"\"\"\nExample demonstrating simulation of fireworks using point sprites.\n(adapted from the \"OpenGL ES 2.0 Programming Guide\")\n\nThis example demonstrates a series of explosions that last one second. The\nvisualization during the explosion is highly optimized using a Vertex Buffer\nObject (VBO). After each explosion, vertex data for the next explosion are\ncalculated, such that each explostion is unique.\n\"\"\"\nimport numpy as np\n\nfrom vispy import app\nfrom vispy.gloo import gl\n\n\nvertex_code = \"\"\"\n#version 120\n\nuniform float time;\nuniform vec3 center;\nattribute float lifetime;\nattribute vec3 start;\nattribute vec3 end;\nvarying float v_lifetime;\nvoid main () {\n    if (time < lifetime) {\n        gl_Position.xyz = start + (time * end) + center;\n        gl_Position.w = 1.0;\n        gl_Position.y -= 1.5 * time * time;\n    } else {\n        gl_Position = vec4(-1000, -1000, 0, 0);\n    }\n    v_lifetime = clamp(1.0 - (time / lifetime), 0.0, 1.0);\n    gl_PointSize = (v_lifetime * v_lifetime) * 40.0;\n}\n\"\"\"\n\nfragment_code = \"\"\"\n#version 120\n\nuniform vec4 color;\nvarying float v_lifetime;\nvoid main()\n{\n    float d = 1 - length(gl_PointCoord - vec2(.5,.5)) / (sqrt(2)/2);\n    gl_FragColor = d*color;\n    gl_FragColor.a = d;\n    gl_FragColor.a *= v_lifetime;\n}\n\"\"\"\n\n\nclass Canvas(app.Canvas):\n    def __init__(self):\n        app.Canvas.__init__(self, size=(800, 600), title='GL Fireworks',\n                            keys='interactive')\n\n    def on_initialize(self, event):\n        # Build & activate program\n        self.program = gl.glCreateProgram()\n        vertex = gl.glCreateShader(gl.GL_VERTEX_SHADER)\n        fragment = gl.glCreateShader(gl.GL_FRAGMENT_SHADER)\n        gl.glShaderSource(vertex, vertex_code)\n        gl.glShaderSource(fragment, fragment_code)\n        gl.glCompileShader(vertex)\n        gl.glCompileShader(fragment)\n        gl.glAttachShader(self.program, vertex)\n        gl.glAttachShader(self.program, fragment)\n        gl.glLinkProgram(self.program)\n        gl.glDetachShader(self.program, vertex)\n        gl.glDetachShader(self.program, fragment)\n        gl.glUseProgram(self.program)\n\n        # Build vertex buffer\n        n = 10000\n        self.data = np.zeros(n, dtype=[('lifetime', np.float32),\n                                       ('start', np.float32, 3),\n                                       ('end', np.float32, 3)])\n        vbuffer = gl.glCreateBuffer()\n        gl.glBindBuffer(gl.GL_ARRAY_BUFFER, vbuffer)\n        gl.glBufferData(gl.GL_ARRAY_BUFFER, self.data, gl.GL_DYNAMIC_DRAW)\n\n        # Bind buffer attributes\n        stride = self.data.strides[0]\n\n        offset = 0\n        loc = gl.glGetAttribLocation(self.program, \"lifetime\")\n        gl.glEnableVertexAttribArray(loc)\n        gl.glVertexAttribPointer(loc, 1, gl.GL_FLOAT, False, stride, offset)\n\n        offset = self.data.dtype[\"lifetime\"].itemsize\n        loc = gl.glGetAttribLocation(self.program, \"start\")\n        gl.glEnableVertexAttribArray(loc)\n        gl.glVertexAttribPointer(loc, 3, gl.GL_FLOAT, False, stride, offset)\n\n        offset = self.data.dtype[\"start\"].itemsize\n        loc = gl.glGetAttribLocation(self.program, \"end\")\n        gl.glEnableVertexAttribArray(loc)\n        gl.glVertexAttribPointer(loc, 3, gl.GL_FLOAT, False, stride, offset)\n\n        # OpenGL initalization\n        self.elapsed_time = 0\n        gl.glClearColor(0, 0, 0, 1)\n        gl.glDisable(gl.GL_DEPTH_TEST)\n        gl.glEnable(gl.GL_BLEND)\n        gl.glBlendFunc(gl.GL_SRC_ALPHA, gl.GL_ONE)\n        gl.glEnable(34370)  # gl.GL_VERTEX_PROGRAM_POINT_SIZE\n        gl.glEnable(34913)  # gl.GL_POINT_SPRITE\n        gl.glViewport(0, 0, *self.physical_size)\n        self.new_explosion()\n        self.timer = app.Timer('auto', self.on_timer, start=True)\n\n    def on_draw(self, event):\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n        gl.glDrawArrays(gl.GL_POINTS, 0, len(self.data))\n\n    def on_resize(self, event):\n        gl.glViewport(0, 0, *event.physical_size)\n\n    def on_timer(self, event):\n        self.elapsed_time += 1. / 60.\n        if self.elapsed_time > 1.5:\n            self.new_explosion()\n            self.elapsed_time = 0.0\n\n        loc = gl.glGetUniformLocation(self.program, \"time\")\n        gl.glUniform1f(loc, self.elapsed_time)\n        self.update()\n\n    def new_explosion(self):\n        n = len(self.data)\n        color = np.random.uniform(0.1, 0.9, 4).astype(np.float32)\n        color[3] = 1.0 / n ** 0.08\n        loc = gl.glGetUniformLocation(self.program, \"color\")\n        gl.glUniform4f(loc, *color)\n\n        center = np.random.uniform(-0.5, 0.5, 3)\n        loc = gl.glGetUniformLocation(self.program, \"center\")\n        gl.glUniform3f(loc, *center)\n\n        self.data['lifetime'] = np.random.normal(2.0, 0.5, (n,))\n        self.data['start'] = np.random.normal(0.0, 0.2, (n, 3))\n        self.data['end'] = np.random.normal(0.0, 1.2, (n, 3))\n        gl.glBufferData(gl.GL_ARRAY_BUFFER, self.data, gl.GL_DYNAMIC_DRAW)\n\nif __name__ == '__main__':\n    c = Canvas()\n    c.show()\n    app.run()\n",
  "# SPDX-FileCopyrightText: 2021 Division of Intelligent Medical Systems, DKFZ\n# SPDX-FileCopyrightText: 2021 Janek Groehl\n# SPDX-License-Identifier: MIT\n\nimport os\n\nimport numpy as np\n\nimport simpa as sp\nfrom simpa import Tags\nfrom simpa.visualisation.matplotlib_data_visualisation import visualise_data\n# FIXME temporary workaround for newest Intel architectures\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\n# TODO: Please make sure that a valid path_config.env file is located in your home directory, or that you\n#  point to the correct file in the PathManager().\npath_manager = sp.PathManager()\n\n# set global params characterizing the simulated volume\nVOLUME_TRANSDUCER_DIM_IN_MM = 75\nVOLUME_PLANAR_DIM_IN_MM = 20\nVOLUME_HEIGHT_IN_MM = 25\nSPACING = 0.25\nRANDOM_SEED = 471\nVOLUME_NAME = \"LinearUnmixingExample_\" + str(RANDOM_SEED)\n\n# since we want to perform linear unmixing, the simulation pipeline should be execute for at least two wavelengths\nWAVELENGTHS = [750, 800, 850]\n\n\ndef create_example_tissue():\n    \"\"\"\n    This is a very simple example script of how to create a tissue definition.\n    It contains a muscular background, an epidermis layer on top of the muscles\n    and two blood vessels.\n    \"\"\"\n    background_dictionary = sp.Settings()\n    background_dictionary[Tags.MOLECULE_COMPOSITION] = sp.TISSUE_LIBRARY.constant(1e-4, 1e-4, 0.9)\n    background_dictionary[Tags.STRUCTURE_TYPE] = Tags.BACKGROUND\n\n    muscle_dictionary = sp.Settings()\n    muscle_dictionary[Tags.PRIORITY] = 1\n    muscle_dictionary[Tags.STRUCTURE_START_MM] = [0, 0, 0]\n    muscle_dictionary[Tags.STRUCTURE_END_MM] = [0, 0, 100]\n    muscle_dictionary[Tags.MOLECULE_COMPOSITION] = sp.TISSUE_LIBRARY.muscle()\n    muscle_dictionary[Tags.CONSIDER_PARTIAL_VOLUME] = True\n    muscle_dictionary[Tags.ADHERE_TO_DEFORMATION] = True\n    muscle_dictionary[Tags.STRUCTURE_TYPE] = Tags.HORIZONTAL_LAYER_STRUCTURE\n\n    vessel_1_dictionary = sp.Settings()\n    vessel_1_dictionary[Tags.PRIORITY] = 3\n    vessel_1_dictionary[Tags.STRUCTURE_START_MM] = [VOLUME_TRANSDUCER_DIM_IN_MM/2,\n                                                    10,\n                                                    5]\n    vessel_1_dictionary[Tags.STRUCTURE_END_MM] = [VOLUME_TRANSDUCER_DIM_IN_MM/2,\n                                                  12,\n                                                  5]\n    vessel_1_dictionary[Tags.STRUCTURE_RADIUS_MM] = 3\n    vessel_1_dictionary[Tags.MOLECULE_COMPOSITION] = sp.TISSUE_LIBRARY.blood(oxygenation=0.99)\n    vessel_1_dictionary[Tags.CONSIDER_PARTIAL_VOLUME] = True\n    vessel_1_dictionary[Tags.STRUCTURE_TYPE] = Tags.CIRCULAR_TUBULAR_STRUCTURE\n\n    vessel_2_dictionary = sp.Settings()\n    vessel_2_dictionary[Tags.PRIORITY] = 3\n    vessel_2_dictionary[Tags.STRUCTURE_START_MM] = [VOLUME_TRANSDUCER_DIM_IN_MM/3,\n                                                    10,\n                                                    5]\n    vessel_2_dictionary[Tags.STRUCTURE_END_MM] = [VOLUME_TRANSDUCER_DIM_IN_MM/3,\n                                                  12,\n                                                  5]\n    vessel_2_dictionary[Tags.STRUCTURE_RADIUS_MM] = 2\n    vessel_2_dictionary[Tags.MOLECULE_COMPOSITION] = sp.TISSUE_LIBRARY.blood(oxygenation=0.75)\n    vessel_2_dictionary[Tags.CONSIDER_PARTIAL_VOLUME] = True\n    vessel_2_dictionary[Tags.STRUCTURE_TYPE] = Tags.CIRCULAR_TUBULAR_STRUCTURE\n\n    epidermis_dictionary = sp.Settings()\n    epidermis_dictionary[Tags.PRIORITY] = 8\n    epidermis_dictionary[Tags.STRUCTURE_START_MM] = [0, 0, 0]\n    epidermis_dictionary[Tags.STRUCTURE_END_MM] = [0, 0, 0.1]\n    epidermis_dictionary[Tags.MOLECULE_COMPOSITION] = sp.TISSUE_LIBRARY.epidermis()\n    epidermis_dictionary[Tags.CONSIDER_PARTIAL_VOLUME] = True\n    epidermis_dictionary[Tags.ADHERE_TO_DEFORMATION] = True\n    epidermis_dictionary[Tags.STRUCTURE_TYPE] = Tags.HORIZONTAL_LAYER_STRUCTURE\n\n    tissue_dict = sp.Settings()\n    tissue_dict[Tags.BACKGROUND] = background_dictionary\n    tissue_dict[\"muscle\"] = muscle_dictionary\n    tissue_dict[\"epidermis\"] = epidermis_dictionary\n    tissue_dict[\"vessel_1\"] = vessel_1_dictionary\n    tissue_dict[\"vessel_2\"] = vessel_2_dictionary\n    return tissue_dict\n\n\n# Seed the numpy random configuration prior to creating the global_settings file in\n# order to ensure that the same volume is generated with the same random seed every time.\nnp.random.seed(RANDOM_SEED)\n\n# Initialize global settings and prepare for simulation pipeline including\n# volume creation and optical forward simulation.\ngeneral_settings = {\n    # These parameters set the general properties of the simulated volume\n    Tags.RANDOM_SEED: RANDOM_SEED,\n    Tags.VOLUME_NAME: VOLUME_NAME,\n    Tags.SIMULATION_PATH: path_manager.get_hdf5_file_save_path(),\n    Tags.SPACING_MM: SPACING,\n    Tags.DIM_VOLUME_Z_MM: VOLUME_HEIGHT_IN_MM,\n    Tags.DIM_VOLUME_X_MM: VOLUME_TRANSDUCER_DIM_IN_MM,\n    Tags.DIM_VOLUME_Y_MM: VOLUME_PLANAR_DIM_IN_MM,\n    Tags.WAVELENGTHS: WAVELENGTHS,\n    Tags.GPU: True,\n    Tags.DO_FILE_COMPRESSION: True\n}\nsettings = sp.Settings(general_settings)\nsettings.set_volume_creation_settings({\n    Tags.SIMULATE_DEFORMED_LAYERS: True,\n    Tags.STRUCTURES: create_example_tissue()\n})\nsettings.set_optical_settings({\n    Tags.OPTICAL_MODEL_NUMBER_PHOTONS: 1e7,\n    Tags.OPTICAL_MODEL_BINARY_PATH: path_manager.get_mcx_binary_path(),\n    Tags.OPTICAL_MODEL: Tags.OPTICAL_MODEL_MCX,\n    Tags.LASER_PULSE_ENERGY_IN_MILLIJOULE: 50\n})\n\n# Set component settings for linear unmixing.\n# In this example we are only interested in the chromophore concentration of oxy- and deoxyhemoglobin and the\n# resulting blood oxygen saturation. We want to perform the algorithm using all three wavelengths defined above.\n# Please take a look at the component for more information.\nsettings[\"linear_unmixing\"] = {\n    Tags.DATA_FIELD: Tags.DATA_FIELD_INITIAL_PRESSURE,\n    Tags.WAVELENGTHS: WAVELENGTHS,\n    Tags.LINEAR_UNMIXING_SPECTRA: sp.get_simpa_internal_absorption_spectra_by_names(\n        [Tags.SIMPA_NAMED_ABSORPTION_SPECTRUM_OXYHEMOGLOBIN, Tags.SIMPA_NAMED_ABSORPTION_SPECTRUM_DEOXYHEMOGLOBIN]\n    ),\n    Tags.LINEAR_UNMIXING_COMPUTE_SO2: True,\n    Tags.LINEAR_UNMIXING_NON_NEGATIVE: True\n}\n\n# Get device for simulation\ndevice = sp.MSOTAcuityEcho(device_position_mm=np.array([VOLUME_TRANSDUCER_DIM_IN_MM/2,\n                                                        VOLUME_PLANAR_DIM_IN_MM/2,\n                                                        0]))\ndevice.update_settings_for_use_of_model_based_volume_creator(settings)\n\n# Run simulation pipeline for all wavelengths in Tag.WAVELENGTHS\npipeline = [\n    sp.ModelBasedVolumeCreationAdapter(settings),\n    sp.MCXAdapter(settings),\n    sp.FieldOfViewCropping(settings),\n]\nsp.simulate(pipeline, settings, device)\n\n# Run linear unmixing component with above specified settings.\nsp.LinearUnmixing(settings, \"linear_unmixing\").run()\n\n# Load linear unmixing result (blood oxygen saturation) and reference absorption for first wavelength.\nfile_path = path_manager.get_hdf5_file_save_path() + \"/\" + VOLUME_NAME + \".hdf5\"\nlu_results = sp.load_data_field(file_path, Tags.LINEAR_UNMIXING_RESULT)\nsO2 = lu_results[\"sO2\"]\n\nmua = sp.load_data_field(file_path, Tags.DATA_FIELD_ABSORPTION_PER_CM, wavelength=WAVELENGTHS[0])\np0 = sp.load_data_field(file_path, Tags.DATA_FIELD_INITIAL_PRESSURE, wavelength=WAVELENGTHS[0])\ngt_oxy = sp.load_data_field(file_path, Tags.DATA_FIELD_OXYGENATION, wavelength=WAVELENGTHS[0])\n\n# Visualize linear unmixing result\nvisualise_data(path_to_hdf5_file=path_manager.get_hdf5_file_save_path() + \"/\" + VOLUME_NAME + \".hdf5\",\n               wavelength=WAVELENGTHS[0],\n               show_initial_pressure=True,\n               show_oxygenation=True,\n               show_linear_unmixing_sO2=True)\n",
  "################################################################################\n# Copyright (c) 2021 ContinualAI.                                              #\n# Copyrights licensed under the MIT License.                                   #\n# See the accompanying LICENSE file for terms.                                 #\n#                                                                              #\n# Date: 12-10-2020                                                             #\n# Author(s): Vincenzo Lomonaco, Hamed Hemati                                   #\n# E-mail: contact@continualai.org                                              #\n# Website: avalanche.continualai.org                                           #\n################################################################################\n\n\"\"\"\nThis is a simple example on how to use the Replay strategy in an online\nbenchmark created using OnlineCLScenario.\n\"\"\"\n\nimport argparse\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor, RandomCrop\nimport torch.optim.lr_scheduler\nfrom avalanche.benchmarks import nc_benchmark\nfrom avalanche.benchmarks.datasets.dataset_utils import default_dataset_location\nfrom avalanche.models import SimpleMLP\nfrom avalanche.training.supervised.strategy_wrappers_online import OnlineNaive\nfrom avalanche.training.plugins import ReplayPlugin\nfrom avalanche.training.storage_policy import ReservoirSamplingBuffer\nfrom avalanche.benchmarks.scenarios.online_scenario import OnlineCLScenario\nfrom avalanche.evaluation.metrics import (\n    forgetting_metrics,\n    accuracy_metrics,\n    loss_metrics,\n)\nfrom avalanche.logging import InteractiveLogger\nfrom avalanche.training.plugins import EvaluationPlugin\n\n\ndef main(args):\n    # --- CONFIG\n    device = torch.device(\n        f\"cuda:{args.cuda}\" if torch.cuda.is_available() and args.cuda >= 0 else \"cpu\"\n    )\n    n_batches = 5\n    # ---------\n\n    # --- TRANSFORMATIONS\n    train_transform = transforms.Compose(\n        [\n            RandomCrop(28, padding=4),\n            ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n        ]\n    )\n    test_transform = transforms.Compose(\n        [ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n    )\n    # ---------\n\n    # --- SCENARIO CREATION\n    mnist_train = MNIST(\n        root=default_dataset_location(\"mnist\"),\n        train=True,\n        download=True,\n        transform=train_transform,\n    )\n    mnist_test = MNIST(\n        root=default_dataset_location(\"mnist\"),\n        train=False,\n        download=True,\n        transform=test_transform,\n    )\n    benchmark = nc_benchmark(\n        mnist_train, mnist_test, n_batches, task_labels=False, seed=1234\n    )\n    # ---------\n\n    # MODEL CREATION\n    model = SimpleMLP(num_classes=benchmark.n_classes)\n\n    # choose some metrics and evaluation method\n    interactive_logger = InteractiveLogger()\n\n    eval_plugin = EvaluationPlugin(\n        accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        forgetting_metrics(experience=True),\n        loggers=[interactive_logger],\n    )\n\n    # CREATE THE STRATEGY INSTANCE (ONLINE-REPLAY)\n    storage_policy = ReservoirSamplingBuffer(max_size=100)\n    replay_plugin = ReplayPlugin(\n        mem_size=100, batch_size=1, storage_policy=storage_policy\n    )\n\n    cl_strategy = OnlineNaive(\n        model,\n        torch.optim.Adam(model.parameters(), lr=0.1),\n        CrossEntropyLoss(),\n        train_passes=1,\n        train_mb_size=10,\n        eval_mb_size=32,\n        device=device,\n        evaluator=eval_plugin,\n        plugins=[replay_plugin],\n    )\n\n    # TRAINING LOOP\n    print(\"Starting experiment...\")\n    results = []\n\n    # Create online benchmark\n    batch_streams = benchmark.streams.values()\n    # ocl_benchmark = OnlineCLScenario(batch_streams)\n    for i, exp in enumerate(benchmark.train_stream):\n        # Create online scenario from experience exp\n        ocl_benchmark = OnlineCLScenario(\n            original_streams=batch_streams, experiences=exp, experience_size=10\n        )\n        # Train on the online train stream of the scenario\n        cl_strategy.train(ocl_benchmark.train_stream)\n        results.append(cl_strategy.eval(benchmark.test_stream))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--cuda\",\n        type=int,\n        default=0,\n        help=\"Select zero-indexed cuda device. -1 to use CPU.\",\n    )\n    args = parser.parse_args()\n    main(args)\n",
  "# ==========================================================================\n#  AIDA Detector description implementation\n# --------------------------------------------------------------------------\n# Copyright (C) Organisation europeenne pour la Recherche nucleaire (CERN)\n# All rights reserved.\n#\n# For the licensing terms see $DD4hepINSTALL/LICENSE.\n# For the list of contributors see $DD4hepINSTALL/doc/CREDITS.\n#\n# ==========================================================================\n#\n#\nfrom __future__ import absolute_import, unicode_literals\nimport os\nimport sys\nimport time\nimport logging\nimport DDG4\nfrom DDG4 import OutputLevel as Output\nfrom g4units import MeV, GeV, m, mm\n#\n#\nlogging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\"\"\"\n\n   dd4hep simulation example setup using the python configuration\n\n   @author  M.Frank\n   @version 1.0\n\n\"\"\"\n\n\ndef show_help():\n  logging.info(\"Check_shape.py -option [-option]                           \")\n  logging.info(\"       -vis                          Enable visualization  \")\n  logging.info(\"       -batch                        Batch execution       \")\n\n\ndef run():\n  hlp = False\n  vis = False\n  dump = False\n  batch = False\n  install_dir = os.environ['DD4hepINSTALL']\n  #\n  for i in list(range(len(sys.argv))):\n    c = sys.argv[i].upper()\n    if c.find('BATCH') < 2 and c.find('BATCH') >= 0:\n      batch = True\n    elif c[:4] == '-VIS':\n      vis = True\n    elif c[:4] == '-DUM':\n      dump = True\n    elif c[:2] == '-H':\n      hlp = True\n\n  if hlp:\n    show_help()\n    sys.exit(1)\n\n  kernel = DDG4.Kernel()\n  description = kernel.detectorDescription()\n  install_dir = os.environ['DD4hepExamplesINSTALL']\n  geant4 = DDG4.Geant4(kernel, tracker='Geant4TrackerCombineAction')\n  #\n  logger.info(\"#  Configure UI\")\n  ui = None\n  if batch:\n    geant4.setupCshUI(ui=None, vis=None)\n    kernel.UI = 'UI'\n  else:\n    ui = geant4.setupCshUI(vis=vis)\n\n  kernel.loadGeometry(str(\"file:\" + install_dir + \"/examples/ClientTests/compact/NestedBoxReflection.xml\"))\n  DDG4.importConstants(description)\n\n  geant4.printDetectors()\n  if dump:\n    seq, act = geant4.addDetectorConstruction(\"Geant4DetectorGeometryConstruction/ConstructGeo\")\n    act.DebugReflections = True\n    act.DebugMaterials = False\n    act.DebugElements = False\n    act.DebugVolumes = False\n    act.DebugShapes = False\n    act.DumpHierarchy = ~0x0\n\n  logger.info(\"#  Configure G4 magnetic field tracking\")\n  geant4.setupTrackingField()\n\n  logger.info(\"#  Setup random generator\")\n  rndm = DDG4.Action(kernel, 'Geant4Random/Random')\n  rndm.Seed = 987654321\n  rndm.initialize()\n  #\n  logger.info(\"#  Configure Event actions\")\n  prt = DDG4.EventAction(kernel, 'Geant4ParticlePrint/ParticlePrint')\n  prt.OutputType = 3  # Print both: table and tree\n  prt.OutputLevel = Output.INFO\n  kernel.eventAction().adopt(prt)\n  #\n  logger.info(\"#  Configure I/O\")\n  geant4.setupROOTOutput('RootOutput', 'BoxReflect_' + time.strftime('%Y-%m-%d_%H-%M'))\n  #\n  gen = DDG4.GeneratorAction(kernel, \"Geant4GeneratorActionInit/GenerationInit\")\n  gen.enableUI()\n  kernel.generatorAction().adopt(gen)\n  #\n  logger.info(\"#  Generation of isotrope tracks of a given multiplicity with overlay:\")\n  gen = DDG4.GeneratorAction(kernel, \"Geant4ParticleGun/IsotropE+\")\n  gen.mask = 4\n  gen.isotrop = True\n  gen.particle = 'e+'\n  gen.Energy = 100 * GeV\n  gen.multiplicity = 200\n  gen.position = (0 * m, 0 * m, 0 * m)\n  gen.direction = (0, 0, 1.)\n  gen.distribution = 'uniform'\n  gen.standalone = False\n  # gen.PhiMin = 0.0*rad\n  # gen.PhiMax = 2.0*math.pi*rad\n  # gen.ThetaMin = 0.0*math.pi*rad\n  # gen.ThetaMax = 1.0*math.pi*rad\n  gen.enableUI()\n  kernel.generatorAction().adopt(gen)\n  #\n  logger.info(\"#  Merge all existing interaction records\")\n  gen = DDG4.GeneratorAction(kernel, \"Geant4InteractionMerger/InteractionMerger\")\n  gen.OutputLevel = 4  # generator_output_level\n  gen.enableUI()\n  kernel.generatorAction().adopt(gen)\n  #\n  logger.info(\"#  Finally generate Geant4 primaries\")\n  gen = DDG4.GeneratorAction(kernel, \"Geant4PrimaryHandler/PrimaryHandler\")\n  gen.OutputLevel = 4  # generator_output_level\n  gen.enableUI()\n  kernel.generatorAction().adopt(gen)\n  #\n  logger.info(\"#  ....and handle the simulation particles.\")\n  part = DDG4.GeneratorAction(kernel, \"Geant4ParticleHandler/ParticleHandler\")\n  kernel.generatorAction().adopt(part)\n  # part.SaveProcesses = ['conv','Decay']\n  part.SaveProcesses = ['Decay']\n  part.MinimalKineticEnergy = 100 * MeV\n  part.OutputLevel = 5  # generator_output_level\n  part.enableUI()\n  user = DDG4.Action(kernel, \"Geant4TCUserParticleHandler/UserParticleHandler\")\n  user.TrackingVolume_Zmax = 3.0 * m\n  user.TrackingVolume_Rmax = 3.0 * m\n  user.enableUI()\n  part.adopt(user)\n  #\n  logger.info(\"#  Now setup the calorimeters\")\n  seq, actions = geant4.setupDetectors()\n  #\n  logger.info(\"#  Now build the physics list:\")\n  geant4.setupPhysics('QGSP_BERT')\n  ph = geant4.addPhysics(str('Geant4PhysicsList/Myphysics'))\n  ph.addPhysicsConstructor(str('G4StepLimiterPhysics'))\n  #\n  # Add special particle types from specialized physics constructor\n  part = geant4.addPhysics('Geant4ExtraParticles/ExtraParticles')\n  part.pdgfile = os.path.join(install_dir, 'examples/DDG4/examples/particle.tbl')\n  #\n  # Add global range cut\n  rg = geant4.addPhysics('Geant4DefaultRangeCut/GlobalRangeCut')\n  rg.RangeCut = 0.7 * mm\n  #\n  #\n  if ui and vis:\n    cmds = []\n    cmds.append('/control/verbose 2')\n    cmds.append('/run/initialize')\n    cmds.append('/vis/open OGL')\n    cmds.append('/vis/verbose errors')\n    cmds.append('/vis/drawVolume')\n    cmds.append('/vis/viewer/set/viewpointThetaPhi 55. 45.')\n    cmds.append('/vis/scene/add/axes 0 0 0 3 m')\n    ui.Commands = cmds\n\n  kernel.configure()\n  kernel.initialize()\n\n  # DDG4.setPrintLevel(Output.DEBUG)\n  kernel.run()\n  kernel.terminate()\n\n\nif __name__ == \"__main__\":\n  run()\n",
  "from seedemu.layers import Base, Routing, Ebgp, PeerRelationship, Ibgp, Ospf\nfrom seedemu.services import WebService\nfrom seedemu.core import Emulator, Binding, Filter\nfrom seedemu.compiler import Docker\n\nemu = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nibgp = Ibgp()\nospf = Ospf()\nweb = WebService()\n\n###############################################################################\n\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\n\n###############################################################################\n# Set up the transit AS (AS150)\n\nas150 = base.createAutonomousSystem(150)\n\nas150.createNetwork('net0')\nas150.createNetwork('net1')\nas150.createNetwork('net2')\n\nr1 = as150.createRouter('r1')\nr2 = as150.createRouter('r2')\nr3 = as150.createRouter('r3')\nr4 = as150.createRouter('r4')\n\nr1.joinNetwork('ix100')\nr1.joinNetwork('net0')\n\nr2.joinNetwork('net0')\nr2.joinNetwork('net1')\n\nr3.joinNetwork('net1')\nr3.joinNetwork('net2')\n\nr4.joinNetwork('net2')\nr4.joinNetwork('ix101')\n\n###############################################################################\n# Create and set up the AS 151\n\nas151 = base.createAutonomousSystem(151)\n\nas151_web = as151.createHost('web')\nweb.install('web151')\nemu.addBinding(Binding('web151', filter = Filter(nodeName = 'web', asn = 151)))\n\nas151_router = as151.createRouter('router0')\n\nas151_net = as151.createNetwork('net0')\n\n\n\nas151_web.joinNetwork('net0')\nas151_router.joinNetwork('net0')\n\nas151_router.joinNetwork('ix100')\n\n###############################################################################\n# Create and set up the AS 152\n\nas152 = base.createAutonomousSystem(152)\n\nas152_web = as152.createHost('web')\nweb.install('web152')\nemu.addBinding(Binding('web152', filter = Filter(nodeName = 'web', asn = 152)))\n\nas152_router = as152.createRouter('router0')\n\nas152_net = as152.createNetwork('net0')\n\n\n\nas152_web.joinNetwork('net0')\nas152_router.joinNetwork('net0')\n\nas152_router.joinNetwork('ix101')\n\n###############################################################################\n# Add BGP peering\n\nebgp.addPrivatePeering(100, 150, 151, abRelationship = PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 150, 152, abRelationship = PeerRelationship.Provider)\n\n###############################################################################\n\nemu.addLayer(base)\nemu.addLayer(routing)\nemu.addLayer(ebgp)\nemu.addLayer(ibgp)\nemu.addLayer(ospf)\nemu.addLayer(web)\n\nemu.dump('component-dump.bin')\n",
  "from seedemu.core import Emulator\nfrom seedemu.layers import Base, Routing, Ebgp, Ospf, Ibgp, PeerRelationship\nfrom seedemu.components import BgpAttackerComponent\nfrom seedemu.compiler import Docker\nfrom seedemu.mergers import DEFAULT_MERGERS\n\n###############################################################################\n# topology:\n#\n# as150 --+\n#          \\__ ix100 -- as2 -- ix101 -- as151 \n#          /\n# as666 --+\n# (hijacking as151)\n###############################################################################\n\nsim = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nospf = Ospf()\nibgp = Ibgp()\n\nbgp_attack = BgpAttackerComponent(attackerAsn = 66)\n\n###############################################################################\n\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\n\n###############################################################################\n\nas150 = base.createAutonomousSystem(150)\n\nas150_r0 = as150.createRouter('r0')\n\nas150_n0 = as150.createNetwork('n0')\n\n\n\nas150_r0.joinNetwork('n0')\nas150_r0.joinNetwork('ix100')\n\n###############################################################################\n\nas2 = base.createAutonomousSystem(2)\n\nas2_r0 = as2.createRouter('r0')\nas2_r1 = as2.createRouter('r1')\n\nas2_n0 = as2.createNetwork('n0')\n\nas2_r0.joinNetwork('n0')\nas2_r1.joinNetwork('n0')\n\nas2_r0.joinNetwork('ix100')\nas2_r1.joinNetwork('ix101')\n\n###############################################################################\n\nas151 = base.createAutonomousSystem(151)\n\nas151_r0 = as151.createRouter('r0')\n\nas151_n0 = as151.createNetwork('n0')\n\n\n\nas151_r0.joinNetwork('n0')\nas151_r0.joinNetwork('ix101')\n\n###############################################################################\n\nsim.addLayer(base)\nsim.addLayer(routing)\nsim.addLayer(ibgp)\nsim.addLayer(ebgp)\nsim.addLayer(ospf)\n\n###############################################################################\n\nbgp_attack.addHijackedPrefix(as151_n0.getPrefix())\nbgp_attack.joinInternetExchange('ix100', '10.100.0.66')\n\nhijack_component = bgp_attack.get()\n\nsim_with_attack = sim.merge(bgp_attack.get(), DEFAULT_MERGERS)\n\n###############################################################################\n\nebgp.addPrivatePeering(100, 2, 150, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 151, PeerRelationship.Provider)\n\n# hijacker's session\nebgp.addPrivatePeering(100, 2, 66, PeerRelationship.Unfiltered)\n\n###############################################################################\n\nsim_with_attack.render()\nsim_with_attack.compile(Docker(selfManagedNetwork = True), 'bgp-attacker-component')",
  "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree.\n\n#!/usr/bin/env python3\n\nimport os\nfrom typing import cast, List, Optional\n\nimport torch\nfrom fbgemm_gpu.split_embedding_configs import EmbOptimType as OptimType\nfrom torch import distributed as dist, nn\nfrom torch.utils.data import DataLoader\nfrom torchrec.datasets.criteo import DEFAULT_CAT_NAMES, DEFAULT_INT_NAMES\nfrom torchrec.datasets.random import RandomRecDataset\nfrom torchrec.distributed import TrainPipelineSparseDist\nfrom torchrec.distributed.embeddingbag import EmbeddingBagCollectionSharder\nfrom torchrec.distributed.model_parallel import DistributedModelParallel\nfrom torchrec.distributed.types import ModuleSharder\nfrom torchrec.models.dlrm import DLRM, DLRMTrain\nfrom torchrec.modules.embedding_configs import EmbeddingBagConfig\nfrom torchrec.modules.embedding_modules import EmbeddingBagCollection\nfrom torchrec.optim.keyed import KeyedOptimizerWrapper\nfrom torchrec.optim.optimizers import in_backward_optimizer_filter\nfrom tqdm import tqdm\n\n\ndef _get_random_dataloader(\n    num_embeddings: int, batch_size: int = 32, pin_memory: bool = False\n) -> DataLoader:\n    return DataLoader(\n        RandomRecDataset(\n            keys=DEFAULT_CAT_NAMES,\n            batch_size=batch_size,\n            hash_size=num_embeddings,\n            ids_per_feature=1,\n            num_dense=len(DEFAULT_INT_NAMES),\n        ),\n        batch_size=None,\n        batch_sampler=None,\n        pin_memory=pin_memory,\n        num_workers=0,\n    )\n\n\ndef train(\n    num_embeddings: int = 1024**2,\n    embedding_dim: int = 128,\n    dense_arch_layer_sizes: Optional[List[int]] = None,\n    over_arch_layer_sizes: Optional[List[int]] = None,\n    learning_rate: float = 0.1,\n) -> None:\n    \"\"\"\n    Constructs and trains a DLRM model (using random dummy data). Each script is run on each process (rank) in SPMD fashion.\n    The embedding layers will be sharded across available ranks\n    \"\"\"\n    if dense_arch_layer_sizes is None:\n        dense_arch_layer_sizes = [64, 128]\n    if over_arch_layer_sizes is None:\n        over_arch_layer_sizes = [64, 1]\n\n    # Init process_group , device, rank, backend\n    rank = int(os.environ[\"LOCAL_RANK\"])\n    if torch.cuda.is_available():\n        device: torch.device = torch.device(f\"cuda:{rank}\")\n        backend = \"nccl\"\n        torch.cuda.set_device(device)\n    else:\n        device: torch.device = torch.device(\"cpu\")\n        backend = \"gloo\"\n    dist.init_process_group(backend=backend)\n\n    # Construct DLRM module\n    eb_configs = [\n        EmbeddingBagConfig(\n            name=f\"t_{feature_name}\",\n            embedding_dim=embedding_dim,\n            num_embeddings=num_embeddings,\n            feature_names=[feature_name],\n        )\n        for feature_idx, feature_name in enumerate(DEFAULT_CAT_NAMES)\n    ]\n    dlrm_model = DLRM(\n        embedding_bag_collection=EmbeddingBagCollection(\n            tables=eb_configs, device=torch.device(\"meta\")\n        ),\n        dense_in_features=len(DEFAULT_INT_NAMES),\n        dense_arch_layer_sizes=dense_arch_layer_sizes,\n        over_arch_layer_sizes=over_arch_layer_sizes,\n        dense_device=device,\n    )\n    train_model = DLRMTrain(dlrm_model)\n\n    # Enable optimizer fusion\n    fused_params = {\n        \"learning_rate\": learning_rate,\n        \"optimizer\": OptimType.EXACT_ROWWISE_ADAGRAD,\n    }\n    sharders = [\n        EmbeddingBagCollectionSharder(fused_params=fused_params),\n    ]\n    # Distribute model across devices\n    model = DistributedModelParallel(\n        module=train_model,\n        device=device,\n        sharders=cast(List[ModuleSharder[nn.Module]], sharders),\n    )\n\n    # Overlap comm/compute/device transfer during training through train_pipeline\n    non_fused_optimizer = KeyedOptimizerWrapper(\n        dict(in_backward_optimizer_filter(model.named_parameters())),\n        lambda params: torch.optim.Adagrad(params, lr=learning_rate),\n    )\n    train_pipeline = TrainPipelineSparseDist(\n        model,\n        non_fused_optimizer,\n        device,\n    )\n\n    # train model\n    train_iterator = iter(\n        _get_random_dataloader(\n            num_embeddings=num_embeddings, pin_memory=backend == \"nccl\"\n        )\n    )\n    for _ in tqdm(range(int(1e4)), mininterval=5.0):\n        train_pipeline.progress(train_iterator)\n\n\nif __name__ == \"__main__\":\n    train()\n",
  "from __future__ import print_function\nimport gunpowder as gp\nimport json\nimport math\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef train(iterations):\n\n    ##################\n    # DECLARE ARRAYS #\n    ##################\n\n    # raw intensities\n    raw = gp.ArrayKey('RAW')\n\n    # objects labelled with unique IDs\n    gt_labels = gp.ArrayKey('LABELS')\n\n    # array of per-voxel affinities to direct neighbors\n    gt_affs= gp.ArrayKey('AFFINITIES')\n\n    # weights to use to balance the loss\n    loss_weights = gp.ArrayKey('LOSS_WEIGHTS')\n\n    # the predicted affinities\n    pred_affs = gp.ArrayKey('PRED_AFFS')\n\n    # the gredient of the loss wrt to the predicted affinities\n    pred_affs_gradients = gp.ArrayKey('PRED_AFFS_GRADIENTS')\n\n    ####################\n    # DECLARE REQUESTS #\n    ####################\n\n    with open('train_net_config.json', 'r') as f:\n        net_config = json.load(f)\n\n    # get the input and output size in world units (nm, in this case)\n    voxel_size = gp.Coordinate((40, 4, 4))\n    input_size = gp.Coordinate(net_config['input_shape'])*voxel_size\n    output_size = gp.Coordinate(net_config['output_shape'])*voxel_size\n\n    # formulate the request for what a batch should (at least) contain\n    request = gp.BatchRequest()\n    request.add(raw, input_size)\n    request.add(gt_affs, output_size)\n    request.add(loss_weights, output_size)\n\n    # when we make a snapshot for inspection (see below), we also want to\n    # request the predicted affinities and gradients of the loss wrt the\n    # affinities\n    snapshot_request = gp.BatchRequest()\n    snapshot_request[pred_affs] = request[gt_affs]\n    snapshot_request[pred_affs_gradients] = request[gt_affs]\n\n    ##############################\n    # ASSEMBLE TRAINING PIPELINE #\n    ##############################\n\n    pipeline = (\n\n        # a tuple of sources, one for each sample (A, B, and C) provided by the\n        # CREMI challenge\n        tuple(\n\n            # read batches from the HDF5 file\n            gp.Hdf5Source(\n                'sample_'+s+'_padded_20160501.hdf',\n                datasets = {\n                    raw: 'volumes/raw',\n                    gt_labels: 'volumes/labels/neuron_ids'\n                }\n            ) +\n\n            # convert raw to float in [0, 1]\n            gp.Normalize(raw) +\n\n            # chose a random location for each requested batch\n            gp.RandomLocation()\n\n            for s in ['A', 'B', 'C']\n        ) +\n\n        # chose a random source (i.e., sample) from the above\n        gp.RandomProvider() +\n\n        # elastically deform the batch\n        gp.ElasticAugment(\n            [4,40,40],\n            [0,2,2],\n            [0,math.pi/2.0],\n            prob_slip=0.05,\n            prob_shift=0.05,\n            max_misalign=25) +\n\n        # apply transpose and mirror augmentations\n        gp.SimpleAugment(transpose_only=[1, 2]) +\n\n        # scale and shift the intensity of the raw array\n        gp.IntensityAugment(\n            raw,\n            scale_min=0.9,\n            scale_max=1.1,\n            shift_min=-0.1,\n            shift_max=0.1,\n            z_section_wise=True) +\n\n        # grow a boundary between labels\n        gp.GrowBoundary(\n            gt_labels,\n            steps=3,\n            only_xy=True) +\n\n        # convert labels into affinities between voxels\n        gp.AddAffinities(\n            [[-1, 0, 0], [0, -1, 0], [0, 0, -1]],\n            gt_labels,\n            gt_affs) +\n\n        # create a weight array that balances positive and negative samples in\n        # the affinity array\n        gp.BalanceLabels(\n            gt_affs,\n            loss_weights) +\n\n        # pre-cache batches from the point upstream\n        gp.PreCache(\n            cache_size=10,\n            num_workers=5) +\n\n        # perform one training iteration for each passing batch (here we use\n        # the tensor names earlier stored in train_net.config)\n        gp.tensorflow.Train(\n            'train_net',\n            net_config['optimizer'],\n            net_config['loss'],\n            inputs={\n                net_config['raw']: raw,\n                net_config['gt_affs']: gt_affs,\n                net_config['loss_weights']: loss_weights\n            },\n            outputs={\n                net_config['pred_affs']: pred_affs\n            },\n            gradients={\n                net_config['pred_affs']: pred_affs_gradients\n            },\n            save_every=1) +\n\n        # save the passing batch as an HDF5 file for inspection\n        gp.Snapshot(\n            {\n                raw: '/volumes/raw',\n                gt_labels: '/volumes/labels/neuron_ids',\n                gt_affs: '/volumes/labels/affs',\n                pred_affs: '/volumes/pred_affs',\n                pred_affs_gradients: '/volumes/pred_affs_gradients'\n            },\n            output_dir='snapshots',\n            output_filename='batch_{iteration}.hdf',\n            every=100,\n            additional_request=snapshot_request,\n            compression_type='gzip') +\n\n        # show a summary of time spend in each node every 10 iterations\n        gp.PrintProfilingStats(every=10)\n    )\n\n    #########\n    # TRAIN #\n    #########\n\n    print(\"Training for\", iterations, \"iterations\")\n\n    with gp.build(pipeline):\n        for i in range(iterations):\n            pipeline.request_batch(request)\n\n    print(\"Finished\")\n\nif __name__ == \"__main__\":\n    train(200000)\n    ",
  "#\n#  ISC License\n#\n#  Copyright (c) 2016, Autonomous Vehicle Systems Lab, University of Colorado at Boulder\n#\n#  Permission to use, copy, modify, and/or distribute this software for any\n#  purpose with or without fee is hereby granted, provided that the above\n#  copyright notice and this permission notice appear in all copies.\n#\n#  THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n#  WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n#  MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n#  ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n#  ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n#  OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n#\n\nr\"\"\"\n\nThis script is a basic demonstration of a script that can be used to rerun a set or subset of Monte Carlo simulations.\n\n.. important::\n   This script can only be run once there exists data produced by the ``scenario_AttFeedbackMC.py`` script.\n\n\n\"\"\"\n\nimport inspect\nimport os\nimport sys\n\nfrom Basilisk.utilities.MonteCarlo.Controller import Controller\nfrom Basilisk.utilities.MonteCarlo.RetentionPolicy import RetentionPolicy\n\nfilename = inspect.getframeinfo(inspect.currentframe()).filename\nfileNameString = os.path.basename(os.path.splitext(__file__)[0])\npath = os.path.dirname(os.path.abspath(filename))\n\nfrom Basilisk import __path__\nbskPath = __path__[0]\n\nsys.path.append(path+\"/../BskSim/scenarios/\")\n\ndef run(time=None):\n    \"\"\"\n    Instructions:\n\n    1) Change the scenario name\n\n    2) Provide the number of processes to spawn\n\n    3) Provide the run numbers you wish to rerun\n\n    4) Add any new retention policies to the bottom\n\n    \"\"\"\n\n    # Step 1-3: Change to the relevant scenario\n    scenarioName = \"scenario_AttFeedback\"\n\n    monteCarlo = Controller()\n    monteCarlo.numProcess = 3 # Specify number of processes to spawn\n    runsList = [1]  # Specify the run numbers to be rerun\n\n    #\n    # # Generic initialization\n    icName = path + \"/\" + scenarioName + \"MC/\"\n    newDataDir = path + \"/\" + scenarioName + \"MC/rerun\"\n\n\n    exec('import '+ scenarioName)\n    simulationModule = eval(scenarioName + \".\" + scenarioName) # ex. scenarioMonteCarlo.scenarioMonteCarlo\n    if time is not None:\n        exec (scenarioName + '.' + scenarioName + '.simBaseTime = time')  # ex. scenarioMonteCarlo.scenarioMonteCarlo.simBaseTime = time\n    executionModule = eval(scenarioName + \".runScenario\") # ex. scenarioMonteCarlo.run\n\n    monteCarlo.setSimulationFunction(simulationModule)\n    monteCarlo.setExecutionFunction(executionModule)\n    monteCarlo.setICDir(icName)\n    monteCarlo.setICRunFlag(True)\n    monteCarlo.setArchiveDir(newDataDir)\n    monteCarlo.setExecutionCount(len(runsList))\n    monteCarlo.setShouldDisperseSeeds(False)\n    monteCarlo.shouldArchiveParameters = False\n\n\n    # Step 4: Add any additional retention policies desired\n    retentionPolicy = RetentionPolicy()\n    retentionPolicy.logRate = int(2E9)\n    retentionPolicy.addMessageLog(\"attGuidMsg\", [\"sigma_BR\"]) \n    monteCarlo.addRetentionPolicy(retentionPolicy)\n\n\n    failed = monteCarlo.runInitialConditions(runsList)\n    assert len(failed) == 0, \"Should run ICs successfully\"\n\n\n\nif __name__ == \"__main__\":\n    run()\n\n",
  "'''\nExample of an iterative reconstruciton with radial phase encoding (RPE) data.\n\nUpper-level demo that illustrates the computation of how to use a non-cartesian\nradial phase-encoding acquisition model to reconstruct data iteratively and\nwithout the use of any k-space density weighting.\n\nUsage:\n  rpe_recon.py [--help | options]\n\nOptions:\n  -f <file>, --file=<file>    raw data file\n                              [default: 3D_RPE_Lowres.h5]\n  -p <path>, --path=<path>    path to data files, defaults to data/examples/MR\n                              subfolder of SIRF root folder\n  -o <file>, --output=<file>  output file for simulated data\n  -e <engn>, --engine=<engn>  reconstruction engine [default: Gadgetron]\n  -n <bool>, --non-cart=<bool> run recon iff non-cartesian code was compiled\n                              [default: False]\n  -r <bool>, --recon=<bool>   run recon iff non-cartesian code was compiled\n                              [default: False]\n  --traj=<str>                trajectory type, must match the data supplied in file\n                              options are cartesian, radial, goldenangle or grpe\n                              [default: grpe]\n  --non-interactive           do not show plots\n'''\n\n## SyneRBI Synergistic Image Reconstruction Framework (SIRF)\n## Copyright 2021 Physikalisch-Technische Bundesanstalt (PTB)\n## Copyright 2015 - 2021 Rutherford Appleton Laboratory STFC\n## Copyright 2015 - 2021 University College London.\n##\n## This is software developed for the Collaborative Computational\n## Project in Synergistic Reconstruction for Biomedical Imaging (formerly CCP PETMR)\n## (http://www.ccpsynerbi.ac.uk/).\n##\n## Licensed under the Apache License, Version 2.0 (the \"License\");\n##   you may not use this file except in compliance with the License.\n##   You may obtain a copy of the License at\n##       http://www.apache.org/licenses/LICENSE-2.0\n##   Unless required by applicable law or agreed to in writing, software\n##   distributed under the License is distributed on an \"AS IS\" BASIS,\n##   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n##   See the License for the specific language governing permissions and\n##   limitations under the License.\n\n__version__ = '0.1.0'\nfrom docopt import docopt\n\nargs = docopt(__doc__, version=__version__)\n\n# import engine module\nexec('from sirf.' + args['--engine'] + ' import *')\n\n# process command-line options\ndata_file = args['--file']\ndata_path = args['--path']\nif data_path is None:\n    data_path = examples_data_path('MR') + '/zenodo/'\noutput_file = args['--output']\nshow_plot = not args['--non-interactive']\ntrajtype = args['--traj']\nrun_recon = str(args['--recon']) == 'True'\n\nimport numpy\n\n# define symmetrical operator for cg-optimisation\ndef EhE(E, image):\n    return E.backward( E.forward(image) )\n\ndef ConjugateGradient(rawdata, num_iter = 10, stop_criterion = 1e-7):\n\n    print('---\\n computing coil sensitivity maps...')\n    csms = CoilSensitivityData()\n    csms.smoothness = 10\n    csms.calculate(rawdata)\n    \n    # create acquisition model based on the acquisition parameters\n    print('---\\n Setting up Acquisition Model...')\n\n    #set up the acquisition model\n    E = AcquisitionModel()\n    E.set_up(rawdata, csms.copy())\n    E.set_coil_sensitivity_maps(csms)\n\n    print('---\\n Backward projection ...')\n    recon_img = E.backward(rawdata)\n    recon_img.fill(0+0j) # for some reason you need to start with this set to zero\n\n    # implement pseudo-code from Wikipedia\n    x = recon_img\n    y = rawdata\n\n    # this is our first residual\n    r = E.backward( y ) - EhE(E,x)\n\n    # this is our cost function at the start\n    rr = r.norm() ** 2\n    rr0 = rr\n\n    # initialize p\n    p = r\n    \n    print('Cost for k = 0: '  + str( rr/ rr0) )\n    \n    for k in range(num_iter):\n\n        Ap = EhE(E, p )\n\n        alpha = rr / Ap.dot(p)\n\n        x = x + alpha * p\n\n        r = r - alpha * Ap\n\n        beta  = r.norm()**2 / rr\n        rr = r.norm()**2\n\n        p = r + beta * p\n\n        relative_residual = numpy.sqrt(rr/rr0)\n\n        print('Cost at step  {} = {}'.format(k+1, relative_residual))\n        \n        if( relative_residual  < stop_criterion ):\n            print('We achieved our desired accuracy. Stopping iterative reconstruction')\n            break\n\n        if k is num_iter-1:\n            print('Reached maximum number of iterations. Stopping reconstruction.')\n\n    return x\n\ndef main():\n    \n    # locate the k-space raw data file\n    input_file = existing_filepath(data_path, data_file)\n\n    # acquisition data will be read from an HDF file input_file\n    # AcquisitionData.set_storage_scheme('memory')\n    acq_data = AcquisitionData(input_file)\n    \n    print('---\\n acquisition data norm: %e' % acq_data.norm())\n\n\n    # pre-process acquisition data only for cartesian readouts\n    if trajtype != 'radial' and trajtype != 'goldenangle':\n        print('---\\n pre-processing acquisition data...')\n        processed_data  = preprocess_acquisition_data(acq_data)\n    else:\n        processed_data = acq_data\n\n    #set the trajectory\n    print('---\\n setting the trajectory...')\n    if trajtype == 'cartesian':\n        pass\n    elif trajtype == 'grpe':\n        processed_data = set_grpe_trajectory(processed_data)\n    elif trajtype == 'radial':\n        processed_data = set_radial2D_trajectory(processed_data)\n    elif trajtype == 'goldenangle':\n        processed_data = set_goldenangle2D_trajectory(processed_data)\n    else:\n        raise NameError('Please submit a trajectory name of the following list: (cartesian, grpe, radial). You gave {}'\\\n                        .format(trajtype))\n\n    # sort processed acquisition data;\n    print('---\\n sorting acquisition data...')\n    processed_data.sort()\n    \n    if run_recon:\n        recon = ConjugateGradient(processed_data, num_iter = 20, stop_criterion = 1e-7)\n        \n        if show_plot:\n            recon.show(title = 'Reconstructed images using CG() (magnitude)')\n            \n    else:\n        print('---\\n Skipping non-cartesian code...')\n\ntry:\n    main()\n    print('\\n=== done with %s' % __file__)\n\nexcept error as err:\n    # display error information\n    print('??? %s' % err.value)\n    exit(1)\n\n",
  "#!/usr/bin/env python\n#\n# Contributors:\n#       Qiming Sun <osirpt.sun@gmail.com>\n#\n\nfrom functools import reduce\nimport numpy\nimport scipy.linalg\nfrom pyscf import scf\nfrom pyscf import gto\nfrom pyscf import mcscf\nfrom pyscf import dmrgscf\nfrom pyscf import mrpt\n#\n# Adjust mpi runtime schedular to execute the calculation with multi-processor\n#\n# NOTE DMRG-NEVPT2 requires about 10 GB memory per processor in this example\n#\ndmrgscf.settings.MPIPREFIX = 'mpirun -np 8'\n\n\n'''\nTriplet and quintet energy gap of Iron-Porphyrin molecule using DMRG-CASSCF\nand DMRG-NEVPT2 methods.  DMRG is an approximate FCI solver.  It can be used\nto handle large active space.  This example is the next step to example\n018-dmet_cas_for_feporph.py\n'''\n\n#\n# Following 018-dmet_cas_for_feporph.py, we still use density matrix embedding\n# theory (DMET) to generate CASSCF initial guess.  The active space includes\n# the Fe double d-shell, 4s shell, and the ligand N 2pz orbitals to describe\n# metal-ligand pi bond and pi backbond.\n#\n\n\n##################################################\n#\n# Define DMET active space\n#\n##################################################\ndef dmet_cas(mc, dm, implst):\n    from pyscf import lo\n    nao = mc.mol.nao_nr()\n    ncore = mc.ncore\n    ncas = mc.ncas\n    nocc = ncore + ncas\n    nimp = len(implst)\n    nbath = ncas - nimp\n    corth = lo.orth.orth_ao(mol, method='meta_lowdin')\n    s = mol.intor_symmetric('cint1e_ovlp_sph')\n    cinv = numpy.dot(corth.T, s)\n    #\n    # Sum over spin-orbital DMs, then transform spin-free DM to orthogonal basis\n    #\n    dm = reduce(numpy.dot, (cinv, dm[0]+dm[1], cinv.T))\n\n    #\n    # Decomposing DM to get impurity orbitals, doubly occupied core orbitals\n    # and entangled bath orbitals.  Active space is consist of impurity plus\n    # truncated bath.\n    #\n    implst = numpy.asarray(implst)\n    notimp = numpy.asarray([i for i in range(nao) if i not in implst])\n    occi, ui = scipy.linalg.eigh(-dm[implst][:,implst])\n    occb, ub = scipy.linalg.eigh(-dm[notimp][:,notimp])\n    bathorb = numpy.dot(corth[:,notimp], ub)\n    imporb = numpy.dot(corth[:,implst], ui)\n    mocore = bathorb[:,:ncore]\n    mocas  = numpy.hstack((imporb, bathorb[:,ncore:ncore+nbath]))\n    moext  = bathorb[:,ncore+nbath:]\n\n    #\n    # Restore core, active and external space to \"canonical\" form.  Spatial\n    # symmetry is reserved in this canonicalization.\n    #\n    hf_orb = mc._scf.mo_coeff\n    fock = reduce(numpy.dot, (s, hf_orb*mc._scf.mo_energy, hf_orb.T, s))\n\n    fockc = reduce(numpy.dot, (mocore.T, fock, mocore))\n    e, u = scipy.linalg.eigh(fockc)\n    mocore = numpy.dot(mocore, u)\n    focka = reduce(numpy.dot, (mocas.T, fock, mocas))\n    e, u = scipy.linalg.eigh(focka)\n    mocas = numpy.dot(mocas, u)\n    focke = reduce(numpy.dot, (moext.T, fock, moext))\n    e, u = scipy.linalg.eigh(focke)\n    moext = numpy.dot(moext, u)\n\n    #\n    # Initial guess\n    #\n    mo_init = numpy.hstack((mocore, mocas, moext))\n    return mo_init\n\n\n\n\n##################################################\n#\n# Quintet\n#\n##################################################\n\nmol = gto.Mole()\nmol.atom = [\n    ['Fe', (0.      , 0.0000  , 0.0000)],\n    ['N' , (1.9764  , 0.0000  , 0.0000)],\n    ['N' , (0.0000  , 1.9884  , 0.0000)],\n    ['N' , (-1.9764 , 0.0000  , 0.0000)],\n    ['N' , (0.0000  , -1.9884 , 0.0000)],\n    ['C' , (2.8182  , -1.0903 , 0.0000)],\n    ['C' , (2.8182  , 1.0903  , 0.0000)],\n    ['C' , (1.0918  , 2.8249  , 0.0000)],\n    ['C' , (-1.0918 , 2.8249  , 0.0000)],\n    ['C' , (-2.8182 , 1.0903  , 0.0000)],\n    ['C' , (-2.8182 , -1.0903 , 0.0000)],\n    ['C' , (-1.0918 , -2.8249 , 0.0000)],\n    ['C' , (1.0918  , -2.8249 , 0.0000)],\n    ['C' , (4.1961  , -0.6773 , 0.0000)],\n    ['C' , (4.1961  , 0.6773  , 0.0000)],\n    ['C' , (0.6825  , 4.1912  , 0.0000)],\n    ['C' , (-0.6825 , 4.1912  , 0.0000)],\n    ['C' , (-4.1961 , 0.6773  , 0.0000)],\n    ['C' , (-4.1961 , -0.6773 , 0.0000)],\n    ['C' , (-0.6825 , -4.1912 , 0.0000)],\n    ['C' , (0.6825  , -4.1912 , 0.0000)],\n    ['H' , (5.0441  , -1.3538 , 0.0000)],\n    ['H' , (5.0441  , 1.3538  , 0.0000)],\n    ['H' , (1.3558  , 5.0416  , 0.0000)],\n    ['H' , (-1.3558 , 5.0416  , 0.0000)],\n    ['H' , (-5.0441 , 1.3538  , 0.0000)],\n    ['H' , (-5.0441 , -1.3538 , 0.0000)],\n    ['H' , (-1.3558 , -5.0416 , 0.0000)],\n    ['H' , (1.3558  , -5.0416 , 0.0000)],\n    ['C' , (2.4150  , 2.4083  , 0.0000)],\n    ['C' , (-2.4150 , 2.4083  , 0.0000)],\n    ['C' , (-2.4150 , -2.4083 , 0.0000)],\n    ['C' , (2.4150  , -2.4083 , 0.0000)],\n    ['H' , (3.1855  , 3.1752  , 0.0000)],\n    ['H' , (-3.1855 , 3.1752  , 0.0000)],\n    ['H' , (-3.1855 , -3.1752 , 0.0000)],\n    ['H' , (3.1855  , -3.1752 , 0.0000)],\n]\nmol.basis = 'ccpvdz'\nmol.verbose = 4\nmol.output = 'fepor-dmrgscf.out'\nmol.spin = 4\nmol.symmetry = True\nmol.build()\n\nmf = scf.ROHF(mol)\nmf = scf.fast_newton(mf)\n\n#\n# CAS(16e, 20o)\n#\n# mcscf.approx_hessian approximates the orbital hessian.  It does not affect\n# results.  The N-2pz orbitals introduces more entanglement to environment.\n# 5 bath orbitals which have the strongest entanglement to impurity are\n# considered in active space.\n#\nmc = mcscf.approx_hessian(dmrgscf.dmrgci.DMRGSCF(mf, 20, 16))\n# Function mol.search_ao_label returns the indices of the required AOs\n# It is equivalent to the following expression\n#idx = [i for i,s in enumerate(mol.ao_labels())\n#       if 'Fe 3d' in s or 'Fe 4d' in s or 'Fe 4s' in s or 'N 2pz' in s]\nidx = mol.search_ao_label(['Fe 3d', 'Fe 4d', 'Fe 4s', 'N 2pz'])\nmo = dmet_cas(mc, mf.make_rdm1(), idx)\n\nmc.fcisolver.wfnsym = 'Ag'\nmc.kernel(mo)\n#mc.analyze()\ne_q = mc.e_tot  # -2244.90267106288\ncas_q = mc.mo_coeff[:,mc.ncore:mc.ncore+mc.ncas]\n\n#\n# call DMRG-NEVPT2 (about 2 days, 100 GB memory)\n#\nept2_q = mrpt.NEVPT(mc).kernel()\n\n\n\n\n\n##################################################\n#\n# Triplet\n#\n##################################################\n\nmol.spin = 2\nmol.build(0, 0)\n\nmf = scf.ROHF(mol)\nmf = scf.fast_newton(mf)\n\n#\n# CAS(16e, 20o)\n#\n# Unlike CAS(8e, 11o) which is easily to draw 4s-character orbitals into the\n# active space, the larger active space, which includes 4s orbitals, does not\n# have such issue on MCSCF wfn.\n#\nmc = mcscf.approx_hessian(dmrgscf.dmrgci.DMRGSCF(mf, 20, 16))\nidx = mol.search_ao_label(['Fe 3d', 'Fe 4d', 'Fe 4s', 'N 2pz'])\nmo = dmet_cas(mc, mf.make_rdm1(), idx3d)\nmc.fcisolver.wfnsym = 'B1g'\nmc.kernel(mo)\nmo = mc.mo_coeff\n#mc.analzye()\ne_t = mc.e_tot  # -2244.88920313881\ncas_t = mc.mo_coeff[:,mc.ncore:mc.ncore+mc.ncas]\n\n#\n# call DMRG-NEVPT2 (about 2 days, 100 GB memory)\n#\nept2_t = mrpt.NEVPT(mc).kernel()\n\nprint('E(T) = %.15g  E(Q) = %.15g  gap = %.15g' % (e_t, e_q, e_t-e_q))\n# E(T) = -2244.88920313881  E(Q) = -2244.90267106288  gap = 0.0134679240700279\n\n# The triplet and quintet active space are not perfectly overlaped\ns = reduce(numpy.dot, (cas_t.T, mol.intor('cint1e_ovlp_sph'), cas_q))\nprint('Active space overlpa <T|Q> ~ %f' % numpy.linalg.det(s))\n\nprint('NEVPT2: E(T) = %.15g  E(Q) = %.15g' % (ept2_t, ept2_q))\n# E(T) = -3.52155285166390  E(Q) = -3.46277436661638\n\n\n\n\n\n\n##################################################\n#\n# Output the active space orbitals to molden format\n#\n##################################################\nfrom pyscf import tools\ntools.molden.from_mo(mol, 'triplet-cas.molden', cas_t)\ntools.molden.from_mo(mol, 'quintet-cas.molden', cas_q)\n",
  "\"\"\"\n.. _ref_volume_averaged_stress_advanced:\n\nAverage elemental stress on a given volume\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis example shows how to find the minimum list of surrounding\nelements for a given node to get a minimum volume.\nFor each list of elements, the elemental stress equivalent is multiplied by the\nvolume of each element. This result is then accumulated to divide it by the\ntotal volume.\n\n\"\"\"\nfrom ansys.dpf import core as dpf\nfrom ansys.dpf.core import examples\nfrom ansys.dpf.core import operators as ops\n\n\n###############################################################################\n# Create a model targeting a given result file\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The model provides easy access to the mesh and time frequency support.\n\nmodel = dpf.Model(examples.find_complex_rst())\nmesh = model.metadata.meshed_region\n\n# Volume size to check\nvolume_check = 4.0e-11\n\n# Get all node IDs in the model to find the minimum amount of\n# surrounding elements to get a minimum volume.\nnodes = mesh.nodes.scoping\nnodes_ids = nodes.ids\nnodes_ids_to_compute = []\nfor i in range(0, 400):\n    nodes_ids_to_compute.append(nodes_ids[i])\nelements = mesh.elements.scoping\nelements_ids = elements.ids\n\n###############################################################################\n# Read the volume by element\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~\nvol_op = ops.result.elemental_volume()\nvol_op.inputs.streams_container(model.metadata.streams_provider)\nvol_field = vol_op.outputs.fields_container()[0]\n\n###############################################################################\n# Find the minimum list of elements by node to get the volume check\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# get the connectivity and inverse connectivity fields\nconnectivity_field = mesh.elements.connectivities_field\nnodal_connectivity_field = mesh.nodes.nodal_connectivity_field\n\nnode_index_to_el_ids = {}\nnode_index_to_found_volume = {}\n# using the with statement with as_local_field allows to bring the server's\n# data locally and to work only on the local process before sending the data\n# updates to the server as the end of the with statement\n# the performances are a lot better using this syntax\n# fmt: off\nwith connectivity_field.as_local_field() as connectivity, \\\n    nodal_connectivity_field.as_local_field() as nodal_connectivity,\\\n        vol_field.as_local_field() as vol:  # fmt: on\n    for i, node in enumerate(nodes_ids_to_compute):\n\n        current_node_indexes = [i]\n        volume = 0.0\n        # Loop through recursively selecting elements attached\n        # to nodes until specified volume is reached\n        while volume_check > volume:\n            volume = 0.0\n            elements_indexes = []\n\n            # Get elements attached to nodes\n            for current_node_index in current_node_indexes:\n                elements_indexes.extend(nodal_connectivity.get_entity_data(i).flatten())\n\n            current_node_indexes = []\n            for index in elements_indexes:\n                # Sum up the volume on those elements\n                volume += vol.get_entity_data(index)[0]\n                # Get all nodes of the current elements for next iteration\n                current_node_indexes.extend(connectivity.get_entity_data(index))\n\n        node_index_to_el_ids[i] = [elements_ids[index] for index in elements_indexes]\n        node_index_to_found_volume[i] = volume\n\n###############################################################################\n# Create workflow\n# ~~~~~~~~~~~~~~~\n# For each list of elements surrounding nodes:\n#\n# - Compute equivalent stress averaged on elements.\n# - Apply dot product seqv.volume.\n# - Sum up those on the list of elements.\n# - Divide this sum by the total volume on these elements.\n#\n\ns = model.results.stress()\nto_elemental = ops.averaging.to_elemental_fc(s)\neqv = ops.invariant.von_mises_eqv_fc(to_elemental)\nvalues_to_sum_field = eqv.outputs.fields_container()[0]\n\n# sum up the seqv by list of elements and create a Field\nseqvsum = dpf.fields_factory.create_scalar_field(len(nodes), dpf.locations.nodal)\ndataseqvsum = []\nvolsum = dpf.fields_factory.create_scalar_field(len(nodes), dpf.locations.nodal)\ndatavolsum = []\n\nwith values_to_sum_field.as_local_field() as values_to_sum:\n    with vol_field.as_local_field() as vol:\n        for key in node_index_to_el_ids:\n            ssum = 0.0\n            for id in node_index_to_el_ids[key]:\n                ssum += (\n                    values_to_sum.get_entity_data_by_id(id)[0] * vol.get_entity_data_by_id(id)[0]\n                )\n            dataseqvsum.append(ssum)\n            datavolsum.append(node_index_to_found_volume[key])\n\nseqvsum.data = dataseqvsum\nseqvsum.scoping.ids = nodes_ids_to_compute\n\nvolsum.data = datavolsum\nvolsum.scoping.ids = nodes_ids_to_compute\n\n# use component wise divide to average the stress by the volume\ndivide = ops.math.component_wise_divide(seqvsum, volsum)\ndivide.run()\n\n###############################################################################\n# Plot equivalent elemental stress and volume averaged elemental equivalent stress\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nmesh.plot(values_to_sum_field)\nmesh.plot(divide.outputs.field())\n\n###############################################################################\n# Use the operator instead\n# ~~~~~~~~~~~~~~~~~~~~~~~~\n# An operator with the same algorithm has been implemented\ns_fc = s.outputs.fields_container()\nsingle_field_vol_fc = dpf.fields_container_factory.over_time_freq_fields_container([vol_field])\n\nsingle_field_fc = dpf.fields_container_factory.over_time_freq_fields_container(\n    [values_to_sum_field]\n)\n\nop = dpf.Operator(\"volume_stress\")\nop.inputs.scoping.connect(nodes)\nop.inputs.stress_fields.connect(single_field_fc)\nop.inputs.volume_fields(single_field_vol_fc)\nop.inputs.volume(volume_check * 10.0)\n\nout = op.get_output(0, dpf.types.field)\nmesh.plot(out)\n",
  "\"\"\"\n.. _pressure_vessel_example:\n\nPressure Vessel\n---------------\n\nThis example demonstrates how to create a basic pressure vessel and\napply a pressure to it.\n\nAlso shown here:\n- Various ways of accessing stress results from MAPDL.\n- Comparison between PRNSOL, VGET (efficient wrapping), and the legacy reader.\n- Notes regarding FULL vs. POWER graphics when using PRNSOL.\n\n\"\"\"\nimport numpy as np\n\nfrom ansys.mapdl.core import launch_mapdl\n\n# start mapdl, enter the preprocessor, and set the units\nmapdl = launch_mapdl()\n\nmapdl.clear()\nmapdl.prep7()\n\n# US Customary system using inches (in, lbf*s2/in, s, °F).\nmapdl.units(\"BIN\")\n\n\n###############################################################################\n# Set the materials and element type\n\nmapdl.et(1, \"SOLID285\")\nmapdl.mp(\"EX\", 1, 10e6)\nmapdl.mp(\"PRXY\", 1, 0.3)\nmapdl.mp(\"DENS\", 1, 0.1)\nprint(mapdl.mplist())\n\n\n###############################################################################\n# Create the Geometry\n\n# area generation\nheight = 10\ninner_width = 2.5\nouter_width = 3.5\nmapdl.rectng(inner_width, outer_width, 0, height)\nmapdl.cyl4(0, height, inner_width, 0, outer_width, 90)\n\n# combine areas\na_comb = mapdl.aadd(1, 2)\nmapdl.aplot(color=\"grey\", background=\"w\", show_area_numbering=True)\n\n# Generate a cylindrical volume by rotating an area pattern about an axis\nmapdl.vrotat(a_comb, pax1=6, arc=90)\nmapdl.vplot(background=\"w\")\n\n\n###############################################################################\n# Create the mesh\nmapdl.smrtsize(1)\nmapdl.esize(0.25, 0)\nmapdl.mshape(1, \"3D\")\nmapdl.mshkey(0)\nmapdl.vmesh(\"ALL\")\nmapdl.eplot(color=\"grey\", background=\"w\")\n\n\n###############################################################################\n# Solve\n\n# boundary condition selection\nmapdl.geometry.area_select([3, 5, 7])\nmapdl.da(\"ALL\", \"SYMM\")\nmapdl.allsel()\n\n# apply pressure\nmapdl.geometry.area_select([1, 6])\nmapdl.sfa(\"ALL\", 1, \"PRES\", 1000)\nmapdl.allsel()\n\n# solver\nmapdl.run(\"/SOL\")\nmapdl.antype(0)\nmapdl.outres(\"ALL\", \"ALL\")\nmapdl.run(\"/STATUS,SOLU\")\nsol_output = mapdl.solve()\nmapdl.finish()\n\n\n###############################################################################\n# Post-Processing\n# ~~~~~~~~~~~~~~~\n# Enter the MAPDL post-postprocessing routine (/POST1) and obtain the\n# von-mises stress for the single static solution. Here, we use MAPDL\n# directly to obtain the results using a wrapper around the VGET\n# method to efficiently obtain results without outputting to disk.\n\n# enter the postprocessing routine\nmapdl.post1()\nmapdl.set(1, 1)\n\n# results directly from MAPDL's VGET command\n# VGET, __VAR__, NODE, , S, EQV\nnnum = mapdl.mesh.nnum\nvon_mises_mapdl = mapdl.post_processing.nodal_eqv_stress()\n\n# we could print out the solution for each node with:\n\nprint(f\"\\nNode  Stress (psi)\")\nfor node_num, stress_value in zip(nnum[:5], von_mises_mapdl[:5]):\n    print(f\"{node_num:<5d} {stress_value:.3f}\")\nprint(\"...\")\n\n# or simply get the maximum stress value and corresponding node\nidx = np.argmax(von_mises_mapdl)\nnode_num = nnum[idx]\nstress_value = von_mises_mapdl[idx]\nprint(f\"\\nMaximum Stress\")\nprint(f\"Node  Stress (psi)\")\nprint(f\"{node_num:<5d} {stress_value:.3f}\")\n\n###############################################################################\n# Plot the results\n\nmapdl.post_processing.plot_nodal_eqv_stress(cpos=\"zy\")\n\n\n###############################################################################\n# We could, alternatively, get the exact same results by directly\n# accessing the result file using the legacy file reader\n# `ansys-mapdl-reader <https://github.com/ansys/pymapdl-reader>`_.\n\n# access the result\nresult = mapdl.result\n\n# Get the von mises stess and show that this is equivalent to the\n# stress obtained from MAPDL.\nnnum, stress = result.principal_nodal_stress(0)\nvon_mises = stress[:, -1]  # von-Mises stress is the right most column\nmin_von_mises, max_von_mises = np.min(von_mises), np.max(von_mises)\nprint(\"All close:\", np.allclose(von_mises, von_mises_mapdl))\n\n###############################################################################\n# That these results are equivalent to results from PRNSOL.\n#\n# .. note::\n#    Enabling POWER GRAPHICS with ``mapdl.graphics('POWER')`` will\n#    change the averaging scheme.\n\nmapdl.header(\"OFF\", \"OFF\", \"OFF\", \"OFF\", \"OFF\", \"OFF\")\ntable = mapdl.prnsol(\"S\", \"PRIN\").splitlines()[1:]\nprnsol_eqv = np.genfromtxt(table)[:, -1]  # eqv is the last column\n\n# show these are equivalent (RTOL due to rounding within PRNSOL)\nprint(\"All close:\", np.allclose(von_mises, prnsol_eqv, rtol=1e-4))\n\nprint(f\"LEGACY Reader and MAPDL VGET Min: {min_von_mises}\")\nprint(f\"PRNSOL MAPDL Min:                 {prnsol_eqv.min()}\")\nprint()\nprint(f\"LEGACY Reader and MAPDL VGET Min: {max_von_mises}\")\nprint(f\"PRNSOL MAPDL Min:                 {prnsol_eqv.max()}\")\n\n###############################################################################\n# Stop mapdl\n# ~~~~~~~~~~\n#\nmapdl.exit()\n",
  "#!/usr/bin/env python3\n#\n# This script is used to test radial transport of n_re with a (constant)  \n# scalar diffusion coefficient. Hot-tail RE generated with exp. temperature drop.\n#\n# By Ida Svenningsson, 2020\n#\n# ###################################################################\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sys\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nfrom DREAM.DREAMOutput import DREAMOutput\nfrom DREAM import runiface\n\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.TransportSettings as Transport\nimport DREAM.Settings.CollisionHandler as Collisions\nimport DREAM.Settings.Equations.ElectricField as Efield\nimport DREAM.Settings.Equations.RunawayElectrons as RE\nimport DREAM.Settings.Equations.HotElectronDistribution as FHot\nimport DREAM.Settings.Equations.ColdElectronTemperature as T_cold\n\n\nfrom DREAM.Settings.Equations.ElectricField import ElectricField\nfrom DREAM.Settings.Equations.ColdElectronTemperature import ColdElectronTemperature\n\nfrom DREAM import DREAMIO\n\nds = DREAMSettings()\n\n# set collision settings\n\n######################\n# COLLISION SETTINGS #\n######################\n\nds.collisions.collfreq_mode = Collisions.COLLFREQ_MODE_SUPERTHERMAL \nds.collisions.collfreq_type = Collisions.COLLFREQ_TYPE_PARTIALLY_SCREENED # NON_SCREENED, PARTIALLY_SCREENED, COMPLETELY_SCREENED\nds.collisions.bremsstrahlung_mode = Collisions.BREMSSTRAHLUNG_MODE_STOPPING_POWER\nds.collisions.lnlambda = Collisions.LNLAMBDA_ENERGY_DEPENDENT\nds.collisions.pstar_mode = Collisions.PSTAR_MODE_COLLISIONAL\n\nrun_init = True\nrun_exp = True\n\n#transport_mode = Transport.TRANSPORT_PRESCRIBED\ntransport_mode = Transport.TRANSPORT_RECHESTER_ROSENBLUTH\n\n#############################\n# Set simulation parameters #\n#############################\n\n# initial run (to get correct E-field profile)\nTmax_init = 1e-11 \nNt_init = 2       \n\n# Exponential temperature drop\nTfinal_exp = 50 \nt0_exp = .5e-3 \nTmax_exp = 10e-3 \nNt_exp = 3000\ntimes_exp = np.linspace(0,Tmax_exp,Nt_exp) \n\nn_D = 1e20 # originally present in the plasma\n\nB0 = 5              # magnetic field strength in Tesla\nE_initial = 5e-3    # initial electric field in V/m\nE_wall = 0.0        # boundary electric field in V/m\nT_initial = 20e3    # initial temperature in eV\nT_in_back = 10      # initial T_cold value (?)\njTot = 1.69e6\n\nNr_kin  = 15        # number of radial grid points\nNp      = 100        # number of momentum grid points\nNxi     = 1         # number of pitch grid points\npMax    = 3         # maximum momentum in m_e*c\ntimes   = [0]       # times at which parameters are given\nradius  = [0, 2]    # span of the radial grid\nradius_wall = 2.15  # location of the wall \n\ndiffusion_coeff = 100 # m/s^2   -- Diffusion coefficient\n\nhotTailGrid_enabled = True\nif hotTailGrid_enabled == False and transport_mode == Transport.TRANSPORT_RECHESTER_ROSENBLUTH:\n    print('WARNING: Using Rechester-Rosenbluth transport requires f_hot. Enabling hot-tail grid...')\n\n# Set up radial grid\nds.radialgrid.setB0(B0)\nds.radialgrid.setMinorRadius(radius[-1])\nds.radialgrid.setWallRadius(radius_wall)\nds.radialgrid.setNr(Nr_kin)\n\n# Set time stepper\nds.timestep.setTmax(Tmax_init)\nds.timestep.setNt(Nt_init)\n\n# Set ions\ndensity_D = n_D\n\n#ds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_DYNAMIC_FULLY_IONIZED, n=density_D)\n#ds.eqsys.n_i.addIon(name='Ne', Z=10, iontype=Ions.IONS_DYNAMIC_NEUTRAL, n=density_Z)\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_PRESCRIBED_FULLY_IONIZED, n=density_D)\n#ds.eqsys.n_i.addIon(name='Ar', Z=18, iontype=Ions.IONS_PRESCRIBED_NEUTRAL, n=n_Z)\n\n\n# Set E_field \nefield = E_initial*np.ones((len(times), len(radius)))\nds.eqsys.E_field.setPrescribedData(efield=efield, times=times, radius=radius)\n\nradialgrid = np.linspace(radius[0],radius[-1],Nr_kin)\ntemp_prof=(1-0.99*(radialgrid/radialgrid[-1])**2).reshape(1,-1)\ntemperature_init = Tfinal_exp+(T_initial*temp_prof - Tfinal_exp)\n\nds.eqsys.T_cold.setPrescribedData(temperature=temperature_init, times=[0], radius=radialgrid)\n\nds.eqsys.n_re.setAvalanche(RE.AVALANCHE_MODE_FLUID)\nds.hottailgrid.setEnabled(False) # To be enabled later\n\n# Disable runaway grid\nds.runawaygrid.setEnabled(False)\n\n# Use the new nonlinear solver\nds.solver.setType(Solver.NONLINEAR)\nds.solver.tolerance.set(reltol=1e-4)\nds.solver.setMaxIterations(maxiter = 100)\nds.solver.setVerbose(False)\nds.output.setTiming(False)\nds.other.include('fluid', 'transport')\n\nif not hotTailGrid_enabled:\n    ds.hottailgrid.setEnabled(False)\nelse:\n    ds.hottailgrid.setEnabled(True)\n    ds.hottailgrid.setNxi(Nxi)\n    ds.hottailgrid.setNp(Np)\n    ds.hottailgrid.setPmax(pMax)\n    nfree_initial, rn0 = ds.eqsys.n_i.getFreeElectronDensity()\n    #ds.eqsys.f_hot.setInitialProfiles(rn0=rn0, n0=nfree_initial*.99, rT0=0, T0=T_initial)\n    ds.eqsys.f_hot.setInitialProfiles(rn0=rn0, n0=nfree_initial*.99, rT0=radialgrid, T0=temperature_init[0,:])\n    ds.eqsys.f_hot.setBoundaryCondition(bc=FHot.BC_F_0)\n    ds.eqsys.f_hot.setAdvectionInterpolationMethod(ad_int=FHot.AD_INTERP_TCDF)\n\n#########################################\n# XXX: Prescribe diffusive transport\n#########################################\nif transport_mode == Transport.TRANSPORT_PRESCRIBED:\n    ds.eqsys.n_re.transport.prescribeDiffusion(drr=diffusion_coeff)\n    ds.eqsys.n_re.transport.setBoundaryCondition(Transport.BC_F_0)\n    # with hyperresistivity:\n#    ds.eqsys.psi_p.transport.prescribeDiffusion(drr=1e-5)\n#    ds.eqsys.psi_p.transport.setBoundaryCondition(Transport.BC_CONSERVATIVE)\nelif transport_mode  == Transport.TRANSPORT_RECHESTER_ROSENBLUTH and hotTailGrid_enabled:\n    ds.eqsys.f_hot.transport.setMagneticPerturbation(1e-5)\n    ds.eqsys.f_hot.transport.setBoundaryCondition(Transport.BC_F_0)\n#########################################\n\n########\n# init # \n########\n# To get the right initial current profile\n\nif run_init:\n    ds.save('initsim.h5')\n    runiface(ds,f'out_1init.h5')\n\n######################################\n# RE-SCALE E-FIELD FOR RIGHT CURRENT #\n######################################\ndo=DREAMOutput(f'out_1init.h5')\nconductivity=do.other.fluid.conductivity.getData()\njprof=(1-(1-0.001**(1/0.41))*(radialgrid/radialgrid[-1])**2)**0.41\nefield=jTot*jprof/conductivity[-1,:]\nds.eqsys.E_field.setPrescribedData(efield=efield, radius=radialgrid)\n\nds.save(f'settings_1init.h5')        \nif run_init:\n    runiface(ds,f'out_1init.h5')\n\n#######\n# exp # \n#######\n# Start expdecay\n\nds3 = DREAMSettings(ds)\nif run_exp:\n    ds3.fromOutput(f'out_1init.h5')\n\ntemperature_exp = Tfinal_exp+(T_initial*temp_prof - Tfinal_exp) * np.exp(-times_exp/t0_exp).reshape(-1,1)\nds3.eqsys.T_cold.setPrescribedData(temperature=temperature_exp, times=times_exp, radius=radialgrid)\nds3.eqsys.E_field.setType(Efield.TYPE_SELFCONSISTENT)\nds3.eqsys.E_field.setBoundaryCondition(bctype = Efield.BC_TYPE_PRESCRIBED, inverse_wall_time = 0, V_loop_wall_R0 = E_wall*2*np.pi)\n\nds3.timestep.setTmax(Tmax_exp)\nds3.timestep.setNt(Nt_exp)\n\nds3.save(f'settings_2exp.h5')\nif run_exp:\n    runiface(ds3,f'out_2exp.h5')\n        \n",
  "# -*- coding: utf-8 -*-\n###\n# (C) Copyright [2021] Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n\nfrom pprint import pprint\nfrom hpeOneView.oneview_client import OneViewClient\nfrom config_loader import try_load_from_file\n\n\nconfig = {\n    \"ip\": \"\",\n    \"credentials\": {\n        \"userName\": \"\",\n        \"password\": \"\"\n    }\n}\n\n# Try load config from a file (if there is a config file)\nconfig = try_load_from_file(config)\n\noneview_client = OneViewClient(config)\nusers = oneview_client.users\nscopes = oneview_client.scopes\n\n# Get the scope Uri\nscope_options = {\n    \"name\": \"SampleScopeForTest\",\n    \"description\": \"Sample Scope description\"\n}\nscope = scopes.get_by_name(scope_options['name'])\nif not scope:\n    scope = scopes.create(scope_options)\nscope_uri = scope.data['uri']\n\noptions = {\n    'emailAddress': 'testUser@example.com',\n    'enabled': 'true',\n    'fullName': 'testUser101',\n    'mobilePhone': '555-2121',\n    'officePhone': '555-1212',\n    'password': 'myPass1234',\n    'permissions': [\n        {\n            'roleName': 'Infrastructure administrator',\n            'scopeUri': scope_uri\n        }\n    ],\n    'type': 'UserAndPermissions',\n    'userName': 'testUser'\n}\n\nmulti_users = [\n    {\n        'emailAddress': 'testUser@example.com',\n        'enabled': 'true',\n        'fullName': 'testUser101',\n        'mobilePhone': '555-2121',\n        'officePhone': '555-1212',\n        'password': 'myPass1234',\n        'permissions': [\n            {\n                'roleName': 'Read only',\n            }\n        ],\n        'type': 'UserAndPermissions',\n        'userName': 'testUser1'\n    },\n    {\n        'emailAddress': 'testUser@example.com',\n        'enabled': 'true',\n        'fullName': 'testUser101',\n        'mobilePhone': '555-2121',\n        'officePhone': '555-1212',\n        'password': 'myPass1234',\n        'permissions': [\n            {\n                'roleName': 'Read only',\n            }\n        ],\n        'type': 'UserAndPermissions',\n        'userName': 'testUser2'\n    }\n]\n\n# Create a User\nuser = users.create(options)\nprint(\"Created user '%s' successfully.\\n  uri = '%s'\\n\" % (user.data['userName'], user.data['uri']))\nprint(user.data)\n\n# Create a Multiple Users\nmulti_user = users.create_multiple_user(multi_users)\nprint(\"\\nCreated multiple users successfully.\\n\")\nprint(multi_user.data)\n\n# Updata the user\ndata = user.data.copy()\ndata[\"password\"] = \"change1234\"\nupdated_user = user.update(data)\nprint(\"\\nThe users is updated successfully....\\n\")\nprint(updated_user.data)\n\n# Add role to userName\nrole_options = [\n    {\n        \"roleName\": \"Backup administrator\"\n    }\n]\nrole = users.add_role_to_userName(\"testUser1\", role_options)\nprint(\"\\nSuccessfully added new role to existing one....\\n\")\nprint(role.data)\n\n# Update role to userName (it will replace entrie role with specified role)\nrole_options = [\n    {\n        \"roleName\": \"Scope administrator\"\n    },\n    {\n        \"roleName\": \"Backup administrator\"\n    },\n    {\n        \"roleName\": \"Infrastructure administrator\"\n    }\n]\n\nrole = users.update_role_to_userName(\"testUser1\", role_options)\nprint(\"\\nSuccessfully updated the role to the username....\\n\")\nprint(role)\n\n# Remove mulitple role from the user\n# If a single role is to be removed, just specifiy [\"role_name\"] or \"role_name\" instead of list.\nrole = users.remove_role_from_username(\"testUser1\", [\"Scope administrator\", \"Backup administrator\"])\nprint(\"\\nRemoved role from the user successfully...\\n\")\nprint(role)\n\n# Get user by name\nuser = users.get_by_userName(options['userName'])\nif user:\n    print(\"\\nFound user by uri = '%s'\\n\" % user.data['uri'])\n\n# Get all users\nprint(\"\\nGet all users\")\nall_users = users.get_all()\npprint(all_users)\n\n# Validates if full name is already in use\nbol = users.validate_full_name(options['fullName'])\nprint(\"Is full name already in use? %s\" % (bol.data))\n\n# Validates if user name is already in use\nbol = users.validate_user_name(options['userName'])\nprint(\"Is user name already in use? %s\" % (bol.data))\n\n# Get the user's role list\nrolelist = users.get_role_associated_with_userName(\"testUser\")\nprint(\"\\n>> Got all the roles for the users\\n\")\nprint(rolelist)\n\n# Get by role\nrole = users.get_user_by_role(\"Infrastructure administrator\")\nprint(\"\\n>> Got the users by role name\\n\")\nprint(role)\n\n# Remove single user\nuser_to_delete = users.get_by_userName(\"testUser\")\nif user_to_delete:\n    user_to_delete.delete()\n    print(\"\\nSuccessfully deleted the testuser2 user.....\\n\")\n\n# Remove Multiple users\nuser_name = [\"testUser1\", \"testUser2\"]\nusers.delete_multiple_user(user_name)\nprint(\"\\nDeleted multiple users successfully...\\n\")\n\n# NOTE: The below script changes the default administrator's password during first-time appliance setup only.\n'''\n# Change Password only during the initial setup of the appliance.\nchange_password_request = {\n    \"oldPassword\": \"mypass1234\",\n    \"newPassword\": \"admin1234\",\n    \"userName\": \"testUser3\"\n}\nchangePasswordResponse = users.change_password(change_password_request)\nprint(\"Changed Password successfully\")\nprint(changePasswordResponse)\n'''\n",
  "\"\"\"\n2D Extractor: CPWG analysis\n---------------------------\nThis example shows how you can use PyAEDT to create a CPWG (coplanar waveguide with ground) design\nin 2D Extractor and run a simulation.\n\"\"\"\n###############################################################################\n# Perform required imports\n# ~~~~~~~~~~~~~~~~~~~~~~~~\n# Perform required imports.\n\nimport os\nimport pyaedt\n\n###############################################################################\n# Set non-graphical mode\n# ~~~~~~~~~~~~~~~~~~~~~~\n# Set non-graphical mode. \n# You can set ``non_graphical`` either to ``True`` or ``False``.\n\nnon_graphical = False\ndesktop_version = \"2023.2\"\n###############################################################################\n# Launch AEDT and 2D Extractor\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Launch AEDT 2023 R2 in graphical mode and launch 2D Extractor. This example\n# uses SI units.\n\nq = pyaedt.Q2d(specified_version=desktop_version,\n               non_graphical=non_graphical,\n               new_desktop_session=True,\n               projectname=pyaedt.generate_unique_name(\"pyaedt_q2d_example\"),\n               designname=\"coplanar_waveguide\")\n\n###############################################################################\n# Define variables\n# ~~~~~~~~~~~~~~~~\n# Define variables.\n\ne_factor = \"e_factor\"\nsig_bot_w = \"sig_bot_w\"\nco_gnd_w = \"gnd_w\"\nclearance = \"clearance\"\ncond_h = \"cond_h\"\nd_h = \"d_h\"\nsm_h = \"sm_h\"\n\nfor var_name, var_value in {\n    \"sig_bot_w\": \"150um\",\n    \"e_factor\": \"2\",\n    \"gnd_w\": \"500um\",\n    \"clearance\": \"150um\",\n    \"cond_h\": \"50um\",\n    \"d_h\": \"150um\",\n    \"sm_h\": \"20um\",\n}.items():\n    q[var_name] = var_value\n\ndelta_w_half = \"({0}/{1})\".format(cond_h, e_factor)\nsig_top_w = \"({1}-{0}*2)\".format(delta_w_half, sig_bot_w)\nco_gnd_top_w = \"({1}-{0}*2)\".format(delta_w_half, co_gnd_w)\nmodel_w = \"{}*2+{}*2+{}\".format(co_gnd_w, clearance, sig_bot_w)\n\n###############################################################################\n# Create primitives\n# ~~~~~~~~~~~~~~~~~\n# Create primitives and define the layer heights.\n\nlayer_1_lh = 0\nlayer_1_uh = cond_h\nlayer_2_lh = layer_1_uh + \"+\" + d_h\nlayer_2_uh = layer_2_lh + \"+\" + cond_h\n\n###############################################################################\n# Create signal\n# ~~~~~~~~~~~~~\n# Create a signal.\n\nbase_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_lh, 0], [sig_bot_w, layer_2_lh, 0]], name=\"signal\")\ntop_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_uh, 0], [sig_top_w, layer_2_uh, 0]])\nq.modeler.move(objid=[top_line_obj], vector=[delta_w_half, 0, 0])\nq.modeler.connect([base_line_obj, top_line_obj])\nq.modeler.move(objid=[base_line_obj], vector=[\"{}+{}\".format(co_gnd_w, clearance), 0, 0])\n\n###############################################################################\n# Create coplanar ground\n# ~~~~~~~~~~~~~~~~~~~~~~\n# Create a coplanar ground.\n\nbase_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_lh, 0], [co_gnd_w, layer_2_lh, 0]],\n                                          name=\"co_gnd_left\")\ntop_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_uh, 0], [co_gnd_top_w, layer_2_uh, 0]])\nq.modeler.move(objid=[top_line_obj], vector=[delta_w_half, 0, 0])\nq.modeler.connect([base_line_obj, top_line_obj])\n\nbase_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_lh, 0], [co_gnd_w, layer_2_lh, 0]],\n                                          name=\"co_gnd_right\")\ntop_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_uh, 0], [co_gnd_top_w, layer_2_uh, 0]])\nq.modeler.move(objid=[top_line_obj], vector=[delta_w_half, 0, 0])\nq.modeler.connect([base_line_obj, top_line_obj])\nq.modeler.move(objid=[base_line_obj], vector=[\"{}+{}*2+{}\".format(co_gnd_w, clearance, sig_bot_w), 0, 0])\n\n###############################################################################\n# Create reference ground plane\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Create a reference ground plane.\n\nq.modeler.create_rectangle(position=[0, layer_1_lh, 0], dimension_list=[model_w, cond_h], name=\"ref_gnd\")\n\n###############################################################################\n# Create dielectric\n# ~~~~~~~~~~~~~~~~~\n# Create a dielectric.\n\nq.modeler.create_rectangle(\n    position=[0, layer_1_uh, 0], dimension_list=[model_w, d_h], name=\"Dielectric\", matname=\"FR4_epoxy\"\n)\n\n###############################################################################\n# Create conformal coating\n# ~~~~~~~~~~~~~~~~~~~~~~~~\n# Create a conformal coating.\n\nsm_obj_list = []\nids = [1,2,3]\nif desktop_version >= \"2023.1\":\n    ids = [0,1,2]\n\nfor obj_name in [\"signal\", \"co_gnd_left\", \"co_gnd_right\"]:\n    obj = q.modeler.get_object_from_name(obj_name)\n    e_obj_list = []\n    for i in ids:\n        e_obj = q.modeler.create_object_from_edge(obj.edges[i])\n        e_obj_list.append(e_obj)\n    e_obj_1 = e_obj_list[0]\n    q.modeler.unite(e_obj_list)\n    new_obj = q.modeler.sweep_along_vector(e_obj_1.id, [0, sm_h, 0])\n    sm_obj_list.append(e_obj_1)\n\nnew_obj = q.modeler.create_rectangle(position=[co_gnd_w, layer_2_lh, 0], dimension_list=[clearance, sm_h])\nsm_obj_list.append(new_obj)\n\nnew_obj = q.modeler.create_rectangle(position=[co_gnd_w, layer_2_lh, 0], dimension_list=[clearance, sm_h])\nq.modeler.move([new_obj], [sig_bot_w + \"+\" + clearance, 0, 0])\nsm_obj_list.append(new_obj)\n\nsm_obj = sm_obj_list[0]\nq.modeler.unite(sm_obj_list)\nsm_obj.material_name = \"SolderMask\"\nsm_obj.color = (0, 150, 100)\nsm_obj.name = \"solder_mask\"\n\n###############################################################################\n# Assign conductor\n# ~~~~~~~~~~~~~~~~\n# Assign a conductor to the signal.\n\nobj = q.modeler.get_object_from_name(\"signal\")\nq.assign_single_conductor(\n    name=obj.name, target_objects=[obj], conductor_type=\"SignalLine\", solve_option=\"SolveOnBoundary\", unit=\"mm\"\n)\n\n###############################################################################\n# Create reference ground\n# ~~~~~~~~~~~~~~~~~~~~~~~\n# Create a reference ground.\n\nobj = [q.modeler.get_object_from_name(i) for i in [\"co_gnd_left\", \"co_gnd_right\", \"ref_gnd\"]]\nq.assign_single_conductor(\n    name=\"gnd\", target_objects=obj, conductor_type=\"ReferenceGround\", solve_option=\"SolveOnBoundary\", unit=\"mm\"\n)\n\n###############################################################################\n# Assign Huray model on signal\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Assign the Huray model on the signal.\n\nobj = q.modeler.get_object_from_name(\"signal\")\nq.assign_huray_finitecond_to_edges(obj.edges, radius=\"0.5um\", ratio=3, name=\"b_\" + obj.name)\n\n###############################################################################\n# Create setup, analyze, and plot\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Create the setup, analyze it, and plot solution data.\n \nsetup = q.create_setup(setupname=\"new_setup\")\n\nsweep = setup.add_sweep(sweepname=\"sweep1\", sweeptype=\"Discrete\")\nsweep.props[\"RangeType\"] = \"LinearStep\"\nsweep.props[\"RangeStart\"] = \"1GHz\"\nsweep.props[\"RangeStep\"] = \"100MHz\"\nsweep.props[\"RangeEnd\"] = \"5GHz\"\nsweep.props[\"SaveFields\"] = False\nsweep.props[\"SaveRadFields\"] = False\nsweep.props[\"Type\"] = \"Interpolating\"\n\nsweep.update()\n\nq.analyze()\n\na = q.post.get_solution_data(expressions=\"Z0(signal,signal)\", context=\"Original\")\na.plot()\n\n###############################################################################\n# Save project and close AEDT\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Save the project and close AEDT.\n\nhome = os.path.expanduser(\"~\")\nq.save_project(os.path.join(home, \"Downloads\", \"pyaedt_example\", q.project_name + \".aedt\"))\nq.release_desktop()\n",
  "#!/usr/bin/env python\n#\n# Author: Oliver J. Backhouse <olbackhouse@gmail.com>\n#         George H. Booth <george.booth@kcl.ac.uk>\n#\n\n'''\nAn example of restricted AGF2 with density fitting, obtaining the 1RDM and dipole moment\n\nDefault AGF2 corresponds to the AGF2(1,0) method outlined in the papers:\n  - O. J. Backhouse, M. Nusspickel and G. H. Booth, J. Chem. Theory Comput., 16, 1090 (2020).\n  - O. J. Backhouse and G. H. Booth, J. Chem. Theory Comput., 16, 6294 (2020).\n'''\n\nfrom pyscf import gto, scf, agf2\nimport numpy as np\nfrom functools import reduce\n\nmol = gto.M(atom='O 0 0 0; H 0 0 1; H 0 1 0', basis='cc-pvdz')\n\nmf = scf.RHF(mol).density_fit(auxbasis='cc-pv5z-ri')\nmf.conv_tol = 1e-12\nmf.run()\n\n# Run an AGF2 calculation\ngf2 = agf2.AGF2(mf)\ngf2.conv_tol = 1e-7\ngf2.run(verbose=4)\n\n# Print the first 3 ionization potentials\ngf2.ipagf2(nroots=3)\n\n# Print the first 3 electron affinities\ngf2.eaagf2(nroots=3)\n\n# Get the MO-basis density matrix and calculate dipole moments:\ndm = gf2.make_rdm1()\n\ndipole = [0.0, 0.0, 0.0]\n# Transform dipole moment integrals into MO basis\nmol.set_common_origin([0,0,0])\nr_ints_ao = mol.intor('cint1e_r_sph', comp=3)\nr_ints_mo = np.empty_like(r_ints_ao)\nfor i in range(3):\n    r_ints_mo[i] = reduce(np.dot,(mf.mo_coeff.T, r_ints_ao[i], mf.mo_coeff))\n    dipole[i] = -np.trace(np.dot(dm, r_ints_mo[i]))\n    # Add nuclear component\n    for j in range(mol.natm):\n        dipole[i] += mol.atom_charge(j) * mol.atom_coord(j)[i]\n\nprint('Dipole moment from AGF2: {} {} {}'.format(dipole[0], dipole[1], dipole[2]))\n",
  "import argparse \nimport numpy as np\nfrom scipy.sparse.linalg import spsolve\nimport matplotlib.pyplot as plt\n\n# 模型数据\nfrom fealpy.pde.poisson_2d import CosCosData\nfrom fealpy.pde.poisson_2d import LShapeRSinData\n\n# 网格 \nfrom fealpy.mesh import PolygonMesh\nfrom fealpy.mesh import HalfEdgeMesh2d\n\n# 非协调空间\nfrom fealpy.functionspace import NonConformingScalarVESpace2d\n\n# 积分子\nfrom fealpy.vem import ScaledMonomialSpaceMassIntegrator2d\nfrom fealpy.vem import NonConformingVEMDoFIntegrator2d\nfrom fealpy.vem import NonConformingScalarVEMH1Projector2d\nfrom fealpy.vem import NonConformingScalarVEML2Projector2d \nfrom fealpy.vem import NonConformingScalarVEMLaplaceIntegrator2d\nfrom fealpy.vem import NonConformingVEMScalarSourceIntegrator2d\nfrom fealpy.vem import PoissonCVEMEstimator\nfrom fealpy.mesh.adaptive_tools import mark\n# 双线性型\nfrom fealpy.vem import BilinearForm\n\n# 线性型\nfrom fealpy.vem import LinearForm\n\nfrom fealpy.boundarycondition import DirichletBC \nfrom fealpy.tools.show import showmultirate\n\n## 参数解析\nparser = argparse.ArgumentParser(description=\n        \"\"\"\n        多边形网格上的任意次非协调虚单元方法  \n        \"\"\")\n\nparser.add_argument('--degree',\n        default=4, type=int,\n        help='虚单元空间的次数, 默认为 1 次.')\n\nparser.add_argument('--maxit',\n        default=400, type=int,\n        help='默认网格加密求解的次数, 默认加密求解 4 次')\n\nparser.add_argument('--theta',\n        default=0.2, type=int,\n        help='自适应参数， 默认0.2')\nargs = parser.parse_args()\n\ndegree = args.degree\nmaxit = args.maxit\ntheta = args.theta\n\n\n#pde = CosCosData()\npde = LShapeRSinData()\ndomain = pde.domain()\n\nerrorType = ['$|| u - \\Pi u_h||_{0,\\Omega}$',\n             '$||\\\\nabla u -  \\\\nabla \\Pi u_h||_{0, \\Omega}$',\n             '$\\eta$']\n\nerrorMatrix = np.zeros((3, maxit), dtype=np.float64)\nNDof = np.zeros(maxit, dtype=np.float64)\n\nmesh = pde.init_mesh(n = 1, meshtype='quad')\nmesh = PolygonMesh.from_mesh(mesh)\nHmesh = HalfEdgeMesh2d.from_mesh(mesh)\n\nfig = plt.figure()\naxes  = fig.gca()\nmesh.add_plot(axes)\nplt.show()\n\nfor i in range(maxit):\n    space = NonConformingScalarVESpace2d(mesh, p=degree)\n    uh = space.function()\n    \n    NDof[i] = space.number_of_global_dofs()\n    \n    #组装刚度矩阵 A \n    m = ScaledMonomialSpaceMassIntegrator2d()\n    M = m.assembly_cell_matrix(space.smspace)\n\n    d = NonConformingVEMDoFIntegrator2d()\n    D = d.assembly_cell_matrix(space, M)\n\n    h1 = NonConformingScalarVEMH1Projector2d(D)\n    PI1 = h1.assembly_cell_matrix(space)\n    G = h1.G\n\n    li = NonConformingScalarVEMLaplaceIntegrator2d(PI1, G, D)\n    bform = BilinearForm(space)\n    bform.add_domain_integrator(li)\n    A = bform.assembly()\n\n    #组装右端 F\n    l2 = NonConformingScalarVEML2Projector2d(M, PI1)\n    PI0 = l2.assembly_cell_matrix(space)\n\n    si = NonConformingVEMScalarSourceIntegrator2d(pde.source, PI0)\n    lform = LinearForm(space)\n    lform.add_domain_integrator(si)\n    F = lform.assembly()\n\n    #处理边界 \n    bc = DirichletBC(space, pde.dirichlet)\n    A, F = bc.apply(A, F, uh)\n\n    uh[:] = spsolve(A, F)\n    sh = space.project_to_smspace(uh, PI1)\n\n    estimator = PoissonCVEMEstimator(space, M, PI1)\n    eta = estimator.residual_estimate(uh, pde.source)\n    \n    print(i,\":\",NDof[i],\",\",np.sqrt(np.sum(eta))) \n    errorMatrix[0, i] = mesh.error(pde.solution, sh.value)\n    errorMatrix[1, i] = mesh.error(pde.gradient, sh.grad_value)\n    errorMatrix[2, i] = np.sqrt(np.sum(eta))\n    \n    isMarkedCell = mark(eta, theta, 'L2')\n    Hmesh.adaptive_refine(isMarkedCell, method='poly')\n    newcell, cellocation = Hmesh.entity('cell')\n    newnode = Hmesh.entity(\"node\")[:]\n    mesh = PolygonMesh(newnode, newcell, cellocation)\n    if NDof[i] > 1e4 :\n        iterations = i \n        break\n    ''' \n    if np.sqrt(np.sum(eta)) < 1e-3 :\n        print(\"eta\", np.sqrt(np.sum(eta)))\n        iterations = i \n        break\n    '''\n    \n    '''log 加密策略\n    options = Hmesh.adaptive_options(HB=None)\n    Hmesh.adaptive(eta, options)\n    newcell, cellocation = Hmesh.entity('cell')\n    newnode = Hmesh.entity(\"node\")[:]\n    mesh = PolygonMesh(newnode, newcell, cellocation)\n    '''\n\n#showmultirate(plt, maxit-10, NDof, errorMatrix, \n#        errorType, propsize=20, lw=2, ms=4)\n\nshowmultirate(plt, iterations-20, NDof[:iterations], errorMatrix[:,:iterations], \n        errorType, propsize=20, lw=2, ms=4)\nnp.savetxt(\"Ndof.txt\", NDof[:iterations], delimiter=',')\nnp.savetxt(\"errorMatrix.txt\", errorMatrix[:,:iterations], delimiter=',')\n\nplt.xlabel('Number of d.o.f', fontdict={'family' : 'Times New Roman', 'size'   : 16})\nplt.ylabel('Error', fontdict={'family' : 'Times New Roman', 'size'   : 16})\nplt.savefig('error.jpg')\nplt.show()\n\nfig1 = plt.figure()\naxes  = fig1.gca()\nmesh.add_plot(axes)\nplt.show()\n\n\n  \n",
  "import numpy as np\nimport scipy.sparse as sps\n\nfrom porepy.viz.exporter import Exporter\nfrom porepy.fracs import importer\n\nfrom porepy.params import tensor\nfrom porepy.params.bc import BoundaryCondition\nfrom porepy.params.data import Parameters\n\nfrom porepy.grids import coarsening as co\n\nfrom porepy.numerics.vem import vem_dual, vem_source\n\n#------------------------------------------------------------------------------#\n\n\ndef add_data(gb, domain, kf):\n    \"\"\"\n    Define the permeability, apertures, boundary conditions\n    \"\"\"\n    gb.add_node_props(['param'])\n    tol = 1e-5\n    a = 1e-4\n\n    for g, d in gb:\n        param = Parameters(g)\n\n        # Permeability\n        kxx = np.ones(g.num_cells) * np.power(kf, g.dim < gb.dim_max())\n        if g.dim == 2:\n            perm = tensor.SecondOrder(g.dim, kxx=kxx, kyy=kxx, kzz=1)\n        else:\n            perm = tensor.SecondOrder(g.dim, kxx=kxx, kyy=1, kzz=1)\n        param.set_tensor(\"flow\", perm)\n\n        # Source term\n        param.set_source(\"flow\", np.zeros(g.num_cells))\n\n        # Assign apertures\n        aperture = np.power(a, gb.dim_max() - g.dim)\n        param.set_aperture(np.ones(g.num_cells) * aperture)\n\n        # Boundaries\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            left = bound_face_centers[0, :] < domain['xmin'] + tol\n            right = bound_face_centers[0, :] > domain['xmax'] - tol\n\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[right] = 'dir'\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[left]] = -aperture \\\n                * g.face_areas[bound_faces[left]]\n            bc_val[bound_faces[right]] = 1\n\n            param.set_bc(\"flow\", BoundaryCondition(g, bound_faces, labels))\n            param.set_bc_val(\"flow\", bc_val)\n        else:\n            param.set_bc(\"flow\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n\n        d['param'] = param\n\n    # Assign coupling permeability\n    gb.add_edge_prop('kn')\n    for e, d in gb.edges_props():\n        gn = gb.sorted_nodes_of_edge(e)\n        aperture = np.power(a, gb.dim_max() - gn[0].dim)\n        d['kn'] = np.ones(gn[0].num_cells) * kf / aperture\n\n#------------------------------------------------------------------------------#\n\n\ndef write_network(file_name):\n    network = \"FID,START_X,START_Y,END_X,END_Y\\n\"\n    network += \"0,0,0.5,1,0.5\\n\"\n    network += \"1,0.5,0,0.5,1\\n\"\n    network += \"2,0.5,0.75,1,0.75\\n\"\n    network += \"3,0.75,0.5,0.75,1\\n\"\n    network += \"4,0.5,0.625,0.75,0.625\\n\"\n    network += \"5,0.625,0.5,0.625,0.75\\n\"\n    with open(file_name, \"w\") as text_file:\n        text_file.write(network)\n\n#------------------------------------------------------------------------------#\n\n\ndef main(kf, description, is_coarse=False, if_export=False):\n    mesh_kwargs = {}\n    mesh_kwargs['mesh_size'] = {'mode': 'constant',\n                                'value': 0.045, 'bound_value': 0.045}\n\n    domain = {'xmin': 0, 'xmax': 1, 'ymin': 0, 'ymax': 1}\n\n    file_name = 'network_geiger.csv'\n    write_network(file_name)\n    gb = importer.dfm_2d_from_csv(file_name, mesh_kwargs, domain)\n    gb.compute_geometry()\n    if is_coarse:\n        co.coarsen(gb, 'by_volume')\n    gb.assign_node_ordering()\n\n    # Assign parameters\n    add_data(gb, domain, kf)\n\n    # Choose and define the solvers and coupler\n    solver_flow = vem_dual.DualVEMMixedDim('flow')\n    A_flow, b_flow = solver_flow.matrix_rhs(gb)\n\n    solver_source = vem_source.IntegralMixedDim('flow')\n    A_source, b_source = solver_source.matrix_rhs(gb)\n\n    up = sps.linalg.spsolve(A_flow + A_source, b_flow + b_source)\n    solver_flow.split(gb, \"up\", up)\n\n    gb.add_node_props([\"discharge\", 'pressure', \"P0u\"])\n    solver_flow.extract_u(gb, \"up\", \"discharge\")\n    solver_flow.extract_p(gb, \"up\", 'pressure')\n    solver_flow.project_u(gb, \"discharge\", \"P0u\")\n\n    if if_export:\n        save = Exporter(gb, \"vem\", folder=\"vem_\" + description)\n        save.write_vtk(['pressure', \"P0u\"])\n\n#------------------------------------------------------------------------------#\n\n\ndef test_vem_blocking():\n    kf = 1e-4\n    main(kf, \"blocking\")\n\n#------------------------------------------------------------------------------#\n\n\ndef test_vem_permeable():\n    kf = 1e4\n    main(kf, \"permeable\")\n\n#------------------------------------------------------------------------------#\n",
  "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport distutils.util\n\nimport numpy as np\nimport fast_tokenizer\nfrom paddlenlp.transformers import AutoTokenizer\nimport fastdeploy as fd\n\n\ndef parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_dir\", required=True, help=\"The directory of model.\")\n    parser.add_argument(\n        \"--vocab_path\",\n        type=str,\n        default=\"\",\n        help=\"The path of tokenizer vocab.\")\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='cpu',\n        choices=['gpu', 'cpu', 'kunlunxin'],\n        help=\"Type of inference device, support 'cpu', 'kunlunxin' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default='onnx_runtime',\n        choices=[\n            'onnx_runtime', 'paddle', 'openvino', 'tensorrt', 'paddle_tensorrt'\n        ],\n        help=\"The inference runtime backend.\")\n    parser.add_argument(\n        \"--batch_size\", type=int, default=1, help=\"The batch size of data.\")\n    parser.add_argument(\n        \"--max_length\",\n        type=int,\n        default=128,\n        help=\"The max length of sequence.\")\n    parser.add_argument(\n        \"--log_interval\",\n        type=int,\n        default=10,\n        help=\"The interval of logging.\")\n    parser.add_argument(\n        \"--use_fp16\",\n        type=distutils.util.strtobool,\n        default=False,\n        help=\"Wheter to use FP16 mode\")\n    parser.add_argument(\n        \"--use_fast\",\n        type=distutils.util.strtobool,\n        default=False,\n        help=\"Whether to use fast_tokenizer to accelarate the tokenization.\")\n    return parser.parse_args()\n\n\ndef batchfy_text(texts, batch_size):\n    batch_texts = []\n    batch_start = 0\n    while batch_start < len(texts):\n        batch_texts += [\n            texts[batch_start:min(batch_start + batch_size, len(texts))]\n        ]\n        batch_start += batch_size\n    return batch_texts\n\n\nclass ErnieForSequenceClassificationPredictor(object):\n    def __init__(self, args):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            'ernie-3.0-medium-zh', use_faster=args.use_fast)\n        self.runtime = self.create_fd_runtime(args)\n        self.batch_size = args.batch_size\n        self.max_length = args.max_length\n\n    def create_fd_runtime(self, args):\n        option = fd.RuntimeOption()\n        model_path = os.path.join(args.model_dir, \"infer.pdmodel\")\n        params_path = os.path.join(args.model_dir, \"infer.pdiparams\")\n        option.set_model_path(model_path, params_path)\n        if args.device == 'kunlunxin':\n            option.use_kunlunxin()\n            option.use_paddle_lite_backend()\n            return fd.Runtime(option)\n        if args.device == 'cpu':\n            option.use_cpu()\n        else:\n            option.use_gpu()\n        if args.backend == 'paddle':\n            option.use_paddle_infer_backend()\n        elif args.backend == 'onnx_runtime':\n            option.use_ort_backend()\n        elif args.backend == 'openvino':\n            option.use_openvino_backend()\n        else:\n            option.use_trt_backend()\n            if args.backend == 'paddle_tensorrt':\n                option.enable_paddle_to_trt()\n                option.enable_paddle_trt_collect_shape()\n            trt_file = os.path.join(args.model_dir, \"infer.trt\")\n            option.set_trt_input_shape(\n                'input_ids',\n                min_shape=[1, args.max_length],\n                opt_shape=[args.batch_size, args.max_length],\n                max_shape=[args.batch_size, args.max_length])\n            option.set_trt_input_shape(\n                'token_type_ids',\n                min_shape=[1, args.max_length],\n                opt_shape=[args.batch_size, args.max_length],\n                max_shape=[args.batch_size, args.max_length])\n            if args.use_fp16:\n                option.enable_trt_fp16()\n                trt_file = trt_file + \".fp16\"\n            option.set_trt_cache_file(trt_file)\n        return fd.Runtime(option)\n\n    def preprocess(self, texts, texts_pair):\n        data = self.tokenizer(\n            texts,\n            texts_pair,\n            max_length=self.max_length,\n            padding=True,\n            truncation=True)\n        input_ids_name = self.runtime.get_input_info(0).name\n        token_type_ids_name = self.runtime.get_input_info(1).name\n        input_map = {\n            input_ids_name: np.array(\n                data[\"input_ids\"], dtype=\"int64\"),\n            token_type_ids_name: np.array(\n                data[\"token_type_ids\"], dtype=\"int64\")\n        }\n        return input_map\n\n    def infer(self, input_map):\n        results = self.runtime.infer(input_map)\n        return results\n\n    def postprocess(self, infer_data):\n        logits = np.array(infer_data[0])\n        max_value = np.max(logits, axis=1, keepdims=True)\n        exp_data = np.exp(logits - max_value)\n        probs = exp_data / np.sum(exp_data, axis=1, keepdims=True)\n        out_dict = {\n            \"label\": probs.argmax(axis=-1),\n            \"confidence\": probs.max(axis=-1)\n        }\n        return out_dict\n\n    def predict(self, texts, texts_pair=None):\n        input_map = self.preprocess(texts, texts_pair)\n        infer_result = self.infer(input_map)\n        output = self.postprocess(infer_result)\n        return output\n\n\nif __name__ == \"__main__\":\n    args = parse_arguments()\n    predictor = ErnieForSequenceClassificationPredictor(args)\n    texts_ds = [\"花呗收款额度限制\", \"花呗支持高铁票支付吗\"]\n    texts_pair_ds = [\"收钱码，对花呗支付的金额有限制吗\", \"为什么友付宝不支持花呗付款\"]\n    batch_texts = batchfy_text(texts_ds, args.batch_size)\n    batch_texts_pair = batchfy_text(texts_pair_ds, args.batch_size)\n\n    for bs, (texts,\n             texts_pair) in enumerate(zip(batch_texts, batch_texts_pair)):\n        outputs = predictor.predict(texts, texts_pair)\n        for i, (sentence1, sentence2) in enumerate(zip(texts, texts_pair)):\n            print(\n                f\"Batch id:{bs}, example id:{i}, sentence1:{sentence1}, sentence2:{sentence2}, label:{outputs['label'][i]}, similarity:{outputs['confidence'][i]:.4f}\"\n            )\n",
  "from seedemu.layers import Base, Routing, Ebgp, PeerRelationship\nfrom seedemu.services import WebService\nfrom seedemu.compiler import Docker\nfrom seedemu.core import Emulator, Binding, Filter\n\nsim = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nweb = WebService()\n\n###############################################################################\n\nbase.createInternetExchange(100)\n\n###############################################################################\n\nas150 = base.createAutonomousSystem(150)\n\nas150_web = as150.createHost('web')\n\nweb.install('web150')\nsim.addBinding(Binding('web150', filter = Filter(asn = 150, nodeName = 'web')))\n\nas150_router = as150.createRouter('router0')\nas150_net = as150.createNetwork('net0')\n\n\n\nas150_web.joinNetwork('net0')\nas150_router.joinNetwork('net0')\n\nas150_router.joinNetwork('ix100')\n\nas150_router.crossConnect(152, 'router0', '10.50.0.1/30')\n\n###############################################################################\n\nas151 = base.createAutonomousSystem(151)\n\nas151_web = as151.createHost('web')\n\nweb.install('web151')\nsim.addBinding(Binding('web151', filter = Filter(asn = 151, nodeName = 'web')))\n\nas151_router = as151.createRouter('router0')\n\nas151_net = as151.createNetwork('net0')\n\n\n\nas151_web.joinNetwork('net0')\nas151_router.joinNetwork('net0')\n\nas151_router.joinNetwork('ix100')\n\n###############################################################################\n\nas152 = base.createAutonomousSystem(152)\n\nas152_web = as152.createHost('web')\n\nweb.install('web152')\nsim.addBinding(Binding('web152', filter = Filter(asn = 152, nodeName = 'web')))\n\nas152_router = as152.createRouter('router0')\n\nas152_net = as152.createNetwork('net0')\n\n\n\nas152_web.joinNetwork('net0')\nas152_router.joinNetwork('net0')\n\nas152_router.crossConnect(150, 'router0', '10.50.0.2/30')\n\n###############################################################################\n\nebgp.addRsPeer(100, 150)\nebgp.addRsPeer(100, 151)\n\nebgp.addCrossConnectPeering(150, 152, PeerRelationship.Provider)\n\n###############################################################################\n\nsim.addLayer(base)\nsim.addLayer(routing)\nsim.addLayer(ebgp)\nsim.addLayer(web)\n\nsim.render()\n\n###############################################################################\n\nsim.compile(Docker(selfManagedNetwork = True), './cross-connect')",
  "'''Listmode-to-sinograms conversion demo.\n\nUsage:\n  listmode_to_sinograms [--help | options]\n\nOptions:\n  -p <path>, --path=<path>     path to data files, defaults to data/examples/PET/mMR\n                               subfolder of SIRF root folder\n  -l <list>, --list=<list>     listmode file [default: list.l.hdr]\n  -o <sino>, --sino=<sino>     output file prefix [default: sinograms]\n  -t <tmpl>, --tmpl=<tmpl>     raw data template [default: mMR_template_span11_small.hs]\n  -i <int>, --interval=<int>   scanning time interval to convert as string '(a,b)'\n                               [default: (0,10)]\n  -e <engn>, --engine=<engn>   reconstruction engine [default: STIR]\n  -s <stsc>, --storage=<stsc>  acquisition data storage scheme [default: memory]\n  --non-interactive            do not show plots\n'''\n\n## SyneRBI Synergistic Image Reconstruction Framework (SIRF)\n## Copyright 2018 - 2019 Rutherford Appleton Laboratory STFC\n## Copyright 2018 University College London.\n##\n## This is software developed for the Collaborative Computational\n## Project in Synergistic Reconstruction for Biomedical Imaging (formerly CCP PETMR)\n## (http://www.ccpsynerbi.ac.uk/).\n##\n## Licensed under the Apache License, Version 2.0 (the \"License\");\n##   you may not use this file except in compliance with the License.\n##   You may obtain a copy of the License at\n##       http://www.apache.org/licenses/LICENSE-2.0\n##   Unless required by applicable law or agreed to in writing, software\n##   distributed under the License is distributed on an \"AS IS\" BASIS,\n##   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n##   See the License for the specific language governing permissions and\n##   limitations under the License.\n\n__version__ = '1.0.0'\nfrom docopt import docopt\nargs = docopt(__doc__, version=__version__)\n\nfrom ast import literal_eval\n\nfrom sirf.Utilities import show_2D_array\n\n# import engine module\nexec('from sirf.' + args['--engine'] + ' import *')\n\n\n# process command-line options\ndata_path = args['--path']\nif data_path is None:\n    # default to data/examples/PET/mMR\n    # Note: seem to need / even on Windows\n    #data_path = os.path.join(examples_data_path('PET'), 'mMR')\n    data_path = examples_data_path('PET') + '/mMR'\nprefix = data_path + '/'\nlist_file = args['--list']\nsino_file = args['--sino']\ntmpl_file = args['--tmpl']\nlist_file = existing_filepath(data_path, list_file)\ntmpl_file = existing_filepath(data_path, tmpl_file)\ninterval = literal_eval(args['--interval'])\nstorage = args['--storage']\nshow_plot = not args['--non-interactive']\n\n\ndef main():\n\n    # select acquisition data storage scheme\n    AcquisitionData.set_storage_scheme(storage)\n\n    # read acquisition data template\n    acq_data_template = AcquisitionData(tmpl_file)\n\n    # create listmode-to-sinograms converter object\n    lm2sino = ListmodeToSinograms()\n\n    # set input, output and template files\n    lm2sino.set_input(list_file)\n    lm2sino.set_output_prefix(sino_file)\n    # the template is used to specify the sizes of the output sinogram.\n    # see the acquisition_data_from_scanner_info demo for an example how to \n    # make your own template file\n    lm2sino.set_template(acq_data_template)\n    # old way (now just an alternative option)\n    # lm2sino.set_template(tmpl_file)\n\n    # set interval\n    lm2sino.set_time_interval(interval[0], interval[1])\n\n    # set some flags as examples (the following values are the defaults)\n    lm2sino.flag_on('store_prompts')\n    lm2sino.flag_off('interactive')\n\n    # set up the converter\n    lm2sino.set_up()\n\n    # convert\n    lm2sino.process()\n\n    # get access to the sinograms\n    acq_data = lm2sino.get_output()\n    # copy the acquisition data into a Python array\n    acq_array = acq_data.as_array()\n    acq_dim = acq_array.shape\n    print('acquisition data dimensions: %dx%dx%dx%d' % acq_dim)\n    z = acq_dim[1]//2\n    if show_plot:\n        show_2D_array('Acquisition data', acq_array[0,z,:,:])\n\n    # compute randoms\n    print('estimating randoms, please wait...')\n    randoms = lm2sino.estimate_randoms()\n    rnd_array = randoms.as_array()\n    if show_plot:\n        show_2D_array('Randoms', rnd_array[0,z,:,:])\n\n\ntry:\n    main()\n    print('\\n=== done with %s' % __file__)\n\nexcept error as err:\n    print('%s' % err.value)\n",
  "# Copyright 2018 DeepMind Technologies Limited. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"An example CQL running on locomotion datasets (mujoco) from D4rl.\"\"\"\n\nfrom absl import app\nfrom absl import flags\nimport acme\nfrom acme import specs\nfrom acme.agents.jax import actor_core as actor_core_lib\nfrom acme.agents.jax import actors\nfrom acme.agents.jax import cql\nfrom acme.datasets import tfds\nfrom acme.examples.offline import helpers as gym_helpers\nfrom acme.jax import variable_utils\nfrom acme.utils import loggers\nimport haiku as hk\nimport jax\nimport optax\n\n# Agent flags\nflags.DEFINE_integer('batch_size', 64, 'Batch size.')\nflags.DEFINE_integer('evaluate_every', 20, 'Evaluation period.')\nflags.DEFINE_integer('evaluation_episodes', 10, 'Evaluation episodes.')\nflags.DEFINE_integer(\n    'num_demonstrations', 10,\n    'Number of demonstration episodes to load from the dataset. If None, loads the full dataset.'\n)\nflags.DEFINE_integer('seed', 0, 'Random seed for learner and evaluator.')\n# CQL specific flags.\nflags.DEFINE_float('policy_learning_rate', 3e-5, 'Policy learning rate.')\nflags.DEFINE_float('critic_learning_rate', 3e-4, 'Critic learning rate.')\nflags.DEFINE_float('fixed_cql_coefficient', None,\n                   'Fixed CQL coefficient. If None, an adaptive one is used.')\nflags.DEFINE_float('cql_lagrange_threshold', 10.,\n                   'Lagrange threshold for the adaptive CQL coefficient.')\n# Environment flags.\nflags.DEFINE_string('env_name', 'HalfCheetah-v2',\n                    'Gym mujoco environment name.')\nflags.DEFINE_string(\n    'dataset_name', 'd4rl_mujoco_halfcheetah/v2-medium',\n    'D4rl dataset name. Can be any locomotion dataset from '\n    'https://www.tensorflow.org/datasets/catalog/overview#d4rl.')\n\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n  key = jax.random.PRNGKey(FLAGS.seed)\n  key_demonstrations, key_learner = jax.random.split(key, 2)\n\n  # Create an environment and grab the spec.\n  environment = gym_helpers.make_environment(task=FLAGS.env_name)\n  environment_spec = specs.make_environment_spec(environment)\n\n  # Get a demonstrations dataset.\n  transitions_iterator = tfds.get_tfds_dataset(FLAGS.dataset_name,\n                                               FLAGS.num_demonstrations)\n  demonstrations = tfds.JaxInMemoryRandomSampleIterator(\n      transitions_iterator, key=key_demonstrations, batch_size=FLAGS.batch_size)\n\n  # Create the networks to optimize.\n  networks = cql.make_networks(environment_spec)\n\n  # Create the learner.\n  learner = cql.CQLLearner(\n      batch_size=FLAGS.batch_size,\n      networks=networks,\n      random_key=key_learner,\n      policy_optimizer=optax.adam(FLAGS.policy_learning_rate),\n      critic_optimizer=optax.adam(FLAGS.critic_learning_rate),\n      fixed_cql_coefficient=FLAGS.fixed_cql_coefficient,\n      cql_lagrange_threshold=FLAGS.cql_lagrange_threshold,\n      demonstrations=demonstrations,\n      num_sgd_steps_per_step=1)\n\n  def evaluator_network(\n      params: hk.Params, key: jax.Array, observation: jax.Array\n  ) -> jax.Array:\n    dist_params = networks.policy_network.apply(params, observation)\n    return networks.sample_eval(dist_params, key)\n\n  actor_core = actor_core_lib.batched_feed_forward_to_actor_core(\n      evaluator_network)\n  variable_client = variable_utils.VariableClient(\n      learner, 'policy', device='cpu')\n  evaluator = actors.GenericActor(\n      actor_core, key, variable_client, backend='cpu')\n\n  eval_loop = acme.EnvironmentLoop(\n      environment=environment,\n      actor=evaluator,\n      logger=loggers.TerminalLogger('evaluation', time_delta=0.))\n\n  # Run the environment loop.\n  while True:\n    for _ in range(FLAGS.evaluate_every):\n      learner.step()\n    eval_loop.run(FLAGS.evaluation_episodes)\n\n\nif __name__ == '__main__':\n  app.run(main)\n",
  "from paz import processors as pr\nfrom paz.abstract import SequentialProcessor, Processor\nfrom processors import MatchBoxes\nimport numpy as np\nimport os\n\n\nclass PreprocessBoxes(SequentialProcessor):\n    \"\"\"Preprocess bounding boxes\n\n    # Arguments\n        num_classes: Int.\n        prior_boxes: Numpy array of shape ``[num_boxes, 4]`` containing\n            prior/default bounding boxes.\n        IOU: Float. Intersection over union used to match boxes.\n        variances: List of two floats indicating variances to be encoded\n            for encoding bounding boxes.\n    \"\"\"\n    def __init__(self, num_classes, prior_boxes, IOU, variances):\n        super(PreprocessBoxes, self).__init__()\n        self.add(MatchBoxes(prior_boxes, IOU),)\n        self.add(pr.EncodeBoxes(prior_boxes, variances))\n        self.add(pr.BoxClassToOneHotVector(num_classes))\n\n\nclass PreprocessImage(SequentialProcessor):\n    \"\"\"Preprocess RGB image by resizing it to the given ``shape``. If a\n    ``mean`` is given it is substracted from image and it not the image gets\n    normalized.\n\n    # Arguments\n        shape: List of two Ints.\n        mean: List of three Ints indicating the per-channel mean to be\n            subtracted.\n    \"\"\"\n    def __init__(self, shape, mean=pr.BGR_IMAGENET_MEAN):\n        super(PreprocessImage, self).__init__()\n        self.add(pr.ResizeImage(shape))\n        self.add(pr.CastImage(float))\n        if mean is None:\n            self.add(pr.NormalizeImage())\n        else:\n            self.add(pr.SubtractMeanImage(mean))\n\n\nclass AugmentImage(SequentialProcessor):\n    \"\"\"Preprocess RGB image by resizing it to the given ``shape``. If a\n    ``mean`` is given it is substracted from image and it not the image gets\n    normalized.\n\n    # Arguments\n        shape: List of two Ints.\n        mean: List of three Ints indicating the per-channel mean to be\n            subtracted.\n    \"\"\"\n    def __init__(self, shape, bkg_paths, mean=pr.BGR_IMAGENET_MEAN):\n        super(AugmentImage, self).__init__()\n        # self.add(LoadImage(4))\n        self.add(pr.ResizeImage(shape))\n        self.add(pr.BlendRandomCroppedBackground(bkg_paths))\n        self.add(pr.RandomContrast())\n        self.add(pr.RandomBrightness())\n        self.add(pr.RandomSaturation(0.7))\n        self.add(pr.RandomHue())\n        self.add(pr.ConvertColorSpace(pr.RGB2BGR))\n\n\nclass AugmentBoxes(SequentialProcessor):\n    \"\"\"Perform data augmentation with bounding boxes.\n\n    # Arguments\n        mean: List of three elements used to fill empty image spaces.\n    \"\"\"\n    def __init__(self, mean=pr.BGR_IMAGENET_MEAN):\n        super(AugmentBoxes, self).__init__()\n        self.add(pr.ToImageBoxCoordinates())\n        self.add(pr.Expand(mean=mean))\n        self.add(pr.RandomSampleCrop())\n        self.add(pr.RandomFlipBoxesLeftRight())\n        self.add(pr.ToNormalizedBoxCoordinates())\n\n\nclass DrawBoxData2D(Processor):\n    def __init__(self, class_names, preprocess=None, colors=None):\n        super(DrawBoxData2D, self).__init__()\n        self.class_names, self.colors = class_names, colors\n        self.to_boxes2D = pr.ToBoxes2D(self.class_names)\n        self.draw_boxes2D = pr.DrawBoxes2D(self.class_names, self.colors)\n        self.preprocess = preprocess\n\n    def call(self, image, boxes):\n        if self.preprocess is not None:\n            image, boxes = self.preprocess(image, boxes)\n            boxes = boxes.astype('int')\n        boxes = self.to_boxes2D(boxes)\n        print(boxes)\n        image = self.draw_boxes2D(image, boxes)\n        return image, boxes\n\n\nclass ShowBoxes(Processor):\n    def __init__(self, class_names, prior_boxes,\n                 variances=[0.1, 0.1, 0.2, 0.2]):\n        super(ShowBoxes, self).__init__()\n        self.deprocess_boxes = SequentialProcessor([\n            pr.DecodeBoxes(prior_boxes, variances),\n            pr.ToBoxes2D(class_names, True),\n            pr.FilterClassBoxes2D(class_names[1:])])\n        self.denormalize_boxes2D = pr.DenormalizeBoxes2D()\n        self.draw_boxes2D = pr.DrawBoxes2D(class_names)\n        self.show_image = pr.ShowImage()\n        self.resize_image = pr.ResizeImage((600, 600))\n\n    def call(self, image, boxes):\n        image = self.resize_image(image)\n        boxes2D = self.deprocess_boxes(boxes)\n        boxes2D = self.denormalize_boxes2D(image, boxes2D)\n        image = self.draw_boxes2D(image, boxes2D)\n        image = (image + pr.BGR_IMAGENET_MEAN).astype(np.uint8)\n        image = image[..., ::-1]\n        self.show_image(image)\n        return image, boxes2D\n\n\nclass AugmentDetection(SequentialProcessor):\n    \"\"\"Augment boxes and images for object detection.\n\n    # Arguments\n        prior_boxes: Numpy array of shape ``[num_boxes, 4]`` containing\n            prior/default bounding boxes.\n        split: Flag from `paz.processors.TRAIN`, ``paz.processors.VAL``\n            or ``paz.processors.TEST``. Certain transformations would take\n            place depending on the flag.\n        num_classes: Int.\n        size: Int. Image size.\n        mean: List of three elements indicating the per channel mean.\n        IOU: Float. Intersection over union used to match boxes.\n        variances: List of two floats indicating variances to be encoded\n            for encoding bounding boxes.\n    \"\"\"\n    def __init__(self, prior_boxes, bkg_paths, split=pr.TRAIN, num_classes=2,\n                 size=300, mean=pr.BGR_IMAGENET_MEAN, IOU=.5,\n                 variances=[0.1, 0.1, 0.2, 0.2]):\n        super(AugmentDetection, self).__init__()\n        # image processors\n        self.augment_image = AugmentImage((size, size), bkg_paths, mean)\n        self.preprocess_image = PreprocessImage((size, size), mean)\n\n        # box processors\n        self.augment_boxes = AugmentBoxes()\n        args = (num_classes, prior_boxes, IOU, variances)\n        self.preprocess_boxes = PreprocessBoxes(*args)\n\n        # pipeline\n        self.add(pr.UnpackDictionary(['image', 'boxes']))\n        self.add(pr.ControlMap(pr.LoadImage(4), [0], [0]))\n        if split == pr.TRAIN:\n            self.add(pr.ControlMap(self.augment_image, [0], [0]))\n            self.add(pr.ControlMap(self.augment_boxes, [0, 1], [0, 1]))\n        self.add(pr.ControlMap(self.preprocess_image, [0], [0]))\n        self.add(pr.ControlMap(self.preprocess_boxes, [1], [1]))\n        self.add(pr.SequenceWrapper(\n            {0: {'image': [size, size, 3]}},\n            {1: {'boxes': [len(prior_boxes), 4 + num_classes]}}))\n\n\ndraw_boxes2D = DrawBoxData2D(['background', 'solar_panel'],\n                             pr.ToImageBoxCoordinates())\nif __name__ == \"__main__\":\n    import tensorflow as tf\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(gpus[0], True)\n\n    from data_manager import CSVLoader\n    from paz.models import SSD300\n    import glob\n    model = SSD300()\n    prior_boxes = model.prior_boxes\n\n    path = 'datasets/solar_panel/BoundingBox.txt'\n    class_names = ['background', 'solar_panel']\n    data_manager = CSVLoader(path, class_names)\n    dataset = data_manager.load_data()\n    home = os.path.expanduser('~')\n    bkg_path = os.path.join(home, '.keras/paz/datasets/voc-backgrounds/')\n    wild_card = bkg_path + '*.png'\n    bkg_paths = glob.glob(wild_card)\n    process = AugmentDetection(prior_boxes, bkg_paths)\n    show_boxes2D = ShowBoxes(class_names, prior_boxes)\n    for sample_arg in range(len(dataset)):\n        sample = dataset[sample_arg]\n        wrapped_outputs = process(sample)\n        image = wrapped_outputs['inputs']['image']\n        boxes = wrapped_outputs['labels']['boxes']\n        image, boxes = show_boxes2D(image, boxes)\n",
  "#\n# Example showing how to create a custom lithium-ion model from submodels\n#\n\nimport pybamm\nimport numpy as np\n\npybamm.set_logging_level(\"INFO\")\n\n# load lithium-ion base model\nmodel = pybamm.lithium_ion.BaseModel(name=\"my li-ion model\")\n\n# set choice of submodels\nmodel.submodels[\"external circuit\"] = pybamm.external_circuit.ExplicitCurrentControl(\n    model.param, model.options\n)\nmodel.submodels[\"current collector\"] = pybamm.current_collector.Uniform(model.param)\nmodel.submodels[\"thermal\"] = pybamm.thermal.isothermal.Isothermal(model.param)\nmodel.submodels[\"porosity\"] = pybamm.porosity.Constant(model.param, model.options)\nmodel.submodels[\n    \"electrolyte diffusion\"\n] = pybamm.electrolyte_diffusion.ConstantConcentration(model.param)\nmodel.submodels[\n    \"electrolyte conductivity\"\n] = pybamm.electrolyte_conductivity.LeadingOrder(model.param)\n\nmodel.submodels[\"sei\"] = pybamm.sei.NoSEI(model.param, model.options)\nmodel.submodels[\"sei on cracks\"] = pybamm.sei.NoSEI(\n    model.param, model.options, cracks=True\n)\nmodel.submodels[\"lithium plating\"] = pybamm.lithium_plating.NoPlating(model.param)\n\n# Loop over negative and positive electrode domains for some submodels\nfor domain in [\"negative\", \"positive\"]:\n    model.submodels[f\"{domain} active material\"] = pybamm.active_material.Constant(\n        model.param, domain, model.options\n    )\n    model.submodels[\n        f\"{domain} electrode potential\"\n    ] = pybamm.electrode.ohm.LeadingOrder(model.param, domain)\n    model.submodels[f\"{domain} particle\"] = pybamm.particle.XAveragedPolynomialProfile(\n        model.param,\n        domain,\n        options={**model.options, \"particle\": \"uniform profile\"},\n        phase=\"primary\",\n    )\n    model.submodels[\n        f\"{domain} total particle concentration\"\n    ] = pybamm.particle.TotalConcentration(\n        model.param, domain, model.options, phase=\"primary\"\n    )\n\n    model.submodels[\n        f\"{domain} open-circuit potential\"\n    ] = pybamm.open_circuit_potential.SingleOpenCircuitPotential(\n        model.param,\n        domain,\n        \"lithium-ion main\",\n        options=model.options,\n        phase=\"primary\",\n    )\n    model.submodels[f\"{domain} interface\"] = pybamm.kinetics.InverseButlerVolmer(\n        model.param, domain, \"lithium-ion main\", options=model.options\n    )\n    model.submodels[\n        f\"{domain} interface utilisation\"\n    ] = pybamm.interface_utilisation.Full(model.param, domain, model.options)\n    model.submodels[\n        f\"{domain} interface current\"\n    ] = pybamm.kinetics.CurrentForInverseButlerVolmer(\n        model.param, domain, \"lithium-ion main\"\n    )\n    model.submodels[\n        f\"{domain} surface potential difference [V]\"\n    ] = pybamm.electrolyte_conductivity.surface_potential_form.Explicit(\n        model.param, domain, model.options\n    )\n    model.submodels[\n        f\"{domain} particle mechanics\"\n    ] = pybamm.particle_mechanics.NoMechanics(model.param, domain, model.options)\n\n# build model\nmodel.build_model()\n\n# create geometry\ngeometry = pybamm.battery_geometry()\n\n# process model and geometry\nparam = model.default_parameter_values\nparam.process_model(model)\nparam.process_geometry(geometry)\n\n# set mesh\n# Note: li-ion base model has defaults for mesh and var_pts\nmesh = pybamm.Mesh(geometry, model.default_submesh_types, model.default_var_pts)\n\n# discretise model\n# Note: li-ion base model has default spatial methods\ndisc = pybamm.Discretisation(mesh, model.default_spatial_methods)\ndisc.process_model(model)\n\n# solve model\nt_eval = np.linspace(0, 3600, 100)\nsolver = pybamm.ScipySolver()\nsolution = solver.solve(model, t_eval)\n\n# plot\nplot = pybamm.QuickPlot(solution)\nplot.dynamic_plot()\n",
  "#!/usr/bin/env python3\n# encoding: utf-8\n\nfrom seedemu import *\n\nCustomGenesisFileContent = '''\\\n{\n        \"nonce\":\"0x0\",\n        \"timestamp\":\"0x621549f1\",\n        \"parentHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\",\n        \"extraData\":\"0x\",\n        \"gasLimit\":\"0x80000000\",\n        \"difficulty\":\"0x0\",\n        \"mixhash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\",\n        \"coinbase\":\"0x0000000000000000000000000000000000000000\",\n        \"number\": \"0x0\",\n        \"gasUsed\": \"0x0\",\n        \"baseFeePerGas\": null,\n        \"config\": {\n            \"chainId\": 11,\n            \"homesteadBlock\": 0,\n            \"eip150Block\": 0,\n            \"eip150Hash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n            \"eip155Block\": 0,\n            \"eip158Block\": 0,\n            \"byzantiumBlock\": 0,\n            \"constantinopleBlock\": 0,\n            \"petersburgBlock\": 0,\n            \"istanbulBlock\": 0,\n            \"ethash\": {\n            }\n        },\n        \"alloc\": {\n        }\n}\n'''\n\nemu = Makers.makeEmulatorBaseWith10StubASAndHosts(1)\n\n# Create the Ethereum layer\n# saveState=True: will set the blockchain folder using `volumes`, \n# so the blockchain data will be preserved when containers are deleted.\neth = EthereumService(saveState = True, override=True)\n\n# Create the 2 Blockchain layers, which is a sub-layer of Ethereum layer\n# Need to specify chainName and consensus when create Blockchain layer.\n\n# blockchain1 is a POW based blockchain \nblockchain1 = eth.createBlockchain(chainName=\"POW\", consensus=ConsensusMechanism.POW)\n\n# blockchain2 is a POA based blockchain \nblockchain2 = eth.createBlockchain(chainName=\"POA\", consensus=ConsensusMechanism.POA)\n\n# Create blockchain1 nodes (POW Etheruem) (nodes in this layer are virtual)\ne1 = blockchain1.createNode(\"pow-eth1\")\ne2 = blockchain1.createNode(\"pow-eth2\")\ne3 = blockchain1.createNode(\"pow-eth3\")\ne4 = blockchain1.createNode(\"pow-eth4\")\n\n# Create blockchain2 nodes (POA Ethereum)\ne5 = blockchain2.createNode(\"poa-eth5\")\ne6 = blockchain2.createNode(\"poa-eth6\")\ne7 = blockchain2.createNode(\"poa-eth7\")\ne8 = blockchain2.createNode(\"poa-eth8\")\n\n# Set bootnodes on e1 and e5. The other nodes can use these bootnodes to find peers.\n# Start mining on e1,e2 and e5,e6\n# To start mine(seal) in POA consensus, the account should be unlocked first. \ne1.setBootNode(True).setBootNodeHttpPort(8090).startMiner()\ne2.startMiner()\ne5.setBootNode(True).unlockAccounts().startMiner()\ne6.unlockAccounts().startMiner()\n\n# Create more accounts with Balance on e3 and e7\n# Create one account with createAccount() method\n# Create multiple accounts with createAccounts() method\ne3.createAccount(balance=20, unit=EthUnit.ETHER, password=\"admin\").unlockAccounts()\ne7.createAccounts(total=3, balance=30, unit=EthUnit.ETHER, password=\"admin\")\n\n# Import account with balance 0 on e2\ne2.importAccount(keyfilePath='./resources/keyfile_to_import', password=\"admin\", balance=10)\n\n# Enable http connection on e3 \n# Set geth http port to 8540 (Default : 8545)\ne3.enableGethHttp().setGethHttpPort(8540)\n\n# # Set custom genesis on e4 geth\n# e4.setGenesis(CustomGenesisFileContent)\n\n# Set custom geth command option on e4\n# Possible to enable geth http using setCustomGethCommandOption() method \n# instead of using enableGethHttp() method\ne4.setCustomGethCommandOption(\"--http --http.addr 0.0.0.0\")\n\n# Enable ws connection on e5 geth\n# Set geth ws port to 8541 (Default : 8546)\ne5.enableGethWs().setGethWsPort(8541)\ne5.enableGethHttp()\n\n# Set nodiscover option on e8 geth\ne8.setNoDiscover()\n\n# Set custom geth binary file instead of installing an original file.\ne3.setCustomGeth(\"./resources/custom_geth\")\n\n# Customizing the display names (for visualization purpose)\nemu.getVirtualNode('pow-eth1').setDisplayName('Ethereum-POW-1')\nemu.getVirtualNode('pow-eth2').setDisplayName('Ethereum-POW-2')\nemu.getVirtualNode('pow-eth3').setDisplayName('Ethereum-POW-3').addPortForwarding(8545, 8540)\nemu.getVirtualNode('pow-eth4').setDisplayName('Ethereum-POW-4')\n\nemu.getVirtualNode('poa-eth5').setDisplayName('Ethereum-POA-5')\nemu.getVirtualNode('poa-eth6').setDisplayName('Ethereum-POA-6')\nemu.getVirtualNode('poa-eth7').setDisplayName('Ethereum-POA-7')\nemu.getVirtualNode('poa-eth8').setDisplayName('Ethereum-POA-8')\n\n# Binding virtual nodes to physical nodes\nemu.addBinding(Binding('pow-eth1', filter = Filter(asn = 150, nodeName='host_0')))\nemu.addBinding(Binding('pow-eth2', filter = Filter(asn = 151, nodeName='host_0')))\nemu.addBinding(Binding('pow-eth3', filter = Filter(asn = 152, nodeName='host_0')))\nemu.addBinding(Binding('pow-eth4', filter = Filter(asn = 153, nodeName='host_0')))\n\nemu.addBinding(Binding('poa-eth5', filter = Filter(asn = 160, nodeName='host_0')))\nemu.addBinding(Binding('poa-eth6', filter = Filter(asn = 161, nodeName='host_0')))\nemu.addBinding(Binding('poa-eth7', filter = Filter(asn = 162, nodeName='host_0')))\nemu.addBinding(Binding('poa-eth8', filter = Filter(asn = 163, nodeName='host_0')))\n\n# Add the layer and save the component to a file\nemu.addLayer(eth)\nemu.dump('component-blockchain.bin')\n\nemu.render()\n\ndocker = Docker(etherViewEnabled=True)\n\n# If output directory exists and override is set to false, we call exit(1)\n# updateOutputdirectory will not be called\nemu.compile(docker, './output')\n",
  "#\n#  Copyright 2019 The FATE Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nimport argparse\nfrom pipeline.utils.tools import load_job_config\nfrom pipeline.backend.pipeline import PipeLine\nfrom pipeline.component import DataTransform\nfrom pipeline.component import Evaluation\nfrom pipeline.component import HomoOneHotEncoder\nfrom pipeline.component.homo_feature_binning import HomoFeatureBinning\nfrom pipeline.component import FederatedSample\nfrom pipeline.component import HomoLR\nfrom pipeline.component import HomoSecureBoost\nfrom pipeline.component import LocalBaseline\nfrom pipeline.component import Reader\nfrom pipeline.interface import Data\nfrom pipeline.interface import Model\n\n\ndef main(config=\"../../config.yaml\", namespace=\"\"):\n    # obtain config\n    if isinstance(config, str):\n        config = load_job_config(config)\n    parties = config.parties\n    guest = parties.guest[0]\n    host = parties.host[0]\n    arbiter = parties.arbiter[0]\n\n    guest_train_data = {\"name\": \"breast_homo_guest\", \"namespace\": f\"experiment{namespace}\"}\n    host_train_data = {\"name\": \"breast_homo_host\", \"namespace\": f\"experiment{namespace}\"}\n\n    pipeline = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest, host=host, arbiter=arbiter)\n\n    reader_0 = Reader(name=\"reader_0\")\n    reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    reader_1 = Reader(name=\"reader_1\")\n    reader_1.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    reader_1.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    data_transform_0 = DataTransform(name=\"data_transform_0\", with_label=True)\n    data_transform_1 = DataTransform(name=\"data_transform_1\")\n\n    federated_sample_0 = FederatedSample(name=\"federated_sample_0\", mode=\"stratified\", method=\"downsample\",\n                                         fractions=[[0, 1.0], [1, 1.0]], task_type=\"homo\")\n\n    homo_binning_0 = HomoFeatureBinning(name='homo_binning_0', sample_bins=10, method=\"recursive_query\")\n    homo_binning_1 = HomoFeatureBinning(name='homo_binning_1')\n\n    homo_onehot_0 = HomoOneHotEncoder(name='homo_onehot_0', need_alignment=True)\n    homo_onehot_1 = HomoOneHotEncoder(name='homo_onehot_1')\n\n    homo_lr_0 = HomoLR(name=\"homo_lr_0\", penalty=\"L2\", tol=0.0001, alpha=1.0,\n                       optimizer=\"rmsprop\", max_iter=5)\n    homo_lr_1 = HomoLR(name=\"homo_lr_1\")\n\n    local_baseline_0 = LocalBaseline(name=\"local_baseline_0\", model_name=\"LogisticRegression\",\n                                     model_opts={\"penalty\": \"l2\", \"tol\": 0.0001, \"C\": 1.0, \"fit_intercept\": True,\n                                                 \"solver\": \"lbfgs\", \"max_iter\": 5, \"multi_class\": \"ovr\"})\n    local_baseline_0.get_party_instance(role='guest', party_id=guest).component_param(need_run=True)\n    local_baseline_0.get_party_instance(role='host', party_id=host).component_param(need_run=True)\n    local_baseline_1 = LocalBaseline(name=\"local_baseline_1\")\n\n    homo_secureboost_0 = HomoSecureBoost(name=\"homo_secureboost_0\", num_trees=3)\n    homo_secureboost_1 = HomoSecureBoost(name=\"homo_secureboost_1\", num_trees=3)\n\n    evaluation_0 = Evaluation(name=\"evaluation_0\")\n    evaluation_1 = Evaluation(name=\"evaluation_1\")\n\n    pipeline.add_component(reader_0)\n    pipeline.add_component(reader_1)\n\n    pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))\n    pipeline.add_component(data_transform_1, data=Data(data=reader_1.output.data),\n                           model=Model(model=data_transform_0.output.model))\n\n    pipeline.add_component(federated_sample_0, data=Data(data=data_transform_0.output.data))\n\n    pipeline.add_component(homo_binning_0, data=Data(data=federated_sample_0.output.data))\n    pipeline.add_component(homo_binning_1, data=Data(data=data_transform_1.output.data),\n                           model=Model(model=homo_binning_0.output.model))\n\n    pipeline.add_component(homo_onehot_0, data=Data(data=homo_binning_0.output.data))\n    pipeline.add_component(homo_onehot_1, data=Data(data=homo_binning_1.output.data),\n                           model=Model(model=homo_onehot_0.output.model))\n\n    pipeline.add_component(homo_lr_0, data=Data(data=homo_onehot_0.output.data))\n    pipeline.add_component(homo_lr_1, data=Data(data=homo_onehot_1.output.data),\n                           model=Model(model=homo_lr_0.output.model))\n\n    pipeline.add_component(local_baseline_0, data=Data(data=homo_onehot_0.output.data))\n    pipeline.add_component(local_baseline_1, data=Data(data=homo_onehot_1.output.data),\n                           model=Model(model=local_baseline_0.output.model))\n\n    pipeline.add_component(homo_secureboost_0, data=Data(data=homo_onehot_0.output.data))\n    pipeline.add_component(homo_secureboost_1, data=Data(data=homo_onehot_1.output.data),\n                           model=Model(model=homo_secureboost_0.output.model))\n\n    pipeline.add_component(evaluation_0,\n                           data=Data(\n                               data=[homo_lr_0.output.data, homo_lr_1.output.data,\n                                     local_baseline_0.output.data, local_baseline_1.output.data]))\n    pipeline.add_component(evaluation_1,\n                           data=Data(\n                               data=[homo_secureboost_0.output.data, homo_secureboost_1.output.data]))\n\n    pipeline.compile()\n\n    pipeline.fit()\n\n    print(pipeline.get_component(\"evaluation_0\").get_summary())\n    print(pipeline.get_component(\"evaluation_1\").get_summary())\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"PIPELINE DEMO\")\n    parser.add_argument(\"-config\", type=str,\n                        help=\"config file\")\n    args = parser.parse_args()\n    if args.config is not None:\n        main(args.config)\n    else:\n        main()\n",
  "# ==========================================================================\n#  AIDA Detector description implementation\n# --------------------------------------------------------------------------\n# Copyright (C) Organisation europeenne pour la Recherche nucleaire (CERN)\n# All rights reserved.\n#\n# For the licensing terms see $DD4hepINSTALL/LICENSE.\n# For the list of contributors see $DD4hepINSTALL/doc/CREDITS.\n#\n# ==========================================================================\n#\nfrom __future__ import absolute_import, unicode_literals\nimport os\nimport sys\nimport DDG4\nfrom DDG4 import OutputLevel as Output\nfrom g4units import keV\n#\n#\n\"\"\"\n\n   dd4hep simulation example setup using the python configuration\n\n   @author  M.Frank\n   @version 1.0\n\n\"\"\"\n\n\ndef run():\n  kernel = DDG4.Kernel()\n  install_dir = os.environ['DD4hepExamplesINSTALL']\n  kernel.loadGeometry(str(\"file:\" + install_dir + \"/examples/OpticalSurfaces/compact/ReadMaterialProperties.xml\"))\n\n  DDG4.importConstants(kernel.detectorDescription(), debug=False)\n  geant4 = DDG4.Geant4(kernel, tracker='Geant4TrackerCombineAction')\n  geant4.printDetectors()\n  # Configure UI\n  if len(sys.argv) > 1:\n    geant4.setupCshUI(macro=sys.argv[1])\n  else:\n    geant4.setupCshUI()\n\n  # Configure field\n  geant4.setupTrackingField(prt=True)\n  # Configure Event actions\n  prt = DDG4.EventAction(kernel, 'Geant4ParticlePrint/ParticlePrint')\n  prt.OutputLevel = Output.DEBUG\n  prt.OutputType = 3  # Print both: table and tree\n  kernel.eventAction().adopt(prt)\n\n  generator_output_level = Output.INFO\n\n  # Configure G4 geometry setup\n  seq, act = geant4.addDetectorConstruction(\"Geant4DetectorGeometryConstruction/ConstructGeo\")\n  act.DebugMaterials = True\n  act.DebugVolumes = True\n  act.DebugShapes = True\n\n  # Setup particle gun\n  gun = geant4.setupGun(\"Gun\", particle='gamma', energy=5 * keV, multiplicity=1)\n  gun.OutputLevel = generator_output_level\n\n  geant4.setupTracker('MaterialTester')\n\n  # Now build the physics list:\n  phys = geant4.setupPhysics('QGSP_BERT')\n  phys.dump()\n\n  geant4.execute()\n\n\nif __name__ == \"__main__\":\n  run()\n",
  "\"\"\"\nExample adapting Avalanche to use Huggingface models and datasets.\nTo run this example you need huggingface datasets and transformers libraries.\n\nYou can install them by running:\npip install datasets transformers\n\"\"\"\n\nfrom avalanche.benchmarks.utils import DataAttribute, ConstantSequence\nfrom avalanche.training.plugins import ReplayPlugin\n\nfrom dataclasses import dataclass\n\nfrom transformers import PreTrainedTokenizerBase\nfrom typing import Optional, Union, Any\n\nfrom transformers.utils import PaddingStrategy\nimport torch\n\nimport avalanche\nimport torch.nn\n\nfrom avalanche.benchmarks import CLScenario, CLStream, CLExperience\nfrom avalanche.evaluation.metrics import accuracy_metrics\nimport avalanche.training.templates.base\nfrom avalanche.benchmarks.utils import AvalancheDataset\nfrom transformers import AutoTokenizer\nfrom transformers import T5ForConditionalGeneration\nfrom datasets import load_dataset\nimport numpy as np\n\n\n@dataclass\nclass CustomDataCollatorSeq2SeqBeta:\n    \"\"\"The collator is a standard huggingface collate.\n    No need to change anything here.\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n    model: Optional[Any] = None\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    label_pad_token_id: int = -100\n    return_tensors: str = \"pt\"\n\n    def __call__(self, features, return_tensors=None):\n        if return_tensors is None:\n            return_tensors = self.return_tensors\n        labels = (\n            [feature[\"labels\"] for feature in features]\n            if \"labels\" in features[0].keys()\n            else None\n        )\n        # We have to pad the labels before calling `tokenizer.pad` as this\n        # method won't pad them and needs them of the\n        # same length to return tensors.\n        if labels is not None:\n            max_label_length = max(len(lab) for lab in labels)\n            if self.pad_to_multiple_of is not None:\n                max_label_length = (\n                    (max_label_length + self.pad_to_multiple_of - 1)\n                    // self.pad_to_multiple_of\n                    * self.pad_to_multiple_of\n                )\n\n            padding_side = self.tokenizer.padding_side\n            for feature in features:\n                remainder = [self.label_pad_token_id] * (\n                    max_label_length - len(feature[\"labels\"])\n                )\n                if isinstance(feature[\"labels\"], list):\n                    feature[\"labels\"] = (\n                        feature[\"labels\"] + remainder\n                        if padding_side == \"right\"\n                        else remainder + feature[\"labels\"]\n                    )\n                elif padding_side == \"right\":\n                    feature[\"labels\"] = np.concatenate(\n                        [feature[\"labels\"], remainder]\n                    ).astype(np.int64)\n                else:\n                    feature[\"labels\"] = np.concatenate(\n                        [remainder, feature[\"labels\"]]\n                    ).astype(np.int64)\n\n        features = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=return_tensors,\n        )\n\n        # prepare decoder_input_ids\n        if (\n            labels is not None\n            and self.model is not None\n            and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n        ):\n            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(\n                labels=features[\"labels\"]\n            )\n            features[\"decoder_input_ids\"] = decoder_input_ids\n\n        return features\n\n\nclass HGNaive(avalanche.training.Naive):\n    \"\"\"There are only a couple of modifications needed to\n    use huggingface:\n    - we add a bunch of attributes corresponding to the batch items,\n        redefining mb_x and mb_y too\n    - _unpack_minibatch sends the dictionary values to the GPU device\n    - forward and criterion are adapted for machine translation tasks.\n    \"\"\"\n\n    @property\n    def mb_attention_mask(self):\n        return self.mbatch[\"attention_mask\"]\n\n    @property\n    def mb_x(self):\n        \"\"\"Current mini-batch input.\"\"\"\n        return self.mbatch[\"input_ids\"]\n\n    @property\n    def mb_y(self):\n        \"\"\"Current mini-batch target.\"\"\"\n        return self.mbatch[\"labels\"]\n\n    @property\n    def mb_decoder_in_ids(self):\n        \"\"\"Current mini-batch target.\"\"\"\n        return self.mbatch[\"decoder_input_ids\"]\n\n    @property\n    def mb_token_type_ids(self):\n        return self.mbatch[3]\n\n    def _unpack_minibatch(self):\n        \"\"\"HuggingFace minibatches are dictionaries of tensors.\n        Move tensors to the current device.\"\"\"\n        for k in self.mbatch.keys():\n            self.mbatch[k] = self.mbatch[k].to(self.device)\n\n    def forward(self):\n        out = self.model(\n            input_ids=self.mb_x,\n            attention_mask=self.mb_attention_mask,\n            labels=self.mb_y,\n        )\n        return out.logits\n\n    def criterion(self):\n        mb_output = self.mb_output.view(-1, self.mb_output.size(-1))\n        ll = self._criterion(mb_output, self.mb_y.view(-1))\n        return ll\n\n\ndef main():\n    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", padding=True)\n    tokenizer.save_pretrained(\"./MLDATA/NLP/hf_tokenizers\")  # CHANGE DIRECTORY\n\n    prefix = \"<2en>\"\n    source_lang = \"de\"\n    target_lang = \"en\"\n    remote_data = load_dataset(\"news_commentary\", \"de-en\")\n\n    def preprocess_function(examples):\n        inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n        targets = [example[target_lang] for example in examples[\"translation\"]]\n        model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(targets, max_length=128, truncation=True)\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n\n    remote_data = remote_data.map(preprocess_function, batched=True)\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n    remote_data = remote_data.remove_columns([\"id\", \"translation\"])\n    remote_data.set_format(type=\"torch\")\n    data_collator = CustomDataCollatorSeq2SeqBeta(tokenizer=tokenizer, model=model)\n\n    train_exps = []\n    for i in range(0, 2):\n        # We use very small experiences only to showcase the library.\n        # Adapt this to your own benchmark\n        exp_data = remote_data[\"train\"].select(range(30 * i, 30 * (i + 1)))\n        tl = DataAttribute(ConstantSequence(i, len(exp_data)), \"targets_task_labels\")\n\n        exp = CLExperience()\n        exp.dataset = AvalancheDataset(\n            [exp_data], data_attributes=[tl], collate_fn=data_collator\n        )\n        train_exps.append(exp)\n\n    benchmark = CLScenario(\n        [\n            CLStream(\"train\", train_exps),\n            # add more stream here (validation, test, out-of-domain, ...)\n        ]\n    )\n    eval_plugin = avalanche.training.plugins.EvaluationPlugin(\n        avalanche.evaluation.metrics.loss_metrics(\n            epoch=True, experience=True, stream=True\n        ),\n        loggers=[avalanche.logging.InteractiveLogger()],\n        strict_checks=False,\n    )\n    plugins = [ReplayPlugin(mem_size=200)]\n    optimizer = torch.optim.Adam(model.parameters(), lr=2)\n    strategy = HGNaive(\n        model,\n        optimizer,\n        torch.nn.CrossEntropyLoss(ignore_index=-100),\n        evaluator=eval_plugin,\n        train_epochs=1,\n        train_mb_size=10,\n        plugins=plugins,\n    )\n    for experience in benchmark.train_stream:\n        strategy.train(experience, collate_fn=data_collator)\n        strategy.eval(benchmark.train_stream)\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "# Copyright (c) 2022 MetPy Developers.\n# Distributed under the terms of the BSD 3-Clause License.\n# SPDX-License-Identifier: BSD-3-Clause\n\"\"\"\n=============================\nSounding Calculation Examples\n=============================\n\nUse functions from `metpy.calc` to perform a number of calculations using sounding data.\n\nThe code below uses example data to perform many sounding calculations for a severe weather\nevent on May 22, 2011 from the Norman, OK sounding.\n\"\"\"\nimport numpy as np\nimport pandas as pd\n\nimport metpy.calc as mpcalc\nfrom metpy.cbook import get_test_data\nfrom metpy.units import units\n\n###########################################\n# Effective Shear Algorithm for use in Supercell Composite Calculation\n\n\ndef effective_layer(p, t, td, h, height_layer=False):\n    \"\"\"A function that determines the effective inflow layer for a convective sounding.\n\n    Uses the default values of Thompason et al. (2004) for CAPE (100 J/kg) and CIN (-250 J/kg).\n\n    Input:\n      - p: sounding pressure with units\n      - T: sounding temperature with units\n      - Td: sounding dewpoint temperature with units\n      - h: sounding heights with units\n\n    Returns:\n      - pbot/hbot, ptop/htop: pressure/height of the bottom level,\n                              pressure/height of the top level\n    \"\"\"\n    from metpy.calc import cape_cin, parcel_profile\n    from metpy.units import units\n\n    pbot = None\n\n    for i in range(p.shape[0]):\n        prof = parcel_profile(p[i:], t[i], td[i])\n        sbcape, sbcin = cape_cin(p[i:], t[i:], td[i:], prof)\n        if sbcape >= 100 * units('J/kg') and sbcin > -250 * units('J/kg'):\n            pbot = p[i]\n            hbot = h[i]\n            bot_idx = i\n            break\n    if not pbot:\n        return None, None\n\n    for i in range(bot_idx + 1, p.shape[0]):\n        prof = parcel_profile(p[i:], t[i], td[i])\n        sbcape, sbcin = cape_cin(p[i:], t[i:], td[i:], prof)\n        if sbcape < 100 * units('J/kg') or sbcin < -250 * units('J/kg'):\n            ptop = p[i]\n            htop = h[i]\n            break\n\n    if height_layer:\n        return hbot, htop\n    else:\n        return pbot, ptop\n\n\n###########################################\n# Upper air data can be obtained using the siphon package, but for this example we will use\n# some of MetPy's sample data.\ncol_names = ['pressure', 'height', 'temperature', 'dewpoint', 'direction', 'speed']\n\ndf = pd.read_fwf(get_test_data('20110522_OUN_12Z.txt', as_file_obj=False),\n                 skiprows=7, usecols=[0, 1, 2, 3, 6, 7], names=col_names)\n\n# Drop any rows with all NaN values for T, Td, winds\ndf = df.dropna(subset=('temperature', 'dewpoint', 'direction', 'speed'\n                       ), how='all').reset_index(drop=True)\n\n###########################################\n# Isolate needed variables from our data file and attach units\np = df['pressure'].values * units.hPa\nT = df['temperature'].values * units.degC\nTd = df['dewpoint'].values * units.degC\nwdir = df['direction'].values * units.degree\nsped = df['speed'].values * units.knot\nheight = df['height'].values * units.meter\n\n###########################################\n# Compute the wind components\nu, v = mpcalc.wind_components(sped, wdir)\n\n###########################################\n# Compute common sounding index parameters\nctotals = mpcalc.cross_totals(p, T, Td)\nkindex = mpcalc.k_index(p, T, Td)\nshowalter = mpcalc.showalter_index(p, T, Td)\ntotal_totals = mpcalc.total_totals_index(p, T, Td)\nvert_totals = mpcalc.vertical_totals(p, T)\n\n###########################################\n# Compture the parcel profile for a surface-based parcel\nprof = mpcalc.parcel_profile(p, T[0], Td[0])\n\n###########################################\n# Compute the corresponding LI, CAPE, CIN values for a surface parcel\nlift_index = mpcalc.lifted_index(p, T, prof)\ncape, cin = mpcalc.cape_cin(p, T, Td, prof)\n\n###########################################\n# Determine the LCL, LFC, and EL for our surface parcel\nlclp, lclt = mpcalc.lcl(p[0], T[0], Td[0])\nlfcp, _ = mpcalc.lfc(p, T, Td)\nel_pressure, _ = mpcalc.el(p, T, Td, prof)\n\n###########################################\n# Compute the characteristics of a mean layer parcel (50-hPa depth)\nml_t, ml_td = mpcalc.mixed_layer(p, T, Td, depth=50 * units.hPa)\nml_p, _, _ = mpcalc.mixed_parcel(p, T, Td, depth=50 * units.hPa)\nmlcape, mlcin = mpcalc.mixed_layer_cape_cin(p, T, prof, depth=50 * units.hPa)\n\n###########################################\n# Compute the characteristics of the most unstable parcel (50-hPa depth)\nmu_p, mu_t, mu_td, _ = mpcalc.most_unstable_parcel(p, T, Td, depth=50 * units.hPa)\nmucape, mucin = mpcalc.most_unstable_cape_cin(p, T, Td, depth=50 * units.hPa)\n\n###########################################\n# Compute the Bunkers Storm Motion vector and use to calculate the critical angle\n(u_storm, v_storm), *_ = mpcalc.bunkers_storm_motion(p, u, v, height)\ncritical_angle = mpcalc.critical_angle(p, u, v, height, u_storm, v_storm)\n\n###########################################\n# Work on the calculations needed to compute the significant tornado parameter\n\n# Estimate height of LCL in meters from hydrostatic thickness\nnew_p = np.append(p[p > lclp], lclp)\nnew_t = np.append(T[p > lclp], lclt)\nlcl_height = mpcalc.thickness_hydrostatic(new_p, new_t)\n\n# Compute Surface-based CAPE\nsbcape, _ = mpcalc.surface_based_cape_cin(p, T, Td)\n\n# Compute SRH, given a motion vector toward the NE at 9.9 m/s\n*_, total_helicity = mpcalc.storm_relative_helicity(height, u, v, depth=1 * units.km,\n                                                    storm_u=u_storm, storm_v=v_storm)\n\n# Copmute Bulk Shear components and then magnitude\nubshr, vbshr = mpcalc.bulk_shear(p, u, v, height=height, depth=6 * units.km)\nbshear = mpcalc.wind_speed(ubshr, vbshr)\n\n# Use all computed pieces to calculate the Significant Tornado parameter\nsig_tor = mpcalc.significant_tornado(sbcape, lcl_height,\n                                     total_helicity, bshear).to_base_units()\n\n###########################################\n# Compute the supercell composite parameter, if possible\n\n# Determine the top and bottom of the effective layer using our own function\nhbot, htop = effective_layer(p, T, Td, height, height_layer=True)\n\n# Perform the calculation of supercell composite if an effective layer exists\nif hbot:\n    esrh = mpcalc.storm_relative_helicity(height, u, v, depth=htop - hbot, bottom=hbot)\n    eubshr, evbshr = mpcalc.bulk_shear(p, u, v, height=height, depth=htop - hbot, bottom=hbot)\n    ebshear = mpcalc.wind_speed(eubshr, evbshr)\n\n    super_comp = mpcalc.supercell_composite(mucape, esrh[0], ebshear)\nelse:\n    super_comp = np.nan\n\n###########################################\n# Print Important Sounding Parameters\nprint('Important Sounding Parameters for KOUN on 22 Mary 2011 12 UTC')\nprint()\nprint(f'        CAPE: {cape:.2f}')\nprint(f'         CIN: {cin:.2f}')\nprint(f'LCL Pressure: {lclp:.2f}')\nprint(f'LFC Pressure: {lfcp:.2f}')\nprint(f' EL Pressure: {el_pressure:.2f}')\nprint()\nprint(f'   Lifted Index: {lift_index:.2f}')\nprint(f'        K-Index: {kindex:.2f}')\nprint(f'Showalter Index: {showalter:.2f}')\nprint(f'   Cross Totals: {ctotals:.2f}')\nprint(f'   Total Totals: {total_totals:.2f}')\nprint(f'Vertical Totals: {vert_totals:.2f}')\nprint()\nprint('Mixed Layer - Lowest 50-hPa')\nprint(f'     ML Temp: {ml_t:.2f}')\nprint(f'     ML Dewp: {ml_td:.2f}')\nprint(f'     ML CAPE: {mlcape:.2f}')\nprint(f'      ML CIN: {mlcin:.2f}')\nprint()\nprint('Most Unstable - Lowest 50-hPa')\nprint(f'     MU Temp: {mu_t:.2f}')\nprint(f'     MU Dewp: {mu_td:.2f}')\nprint(f' MU Pressure: {mu_p:.2f}')\nprint(f'     MU CAPE: {mucape:.2f}')\nprint(f'      MU CIN: {mucin:.2f}')\nprint()\nprint('Bunkers Storm Motion Vector')\nprint(f'  u_storm: {u_storm:.2f}')\nprint(f'  v_storm: {v_storm:.2f}')\nprint(f'Critical Angle: {critical_angle:.2f}')\nprint()\nprint(f'Storm Relative Helicity: {total_helicity:.2f}')\nprint(f'Significant Tornado Parameter: {sig_tor:.2f}')\nprint(f'Supercell Composite Parameter: {super_comp:.2f}')\n",
  "# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom helpers.supervised_pt_ditto import SupervisedPTDittoHelper\nfrom learners.supervised_monai_prostate_learner import SupervisedMonaiProstateLearner\nfrom monai.losses import DiceLoss\nfrom monai.networks.nets.unet import UNet\n\nfrom nvflare.apis.dxo import DXO, DataKind, MetaKey, from_shareable\nfrom nvflare.apis.fl_constant import ReturnCode\nfrom nvflare.apis.fl_context import FLContext\nfrom nvflare.apis.shareable import Shareable, make_reply\nfrom nvflare.apis.signal import Signal\nfrom nvflare.app_common.app_constant import AppConstants\n\n\nclass SupervisedMonaiProstateDittoLearner(SupervisedMonaiProstateLearner):\n    def __init__(\n        self,\n        train_config_filename,\n        aggregation_epochs: int = 1,\n        ditto_model_epochs: int = 1,\n        train_task_name: str = AppConstants.TASK_TRAIN,\n    ):\n        \"\"\"Trainer for prostate segmentation task. It inherits from MONAI trainer.\n\n        Args:\n            train_config_filename: directory of config file.\n            aggregation_epochs: the number of training epochs of global model for a round. Defaults to 1.\n            ditto_model_epochs: the number of training epochs of personalized model for a round. Defaults to 1.\n            train_task_name: name of the task to train the model.\n\n        Returns:\n            a Shareable with the updated local model after running `execute()`\n        \"\"\"\n        SupervisedMonaiProstateLearner.__init__(\n            self,\n            train_config_filename=train_config_filename,\n            aggregation_epochs=aggregation_epochs,\n            train_task_name=train_task_name,\n        )\n        self.ditto_helper = None\n        self.ditto_model_epochs = ditto_model_epochs\n\n    def train_config(self, fl_ctx: FLContext):\n        # Initialize superclass\n        SupervisedMonaiProstateLearner.train_config(self, fl_ctx)\n\n        engine = fl_ctx.get_engine()\n        ws = engine.get_workspace()\n        app_dir = ws.get_app_dir(fl_ctx.get_job_id())\n\n        # Initialize PTDittoHelper\n        ditto_model = UNet(\n            spatial_dims=3,\n            in_channels=1,\n            out_channels=1,\n            channels=(16, 32, 64, 128, 256),\n            strides=(2, 2, 2, 2),\n            num_res_units=2,\n        ).to(self.device)\n        ditto_optimizer = optim.SGD(\n            ditto_model.parameters(),\n            lr=self.config_info[\"ditto_learning_rate\"],\n            momentum=0.9,\n        )\n        self.ditto_helper = SupervisedPTDittoHelper(\n            criterion=DiceLoss(sigmoid=True),\n            model=ditto_model,\n            optimizer=ditto_optimizer,\n            device=self.device,\n            app_dir=app_dir,\n            ditto_lambda=self.config_info[\"ditto_lambda\"],\n            model_epochs=self.ditto_model_epochs,\n        )\n\n    def train(\n        self,\n        shareable: Shareable,\n        fl_ctx: FLContext,\n        abort_signal: Signal,\n    ) -> Shareable:\n        \"\"\"Training task pipeline for Ditto\n        Get global model weights (potentially with HE)\n        Prepare for fedprox loss\n        Load Ditto personalized model info\n        Local training reference model and personalized model\n        Return updated weights of reference model (model_diff)\n        \"\"\"\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n\n        # get round information\n        current_round = shareable.get_header(AppConstants.CURRENT_ROUND)\n        total_rounds = shareable.get_header(AppConstants.NUM_ROUNDS)\n        self.log_info(fl_ctx, f\"Current/Total Round: {current_round + 1}/{total_rounds}\")\n        self.log_info(fl_ctx, f\"Client identity: {fl_ctx.get_identity_name()}\")\n\n        # update local model weights with received weights\n        dxo = from_shareable(shareable)\n        global_weights = dxo.data\n\n        # Before loading weights, tensors might need to be reshaped to support HE for secure aggregation.\n        local_var_dict = self.model.state_dict()\n        model_keys = global_weights.keys()\n        for var_name in local_var_dict:\n            if var_name in model_keys:\n                weights = global_weights[var_name]\n                try:\n                    # reshape global weights to compute difference later on\n                    global_weights[var_name] = np.reshape(weights, local_var_dict[var_name].shape)\n                    # update the local dict\n                    local_var_dict[var_name] = torch.as_tensor(global_weights[var_name])\n                except Exception as e:\n                    raise ValueError(\"Convert weight from {} failed with error: {}\".format(var_name, str(e)))\n        self.model.load_state_dict(local_var_dict)\n\n        # Load Ditto personalized model\n        self.ditto_helper.load_model(local_var_dict)\n\n        # local steps\n        epoch_len = len(self.train_loader)\n        self.log_info(fl_ctx, f\"Local steps per epoch: {epoch_len}\")\n\n        # make a copy of model_global as reference for\n        # 1. FedProx loss of reference model\n        # 2. Ditto loss of personalized model\n        model_global = copy.deepcopy(self.model)\n        for param in model_global.parameters():\n            param.requires_grad = False\n\n        # local train reference model\n        self.local_train(\n            fl_ctx=fl_ctx,\n            train_loader=self.train_loader,\n            model_global=model_global,\n            abort_signal=abort_signal,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n        self.epoch_of_start_time += self.aggregation_epochs\n\n        # local train ditto model\n        self.ditto_helper.local_train(\n            train_loader=self.train_loader,\n            model_global=model_global,\n            abort_signal=abort_signal,\n            writer=self.writer,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n\n        # local valid ditto model each round\n        metric = self.local_valid(\n            self.ditto_helper.model,\n            self.valid_loader,\n            abort_signal,\n            tb_id=\"val_metric_per_model\",\n            record_epoch=self.ditto_helper.epoch_global,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n        self.log_info(fl_ctx, f\"val_metric_per_model: {metric:.4f}\")\n        # save model\n        self.ditto_helper.update_metric_save_model(metric=metric)\n\n        # compute delta model, global model has the primary key set\n        local_weights = self.model.state_dict()\n        model_diff = {}\n        for name in global_weights:\n            if name not in local_weights:\n                continue\n            model_diff[name] = np.subtract(local_weights[name].cpu().numpy(), global_weights[name], dtype=np.float32)\n            if np.any(np.isnan(model_diff[name])):\n                self.system_panic(f\"{name} weights became NaN...\", fl_ctx)\n                return make_reply(ReturnCode.EXECUTION_EXCEPTION)\n\n        # flush the tb writer\n        self.writer.flush()\n\n        # build the shareable\n        dxo = DXO(data_kind=DataKind.WEIGHT_DIFF, data=model_diff)\n        dxo.set_meta_prop(MetaKey.NUM_STEPS_CURRENT_ROUND, epoch_len)\n\n        self.log_info(fl_ctx, \"Local epochs finished. Returning shareable\")\n        return dxo.to_shareable()\n",
  "# This script explains the basic functionality of ``SequentialProcessors`` for\n# data augmentation in an object-detection task.\n\nimport os\nimport numpy as np\nfrom tensorflow.keras.utils import get_file\n\nfrom paz.abstract import SequentialProcessor, ProcessingSequence\nfrom paz.models.detection.utils import create_prior_boxes\nimport paz.processors as pr\nimport paz.backend as P\n\n\n# let's download a test image and put it inside our PAZ directory\nIMAGE_URL = ('https://github.com/oarriaga/altamira-data/releases/download'\n             '/v0.9/object_detection_augmentation.png')\nfilename = os.path.basename(IMAGE_URL)\nimage_fullpath = get_file(filename, IMAGE_URL, cache_subdir='paz/tutorials')\n\n# Images\n\n\n# We can also create sequential pipelines by inheriting ``SequentialProcessor``\nclass AugmentImage(SequentialProcessor):\n    def __init__(self):\n        super(AugmentImage, self).__init__()\n        self.add(pr.RandomContrast())\n        self.add(pr.RandomBrightness())\n        self.add(pr.RandomSaturation())\n        self.add(pr.RandomHue())\n\n\nclass PreprocessImage(SequentialProcessor):\n    def __init__(self, shape, mean=pr.BGR_IMAGENET_MEAN):\n        super(PreprocessImage, self).__init__()\n        self.add(pr.ResizeImage(shape))\n        self.add(pr.CastImage(float))\n        if mean is None:\n            self.add(pr.NormalizeImage())\n        else:\n            self.add(pr.SubtractMeanImage(mean))\n\n\n# Let's see who it works:\npreprocess_image, augment_image = PreprocessImage((300, 300)), AugmentImage()\nprint('Image pre-processing examples')\nfor _ in range(10):\n    image = P.image.load_image(image_fullpath)\n    image = preprocess_image(augment_image(image))\n    P.image.show_image(image.astype('uint8'))\n\n# Boxes\n\n# Let's first build our box labels:\n# For a tutorial on how to build your box labels check here:\n# paz/examples/tutorials/bounding_boxes.py\nH, W = P.image.load_image(image_fullpath).shape[:2]\nclass_names = ['background', 'human', 'horse']\nbox_data = np.array([[200 / W, 60 / H, 300 / W, 200 / H, 1],\n                     [100 / W, 90 / H, 400 / W, 300 / H, 2]])\n\n\n# The augment boxes pipeline\nclass AugmentBoxes(SequentialProcessor):\n    def __init__(self, mean=pr.BGR_IMAGENET_MEAN):\n        super(AugmentBoxes, self).__init__()\n        self.add(pr.ToImageBoxCoordinates())\n        self.add(pr.Expand(mean=mean))\n        self.add(pr.RandomSampleCrop())\n        self.add(pr.RandomFlipBoxesLeftRight())\n        self.add(pr.ToNormalizedBoxCoordinates())\n\n\n# We now visualize our current box augmentation\n# For that we build a quick pipeline for drawing our boxes\ndraw_boxes = SequentialProcessor([\n    pr.ControlMap(pr.ToBoxes2D(class_names, False), [1], [1]),\n    pr.ControlMap(pr.DenormalizeBoxes2D(), [0, 1], [1], {0: 0}),\n    pr.DrawBoxes2D(class_names),\n    pr.ShowImage()])\n\n# Let's test it our box data augmentation pipeline\naugment_boxes = AugmentBoxes()\nprint('Box augmentation examples')\nfor _ in range(10):\n    image = P.image.load_image(image_fullpath)\n    image, boxes = augment_boxes(image, box_data.copy())\n    draw_boxes(P.image.resize_image(image, (300, 300)), boxes)\n\n\n# There is also some box-preprocessing that is required.\n# Mostly we must match our boxes to a set of default (prior) boxes.\n# Then we must encode them and expand the class label to a one-hot vector.\nclass PreprocessBoxes(SequentialProcessor):\n    def __init__(self, num_classes, prior_boxes, IOU, variances):\n        super(PreprocessBoxes, self).__init__()\n        self.add(pr.MatchBoxes(prior_boxes, IOU),)\n        self.add(pr.EncodeBoxes(prior_boxes, variances))\n        self.add(pr.BoxClassToOneHotVector(num_classes))\n\n\n# Putting everything together in a single processor:\nclass AugmentDetection(SequentialProcessor):\n    def __init__(self, prior_boxes, split=pr.TRAIN, num_classes=21, size=300,\n                 mean=pr.BGR_IMAGENET_MEAN, IOU=.5,\n                 variances=[0.1, 0.1, 0.2, 0.2]):\n        super(AugmentDetection, self).__init__()\n\n        # image processors\n        self.augment_image = AugmentImage()\n        self.augment_image.add(pr.ConvertColorSpace(pr.RGB2BGR))\n        self.preprocess_image = PreprocessImage((size, size), mean)\n\n        # box processors\n        self.augment_boxes = AugmentBoxes()\n        args = (num_classes, prior_boxes, IOU, variances)\n        self.preprocess_boxes = PreprocessBoxes(*args)\n\n        # pipeline\n        self.add(pr.UnpackDictionary(['image', 'boxes']))\n        self.add(pr.ControlMap(pr.LoadImage(), [0], [0]))\n        if split == pr.TRAIN:\n            self.add(pr.ControlMap(self.augment_image, [0], [0]))\n            self.add(pr.ControlMap(self.augment_boxes, [0, 1], [0, 1]))\n        self.add(pr.ControlMap(self.preprocess_image, [0], [0]))\n        self.add(pr.ControlMap(self.preprocess_boxes, [1], [1]))\n        self.add(pr.SequenceWrapper(\n            {0: {'image': [size, size, 3]}},\n            {1: {'boxes': [len(prior_boxes), 4 + num_classes]}}))\n\n\nprior_boxes = create_prior_boxes()\ndraw_boxes.processors[0].processor.one_hot_encoded = True\ndraw_boxes.insert(0, pr.ControlMap(pr.DecodeBoxes(prior_boxes), [1], [1]))\ndraw_boxes.insert(2, pr.ControlMap(\n    pr.FilterClassBoxes2D(class_names[1:]), [1], [1]))\n\n\ndef deprocess_image(image):\n    image = (image + pr.BGR_IMAGENET_MEAN).astype('uint8')\n    return P.image.convert_color_space(image, pr.BGR2RGB)\n\n\naugmentator = AugmentDetection(prior_boxes, num_classes=len(class_names))\nprint('Image and boxes augmentations')\nfor _ in range(10):\n    sample = {'image': image_fullpath, 'boxes': box_data.copy()}\n    data = augmentator(sample)\n    image, boxes = data['inputs']['image'], data['labels']['boxes']\n    image = deprocess_image(image)\n    draw_boxes(image, boxes)\n\n# Note that we change the input and output format from lists to a dictionaries.\n# The input changed by adding the ``pr.UnpackDictionary`` processor, and the\n# output changed by the ``pr.SequenceWrapper`` processor.\n# The ``pr.SequenceWrapper`` method allows us to easily connect the complete\n# pipeline to a Sequence Generator.\ndata = [{'image': image_fullpath, 'boxes': box_data}]\nprint('Image and boxes augmentations with generator')\nbatch_size = 1\nsequence = ProcessingSequence(augmentator, batch_size, data)\nfor _ in range(10):\n    batch = sequence.__getitem__(0)\n    batch_images, batch_boxes = batch[0]['image'], batch[1]['boxes']\n    image, boxes = batch_images[0], batch_boxes[0]\n    image = deprocess_image(image)\n    draw_boxes(image, boxes)\n",
  "#!/usr/bin/env python\n#\n# Author: Hung Pham <pqh3.14@gmail.com>\n#\n\"\"\"\nThis example shows how to construct MLWFs from the pywannier90 tool.\nThe MLWFs are then used to interpolate the k-space hamiltonian, hence the band structure\n\"\"\"\n\nimport numpy as np\nfrom pyscf.pbc import gto, scf, cc, df\nfrom pyscf.pbc.tools import pywannier90\n\n\n# Definte unit cell \ncell = gto.Cell()\ncell.atom = [['Si',[0.0,0.0,0.0]], ['Si',[1.35775, 1.35775, 1.35775]]]\ncell.a = [[0.0, 2.7155, 2.7155], [2.7155, 0.0, 2.7155], [2.7155, 2.7155, 0.0]]\ncell.basis = 'gth-dzv'\ncell.pseudo = 'gth-pade'\ncell.exp_to_discard = 0.1\ncell.build()\n\n\n# PBE calculation\nkmesh = [3, 3, 3]\nkpts = cell.make_kpts(kmesh)\nnkpts = kpts.shape[0]\nkks = scf.KKS(cell, kpts)\nkks.xc = 'PBE'\nkks.chkfile = 'chkfile'\nkks.init_guess = 'chkfile'\nkks.run()\n\n\n# the kks object can be saved and loaded before running pyWannier90\n# Hence, one does not need to perform the kks calculation every time \n# the localization parameters change, for example, the guessing orbitals or the disentanglement windows\npywannier90.save_kmf(kks, 'chk_PBE')\nkmf = pywannier90.load_kmf('chk_PBE')\n\n\n# (1) Construct MLWFs\nnum_wann = 8\nkeywords = \\\n\"\"\"\nbegin projections\nSi:sp3\nend projections\n\"\"\"\nw90 = pywannier90.W90(kmf, cell, kmesh, num_wann, other_keywords=keywords)\nw90.kernel()    \n\n\n# (2) Export the MWLFs in the xsf format for plotting with VESTA\nw90.plot_wf(supercell=kmesh, grid=[20,20,20])\n\n\n# (3) Export wannier90.mmn, wannier90.amn, wannier90.eig matrix and then run a wannier90 using these\nw90.export_AME()\nw90.kernel(external_AME='wannier90')\n\n\n# (4) Interpolate the Fock or band structure using the Slater-Koster scheme\n# This can be applied to either a (restricted or unrestricted) DFT or HF wfs to get the band structure\nband_kpts = kpts + 0.5 * kpts[1]\nfrac_kpts = cell.get_scaled_kpts(band_kpts)\ninterpolated_fock = w90.interpolate_ham_kpts(frac_kpts)     # the interpolated Fock\nbands = w90.interpolate_band(frac_kpts)                     # the interpolated band by pyWannier90\nbands_ref = kks.get_bands(band_kpts)                        # Band interpolated by PySCF\n\n\n# This difference should be decreasing when a denser k-mesh is used\nprint(\"Difference in the eigenvalues interpolated by scf.get_bands function and by pywannier90: %10.8e\" % \\\n(abs(bands[0] -bands_ref[0]).max()))\n\n\n# (5) Plotting band structure using mcu: https://github.com/hungpham2017/mcu\nimport mcu\npyscf_plot = mcu.PYSCF(cell)\nkpath = '''    \nL 0.5 0.5 0.5       \nG 0.0 0.0 0.0\nX 0.5 0.0 0.5\nW 0.5 0.25 0.75\nK 0.375 0.375 0.75\nG 0.0 0.0 0.0\n'''\nfrac_kpts, abs_kpts = pyscf_plot.make_kpts(kpath, 11)\nbands = w90.interpolate_band(frac_kpts, use_ws_distance=True)\npyscf_plot.set_kpts_bands([frac_kpts, bands])\npyscf_plot.get_bandgap()\npyscf_plot.plot_band(ylim=[-17,17], klabel=kpath)\n",
  "# -*- coding: utf-8 -*-\n\"\"\"\n==================\nUse the CRM corpus\n==================\n\nThis shows how to use the CRM corpus functions.\n\n@author: rkmaddox\n\"\"\"\n\nfrom expyfun._utils import _TempDir\nfrom expyfun import ExperimentController, analyze, building_doc\nfrom expyfun.stimuli import (crm_prepare_corpus, crm_sentence, crm_info,\n                             crm_response_menu, add_pad, CRMPreload)\n\nimport numpy as np\n\nprint(__doc__)\n\ncrm_path = _TempDir()\nfs = 40000\n\n###############################################################################\n# Prepare the corpus\n# ------------------\n#\n# For simplicity, here we prepare just two talkers at the native 40000 Hz\n# sampling rate.\n#\n# .. note:: For your experiment, you only need to prepare the corpus once per\n#           sampling rate, you should probably use the default path, and you\n#           should just do all the talkers at once. For the example, we are\n#           using fs=40000 and only doing two talkers so that the stimulus\n#           preparation is very fast, and a temp directory so that we don't\n#           interfere with any other prepared corpuses. Your code will likely\n#           look like this line, and not appear in your actual experiment\n#           script::\n#\n#               >>> crm_prepare_corpus(24414)\n#\n\ncrm_prepare_corpus(fs, path_out=crm_path, overwrite=True,\n                   talker_list=[dict(sex=0, talker_num=0),\n                                dict(sex=1, talker_num=0)])\n\n# print the valid callsigns\nprint('Valid callsigns are {0}'.format(crm_info()['callsign']))\n\n# read a sentence in from the hard drive\nx1 = 0.5 * crm_sentence(fs, 'm', '0', 'c', 'r', '5', path=crm_path)\n\n# preload all the talkers and get a second sentence from memory\ncrm = CRMPreload(fs, path=crm_path)\nx2 = crm.sentence('f', '0', 'ringo', 'green', '6')\n\nx = add_pad([x1, x2], alignment='start')\n\n###############################################################################\n# Now we actually run the experiment.\n\nmax_wait = 0.01 if building_doc else 3\nwith ExperimentController(\n        exp_name='CRM corpus example', window_size=(720, 480),\n        full_screen=False, participant='foo', session='foo', version='dev',\n        output_dir=None, stim_fs=40000) as ec:\n    ec.screen_text('Report the color and number spoken by the female '\n                   'talker.', wrap=True)\n    screenshot = ec.screenshot()\n    ec.flip()\n    ec.wait_secs(max_wait)\n\n    ec.load_buffer(x)\n    ec.identify_trial(ec_id='', ttl_id=[])\n    ec.start_stimulus()\n    ec.wait_secs(x.shape[-1] / float(fs))\n\n    resp = crm_response_menu(ec, max_wait=0.01 if building_doc else np.inf)\n    if resp == ('g', '6'):\n        ec.screen_prompt('Correct!', max_wait=max_wait)\n    else:\n        ec.screen_prompt('Incorrect.', max_wait=max_wait)\n    ec.trial_ok()\n\nanalyze.plot_screen(screenshot)\n",
  "# -*- coding: utf-8 -*-\n\"\"\"\nExample demonstrating a variety of scatter plot features.\n\"\"\"\n\n\n\n## Add path to library (just for examples; you do not need this)\nimport initExample\n\nfrom pyqtgraph.Qt import QtGui, QtCore\nimport pyqtgraph as pg\nimport numpy as np\nfrom collections import namedtuple\nfrom itertools import chain\n\napp = pg.mkQApp(\"Scatter Plot Item Example\") \nmw = QtGui.QMainWindow()\nmw.resize(800,800)\nview = pg.GraphicsLayoutWidget()  ## GraphicsView with GraphicsLayout inserted by default\nmw.setCentralWidget(view)\nmw.show()\nmw.setWindowTitle('pyqtgraph example: ScatterPlot')\n\n## create four areas to add plots\nw1 = view.addPlot()\nw2 = view.addViewBox()\nw2.setAspectLocked(True)\nview.nextRow()\nw3 = view.addPlot()\nw4 = view.addPlot()\nprint(\"Generating data, this takes a few seconds...\")\n\n## Make all plots clickable\nclickedPen = pg.mkPen('b', width=2)\nlastClicked = []\ndef clicked(plot, points):\n    global lastClicked\n    for p in lastClicked:\n        p.resetPen()\n    print(\"clicked points\", points)\n    for p in points:\n        p.setPen(clickedPen)\n    lastClicked = points\n\n\n## There are a few different ways we can draw scatter plots; each is optimized for different types of data:\n\n## 1) All spots identical and transform-invariant (top-left plot).\n## In this case we can get a huge performance boost by pre-rendering the spot\n## image and just drawing that image repeatedly.\n\nn = 300\ns1 = pg.ScatterPlotItem(size=10, pen=pg.mkPen(None), brush=pg.mkBrush(255, 255, 255, 120))\npos = np.random.normal(size=(2,n), scale=1e-5)\nspots = [{'pos': pos[:,i], 'data': 1} for i in range(n)] + [{'pos': [0,0], 'data': 1}]\ns1.addPoints(spots)\nw1.addItem(s1)\ns1.sigClicked.connect(clicked)\n\n\n## 2) Spots are transform-invariant, but not identical (top-right plot).\n## In this case, drawing is almsot as fast as 1), but there is more startup\n## overhead and memory usage since each spot generates its own pre-rendered\n## image.\n\nTextSymbol = namedtuple(\"TextSymbol\", \"label symbol scale\")\n\ndef createLabel(label, angle):\n    symbol = QtGui.QPainterPath()\n    #symbol.addText(0, 0, QFont(\"San Serif\", 10), label)\n    f = QtGui.QFont()\n    f.setPointSize(10)\n    symbol.addText(0, 0, f, label)\n    br = symbol.boundingRect()\n    scale = min(1. / br.width(), 1. / br.height())\n    tr = QtGui.QTransform()\n    tr.scale(scale, scale)\n    tr.rotate(angle)\n    tr.translate(-br.x() - br.width()/2., -br.y() - br.height()/2.)\n    return TextSymbol(label, tr.map(symbol), 0.1 / scale)\n\nrandom_str = lambda : (''.join([chr(np.random.randint(ord('A'),ord('z'))) for i in range(np.random.randint(1,5))]), np.random.randint(0, 360))\n\ns2 = pg.ScatterPlotItem(size=10, pen=pg.mkPen('w'), pxMode=True)\npos = np.random.normal(size=(2,n), scale=1e-5)\nspots = [{'pos': pos[:,i], 'data': 1, 'brush':pg.intColor(i, n), 'symbol': i%10, 'size': 5+i/10.} for i in range(n)]\ns2.addPoints(spots)\nspots = [{'pos': pos[:,i], 'data': 1, 'brush':pg.intColor(i, n), 'symbol': label[1], 'size': label[2]*(5+i/10.)} for (i, label) in [(i, createLabel(*random_str())) for i in range(n)]]\ns2.addPoints(spots)\nw2.addItem(s2)\ns2.sigClicked.connect(clicked)\n\n\n## 3) Spots are not transform-invariant, not identical (bottom-left).\n## This is the slowest case, since all spots must be completely re-drawn\n## every time because their apparent transformation may have changed.\n\ns3 = pg.ScatterPlotItem(\n    pxMode=False,  # Set pxMode=False to allow spots to transform with the view\n    hoverable=True,\n    hoverPen=pg.mkPen('g'),\n    hoverSize=1e-6\n)\nspots3 = []\nfor i in range(10):\n    for j in range(10):\n        spots3.append({'pos': (1e-6*i, 1e-6*j), 'size': 1e-6, 'pen': {'color': 'w', 'width': 2}, 'brush':pg.intColor(i*10+j, 100)})\ns3.addPoints(spots3)\nw3.addItem(s3)\ns3.sigClicked.connect(clicked)\n\n## Test performance of large scatterplots\n\ns4 = pg.ScatterPlotItem(\n    size=10,\n    pen=pg.mkPen(None),\n    brush=pg.mkBrush(255, 255, 255, 20),\n    hoverable=True,\n    hoverSymbol='s',\n    hoverSize=15,\n    hoverPen=pg.mkPen('r', width=2),\n    hoverBrush=pg.mkBrush('g'),\n)\nn = 10000\npos = np.random.normal(size=(2, n), scale=1e-9)\ns4.addPoints(\n    x=pos[0],\n    y=pos[1],\n    # size=(np.random.random(n) * 20.).astype(int),\n    # brush=[pg.mkBrush(x) for x in np.random.randint(0, 256, (n, 3))],\n    data=np.arange(n)\n)\nw4.addItem(s4)\ns4.sigClicked.connect(clicked)\n\nif __name__ == '__main__':\n    pg.exec()\n",
  "\"\"\"\n.. _ref_contact_example:\n\nContact Element Example\n~~~~~~~~~~~~~~~~~~~~~~~\n\nThis example demonstrates how to create contact elements for general\ncontact.\n\nBegin by launching MAPDL.\n\n\"\"\"\nfrom ansys.mapdl import core as pymapdl\n\nmapdl = pymapdl.launch_mapdl()\n\n###############################################################################\n# Enter the pre-processor, create a block and mesh it with tetrahedral\n# elements.\n#\nmapdl.prep7()\n\nvnum0 = mapdl.block(0, 1, 0, 1, 0, 0.5)\n\nmapdl.et(1, 187)\nmapdl.esize(0.1)\n\nmapdl.vmesh(vnum0)\nmapdl.eplot()\n\n###############################################################################\n# Second a volume block above the existing block and mesh it with\n# quadratic hexahedral elements.  Ensure that these blocks do not\n# touch by starting it slightly higher than the existing block.\n#\n# Note how these two blocks do not touch and the mesh is non-conformal.\n\nmapdl.esize(0.09)\nmapdl.et(2, 186)\nmapdl.type(2)\nvnum1 = mapdl.block(0, 1, 0, 1, 0.50001, 1)\nmapdl.vmesh(vnum1)\nmapdl.eplot()\n\n\n###############################################################################\n# Select all the elements at the intersection between the two blocks\n# and generate contact elements.\n\nmapdl.nsel(\"s\", \"loc\", \"z\", 0.5, 0.50001)\nmapdl.esln(\"s\")\noutput = mapdl.gcgen(\"NEW\", splitkey=\"SPLIT\", selopt=\"SELECT\")\nprint(output)\n\n###############################################################################\n# Plot the contact element pairs.  Note from the command output above\n# that the section IDs are 5 and 6.\n#\n# Here, we plot the element mesh as a wire-frame to show that the\n# contact pairs overlap.\n\nmapdl.esel(\"S\", \"SEC\", vmin=5, vmax=6)\nmapdl.eplot(style=\"wireframe\", line_width=3)\n\n###############################################################################\n# Stop mapdl\n# ~~~~~~~~~~\n#\nmapdl.exit()\n",
  "#!/usr/bin/env python3\n# encoding: utf-8\n\nfrom seedemu import *\n\n# Create Emulator Base with 10 Stub AS (150-154, 160-164) using Makers utility method.\n# hosts_per_stub_as=3 : create 3 hosts per one stub AS.\n# It will create hosts(physical node) named `host_{}`.format(counter), counter starts from 0. \nhosts_per_stub_as = 3\nemu = Makers.makeEmulatorBaseWith10StubASAndHosts(hosts_per_stub_as = hosts_per_stub_as)\n\n# Create the Ethereum layer\neth = EthereumService()\n\n# Create the Blockchain layer which is a sub-layer of Ethereum layer.\n# chainName=\"pos\": set the blockchain name as \"pos\"\n# consensus=\"ConsensusMechnaism.POS\" : set the consensus of the blockchain as \"ConsensusMechanism.POS\".\n# supported consensus option: ConsensusMechanism.POA, ConsensusMechanism.POW, ConsensusMechanism.POS\nblockchain = eth.createBlockchain(chainName=\"pos\", consensus=ConsensusMechanism.POS)\n\n# set `terminal_total_difficulty`, which is the value to designate when the Merge is happen.\nblockchain.setTerminalTotalDifficulty(30)\n\nasns = [150, 151, 152, 153, 154, 160, 161, 162, 163, 164]\n\n###################################################\n# Ethereum Node\n\ni = 1\nfor asn in asns:\n    for id in range(hosts_per_stub_as):        \n        # Create a blockchain virtual node named \"eth{}\".format(i)\n        e:EthereumServer = blockchain.createNode(\"eth{}\".format(i))   \n        \n        # Create Docker Container Label named 'Ethereum-POS-i'\n        e.appendClassName('Ethereum-POS-{}'.format(i))\n\n        # Enable Geth to communicate with geth node via http\n        e.enableGethHttp()\n\n        # Set host in asn 150 with id 0 (ip : 10.150.0.71) as BeaconSetupNode.\n        if asn == 150 and id == 0:\n                e.setBeaconSetupNode()\n\n        # Set host in asn 150 with id 1 (ip : 10.150.0.72) as BootNode. \n        # This node will serve as a BootNode in both execution layer (geth) and consensus layer (lighthouse).\n        if asn == 150 and id == 1:\n                e.setBootNode(True)\n\n        # Set hosts in asn 152 and 162 with id 0 and 1 as validator node. \n        # Validator is added by deposit 32 Ethereum and is activated in realtime after the Merge.\n        # isManual=True : deposit 32 Ethereum by manual. \n        #                 Other than deposit part, create validator key and running a validator node is done by codes.  \n        if asn in [151]:\n            if id == 0:\n                e.enablePOSValidatorAtRunning()\n            if id == 1:\n                e.enablePOSValidatorAtRunning(is_manual=True)\n        \n        # Set hosts in asn 152, 153, 154, and 160 as validator node.\n        # These validators are activated by default from genesis status.\n        # Before the Merge, when the consensus in this blockchain is still POA, \n        # these hosts will be the signer nodes.\n        if asn in [152,153,154,160,161,162,163,164]:\n            e.enablePOSValidatorAtGenesis()\n            e.startMiner()\n\n        # Customizing the display names (for visualiztion purpose)\n        if e.isBeaconSetupNode():\n            emu.getVirtualNode('eth{}'.format(i)).setDisplayName('Ethereum-BeaconSetup')\n        else:\n            emu.getVirtualNode('eth{}'.format(i)).setDisplayName('Ethereum-POS-{}'.format(i))\n\n        # Binding the virtual node to the physical node. \n        emu.addBinding(Binding('eth{}'.format(i), filter=Filter(asn=asn, nodeName='host_{}'.format(id))))\n\n        i = i+1\n\n\n# Add layer to the emulator\nemu.addLayer(eth)\n\nemu.render()\n\n# Enable internetMap\n# Enable etherView\ndocker = Docker(internetMapEnabled=True, etherViewEnabled=True)\n\nemu.compile(docker, './output', override = True)\n",
  "#\n#  Copyright 2019 The FATE Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nimport argparse\nimport json\n\nfrom pipeline.backend.pipeline import PipeLine\nfrom pipeline.component import DataTransform\nfrom pipeline.component import Evaluation\nfrom pipeline.component import HomoLR\nfrom pipeline.component import Reader\nfrom pipeline.component import FeatureScale\nfrom pipeline.interface import Data\nfrom pipeline.utils.tools import load_job_config\n\n\ndef main(config=\"../../config.yaml\", namespace=\"\"):\n    # obtain config\n    if isinstance(config, str):\n        config = load_job_config(config)\n    parties = config.parties\n    guest = parties.guest[0]\n    host = parties.host[0]\n    arbiter = parties.arbiter[0]\n\n    guest_train_data = {\"name\": \"breast_homo_guest\", \"namespace\": f\"experiment{namespace}\"}\n    host_train_data = {\"name\": \"breast_homo_host\", \"namespace\": f\"experiment{namespace}\"}\n\n    # initialize pipeline\n    pipeline = PipeLine()\n    # set job initiator\n    pipeline.set_initiator(role='guest', party_id=guest)\n    # set participants information\n    pipeline.set_roles(guest=guest, host=host, arbiter=arbiter)\n\n    # define Reader components to read in data\n    reader_0 = Reader(name=\"reader_0\")\n    # configure Reader for guest\n    reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    # configure Reader for host\n    reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    # define DataTransform components\n    data_transform_0 = DataTransform(\n        name=\"data_transform_0\",\n        with_label=True,\n        output_format=\"dense\")  # start component numbering at 0\n\n    scale_0 = FeatureScale(name='scale_0')\n    param = {\n        \"penalty\": \"L2\",\n        \"optimizer\": \"sgd\",\n        \"tol\": 1e-05,\n        \"alpha\": 0.01,\n        \"max_iter\": 30,\n        \"early_stop\": \"diff\",\n        \"batch_size\": -1,\n        \"learning_rate\": 0.15,\n        \"decay\": 1,\n        \"decay_sqrt\": True,\n        \"init_param\": {\n            \"init_method\": \"zeros\"\n        },\n        \"cv_param\": {\n            \"n_splits\": 4,\n            \"shuffle\": True,\n            \"random_seed\": 33,\n            \"need_cv\": False\n        }\n    }\n\n    homo_lr_0 = HomoLR(name='homo_lr_0', **param)\n\n    # add components to pipeline, in order of task execution\n    pipeline.add_component(reader_0)\n    pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))\n    # set data input sources of intersection components\n    pipeline.add_component(scale_0, data=Data(data=data_transform_0.output.data))\n    pipeline.add_component(homo_lr_0, data=Data(train_data=scale_0.output.data))\n    evaluation_0 = Evaluation(name=\"evaluation_0\", eval_type=\"binary\")\n    evaluation_0.get_party_instance(role='host', party_id=host).component_param(need_run=False)\n    pipeline.add_component(evaluation_0, data=Data(data=homo_lr_0.output.data))\n\n    # compile pipeline once finished adding modules, this step will form conf and dsl files for running job\n    pipeline.compile()\n\n    # fit model\n    pipeline.fit()\n\n    deploy_components = [data_transform_0, scale_0, homo_lr_0]\n    pipeline.deploy_component(components=deploy_components)\n    #\n    predict_pipeline = PipeLine()\n    # # add data reader onto predict pipeline\n    predict_pipeline.add_component(reader_0)\n    # # add selected components from train pipeline onto predict pipeline\n    # # specify data source\n    predict_pipeline.add_component(\n        pipeline, data=Data(\n            predict_input={\n                pipeline.data_transform_0.input.data: reader_0.output.data}))\n    predict_pipeline.compile()\n    predict_pipeline.predict()\n\n    dsl_json = predict_pipeline.get_predict_dsl()\n    conf_json = predict_pipeline.get_predict_conf()\n    # import json\n    json.dump(dsl_json, open('./homo-lr-normal-predict-dsl.json', 'w'), indent=4)\n    json.dump(conf_json, open('./homo-lr-normal-predict-conf.json', 'w'), indent=4)\n\n    # query component summary\n    print(json.dumps(pipeline.get_component(\"homo_lr_0\").get_summary(), indent=4, ensure_ascii=False))\n    print(json.dumps(pipeline.get_component(\"evaluation_0\").get_summary(), indent=4, ensure_ascii=False))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"PIPELINE DEMO\")\n    parser.add_argument(\"-config\", type=str,\n                        help=\"config file\")\n    args = parser.parse_args()\n    if args.config is not None:\n        main(args.config)\n    else:\n        main()\n",
  "#!/usr/bin/env python3\n#\n# This example shows how to set up a self-consistent fluid DREAM run,\n# where no kinetic equations are solved, but the electric field and\n# temperature are evolved self-consistently.\n#\n# Run as\n#\n#   $ ./basic.py\n#\n# ###################################################################\n\nimport numpy as np\nimport sys\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nfrom DREAM import runiface\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.CollisionHandler as Collisions\nimport DREAM.Settings.Equations.ElectricField as Efield\nimport DREAM.Settings.Equations.HotElectronDistribution as FHot\nimport DREAM.Settings.Equations.ColdElectronTemperature as T_cold\n\n\nfrom DREAM.Settings.Equations.ElectricField import ElectricField\nfrom DREAM.Settings.Equations.ColdElectronTemperature import ColdElectronTemperature\n\nds = DREAMSettings()\n\n# set collision settings\nds.collisions.collfreq_mode = Collisions.COLLFREQ_MODE_FULL\nds.collisions.collfreq_type = Collisions.COLLFREQ_TYPE_PARTIALLY_SCREENED\n#ds.collisions.bremsstrahlung_mode = Collisions.BREMSSTRAHLUNG_MODE_NEGLECT\nds.collisions.bremsstrahlung_mode = Collisions.BREMSSTRAHLUNG_MODE_STOPPING_POWER\n#ds.collisions.lnlambda = Collisions.LNLAMBDA_CONSTANT\nds.collisions.lnlambda = Collisions.LNLAMBDA_ENERGY_DEPENDENT\n\n#############################\n# Set simulation parameters #\n#############################\n\nTmax_restart2 = 1e-4\nNt_restart2 = 50\n\n# time resolution of restarted simulation\nTmax_restart = 1e-5 # simulation time in seconds\nNt_restart = 20     # number of time steps\n\nn_D = 1e20\nn_Z = 0.3e20\n\nB0 = 5.3            # magnetic field strength in Tesla\nE_initial = 0.00032 # initial electric field in V/m\nE_wall = 0.0        # boundary electric field in V/m\nT_initial = 25e3    # initial temperature in eV\n\nTmax_init = 1e-11   # simulation time in seconds\nNt_init = 2         # number of time steps\nNr = 5              # number of radial grid points\nNp = 200            # number of momentum grid points\nNxi = 5             # number of pitch grid points\npMax = 2.0          # maximum momentum in m_e*c\ntimes  = [0]        # times at which parameters are given\nradius = [0, 2]     # span of the radial grid\nradius_wall = 2.15  # location of the wall \n\nT_selfconsistent    = True\nhotTailGrid_enabled = True\n# Set up radial grid\nds.radialgrid.setB0(B0)\nds.radialgrid.setMinorRadius(radius[-1])\nds.radialgrid.setWallRadius(radius_wall)\nds.radialgrid.setNr(Nr)\n# Set time stepper\nds.timestep.setTmax(Tmax_init)\nds.timestep.setNt(Nt_init)\n\n# Set ions\n#density_D = n_D*np.ones(len(radius))\n#density_Ne = n_Ne*np.ones(len(radius))\ndensity_D = n_D\ndensity_Z = n_Z\n\nds.eqsys.n_i.addIon(name='D', T=T_initial, Z=1, iontype=Ions.IONS_DYNAMIC_FULLY_IONIZED, n=density_D)\nds.eqsys.n_i.addIon(name='Ar', Z=18, iontype=Ions.IONS_DYNAMIC_NEUTRAL, n=density_Z)\n#ds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_PRESCRIBED_FULLY_IONIZED, n=1e20)\n#ds.eqsys.n_i.addIon(name='Ar', Z=18, iontype=Ions.IONS_PRESCRIBED_NEUTRAL, n=1e20)\n\n\n# Set E_field \nefield = E_initial*np.ones((len(times), len(radius)))\nds.eqsys.E_field.setPrescribedData(efield=efield, times=times, radius=radius)\n\ntemperature = T_initial * np.ones((len(times), len(radius)))\nds.eqsys.T_cold.setPrescribedData(temperature=temperature, times=times, radius=radius)\n\nif not hotTailGrid_enabled:\n    ds.hottailgrid.setEnabled(False)\nelse:\n    ds.hottailgrid.setNxi(Nxi)\n    ds.hottailgrid.setNp(Np)\n    ds.hottailgrid.setPmax(pMax)\n    nfree_initial, rn0 = ds.eqsys.n_i.getFreeElectronDensity()\n    ds.eqsys.f_hot.setBoundaryCondition(bc=FHot.BC_F_0)\n    ds.eqsys.f_hot.setInitialProfiles(rn0=rn0, n0=nfree_initial, rT0=0, T0=T_initial)\n    ds.eqsys.f_hot.setAdvectionInterpolationMethod(ad_int=FHot.AD_INTERP_TCDF,ad_jac=FHot.AD_INTERP_JACOBIAN_FULL) # TCDF, SMART, QUICK, \n    ds.eqsys.f_hot.enableIonJacobian(False)\n\n# Disable runaway grid\nds.runawaygrid.setEnabled(False)\n\n\n# Use the linear solver\n#ds.solver.setType(Solver.LINEAR_IMPLICIT)\n\n# Use the nonlinear solver\nds.solver.setType(Solver.NONLINEAR)\nds.solver.setTolerance(reltol=1e-4)\nds.solver.setMaxIterations(maxiter = 100)\nds.solver.setVerbose(False)\n\n\nds.other.include('fluid', 'scalar')\n\n\n# Save settings to HDF5 file\nds.save('init_settings.h5')\nruniface(ds, 'output_init.h5', quiet=False)\n\n\n###########\n# RESTART #\n###########\n\nds2 = DREAMSettings(ds)\n\nds2.fromOutput('output_init.h5')\n\nds2.eqsys.E_field.setType(Efield.TYPE_SELFCONSISTENT)\nds2.eqsys.E_field.setBoundaryCondition(bctype = Efield.BC_TYPE_PRESCRIBED, inverse_wall_time = 0, V_loop_wall_R0 = E_wall*2*np.pi)\nif T_selfconsistent:\n    ds2.eqsys.T_cold.setType(ttype=T_cold.TYPE_SELFCONSISTENT)\n    ds.eqsys.T_cold.setRecombinationRadiation(False)\n\n\nds2.timestep.setTmax(Tmax_restart)\nds2.timestep.setNt(Nt_restart)\n\nds2.save('restart_settings.h5')\nruniface(ds2, 'output_restart.h5', quiet=False)\n\n#############\n# RESTART 2 #\n#############\nds3 = DREAMSettings(ds2)\nds3.fromOutput('output_restart.h5')\n\nds3.timestep.setTmax(Tmax_restart2)\nds3.timestep.setNt(Nt_restart2)\n\nds3.save('second_restart_settings.h5')\nruniface(ds3, 'output.h5', quiet=False)\n",
  "import sys\n\nif \"pyodide\" in sys.modules:\n    # psutil doesn't work on pyodide--use fake data instead\n    from fakepsutil import cpu_count, cpu_percent\nelse:\n    from psutil import cpu_count, cpu_percent\n\nfrom math import ceil\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom shiny import App, Inputs, Outputs, Session, reactive, render, ui\n\n# The agg matplotlib backend seems to be a little more efficient than the default when\n# running on macOS, and also gives more consistent results across operating systems\nmatplotlib.use(\"agg\")\n\n# max number of samples to retain\nMAX_SAMPLES = 1000\n# secs between samples\nSAMPLE_PERIOD = 1\n\n\nncpu = cpu_count(logical=True)\n\napp_ui = ui.page_fluid(\n    ui.tags.style(\n        \"\"\"\n        /* Don't apply fade effect, it's constantly recalculating */\n        .recalculating {\n            opacity: 1;\n        }\n        tbody > tr:last-child {\n            /*border: 3px solid var(--bs-dark);*/\n            box-shadow:\n                0 0 2px 1px #fff, /* inner white */\n                0 0 4px 2px #0ff, /* middle cyan */\n                0 0 5px 3px #00f; /* outer blue */\n        }\n        #table table {\n            table-layout: fixed;\n            width: %s;\n            font-size: 0.8em;\n        }\n        th, td {\n            text-align: center;\n        }\n        \"\"\"\n        % f\"{ncpu*4}em\"\n    ),\n    ui.h3(\"CPU Usage %\", class_=\"mt-2\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"cmap\",\n                \"Colormap\",\n                {\n                    \"inferno\": \"inferno\",\n                    \"viridis\": \"viridis\",\n                    \"copper\": \"copper\",\n                    \"prism\": \"prism (not recommended)\",\n                },\n            ),\n            ui.p(ui.input_action_button(\"reset\", \"Clear history\", class_=\"btn-sm\")),\n            ui.input_switch(\"hold\", \"Freeze output\", value=False),\n            class_=\"mb-3\",\n        ),\n        ui.panel_main(\n            ui.div(\n                {\"class\": \"card mb-3\"},\n                ui.div(\n                    {\"class\": \"card-body\"},\n                    ui.h5({\"class\": \"card-title mt-0\"}, \"Graphs\"),\n                    ui.output_plot(\"plot\", height=f\"{ncpu * 40}px\"),\n                ),\n                ui.div(\n                    {\"class\": \"card-footer\"},\n                    ui.input_numeric(\"sample_count\", \"Number of samples per graph\", 50),\n                ),\n            ),\n            ui.div(\n                {\"class\": \"card\"},\n                ui.div(\n                    {\"class\": \"card-body\"},\n                    ui.h5({\"class\": \"card-title m-0\"}, \"Heatmap\"),\n                ),\n                ui.div(\n                    {\"class\": \"card-body overflow-auto pt-0\"},\n                    ui.output_table(\"table\"),\n                ),\n                ui.div(\n                    {\"class\": \"card-footer\"},\n                    ui.input_numeric(\"table_rows\", \"Rows to display\", 5),\n                ),\n            ),\n        ),\n    ),\n)\n\n\n@reactive.Calc\ndef cpu_current():\n    reactive.invalidate_later(SAMPLE_PERIOD)\n    return cpu_percent(percpu=True)\n\n\ndef server(input: Inputs, output: Outputs, session: Session):\n    cpu_history = reactive.Value(None)\n\n    @reactive.Calc\n    def cpu_history_with_hold():\n        # If \"hold\" is on, grab an isolated snapshot of cpu_history; if not, then do a\n        # regular read\n        if not input.hold():\n            return cpu_history()\n        else:\n            # Even if frozen, we still want to respond to input.reset()\n            input.reset()\n            with reactive.isolate():\n                return cpu_history()\n\n    @reactive.Effect\n    def collect_cpu_samples():\n        \"\"\"cpu_percent() reports just the current CPU usage sample; this Effect gathers\n        them up and stores them in the cpu_history reactive value, in a numpy 2D array\n        (rows are CPUs, columns are time).\"\"\"\n\n        new_data = np.vstack(cpu_current())\n        with reactive.isolate():\n            if cpu_history() is None:\n                cpu_history.set(new_data)\n            else:\n                combined_data = np.hstack([cpu_history(), new_data])\n                # Throw away extra data so we don't consume unbounded amounts of memory\n                if combined_data.shape[1] > MAX_SAMPLES:\n                    combined_data = combined_data[:, -MAX_SAMPLES:]\n                cpu_history.set(combined_data)\n\n    @reactive.Effect(priority=100)\n    @reactive.event(input.reset)\n    def reset_history():\n        cpu_history.set(None)\n\n    @output\n    @render.plot\n    def plot():\n        history = cpu_history_with_hold()\n\n        if history is None:\n            history = np.array([])\n            history.shape = (ncpu, 0)\n\n        nsamples = input.sample_count()\n\n        # Throw away samples too old to fit on the plot\n        if history.shape[1] > nsamples:\n            history = history[:, -nsamples:]\n\n        ncols = 2\n        nrows = int(ceil(ncpu / ncols))\n        fig, axeses = plt.subplots(\n            nrows=nrows,\n            ncols=ncols,\n            squeeze=False,\n        )\n        for i in range(0, ncols * nrows):\n            row = i // ncols\n            col = i % ncols\n            axes = axeses[row, col]\n            if i >= len(history):\n                axes.set_visible(False)\n                continue\n            data = history[i]\n            axes.yaxis.set_label_position(\"right\")\n            axes.yaxis.tick_right()\n            axes.set_xlim(-(nsamples - 1), 0)\n            axes.set_ylim(0, 100)\n\n            assert len(data) <= nsamples\n\n            # Set up an array of x-values that will right-align the data relative to the\n            # plotting area\n            x = np.arange(0, len(data))\n            x = np.flip(-x)\n\n            # Color bars by cmap\n            color = plt.get_cmap(input.cmap())(data / 100)\n            axes.bar(x, data, color=color, linewidth=0, width=1.0)\n\n            axes.set_yticks([25, 50, 75])\n            for ytl in axes.get_yticklabels():\n                if col == ncols - 1 or i == ncpu - 1 or True:\n                    ytl.set_fontsize(7)\n                else:\n                    ytl.set_visible(False)\n                    hide_ticks(axes.yaxis)\n            for xtl in axes.get_xticklabels():\n                xtl.set_visible(False)\n            hide_ticks(axes.xaxis)\n            axes.grid(True, linewidth=0.25)\n\n        return fig\n\n    @output\n    @render.table\n    def table():\n        history = cpu_history_with_hold()\n        latest = pd.DataFrame(history).transpose().tail(input.table_rows())\n        if latest.shape[0] == 0:\n            return latest\n        return (\n            latest.style.format(precision=0)\n            .hide(axis=\"index\")\n            .set_table_attributes(\n                'class=\"dataframe shiny-table table table-borderless font-monospace\"'\n            )\n            .background_gradient(cmap=input.cmap(), vmin=0, vmax=100)\n        )\n\n\ndef hide_ticks(axis):\n    for ticks in [axis.get_major_ticks(), axis.get_minor_ticks()]:\n        for tick in ticks:\n            tick.tick1line.set_visible(False)\n            tick.tick2line.set_visible(False)\n            tick.label1.set_visible(False)\n            tick.label2.set_visible(False)\n\n\napp = App(app_ui, server)\n",
  "\"\"\"\nHFSS: component antenna array\n-----------------------------\nThis example shows how you can use PyAEDT to create an example using a 3D component file. It sets up\nthe analysis, solves it, and uses postprocessing functions to create plots using Matplotlib and\nPyVista without opening the HFSS user interface. This examples runs only on Windows using CPython.\n\"\"\"\n##########################################################\n# Perform required imports\n# ~~~~~~~~~~~~~~~~~~~~~~~~\n# Perform required imports.\n\nimport os\nimport pyaedt\n\n##########################################################\n# Set non-graphical mode\n# ~~~~~~~~~~~~~~~~~~~~~~\n# Set non-graphical mode. \n# You can set ``non_graphical`` either to ``True`` or ``False``.\n\nnon_graphical = False\n\n##########################################################\n# Download 3D component\n# ~~~~~~~~~~~~~~~~~~~~~\n# Download the 3D component that is needed to run the example.\nexample_path = pyaedt.downloads.download_3dcomponent()\n\n##########################################################\n# Launch HFSS and save project\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Launch HFSS and save the project.\nproject_name = pyaedt.generate_unique_project_name(project_name=\"array\")\nhfss = pyaedt.Hfss(projectname=project_name,\n                   specified_version=\"2023.2\",\n                   designname=\"Array_Simple\",\n                   non_graphical=non_graphical,\n                   new_desktop_session=True)\n\nprint(\"Project name \" + project_name)\n\n##########################################################\n# Read array definition from JSON file\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Read the array definition from a JSON file. A JSON file\n# can contain all information needed to import and set up a\n# full array in HFSS.\n# \n# If a 3D component is not available in the design, it is loaded\n# into the dictionary from the path that you specify. The following\n# code edits the dictionary to point to the location of the A3DCOMP file.\n\ndict_in = pyaedt.data_handler.json_to_dict(os.path.join(example_path, \"array_simple.json\"))\ndict_in[\"Circ_Patch_5GHz1\"] = os.path.join(example_path, \"Circ_Patch_5GHz.a3dcomp\")\ndict_in[\"cells\"][(3, 3)] = {\"name\": \"Circ_Patch_5GHz1\"}\nhfss.add_3d_component_array_from_json(dict_in)\n\n##########################################################\n# Set up simulation\n# ~~~~~~~~~~~~~~~~~\n# Set up a simulation and analyze it.\n\nsetup = hfss.create_setup()\nsetup.props[\"Frequency\"] = \"5GHz\"\nsetup.props[\"MaximumPasses\"] = 3\n\nhfss.analyze(num_cores=4)\n\n##########################################################\n# Get far field data\n# ~~~~~~~~~~~~~~~~~~\n# Get far field data. After the simulation completes, the far\n# field data is generated port by port and stored in a data class.\n\nffdata = hfss.get_antenna_ffd_solution_data(sphere_name=\"Infinite Sphere1\", setup_name=hfss.nominal_adaptive,\n                                            frequencies=[5e9])\n\n##########################################################\n# Generate contour plot\n# ~~~~~~~~~~~~~~~~~~~~~\n# Generate a contour plot. You can define the Theta scan\n# and Phi scan.\n\nffdata.plot_farfield_contour(qty_str='RealizedGain', convert_to_db=True,\n                             title='Contour at {}Hz'.format(ffdata.frequency))\n\n##########################################################\n# Generate 2D cutout plots\n# ~~~~~~~~~~~~~~~~~~~~~~~~\n# Generate 2D cutout plots. You can define the Theta scan\n# and Phi scan.\n\nffdata.plot_2d_cut(primary_sweep='theta', secondary_sweep_value=[-180, -75, 75],\n                   qty_str='RealizedGain',\n                   title='Azimuth at {}Hz'.format(ffdata.frequency),\n                   convert_to_db=True)\n\nffdata.plot_2d_cut(primary_sweep=\"phi\", secondary_sweep_value=30,\n                   qty_str='RealizedGain',\n                   title='Elevation',\n                   convert_to_db=True)\n\n##########################################################\n# Generate 3D polar plots in Matplotlib\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Generate 3D polar plots in Matplotlib. You can define\n# the Theta scan and Phi scan.\n\nffdata.polar_plot_3d(qty_str='RealizedGain',\n                     convert_to_db=True)\n\n##########################################################\n# Generate 3D plots in PyVista\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Generate 3D plots in PyVista. You can define the Theta and Phi\n# scan angles. You can change the easy-to-use interactive plot\n# that is generated on the fly. \n\nffdata.polar_plot_3d_pyvista(qty_str='RealizedGain',\n                             convert_to_db=True,\n                             export_image_path=os.path.join(hfss.working_directory, \"picture.jpg\"),\n                             show=False)\n\n##########################################################\n# Release AEDT\n# ~~~~~~~~~~~~\n# Release AEDT.\n\nhfss.release_desktop()\n",
  "from sc2 import maps\nfrom sc2.bot_ai import BotAI\nfrom sc2.data import Difficulty, Race\nfrom sc2.ids.ability_id import AbilityId\nfrom sc2.ids.buff_id import BuffId\nfrom sc2.ids.unit_typeid import UnitTypeId\nfrom sc2.main import run_game\nfrom sc2.player import Bot, Computer\n\n\nclass ThreebaseVoidrayBot(BotAI):\n\n    # pylint: disable=R0912\n    async def on_step(self, iteration):\n        target_base_count = 3\n        target_stargate_count = 3\n\n        if iteration == 0:\n            await self.chat_send(\"(glhf)\")\n\n        if not self.townhalls.ready:\n            # Attack with all workers if we don't have any nexuses left, attack-move on enemy spawn (doesn't work on 4 player map) so that probes auto attack on the way\n            for worker in self.workers:\n                worker.attack(self.enemy_start_locations[0])\n            return\n\n        nexus = self.townhalls.ready.random\n\n        # If this random nexus is not idle and has not chrono buff, chrono it with one of the nexuses we have\n        if not nexus.is_idle and not nexus.has_buff(BuffId.CHRONOBOOSTENERGYCOST):\n            nexuses = self.structures(UnitTypeId.NEXUS)\n            abilities = await self.get_available_abilities(nexuses)\n            for loop_nexus, abilities_nexus in zip(nexuses, abilities):\n                if AbilityId.EFFECT_CHRONOBOOSTENERGYCOST in abilities_nexus:\n                    loop_nexus(AbilityId.EFFECT_CHRONOBOOSTENERGYCOST, nexus)\n                    break\n\n        # If we have at least 5 void rays, attack closes enemy unit/building, or if none is visible: attack move towards enemy spawn\n        if self.units(UnitTypeId.VOIDRAY).amount > 5:\n            for vr in self.units(UnitTypeId.VOIDRAY):\n                # Activate charge ability if the void ray just attacked\n                if vr.weapon_cooldown > 0:\n                    vr(AbilityId.EFFECT_VOIDRAYPRISMATICALIGNMENT)\n                # Choose target and attack, filter out invisible targets\n                targets = (self.enemy_units | self.enemy_structures).filter(lambda unit: unit.can_be_attacked)\n                if targets:\n                    target = targets.closest_to(vr)\n                    vr.attack(target)\n                else:\n                    vr.attack(self.enemy_start_locations[0])\n\n        # Distribute workers in gas and across bases\n        await self.distribute_workers()\n\n        # If we are low on supply, build pylon\n        if (\n            self.supply_left < 2 and self.already_pending(UnitTypeId.PYLON) == 0\n            or self.supply_used > 15 and self.supply_left < 4 and self.already_pending(UnitTypeId.PYLON) < 2\n        ):\n            # Always check if you can afford something before you build it\n            if self.can_afford(UnitTypeId.PYLON):\n                await self.build(UnitTypeId.PYLON, near=nexus)\n\n        # Train probe on nexuses that are undersaturated (avoiding distribute workers functions)\n        # if nexus.assigned_harvesters < nexus.ideal_harvesters and nexus.is_idle:\n        if self.supply_workers + self.already_pending(UnitTypeId.PROBE) < self.townhalls.amount * 22 and nexus.is_idle:\n            if self.can_afford(UnitTypeId.PROBE):\n                nexus.train(UnitTypeId.PROBE)\n\n        # If we have less than 3 nexuses and none pending yet, expand\n        if self.townhalls.ready.amount + self.already_pending(UnitTypeId.NEXUS) < 3:\n            if self.can_afford(UnitTypeId.NEXUS):\n                await self.expand_now()\n\n        # Once we have a pylon completed\n        if self.structures(UnitTypeId.PYLON).ready:\n            pylon = self.structures(UnitTypeId.PYLON).ready.random\n            if self.structures(UnitTypeId.GATEWAY).ready:\n                # If we have gateway completed, build cyber core\n                if not self.structures(UnitTypeId.CYBERNETICSCORE):\n                    if (\n                        self.can_afford(UnitTypeId.CYBERNETICSCORE)\n                        and self.already_pending(UnitTypeId.CYBERNETICSCORE) == 0\n                    ):\n                        await self.build(UnitTypeId.CYBERNETICSCORE, near=pylon)\n            else:\n                # If we have no gateway, build gateway\n                if self.can_afford(UnitTypeId.GATEWAY) and self.already_pending(UnitTypeId.GATEWAY) == 0:\n                    await self.build(UnitTypeId.GATEWAY, near=pylon)\n\n        # Build gas near completed nexuses once we have a cybercore (does not need to be completed\n        if self.structures(UnitTypeId.CYBERNETICSCORE):\n            for nexus in self.townhalls.ready:\n                vgs = self.vespene_geyser.closer_than(15, nexus)\n                for vg in vgs:\n                    if not self.can_afford(UnitTypeId.ASSIMILATOR):\n                        break\n\n                    worker = self.select_build_worker(vg.position)\n                    if worker is None:\n                        break\n\n                    if not self.gas_buildings or not self.gas_buildings.closer_than(1, vg):\n                        worker.build_gas(vg)\n                        worker.stop(queue=True)\n\n        # If we have less than 3  but at least 3 nexuses, build stargate\n        if self.structures(UnitTypeId.PYLON).ready and self.structures(UnitTypeId.CYBERNETICSCORE).ready:\n            pylon = self.structures(UnitTypeId.PYLON).ready.random\n            if (\n                self.townhalls.ready.amount + self.already_pending(UnitTypeId.NEXUS) >= target_base_count\n                and self.structures(UnitTypeId.STARGATE).ready.amount + self.already_pending(UnitTypeId.STARGATE) <\n                target_stargate_count\n            ):\n                if self.can_afford(UnitTypeId.STARGATE):\n                    await self.build(UnitTypeId.STARGATE, near=pylon)\n\n        # Save up for expansions, loop over idle completed stargates and queue void ray if we can afford\n        if self.townhalls.amount >= 3:\n            for sg in self.structures(UnitTypeId.STARGATE).ready.idle:\n                if self.can_afford(UnitTypeId.VOIDRAY):\n                    sg.train(UnitTypeId.VOIDRAY)\n\n\ndef main():\n    run_game(\n        maps.get(\"(2)CatalystLE\"),\n        [Bot(Race.Protoss, ThreebaseVoidrayBot()),\n         Computer(Race.Protoss, Difficulty.Easy)],\n        realtime=False,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "\"\"\"\nHFSS 3D Layout: SIwave DCIR analysis in HFSS 3D Layout\n------------------------------------------------------\nThis example shows how you can use configure HFSS 3D Layout for SIwave DCIR\nanalysis.\n\"\"\"\n\nimport os\nimport tempfile\nimport pyaedt\n\n###############################################################################\n# Configure EDB for DCIR analysis\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Copy example into temporary folder\ntemp_dir = tempfile.gettempdir()\ndst_dir = os.path.join(temp_dir, pyaedt.generate_unique_name(\"pyaedt_dcir\"))\nos.mkdir(dst_dir)\nlocal_path = pyaedt.downloads.download_aedb(dst_dir)\n\n#####################################################################################\n# Load example board into EDB\n\nedbversion = \"2023.2\"\nappedb = pyaedt.Edb(local_path, edbversion=edbversion)\n\n#####################################################################################\n# Create pin group on VRM positive pins\n\ngnd_name = \"GND\"\nappedb.siwave.create_pin_group_on_net(\n    reference_designator=\"U3A1\",\n    net_name=\"BST_V3P3_S5\",\n    group_name=\"U3A1-BST_V3P3_S5\")\n\n#####################################################################################\n# Create pin group on VRM negative pins\n\nappedb.siwave.create_pin_group_on_net(\n    reference_designator=\"U3A1\",\n    net_name=\"GND\",\n    group_name=\"U3A1-GND\")\n\n#####################################################################################\n# Create voltage source between VRM positive and negative pin groups\nappedb.siwave.create_voltage_source_on_pin_group(\n    pos_pin_group_name=\"U3A1-BST_V3P3_S5\",\n    neg_pin_group_name=\"U3A1-GND\",\n    magnitude=3.3,\n    name=\"U3A1-BST_V3P3_S5\"\n)\n\n#####################################################################################\n# Create pin group on sink component positive pins\n\nappedb.siwave.create_pin_group_on_net(\n    reference_designator=\"U2A5\",\n    net_name=\"V3P3_S5\",\n    group_name=\"U2A5-V3P3_S5\")\n\n#####################################################################################\n# Create pin group on sink component negative pins\n\nappedb.siwave.create_pin_group_on_net(\n    reference_designator=\"U2A5\",\n    net_name=\"GND\",\n    group_name=\"U2A5-GND\")\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Create place current source between sink component positive and negative pin groups\nappedb.siwave.create_current_source_on_pin_group(\n    pos_pin_group_name=\"U2A5-V3P3_S5\",\n    neg_pin_group_name=\"U2A5-GND\",\n    magnitude=1,\n    name=\"U2A5-V3P3_S5\"\n)\n\n###############################################################################\n# Add SIwave DCIR analysis\n\nappedb.siwave.add_siwave_dc_analysis(name=\"my_setup\")\n\n###############################################################################\n# Save and close EDB\n# ~~~~~~~~~~~~~~~~~~\n# Save and close EDB.\n\nappedb.save_edb()\nappedb.close_edb()\n\n###############################################################################\n# Analysis DCIR in AEDT\n# ~~~~~~~~~~~~~~~~~~~~~\n# Launch AEDT and import the configured EDB and analysis DCIR\ndesktop = pyaedt.Desktop(edbversion, non_graphical=False, new_desktop_session=True)\nhfss3dl = pyaedt.Hfss3dLayout(local_path)\nhfss3dl.analyze()\nhfss3dl.save_project()\n\n###############################################################################\n# Get element data\n# ~~~~~~~~~~~~~~~~~~~\n# Get loop resistance\n\nloop_resistance = hfss3dl.get_dcir_element_data_loop_resistance(setup_name=\"my_setup\")\nprint(loop_resistance)\n\n# ~~~~~~~~~~~~~~~~~~~\n# Get current source\n\ncurrent_source = hfss3dl.get_dcir_element_data_current_source(setup_name=\"my_setup\")\nprint(current_source)\n\n# ~~~~~~~~~~~~~~~~~~~\n# Get via information\n\nvia = hfss3dl.get_dcir_element_data_via(setup_name=\"my_setup\")\nprint(via)\n\n\n###############################################################################\n# Get voltage\n# ~~~~~~~~~~~\n# Get voltage from dcir solution data\nvoltage = hfss3dl.get_dcir_solution_data(\n    setup_name=\"my_setup\",\n    show=\"Sources\",\n    category=\"Voltage\")\nprint({expression: voltage.data_magnitude(expression) for  expression in voltage.expressions})\n\n###############################################################################\n# Close AEDT\n# ~~~~~~~~~~\nhfss3dl.close_project()\ndesktop.release_desktop()\n",
  "\"\"\"\nSimple example that show how to use Avalanche for Question Answering on \nSquad by using T5\n\"\"\"\nfrom avalanche.benchmarks.utils import DataAttribute, ConstantSequence\nfrom avalanche.training.plugins import ReplayPlugin\nfrom transformers import DataCollatorForSeq2Seq\nimport torch\nimport avalanche\nimport torch.nn\nfrom avalanche.benchmarks import CLScenario, CLStream, CLExperience\nimport avalanche.training.templates.base\nfrom avalanche.benchmarks.utils import AvalancheDataset\nfrom transformers import AutoTokenizer\nfrom transformers import T5ForConditionalGeneration\nfrom datasets import load_dataset\nimport numpy as np\n\n\nclass HGNaive(avalanche.training.Naive):\n    \"\"\"There are only a couple of modifications needed to\n    use huggingface:\n    - we add a bunch of attributes corresponding to the batch items,\n        redefining mb_x and mb_y too\n    - _unpack_minibatch sends the dictionary values to the GPU device\n    - forward and criterion are adapted for machine translation tasks.\n    \"\"\"\n\n    @property\n    def mb_attention_mask(self):\n        return self.mbatch[\"attention_mask\"]\n\n    @property\n    def mb_x(self):\n        \"\"\"Current mini-batch input.\"\"\"\n        return self.mbatch[\"input_ids\"]\n\n    @property\n    def mb_y(self):\n        \"\"\"Current mini-batch target.\"\"\"\n        return self.mbatch[\"labels\"]\n\n    @property\n    def mb_decoder_in_ids(self):\n        \"\"\"Current mini-batch target.\"\"\"\n        return self.mbatch[\"decoder_input_ids\"]\n\n    @property\n    def mb_token_type_ids(self):\n        return self.mbatch[3]\n\n    def _unpack_minibatch(self):\n        \"\"\"HuggingFace minibatches are dictionaries of tensors.\n        Move tensors to the current device.\"\"\"\n        for k in self.mbatch.keys():\n            self.mbatch[k] = self.mbatch[k].to(self.device)\n\n    def forward(self):\n        out = self.model(\n            input_ids=self.mb_x,\n            attention_mask=self.mb_attention_mask,\n            labels=self.mb_y,\n        )\n        return out.logits\n\n    def criterion(self):\n        mb_output = self.mb_output.view(-1, self.mb_output.size(-1))\n        ll = self._criterion(mb_output, self.mb_y.view(-1))\n        return ll\n\n\ndef main():\n    # Load SQuAD datasets from HuggingFace\n    squad_tr = load_dataset(\"squad\", split=\"train\")\n    squad_val = load_dataset(\"squad\", split=\"validation\")\n\n    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n    encoder_max_len = tokenizer.model_max_length\n    decoder_max_len = 60\n\n    \"\"\"\n    Define a preprocessing function (code from HuggingFace) to:\n    1. Convert squad dataset to be used in a text 2 text setting\n    2. Encode the question and context with the tokenizer of T5 model\n    \"\"\"\n\n    def t2t_converter(example):\n        example[\"input_text\"] = f\"question: {example['question']}\"\n        +f\"context: {example['context']} </s>\"\n        example[\"target_text\"] = f\"{example['answers']['text'][0]} </s>\"\n        return example\n\n    def preprocess_function(\n        examples, encoder_max_len=encoder_max_len, decoder_max_len=decoder_max_len\n    ):\n        encoder_inputs = tokenizer(\n            examples[\"input_text\"],\n            truncation=True,\n            return_tensors=\"pt\",\n            max_length=encoder_max_len,\n            pad_to_max_length=True,\n        )\n\n        decoder_inputs = tokenizer(\n            examples[\"target_text\"],\n            truncation=True,\n            return_tensors=\"pt\",\n            max_length=decoder_max_len,\n            pad_to_max_length=True,\n        )\n\n        input_ids = encoder_inputs[\"input_ids\"]\n        input_attention = encoder_inputs[\"attention_mask\"]\n        target_ids = decoder_inputs[\"input_ids\"]\n        target_attention = decoder_inputs[\"attention_mask\"]\n\n        outputs = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": input_attention,\n            \"labels\": target_ids,\n            \"decoder_attention_mask\": target_attention,\n        }\n        return outputs\n\n    # Map the preprocessing function to the dataset so that it's applied to\n    # all examples\n    squad_tr = squad_tr.map(t2t_converter)\n    squad_tr = squad_tr.map(preprocess_function, batched=True)\n    squad_tr = squad_tr.remove_columns(\n        [\"id\", \"title\", \"context\", \"question\", \"answers\", \"input_text\", \"target_text\"]\n    )\n    squad_val = squad_val.map(t2t_converter)\n    squad_val = squad_val.map(preprocess_function, batched=True)\n    # ,' input_text', 'target_text'])\n    squad_val = squad_val.remove_columns(\n        [\"id\", \"title\", \"context\", \"question\", \"answers\"]\n    )\n\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n    # Use a standard data collator for QA\n    data_collator = DataCollatorForSeq2Seq(tokenizer)\n\n    train_exps = []\n    for i in range(0, 2):\n        # We use very small experiences only to showcase the library.\n        # Adapt this to your own benchmark\n        exp_data = squad_tr.select(range(30 * i, 30 * (i + 1)))\n        tl = DataAttribute(ConstantSequence(i, len(exp_data)), \"targets_task_labels\")\n\n        exp = CLExperience()\n        exp.dataset = AvalancheDataset(\n            [exp_data], data_attributes=[tl], collate_fn=data_collator\n        )\n        train_exps.append(exp)\n    tl = DataAttribute(ConstantSequence(2, len(squad_val)), \"targets_task_labels\")\n    val_exp = CLExperience()\n    val_exp.dataset = AvalancheDataset(\n        [squad_val], data_attributes=[tl], collate_fn=data_collator\n    )\n    val_exp = [val_exp]\n\n    benchmark = CLScenario(\n        [\n            CLStream(\"train\", train_exps),\n            CLStream(\"valid\", val_exp)\n            # add more stream here (validation, test, out-of-domain, ...)\n        ]\n    )\n    eval_plugin = avalanche.training.plugins.EvaluationPlugin(\n        avalanche.evaluation.metrics.loss_metrics(\n            epoch=True, experience=True, stream=True\n        ),\n        loggers=[avalanche.logging.InteractiveLogger()],\n        strict_checks=False,\n    )\n    plugins = [ReplayPlugin(mem_size=200)]\n    optimizer = torch.optim.Adam(model.parameters(), lr=2)\n    strategy = HGNaive(\n        model,\n        optimizer,\n        torch.nn.CrossEntropyLoss(ignore_index=-100),\n        evaluator=eval_plugin,\n        train_epochs=1,\n        train_mb_size=10,\n        plugins=plugins,\n    )\n    for experience in benchmark.train_stream:\n        strategy.train(experience, collate_fn=data_collator)\n        strategy.eval(benchmark.train_stream)\n\n    # Test the model:\n    model.eval()\n    question = \"Which libraries is Avalanche based upon?\"\n    context = \"\"\"\n    Avalanche is an End-to-End Continual Learning Library \n    based on PyTorch, born within ContinualAI with the goal of providing \n    a shared and collaborative open-source (MIT licensed) codebase for fast\n    prototyping, training and reproducible evaluation of continual learning\n    algorithms.\"\n    \"\"\"\n\n    input_text = f\"answer_me: {question} context: {context} </s>\"\n    encoded_query = tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        pad_to_max_length=True,\n        truncation=True,\n        max_length=250,\n    )\n    input_ids = encoded_query[\"input_ids\"]\n    attention_mask = encoded_query[\"attention_mask\"]\n    generated_answer = model.generate(\n        input_ids,\n        attention_mask=attention_mask,\n        max_length=50,\n        top_p=0.95,\n        top_k=50,\n        repetition_penalty=2.0,\n    )\n\n    decoded_answer = tokenizer.batch_decode(generated_answer, skip_special_tokens=True)\n    print(f\"Answer: {decoded_answer}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "import numpy as np\nimport scipy.sparse as sps\nimport os\nimport sys\n\nfrom porepy.viz import exporter\nfrom porepy.fracs import importer\n\nfrom porepy.params import tensor\nfrom porepy.params.bc import BoundaryCondition\nfrom porepy.params.data import Parameters\n\nfrom porepy.grids import coarsening as co\n\nfrom porepy.numerics.vem import dual\nfrom porepy.numerics.fv.transport import upwind\nfrom porepy.numerics.fv import tpfa, mass_matrix\n\n#------------------------------------------------------------------------------#\n\n\ndef add_data_darcy(gb, domain, tol):\n    gb.add_node_props(['param', 'is_tangent'])\n\n    apert = 1e-2\n\n    km = 7.5 * 1e-11\n    kf_t = 1e-5 * km\n    kf_n = 1e-5 * km\n\n    for g, d in gb:\n        param = Parameters(g)\n\n        rock = g.dim == gb.dim_max()\n        kxx = km if rock else kf_t\n        d['is_tangential'] = True\n        perm = tensor.SecondOrder(g.dim, kxx * np.ones(g.num_cells))\n        param.set_tensor(\"flow\", perm)\n\n        param.set_source(\"flow\", np.zeros(g.num_cells))\n\n        param.set_aperture(np.power(apert, gb.dim_max() - g.dim))\n\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            top = bound_face_centers[1, :] > domain['ymax'] - tol\n            bottom = bound_face_centers[1, :] < domain['ymin'] + tol\n            left = bound_face_centers[0, :] < domain['xmin'] + tol\n            right = bound_face_centers[0, :] > domain['xmax'] - tol\n            boundary = np.logical_or(left, right)\n\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[boundary] = ['dir']\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[left]] = 30 * 1e6\n\n            param.set_bc(\"flow\", BoundaryCondition(g, bound_faces, labels))\n            param.set_bc_val(\"flow\", bc_val)\n        else:\n            param.set_bc(\"flow\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n\n        d['param'] = param\n\n    # Assign coupling permeability\n    gb.add_edge_prop('kn')\n    for e, d in gb.edges_props():\n        g = gb.sorted_nodes_of_edge(e)[0]\n        d['kn'] = kf_n / gb.node_prop(g, 'param').get_aperture()\n\n#------------------------------------------------------------------------------#\n\n\ndef add_data_advection(gb, domain, tol):\n\n    # Porosity\n    phi_m = 1e-1\n    phi_f = 9 * 1e-1\n\n    # Density\n    rho_w = 1e3  # kg m^{-3}\n    rho_s = 2 * 1e3  # kg m^{-3}\n\n    # heat capacity\n    c_w = 4 * 1e3  # J kg^{-1} K^{-1}\n    c_s = 8 * 1e2  # J kg^{-1} K^{-1}\n\n    c_m = phi_m * rho_w * c_w + (1 - phi_m) * rho_s * c_s\n    c_f = phi_f * rho_w * c_w + (1 - phi_f) * rho_s * c_s\n\n    for g, d in gb:\n        param = d['param']\n\n        rock = g.dim == gb.dim_max()\n        source = np.zeros(g.num_cells)\n        param.set_source(\"transport\", source)\n\n        param.set_porosity(1)\n        param.set_discharge(d['discharge'])\n\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            top = bound_face_centers[1, :] > domain['ymax'] - tol\n            bottom = bound_face_centers[1, :] < domain['ymin'] + tol\n            left = bound_face_centers[0, :] < domain['xmin'] + tol\n            right = bound_face_centers[0, :] > domain['xmax'] - tol\n            boundary = np.logical_or(left, right)\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[boundary] = ['dir']\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[left]] = 1\n\n            param.set_bc(\"transport\", BoundaryCondition(\n                g, bound_faces, labels))\n            param.set_bc_val(\"transport\", bc_val)\n        else:\n            param.set_bc(\"transport\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n        d['param'] = param\n\n    # Assign coupling discharge\n    gb.add_edge_prop('param')\n    for e, d in gb.edges_props():\n        g = gb.sorted_nodes_of_edge(e)[1]\n        discharge = gb.node_prop(g, 'param').get_discharge()\n        d['param'] = Parameters(g)\n        d['param'].set_discharge(discharge)\n\n#------------------------------------------------------------------------------#\n#------------------------------------------------------------------------------#\n\n\ntol = 1e-4\nexport_folder = 'example_5_2_2'\n\nT = 40 * np.pi * 1e7\nNt = 20  # 10 20 40 80 160 320 640 1280 2560 5120 - 100000\ndeltaT = T / Nt\nexport_every = 1\nif_coarse = True\n\nmesh_kwargs = {}\nmesh_kwargs['mesh_size'] = {'mode': 'weighted',\n                            'value': 500,\n                            'bound_value': 500,\n                            'tol': tol}\n\ndomain = {'xmin': 0, 'xmax': 700, 'ymin': 0, 'ymax': 600}\ngb = importer.from_csv('network.csv', mesh_kwargs, domain)\ngb.compute_geometry()\nif if_coarse:\n    co.coarsen(gb, 'by_volume')\ngb.assign_node_ordering()\n\n# Choose and define the solvers and coupler\ndarcy = dual.DualVEMMixDim(\"flow\")\n\n# Assign parameters\nadd_data_darcy(gb, domain, tol)\n\nA, b = darcy.matrix_rhs(gb)\n\nup = sps.linalg.spsolve(A, b)\ndarcy.split(gb, \"up\", up)\n\ngb.add_node_props(['pressure', \"P0u\", \"discharge\"])\ndarcy.extract_u(gb, \"up\", \"discharge\")\ndarcy.extract_p(gb, \"up\", 'pressure')\ndarcy.project_u(gb, \"discharge\", \"P0u\")\n\n# compute the flow rate\ntotal_flow_rate = 0\nfor g, d in gb:\n    bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n    if bound_faces.size != 0:\n        bound_face_centers = g.face_centers[:, bound_faces]\n        left = bound_face_centers[0, :] < domain['xmin'] + tol\n        flow_rate = d['discharge'][bound_faces[left]]\n        total_flow_rate += np.sum(flow_rate)\n\nexporter.export_vtk(gb, 'darcy', ['pressure', \"P0u\"], folder=export_folder,\n                    binary=False)\n\n#################################################################\n\n\nphysics = 'transport'\nadvection = upwind.UpwindMixDim(physics)\nmass = mass_matrix.MassMatrixMixDim(physics)\ninvMass = mass_matrix.InvMassMatrixMixDim(physics)\n\n# Assign parameters\nadd_data_advection(gb, domain, tol)\n\ngb.add_node_prop('deltaT', prop=deltaT)\n\nU, rhs_u = advection.matrix_rhs(gb)\nM, _ = mass.matrix_rhs(gb)\nOF = advection.outflow(gb)\nM_U = M + U\n\nrhs = rhs_u\n\n# Perform an LU factorization to speedup the solver\nIE_solver = sps.linalg.factorized((M_U).tocsc())\n\ntheta = np.zeros(rhs.shape[0])\n\n# Loop over the time\ntime = np.empty(Nt)\nfile_name = \"theta\"\ni_export = 0\nstep_to_export = np.empty(0)\n\nproduction = np.zeros(Nt)\n\nfor i in np.arange(Nt):\n    print(\"Time step\", i, \" of \", Nt, \" time \", i * deltaT, \" deltaT \", deltaT)\n    # Update the solution\n    production[i] = np.sum(OF.dot(theta)) / total_flow_rate\n    theta = IE_solver(M.dot(theta) + rhs)\n\n    if i % export_every == 0:\n        print(\"Export solution at\", i)\n        advection.split(gb, \"theta\", theta)\n\n        exporter.export_vtk(gb, file_name, [\"theta\"], time_step=i_export,\n                            binary=False, folder=export_folder)\n        step_to_export = np.r_[step_to_export, i]\n        i_export += 1\n\nexporter.export_pvd(gb, file_name, step_to_export *\n                    deltaT, folder=export_folder)\n\ntimes = deltaT * np.arange(Nt)\nnp.savetxt(export_folder + '/production.txt', (times, np.abs(production)),\n           delimiter=',')\n",
  "'''\nScatter estimation demo.\n\nNOTE: Must be used after running acquisition_sensitivity_from_attenuation.py\n(to create attenuation correction factors file) and randoms_from_listmode.py\n(to create raw data sinograms file and randoms sinograms file).\n\nUsage:\n  scatter_estimation [--help | options]\n\nOptions: (defaults are set to work for mMR data processed in the current directory)\n  -f <file>, --file=<file>    raw data file [default: sinograms_f1g1d0b0.hs]\n  -r <file>, --randoms=<file>  filename with randoms [default: randoms.hs]\n  -p <path>, --path=<path>    path to normalization and attenuation files,\n                              defaults to data/examples/PET/mMR\n  -n <norm>, --norm=<norm>    normalization file [default: norm.n.hdr]\n  -a <file>, --attenuation_image=<file>\n                              attenuation image file [default: mu_map.hv]\n  -A <file>, --attenuation_correction_factors=<file>\n                              attenuation correction factors file [default: acf.hs]\n  -o <file>, --output=<file>  output prefix for scatter estimates [default: scatter_estimate]\n                              (\"_#.hs\" will be appended, with # the iteration number).\n                              Set this to an empty string to prevent output on disk.\n  --non-interactive           do not show plots\n'''\n\n## CCP SyneRBI Synergistic Image Reconstruction Framework (SIRF)\n## Copyright 2019 University of Hull\n## Copyright 2020-2021 University College London\n##\n## This is software developed for the Collaborative Computational\n## Project in Synergistic Reconstruction for Biomedical Imaging (formerly CCP PETMR)\n## (http://www.ccpsynerbi.ac.uk/).\n##\n## Licensed under the Apache License, Version 2.0 (the \"License\");\n##   you may not use this file except in compliance with the License.\n##   You may obtain a copy of the License at\n##       http://www.apache.org/licenses/LICENSE-2.0\n##   Unless required by applicable law or agreed to in writing, software\n##   distributed under the License is distributed on an \"AS IS\" BASIS,\n##   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n##   See the License for the specific language governing permissions and\n##   limitations under the License.\n\n__version__ = '1.0.0'\nfrom docopt import docopt\n\nargs = docopt(__doc__, version=__version__)\n\n# import engine module\nimport sirf.STIR as PET\n\nfrom sirf.Utilities import show_2D_array\nimport PET_plot_functions\n#import os\n\n\n# process command-line options\nraw_data_file = args['--file']\nrandoms_data_file = args['--randoms']\nacf_file = args['--attenuation_correction_factors']\ndata_path = args['--path']\nif data_path is None:\n    data_path = PET.examples_data_path('PET') + '/mMR'\nnorm_file = PET.existing_filepath(data_path, args['--norm'])\nmu_map_file = PET.existing_filepath(data_path, args['--attenuation_image'])\noutput_prefix = args['--output']\ninteractive = not args['--non-interactive']\n\n\ndef main():\n\n    # direct all engine's messages to files\n    msg_red = PET.MessageRedirector('info.txt', 'warn.txt', 'errr.txt')\n\n    PET.AcquisitionData.set_storage_scheme('memory')\n\n    # Create the Scatter Estimator\n    # We can use a STIR parameter file like this\n    # par_file_path = os.path.join(os.path.dirname(__file__), '..', '..', 'parameter_files')\n    # se = PET.ScatterEstimator(PET.existing_filepath(par_file_path, 'scatter_estimation.par'))\n    # However, we will just use all defaults here, and set variables below.\n    se = PET.ScatterEstimator()\n\n    prompts = PET.AcquisitionData(raw_data_file)\n    se.set_input(prompts)\n    se.set_attenuation_image(PET.ImageData(mu_map_file))\n    if randoms_data_file is None:\n        randoms = None\n    else:\n        randoms = PET.AcquisitionData(randoms_data_file)\n        se.set_randoms(randoms)\n    if not(norm_file is None):\n        se.set_asm(PET.AcquisitionSensitivityModel(norm_file))\n    if not(acf_file is None):\n        se.set_attenuation_correction_factors(PET.AcquisitionData(acf_file))\n    # could set number of iterations if you want to\n    se.set_num_iterations(1)\n    print(\"number of scatter iterations that will be used: %d\" % se.get_num_iterations())\n    se.set_output_prefix(output_prefix)\n    se.set_up()\n    se.process()\n    scatter_estimate = se.get_output()\n\n    if not interactive:\n        return\n\n    ## show estimated scatter data\n    scatter_estimate_as_array = scatter_estimate.as_array()\n    show_2D_array('Scatter estimate', scatter_estimate_as_array[0, 0, :, :])\n\n    ## let's draw some profiles to check\n    # we will average over all sinograms to reduce noise\n    PET_plot_functions.plot_sinogram_profile(prompts, randoms=randoms, scatter=scatter_estimate)\n\ntry:\n    main()\n    print('done')\nexcept PET.error as err:\n    print('%s' % err.value)\n",
  "# Copyright 2020 by B. Knueven, D. Mildebrath, C. Muir, J-P Watson, and D.L. Woodruff\n# This software is distributed under the 3-clause BSD License.\n# general example driver for the hydro example with cylinders\n# Modfied April 2022 by DLW to illustrate config.py\n\nimport hydro\n\nimport mpisppy.utils.sputils as sputils\n\nfrom mpisppy.spin_the_wheel import WheelSpinner\nfrom mpisppy.utils import config\nimport mpisppy.utils.cfg_vanilla as vanilla\n\nimport mpisppy.cylinders as cylinders\n\nwrite_solution = True\n\ndef _parse_args():\n    # create a Config object, get values for it, and return it\n    cfg = config.Config()\n    cfg.multistage()\n    cfg.ph_args()\n    cfg.two_sided_args()\n    cfg.xhatlooper_args()\n    cfg.xhatshuffle_args()\n    cfg.lagrangian_args()\n    cfg.xhatspecific_args()\n\n    cfg.add_to_config(name =\"stage2EFsolvern\",\n                         description=\"Solver to use for xhatlooper stage2ef option (default None)\",\n                         domain = str,\n                         default=None)\n\n    cfg.parse_command_line(\"farmer_cylinders\")\n    return cfg\n\n\ndef main():\n    cfg = _parse_args()\n\n    _parse_args()  # updates cfg\n\n    BFs = cfg[\"branching_factors\"]\n    if len(BFs) != 2:\n        raise RuntimeError(\"Hydro is a three stage problem, so it needs 2 BFs\")\n\n    xhatshuffle = cfg[\"xhatshuffle\"]\n    lagrangian = cfg[\"lagrangian\"]\n\n    # This is multi-stage, so we need to supply node names\n    all_nodenames = sputils.create_nodenames_from_branching_factors(BFs)\n\n    ScenCount = BFs[0] * BFs[1]\n    scenario_creator_kwargs = {\"branching_factors\": BFs}\n    all_scenario_names = [f\"Scen{i+1}\" for i in range(ScenCount)]\n    scenario_creator = hydro.scenario_creator\n    scenario_denouement = hydro.scenario_denouement\n    rho_setter = None\n    \n    # Things needed for vanilla cylinders\n    beans = (cfg, scenario_creator, scenario_denouement, all_scenario_names)\n    \n    # Vanilla PH hub\n    hub_dict = vanilla.ph_hub(*beans,\n                              scenario_creator_kwargs=scenario_creator_kwargs,\n                              ph_extensions=None,\n                              rho_setter = rho_setter,\n                              all_nodenames = all_nodenames,\n                             )\n\n    # Standard Lagrangian bound spoke\n    if lagrangian:\n        lagrangian_spoke = vanilla.lagrangian_spoke(*beans,\n                                                    scenario_creator_kwargs=scenario_creator_kwargs,\n                                                    rho_setter = rho_setter,\n                                                    all_nodenames = all_nodenames,\n                                                   )\n\n\n    # xhat looper bound spoke\n    \n    if xhatshuffle:\n        xhatshuffle_spoke = vanilla.xhatshuffle_spoke(*beans,\n                                                      all_nodenames=all_nodenames,\n                                                      scenario_creator_kwargs=scenario_creator_kwargs,\n                                                     )\n\n    list_of_spoke_dict = list()\n    if lagrangian:\n        list_of_spoke_dict.append(lagrangian_spoke)\n    if xhatshuffle:\n        list_of_spoke_dict.append(xhatshuffle_spoke)\n\n    if cfg.stage2EFsolvern is not None:\n        xhatshuffle_spoke[\"opt_kwargs\"][\"options\"][\"stage2EFsolvern\"] = cfg[\"stage2EFsolvern\"]\n        xhatshuffle_spoke[\"opt_kwargs\"][\"options\"][\"branching_factors\"] = cfg[\"branching_factors\"]\n\n    wheel = WheelSpinner(hub_dict, list_of_spoke_dict)\n    wheel.spin()\n\n    if wheel.global_rank == 0:  # we are the reporting hub rank\n        print(f\"BestInnerBound={wheel.BestInnerBound} and BestOuterBound={wheel.BestOuterBound}\")\n    \n    if write_solution:\n        wheel.write_first_stage_solution('hydro_first_stage.csv')\n        wheel.write_tree_solution('hydro_full_solution')\n\nif __name__ == \"__main__\":\n    main()\n",
  "\"\"\"\nResult: https://www.youtube.com/watch?v=Qu7HJrsEYFg\n\nThis is how we can imagine knights dancing at the 15th century, based on a very\nserious historical study here: https://www.youtube.com/watch?v=zvCvOC2VwDc\n\nHere is what we do:\n\n0. Get the video of a dancing knight, and a (Creative Commons) audio music file.\n1. Load the audio file and automatically find the tempo.\n2. Load the video and automatically find a segment that loops well\n3. Extract this segment, slow it down so that it matches the audio tempo, and make\n   it loop forever.\n4. Symmetrize this segment so that we will get two knights instead of one\n5. Add a title screen and some credits, write to a file.\n\nThis example has been originally edited in an IPython Notebook, which makes it\neasy to preview and fine-tune each part of the editing.\n\"\"\"\n\nimport os\nimport sys\n\nfrom moviepy import *\nfrom moviepy.audio.tools.cuts import find_audio_period\nfrom moviepy.video.tools.cuts import find_video_period\n\n\n# Next lines are for downloading the required videos from Youtube.\n# To do this you must have youtube-dl installed, otherwise you will need to\n# download the videos by hand and rename them, as follows:\n#     https://www.youtube.com/watch?v=zvCvOC2VwDc => knights.mp4\n#     https://www.youtube.com/watch?v=lkY3Ek9VPtg => frontier.mp4\n\nif not os.path.exists(\"knights.mp4\") or not os.path.exists(\"frontier.webm\"):\n    retcode1 = os.system(\"youtube-dl zvCvOC2VwDc -o knights\")\n    retcode2 = os.system(\"youtube-dl lkY3Ek9VPtg -o frontier\")\n    if retcode1 != 0 or retcode2 != 0:\n        sys.stderr.write(\n            \"Error downloading videos. Check that you've installed youtube-dl.\\n\"\n        )\n        sys.exit(1)\n\n# ==========\n\n\n# LOAD, EDIT, ANALYZE THE AUDIO\n\naudio = (\n    AudioFileClip(\"frontier.webm\")\n    .subclip((4, 7), (4, 18))\n    .audio_fadein(1)\n    .audio_fadeout(1)\n)\n\naudio_period = find_audio_period(audio)\nprint(\"Analyzed the audio, found a period of %.02f seconds\" % audio_period)\n\n\n# LOAD, EDIT, ANALYZE THE VIDEO\n\nclip = (\n    VideoFileClip(\"knights.mp4\", audio=False)\n    .subclip((1, 24.15), (1, 26))\n    .crop(x1=500, x2=1350)\n)\n\nvideo_period = find_video_period(clip, start_time=0.3)\nprint(\"Analyzed the video, found a period of %.02f seconds\" % video_period)\n\nedited_right = (\n    clip.subclip(0, video_period)\n    .speedx(final_duration=2 * audio_period)\n    .fx(vfx.loop, duration=audio.duration)\n    .subclip(0.25)\n)\n\nedited_left = edited_right.fx(vfx.mirror_x)\n\ndancing_knights = (\n    clips_array([[edited_left, edited_right]])\n    .fadein(1)\n    .fadeout(1)\n    .with_audio(audio)\n    .subclip(0.3)\n)\n\n\n# MAKE THE TITLE SCREEN\n\ntxt_title = (\n    TextClip(\n        \"15th century dancing\\n(hypothetical)\",\n        font_size=70,\n        font=\"Century-Schoolbook-Roman\",\n        color=\"white\",\n    )\n    .margin(top=15, opacity=0)\n    .with_position((\"center\", \"top\"))\n)\n\ntitle = (\n    CompositeVideoClip([dancing_knights.to_ImageClip(), txt_title])\n    .fadein(0.5)\n    .with_duration(3.5)\n)\n\n\n# MAKE THE CREDITS SCREEN\n\ntxt_credits = \"\"\"\nCREDITS\n\nVideo excerpt: Le combat en armure au XVe siècle\nBy J. Donzé, D. Jaquet, T. Schmuziger,\nUniversité de Genève, Musée National de Moyen Age\n\nMusic: \"Frontier\", by DOCTOR VOX\nUnder licence Creative Commons\nhttps://www.youtube.com/user/DOCTORVOXofficial\n\nVideo editing © Zulko 2014\n Licence Creative Commons (CC BY 4.0)\nEdited with MoviePy: http://zulko.github.io/moviepy/\n\"\"\"\n\ncredits = (\n    TextClip(\n        txt_credits,\n        color=\"white\",\n        font=\"Century-Schoolbook-Roman\",\n        font_size=35,\n        kerning=-2,\n        interline=-1,\n        bg_color=\"black\",\n        size=title.size,\n    )\n    .with_duration(2.5)\n    .fadein(0.5)\n    .fadeout(0.5)\n)\n\n\n# ASSEMBLE EVERYTHING, WRITE TO FILE\n\nfinal = concatenate_videoclips([title, dancing_knights, credits])\n\nfinal.write_videofile(\n    \"dancing_knights.mp4\", fps=clip.fps, audio_bitrate=\"1000k\", bitrate=\"4000k\"\n)\n",
  "# ==========================================================================\n#  AIDA Detector description implementation\n# --------------------------------------------------------------------------\n# Copyright (C) Organisation europeenne pour la Recherche nucleaire (CERN)\n# All rights reserved.\n#\n# For the licensing terms see $DD4hepINSTALL/LICENSE.\n# For the list of contributors see $DD4hepINSTALL/doc/CREDITS.\n#\n# ==========================================================================\n#\nfrom __future__ import absolute_import, unicode_literals\nimport os\nimport sys\nimport DDG4\nfrom DDG4 import OutputLevel as Output\nfrom g4units import keV\n#\n#\n\"\"\"\n\n   dd4hep simulation example setup using the python configuration\n\n   @author  M.Frank\n   @version 1.0\n\n\"\"\"\n\n\ndef run():\n  kernel = DDG4.Kernel()\n  install_dir = os.environ['DD4hepExamplesINSTALL']\n  kernel.loadGeometry(str(\"file:\" + install_dir + \"/examples/OpticalSurfaces/compact/OpNovice.xml\"))\n\n  DDG4.importConstants(kernel.detectorDescription(), debug=False)\n  geant4 = DDG4.Geant4(kernel, tracker='Geant4TrackerCombineAction')\n  geant4.printDetectors()\n  # Configure UI\n  if len(sys.argv) > 1:\n    geant4.setupCshUI(macro=sys.argv[1])\n  else:\n    geant4.setupCshUI()\n\n  # Configure field\n  geant4.setupTrackingField(prt=True)\n  # Configure Event actions\n  prt = DDG4.EventAction(kernel, 'Geant4ParticlePrint/ParticlePrint')\n  prt.OutputLevel = Output.DEBUG\n  prt.OutputType = 3  # Print both: table and tree\n  kernel.eventAction().adopt(prt)\n\n  generator_output_level = Output.INFO\n\n  # Configure G4 geometry setup\n  seq, act = geant4.addDetectorConstruction(\"Geant4DetectorGeometryConstruction/ConstructGeo\")\n  act.DebugMaterials = True\n  act.DebugElements = False\n  act.DebugVolumes = True\n  act.DebugShapes = True\n  act.DebugSurfaces = True\n\n  # Configure I/O\n  # evt_root = geant4.setupROOTOutput('RootOutput','OpNovice_'+time.strftime('%Y-%m-%d_%H-%M'))\n\n  # Setup particle gun\n  gun = geant4.setupGun(\"Gun\", particle='gamma', energy=5 * keV, multiplicity=1)\n  gun.OutputLevel = generator_output_level\n\n  # And handle the simulation particles.\n  \"\"\"\n  part = DDG4.GeneratorAction(kernel,\"Geant4ParticleHandler/ParticleHandler\")\n  kernel.generatorAction().adopt(part)\n  part.SaveProcesses = ['Decay']\n  part.MinimalKineticEnergy = 100*MeV\n  part.OutputLevel = Output.INFO #generator_output_level\n  part.enableUI()\n  user = DDG4.Action(kernel,\"Geant4TCUserParticleHandler/UserParticleHandler\")\n  user.TrackingVolume_Zmax = 3.0*m\n  user.TrackingVolume_Rmax = 3.0*m\n  user.enableUI()\n  part.adopt(user)\n  \"\"\"\n  geant4.setupTracker('BubbleDevice')\n\n  # Now build the physics list:\n  phys = geant4.setupPhysics('')\n  ph = DDG4.PhysicsList(kernel, 'Geant4OpticalPhotonPhysics/OpticalGammaPhys')\n  ph.VerboseLevel = 2\n  ph.addParticleGroup('G4BosonConstructor')\n  ph.addParticleGroup('G4LeptonConstructor')\n  ph.addParticleGroup('G4MesonConstructor')\n  ph.addParticleGroup('G4BaryonConstructor')\n  ph.addParticleGroup('G4IonConstructor')\n  ph.addParticleConstructor('G4OpticalPhoton')\n\n  ph.addDiscreteParticleProcess('gamma', 'G4GammaConversion')\n  ph.addDiscreteParticleProcess('gamma', 'G4ComptonScattering')\n  ph.addDiscreteParticleProcess('gamma', 'G4PhotoElectricEffect')\n  ph.addParticleProcess(str('e[+-]'), str('G4eMultipleScattering'), -1, 1, 1)\n  ph.addParticleProcess(str('e[+-]'), str('G4eIonisation'), -1, 2, 2)\n  ph.addParticleProcess(str('e[+-]'), str('G4eBremsstrahlung'), -1, 3, 3)\n  ph.addParticleProcess(str('e+'), str('G4eplusAnnihilation'), 0, -1, 4)\n  ph.addParticleProcess(str('mu[+-]'), str('G4MuMultipleScattering'), -1, 1, 1)\n  ph.addParticleProcess(str('mu[+-]'), str('G4MuIonisation'), -1, 2, 2)\n  ph.addParticleProcess(str('mu[+-]'), str('G4MuBremsstrahlung'), -1, 3, 3)\n  ph.addParticleProcess(str('mu[+-]'), str('G4MuPairProduction'), -1, 4, 4)\n  ph.enableUI()\n  phys.adopt(ph)\n\n  ph = DDG4.PhysicsList(kernel, 'Geant4ScintillationPhysics/ScintillatorPhys')\n  ph.ScintillationYieldFactor = 1.0\n  ph.ScintillationExcitationRatio = 1.0\n  ph.TrackSecondariesFirst = False\n  ph.VerboseLevel = 2\n  ph.enableUI()\n  phys.adopt(ph)\n\n  ph = DDG4.PhysicsList(kernel, 'Geant4CerenkovPhysics/CerenkovPhys')\n  ph.MaxNumPhotonsPerStep = 10\n  ph.MaxBetaChangePerStep = 10.0\n  ph.TrackSecondariesFirst = True\n  ph.VerboseLevel = 2\n  ph.enableUI()\n  phys.adopt(ph)\n\n  phys.dump()\n\n  geant4.execute()\n\n\nif __name__ == \"__main__\":\n  run()\n",
  "import numpy as np\nimport scipy.sparse as sps\n\nfrom porepy.viz import exporter\nfrom porepy.fracs import importer\n\nfrom porepy.params import tensor\nfrom porepy.params.bc import BoundaryCondition\nfrom porepy.params.data import Parameters\n\nfrom porepy.grids import coarsening as co\n\nfrom porepy.numerics.vem import dual\n\nfrom porepy.utils import comp_geom as cg\nfrom porepy.utils import sort_points\n\n#------------------------------------------------------------------------------#\n\n\ndef add_data(gb, domain):\n    \"\"\"\n    Define the permeability, apertures, boundary conditions\n    \"\"\"\n    gb.add_node_props(['param', 'is_tangent'])\n    tol = 1e-3\n    a = 1e-2\n\n    for g, d in gb:\n        param = Parameters(g)\n\n        # Permeability\n        d['is_tangential'] = True\n        if g.dim == 2:\n            kxx = 1e-14 * np.ones(g.num_cells)\n        else:\n            kxx = 1e-8 * np.ones(g.num_cells)\n\n        perm = tensor.SecondOrder(g.dim, kxx)\n        param.set_tensor(\"flow\", perm)\n\n        # Source term\n        param.set_source(\"flow\", np.zeros(g.num_cells))\n\n        # Assign apertures\n        aperture = np.power(a, gb.dim_max() - g.dim)\n        param.set_aperture(np.ones(g.num_cells) * aperture)\n\n        # Boundaries\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            left = bound_face_centers[0, :] < domain['xmin'] + tol\n            right = bound_face_centers[0, :] > domain['xmax'] - tol\n\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[np.logical_or(left, right)] = 'dir'\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[left]] = 1013250\n\n            param.set_bc(\"flow\", BoundaryCondition(g, bound_faces, labels))\n            param.set_bc_val(\"flow\", bc_val)\n        else:\n            param.set_bc(\"flow\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n\n        d['param'] = param\n\n    # Assign coupling permeability\n    gb.add_edge_prop('kn')\n    for e, d in gb.edges_props():\n        gn = gb.sorted_nodes_of_edge(e)\n        aperture = np.power(a, gb.dim_max() - gn[0].dim)\n        d['kn'] = 1e-10 * np.ones(gn[0].num_cells) / aperture\n\n#------------------------------------------------------------------------------#\n\n\ndef plot_over_line(gb, pts, name, tol):\n\n    values = np.zeros(pts.shape[1])\n    is_found = np.zeros(pts.shape[1], dtype=np.bool)\n\n    for g, d in gb:\n        if g.dim < gb.dim_max():\n            continue\n\n        if not cg.is_planar(np.hstack((g.nodes, pts)), tol=1e-4):\n            continue\n\n        faces_cells, _, _ = sps.find(g.cell_faces)\n        nodes_faces, _, _ = sps.find(g.face_nodes)\n\n        normal = cg.compute_normal(g.nodes)\n        for c in np.arange(g.num_cells):\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c + 1])\n            pts_id_c = np.array([nodes_faces[g.face_nodes.indptr[f]:\n                                             g.face_nodes.indptr[f + 1]]\n                                 for f in faces_cells[loc]]).T\n            pts_id_c = sort_points.sort_point_pairs(pts_id_c)[0, :]\n            pts_c = g.nodes[:, pts_id_c]\n\n            mask = np.where(np.logical_not(is_found))[0]\n            if mask.size == 0:\n                break\n            check = np.zeros(mask.size, dtype=np.bool)\n            last = False\n            for i, pt in enumerate(pts[:, mask].T):\n                check[i] = cg.is_point_in_cell(pts_c, pt)\n                if last and not check[i]:\n                    break\n            is_found[mask] = check\n            values[mask[check]] = d[name][c]\n\n    return values\n\n##------------------------------------------------------------------------------#\n\n\ntol = 1e-4\nmesh_kwargs = {}\nmesh_kwargs['mesh_size'] = {'mode': 'weighted',\n                            'value': 500,\n                            'bound_value': 500,\n                            'tol': tol}\n\ndomain = {'xmin': 0, 'xmax': 700, 'ymin': 0, 'ymax': 600}\ngb = importer.from_csv('network.csv', mesh_kwargs, domain)\ngb.compute_geometry()\nco.coarsen(gb, 'by_volume')\ngb.assign_node_ordering()\n\n# Assign parameters\nadd_data(gb, domain)\n\n# Choose and define the solvers and coupler\nsolver = dual.DualVEMMixDim(\"flow\")\nA, b = solver.matrix_rhs(gb)\n\nup = sps.linalg.spsolve(A, b)\nsolver.split(gb, \"up\", up)\n\ngb.add_node_props([\"discharge\", 'pressure', \"P0u\"])\nsolver.extract_u(gb, \"up\", \"discharge\")\nsolver.extract_p(gb, \"up\", 'pressure')\nsolver.project_u(gb, \"discharge\", \"P0u\")\n\nexporter.export_vtk(gb, 'vem', ['pressure', \"P0u\"], folder='example_5_1_2')\n\n# This part is very slow and not optimized, it's just to obtain the plots once.\nb_box = gb.bounding_box()\nN_pts = 1000\ny_range = np.linspace(b_box[0][1] + tol, b_box[1][1] - tol, N_pts)\npts = np.stack((625 * np.ones(N_pts), y_range, np.zeros(N_pts)))\nvalues = plot_over_line(gb, pts, 'pressure', tol)\n\narc_length = y_range - b_box[0][1]\nnp.savetxt(\"example_5_1_2/vem_x_625.csv\", (arc_length, values))\n\nx_range = np.linspace(b_box[0][0] + tol, b_box[1][0] - tol, N_pts)\npts = np.stack((x_range, 500 * np.ones(N_pts), np.zeros(N_pts)))\nvalues = plot_over_line(gb, pts, 'pressure', tol)\n\narc_length = x_range - b_box[0][0]\nnp.savetxt(\"example_5_1_2/vem_y_500.csv\", (arc_length, values))\n\n\nprint(\"diam\", gb.diameter(lambda g: g.dim == gb.dim_max()))\nprint(\"num_cells 2d\", gb.num_cells(lambda g: g.dim == 2))\nprint(\"num_cells 1d\", gb.num_cells(lambda g: g.dim == 1))\n",
  "#!/usr/bin/env python\n# encoding: utf-8\n# __author__ = 'Demon'\nfrom seedemu.layers import Base, Routing, Ebgp, PeerRelationship, Ibgp, Ospf\nfrom seedemu.compiler import Docker\nfrom seedemu.services import DomainNameCachingService\nfrom seedemu.core import Emulator, Binding, Filter, Node\nfrom typing import List\n\nsim = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nibgp = Ibgp()\nospf = Ospf()\n\n\ndef make_stub_as(asn: int, exchange: str):\n    stub_as = base.createAutonomousSystem(asn)\n    host = stub_as.createHost('host0')\n    host1 = stub_as.createHost('host1')\n    host2 = stub_as.createHost('host2')\n    # host3 = stub_as.createHost('host3')\n    # host4 = stub_as.createHost('host4')\n    # host5 = stub_as.createHost('host5')\n    # ldns_host = stub_as.createHost('ldns') #used for local dns service\n\n    router = stub_as.createRouter('router0')\n    net = stub_as.createNetwork('net0')\n\n    \n    router.joinNetwork('net0')\n    host.joinNetwork('net0')\n    host1.joinNetwork('net0')\n    host2.joinNetwork('net0')\n    # host3.joinNetwork('net0')\n    # host4.joinNetwork('net0')\n    # host5.joinNetwork('net0')\n    # ldns_host.joinNetwork('net0')\n\n    router.joinNetwork(exchange)\n\n##############Install local DNS###############################################\n# ldns.install('local-dns-150').setConfigureResolvconf(True)\n# ldns.install('local-dns-151').setConfigureResolvconf(True)\n# ldns.install('local-dns-152').setConfigureResolvconf(True)\n# ldns.install('local-dns-153').setConfigureResolvconf(True)\n# ldns.install('local-dns-154').setConfigureResolvconf(True)\n# ldns.install('local-dns-160').setConfigureResolvconf(True)\n# ldns.install('local-dns-161').setConfigureResolvconf(True)\n#\n# #Add bindings for local dns:\n# sim.addBinding(Binding('local-dns-150', filter = Filter(asn=150, nodeName=\"ldns\")))\n# sim.addBinding(Binding('local-dns-151', filter = Filter(asn=151, nodeName=\"ldns\")))\n# sim.addBinding(Binding('local-dns-152', filter = Filter(asn=152, nodeName=\"ldns\")))\n# sim.addBinding(Binding('local-dns-153', filter = Filter(asn=153, nodeName=\"ldns\")))\n# sim.addBinding(Binding('local-dns-154', filter = Filter(asn=154, nodeName=\"ldns\")))\n# sim.addBinding(Binding('local-dns-160', filter = Filter(asn=160, nodeName=\"ldns\")))\n# sim.addBinding(Binding('local-dns-161', filter = Filter(asn=161, nodeName=\"ldns\")))\n\n##############################################################################\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\nbase.createInternetExchange(102)\n\nmake_stub_as(150, 'ix100')\nmake_stub_as(151, 'ix100')\n\nmake_stub_as(152, 'ix101')\nmake_stub_as(153, 'ix101')\nmake_stub_as(154, 'ix101')\n\nmake_stub_as(160, 'ix102')\nmake_stub_as(161, 'ix102')\n\n###############################################################################\n\nas2 = base.createAutonomousSystem(2)\n\nas2_100 = as2.createRouter('r0')\nas2_101 = as2.createRouter('r1')\nas2_102 = as2.createRouter('r2')\n\nas2_100.joinNetwork('ix100')\nas2_101.joinNetwork('ix101')\nas2_102.joinNetwork('ix102')\n\nas2_net_100_101 = as2.createNetwork('n01')\nas2_net_101_102 = as2.createNetwork('n12')\nas2_net_102_100 = as2.createNetwork('n20')\n\n\n\n\n\nas2_100.joinNetwork('n01')\nas2_101.joinNetwork('n01')\n\nas2_101.joinNetwork('n12')\nas2_102.joinNetwork('n12')\n\nas2_102.joinNetwork('n20')\nas2_100.joinNetwork('n20')\n\n###############################################################################\n\nas3 = base.createAutonomousSystem(3)\n\nas3_101 = as3.createRouter('r1')\nas3_102 = as3.createRouter('r2')\n\nas3_101.joinNetwork('ix101')\nas3_102.joinNetwork('ix102')\n\nas3_net_101_102 = as3.createNetwork('n12')\n\n\n\nas3_101.joinNetwork('n12')\nas3_102.joinNetwork('n12')\n\n###############################################################################\n\nebgp.addPrivatePeering(100, 2, 150, PeerRelationship.Provider)\nebgp.addPrivatePeering(100, 150, 151, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(101, 2, 3, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 154, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 154, PeerRelationship.Provider)\n\n\nebgp.addPrivatePeering(102, 2, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 161, PeerRelationship.Provider)\n\n###############################################################################\n\n\nsim.addLayer(base)\nsim.addLayer(routing)\nsim.addLayer(ebgp)\nsim.addLayer(ibgp)\nsim.addLayer(ospf)\n\nsim.dump('base-component.bin')",
  "\"\"\"\nMaxwell 2D: transient winding analysis\n--------------------------------------\nThis example shows how you can use PyAEDT to create a project in Maxwell 2D\nand run a transient simulation. It runs only on Windows using CPython.\n\nThe following libraries are required for the advanced postprocessing features\nused in this example:\n\n- `Matplotlib <https://pypi.org/project/matplotlib/>`_\n- `Numpty <https://pypi.org/project/numpy/>`_\n- `PyVista <https://pypi.org/project/pyvista/>`_\n\nInstall these libraries with:\n\n.. code::\n\n   pip install numpy pyvista matplotlib\n\n\"\"\"\n###############################################################################\n# Perform required imports\n# ~~~~~~~~~~~~~~~~~~~~~~~~\n# Perform required imports.\n\nimport os\nimport pyaedt\n\n###############################################################################\n# Set non-graphical mode\n# ~~~~~~~~~~~~~~~~~~~~~~\n# Set non-graphical mode. \n# You can set ``non_graphical`` either to ``True`` or ``False``.\n\nnon_graphical = False\n\n###############################################################################\n# Insert Maxwell 2D design and save project\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Insert a Maxwell 2D design and save the project.\n\nmaxwell_2d = pyaedt.Maxwell2d(solution_type=\"TransientXY\", specified_version=\"2023.2\", non_graphical=non_graphical,\n                              new_desktop_session=True, projectname=pyaedt.generate_unique_project_name())\n\n###############################################################################\n# Create rectangle and duplicate it\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Create a rectangle and duplicate it.\n\nrect1 = maxwell_2d.modeler.create_rectangle([0, 0, 0], [10, 20], name=\"winding\", matname=\"copper\")\nadded = rect1.duplicate_along_line([14, 0, 0])\nrect2 = maxwell_2d.modeler[added[0]]\n\n###############################################################################\n# Create air region\n# ~~~~~~~~~~~~~~~~~\n# Create an air region.\n\nregion = maxwell_2d.modeler.create_region([100, 100, 100, 100, 100, 100])\n\n###############################################################################\n# Assign windings and balloon\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Assigns windings to the sheets and a balloon to the air region.\n\nmaxwell_2d.assign_winding([rect1.name, rect2.name], name=\"PHA\")\nmaxwell_2d.assign_balloon(region.edges)\n\n###############################################################################\n# Plot model\n# ~~~~~~~~~~\n# Plot the model.\n\nmaxwell_2d.plot(show=False, export_path=os.path.join(maxwell_2d.working_directory, \"Image.jpg\"), plot_air_objects=True)\n\n###############################################################################\n# Create setup\n# ~~~~~~~~~~~~\n# Create the transient setup.\n\nsetup = maxwell_2d.create_setup()\nsetup.props[\"StopTime\"] = \"0.02s\"\nsetup.props[\"TimeStep\"] = \"0.0002s\"\nsetup.props[\"SaveFieldsType\"] = \"Every N Steps\"\nsetup.props[\"N Steps\"] = \"1\"\nsetup.props[\"Steps From\"] = \"0s\"\nsetup.props[\"Steps To\"] = \"0.002s\"\n\n###############################################################################\n# Create rectangular plot\n# ~~~~~~~~~~~~~~~~~~~~~~~\n# Create a rectangular plot.\n\nmaxwell_2d.post.create_report(\n    \"InputCurrent(PHA)\", domain=\"Time\", primary_sweep_variable=\"Time\", plotname=\"Winding Plot 1\"\n)\n\n###############################################################################\n# Solve model\n# ~~~~~~~~~~~\n# Solve the model.\n\nmaxwell_2d.analyze(use_auto_settings=False)\n\n###############################################################################\n# Create output and plot using PyVista\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Create the output and plot it using PyVista.\n\ncutlist = [\"Global:XY\"]\nface_lists = rect1.faces\nface_lists += rect2.faces\ntimesteps = [str(i * 2e-4) + \"s\" for i in range(11)]\nid_list = [f.id for f in face_lists]\n\nanimatedGif = maxwell_2d.post.plot_animated_field(\n    \"Mag_B\",\n    id_list,\n    \"Surface\",\n    intrinsics={\"Time\": \"0s\"},\n    variation_variable=\"Time\",\n    variation_list=timesteps,\n    show=False,\n    export_gif=False,\n)\nanimatedGif.isometric_view = False\nanimatedGif.camera_position = [15, 15, 80]\nanimatedGif.focal_point = [15, 15, 0]\nanimatedGif.roll_angle = 0\nanimatedGif.elevation_angle = 0\nanimatedGif.azimuth_angle = 0\n# Set off_screen to False to visualize the animation.\n# animatedGif.off_screen = False\nanimatedGif.animate()\n\n###############################################################################\n# Generate plot outside of AEDT\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Generate the same plot outside AEDT.\n\nsolutions = maxwell_2d.post.get_solution_data(\"InputCurrent(PHA)\", primary_sweep_variable=\"Time\")\nsolutions.plot()\n\n###############################################\n# Close AEDT\n# ~~~~~~~~~~\n# Close AEDT.\n\nmaxwell_2d.release_desktop()\n",
  "\"\"\"\n================\nVisualize Wrench\n================\n\nWe see a 6-DOF robot arm, and we assume that we have a force/torque sensor\nthat measures the force of a spherical mass (gray sphere) at the tool center\npoint (TCP). We can draw the screw representation of the wrench in the TCP\nframe as a force along a screw axis from the red sphere to the green sphere.\nThen we use the adjoint representation of the transformation from the base\nto the TCP to transform the wrench to the robot's base frame. This wrench\nhas a force component and a torque component, which we can also visualize\nas a screw: the red sphere indicates the point q on the screw axis, the\nstraight black line shows the screw axis, the red line indicates the\ndirection to the initial configuration and the green line indicates the\ndirection to the displaced configuration in which the instantaneous\nwrench would move the base.\n\"\"\"\nimport os\nimport numpy as np\nfrom pytransform3d.urdf import UrdfTransformManager\nimport pytransform3d.transformations as pt\nimport pytransform3d.visualizer as pv\n\n\ndef plot_screw(figure, q=np.zeros(3), s_axis=np.array([1.0, 0.0, 0.0]),\n               h=1.0, theta=1.0, A2B=None, s=1.0):\n    \"\"\"Plot transformation about and along screw axis.\n\n    Parameters\n    ----------\n    figure : Figure\n        Interface to Open3D's visualizer\n\n    q : array-like, shape (3,), optional (default: [0, 0, 0])\n        Vector to a point on the screw axis\n\n    s_axis : array-like, shape (3,), optional (default: [1, 0, 0])\n        Direction vector of the screw axis\n\n    h : float, optional (default: 1)\n        Pitch of the screw. The pitch is the ratio of translation and rotation\n        of the screw axis. Infinite pitch indicates pure translation.\n\n    theta : float, optional (default: 1)\n        Rotation angle. h * theta is the translation.\n\n    A2B : array-like, shape (4, 4), optional (default: I)\n        Origin of the screw\n\n    s : float, optional (default: 1)\n        Scaling of the axis and angle that will be drawn\n    \"\"\"\n    from pytransform3d.rotations import (\n        vector_projection, angle_between_vectors, perpendicular_to_vectors,\n        slerp_weights)\n    from pytransform3d.transformations import (\n        check_screw_parameters, transform, translate_transform,\n        vector_to_point, vector_to_direction, vectors_to_points)\n\n    if A2B is None:\n        A2B = np.eye(4)\n\n    q, s_axis, h = check_screw_parameters(q, s_axis, h)\n\n    origin_projected_on_screw_axis = q + vector_projection(-q, s_axis)\n\n    pure_translation = np.isinf(h)\n\n    if not pure_translation:\n        screw_axis_to_old_frame = -origin_projected_on_screw_axis\n        screw_axis_to_rotated_frame = perpendicular_to_vectors(\n            s_axis, screw_axis_to_old_frame)\n        screw_axis_to_translated_frame = h * s_axis\n\n        arc = np.empty((100, 3))\n        angle = angle_between_vectors(screw_axis_to_old_frame,\n                                      screw_axis_to_rotated_frame)\n        for i, t in enumerate(zip(np.linspace(0, 2 * theta / np.pi, len(arc)),\n                                  np.linspace(0.0, 1.0, len(arc)))):\n            t1, t2 = t\n            w1, w2 = slerp_weights(angle, t1)\n            arc[i] = (origin_projected_on_screw_axis\n                      + w1 * screw_axis_to_old_frame\n                      + w2 * screw_axis_to_rotated_frame\n                      + screw_axis_to_translated_frame * t2 * theta)\n\n    q = transform(A2B, vector_to_point(q))[:3]\n    s_axis = transform(A2B, vector_to_direction(s_axis))[:3]\n    if not pure_translation:\n        arc = transform(A2B, vectors_to_points(arc))[:, :3]\n        origin_projected_on_screw_axis = transform(\n            A2B, vector_to_point(origin_projected_on_screw_axis))[:3]\n\n    # Screw axis\n    Q = translate_transform(np.eye(4), q)\n    fig.plot_sphere(radius=s * 0.02, A2B=Q, c=[1, 0, 0])\n    if pure_translation:\n        s_axis *= theta\n        Q_plus_S_axis = translate_transform(np.eye(4), q + s_axis)\n        fig.plot_sphere(radius=s * 0.02, A2B=Q_plus_S_axis, c=[0, 1, 0])\n    P = np.array([\n        [q[0] - s * s_axis[0], q[1] - s * s_axis[1], q[2] - s * s_axis[2]],\n        [q[0] + (1 + s) * s_axis[0],\n         q[1] + (1 + s) * s_axis[1], q[2] + (1 + s) * s_axis[2]]\n    ])\n    figure.plot(P=P, c=[0, 0, 0])\n\n    if not pure_translation:\n        # Transformation\n        figure.plot(arc, c=[0, 0, 0])\n\n        for i, c in zip([0, -1], [[1, 0, 0], [0, 1, 0]]):\n            arc_bound = np.vstack((origin_projected_on_screw_axis, arc[i]))\n            figure.plot(arc_bound, c=c)\n\n\nBASE_DIR = \"test/test_data/\"\ndata_dir = BASE_DIR\nsearch_path = \".\"\nwhile (not os.path.exists(data_dir) and\n       os.path.dirname(search_path) != \"pytransform3d\"):\n    search_path = os.path.join(search_path, \"..\")\n    data_dir = os.path.join(search_path, BASE_DIR)\n\ntm = UrdfTransformManager()\nfilename = os.path.join(data_dir, \"robot_with_visuals.urdf\")\nwith open(filename, \"r\") as f:\n    robot_urdf = f.read()\n    tm.load_urdf(robot_urdf, mesh_path=data_dir)\ntm.set_joint(\"joint2\", 0.2 * np.pi)\ntm.set_joint(\"joint3\", 0.2 * np.pi)\ntm.set_joint(\"joint5\", 0.1 * np.pi)\ntm.set_joint(\"joint6\", 0.5 * np.pi)\n\nee2base = tm.get_transform(\"tcp\", \"robot_arm\")\nbase2ee = tm.get_transform(\"robot_arm\", \"tcp\")\n\nmass = 1.0\nwrench_in_ee = np.array([0.0, 0.0, 0.0, 0.0, -9.81, 0.0]) * mass\nwrench_in_base = np.dot(pt.adjoint_from_transform(base2ee).T, wrench_in_ee)\n\nfig = pv.figure()\n\nfig.plot_graph(tm, \"robot_arm\", s=0.1, show_visuals=True)\n\nfig.plot_transform(s=0.4)\nfig.plot_transform(A2B=ee2base, s=0.1)\n\nmass2base = np.copy(ee2base)\nmass2base[2, 3] += 0.075\nfig.plot_sphere(radius=0.025, A2B=mass2base)\n\nS, theta = pt.screw_axis_from_exponential_coordinates(wrench_in_base)\nq, s, h = pt.screw_parameters_from_screw_axis(S)\nplot_screw(fig, q, s, h, theta * 0.05)\n\nS, theta = pt.screw_axis_from_exponential_coordinates(wrench_in_ee)\nq, s, h = pt.screw_parameters_from_screw_axis(S)\nplot_screw(fig, q, s, h, theta * 0.05, A2B=ee2base)\n\nfig.view_init()\nif \"__file__\" in globals():\n    fig.show()\nelse:\n    fig.save_image(\"__open3d_rendered_image.jpg\")\n",
  "\"\"\"\n.. _ref_parametric_example:\n\nParametric Geometric Objects\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nCreating parametric objects\n\"\"\"\n\nfrom math import pi\n\n# sphinx_gallery_thumbnail_number = 12\nimport pyvista as pv\n\n###############################################################################\n# This example demonstrates how to plot parametric objects using pyvista\n#\n# Supertoroid\n# +++++++++++\n\nsupertoroid = pv.ParametricSuperToroid(n1=0.5)\nsupertoroid.plot(color='lightblue', smooth_shading=True)\n\n###############################################################################\n# Parametric Ellipsoid\n# ++++++++++++++++++++\n\n# Ellipsoid with a long x axis\nellipsoid = pv.ParametricEllipsoid(10, 5, 5)\nellipsoid.plot(color='lightblue')\n\n\n###############################################################################\n# Partial Parametric Ellipsoid\n# ++++++++++++++++++++++++++++\n\n# cool plotting direction\ncpos = [\n    (21.9930, 21.1810, -30.3780),\n    (-1.1640, -1.3098, -0.1061),\n    (0.8498, -0.2515, 0.4631),\n]\n\n\n# half ellipsoid\npart_ellipsoid = pv.ParametricEllipsoid(10, 5, 5, max_v=pi / 2)\npart_ellipsoid.plot(color='lightblue', smooth_shading=True, cpos=cpos)\n\n\n###############################################################################\n# Pseudosphere\n# ++++++++++++\n\npseudosphere = pv.ParametricPseudosphere()\npseudosphere.plot(color='lightblue', smooth_shading=True)\n\n###############################################################################\n# Bohemian Dome\n# +++++++++++++\n\n\nbohemiandome = pv.ParametricBohemianDome()\nbohemiandome.plot(color='lightblue')\n\n###############################################################################\n# Bour\n# ++++\n\nbour = pv.ParametricBour()\nbour.plot(color='lightblue')\n\n###############################################################################\n# Boy's Surface\n# +++++++++++++\n\nboy = pv.ParametricBoy()\nboy.plot(color='lightblue')\n\n###############################################################################\n# Catalan Minimal\n# +++++++++++++++\n\ncatalanminimal = pv.ParametricCatalanMinimal()\ncatalanminimal.plot(color='lightblue')\n\n###############################################################################\n# Conic Spiral\n# ++++++++++++\n\nconicspiral = pv.ParametricConicSpiral()\nconicspiral.plot(color='lightblue')\n\n###############################################################################\n# Cross Cap\n# +++++++++\n\ncrosscap = pv.ParametricCrossCap()\ncrosscap.plot(color='lightblue')\n\n###############################################################################\n# Dini\n# ++++\n\ndini = pv.ParametricDini()\ndini.plot(color='lightblue')\n\n###############################################################################\n# Enneper\n# +++++++\n\nenneper = pv.ParametricEnneper()\nenneper.plot(cpos=\"yz\")\n\n###############################################################################\n# Figure-8 Klein\n# ++++++++++++++\n\nfigure8klein = pv.ParametricFigure8Klein()\nfigure8klein.plot()\n\n###############################################################################\n# Henneberg\n# +++++++++\n\nhenneberg = pv.ParametricHenneberg()\nhenneberg.plot(color='lightblue')\n\n###############################################################################\n# Klein\n# +++++\n\nklein = pv.ParametricKlein()\nklein.plot(color='lightblue')\n\n###############################################################################\n# Kuen\n# ++++\n\nkuen = pv.ParametricKuen()\nkuen.plot(color='lightblue')\n\n###############################################################################\n# Mobius\n# ++++++\n\nmobius = pv.ParametricMobius()\nmobius.plot(color='lightblue')\n\n###############################################################################\n# Plucker Conoid\n# ++++++++++++++\n\npluckerconoid = pv.ParametricPluckerConoid()\npluckerconoid.plot(color='lightblue')\n\n\n###############################################################################\n# Random Hills\n# ++++++++++++\n\nrandomhills = pv.ParametricRandomHills()\nrandomhills.plot(color='lightblue')\n\n###############################################################################\n# Roman\n# +++++\n\nroman = pv.ParametricRoman()\nroman.plot(color='lightblue')\n\n###############################################################################\n# Super Ellipsoid\n# +++++++++++++++\n\nsuperellipsoid = pv.ParametricSuperEllipsoid(n1=0.1, n2=2)\nsuperellipsoid.plot(color='lightblue')\n\n###############################################################################\n# Torus\n# +++++\n\ntorus = pv.ParametricTorus()\ntorus.plot(color='lightblue')\n\n###############################################################################\n# Circular Arc\n# ++++++++++++\n\npointa = [-1, 0, 0]\npointb = [0, 1, 0]\ncenter = [0, 0, 0]\nresolution = 100\n\narc = pv.CircularArc(pointa, pointb, center, resolution)\n\npl = pv.Plotter()\npl.add_mesh(arc, color='k', line_width=4)\npl.show_bounds()\npl.view_xy()\npl.show()\n\n\n###############################################################################\n# Extruded Half Arc\n# +++++++++++++++++\n\npointa = [-1, 0, 0]\npointb = [1, 0, 0]\ncenter = [0, 0, 0]\nresolution = 100\n\narc = pv.CircularArc(pointa, pointb, center, resolution)\npoly = arc.extrude([0, 0, 1])\npoly.plot(color='lightblue', cpos='iso', show_edges=True)\n",
  "from seedemu.layers import Base, Routing, Ebgp, PeerRelationship, Ibgp, Ospf\nfrom seedemu.services import WebService\nfrom seedemu.compiler import Docker\nfrom seedemu.core import Emulator, Filter, Binding\nfrom seedemu.components import BgpAttackerComponent\nfrom seedemu.mergers import DEFAULT_MERGERS\n\nemu = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nibgp = Ibgp()\nospf = Ospf()\nweb = WebService()\n\n###############################################################################\n\ndef make_stub_as(asn: int, exchange: str):\n    stub_as = base.createAutonomousSystem(asn)\n\n    web_server = stub_as.createHost('web')\n    web.install('web{}'.format(asn))\n    emu.addBinding(Binding('web{}'.format(asn), filter = Filter(asn = asn, nodeName = 'web')))\n\n    router = stub_as.createRouter('router0')\n\n    net = stub_as.createNetwork('net0')\n\n    web_server.joinNetwork('net0')\n    router.joinNetwork('net0')\n\n    router.joinNetwork(exchange)\n\n###############################################################################\n\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\nbase.createInternetExchange(102)\n\n###############################################################################\n\nmake_stub_as(150, 'ix100')\nmake_stub_as(151, 'ix100')\n\nmake_stub_as(152, 'ix101')\n\nmake_stub_as(160, 'ix102')\nmake_stub_as(161, 'ix102')\n\n###############################################################################\n\nas2 = base.createAutonomousSystem(2)\n\nas2_100 = as2.createRouter('r0')\nas2_101 = as2.createRouter('r1')\nas2_102 = as2.createRouter('r2')\n\nas2_100.joinNetwork('ix100')\nas2_101.joinNetwork('ix101')\nas2_102.joinNetwork('ix102')\n\nas2_net_100_101 = as2.createNetwork('n01')\nas2_net_101_102 = as2.createNetwork('n12')\nas2_net_102_100 = as2.createNetwork('n20')\n\nas2_100.joinNetwork('n01')\nas2_101.joinNetwork('n01')\n\nas2_101.joinNetwork('n12')\nas2_102.joinNetwork('n12')\n\nas2_102.joinNetwork('n20')\nas2_100.joinNetwork('n20')\n\n###############################################################################\n\nas3 = base.createAutonomousSystem(3)\n\nas3_101 = as3.createRouter('r1')\nas3_102 = as3.createRouter('r2')\n\nas3_101.joinNetwork('ix101')\nas3_102.joinNetwork('ix102')\n\nas3_net_101_102 = as3.createNetwork('n12')\n\nas3_101.joinNetwork('n12')\nas3_102.joinNetwork('n12')\n\n###############################################################################\n\nebgp.addPrivatePeering(100, 2, 150, PeerRelationship.Provider)\nebgp.addPrivatePeering(100, 150, 151, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(101, 2, 3, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 152, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(102, 2, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 161, PeerRelationship.Provider)\n\n###############################################################################\n\nemu.addLayer(base)\nemu.addLayer(routing)\nemu.addLayer(ebgp)\nemu.addLayer(ibgp)\nemu.addLayer(ospf)\nemu.addLayer(web)\n\n###############################################################################\n# Add BGP attacker component\n\nbgp_attacker = BgpAttackerComponent(attackerAsn = 66)\n\nbgp_attacker.addHijackedPrefix('10.151.0.0/25')\nbgp_attacker.addHijackedPrefix('10.151.0.128/25')\nbgp_attacker.joinInternetExchange('ix101', '10.101.0.66')\n\nebgp.addPrivatePeering(101, 2, 66, PeerRelationship.Unfiltered)\n\nemu_new = emu.merge(bgp_attacker.get(), DEFAULT_MERGERS)\nemu_new.render()\n\n###############################################################################\n\nemu_new.compile(Docker(selfManagedNetwork = True), './output2')\n",
  "import numpy as np\nimport scipy.sparse as sps\n\nfrom porepy.viz import exporter\nfrom porepy.fracs import importer\n\nfrom porepy.params import tensor\nfrom porepy.params.bc import BoundaryCondition\nfrom porepy.params.data import Parameters\n\nfrom porepy.grids import coarsening as co\n\nfrom porepy.numerics.vem import dual\nfrom porepy.utils import comp_geom as cg\n\n#------------------------------------------------------------------------------#\n\n\ndef add_data(gb, domain, kf):\n    \"\"\"\n    Define the permeability, apertures, boundary conditions\n    \"\"\"\n    gb.add_node_props(['param'])\n    tol = 1e-5\n    a = 1e-4\n\n    for g, d in gb:\n        param = Parameters(g)\n\n        # Permeability\n        kxx = np.ones(g.num_cells) * np.power(kf, g.dim < gb.dim_max())\n        if g.dim == 2:\n            perm = tensor.SecondOrder(3, kxx=kxx, kyy=kxx, kzz=1)\n        else:\n            perm = tensor.SecondOrder(3, kxx=kxx, kyy=1, kzz=1)\n            if g.dim == 1:\n                R = cg.project_line_matrix(g.nodes, reference=[1, 0, 0])\n                perm.rotate(R)\n\n        param.set_tensor(\"flow\", perm)\n\n        # Source term\n        param.set_source(\"flow\", np.zeros(g.num_cells))\n\n        # Assign apertures\n        aperture = np.power(a, gb.dim_max() - g.dim)\n        param.set_aperture(np.ones(g.num_cells) * aperture)\n\n        # Boundaries\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            left = bound_face_centers[0, :] < domain['xmin'] + tol\n            right = bound_face_centers[0, :] > domain['xmax'] - tol\n\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[right] = 'dir'\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[left]] = -aperture \\\n                * g.face_areas[bound_faces[left]]\n            bc_val[bound_faces[right]] = 1\n\n            param.set_bc(\"flow\", BoundaryCondition(g, bound_faces, labels))\n            param.set_bc_val(\"flow\", bc_val)\n        else:\n            param.set_bc(\"flow\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n\n        d['param'] = param\n\n    # Assign coupling permeability\n    gb.add_edge_prop('kn')\n    for e, d in gb.edges_props():\n        gn = gb.sorted_nodes_of_edge(e)\n        aperture = np.power(a, gb.dim_max() - gn[0].dim)\n        d['kn'] = np.ones(gn[0].num_cells) * kf / aperture\n\n#------------------------------------------------------------------------------#\n\n\ndef write_network(file_name):\n    network = \"FID,START_X,START_Y,END_X,END_Y\\n\"\n    network += \"0,0,0.5,1,0.5\\n\"\n    network += \"1,0.5,0,0.5,1\\n\"\n    network += \"2,0.5,0.75,1,0.75\\n\"\n    network += \"3,0.75,0.5,0.75,1\\n\"\n    network += \"4,0.5,0.625,0.75,0.625\\n\"\n    network += \"5,0.625,0.5,0.625,0.75\\n\"\n    with open(file_name, \"w\") as text_file:\n        text_file.write(network)\n\n#------------------------------------------------------------------------------#\n\n\ndef main(kf, description, mesh_size):\n    mesh_kwargs = {}\n    mesh_kwargs['mesh_size'] = {'mode': 'constant',\n                                'value': mesh_size, 'bound_value': mesh_size}\n\n    domain = {'xmin': 0, 'xmax': 1, 'ymin': 0, 'ymax': 1}\n    if_coarse = True\n\n    folder = 'example_5_1_1_' + description\n\n    file_name = 'network_geiger.csv'\n    write_network(file_name)\n    gb = importer.from_csv(file_name, mesh_kwargs, domain)\n    gb.compute_geometry()\n\n    g_fine = gb.get_grids(lambda g: g.dim == gb.dim_max())[0].copy()\n\n    if if_coarse:\n        partition = co.create_aggregations(gb)\n        partition = co.reorder_partition(partition)\n        co.generate_coarse_grid(gb, partition)\n\n    gb.assign_node_ordering()\n\n    # Assign parameters\n    add_data(gb, domain, kf)\n\n    # Choose and define the solvers and coupler\n    solver = dual.DualVEMMixDim('flow')\n    A, b = solver.matrix_rhs(gb)\n\n    up = sps.linalg.spsolve(A, b)\n    solver.split(gb, \"up\", up)\n\n    gb.add_node_props([\"discharge\", 'pressure', \"P0u\"])\n    solver.extract_u(gb, \"up\", \"discharge\")\n    solver.extract_p(gb, \"up\", 'pressure')\n    solver.project_u(gb, \"discharge\", \"P0u\")\n\n    exporter.export_vtk(\n        gb, 'vem', ['pressure', \"P0u\"], folder=folder, binary=False)\n\n    if if_coarse:\n        partition = partition[gb.grids_of_dimension(gb.dim_max())[0]]\n        p = np.array([d['pressure']\n                      for g, d in gb if g.dim == gb.dim_max()]).ravel()\n        data = {'partition': partition, 'pressure': p[partition]}\n        exporter.export_vtk(g_fine, 'sub_grid', data, binary=False,\n                            folder=folder)\n\n    print(\"diam\", gb.diameter(lambda g: g.dim == gb.dim_max()))\n    print(\"num_cells 2d\", gb.num_cells(lambda g: g.dim == 2))\n    print(\"num_cells 1d\", gb.num_cells(lambda g: g.dim == 1))\n\n#------------------------------------------------------------------------------#\n\n\ndef vem_blocking():\n    kf = 1e-4\n    mesh_size = 0.035 / np.array([1, 2, 4])\n\n    for i in np.arange(mesh_size.size):\n        main(kf, \"blocking_\" + str(i), mesh_size[i])\n\n#------------------------------------------------------------------------------#\n\n\ndef vem_permeable():\n    kf = 1e4\n    mesh_size = 0.035 / np.array([1, 2, 4])\n\n    for i in np.arange(mesh_size.size):\n        main(kf, \"permeable_\" + str(i), mesh_size[i])\n\n#------------------------------------------------------------------------------#\n\n\nvem_blocking()\nvem_permeable()\n",
  "# -*- coding: utf-8 -*-\n\"\"\"\nExample experiment using a shutter and a spectrometer\n\nThis demonstrates how to carry out an experiment that runs for a while in \nthe background, without locking up the UI.\n\nrwb27, May 2016\n\n\"\"\"\n\nimport nplab\nimport nplab.utils.gui \nfrom nplab.instrument.spectrometer import Spectrometer\nfrom nplab.instrument.shutter import Shutter\nfrom nplab.experiment import Experiment, ExperimentStopped\nfrom nplab.utils.notified_property import DumbNotifiedProperty\nfrom nplab.ui.ui_tools import QuickControlBox\nfrom nplab.utils.gui import show_guis\nfrom nplab.utils.gui import QtWidgets, QtCore, QtGui, get_qt_app, uic\nfrom nplab.ui.ui_tools import UiTools\n\nclass DumbIrradiationExperiment(Experiment):\n    \"\"\"An example experiment that opens and closes a shutter, and takes spectra.\"\"\"\n    irradiation_time = DumbNotifiedProperty(1.0)\n    wait_time = DumbNotifiedProperty(0.5)\n    log_to_console = True\n    \n    def __init__(self):\n        super(DumbIrradiationExperiment, self).__init__()\n        \n        self.shutter = Shutter.get_instance()\n        self.spectrometer = Spectrometer.get_instance()\n        \n    def run(self):\n        try:\n            dg = self.create_data_group(\"irradiation_%d\")\n            while True:\n                self.log(\"opening shutter\")\n                self.shutter.open_shutter()\n                self.wait_or_stop(self.irradiation_time)\n                self.shutter.close_shutter()\n                self.log(\"closed shutter\")\n                self.wait_or_stop(self.wait_time)\n                spectrum = self.spectrometer.read_spectrum(bundle_metadata=True)\n                dg.create_dataset(\"spectrum_%d\", data=spectrum)\n        except ExperimentStopped:\n            pass #don't raise an error if we just clicked \"stop\"\n        finally:\n            self.shutter.close_shutter() #close the shutter, important if we abort\n            \n    def get_qt_ui(self):\n        \"\"\"Return a user interface for the experiment\"\"\"\n        gb = QuickControlBox(\"Irradiation Experiment\")\n        gb.add_doublespinbox(\"irradiation_time\")\n        gb.add_doublespinbox(\"wait_time\")\n        gb.add_button(\"start\")\n        gb.add_button(\"stop\")\n        gb.auto_connect_by_name(self)\n        return gb\n\nclass DumbIrradiationExperiment_Gui(QtWidgets.QMainWindow, UiTools):\n    \"\"\"\n    Import and editing of Pump probe gui including the replacement of widgets and formating of buttons\n    \"\"\"\n    #, lockin, XYstage, Zstage, spectrometer, stepper,\n    def __init__(self, spec,shutter, experiment, parent=None):\n        super(DumbIrradiationExperiment_Gui, self).__init__(parent)\n        #Load ui code\n        uic.loadUi('DumbIrradiationExperimentGui.ui', self)\n        \n        #grabbing the current H5PY and intiating the data_browser\n        self.data_file = nplab.current_datafile()\n        self.data_file_tab = self.replace_widget(self.DataBrowser_tab_layout,self.DataBrowser_widget,self.data_file.get_qt_ui())\n        \n        #setup spectrometer tab gui and widget\n        self.spectrometer = spec\n        self.Spectrometer_widget = self.replace_widget(self.Spectrometer_Layout,self.Spectrometer_widget,self.spectrometer.get_qt_ui(display_only = True))\n        self.spectrometer_tab = self.replace_widget(self.Spectrometer_tab_Layout,self.Spectrometer_tab_widget,self.spectrometer.get_qt_ui())\n        \n        #Setting up stepper and Lockin widget \n            # Display\n        self.Experiment = experiment\n        self.Experiment_controls_widget = self.replace_widget(self.Main_layout,self.Experiment_controls_widget,self.Experiment.get_qt_ui())\n            #Shutter control widget\n        self.shutter = shutter\n        self.StageControls_widget = self.replace_widget(self.Main_layout,self.shutter_controls_widget,self.shutter.get_qt_ui())\n\n\n    \n            \nif __name__ == '__main__':\n    from nplab.instrument.spectrometer import DummySpectrometer\n    from nplab.instrument.shutter import DummyShutter    \n    \n    spectrometer = DummySpectrometer()\n    shutter = DummyShutter()\n    \n    experiment = DumbIrradiationExperiment()\n    \n    df = nplab.current_datafile()\n    \n#    show_guis([spectrometer, shutter, experiment, df])\n    app = get_qt_app()\n    gui = DumbIrradiationExperiment_Gui(spec = spectrometer, shutter = shutter, experiment = experiment)\n    gui.show()    \n",
  "#!/usr/bin/env python\n\n'''\nShowing use of the parallelized CCSD with K-point sampling.\n'''\n\nimport numpy as np\nfrom pyscf.pbc import cc as pbccc\nfrom pyscf.pbc import scf as pbchf\nfrom pyscf.pbc import gto\nfrom pyscf.pbc.tools.pbc import super_cell\n\nnmp = [1, 1, 2]\ncell = gto.M(\n    unit='B',\n    a=[[0., 3.37013733, 3.37013733],\n       [3.37013733, 0., 3.37013733],\n       [3.37013733, 3.37013733, 0.]],\n    mesh=[24,]*3,\n    atom='''C 0 0 0\n              C 1.68506866 1.68506866 1.68506866''',\n    basis='gth-szv',\n    pseudo='gth-pade',\n    verbose=4\n)\n\n# We build a supercell composed of 'nmp' replicated units and run\n# our usual molecular Hartree-Fock program, but using integrals\n# between periodic gaussians.\n#cell = build_cell(ase_atom, ke=50., basis=basis)\nsupcell = super_cell(cell, nmp)\nmf = pbchf.RHF(supcell)\nmf.kernel()\nsupcell_energy = mf.energy_tot() / np.prod(nmp)\n\n# A wrapper calling molecular CC method for gamma point calculation.\nmycc = pbccc.RCCSD(mf)\ngccsd_energy = mycc.ccsd()[0] / np.prod(nmp)\neip, wip = mycc.ipccsd(nroots=2)\neea, wea = mycc.eaccsd(nroots=2)\n\n# We now begin our k-point calculations for the same system, making\n# sure we shift the k-points to be gamma-centered.\nkpts = cell.make_kpts(nmp)\nkpts -= kpts[0]\nkmf = pbchf.KRHF(cell, kpts)\nkpoint_energy = kmf.kernel()\n\nmykcc = pbccc.KRCCSD(kmf)\nkccsd_energy = mykcc.ccsd()[0]\nekcc = mykcc.ecc\n# We look at a gamma-point transition for IP/EA\nekip, wkip = mykcc.ipccsd(nroots=2, kptlist=[0])\nekea, wkea = mykcc.eaccsd(nroots=2, kptlist=[0])\n\nprint('Difference between gamma/k-point mean-field calculation = %.15g' % (\n    abs(supcell_energy-kpoint_energy)))\nprint('Difference between gamma/k-point ccsd calculation = %.15g' % (\n    abs(gccsd_energy - kccsd_energy)))\nprint('Difference between gamma/k-point ip-eomccsd calculation = %.15g' % (\n    np.linalg.norm(np.array(eip) - np.array(ekip))))\nprint('Difference between gamma/k-point ea-eomccsd calculation = %.15g' % (\n    np.linalg.norm(np.array(eea) - np.array(ekea))))\n",
  "'''Steepest ascent demo.\nApplies few steps of steepest ascent for the maximization of Poisson \nlog-likelihood objective function using subset gradients.\n\nUsage:\n  steepest_ascent [--help | options]\n\nOptions:\n  -e <engn>, --engine=<engn>  reconstruction engine [default: STIR]\n  -f <file>, --file=<file>    raw data file\n                              [default: my_forward_projection.hs]\n  -p <path>, --path=<path>    path to data files, defaults to data/examples/PET\n                              subfolder of SIRF root folder\n  -s <nstp>, --steps=<nstp>   number of steepest descent steps [default: 3]\n  -o, --optimal               use locally optimal steepest ascent\n  -v, --verbose               verbose\n  --non-interactive           do not show plots\n'''\n\n## SyneRBI Synergistic Image Reconstruction Framework (SIRF)\n## Copyright 2015 - 2020 Rutherford Appleton Laboratory STFC\n## Copyright 2015 - 2020 University College London.\n##\n## This is software developed for the Collaborative Computational\n## Project in Synergistic Reconstruction for Biomedical Imaging (formerly CCP PETMR)\n## (http://www.ccpsynerbi.ac.uk/).\n##\n## Licensed under the Apache License, Version 2.0 (the \"License\");\n##   you may not use this file except in compliance with the License.\n##   You may obtain a copy of the License at\n##       http://www.apache.org/licenses/LICENSE-2.0\n##   Unless required by applicable law or agreed to in writing, software\n##   distributed under the License is distributed on an \"AS IS\" BASIS,\n##   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n##   See the License for the specific language governing permissions and\n##   limitations under the License.\n\n__version__ = '0.1.0'\nfrom docopt import docopt\nargs = docopt(__doc__, version=__version__)\n\n# import engine module\nexec('from sirf.' + args['--engine'] + ' import *')\n\n\n# process command-line options\nsteps = int(args['--steps'])\nopt = args['--optimal']\nverbose = args['--verbose']\ndata_file = args['--file']\ndata_path = args['--path']\nif data_path is None:\n    data_path = examples_data_path('PET')\nraw_data_file = existing_filepath(data_path, data_file)\nshow_plot = not args['--non-interactive']\n\n\nif opt:\n    import scipy.optimize\n\n\ndef trunc(image):\n    arr = image.as_array()\n    arr[arr < 0 ] = 0\n    out = image.copy()\n    out.fill(arr)\n    return out\n\n\ndef main():\n\n    # engine's messages go to files\n    msg_red = MessageRedirector('info.txt', 'warn.txt', 'errr.txt')\n\n    # create acquisition model\n    acq_model = AcquisitionModelUsingRayTracingMatrix()\n\n    # PET acquisition data to be read from the file specified by --file option\n    print('raw data: %s' % raw_data_file)\n    acq_data = AcquisitionData(raw_data_file)\n\n    # create filter that zeroes the image outside a cylinder of the same\n    # diameter as the image xy-section size\n    filter = TruncateToCylinderProcessor()\n\n    # create initial image estimate\n    nx = 111\n    ny = 111\n    nz = 31\n    image_size = (nz, ny, nx)\n    voxel_size = (3.375, 3, 3) # sizes are in mm\n    image = ImageData()\n    image.initialise(image_size, voxel_size)\n    image.fill(1.0)\n    # apply the filter to the image\n    filter.apply(image)\n\n    # create objective function of Poisson logarithmic likelihood type\n    # compatible with the acquisition data type\n    obj_fun = make_Poisson_loglikelihood(acq_data)\n    obj_fun.set_acquisition_model(acq_model)\n    obj_fun.set_num_subsets(12)\n    obj_fun.set_up(image)\n\n    if show_plot:\n        # display the initial image\n        image.show(20)\n\n    print('computing initial objective function value...')\n    print('objective function value: %e' % (obj_fun.value(image)))\n\n    if verbose:\n        disp = 3\n        if opt:\n            print('NOTE: below f(x) is the negative of the objective function value')\n    else:\n        disp = 0\n    eps = 1e-6 # single precision round-off error level\n\n    for iter in range(steps):\n\n        # obtain gradient for subset = iter\n        grad = obj_fun.get_subset_gradient(image, iter % 12)\n        # zero the gradient outside the cylindric FOV\n        filter.apply(grad)\n\n        # compute step size bazed on an estimate of the largest\n        # eigenvalue lmd_max of the Hessian H\n        # note that lmd_max = max |H v|/|v|\n        if iter == 0:\n            image0 = image\n            grad0 = grad\n            # in the quadratic case F(v) = (H v, v)/2,\n            # grad F(v) = H v, hence a rough idea about lmd_max \n            # is given by\n            lmd_max = 2*grad.norm()/image.norm()\n            tau = 1/lmd_max\n            maxstep = tau\n        else:\n            di = image - image0\n            dg = grad - grad0 \n            # dg = H di, hence a rough idea about lmd_max is given by\n            lmd_max = 2*dg.norm()/di.norm()\n            # alternative smaller estimate for lmd_max is\n            #lmd_max = -2*dg.dot(di)/di.dot(di)\n            tau = min(maxstep, 1/lmd_max)\n\n        if opt:\n            # find the optimal step size tau\n            fun = lambda x: -obj_fun.value(image + x*grad)\n            tau = scipy.optimize.fminbound \\\n                (fun, 0, 2*maxstep, xtol = 1e-4, maxfun = 4, disp = disp)\n\n        print('using step size %f' % tau)\n\n        # perform truncated steepest descent step\n        new_image = trunc(image + tau*grad)\n        diff = new_image - image\n        rc = diff.norm()/image.norm()\n        print('step %d, change in image %e' % (iter, rc))\n        image = new_image\n        # filter the new image\n        filter.apply(image)\n\n        if show_plot:\n            # display the current image estimate\n            image.show(20)\n\n    if not opt or disp == 0:\n        print('computing attained objective function value...')\n        print('objective function value: %e' % (obj_fun.value(image)))\n\n\n# if anything goes wrong, an exception will be thrown \n# (cf. Error Handling section in the spec)\ntry:\n    main()\n    print('\\n=== done with %s' % __file__)\n\nexcept error as err:\n    # display error information\n    print('%s' % err.value)\n",
  "#!/usr/bin/env python\nr\"\"\"\nIn this example we solve a FOCUS like Stage II coil optimisation problem for finite build coils.\nWe approximate each finite build coil using a multifilament approach. To model\nthe multilament pack we follow the approach of\n\n    Optimization of finite-build stellarator coils,\n    Singh, Luquant, et al.  Journal of Plasma Physics 86.4 (2020).\n\nThis means, that in addition to the degrees of freedom for the shape of the\ncoils, we have additional degrees of freedom for the rotation of the coil pack.\n\nThe objective is given by\n\n    J = (1/2) ∫ |B_{BiotSavart}·n - B_{External}·n|^2 ds\n        + LENGTH_PEN * Σ ½(CurveLength - L0)^2\n        + DIST_PEN * PairwiseDistancePenalty\n\nThe target equilibrium is the QA configuration of\n\n    Magnetic fields with precise quasisymmetry for plasma confinement,\n    Landreman, M., & Paul, E. (2022), Physical Review Letters, 128(3), 035001.\n\n\"\"\"\n\nimport os\nimport numpy as np\nfrom pathlib import Path\nfrom scipy.optimize import minimize\nfrom simsopt.field import BiotSavart, Current, Coil, apply_symmetries_to_curves, apply_symmetries_to_currents\nfrom simsopt.geo import (curves_to_vtk, create_equally_spaced_curves, create_multifilament_grid,\n                         CurveLength, CurveCurveDistance, SurfaceRZFourier)\nfrom simsopt.objectives import QuadraticPenalty, SquaredFlux\nfrom simsopt.util import in_github_actions\n\n# Number of unique coil shapes, i.e. the number of coils per half field period:\n# (Since the configuration has nfp = 2, multiply by 4 to get the total number of coils.)\nncoils = 4\n\n# Major radius for the initial circular coils:\nR0 = 1.00\n\n# Minor radius for the initial circular coils:\nR1 = 0.70\n\n# Number of Fourier modes describing each Cartesian component of each coil:\norder = 5\n\n# Weight on the curve length penalty in the objective function:\nLENGTH_PEN = 1e-2\n\n# Threshhold and weight for the coil-to-coil distance penalty in the objective function:\nDIST_MIN = 0.1\nDIST_PEN = 10\n\n# Settings for multifilament approximation.  In the following\n# parameters, note that \"normal\" and \"binormal\" refer not to the\n# Frenet frame but rather to the \"coil centroid frame\" defined by\n# Singh et al., before rotation.\nnumfilaments_n = 2  # number of filaments in normal direction\nnumfilaments_b = 3  # number of filaments in bi-normal direction\ngapsize_n = 0.02  # gap between filaments in normal direction\ngapsize_b = 0.04  # gap between filaments in bi-normal direction\nrot_order = 1  # order of the Fourier expression for the rotation of the filament pack, i.e. maximum Fourier mode number\n\n# Number of iterations to perform:\nMAXITER = 50 if in_github_actions else 400\n\n#######################################################\n# End of input parameters.\n#######################################################\n\n# File for the desired boundary magnetic surface:\nTEST_DIR = (Path(__file__).parent / \"..\" / \"..\" / \"tests\" / \"test_files\").resolve()\nfilename = TEST_DIR / 'input.LandremanPaul2021_QA'\n\n# Directory for output\nOUT_DIR = \"./output/\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nconfig_str = f\"rot_order_{rot_order}_nfn_{numfilaments_n}_nfb_{numfilaments_b}\"\n\n# Initialize the boundary magnetic surface:\nnphi = 32\nntheta = 32\ns = SurfaceRZFourier.from_vmec_input(filename, range=\"half period\", nphi=nphi, ntheta=ntheta)\n\nnfil = numfilaments_n * numfilaments_b\nbase_curves = create_equally_spaced_curves(ncoils, s.nfp, stellsym=True, R0=R0, R1=R1, order=order)\nbase_currents = []\nfor i in range(ncoils):\n    curr = Current(1.)\n    # since the target field is zero, one possible solution is just to set all\n    # currents to 0. to avoid the minimizer finding that solution, we fix one\n    # of the currents\n    if i == 0:\n        curr.fix_all()\n    base_currents.append(curr * (1e5/nfil))\n\n# use sum here to concatenate lists\nbase_curves_finite_build = sum([\n    create_multifilament_grid(c, numfilaments_n, numfilaments_b, gapsize_n, gapsize_b, rotation_order=rot_order) for c in base_curves], [])\nbase_currents_finite_build = sum([[c]*nfil for c in base_currents], [])\n\n# apply stellarator and rotation symmetries\ncurves_fb = apply_symmetries_to_curves(base_curves_finite_build, s.nfp, True)\ncurrents_fb = apply_symmetries_to_currents(base_currents_finite_build, s.nfp, True)\n# also apply symmetries to the underlying base curves, as we use those in the\n# curve-curve distance penalty\ncurves = apply_symmetries_to_curves(base_curves, s.nfp, True)\n\ncoils_fb = [Coil(c, curr) for (c, curr) in zip(curves_fb, currents_fb)]\nbs = BiotSavart(coils_fb)\nbs.set_points(s.gamma().reshape((-1, 3)))\n\ncurves_to_vtk(curves, OUT_DIR + \"curves_init\")\ncurves_to_vtk(curves_fb, OUT_DIR + f\"curves_init_fb_{config_str}\")\n\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(OUT_DIR + f\"surf_init_fb_{config_str}\", extra_data=pointData)\n\n# Define the objective function:\nJf = SquaredFlux(s, bs)\nJls = [CurveLength(c) for c in base_curves]\nJdist = CurveCurveDistance(curves, DIST_MIN)\n\n# Form the total objective function. To do this, we can exploit the\n# fact that Optimizable objects with J() and dJ() functions can be\n# multiplied by scalars and added:\nJF = Jf \\\n    + LENGTH_PEN * sum(QuadraticPenalty(Jls[i], Jls[i].J(), \"max\") for i in range(len(base_curves))) \\\n    + DIST_PEN * Jdist\n\n# We don't have a general interface in SIMSOPT for optimisation problems that\n# are not in least-squares form, so we write a little wrapper function that we\n# pass directly to scipy.optimize.minimize\n\n\ndef fun(dofs):\n    JF.x = dofs\n    J = JF.J()\n    grad = JF.dJ()\n    cl_string = \", \".join([f\"{J.J():.3f}\" for J in Jls])\n    mean_AbsB = np.mean(bs.AbsB())\n    jf = Jf.J()\n    kap_string = \", \".join(f\"{np.max(c.kappa()):.1f}\" for c in base_curves)\n    print(f\"J={J:.3e}, Jflux={jf:.3e}, sqrt(Jflux)/Mean(|B|)={np.sqrt(jf)/mean_AbsB:.3e}, CoilLengths=[{cl_string}], [{kap_string}], ||∇J||={np.linalg.norm(grad):.3e}\")\n    return 1e-4*J, 1e-4*grad\n\n\nprint(\"\"\"\n################################################################################\n### Perform a Taylor test ######################################################\n################################################################################\n\"\"\")\nf = fun\ndofs = JF.x\nnp.random.seed(1)\nh = np.random.uniform(size=dofs.shape)\nJ0, dJ0 = f(dofs)\ndJh = sum(dJ0 * h)\nfor eps in [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]:\n    J1, _ = f(dofs + eps*h)\n    J2, _ = f(dofs - eps*h)\n    print(\"err\", (J1-J2)/(2*eps) - dJh)\n\nprint(\"\"\"\n################################################################################\n### Run the optimisation #######################################################\n################################################################################\n\"\"\")\n\nres = minimize(fun, dofs, jac=True, method='L-BFGS-B', options={'maxiter': MAXITER, 'maxcor': 400, 'gtol': 1e-20, 'ftol': 1e-20}, tol=1e-20)\n\ncurves_to_vtk(curves_fb, OUT_DIR + f\"curves_opt_fb_{config_str}\")\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(OUT_DIR + f\"surf_opt_fb_{config_str}\", extra_data=pointData)\n",
  "# ==========================================================================\n#  AIDA Detector description implementation\n# --------------------------------------------------------------------------\n# Copyright (C) Organisation europeenne pour la Recherche nucleaire (CERN)\n# All rights reserved.\n#\n# For the licensing terms see $DD4hepINSTALL/LICENSE.\n# For the list of contributors see $DD4hepINSTALL/doc/CREDITS.\n#\n# ==========================================================================\n#\nfrom __future__ import absolute_import, unicode_literals\nimport logging\n#\nlogging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n#\n#\n\"\"\"\n\n   dd4hep simulation example setup using the python configuration\n\n\"\"\"\n\n\ndef run():\n  import os\n  import DDG4\n  from DDG4 import OutputLevel as Output\n  from g4units import GeV, keV\n\n  kernel = DDG4.Kernel()\n  install_dir = os.environ['DD4hepExamplesINSTALL']\n  kernel.loadGeometry(str(\"file:\" + install_dir + \"/examples/ClientTests/compact/SiliconBlock.xml\"))\n\n  DDG4.importConstants(kernel.detectorDescription(), debug=False)\n  geant4 = DDG4.Geant4(kernel, tracker='Geant4TrackerCombineAction')\n  geant4.printDetectors()\n  # Configure UI\n  geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=False)\n\n  # Configure field\n  geant4.setupTrackingField(prt=True)\n  # Configure Event actions\n  prt = DDG4.EventAction(kernel, 'Geant4ParticlePrint/ParticlePrint')\n  prt.OutputLevel = Output.DEBUG\n  prt.OutputType = 3  # Print both: table and tree\n  kernel.eventAction().adopt(prt)\n\n  generator_output_level = Output.INFO\n\n  # Configure G4 geometry setup\n  seq, act = geant4.addDetectorConstruction(\"Geant4DetectorGeometryConstruction/ConstructGeo\")\n  act.DebugMaterials = True\n  act.DebugElements = False\n  act.DebugVolumes = True\n  act.DebugShapes = True\n  act.DebugSurfaces = True\n\n  # Setup particle gun\n  gun = geant4.setupGun(\"Gun\", particle='gamma', energy=1 * GeV, multiplicity=1)\n  gun.direction = (0.0, 0.0, 1.0)\n  gun.OutputLevel = generator_output_level\n  kernel.NumEvents = 10\n  # Instantiate the stepping action\n  stepping = DDG4.SteppingAction(kernel, 'TestSteppingAction/MyStepper')\n  kernel.steppingAction().add(stepping)\n\n  # And handle the simulation particles.\n  part = DDG4.GeneratorAction(kernel, \"Geant4ParticleHandler/ParticleHandler\")\n  kernel.generatorAction().adopt(part)\n  part.SaveProcesses = ['conv', 'Decay']\n  part.MinimalKineticEnergy = 1 * keV\n  part.KeepAllParticles = False\n  part.PrintEndTracking = True\n  part.enableUI()\n\n  # Now build the physics list:\n  phys = geant4.setupPhysics('QGSP_BERT')\n  phys.dump()\n  # Start the engine...\n  geant4.execute()\n\n\nif __name__ == \"__main__\":\n  run()\n",
  "import blenderproc as bproc\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('blend_path', nargs='?', default=\"resources/haven/models/ArmChair_01/ArmChair_01.blend\", help=\"Path to the blend file, from the haven dataset, browse the model folder, for all possible options\")\nparser.add_argument('haven_path', nargs='?', default=\"resources/haven\", help=\"The folder where the `hdri` folder can be found, to load an world environment\")\nparser.add_argument('output_dir', nargs='?', default=\"examples/datasets/haven/output\", help=\"Path to where the final files will be saved\")\nargs = parser.parse_args()\n\nbproc.init()\n\n# Load the object into the scene\nobjs = bproc.loader.load_blend(args.blend_path)\n\n# Set a random hdri from the given haven directory as background\nhaven_hdri_path = bproc.loader.get_random_world_background_hdr_img_path_from_haven(args.haven_path)\nbproc.world.set_world_background_hdr_img(haven_hdri_path)\n\n# define a light and set its location and energy level\nlight = bproc.types.Light()\nlight.set_type(\"POINT\")\nlight.set_location([5, -5, 5])\nlight.set_energy(1000)\n\n# Find point of interest, all cam poses should look towards it\npoi = bproc.object.compute_poi(objs)\n# Sample five camera poses\nfor i in range(5):\n    # Sample random camera location around the object\n    location = bproc.sampler.part_sphere([0, 0, 0], radius=3, part_sphere_dir_vector=[1, 0, 0], mode=\"SURFACE\")\n    # Compute rotation based on vector going from location towards poi\n    rotation_matrix = bproc.camera.rotation_from_forward_vec(poi - location)\n    # Add homog cam pose based on location an rotation\n    cam2world_matrix = bproc.math.build_transformation_mat(location, rotation_matrix)\n    bproc.camera.add_camera_pose(cam2world_matrix)\n\n# activate normal and depth rendering\nbproc.renderer.enable_normals_output()\nbproc.renderer.enable_depth_output(activate_antialiasing=False)\n\n# render the whole pipeline\ndata = bproc.renderer.render()\n\n# write the data to a .hdf5 container\nbproc.writer.write_hdf5(args.output_dir, data)\n",
  "\"\"\"\n This file is part of nucypher.\n\n nucypher is free software: you can redistribute it and/or modify\n it under the terms of the GNU Affero General Public License as published by\n the Free Software Foundation, either version 3 of the License, or\n (at your option) any later version.\n\n nucypher is distributed in the hope that it will be useful,\n but WITHOUT ANY WARRANTY; without even the implied warranty of\n MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n GNU Affero General Public License for more details.\n\n You should have received a copy of the GNU Affero General Public License\n along with nucypher.  If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\nimport base64\nimport datetime\nimport json\nimport os\nimport shutil\nfrom getpass import getpass\nfrom pathlib import Path\n\nimport maya\n\nfrom nucypher.blockchain.eth.signers import Signer\nfrom nucypher.characters.lawful import Bob, Alice\nfrom nucypher.policy.payment import SubscriptionManagerPayment\nfrom nucypher.utilities.ethereum import connect_web3_provider\nfrom nucypher.utilities.logging import GlobalLoggerSettings\n\n######################\n# Boring setup stuff #\n######################\n\nLOG_LEVEL = 'info'\nGlobalLoggerSettings.set_log_level(log_level_name=LOG_LEVEL)\nGlobalLoggerSettings.start_console_logging()\n\nTEMP_ALICE_DIR = Path('/', 'tmp', 'heartbeat-demo-alice')\nPOLICY_FILENAME = \"policy-metadata.json\"\nshutil.rmtree(TEMP_ALICE_DIR, ignore_errors=True)\n\ntry:\n\n    # Replace with ethereum RPC endpoint\n    L1_PROVIDER = os.environ['DEMO_L1_PROVIDER_URI']\n    L2_PROVIDER = os.environ['DEMO_L2_PROVIDER_URI']\n\n    # Replace with wallet filepath.\n    WALLET_FILEPATH = os.environ['DEMO_L2_WALLET_FILEPATH']\n    SIGNER_URI = f'keystore://{WALLET_FILEPATH}'\n\n    # Replace with alice's ethereum address\n    ALICE_ADDRESS = os.environ['DEMO_ALICE_ADDRESS']\n\nexcept KeyError:\n    raise RuntimeError('Missing environment variables to run demo.')\n\nL1_NETWORK = 'mainnet'  # 'tapir'\nL2_NETWORK = 'polygon'  # 'mumbai'\n\n\n#######################################\n# Alicia, the Authority of the Policy #\n#######################################\n\nconnect_web3_provider(eth_provider_uri=L1_PROVIDER)  # Connect to the ethereum provider.\nconnect_web3_provider(eth_provider_uri=L2_PROVIDER)  # Connect to the layer 2 provider.\n\n\n# Setup and unlock alice's ethereum wallet.\n# WARNING: Never give your mainnet password or mnemonic phrase to anyone.\n# Do not use mainnet keys, create a dedicated software wallet to use for this demo.\nwallet = Signer.from_signer_uri(SIGNER_URI)\npassword = os.environ.get('DEMO_ALICE_PASSWORD') or getpass(f\"Enter password to unlock Alice's wallet ({ALICE_ADDRESS[:8]}): \")\nwallet.unlock_account(account=ALICE_ADDRESS, password=password)\n\n# This is Alice's payment method.\npayment_method = SubscriptionManagerPayment(\n    network=L2_NETWORK,\n    eth_provider=L2_PROVIDER\n)\n\n# This is Alicia.\nalicia = Alice(\n    checksum_address=ALICE_ADDRESS,\n    signer=wallet,\n    domain=L1_NETWORK,\n    eth_provider_uri=L1_PROVIDER,\n    payment_method=payment_method\n)\n\n# Alice puts her public key somewhere for Bob to find later...\nalice_verifying_key = alicia.stamp.as_umbral_pubkey()\n\n# Let's get to learn about the NuCypher network\nalicia.start_learning_loop(now=True)\n\n# At this point, Alicia is fully operational and can create policies.\n# The Policy Label is a bytestring that categorizes the data that Alicia wants to share.\n# Note: we add some random chars to create different policies, only for demonstration purposes\nlabel = \"heart-data-❤️-\"+os.urandom(4).hex()\nlabel = label.encode()\n\n# Alicia can create the public key associated to the policy label,\n# even before creating any associated policy.\npolicy_pubkey = alicia.get_policy_encrypting_key_from_label(label)\n\nprint(\"The policy public key for \"\n      \"label '{}' is {}\".format(label.decode(\"utf-8\"), bytes(policy_pubkey).hex()))\n\n# Data Sources can produce encrypted data for access policies\n# that **don't exist yet**.\n# In this example, we create a local file with encrypted data, containing\n# heart rate measurements from a heart monitor\nimport heart_monitor\nheart_monitor.generate_heart_rate_samples(policy_pubkey,\n                                          samples=50,\n                                          save_as_file=True)\n\n\n# Alicia now wants to share data associated with this label.\n# To do so, she needs the public key of the recipient.\n# In this example, we generate it on the fly (for demonstration purposes)\nfrom doctor_keys import get_doctor_pubkeys\ndoctor_pubkeys = get_doctor_pubkeys()\n\n# We create a view of the Bob who's going to be granted access.\ndoctor_strange = Bob.from_public_keys(verifying_key=doctor_pubkeys['sig'],\n                                      encrypting_key=doctor_pubkeys['enc'],\n                                      federated_only=True)\n\n# Here are our remaining Policy details, such as:\n# - Policy expiration date\npolicy_end_datetime = maya.now() + datetime.timedelta(days=1)\n# - m-out-of-n: This means Alicia splits the re-encryption key in 5 pieces and\n#               she requires Bob to seek collaboration of at least 3 Ursulas\nthreshold, shares = 2, 3\n\n\n# With this information, Alicia creates a policy granting access to Bob.\n# The policy is sent to the NuCypher network.\nprint(\"Creating access policy for the Doctor...\")\npolicy = alicia.grant(bob=doctor_strange,\n                      label=label,\n                      threshold=threshold,\n                      shares=shares,\n                      expiration=policy_end_datetime)\nprint(\"Done!\")\n\n# For the demo, we need a way to share with Bob some additional info\n# about the policy, so we store it in a JSON file\npolicy_info = {\n    \"policy_pubkey\": bytes(policy.public_key).hex(),\n    \"alice_sig_pubkey\": bytes(alicia.stamp).hex(),\n    \"label\": label.decode(\"utf-8\"),\n    \"treasure_map\": base64.b64encode(bytes(policy.treasure_map)).decode()\n}\n\nfilename = POLICY_FILENAME\nwith open(filename, 'w') as f:\n    json.dump(policy_info, f)\n",
  "# -*- coding: utf-8 -*-\n#\n#    BitcoinLib - Python Cryptocurrency Library\n#\n#    EXAMPLES - Encoding helper methods\n#\n#    © 2017 September - 1200 Web Development <http://1200wd.com/>\n#\n\nfrom bitcoinlib.encoding import *\n\n#\n# Change Base conversion examples\n#\nexamples = [\n    ('4c52127a72fb42b82439ab18697dcfcfb96ac63ba8209833b2e29f2302b8993f45e743412d65c7a571da70259d4f6795e98af20e6e'\n     '57603314a662a49c198199', 16, 256),\n    ('LR\u0012zrûB¸$9«\u0018i}ÏÏ¹jÆ;¨ 3²â#\u0002¸?EçCA-eÇ¥qÚp%Ogéò\u000enW`3\u0014¦b¤\u0019', 256, 16),\n    # ('LR\u0012zrûB¸$9«\u0018i}ÏÏ¹jÆ;¨ 3²â#\u0002¸?EçCA-eÇ¥qÚp%Ogéò\u000enW`3\u0014¦b¤\u0019', 16, 256),  # Test EncodingError\n    ('L1odb1uUozbfK2NrsMyhJfvRsxGM2AxixgPL8vG9BUBnE6W1VyTX', 58, 16),\n    ('FF', 16, 10),\n    ('AF', 16, 2),\n    (200, 10, 16, 2),\n    (200, 10, 16, 4),\n    ('thisisfunny', 32, 3),\n    ('1LeNnaRtV52nNtZXvtw6PaGKpk46hU1Xmx', 58, 16),\n    ('1LeNnaRtV52nNtZXvtw6PaGKpk46hU1Xmx', 58, 32),\n    ('1LeNnaRtV52nNtZXvtw6PaGKpk46hU1Xmx', 58, 256),\n    ('1LeNnaRtV52nNtZXvtw6PaGKpk46hU1Xmx', 58, 2048),\n    ([b'\\0', b'\\x12', b'L'], 256, 16, 6),\n    (\"為 宋 暴 治 伯 及 灘 冶 忙 逃 湘 艇 例 讓 忠\", 256, 16),\n    (b'\\x00\\t\\xc6\\xe7\\x11\\x18\\xd8\\xf1+\\xeck\\\\a\\x88K5g|\\n\\n\\xe3*\\x02\\x1f\\x87', 256, 58),\n    (b'\\0', 256, 10),\n    (\"\\x00\\x01\\tfw`\\x06\\x95=UgC\\x9e^9\\xf8j\\r';\\xee\\xd6\\x19g\\xf6\", 256, 58),\n    (b'LR\\x12zr\\xfbB\\xb8$9\\xab\\x18i}\\xcf\\xcf\\xb9j\\xc6;\\xa8 \\x983\\xb2\\xe2\\x9f#\\x02\\xb8\\x99?E\\xe7CA-e\\xc7\\xa5q'\n     b'\\xdap%\\x9dOg\\x95\\xe9\\x8a\\xf2\\x0enW`3\\x14\\xa6b\\xa4\\x9c\\x19\\x81\\x99', 256, 16),\n]\n\nprint(\"\\n=== Change base: convert from base N to base M ===\")\nfor example in examples:\n    print(\"\\n>>> change_base%s     # Change from base%d to base%d\" %\n          (example, example[1], example[2]))\n    print(\"%s\" % change_base(*example))\n\n#\n# Address and Script conversion examples\n#\nprint(\"\\n=== Conversion of Bitcoin Addresses to Public Key Hashes ===\")\naddrs = ['1KcBA4i4Qqu1oRjobyWU3R5UXUorLQ3jUg', '1111111111111111111114oLvT2',\n         '1QLbz7JHiBTspS962RLKV8GndWFwi5j6Qr']\nfor addr in addrs:\n    print(\"Public Key Hash of address '%s' is '%s'\" % (addr, addr_to_pubkeyhash(addr, True)))\n\nprint(\"\\n=== From Public Key Hashes to address ===\")\nprint(pubkeyhash_to_addr('13d215d212cd5188ae02c5635faabdc4d7d4ec91'))\nprint(pubkeyhash_to_addr('00' * 20))\n\nprint(\"\\n=== Create PKH from redeemscript ===\")\nredeemscript = '5221023dd6aeaa2acb92cbea35820361e5fd07af10f4b01c985adec30848b424756a6c210381cd2bb2a38d939fa677a5dcc' \\\n               '981ee0630b32b956b2e6dc3e1c028e6d09db5a72103d2c6d31cabe4025c25879010465f501194b352823c553660d303adfa' \\\n               '9a26ad3c53ae'\nprint(to_hexstring(hash160(to_bytes(redeemscript))))\n\n#\n# Other type conversions and normalizations\n#\n\nder_signature = '3045022100f952ff1b290c54d8b9fd35573b50f1af235632c595bb2f10b34127fb82f66d18022068b59150f825a81032c' \\\n                '22ce2db091d6fd47294c9e2144fa0291949402e3003ce'\nprint(\"\\n=== Convert DER encoded signature ===\")\nprint(convert_der_sig(to_bytes(der_signature)))\n\nprint(\"\\n=== Varbyte Int conversions ===\")\nprint(\"Number 1000 as Varbyte Integer (hexstring): %s\" % to_hexstring(int_to_varbyteint(1000)))\nprint(\"Converted back (3 is size in bytes: 1 size byte + integer in bytes): \", varbyteint_to_int(to_bytes('fde803')))\n\n# Normalize data\nprint(\"\\n=== Normalizations ===\")\ndata = [\n    u\"guion cruz envío papel otoño percha hazaña salir joya gorra íntimo actriz\",\n    u'\\u2167',\n    u'\\uFDFA',\n    \"あじわう　ちしき　たわむれる　おくさま　しゃそう　うんこう　ひてい　みほん　たいほ　てのひら　りこう　わかれる　かいすいよく　こもん　ねもと\",\n    '12345',\n]\n\nfor dt in data:\n    print(\"\\nInput data\", dt)\n    print(\"Normalized unicode string (normalize_string): \", normalize_string(dt))\n    print(\"Normalized variable (normalize_var): \", normalize_var(dt))\n\n\n# Convert Bech32 address to Public key hash\naddress = \"BC1QW508D6QEJXTDG4Y5R3ZARVARY0C5XW7KV8F3T4\"\npkh = \"0014751e76e8199196d454941c45d1b3a323f1433bd6\"\npkh_converted = addr_bech32_to_pubkeyhash(address, prefix='bc', include_witver=True, as_hex=True)\nprint(pkh, \" == \", pkh_converted)\naddr = pubkeyhash_to_addr_bech32(pkh_converted, address[:2].lower())\n",
  "################################################################################\n# Copyright (c) 2022 ContinualAI.                                              #\n# Copyrights licensed under the MIT License.                                   #\n# See the accompanying LICENSE file for terms.                                 #\n#                                                                              #\n# Date: 05-17-2022                                                             #\n# Author: Jia Shi, Zhiqiu Lin                                                  #\n# E-mail: jiashi@andrew.cmu.edu, zl279@cornell.edu                             #\n# Website: https://clear-benchmark.github.io                                   #\n################################################################################\n\n\"\"\"\nExample: Training and evaluating on CLEAR benchmark (RGB images)\n\"\"\"\n\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nimport torchvision\n\nfrom avalanche.evaluation.metrics import (\n    forgetting_metrics,\n    accuracy_metrics,\n    loss_metrics,\n    timing_metrics,\n    cpu_usage_metrics,\n    confusion_matrix_metrics,\n    disk_usage_metrics,\n)\nfrom avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\nfrom avalanche.training.plugins import EvaluationPlugin\nfrom avalanche.training.plugins.lr_scheduling import LRSchedulerPlugin\nfrom avalanche.training.supervised import Naive\nfrom avalanche.benchmarks.classic.clear import CLEAR, CLEARMetric\n\n\n# For CLEAR dataset setup\nDATASET_NAME = \"clear100_cvpr2022\"\nNUM_CLASSES = {\n    \"clear10_neurips_2021\": 11,\n    \"clear100_cvpr2022\": 100,\n    \"clear10\": 11,\n    \"clear100\": 100,\n}\nassert DATASET_NAME in NUM_CLASSES.keys()\n\n# please refer to paper for discussion on streaming v.s. iid protocol\nEVALUATION_PROTOCOL = \"streaming\"  # trainset = testset per timestamp\n# EVALUATION_PROTOCOL = \"iid\"  # 7:3 trainset_size:testset_size\n\n# For saving the datasets/models/results/log files\nROOT = Path(\"..\")\nDATA_ROOT = ROOT / DATASET_NAME\nMODEL_ROOT = ROOT / \"models\"\nDATA_ROOT.mkdir(parents=True, exist_ok=True)\nMODEL_ROOT.mkdir(parents=True, exist_ok=True)\n\n# Define hyperparameters/scheduler/augmentation\nHPARAM = {\n    \"batch_size\": 256,\n    \"num_epoch\": 100,\n    \"step_scheduler_decay\": 30,\n    \"scheduler_step\": 0.1,\n    \"start_lr\": 0.01,\n    \"weight_decay\": 1e-5,\n    \"momentum\": 0.9,\n}\n\n\ndef make_scheduler(optimizer, step_size, gamma=0.1):\n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer, step_size=step_size, gamma=gamma\n    )\n    return scheduler\n\n\ndef main():\n    model = torchvision.models.resnet18(pretrained=False)\n\n    normalize = torchvision.transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n    )\n    train_transform = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.Resize(224),\n            torchvision.transforms.RandomCrop(224),\n            torchvision.transforms.ToTensor(),\n            normalize,\n        ]\n    )\n    test_transform = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.Resize(224),\n            torchvision.transforms.CenterCrop(224),\n            torchvision.transforms.ToTensor(),\n            normalize,\n        ]\n    )\n\n    # log to Tensorboard\n    tb_logger = TensorboardLogger(ROOT)\n\n    # log to text file\n    text_logger = TextLogger(open(ROOT / \"log.txt\", \"w+\"))\n\n    # print to stdout\n    interactive_logger = InteractiveLogger()\n\n    eval_plugin = EvaluationPlugin(\n        accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        timing_metrics(epoch=True, epoch_running=True),\n        forgetting_metrics(experience=True, stream=True),\n        cpu_usage_metrics(experience=True),\n        confusion_matrix_metrics(\n            num_classes=NUM_CLASSES[DATASET_NAME], save_image=False, stream=True\n        ),\n        disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        loggers=[interactive_logger, text_logger, tb_logger],\n    )\n\n    if EVALUATION_PROTOCOL == \"streaming\":\n        seed = None\n    else:\n        seed = 0\n\n    benchmark = CLEAR(\n        data_name=DATASET_NAME,\n        evaluation_protocol=EVALUATION_PROTOCOL,\n        feature_type=None,\n        seed=seed,\n        train_transform=train_transform,\n        eval_transform=test_transform,\n        dataset_root=DATA_ROOT,\n    )\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    optimizer = torch.optim.SGD(\n        model.parameters(),\n        lr=HPARAM[\"start_lr\"],\n        weight_decay=HPARAM[\"weight_decay\"],\n        momentum=HPARAM[\"momentum\"],\n    )\n    scheduler = make_scheduler(\n        optimizer,\n        HPARAM[\"step_scheduler_decay\"],\n        HPARAM[\"scheduler_step\"],\n    )\n\n    plugin_list = [LRSchedulerPlugin(scheduler)]\n    cl_strategy = Naive(\n        model,\n        optimizer,\n        torch.nn.CrossEntropyLoss(),\n        train_mb_size=HPARAM[\"batch_size\"],\n        train_epochs=HPARAM[\"num_epoch\"],\n        eval_mb_size=HPARAM[\"batch_size\"],\n        evaluator=eval_plugin,\n        device=device,\n        plugins=plugin_list,\n    )\n\n    # TRAINING LOOP\n    print(\"Starting experiment...\")\n    results = []\n    print(\"Current protocol : \", EVALUATION_PROTOCOL)\n    for index, experience in enumerate(benchmark.train_stream):\n        print(\"Start of experience: \", experience.current_experience)\n        print(\"Current Classes: \", experience.classes_in_this_experience)\n        res = cl_strategy.train(experience)\n        torch.save(\n            model.state_dict(), str(MODEL_ROOT / f\"model{str(index).zfill(2)}.pth\")\n        )\n        print(\"Training completed\")\n        print(\n            \"Computing accuracy on the whole test set with\"\n            f\" {EVALUATION_PROTOCOL} evaluation protocol\"\n        )\n        results.append(cl_strategy.eval(benchmark.test_stream))\n    # generate accuracy matrix\n    num_timestamp = len(results)\n    accuracy_matrix = np.zeros((num_timestamp, num_timestamp))\n    for train_idx in range(num_timestamp):\n        for test_idx in range(num_timestamp):\n            accuracy_matrix[train_idx][test_idx] = results[train_idx][\n                f\"Top1_Acc_Stream/eval_phase/test_stream\"\n                f\"/Task00{test_idx}/Exp00{test_idx}\"\n            ]\n    print(\"Accuracy_matrix : \")\n    print(accuracy_matrix)\n    metric = CLEARMetric().get_metrics(accuracy_matrix)\n    print(metric)\n\n    metric_log = open(ROOT / \"metric_log.txt\", \"w+\")\n    metric_log.write(f\"Protocol: {EVALUATION_PROTOCOL} \" f\"Seed: {seed} \")\n    json.dump(accuracy_matrix.tolist(), metric_log, indent=6)\n    json.dump(metric, metric_log, indent=6)\n    metric_log.close()\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "import blenderproc as bproc\nimport os\nimport numpy as np\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('scene_net_obj_path', help=\"Path to the used scene net `.obj` file, download via scripts/download_scenenet.py\")\nparser.add_argument('scene_texture_path', help=\"Path to the downloaded texture files, you can find them at http://tinyurl.com/zpc9ppb\")\nparser.add_argument('output_dir', nargs='?', default=\"examples/datasets/scenenet/output\", help=\"Path to where the final files, will be saved\")\nargs = parser.parse_args()\n\nbproc.init()\n\n# Load the scenenet room and label its objects with category ids based on the nyu mapping\nlabel_mapping = bproc.utility.LabelIdMapping.from_csv(bproc.utility.resolve_resource(os.path.join('id_mappings', 'nyu_idset.csv')))\nobjs = bproc.loader.load_scenenet(args.scene_net_obj_path, args.scene_texture_path, label_mapping)\n\n# In some scenes floors, walls and ceilings are one object that needs to be split first\n# Collect all walls\nwalls = bproc.filter.by_cp(objs, \"category_id\", label_mapping.id_from_label(\"wall\"))\n# Extract floors from the objects\nnew_floors = bproc.object.extract_floor(walls, new_name_for_object=\"floor\", should_skip_if_object_is_already_there=True)\n# Set category id of all new floors\nfor floor in new_floors:\n    floor.set_cp(\"category_id\", label_mapping.id_from_label(\"floor\"))\n# Add new floors to our total set of objects\nobjs += new_floors\n\n# Extract ceilings from the objects\nnew_ceilings = bproc.object.extract_floor(walls, new_name_for_object=\"ceiling\", up_vector_upwards=False, should_skip_if_object_is_already_there=True)\n# Set category id of all new ceiling\nfor ceiling in new_ceilings:\n    ceiling.set_cp(\"category_id\", label_mapping.id_from_label(\"ceiling\"))\n# Add new ceilings to our total set of objects\nobjs += new_ceilings\n\n# Make all lamp objects emit light\nlamps = bproc.filter.by_attr(objs, \"name\", \".*[l|L]amp.*\", regex=True)\nbproc.lighting.light_surface(lamps, emission_strength=15)\n# Also let all ceiling objects emit a bit of light, so the whole room gets more bright\nceilings = bproc.filter.by_attr(objs, \"name\", \".*[c|C]eiling.*\", regex=True)\nbproc.lighting.light_surface(ceilings, emission_strength=2, emission_color=[1,1,1,1])\n\n# Init bvh tree containing all mesh objects\nbvh_tree = bproc.object.create_bvh_tree_multi_objects(objs)\n\n# Find all floors in the scene, so we can sample locations above them\nfloors = bproc.filter.by_cp(objs, \"category_id\", label_mapping.id_from_label(\"floor\"))\nposes = 0\ntries = 0\nwhile tries < 10000 and poses < 5:\n    tries += 1\n    # Sample point above the floor in height of [1.5m, 1.8m]\n    location = bproc.sampler.upper_region(floors, min_height=1.5, max_height=1.8)\n    # Check that there is no object between the sampled point and the floor\n    _, _, _, _, hit_object, _ = bproc.object.scene_ray_cast(location, [0, 0, -1])\n    if hit_object not in floors:\n        continue\n\n    # Sample rotation (fix around X and Y axis)\n    rotation = np.random.uniform([1.2217, 0, 0], [1.2217, 0, 2 * np.pi])\n    cam2world_matrix = bproc.math.build_transformation_mat(location, rotation)\n\n    # Check that there is no obstacle in front of the camera closer than 1m\n    if not bproc.camera.perform_obstacle_in_view_check(cam2world_matrix, {\"min\": 1.0}, bvh_tree):\n        continue\n\n    # Check that the interesting score is not too low\n    if bproc.camera.scene_coverage_score(cam2world_matrix) < 0.1:\n        continue\n\n    # If all checks were passed, add the camera pose\n    bproc.camera.add_camera_pose(cam2world_matrix)\n    poses += 1\n\n# activate normal and depth rendering\nbproc.renderer.enable_normals_output()\nbproc.renderer.enable_depth_output(activate_antialiasing=False)\nbproc.renderer.enable_segmentation_output(map_by=[\"category_id\"])\n\n# render the whole pipeline\ndata = bproc.renderer.render()\n\n# write the data to a .hdf5 container\nbproc.writer.write_hdf5(args.output_dir, data)\n",
  "# -----------------------------------------------------------------------------\n# BSD 3-Clause License\n#\n# Copyright (c) 2023, Science and Technology Facilities Council\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n#\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n#\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n# -----------------------------------------------------------------------------\n# Authors: R. W. Ford, A. R. Porter and S. Siso, STFC Daresbury Lab\n#          I. Kavcic, Met Office\n#          J. Henrichs, Bureau of Meteorology\n\n'''PSyclone transformation script for the LFRic API to apply all the\nDistibutedMemory, OpenMP coloring and serial transformations possible.\n\n'''\nfrom psyclone.domain.common.transformations import KernelModuleInlineTrans\nfrom psyclone.domain.lfric import LFRicConstants\nfrom psyclone.dynamo0p3 import DynHaloExchange, DynHaloExchangeStart\nfrom psyclone.psyir.transformations import Matmul2CodeTrans\nfrom psyclone.psyir.nodes import BinaryOperation, Container, KernelSchedule\nfrom psyclone.transformations import Dynamo0p3ColourTrans, \\\n                                     Dynamo0p3OMPLoopTrans, \\\n                                     OMPParallelTrans, \\\n                                     Dynamo0p3RedundantComputationTrans, \\\n                                     Dynamo0p3AsyncHaloExchangeTrans, \\\n                                     MoveTrans, \\\n                                     TransformationError\n\nENABLE_REDUNDANT_COMPUTATION = True\nENABLE_ASYNC_HALOS = True\nENABLE_OMP_COLOURING = True\nENABLE_INTRINSIC_INLINING = True\n# LFRicLoopFuseTrans and DynKernelConstTrans could also be included but there\n# are some issues to overcome, e.g. TODO #2232\n\n\ndef trans(psy):\n    ''' Apply all possible LFRic transformations. '''\n    rtrans = Dynamo0p3RedundantComputationTrans()\n    ctrans = Dynamo0p3ColourTrans()\n    otrans = Dynamo0p3OMPLoopTrans()\n    oregtrans = OMPParallelTrans()\n    inline_trans = KernelModuleInlineTrans()\n    matmul_trans = Matmul2CodeTrans()\n    const = LFRicConstants()\n    ahex_trans = Dynamo0p3AsyncHaloExchangeTrans()\n    mtrans = MoveTrans()\n\n    # Loop over all of the Invokes in the PSy object\n    for invoke in psy.invokes.invoke_list:\n        schedule = invoke.schedule\n\n        if ENABLE_REDUNDANT_COMPUTATION:\n            # Make setval_* compute redundantly to the level 1 halo if it\n            # is in its own loop\n            for loop in schedule.loops():\n                if loop.iteration_space == \"dof\":\n                    if len(loop.kernels()) == 1:\n                        if loop.kernels()[0].name in [\"setval_c\", \"setval_x\"]:\n                            rtrans.apply(loop, options={\"depth\": 1})\n\n        if ENABLE_ASYNC_HALOS:\n            # This transformation splits all synchronous halo exchanges\n            for h_ex in schedule.walk(DynHaloExchange):\n                ahex_trans.apply(h_ex)\n\n            # This transformation moves the start of the halo exchanges as\n            # far as possible offering the potential for overlap between\n            # communication and computation\n            location_cursor = 0\n            for ahex in schedule.walk(DynHaloExchangeStart):\n                if ahex.position <= location_cursor:\n                    continue\n                try:\n                    mtrans.apply(ahex, schedule.children[location_cursor])\n                    location_cursor += 1\n                except TransformationError:\n                    pass\n\n        if ENABLE_OMP_COLOURING:\n            # Colour loops over cells unless they are on discontinuous\n            # spaces or over dofs\n            for loop in schedule.loops():\n                if loop.iteration_space == \"cell_column\" \\\n                    and loop.field_space.orig_name \\\n                        not in const.VALID_DISCONTINUOUS_NAMES:\n                    ctrans.apply(loop)\n\n            # Add OpenMP to loops unless they are over colours or are null\n            for loop in schedule.loops():\n                if loop.loop_type not in [\"colours\", \"null\"]:\n                    oregtrans.apply(loop)\n                    otrans.apply(loop, options={\"reprod\": True})\n\n        # Transformations that modify kernel code will need to have the\n        # kernels inlined first\n        if ENABLE_INTRINSIC_INLINING:\n            for kernel in schedule.coded_kernels():\n                try:\n                    inline_trans.apply(kernel)\n                except TransformationError:\n                    pass\n\n    # Then transform all the kernels inlined into the module\n    if psy.invokes.invoke_list:\n        root = psy.invokes.invoke_list[0].schedule.ancestor(Container)\n        for kschedule in root.walk(KernelSchedule):\n            if ENABLE_INTRINSIC_INLINING:\n                # Expand MATMUL intrinsic\n                for bop in kschedule.walk(BinaryOperation):\n                    if bop.operator == BinaryOperation.Operator.MATMUL:\n                        try:\n                            matmul_trans.apply(bop)\n                        except TransformationError:\n                            pass\n\n    return psy\n",
  "#!/usr/bin/env python3\n# encoding: utf-8\n\nfrom Lib.RansomwareService import RansomwareClientService, RansomwareService, RansomwareServerFileTemplates\nfrom Lib.TorService import *\n\nfrom seedemu.core.Emulator import *\nfrom seedemu.services.DomainNameService import *\nfrom seedemu.services.DomainNameCachingService import *\nfrom seedemu.core.Binding import Action, Filter, Binding\nfrom seedemu.layers.Base import Base\nfrom seedemu.core.Node import *\nfrom seedemu.compiler.Docker import *\nimport random\nimport os\n\n\nemu = Emulator()\n\n# Load the base layer from the mini Internet example\nemu.load('./base-component.bin')\nbase: Base = emu.getLayer(\"Base\")\n\n# Create a Ransomware Service\nransomware = RansomwareService()\nransomware.install('ransom-attacker').supportBotnet(False).supportTor(False)\nemu.getVirtualNode('ransom-attacker').setDisplayName('Ransom-Attacker')\nbase.getAutonomousSystem(170).createHost('ransom-attacker').joinNetwork('net0', address='10.170.0.99')\nemu.addBinding(Binding(\"ransom-attacker\", filter=Filter(asn=170, nodeName='ransom-attacker')))\n\nvictim = RansomwareClientService()\n\nfor i in range(1, 17):\n   victim_name =  'victim-{}'.format(i)\n   display_name = 'Ransom-Victim-{}'.format(i)\n   victim.install(victim_name).supportBotnet(False)\n   emu.getVirtualNode(victim_name).setDisplayName(display_name)\n   emu.addBinding(Binding(victim_name, filter=Filter(nodeName=\"host\"), action=Action.RANDOM))\n\n#################################################################\n# Create a Tor component\n\n# Create the Tor service layer\ntor = TorService()\n\n# Different types of Tor nodes (virtual nodes)\nvnodes = {\n   \"da-1\":     TorNodeType.DA,\n   \"da-2\":     TorNodeType.DA,\n   \"da-3\":     TorNodeType.DA,\n   \"da-4\":     TorNodeType.DA,\n   \"da-5\":     TorNodeType.DA,\n   \"client-1\": TorNodeType.CLIENT,\n   \"client-2\": TorNodeType.CLIENT,\n   \"relay-1\":  TorNodeType.RELAY,\n   \"relay-2\":  TorNodeType.RELAY,\n   \"relay-3\":  TorNodeType.RELAY,\n   \"relay-4\":  TorNodeType.RELAY,\n   \"exit-1\":   TorNodeType.EXIT,\n   \"exit-2\":   TorNodeType.EXIT,\n   \"hidden-service\": TorNodeType.HS\n}\n\n# Create the virtual nodes\nfor i, (name, nodeType) in enumerate(vnodes.items()):\n   if nodeType == TorNodeType.HS: \n      # Create 3 hidden services as bot-controller opens 4 ports (445, 446, 447, 448)\n      tor.install(name).setRole(nodeType).linkByVnode(\"ransom-attacker\", [445, 446, 447, 448])\n   else:\n      tor.install(name).setRole(nodeType)\n\n   # Customize the display names.\n   emu.getVirtualNode(name).setDisplayName(\"Tor-{}\".format(name))\n\n# Bind virtual nodes to physical nodes\nas_list = [150, 151, 152, 153, 154, 160, 161, 162, 163, 164, 170, 171]\nfor i, (name, nodeType) in enumerate(vnodes.items()):\n    # Pick an autonomous system randomly from the list,\n    # and create a new host for each Tor node\n    asn = random.choice(as_list)\n    emu.addBinding(Binding(name, filter=Filter(asn=asn, nodeName=name), action=Action.NEW))\n\n\n#################################################################\n# Create a DNS layer\ndns = DomainNameService()\n\n# Create one nameserver for the root zone\ndns.install('root-server').addZone('.')\n\n# Create nameservers for TLD and ccTLD zones\ndns.install('com-server').addZone('com.')\ndns.install('edu-server').addZone('edu.')\n\n# Create nameservers for second-level zones\ndns.install('ns-syr-edu').addZone('syr.edu.')\ndns.install('killswitch').addZone('iuqerfsodp9ifjaposdfjhgosurijfaewrwergwea.com.')\n\n# Add records to zones\ndns.getZone('syr.edu.').addRecord('@ A 128.230.18.63')\n#dns.getZone('iuqerfsodp9ifjaposdfjhgosurijfaewrwergwea.com.').addRecord('@ A 5.5.5.5').addRecord('www A 5.5.5.5')\n\n# Customize the display name (for visualization purpose)\nemu.getVirtualNode('root-server').setDisplayName('Root')\nemu.getVirtualNode('com-server').setDisplayName('COM')\nemu.getVirtualNode('edu-server').setDisplayName('EDU')\nemu.getVirtualNode('ns-syr-edu').setDisplayName('syr.edu')\nemu.getVirtualNode('killswitch').setDisplayName('killswitch')\n\n# Bind the virtual nodes in the DNS infrastructure layer to physical nodes.\nemu.addBinding(Binding('root-server', filter=Filter(asn=171), action=Action.NEW))\nemu.addBinding(Binding('com-server', filter=Filter(asn=150), action=Action.NEW))\nemu.addBinding(Binding('edu-server', filter=Filter(asn=152), action=Action.NEW))\nemu.addBinding(Binding('ns-syr-edu', filter=Filter(asn=154), action=Action.NEW))\nemu.addBinding(Binding('killswitch', filter=Filter(asn=161), action=Action.NEW))\n\n# Create one local DNS server (virtual nodes).\nldns = DomainNameCachingService()\nldns.install('global-dns')\n\n# Customize the display name (for visualization purpose)\nemu.getVirtualNode('global-dns').setDisplayName('Global DNS')\n\n# Create new host in AS-153, use them to host the local DNS server.\nas153 = base.getAutonomousSystem(153)\nas153.createHost('local-dns').joinNetwork('net0', address = '10.153.0.53')\n\n# Bind the Local DNS virtual node to physical node\nemu.addBinding(Binding('global-dns', filter=Filter(asn=153, nodeName='local-dns')))\n\n# Add 10.153.0.53 as the local DNS server for all the other nodes\nbase.setNameServers(['10.153.0.53'])\n\n\nemu.addLayer(ldns)\nemu.addLayer(dns)\nemu.addLayer(tor)\nemu.addLayer(ransomware)\nemu.addLayer(victim)\nemu.render()\n\n# Use the \"morris-worm-base\" custom base image\ndocker = Docker()\n\ndocker.addImage(DockerImage('morris-worm-base', [], local = True))\ndocker.addImage(DockerImage('handsonsecurity/seed-ubuntu:large', [], local=False))\n\nvictim_nodes = base.getNodesByName('host')\nfor victim in victim_nodes:\n   docker.setImageOverride(victim, 'morris-worm-base')\n\nattacker_node = base.getNodesByName('ransom-attacker')\ndocker.setImageOverride(attacker_node[0], 'handsonsecurity/seed-ubuntu:large')\n\nemu.compile(docker, './output', override=True)\n\nos.system('cp -r container_files/morris-worm-base ./output')\nos.system('cp -r container_files/z_start.sh ./output')\nos.system('chmod a+x ./output/z_start.sh')\n",
  "#!/usr/bin/env python3\n#\n\"\"\"\nThis script include the test code of the adaptive VEM for the simplified\nfriction problem on the halfedge polygon mesh.\n\nNote\n----\n\nExample\n-------\n\n\"\"\"\n\nimport numpy as np\nimport sys\nimport matplotlib.pyplot as plt\n\nfrom fealpy.pde.sfc_2d import SFCModelData1\nfrom fealpy.vem.SFCVEMModel2d import SFCVEMModel2d\nfrom fealpy.mesh import PolygonMesh, HalfEdgePolygonMesh\nfrom fealpy.mesh import TriangleMeshWithInfinityNode\nfrom fealpy.tools import showmultirate\n\n\nimport pickle\n\nmaxit = int(sys.argv[1])\ntheta = float(sys.argv[2])\nk = int(sys.argv[3])\n\n# prepare the pde model\npde = SFCModelData1()\n\n# mesh\nmesh = pde.init_mesh(n=4, meshtype='tri')\nmesh = TriangleMeshWithInfinityNode(mesh)\npnode, pcell, pcellLocation = mesh.to_polygonmesh()\nmesh = PolygonMesh(pnode, pcell, pcellLocation)\nmesh = HalfEdgePolygonMesh.from_polygonmesh(mesh)\n\n\nerrorType = ['$\\eta$', '$\\Psi_0$', '$\\Psi_1$']\nNdof = np.zeros((maxit,), dtype=np.int)\nerrorMatrix = np.zeros((len(errorType), maxit), dtype=np.float)\ndata = {}\n\nfor i in range(maxit):\n    print(i, \"-th step\")\n    vem = SFCVEMModel2d(pde, mesh, p=1, q=4)\n    if i == 0:\n        vem.solve(rho=0.7, maxit=40000)\n    else:\n        vem.solve(rho=0.9, maxit=40000, uh=data[2*(i-1)], lh=data[2*(i-1)+1])\n\n    data[2*i] = vem.uh\n    data[2*i+1] = vem.lh\n\n    eta = vem.residual_estimator()\n    psi0, psi1 = vem.high_order_term()\n    Ndof[i] = vem.space.number_of_global_dofs()\n    errorMatrix[0, i] = np.sqrt(np.sum(eta**2))\n    errorMatrix[1, i] = np.sqrt(psi0)\n    errorMatrix[2, i] = np.sqrt(psi1)\n\n    # save data\n    fname = sys.argv[1] + 'vem' + str(i) + '.space'\n    f = open(fname, 'wb')\n    pickle.dump([vem, data], f)\n    f.close()\n\n    fname = sys.argv[1] + 'error' + '-' + str(i) + '.data'\n    f = open(fname, 'wb')\n    pickle.dump([Ndof[:i+1], errorMatrix[:, :i+1], errorType], f)\n    f.close()\n\n    fig1 = plt.figure()\n    axes = fig1.gca()\n    mesh.add_plot(axes)\n    plt.savefig(sys.argv[1] + str(i) + '-mesh.png')\n    plt.close()\n\n    if i < maxit - 1:\n        isMarkedCell = mesh.refine_marker(eta, theta, method=\"L2\")\n        mesh.refine(isMarkedCell, data=data)\n\nfname = sys.argv[1] + 'error.data'\nf = open(fname, 'wb')\npickle.dump([Ndof, errorMatrix, errorType], f)\nshowmultirate(plt, k, Ndof, errorMatrix, errorType)\nplt.show()\n\n\"\"\"\nnode = mesh.entity('node')\nx = node[:, 0]\ny = node[:, 1]\ntri = qtree.leaf_cell(celltype='tri')\n\nfig0 = plt.figure()\nfig0.set_facecolor('white')\naxes = fig0.gca(projection='3d')\naxes.plot_trisurf(x, y, tri, vem.uh[:len(x)], cmap=plt.cm.jet, lw=0.0)\nplt.savefig(sys.argv[1] + str(i) + '-solution.png')\nplt.close()\n\"\"\"\n",
  "\"\"\" To view this example, first start a Bokeh server:\n\n    bokeh serve --allow-websocket-origin=localhost:8000\n\nAnd then load the example into the Bokeh server by\nrunning the script:\n\n    python widget.py\n\nin this directory. Finally, start a simple web server\nby running:\n\n    python -m SimpleHTTPServer  (python 2)\n\nor\n\n    python -m http.server  (python 3)\n\nin this directory. Navigate to\n\n    http://localhost:8000/widget.html\n\n\"\"\"\nfrom __future__ import print_function\n\nimport io\n\nfrom numpy import pi\n\nfrom bokeh.client import push_session\nfrom bokeh.document import Document\nfrom bokeh.embed import autoload_server\nfrom bokeh.layouts import row, column\nfrom bokeh.models import (Plot, DataRange1d, LinearAxis, CategoricalAxis,\n                          Legend, ColumnDataSource, Grid, Line,\n                          SingleIntervalTicker, Quad, Select, FactorRange)\nfrom bokeh.sampledata.population import load_population\n\ndocument = Document()\n\nsession = push_session(document)\n\ndf = load_population()\nrevision = 2012\n\nyear = 2010\nlocation = \"World\"\n\nyears = [str(x) for x in sorted(df.Year.unique())]\nlocations = sorted(df.Location.unique())\n\nsource_pyramid = ColumnDataSource(data=dict())\n\ndef pyramid():\n    xdr = DataRange1d()\n    ydr = DataRange1d()\n\n    plot = Plot(title=None, x_range=xdr, y_range=ydr, plot_width=600, plot_height=600)\n\n    xaxis = LinearAxis()\n    plot.add_layout(xaxis, 'below')\n    yaxis = LinearAxis(ticker=SingleIntervalTicker(interval=5))\n    plot.add_layout(yaxis, 'left')\n\n    plot.add_layout(Grid(dimension=0, ticker=xaxis.ticker))\n    plot.add_layout(Grid(dimension=1, ticker=yaxis.ticker))\n\n    male_quad = Quad(left=\"male\", right=0, bottom=\"groups\", top=\"shifted\", fill_color=\"#3B8686\")\n    male_quad_glyph = plot.add_glyph(source_pyramid, male_quad)\n\n    female_quad = Quad(left=0, right=\"female\", bottom=\"groups\", top=\"shifted\", fill_color=\"#CFF09E\")\n    female_quad_glyph = plot.add_glyph(source_pyramid, female_quad)\n\n    plot.add_layout(Legend(items=[\n        (\"Male\"   , [male_quad_glyph]),\n        (\"Female\" , [female_quad_glyph]),\n    ]))\n\n    return plot\n\nsource_known = ColumnDataSource(data=dict(x=[], y=[]))\nsource_predicted = ColumnDataSource(data=dict(x=[], y=[]))\n\ndef population():\n    xdr = FactorRange(factors=years)\n    ydr = DataRange1d()\n\n    plot = Plot(title=None, x_range=xdr, y_range=ydr, plot_width=800, plot_height=200)\n\n    plot.add_layout(CategoricalAxis(major_label_orientation=pi/4), 'below')\n\n    line_known = Line(x=\"x\", y=\"y\", line_color=\"violet\", line_width=2)\n    line_known_glyph = plot.add_glyph(source_known, line_known)\n\n    line_predicted = Line(x=\"x\", y=\"y\", line_color=\"violet\", line_width=2, line_dash=\"dashed\")\n    line_predicted_glyph = plot.add_glyph(source_predicted, line_predicted)\n\n    plot.add_layout(Legend(\n        location=\"bottom_right\",\n        items=[\n            (\"known\"     , [line_known_glyph]),\n            (\"predicted\" , [line_predicted_glyph]),\n        ])\n    )\n\n    return plot\n\ndef update_pyramid():\n    pyramid = df[(df.Location == location) & (df.Year == year)]\n\n    male = pyramid[pyramid.Sex == \"Male\"]\n    female = pyramid[pyramid.Sex == \"Female\"]\n\n    total = male.Value.sum() + female.Value.sum()\n\n    male_percent = -male.Value/total\n    female_percent = female.Value/total\n\n    groups = male.AgeGrpStart.tolist()\n    shifted = groups[1:] + [groups[-1] + 5]\n\n    source_pyramid.data = dict(\n        groups=groups,\n        shifted=shifted,\n        male=male_percent,\n        female=female_percent,\n    )\n\ndef update_population():\n    population = df[df.Location == location].groupby(df.Year).Value.sum()\n    aligned_revision = revision//10 * 10\n\n    known = population[population.index <= aligned_revision]\n    predicted = population[population.index >= aligned_revision]\n\n    source_known.data = dict(x=known.index.map(str), y=known.values)\n    source_predicted.data = dict(x=predicted.index.map(str), y=predicted.values)\n\ndef update_data():\n    update_population()\n    update_pyramid()\n\ndef on_year_change(attr, old, new):\n    global year\n    year = int(new)\n    update_data()\n\ndef on_location_change(attr, old, new):\n    global location\n    location = new\n    update_data()\n\ndef create_layout():\n    year_select = Select(title=\"Year:\", value=\"2010\", options=years)\n    location_select = Select(title=\"Location:\", value=\"World\", options=locations)\n\n    year_select.on_change('value', on_year_change)\n    location_select.on_change('value', on_location_change)\n\n    controls = row(children=[year_select, location_select])\n    layout = column(children=[controls, pyramid(), population()])\n\n    return layout\n\nlayout = create_layout()\n\nupdate_data()\n\nhtml = \"\"\"\n<html>\n    <head></head>\n    <body>\n        %s\n    </body>\n</html>\n\"\"\" % autoload_server(layout, session_id=session.id)\n\nwith io.open(\"widget.html\", mode='w+', encoding='utf-8') as f:\n    f.write(html)\n\nprint(__doc__)\n\ndocument.add_root(layout)\n\nif __name__ == \"__main__\":\n    print(\"\\npress ctrl-C to exit\")\n    session.loop_until_closed()\n",
  "#\n#  Copyright 2019 The FATE Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nimport argparse\n\nimport torch as t\nfrom torch import nn\n\nfrom pipeline import fate_torch_hook\nfrom pipeline.backend.pipeline import PipeLine\nfrom pipeline.component import DataTransform\nfrom pipeline.component import Evaluation\nfrom pipeline.component import HeteroNN\nfrom pipeline.component import Intersection\nfrom pipeline.component import Reader\nfrom pipeline.interface import Data\nfrom pipeline.utils.tools import load_job_config\n\nfate_torch_hook(t)\n\n\ndef main(config=\"../../config.yaml\", namespace=\"\"):\n    # obtain config\n    if isinstance(config, str):\n        config = load_job_config(config)\n    parties = config.parties\n    guest = parties.guest[0]\n    host = parties.host[0]\n\n    guest_train_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_train_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    pipeline = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest, host=host)\n\n    reader_0 = Reader(name=\"reader_0\")\n    reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    data_transform_0 = DataTransform(name=\"data_transform_0\")\n    data_transform_0.get_party_instance(role='guest', party_id=guest).component_param(with_label=True)\n    data_transform_0.get_party_instance(role='host', party_id=host).component_param(with_label=False)\n\n    intersection_0 = Intersection(name=\"intersection_0\")\n\n    hetero_nn_0 = HeteroNN(name=\"hetero_nn_0\", epochs=5,\n                           interactive_layer_lr=0.01, batch_size=128, validation_freqs=1, task_type='classification',\n                           selector_param={\"method\": \"relative\"})\n    guest_nn_0 = hetero_nn_0.get_party_instance(role='guest', party_id=guest)\n    host_nn_0 = hetero_nn_0.get_party_instance(role='host', party_id=host)\n\n    # define model\n    guest_bottom = t.nn.Sequential(\n        nn.Linear(10, 4),\n        nn.ReLU(),\n        nn.Dropout(p=0.2)\n    )\n\n    guest_top = t.nn.Sequential(\n        nn.Linear(4, 1),\n        nn.Sigmoid()\n    )\n\n    host_bottom = t.nn.Sequential(\n        nn.Linear(20, 4),\n        nn.ReLU(),\n        nn.Dropout(p=0.2)\n    )\n\n    # use interactive layer after fate_torch_hook\n    # add drop out in this layer\n    interactive_layer = t.nn.InteractiveLayer(out_dim=4, guest_dim=4, host_dim=4, host_num=1, dropout=0.2)\n\n    guest_nn_0.add_top_model(guest_top)\n    guest_nn_0.add_bottom_model(guest_bottom)\n    host_nn_0.add_bottom_model(host_bottom)\n\n    optimizer = t.optim.Adam(lr=0.01)  # you can initialize optimizer without parameters after fate_torch_hook\n    loss = t.nn.BCELoss()\n\n    hetero_nn_0.set_interactive_layer(interactive_layer)\n    hetero_nn_0.compile(optimizer=optimizer, loss=loss)\n\n    evaluation_0 = Evaluation(name='eval_0', eval_type='binary')\n\n    # define components IO\n    pipeline.add_component(reader_0)\n    pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))\n    pipeline.add_component(intersection_0, data=Data(data=data_transform_0.output.data))\n    pipeline.add_component(hetero_nn_0, data=Data(train_data=intersection_0.output.data))\n    pipeline.add_component(evaluation_0, data=Data(data=hetero_nn_0.output.data))\n    pipeline.compile()\n    pipeline.fit()\n\n    print(pipeline.get_component(\"hetero_nn_0\").get_summary())\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"PIPELINE DEMO\")\n    parser.add_argument(\"-config\", type=str,\n                        help=\"config file\")\n    args = parser.parse_args()\n    if args.config is not None:\n        main(args.config)\n    else:\n        main()\n",
  "#!/usr/bin/env python\n\n\nfrom manim import *\n\n# To watch one of these scenes, run the following:\n# python --quality m manim -p example_scenes.py SquareToCircle\n#\n# Use the flag --quality l for a faster rendering at a lower quality.\n# Use -s to skip to the end and just save the final frame\n# Use the -p to have preview of the animation (or image, if -s was\n# used) pop up once done.\n# Use -n <number> to skip ahead to the nth animation of a scene.\n# Use -r <number> to specify a resolution (for example, -r 1920,1080\n# for a 1920x1080 video)\n\n\nclass OpeningManim(Scene):\n    def construct(self):\n        title = Tex(r\"This is some \\LaTeX\")\n        basel = MathTex(r\"\\sum_{n=1}^\\infty \\frac{1}{n^2} = \\frac{\\pi^2}{6}\")\n        VGroup(title, basel).arrange(DOWN)\n        self.play(\n            Write(title),\n            FadeIn(basel, shift=DOWN),\n        )\n        self.wait()\n\n        transform_title = Tex(\"That was a transform\")\n        transform_title.to_corner(UP + LEFT)\n        self.play(\n            Transform(title, transform_title),\n            LaggedStart(*(FadeOut(obj, shift=DOWN) for obj in basel)),\n        )\n        self.wait()\n\n        grid = NumberPlane()\n        grid_title = Tex(\"This is a grid\", font_size=72)\n        grid_title.move_to(transform_title)\n\n        self.add(grid, grid_title)  # Make sure title is on top of grid\n        self.play(\n            FadeOut(title),\n            FadeIn(grid_title, shift=UP),\n            Create(grid, run_time=3, lag_ratio=0.1),\n        )\n        self.wait()\n\n        grid_transform_title = Tex(\n            r\"That was a non-linear function \\\\ applied to the grid\",\n        )\n        grid_transform_title.move_to(grid_title, UL)\n        grid.prepare_for_nonlinear_transform()\n        self.play(\n            grid.animate.apply_function(\n                lambda p: p\n                + np.array(\n                    [\n                        np.sin(p[1]),\n                        np.sin(p[0]),\n                        0,\n                    ],\n                ),\n            ),\n            run_time=3,\n        )\n        self.wait()\n        self.play(Transform(grid_title, grid_transform_title))\n        self.wait()\n\n\nclass SquareToCircle(Scene):\n    def construct(self):\n        circle = Circle()\n        square = Square()\n        square.flip(RIGHT)\n        square.rotate(-3 * TAU / 8)\n        circle.set_fill(PINK, opacity=0.5)\n\n        self.play(Create(square))\n        self.play(Transform(square, circle))\n        self.play(FadeOut(square))\n\n\nclass WarpSquare(Scene):\n    def construct(self):\n        square = Square()\n        self.play(\n            ApplyPointwiseFunction(\n                lambda point: complex_to_R3(np.exp(R3_to_complex(point))),\n                square,\n            ),\n        )\n        self.wait()\n\n\nclass WriteStuff(Scene):\n    def construct(self):\n        example_text = Tex(\"This is a some text\", tex_to_color_map={\"text\": YELLOW})\n        example_tex = MathTex(\n            \"\\\\sum_{k=1}^\\\\infty {1 \\\\over k^2} = {\\\\pi^2 \\\\over 6}\",\n        )\n        group = VGroup(example_text, example_tex)\n        group.arrange(DOWN)\n        group.width = config[\"frame_width\"] - 2 * LARGE_BUFF\n\n        self.play(Write(example_text))\n        self.play(Write(example_tex))\n        self.wait()\n\n\nclass UpdatersExample(Scene):\n    def construct(self):\n        decimal = DecimalNumber(\n            0,\n            show_ellipsis=True,\n            num_decimal_places=3,\n            include_sign=True,\n        )\n        square = Square().to_edge(UP)\n\n        decimal.add_updater(lambda d: d.next_to(square, RIGHT))\n        decimal.add_updater(lambda d: d.set_value(square.get_center()[1]))\n        self.add(square, decimal)\n        self.play(\n            square.animate.to_edge(DOWN),\n            rate_func=there_and_back,\n            run_time=5,\n        )\n        self.wait()\n\n\nclass SpiralInExample(Scene):\n    def construct(self):\n        logo_green = \"#81b29a\"\n        logo_blue = \"#454866\"\n        logo_red = \"#e07a5f\"\n\n        font_color = \"#ece6e2\"\n\n        pi = MathTex(r\"\\pi\").scale(7).set_color(font_color)\n        pi.shift(2.25 * LEFT + 1.5 * UP)\n\n        circle = Circle(color=logo_green, fill_opacity=0.7, stroke_width=0).shift(LEFT)\n        square = Square(color=logo_blue, fill_opacity=0.8, stroke_width=0).shift(UP)\n        triangle = Triangle(color=logo_red, fill_opacity=0.9, stroke_width=0).shift(\n            RIGHT\n        )\n        pentagon = Polygon(\n            *[\n                [np.cos(2 * np.pi / 5 * i), np.sin(2 * np.pi / 5 * i), 0]\n                for i in range(5)\n            ],\n            color=PURPLE_B,\n            fill_opacity=1,\n            stroke_width=0\n        ).shift(UP + 2 * RIGHT)\n        shapes = VGroup(triangle, square, circle, pentagon, pi)\n        self.play(SpiralIn(shapes, fade_in_fraction=0.9))\n        self.wait()\n        self.play(FadeOut(shapes))\n\n\nTriangle.set_default(stroke_width=20)\n\n\nclass LineJoints(Scene):\n    def construct(self):\n        t1 = Triangle()\n        t2 = Triangle(line_join=LineJointType.ROUND)\n        t3 = Triangle(line_join=LineJointType.BEVEL)\n\n        grp = VGroup(t1, t2, t3).arrange(RIGHT)\n        grp.set(width=config.frame_width - 1)\n\n        self.add(grp)\n\n\n# See many more examples at https://docs.manim.community/en/stable/examples.html\n",
  "\"\"\"\nHFSS 3D Layout: PCB and EDB in 3D layout\n----------------------------------------\nThis example shows how you can use HFSS 3D Layout combined with EDB to\ninteract with a 3D layout.\n\"\"\"\n\n\nimport os\nimport tempfile\nimport pyaedt\n\ntmpfold = tempfile.gettempdir()\ntemp_folder = os.path.join(tmpfold, pyaedt.generate_unique_name(\"Example\"))\nif not os.path.exists(temp_folder):\n    os.makedirs(temp_folder)\nprint(temp_folder)\n\n###############################################################################\n# Copy example into temporary folder\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Copy an example into the temporary folder.\n\ntargetfile = pyaedt.downloads.download_aedb()\nprint(targetfile)\naedt_file = targetfile[:-12] + \"aedt\"\n\n###############################################################################\n# Set non-graphical mode\n# ~~~~~~~~~~~~~~~~~~~~~~\n# Set non-graphical mode. \n# You can set ``non_graphical`` either to ``True`` or ``False``.\n\nnon_graphical = False\nNewThread = True\n\n###############################################################################\n# Launch AEDT\n# ~~~~~~~~~~~\n# Launch AEDT 2022R2 in graphical mode using SI units.\n\ndesktopVersion = \"2023.2\"\n\n###############################################################################\n# Initialize AEDT and launch HFSS 3D Layout\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Initialize AEDT and launch HFSS 3D Layout.\n# The ``h3d`` object contains the :class:`pyaedt.Edb` class query methods.\n\nd = pyaedt.launch_desktop(desktopVersion, non_graphical, NewThread)\nif os.path.exists(aedt_file):\n    os.remove(aedt_file)\nh3d = pyaedt.Hfss3dLayout(targetfile)\nh3d.save_project(os.path.join(temp_folder, \"edb_demo.aedt\"))\n\n###############################################################################\n# Print boundaries\n# ~~~~~~~~~~~~~~~~\n# Print boundaries from the ``setups`` object.\n\nh3d.boundaries\n\n###############################################################################\n# Hide all nets\n# ~~~~~~~~~~~~~\n# Hide all nets.\n\nh3d.modeler.change_net_visibility(visible=False)\n\n###############################################################################\n# Show only two nets\n# ~~~~~~~~~~~~~~~~~~\n# Show only two specified nets.\n\nh3d.modeler.change_net_visibility([\"A0_GPIO\", \"A0_MUX\"], visible=True)\nedb = h3d.modeler.edb\nedb.nets.plot([\"A0_GPIO\", \"A0_MUX\"])\n\n###############################################################################\n# Show all layers\n# ~~~~~~~~~~~~~~~\n# Show all layers.\n\nfor layer in h3d.modeler.layers.all_signal_layers:\n    layer.is_visible = True\n\n###############################################################################\n# Change layer color\n# ~~~~~~~~~~~~~~~~~~\n# Change the layer color.\n\nlayer = h3d.modeler.layers.layers[h3d.modeler.layers.layer_id(\"TOP\")]\nlayer.set_layer_color(0, 255, 0)\nh3d.modeler.fit_all()\n\n###############################################################################\n# Disable component visibility\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Disable component visibility for ``\"TOP\"`` and ``\"BOTTOM\"``.\n# The :func:`pyaedt.modules.LayerStackup.Layer.update_stackup_layer` method\n# applies modifications to the layout.\n\ntop = h3d.modeler.layers.layers[h3d.modeler.layers.layer_id(\"TOP\")]\ntop.is_visible_component = False\n\nbot = h3d.modeler.layers.layers[h3d.modeler.layers.layer_id(\"BOTTOM\")]\nbot.is_visible_component = False\n\n###############################################################################\n# Fit all\n# ~~~~~~~\n# Fit all so that you can visualize all.\n\nh3d.modeler.fit_all()\n\n###############################################################################\n# Close AEDT\n# ~~~~~~~~~~\n# After the simulation completes, you can close AEDT or release it using the\n# :func:`pyaedt.Desktop.release_desktop` method.\n# All methods provide for saving the project before closing.\n\nh3d.close_project()\nd.release_desktop()\n",
  "#!/usr/bin/env python\n\n'''\nMP2 with k-points sampling\n'''\n\nfrom functools import reduce\nimport numpy\nfrom pyscf.pbc import gto, scf, mp\n\ncell = gto.Cell()\ncell.atom='''\nC 0.000000000000   0.000000000000   0.000000000000\nC 1.685068664391   1.685068664391   1.685068664391\n'''\ncell.basis = 'gth-szv'\ncell.pseudo = 'gth-pade'\ncell.a = '''\n0.000000000, 3.370137329, 3.370137329\n3.370137329, 0.000000000, 3.370137329\n3.370137329, 3.370137329, 0.000000000'''\ncell.unit = 'B'\ncell.verbose = 5\ncell.build()\n\n#\n# KHF and KMP2 with 2x2x2 k-points\n#\nkpts = cell.make_kpts([2,2,2])\nkmf = scf.KRHF(cell)\nkmf.kpts = kpts\nehf = kmf.kernel()\n\nmypt = mp.KMP2(kmf)\nmypt.kernel()\nprint(\"KMP2 energy (per unit cell) =\", mypt.e_tot)\n\n#\n# The KHF and KMP2 for single k-point calculation.\n#\nkpts = cell.get_abs_kpts([0.25, 0.25, 0.25])\nkmf = scf.KRHF(cell)\nkmf.kpts = kpts\nehf = kmf.kernel()\n\nmypt = mp.KMP2(kmf)\nmypt.kernel()\nprint(\"KMP2 energy (per unit cell) =\", mypt.e_tot)\n\n\n#\n# The PBC module provides an separated implementation specified for the single\n# k-point calculations.  They are more efficient than the general implementation\n# with k-point sampling.  For gamma point, integrals and orbitals are all real\n# in this implementation.  They can be mixed with other post-HF methods that\n# were provided in the molecular program.\n#\nkpt = cell.get_abs_kpts([0.25, 0.25, 0.25])\nmf = scf.RHF(cell, kpt=kpt)\nehf = mf.kernel()\n\nmypt = mp.RMP2(mf).run()\nprint(\"RMP2 energy (per unit cell) at k-point =\", mypt.e_tot)\ndm1 = mypt.make_rdm1()\ndm2 = mypt.make_rdm2()\nnmo = mf.mo_coeff.shape[1]\neri_mo = mf.with_df.ao2mo(mf.mo_coeff, kpts=kpt).reshape([nmo]*4)\nh1 = reduce(numpy.dot, (mf.mo_coeff.conj().T, mf.get_hcore(), mf.mo_coeff))\ne_tot = numpy.einsum('ij,ji', h1, dm1) + numpy.einsum('ijkl,jilk', eri_mo, dm2)*.5 + mf.energy_nuc()\nprint(\"RMP2 energy based on MP2 density matrices =\", e_tot.real)\n\n\nmf = scf.addons.convert_to_uhf(mf)\nmypt = mp.UMP2(mf).run()\nprint(\"UMP2 energy (per unit cell) at k-point =\", mypt.e_tot)\ndm1a, dm1b = mypt.make_rdm1()\ndm2aa, dm2ab, dm2bb = mypt.make_rdm2()\nnmo = dm1a.shape[0]\neri_aa = mf.with_df.ao2mo(mf.mo_coeff[0], kpts=kpt).reshape([nmo]*4)\neri_bb = mf.with_df.ao2mo(mf.mo_coeff[1], kpts=kpt).reshape([nmo]*4)\neri_ab = mf.with_df.ao2mo((mf.mo_coeff[0],mf.mo_coeff[0],mf.mo_coeff[1],mf.mo_coeff[1]), kpts=kpt).reshape([nmo]*4)\nhcore = mf.get_hcore()\nh1a = reduce(numpy.dot, (mf.mo_coeff[0].conj().T, hcore, mf.mo_coeff[0]))\nh1b = reduce(numpy.dot, (mf.mo_coeff[1].conj().T, hcore, mf.mo_coeff[1]))\ne_tot = (numpy.einsum('ij,ji', h1a, dm1a) +\n         numpy.einsum('ij,ji', h1b, dm1b) +\n         numpy.einsum('ijkl,jilk', eri_aa, dm2aa)*.5 +\n         numpy.einsum('ijkl,jilk', eri_ab, dm2ab)    +\n         numpy.einsum('ijkl,jilk', eri_bb, dm2bb)*.5 + mf.energy_nuc())\nprint(\"UMP2 energy based on MP2 density matrices =\", e_tot.real)\n\n\nmf = scf.addons.convert_to_ghf(mf)\nmypt = mp.GMP2(mf).run()\nprint(\"GMP2 energy (per unit cell) at k-point =\", mypt.e_tot)\ndm1 = mypt.make_rdm1()\ndm2 = mypt.make_rdm2()\nnao = cell.nao_nr()\nnmo = mf.mo_coeff.shape[1]\nmo = mf.mo_coeff[:nao] + mf.mo_coeff[nao:]\neri_mo = mf.with_df.ao2mo(mo, kpts=kpt).reshape([nmo]*4)\norbspin = mf.mo_coeff.orbspin\neri_mo[orbspin[:,None]!=orbspin] = 0\neri_mo[:,:,orbspin[:,None]!=orbspin] = 0\nh1 = reduce(numpy.dot, (mf.mo_coeff.conj().T, mf.get_hcore(), mf.mo_coeff))\ne_tot = numpy.einsum('ij,ji', h1, dm1) + numpy.einsum('ijkl,jilk', eri_mo, dm2)*.5 + mf.energy_nuc()\nprint(\"GMP2 energy based on MP2 density matrices =\", e_tot.real)\n\n",
  "# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom helpers.supervised_pt_ditto import SupervisedPTDittoHelper\nfrom learners.supervised_monai_prostate_learner import SupervisedMonaiProstateLearner\nfrom monai.losses import DiceLoss\nfrom monai.networks.nets.unet import UNet\n\nfrom nvflare.apis.dxo import DXO, DataKind, MetaKey, from_shareable\nfrom nvflare.apis.fl_constant import ReturnCode\nfrom nvflare.apis.fl_context import FLContext\nfrom nvflare.apis.shareable import Shareable, make_reply\nfrom nvflare.apis.signal import Signal\nfrom nvflare.app_common.app_constant import AppConstants\n\n\nclass SupervisedMonaiProstateDittoLearner(SupervisedMonaiProstateLearner):\n    def __init__(\n        self,\n        train_config_filename,\n        aggregation_epochs: int = 1,\n        ditto_model_epochs: int = 1,\n        train_task_name: str = AppConstants.TASK_TRAIN,\n    ):\n        \"\"\"Trainer for prostate segmentation task. It inherits from MONAI trainer.\n\n        Args:\n            train_config_filename: directory of config file.\n            aggregation_epochs: the number of training epochs of global model for a round. Defaults to 1.\n            ditto_model_epochs: the number of training epochs of personalized model for a round. Defaults to 1.\n            train_task_name: name of the task to train the model.\n\n        Returns:\n            a Shareable with the updated local model after running `execute()`\n        \"\"\"\n        SupervisedMonaiProstateLearner.__init__(\n            self,\n            train_config_filename=train_config_filename,\n            aggregation_epochs=aggregation_epochs,\n            train_task_name=train_task_name,\n        )\n        self.ditto_helper = None\n        self.ditto_model_epochs = ditto_model_epochs\n\n    def train_config(self, fl_ctx: FLContext):\n        # Initialize superclass\n        SupervisedMonaiProstateLearner.train_config(self, fl_ctx)\n\n        engine = fl_ctx.get_engine()\n        ws = engine.get_workspace()\n        app_dir = ws.get_app_dir(fl_ctx.get_job_id())\n\n        # Initialize PTDittoHelper\n        ditto_model = UNet(\n            spatial_dims=2,\n            in_channels=1,\n            out_channels=1,\n            channels=(16, 32, 64, 128, 256),\n            strides=(2, 2, 2, 2),\n            num_res_units=2,\n        ).to(self.device)\n        ditto_optimizer = optim.Adam(ditto_model.parameters(), lr=self.config_info[\"ditto_learning_rate\"])\n        self.ditto_helper = SupervisedPTDittoHelper(\n            criterion=DiceLoss(sigmoid=True),\n            model=ditto_model,\n            optimizer=ditto_optimizer,\n            device=self.device,\n            app_dir=app_dir,\n            ditto_lambda=self.config_info[\"ditto_lambda\"],\n            model_epochs=self.ditto_model_epochs,\n        )\n\n    def train(\n        self,\n        shareable: Shareable,\n        fl_ctx: FLContext,\n        abort_signal: Signal,\n    ) -> Shareable:\n        \"\"\"Training task pipeline for Ditto\n        Get global model weights (potentially with HE)\n        Prepare for fedprox loss\n        Load Ditto personalized model info\n        Local training reference model and personalized model\n        Return updated weights of reference model (model_diff)\n        \"\"\"\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n\n        # get round information\n        current_round = shareable.get_header(AppConstants.CURRENT_ROUND)\n        total_rounds = shareable.get_header(AppConstants.NUM_ROUNDS)\n        self.log_info(fl_ctx, f\"Current/Total Round: {current_round + 1}/{total_rounds}\")\n        self.log_info(fl_ctx, f\"Client identity: {fl_ctx.get_identity_name()}\")\n\n        # update local model weights with received weights\n        dxo = from_shareable(shareable)\n        global_weights = dxo.data\n\n        # Before loading weights, tensors might need to be reshaped to support HE for secure aggregation.\n        local_var_dict = self.model.state_dict()\n        model_keys = global_weights.keys()\n        for var_name in local_var_dict:\n            if var_name in model_keys:\n                weights = global_weights[var_name]\n                try:\n                    # reshape global weights to compute difference later on\n                    global_weights[var_name] = np.reshape(weights, local_var_dict[var_name].shape)\n                    # update the local dict\n                    local_var_dict[var_name] = torch.as_tensor(global_weights[var_name])\n                except Exception as e:\n                    raise ValueError(\"Convert weight from {} failed with error: {}\".format(var_name, str(e)))\n        self.model.load_state_dict(local_var_dict)\n\n        # Load Ditto personalized model\n        self.ditto_helper.load_model(local_var_dict)\n\n        # local steps\n        epoch_len = len(self.train_loader)\n        self.log_info(fl_ctx, f\"Local steps per epoch: {epoch_len}\")\n\n        # make a copy of model_global as reference for\n        # 1. FedProx loss of reference model\n        # 2. Ditto loss of personalized model\n        model_global = copy.deepcopy(self.model)\n        for param in model_global.parameters():\n            param.requires_grad = False\n\n        # local train reference model\n        self.local_train(\n            fl_ctx=fl_ctx,\n            train_loader=self.train_loader,\n            model_global=model_global,\n            abort_signal=abort_signal,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n        self.epoch_of_start_time += self.aggregation_epochs\n\n        # local train ditto model\n        self.ditto_helper.local_train(\n            train_loader=self.train_loader,\n            model_global=model_global,\n            abort_signal=abort_signal,\n            writer=self.writer,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n\n        # local valid ditto model each round\n        metric = self.local_valid(\n            self.ditto_helper.model,\n            self.valid_loader,\n            abort_signal,\n            tb_id=\"val_metric_per_model\",\n            record_epoch=self.ditto_helper.epoch_global,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n        self.log_info(fl_ctx, f\"val_metric_per_model: {metric:.4f}\")\n        # save model\n        self.ditto_helper.update_metric_save_model(metric=metric)\n\n        # compute delta model, global model has the primary key set\n        local_weights = self.model.state_dict()\n        model_diff = {}\n        for name in global_weights:\n            if name not in local_weights:\n                continue\n            model_diff[name] = np.subtract(local_weights[name].cpu().numpy(), global_weights[name], dtype=np.float32)\n            if np.any(np.isnan(model_diff[name])):\n                self.system_panic(f\"{name} weights became NaN...\", fl_ctx)\n                return make_reply(ReturnCode.EXECUTION_EXCEPTION)\n\n        # flush the tb writer\n        self.writer.flush()\n\n        # build the shareable\n        dxo = DXO(data_kind=DataKind.WEIGHT_DIFF, data=model_diff)\n        dxo.set_meta_prop(MetaKey.NUM_STEPS_CURRENT_ROUND, epoch_len)\n\n        self.log_info(fl_ctx, \"Local epochs finished. Returning shareable\")\n        return dxo.to_shareable()\n",
  "#!/usr/bin/env python\n\n'''\nThe coupling matrix\n        < D^* A | H | D A^* >\nbetween donor and acceptor (* means excited state) for singlet energy transfer\n(SET) and triplet energy transfer (TET) involves two types of intermolecular\n2e integrals. They are the J-type integrals (D_i D_a | A_b A_j) and the K-type\nintegrals (D_i A_j | A_b D_a). The SET coupling corresponds to the\nspin-conserved transfer process. The matrix element has two terms: J - K. The\nTET coupling corresponds to the spin-flipped process and only the J integral\nis required in the coupling matrix.\n'''\n\nimport numpy as np\nimport scipy.linalg\nfrom pyscf import gto, scf, tdscf, lib\n\n# CIS calculations for the excited states of two molecules\nmolA = gto.M(atom='H 0.5 0.2 0.1; F 0 -0.1 -0.2', basis='ccpvdz')\nmfA = scf.RHF(molA).run()\nmoA = mfA.mo_coeff\no_A = moA[:,mfA.mo_occ!=0]\nv_A = moA[:,mfA.mo_occ==0]\ntdA = mfA.TDA().run()\n\nmolB = gto.M(atom='C 0.9 0.2 0; O 0.1 .2 .1', basis='ccpvtz')\nmfB = scf.RHF(molB).run()\nmoB = mfB.mo_coeff\no_B = moB[:,mfB.mo_occ!=0]\nv_B = moB[:,mfB.mo_occ==0]\ntdB = mfB.TDA().run()\n\n# CIS coeffcients\nstate_id = 2  # The third excited state\nt1_A = tdA.xy[state_id][0]\nt1_B = tdB.xy[state_id][0]\n\n# The intermolecular 2e integrals\nmolAB = molA + molB\nnaoA = molA.nao\neri = molAB.intor('int2e')\neri_AABB = eri[:naoA,:naoA,naoA:,naoA:]\neri_ABBA = eri[:naoA,naoA:,naoA:,:naoA]\n\n# Transform integrals to MO basis\neri_iabj = lib.einsum('pqrs,pi,qa,rb,sj->iabj', eri_AABB, o_A, v_A, v_B, o_B)\neri_ijba = lib.einsum('pqrs,pi,qj,rb,sa->ijba', eri_ABBA, o_A, o_B, v_B, v_A)\n\n# J-type coupling and K-type coupling\ncJ = np.einsum('iabj,ia,jb->', eri_iabj, t1_A, t1_B)\ncK = np.einsum('ijba,ia,jb->', eri_ijba, t1_A, t1_B)\nprint(cJ * 2 - cK)\n\n\n\n#\n# The coupling integrals can be computed more efficiently using the functions\n# defined in the following\n#\ndef jk_ints(molA, molB, dm_ia, dm_jb):\n    '''Given two molecules and their (transition) density matrices, compute\n    the Coulomb integrals and exchange integrals across the two molecules\n\n    On return,\n    cJ = ( ia | jb ) * dm_ia * dm_jb\n    cK = ( ij | ab ) * dm_ia * dm_jb\n    '''\n    from pyscf.scf import jk, _vhf\n    naoA = molA.nao\n    naoB = molB.nao\n    assert(dm_ia.shape == (naoA, naoA))\n    assert(dm_jb.shape == (naoB, naoB))\n\n    molAB = molA + molB\n    vhfopt = _vhf.VHFOpt(molAB, 'int2e', 'CVHFnrs8_prescreen',\n                         'CVHFsetnr_direct_scf',\n                         'CVHFsetnr_direct_scf_dm')\n    dmAB = scipy.linalg.block_diag(dm_ia, dm_jb)\n    #### Initialization for AO-direct JK builder\n    # The prescreen function CVHFnrs8_prescreen indexes q_cond and dm_cond\n    # over the entire basis.  \"set_dm\" in function jk.get_jk/direct_bindm only\n    # creates a subblock of dm_cond which is not compatible with\n    # CVHFnrs8_prescreen.\n    vhfopt.set_dm(dmAB, molAB._atm, molAB._bas, molAB._env)\n    # Then skip the \"set_dm\" initialization in function jk.get_jk/direct_bindm.\n    vhfopt._dmcondname = None\n    ####\n\n    # Coulomb integrals\n    with lib.temporary_env(vhfopt._this.contents,\n                           fprescreen=_vhf._fpointer('CVHFnrs8_vj_prescreen')):\n        shls_slice = (0        , molA.nbas , 0        , molA.nbas,\n                      molA.nbas, molAB.nbas, molA.nbas, molAB.nbas)  # AABB\n        vJ = jk.get_jk(molAB, dm_jb, 'ijkl,lk->s2ij', shls_slice=shls_slice,\n                       vhfopt=vhfopt, aosym='s4', hermi=1)\n        cJ = np.einsum('ia,ia->', vJ, dm_ia)\n\n    # Exchange integrals\n    with lib.temporary_env(vhfopt._this.contents,\n                           fprescreen=_vhf._fpointer('CVHFnrs8_vk_prescreen')):\n        shls_slice = (0        , molA.nbas , molA.nbas, molAB.nbas,\n                      molA.nbas, molAB.nbas, 0        , molA.nbas)  # ABBA\n        vK = jk.get_jk(molAB, dm_jb, 'ijkl,jk->il', shls_slice=shls_slice,\n                       vhfopt=vhfopt, aosym='s1', hermi=0)\n        cK = np.einsum('ia,ia->', vK, dm_ia)\n\n    return cJ, cK\n\ndef eval_coupling(molA, molB, dmA, dmB, dm_ia, dm_jb, xc=None):\n    '''\n    Evaluate the coupling term including J, K and DFT XC contributions\n    Eq. (11) of JCTC 13, 3493 (2017)\n\n        2J - c_HF*K + (1-c_HF) fxc\n\n    dmA and dmB are ground state density matrices\n\n    dm_ia = MO_i * MO_a  of molA\n    dm_jb = MO_j * MO_b  of molB\n    '''\n    from pyscf import dft\n    from pyscf.scf import jk, _vhf\n    from pyscf.dft import numint\n    molAB = molA + molB\n    naoA = molA.nao\n    naoB = molB.nao\n    nao = naoA + naoB\n    assert(dm_ia.shape == (naoA, naoA))\n    assert(dm_jb.shape == (naoB, naoB))\n\n    vhfopt = _vhf.VHFOpt(molAB, 'int2e', 'CVHFnrs8_prescreen',\n                         'CVHFsetnr_direct_scf',\n                         'CVHFsetnr_direct_scf_dm')\n    dmAB = scipy.linalg.block_diag(dm_ia, dm_jb)\n    #### Initialization for AO-direct JK builder\n    # The prescreen function CVHFnrs8_prescreen indexes q_cond and dm_cond\n    # over the entire basis.  \"set_dm\" in function jk.get_jk/direct_bindm only\n    # creates a subblock of dm_cond which is not compatible with\n    # CVHFnrs8_prescreen.\n    vhfopt.set_dm(dmAB, molAB._atm, molAB._bas, molAB._env)\n    # Then skip the \"set_dm\" initialization in function jk.get_jk/direct_bindm.\n    vhfopt._dmcondname = None\n    ####\n\n    with lib.temporary_env(vhfopt._this.contents,\n                           fprescreen=_vhf._fpointer('CVHFnrs8_vj_prescreen')):\n        shls_slice = (0        , molA.nbas , 0        , molA.nbas,\n                      molA.nbas, molAB.nbas, molA.nbas, molAB.nbas)  # AABB\n        vJ = jk.get_jk(molAB, dm_jb, 'ijkl,lk->s2ij', shls_slice=shls_slice,\n                       vhfopt=vhfopt, aosym='s4', hermi=1)\n        cJ = np.einsum('ia,ia->', vJ, dm_ia)\n\n    with lib.temporary_env(vhfopt._this.contents,\n                           fprescreen=_vhf._fpointer('CVHFnrs8_vk_prescreen')):\n        shls_slice = (0        , molA.nbas , molA.nbas, molAB.nbas,\n                      molA.nbas, molAB.nbas, 0        , molA.nbas)  # ABBA\n        vK = jk.get_jk(molAB, dm_jb, 'ijkl,jk->il', shls_slice=shls_slice,\n                       vhfopt=vhfopt, aosym='s1', hermi=0)\n        cK = np.einsum('ia,ia->', vK, dm_ia)\n\n    if xc is None:  # CIS coupling term\n        return cJ * 2 - cK\n\n    else:\n        ni = numint.NumInt()\n        omega, alpha, hyb = ni.rsh_and_hybrid_coeff(xc)\n\n        cK *= hyb\n\n        if omega > 1e-10:  # For range separated Coulomb\n            with lib.temporary_env(vhfopt._this.contents,\n                                   fprescreen=_vhf._fpointer('CVHFnrs8_vk_prescreen')):\n                with molAB.with_range_coulomb(omega):\n                    vK = jk.get_jk(molAB, dm_jb, 'ijkl,jk->il', shls_slice=shls_slice,\n                                   vhfopt=vhfopt, aosym='s1', hermi=0)\n                cK += np.einsum('ia,ia->', vK, dm_ia) * (alpha - hyb)\n\n    grids = dft.Grids(molAB)\n    xctype = ni._xc_type(xc)\n\n    def make_rhoA(ao, dmA):\n        return ni.eval_rho(molA, ao[...,:naoA], dmA, xctype=xctype)\n    def make_rhoB(ao, dmB):\n        return ni.eval_rho(molB, ao[...,naoA:], dmB, xctype=xctype)\n\n    cXC = 0\n    if xctype == 'LDA':\n        ao_deriv = 0\n        for ao, mask, weight, coords in ni.block_loop(molAB, grids, nao, ao_deriv):\n            # rho0 = ground state density of A + B\n            rho0 = make_rhoA(ao, dmA) + make_rhoB(ao, dmB)\n            fxc = ni.eval_xc(xc, rho0, 0, deriv=2)[2]\n            frr = fxc[0]\n\n            rhoA = make_rhoA(ao, dm_ia)\n            rhoB = make_rhoB(ao, dm_jb)\n            cXC += np.einsum('i,i,i,i->', weight, frr, rhoA, rhoB)\n\n    elif xctype == 'GGA':\n        ao_deriv = 1\n        for ao, mask, weight, coords in ni.block_loop(molAB, grids, nao, ao_deriv):\n            # rho0 = ground state density of A + B\n            rho0 = make_rhoA(ao, dmA) + make_rhoB(ao, dmB)\n            vxc, fxc = ni.eval_xc(xc, rho0, 0, deriv=2)[1:3]\n            vgamma = vxc[1]\n            frho, frhogamma, fgg = fxc[:3]\n\n            rhoA = make_rhoA(ao, dm_ia)\n            rhoB = make_rhoB(ao, dm_jb)\n            sigmaA = np.einsum('xi,xi->i', rho0[1:4], rhoA[1:4])\n            sigmaB = np.einsum('xi,xi->i', rho0[1:4], rhoB[1:4])\n            cXC += np.einsum('i,i,i,i->', weight, frho, rhoA[0], rhoB[0])\n            cXC += np.einsum('i,i,i,i->', weight, frhogamma, sigmaA, rhoB[0]) * 2\n            cXC += np.einsum('i,i,i,i->', weight, frhogamma, sigmaB, rhoA[0]) * 2\n            cXC += np.einsum('i,i,i,i->', weight, fgg, sigmaA, sigmaB) * 4\n            cXC += np.einsum('i,i,xi,xi->', weight, vgamma, rhoA[1:4], rhoB[1:4]) * 2\n\n    return cJ * 2 - cK + cXC\n\ndm_ia = o_A.dot(t1_A).dot(v_A.T)\ndm_jb = o_B.dot(t1_B).dot(v_B.T)\ncJ, cK = jk_ints(molA, molB, dm_ia, dm_jb)\nprint(cJ * 2 - cK)\n\n# Evaluate the overall coupling term\nprint(eval_coupling(molA, molB, mfA.make_rdm1(), mfB.make_rdm1(), dm_ia, dm_jb))\nprint(eval_coupling(molA, molB, mfA.make_rdm1(), mfB.make_rdm1(), dm_ia, dm_jb, 'b3lyp'))\n",
  "#!/usr/bin/python\n\n# ------------------------------------------------------------------------------\n# menu command functions\n# ------------------------------------------------------------------------------\n\n\ndef zoom_in(graph):\n    \"\"\"\n    Set the node graph to zoom in by 0.1\n    \"\"\"\n    zoom = graph.get_zoom() + 0.1\n    graph.set_zoom(zoom)\n\n\ndef zoom_out(graph):\n    \"\"\"\n    Set the node graph to zoom in by 0.1\n    \"\"\"\n    zoom = graph.get_zoom() - 0.2\n    graph.set_zoom(zoom)\n\n\ndef reset_zoom(graph):\n    \"\"\"\n    Reset zoom level.\n    \"\"\"\n    graph.reset_zoom()\n\n\ndef layout_h_mode(graph):\n    \"\"\"\n    Set node graph layout direction to horizontal.\n    \"\"\"\n    graph.set_layout_direction(0)\n\n\ndef layout_v_mode(graph):\n    \"\"\"\n    Set node graph layout direction to vertical.\n    \"\"\"\n    graph.set_layout_direction(1)\n\n\ndef open_session(graph):\n    \"\"\"\n    Prompts a file open dialog to load a session.\n    \"\"\"\n    current = graph.current_session()\n    file_path = graph.load_dialog(current)\n    if file_path:\n        graph.load_session(file_path)\n\n\ndef import_session(graph):\n    \"\"\"\n    Prompts a file open dialog to load a session.\n    \"\"\"\n    current = graph.current_session()\n    file_path = graph.load_dialog(current)\n    if file_path:\n        graph.import_session(file_path)\n\n\ndef save_session(graph):\n    \"\"\"\n    Prompts a file save dialog to serialize a session if required.\n    \"\"\"\n    current = graph.current_session()\n    if current:\n        graph.save_session(current)\n        msg = 'Session layout saved:\\n{}'.format(current)\n        viewer = graph.viewer()\n        viewer.message_dialog(msg, title='Session Saved')\n    else:\n        save_session_as(graph)\n\n\ndef save_session_as(graph):\n    \"\"\"\n    Prompts a file save dialog to serialize a session.\n    \"\"\"\n    current = graph.current_session()\n    file_path = graph.save_dialog(current)\n    if file_path:\n        graph.save_session(file_path)\n\n\ndef clear_session(graph):\n    \"\"\"\n    Prompts a warning dialog to new a node graph session.\n    \"\"\"\n    if graph.question_dialog('Clear Current Session?', 'Clear Session'):\n        graph.clear_session()\n\n\ndef clear_undo(graph):\n    \"\"\"\n    Prompts a warning dialog to clear undo.\n    \"\"\"\n    viewer = graph.viewer()\n    msg = 'Clear all undo history, Are you sure?'\n    if viewer.question_dialog('Clear Undo History', msg):\n        graph.clear_undo_stack()\n\n\ndef copy_nodes(graph):\n    \"\"\"\n    Copy nodes to the clipboard.\n    \"\"\"\n    graph.copy_nodes()\n\n\ndef cut_nodes(graph):\n    \"\"\"\n    Cut nodes to the clip board.\n    \"\"\"\n    graph.cut_nodes()\n\n\ndef paste_nodes(graph):\n    \"\"\"\n    Pastes nodes copied from the clipboard.\n    \"\"\"\n    graph.paste_nodes()\n\n\ndef delete_nodes(graph):\n    \"\"\"\n    Delete selected node.\n    \"\"\"\n    graph.delete_nodes(graph.selected_nodes())\n\n\ndef extract_nodes(graph):\n    \"\"\"\n    Extract selected nodes.\n    \"\"\"\n    graph.extract_nodes(graph.selected_nodes())\n\n\ndef clear_node_connections(graph):\n    \"\"\"\n    Clear port connection on selected nodes.\n    \"\"\"\n    graph.undo_stack().beginMacro('clear selected node connections')\n    for node in graph.selected_nodes():\n        for port in node.input_ports() + node.output_ports():\n            port.clear_connections()\n    graph.undo_stack().endMacro()\n\n\ndef select_all_nodes(graph):\n    \"\"\"\n    Select all nodes.\n    \"\"\"\n    graph.select_all()\n\n\ndef clear_node_selection(graph):\n    \"\"\"\n    Clear node selection.\n    \"\"\"\n    graph.clear_selection()\n\n\ndef invert_node_selection(graph):\n    \"\"\"\n    Invert node selection.\n    \"\"\"\n    graph.invert_selection()\n\n\ndef disable_nodes(graph):\n    \"\"\"\n    Toggle disable on selected nodes.\n    \"\"\"\n    graph.disable_nodes(graph.selected_nodes())\n\n\ndef duplicate_nodes(graph):\n    \"\"\"\n    Duplicated selected nodes.\n    \"\"\"\n    graph.duplicate_nodes(graph.selected_nodes())\n\n\ndef expand_group_node(graph):\n    \"\"\"\n    Expand selected group node.\n    \"\"\"\n    selected_nodes = graph.selected_nodes()\n    if not selected_nodes:\n        graph.message_dialog('Please select a \"GroupNode\" to expand.')\n        return\n    graph.expand_group_node(selected_nodes[0])\n\n\ndef fit_to_selection(graph):\n    \"\"\"\n    Sets the zoom level to fit selected nodes.\n    \"\"\"\n    graph.fit_to_selection()\n\n\ndef show_undo_view(graph):\n    \"\"\"\n    Show the undo list widget.\n    \"\"\"\n    graph.undo_view.show()\n\n\ndef curved_pipe(graph):\n    \"\"\"\n    Set node graph pipes layout as curved.\n    \"\"\"\n    from NodeGraphQt.constants import PipeLayoutEnum\n    graph.set_pipe_style(PipeLayoutEnum.CURVED.value)\n\n\ndef straight_pipe(graph):\n    \"\"\"\n    Set node graph pipes layout as straight.\n    \"\"\"\n    from NodeGraphQt.constants import PipeLayoutEnum\n    graph.set_pipe_style(PipeLayoutEnum.STRAIGHT.value)\n\n\ndef angle_pipe(graph):\n    \"\"\"\n    Set node graph pipes layout as angled.\n    \"\"\"\n    from NodeGraphQt.constants import PipeLayoutEnum\n    graph.set_pipe_style(PipeLayoutEnum.ANGLE.value)\n\n\ndef bg_grid_none(graph):\n    \"\"\"\n    Turn off the background patterns.\n    \"\"\"\n    from NodeGraphQt.constants import ViewerEnum\n    graph.set_grid_mode(ViewerEnum.GRID_DISPLAY_NONE.value)\n\n\ndef bg_grid_dots(graph):\n    \"\"\"\n    Set background node graph background with grid dots.\n    \"\"\"\n    from NodeGraphQt.constants import ViewerEnum\n    graph.set_grid_mode(ViewerEnum.GRID_DISPLAY_DOTS.value)\n\n\ndef bg_grid_lines(graph):\n    \"\"\"\n    Set background node graph background with grid lines.\n    \"\"\"\n    from NodeGraphQt.constants import ViewerEnum\n    graph.set_grid_mode(ViewerEnum.GRID_DISPLAY_LINES.value)\n\n\ndef layout_graph_down(graph):\n    \"\"\"\n    Auto layout the nodes down stream.\n    \"\"\"\n    nodes = graph.selected_nodes() or graph.all_nodes()\n    graph.auto_layout_nodes(nodes=nodes, down_stream=True)\n\n\ndef layout_graph_up(graph):\n    \"\"\"\n    Auto layout the nodes up stream.\n    \"\"\"\n    nodes = graph.selected_nodes() or graph.all_nodes()\n    graph.auto_layout_nodes(nodes=nodes, down_stream=False)\n\n\ndef toggle_node_search(graph):\n    \"\"\"\n    show/hide the node search widget.\n    \"\"\"\n    graph.toggle_node_search()\n",
  "#!/usr/bin/env python\n# Impacket - Collection of Python classes for working with network protocols.\n#\n# Copyright (C) 2023 Fortra. All rights reserved.\n#\n# This software is provided under a slightly modified version\n# of the Apache Software License. See the accompanying LICENSE file\n# for more information.\n#\n# Description:\n#   Simple ICMP6 ping.\n#\n#   This implementation of ping uses the ICMP echo and echo-reply packets\n#   to check the status of a host. If the remote host is up, it should reply\n#   to the echo probe with an echo-reply packet.\n#   Note that this isn't a definite test, as in the case the remote host is up\n#   but refuses to reply the probes.\n#   Also note that the user must have special access to be able to open a raw\n#   socket, which this program requires.\n#\n# Authors:\n#   Alberto Solino (@agsolino)\n#\n# Reference for:\n#   ImpactPacket: ICMP6\n#   ImpactDecoder\n#\n\nimport select\nimport socket\nimport time\nimport sys\n\nfrom impacket import ImpactDecoder, IP6, ICMP6, version\n\nprint(version.BANNER)\n\nif len(sys.argv) < 3:\n    print(\"Use: %s <src ip> <dst ip>\" % sys.argv[0])\n    sys.exit(1)\n\nsrc = sys.argv[1]\ndst = sys.argv[2]\n\n# Create a new IP packet and set its source and destination addresses.\n\nip = IP6.IP6()\nip.set_ip_src(src)\nip.set_ip_dst(dst)\nip.set_traffic_class(0)\nip.set_flow_label(0)\nip.set_hop_limit(64)\n\n# Open a raw socket. Special permissions are usually required.\ns = socket.socket(socket.AF_INET6, socket.SOCK_RAW, socket.IPPROTO_ICMPV6)\n\npayload = b\"A\"*156\n\nprint(\"PING %s %d data bytes\" % (dst, len(payload)))\nseq_id = 0\nwhile 1:\n    # Give the ICMP packet the next ID in the sequence.\n    seq_id += 1\n    icmp = ICMP6.ICMP6.Echo_Request(1, seq_id, payload)\n\n    # Have the IP packet contain the ICMP packet (along with its payload).\n    ip.contains(icmp)\n    ip.set_next_header(ip.child().get_ip_protocol_number())\n    ip.set_payload_length(ip.child().get_size())\n    icmp.calculate_checksum()\n\n    # Send it to the target host.\n    s.sendto(icmp.get_packet(), (dst, 0))\n\n    # Wait for incoming replies.\n    if s in select.select([s], [], [], 1)[0]:\n        reply = s.recvfrom(2000)[0]\n\n        # Use ImpactDecoder to reconstruct the packet hierarchy.\n        rip = ImpactDecoder.ICMP6Decoder().decode(reply)\n\n        # If the packet matches, report it to the user.\n        if ICMP6.ICMP6.ECHO_REPLY == rip.get_type():\n            print(\"%d bytes from %s: icmp_seq=%d \" % (rip.child().get_size()-4, dst, rip.get_echo_sequence_number()))\n\n        time.sleep(1)\n",
  "# Copyright 2020 by B. Knueven, D. Mildebrath, C. Muir, J-P Watson, and D.L. Woodruff\n# This software is distributed under the 3-clause BSD License.\n\n# TBD: put in more options: threads, mipgaps for spokes\n\n# There is  manipulation of the mip gap,\n#  so we need modifications of the vanilla dicts.\n# Notice also that this uses MutliExtensions\nimport sys\nimport json\nimport uc_funcs as uc\n\nimport mpisppy.utils.sputils as sputils\nfrom mpisppy.spin_the_wheel import WheelSpinner\n\nfrom mpisppy.extensions.extension import MultiExtension\nfrom mpisppy.extensions.fixer import Fixer\nfrom mpisppy.extensions.mipgapper import Gapper\nfrom mpisppy.extensions.xhatclosest import XhatClosest\nfrom mpisppy.utils import config\nimport mpisppy.utils.cfg_vanilla as vanilla\nfrom mpisppy.extensions.cross_scen_extension import CrossScenarioExtension\n\ndef _parse_args():\n    cfg = config.Config()\n    cfg.popular_args()\n    cfg.num_scens_required()\n    cfg.ph_args()\n    cfg.two_sided_args()\n    cfg.aph_args()\n    cfg.fixer_args()\n    cfg.fwph_args()\n    cfg.lagrangian_args()\n    cfg.xhatlooper_args()\n    cfg.xhatshuffle_args()\n    cfg.cross_scenario_cuts_args()\n    cfg.add_to_config(\"ph_mipgaps_json\",\n                         description=\"json file with mipgap schedule (default None)\",\n                         domain=str,\n                         default=None)\n    cfg.add_to_config(\"solution_dir\",\n                         description=\"writes a tree solution to the provided directory\"\n                                      \" (default None)\",\n                         domain=str,\n                         default=None)\n    cfg.add_to_config(\"xhat_closest_tree\",\n                         description=\"Uses XhatClosest to compute a tree solution after\"\n                                     \" PH termination (default False)\",\n                         domain=bool,\n                         default=False)\n    cfg.add_to_config(\"run_aph\",\n                         description=\"Run with async projective hedging instead of progressive hedging\",\n                         domain=bool,\n                         default=False)\n    cfg.parse_command_line(\"uc_cylinders\")\n    return cfg\n\n\ndef main():\n\n    cfg = _parse_args()\n\n    num_scen = cfg.num_scens\n\n    fwph = cfg.fwph\n    xhatlooper = cfg.xhatlooper\n    xhatshuffle = cfg.xhatshuffle\n    lagrangian = cfg.lagrangian\n    fixer = cfg.fixer\n    fixer_tol = cfg.fixer_tol\n    cross_scenario_cuts = cfg.cross_scenario_cuts\n\n    scensavail = [3,5,10,25,50,100]\n    if num_scen not in scensavail:\n        raise RuntimeError(\"num-scen was {}, but must be in {}\".\\\n                           format(num_scen, scensavail))\n\n    scenario_creator_kwargs = {\n        \"scenario_count\": num_scen,\n        \"path\": str(num_scen) + \"scenarios_r1\",\n    }\n    scenario_creator = uc.scenario_creator\n    scenario_denouement = uc.scenario_denouement\n    all_scenario_names = [f\"Scenario{i+1}\" for i in range(num_scen)]\n    rho_setter = uc._rho_setter\n\n    # Things needed for vanilla cylinders\n    beans = (cfg, scenario_creator, scenario_denouement, all_scenario_names)\n\n    ### start ph spoke ###\n    if cfg.run_aph:\n        hub_dict = vanilla.aph_hub(*beans,\n                                   scenario_creator_kwargs=scenario_creator_kwargs,\n                                   ph_extensions=MultiExtension,\n                                   rho_setter = rho_setter)\n    else:\n        hub_dict = vanilla.ph_hub(*beans,\n                                  scenario_creator_kwargs=scenario_creator_kwargs,\n                                  ph_extensions=MultiExtension,\n                                  rho_setter = rho_setter)\n\n    # Extend and/or correct the vanilla dictionary\n    ext_classes =  [Gapper]\n    if fixer:\n        ext_classes.append(Fixer)\n    if cross_scenario_cuts:\n        ext_classes.append(CrossScenarioExtension)\n    if cfg.xhat_closest_tree:\n        ext_classes.append(XhatClosest)\n\n    hub_dict[\"opt_kwargs\"][\"extension_kwargs\"] = {\"ext_classes\" : ext_classes}\n    if cross_scenario_cuts:\n        hub_dict[\"opt_kwargs\"][\"options\"][\"cross_scen_options\"]\\\n            = {\"check_bound_improve_iterations\" : cfg.cross_scenario_iter_cnt}\n\n    if fixer:\n        hub_dict[\"opt_kwargs\"][\"options\"][\"fixeroptions\"] = {\n            \"verbose\": cfg.verbose,\n            \"boundtol\": fixer_tol,\n            \"id_fix_list_fct\": uc.id_fix_list_fct,\n        }\n    if cfg.xhat_closest_tree:\n        hub_dict[\"opt_kwargs\"][\"options\"][\"xhat_closest_options\"] = {\n            \"xhat_solver_options\" : dict(),\n            \"keep_solution\" : True\n        }\n\n    if cfg.ph_mipgaps_json is not None:\n        with open(cfg.ph_mipgaps_json) as fin:\n            din = json.load(fin)\n        mipgapdict = {int(i): din[i] for i in din}\n    else:\n        mipgapdict = None\n    hub_dict[\"opt_kwargs\"][\"options\"][\"gapperoptions\"] = {\n        \"verbose\": cfg.verbose,\n        \"mipgapdict\": mipgapdict\n        }\n\n    if cfg.default_rho is None:\n        # since we are using a rho_setter anyway\n        hub_dict[\"opt_kwargs\"][\"options\"][\"defaultPHrho\"] = 1\n    ### end ph spoke ###\n\n    # FWPH spoke\n    if fwph:\n        fw_spoke = vanilla.fwph_spoke(*beans, scenario_creator_kwargs=scenario_creator_kwargs)\n\n    # Standard Lagrangian bound spoke\n    if lagrangian:\n        lagrangian_spoke = vanilla.lagrangian_spoke(*beans,\n                                              scenario_creator_kwargs=scenario_creator_kwargs,\n                                              rho_setter = rho_setter)\n\n    # xhat looper bound spoke\n    if xhatlooper:\n        xhatlooper_spoke = vanilla.xhatlooper_spoke(*beans, scenario_creator_kwargs=scenario_creator_kwargs)\n\n    # xhat shuffle bound spoke\n    if xhatshuffle:\n        xhatshuffle_spoke = vanilla.xhatshuffle_spoke(*beans, scenario_creator_kwargs=scenario_creator_kwargs)\n\n    # cross scenario cut spoke\n    if cross_scenario_cuts:\n        cross_scenario_cuts_spoke = vanilla.cross_scenario_cuts_spoke(*beans, scenario_creator_kwargs=scenario_creator_kwargs)\n\n    list_of_spoke_dict = list()\n    if fwph:\n        list_of_spoke_dict.append(fw_spoke)\n    if lagrangian:\n        list_of_spoke_dict.append(lagrangian_spoke)\n    if xhatlooper:\n        list_of_spoke_dict.append(xhatlooper_spoke)\n    if xhatshuffle:\n        list_of_spoke_dict.append(xhatshuffle_spoke)\n    if cross_scenario_cuts:\n        list_of_spoke_dict.append(cross_scenario_cuts_spoke)\n\n    wheel = WheelSpinner(hub_dict, list_of_spoke_dict)\n    wheel.spin()\n\n    if cfg.solution_dir is not None:\n        wheel.write_tree_solution(cfg.solution_dir, uc.scenario_tree_solution_writer)\n\n    wheel.write_first_stage_solution('uc_cyl_nonants.npy',\n            first_stage_solution_writer=sputils.first_stage_nonant_npy_serializer)\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "from __future__ import (absolute_import, division, print_function)\n\nfrom mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nimport numpy as np\n# set up orthographic map projection with\n# perspective of satellite looking down at 50N, 100W.\n# use low resolution coastlines.\nbmap = Basemap(projection='ortho',lat_0=45,lon_0=-100,resolution='l')\n# draw coastlines, country boundaries, fill continents.\nbmap.drawcoastlines(linewidth=0.25)\nbmap.drawcountries(linewidth=0.25)\nbmap.fillcontinents(color='coral',lake_color='aqua')\n# draw the edge of the map projection region (the projection limb)\nbmap.drawmapboundary(fill_color='aqua')\n# draw lat/lon grid lines every 30 degrees.\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n# lat/lon coordinates of five cities.\nlats=[40.02,32.73,38.55,48.25,17.29]\nlons=[-105.16,-117.16,-77.00,-114.21,-88.10]\ncities=['Boulder, CO','San Diego, CA',\n        'Washington, DC','Whitefish, MT','Belize City, Belize']\n# compute the native map projection coordinates for cities.\nxc,yc = bmap(lons,lats)\n# plot filled circles at the locations of the cities.\nbmap.plot(xc,yc,'bo')\n# plot the names of those five cities.\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9)\n# make up some data on a regular lat/lon grid.\nnlats = 73; nlons = 145; delta = 2.*np.pi/(nlons-1)\nlats = (0.5*np.pi-delta*np.indices((nlats,nlons))[0,:,:])\nlons = (delta*np.indices((nlats,nlons))[1,:,:])\nwave = 0.75*(np.sin(2.*lats)**8*np.cos(4.*lons))\nmean = 0.5*np.cos(2.*lats)*((np.sin(2.*lats))**2 + 2.)\n# compute native map projection coordinates of lat/lon grid.\nx, y = bmap(lons*180./np.pi, lats*180./np.pi)\n# contour data over the map.\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('filled continent background')\n\n# as above, but use land-sea mask image as map background.\nfig = plt.figure()\nbmap.drawmapboundary()\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n# plot filled circles at the locations of the cities.\nbmap.plot(xc,yc,'wo')\n# plot the names of five cities.\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9,color='w')\n# contour data over the map.\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('land-sea mask background')\nbmap.drawlsmask(ocean_color='aqua',land_color='coral')\n\n# as above, but use blue marble image as map background.\nfig = plt.figure()\nbmap.drawmapboundary()\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n# plot filled circles at the locations of the cities.\nbmap.plot(xc,yc,'wo')\n# plot the names of five cities.\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9,color='w')\n# contour data over the map.\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('blue marble background')\nbmap.bluemarble()\n\n# as above, but use shaded relief image as map background.\nfig = plt.figure()\nbmap.drawmapboundary()\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n# plot filled circles at the locations of the cities.\nbmap.plot(xc,yc,'wo')\n# plot the names of five cities.\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9,color='w')\n# contour data over the map.\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('shaded relief background')\nbmap.shadedrelief()\n\n# as above, but use etopo image as map background.\nfig = plt.figure()\nbmap.drawmapboundary()\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n# plot filled circles at the locations of the cities.\nbmap.plot(xc,yc,'wo')\n# plot the names of five cities.\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9,color='w')\n# contour data over the map.\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('etopo background')\nbmap.etopo()\n\n# as above, but use etopo image as map background overlaid with\n# land-sea mask image where land areas are transparent (so etopo\n# image shows through over land).\nfig = plt.figure()\nbmap.drawmapboundary()\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n# plot filled circles at the locations of the cities.\nbmap.plot(xc,yc,'wo')\n# plot the names of five cities.\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9,color='w')\n# contour data over the map.\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('etopo background with oceans masked')\nbmap.etopo()\nbmap.drawlsmask(ocean_color='DarkBlue',land_color=(255,255,255,1))\n\nplt.show()\n",
  "#!/usr/bin/python\n#\n# strlen_hist_ifunc.py     Histogram of system-wide strlen return values.\n# This can be used instead of strlen_hist.py if strlen is indirect function.\n\nfrom __future__ import print_function\nfrom bcc import BPF\nfrom bcc.libbcc import lib, bcc_symbol, bcc_symbol_option\n\nimport ctypes as ct\nimport sys\nimport time\n\nNAME = 'c'\nSYMBOL = 'strlen'\nSTT_GNU_IFUNC = 1 << 10\n\nHIST_BPF_TEXT = \"\"\"\n#include <uapi/linux/ptrace.h>\nBPF_HISTOGRAM(dist);\nint count(struct pt_regs *ctx) {\n    dist.increment(bpf_log2l(PT_REGS_RC(ctx)));\n    return 0;\n}\n\"\"\"\n\nSUBMIT_FUNC_ADDR_BPF_TEXT = \"\"\"\n#include <uapi/linux/ptrace.h>\n\nBPF_PERF_OUTPUT(impl_func_addr);\nvoid submit_impl_func_addr(struct pt_regs *ctx) {\n    u64 addr = PT_REGS_RC(ctx);\n    impl_func_addr.perf_submit(ctx, &addr, sizeof(addr));\n}\n\n\nBPF_PERF_OUTPUT(resolv_func_addr);\nint submit_resolv_func_addr(struct pt_regs *ctx) {\n    u64 rip = PT_REGS_IP(ctx);\n    resolv_func_addr.perf_submit(ctx, &rip, sizeof(rip));\n    return 0;\n}\n\"\"\"\n\n\ndef get_indirect_function_sym(module, symname):\n    sym = bcc_symbol()\n    sym_op = bcc_symbol_option()\n    sym_op.use_debug_file = 1\n    sym_op.check_debug_file_crc = 1\n    sym_op.lazy_symbolize = 1\n    sym_op.use_symbol_type = STT_GNU_IFUNC\n    if lib.bcc_resolve_symname(\n            module.encode(),\n            symname.encode(),\n            0x0,\n            0,\n            ct.byref(sym_op),\n            ct.byref(sym),\n    ) < 0:\n        return None\n    else:\n        return sym\n\n\ndef set_impl_func_addr(cpu, data, size):\n    addr = ct.cast(data, ct.POINTER(ct.c_uint64)).contents.value\n    global impl_func_addr\n    impl_func_addr = addr\n\n\ndef set_resolv_func_addr(cpu, data, size):\n    addr = ct.cast(data, ct.POINTER(ct.c_uint64)).contents.value\n    global resolv_func_addr\n    resolv_func_addr = addr\n\n\ndef find_impl_func_offset(ifunc_symbol):\n    b = BPF(text=SUBMIT_FUNC_ADDR_BPF_TEXT)\n    b.attach_uprobe(name=NAME, sym=SYMBOL, fn_name=b'submit_resolv_func_addr')\n    b['resolv_func_addr'].open_perf_buffer(set_resolv_func_addr)\n    b.attach_uretprobe(name=NAME, sym=SYMBOL, fn_name=b\"submit_impl_func_addr\")\n    b['impl_func_addr'].open_perf_buffer(set_impl_func_addr)\n\n    print('wait for the first {} call'.format(SYMBOL))\n    while True:\n        try:\n            if resolv_func_addr and impl_func_addr:\n                b.detach_uprobe(name=NAME, sym=SYMBOL)\n                b.detach_uretprobe(name=NAME, sym=SYMBOL)\n                b.cleanup()\n                break\n            b.perf_buffer_poll()\n        except KeyboardInterrupt:\n            exit()\n    print('IFUNC resolution of {} is performed'.format(SYMBOL))\n    print('resolver function address: {:#x}'.format(resolv_func_addr))\n    print('resolver function offset: {:#x}'.format(ifunc_symbol.offset))\n    print('function implementation address: {:#x}'.format(impl_func_addr))\n    impl_func_offset = impl_func_addr - resolv_func_addr + ifunc_symbol.offset\n    print('function implementation offset: {:#x}'.format(impl_func_offset))\n    return impl_func_offset\n\n\ndef main():\n    ifunc_symbol = get_indirect_function_sym(NAME, SYMBOL)\n    if not ifunc_symbol:\n        sys.stderr.write('{} is not an indirect function. abort!\\n'.format(SYMBOL))\n        exit(1)\n\n    impl_func_offset = find_impl_func_offset(ifunc_symbol)\n\n    b = BPF(text=HIST_BPF_TEXT)\n    b.attach_uretprobe(name=ct.cast(ifunc_symbol.module, ct.c_char_p).value,\n                       addr=impl_func_offset,\n                       fn_name=b'count')\n    dist = b['dist']\n    try:\n        while True:\n            time.sleep(1)\n            print('%-8s\\n' % time.strftime('%H:%M:%S'), end='')\n            dist.print_log2_hist(SYMBOL + ' return:')\n            dist.clear()\n\n    except KeyboardInterrupt:\n        pass\n\n\nresolv_func_addr = 0\nimpl_func_addr = 0\n\nmain()\n",
  "#\n#  Copyright 2019 The FATE Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nimport argparse\n\nimport torch as t\nfrom torch import nn\n\nfrom pipeline import fate_torch_hook\nfrom pipeline.backend.pipeline import PipeLine\nfrom pipeline.component import DataTransform\nfrom pipeline.component import Evaluation\nfrom pipeline.component import HeteroNN\nfrom pipeline.component import Intersection\nfrom pipeline.component import Reader\nfrom pipeline.interface import Data\nfrom pipeline.utils.tools import load_job_config\n\nfate_torch_hook(t)\n\n\ndef main(config=\"../../config.yaml\", namespace=\"\"):\n    # obtain config\n    if isinstance(config, str):\n        config = load_job_config(config)\n    parties = config.parties\n    guest = parties.guest[0]\n    host = parties.host[0]\n\n    guest_train_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_train_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n    guest_val_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_val_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    pipeline = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest, host=host)\n\n    reader_0 = Reader(name=\"reader_0\")\n    reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    reader_1 = Reader(name=\"reader_1\")\n    reader_1.get_party_instance(role='guest', party_id=guest).component_param(table=guest_val_data)\n    reader_1.get_party_instance(role='host', party_id=host).component_param(table=host_val_data)\n\n    data_transform_0 = DataTransform(name=\"data_transform_0\")\n    data_transform_0.get_party_instance(role='guest', party_id=guest).component_param(with_label=True)\n    data_transform_0.get_party_instance(role='host', party_id=host).component_param(with_label=False)\n\n    data_transform_1 = DataTransform(name=\"data_transform_1\")\n    data_transform_1.get_party_instance(role='guest', party_id=guest).component_param(with_label=True)\n    data_transform_1.get_party_instance(role='host', party_id=host).component_param(with_label=False)\n\n    intersection_0 = Intersection(name=\"intersection_0\")\n    intersection_1 = Intersection(name=\"intersection_1\")\n\n    hetero_nn_0 = HeteroNN(name=\"hetero_nn_0\", epochs=100,\n                           interactive_layer_lr=0.01, batch_size=-1, task_type='classification',\n                           callback_param={\n                               \"callbacks\": [\"EarlyStopping\"],\n                               \"validation_freqs\": 1,\n                               \"early_stopping_rounds\": 2,\n                               \"use_first_metric_only\": True,\n                               \"metrics\": [\"AUC\"]\n                           }\n                           )\n    guest_nn_0 = hetero_nn_0.get_party_instance(role='guest', party_id=guest)\n    host_nn_0 = hetero_nn_0.get_party_instance(role='host', party_id=host)\n\n    # define model\n    guest_bottom = t.nn.Sequential(\n        nn.Linear(10, 2),\n        nn.ReLU()\n    )\n\n    guest_top = t.nn.Sequential(\n        nn.Linear(2, 1),\n        nn.Sigmoid()\n    )\n\n    host_bottom = t.nn.Sequential(\n        nn.Linear(20, 2),\n        nn.ReLU()\n    )\n\n    # use interactive layer after fate_torch_hook\n    interactive_layer = t.nn.InteractiveLayer(out_dim=2, guest_dim=2, host_dim=2, host_num=1)\n\n    guest_nn_0.add_top_model(guest_top)\n    guest_nn_0.add_bottom_model(guest_bottom)\n    host_nn_0.add_bottom_model(host_bottom)\n\n    optimizer = t.optim.Adam(lr=0.01)  # you can initialize optimizer without parameters after fate_torch_hook\n    loss = t.nn.BCELoss()\n\n    hetero_nn_0.set_interactive_layer(interactive_layer)\n    hetero_nn_0.compile(optimizer=optimizer, loss=loss)\n\n    evaluation_0 = Evaluation(name='eval_0', eval_type='binary')\n\n    # define components IO\n    pipeline.add_component(reader_0)\n    pipeline.add_component(reader_1)\n    pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))\n    pipeline.add_component(data_transform_1, data=Data(data=reader_1.output.data))\n    pipeline.add_component(intersection_0, data=Data(data=data_transform_0.output.data))\n    pipeline.add_component(intersection_1, data=Data(data=data_transform_1.output.data))\n    pipeline.add_component(hetero_nn_0, data=Data(train_data=intersection_0.output.data,\n                                                  validate_data=intersection_1.output.data))\n    pipeline.add_component(evaluation_0, data=Data(data=hetero_nn_0.output.data))\n    pipeline.compile()\n    pipeline.fit()\n\n    print(pipeline.get_component(\"hetero_nn_0\").get_summary())\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"PIPELINE DEMO\")\n    parser.add_argument(\"-config\", type=str,\n                        help=\"config file\")\n    args = parser.parse_args()\n    if args.config is not None:\n        main(args.config)\n    else:\n        main()\n",
  "# Note: The model and training settings do not follow the reference settings\n# from the paper. The settings are chosen such that the example can easily be\n# run on a small dataset with a single GPU.\n\nimport copy\n\nimport torch\nimport torchvision\nfrom sklearn.cluster import KMeans\nfrom torch import nn\n\nfrom lightly.loss.memory_bank import MemoryBankModule\nfrom lightly.models import utils\nfrom lightly.models.modules.heads import (\n    SMoGPredictionHead,\n    SMoGProjectionHead,\n    SMoGPrototypes,\n)\nfrom lightly.transforms.smog_transform import SMoGTransform\n\n\nclass SMoGModel(nn.Module):\n    def __init__(self, backbone):\n        super().__init__()\n\n        self.backbone = backbone\n        self.projection_head = SMoGProjectionHead(512, 2048, 128)\n        self.prediction_head = SMoGPredictionHead(128, 2048, 128)\n\n        self.backbone_momentum = copy.deepcopy(self.backbone)\n        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n\n        utils.deactivate_requires_grad(self.backbone_momentum)\n        utils.deactivate_requires_grad(self.projection_head_momentum)\n\n        self.n_groups = 300\n        self.smog = SMoGPrototypes(\n            group_features=torch.rand(self.n_groups, 128), beta=0.99\n        )\n\n    def _cluster_features(self, features: torch.Tensor) -> torch.Tensor:\n        # clusters the features using sklearn\n        # (note: faiss is probably more efficient)\n        features = features.cpu().numpy()\n        kmeans = KMeans(self.n_groups).fit(features)\n        clustered = torch.from_numpy(kmeans.cluster_centers_).float()\n        clustered = torch.nn.functional.normalize(clustered, dim=1)\n        return clustered\n\n    def reset_group_features(self, memory_bank):\n        # see https://arxiv.org/pdf/2207.06167.pdf Table 7b)\n        features = memory_bank.bank\n        group_features = self._cluster_features(features.t())\n        self.smog.set_group_features(group_features)\n\n    def reset_momentum_weights(self):\n        # see https://arxiv.org/pdf/2207.06167.pdf Table 7b)\n        self.backbone_momentum = copy.deepcopy(self.backbone)\n        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n        utils.deactivate_requires_grad(self.backbone_momentum)\n        utils.deactivate_requires_grad(self.projection_head_momentum)\n\n    def forward(self, x):\n        features = self.backbone(x).flatten(start_dim=1)\n        encoded = self.projection_head(features)\n        predicted = self.prediction_head(encoded)\n        return encoded, predicted\n\n    def forward_momentum(self, x):\n        features = self.backbone_momentum(x).flatten(start_dim=1)\n        encoded = self.projection_head_momentum(features)\n        return encoded\n\n\nbatch_size = 256\n\nresnet = torchvision.models.resnet18()\nbackbone = nn.Sequential(*list(resnet.children())[:-1])\nmodel = SMoGModel(backbone)\n\n# memory bank because we reset the group features every 300 iterations\nmemory_bank_size = 300 * batch_size\nmemory_bank = MemoryBankModule(size=memory_bank_size)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\ntransform = SMoGTransform(\n    crop_sizes=(32, 32),\n    crop_counts=(1, 1),\n    gaussian_blur_probs=(0.0, 0.0),\n    crop_min_scales=(0.2, 0.2),\n    crop_max_scales=(1.0, 1.0),\n)\ndataset = torchvision.datasets.CIFAR10(\n    \"datasets/cifar10\", download=True, transform=transform\n)\n# or create a dataset from a folder containing images or videos:\n# dataset = LightlyDataset(\"path/to/folder\", transform=transform)\n\ndataloader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=256,\n    shuffle=True,\n    drop_last=True,\n    num_workers=8,\n)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(\n    model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-6\n)\n\nglobal_step = 0\n\nprint(\"Starting Training\")\nfor epoch in range(10):\n    total_loss = 0\n    for batch_idx, batch in enumerate(dataloader):\n        (x0, x1) = batch[0]\n\n        if batch_idx % 2:\n            # swap batches every second iteration\n            x1, x0 = x0, x1\n\n        x0 = x0.to(device)\n        x1 = x1.to(device)\n\n        if global_step > 0 and global_step % 300 == 0:\n            # reset group features and weights every 300 iterations\n            model.reset_group_features(memory_bank=memory_bank)\n            model.reset_momentum_weights()\n        else:\n            # update momentum\n            utils.update_momentum(model.backbone, model.backbone_momentum, 0.99)\n            utils.update_momentum(\n                model.projection_head, model.projection_head_momentum, 0.99\n            )\n\n        x0_encoded, x0_predicted = model(x0)\n        x1_encoded = model.forward_momentum(x1)\n\n        # update group features and get group assignments\n        assignments = model.smog.assign_groups(x1_encoded)\n        group_features = model.smog.get_updated_group_features(x0_encoded)\n        logits = model.smog(x0_predicted, group_features, temperature=0.1)\n        model.smog.set_group_features(group_features)\n\n        loss = criterion(logits, assignments)\n\n        # use memory bank to periodically reset the group features with k-means\n        memory_bank(x0_encoded, update=True)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        global_step += 1\n        total_loss += loss.detach()\n\n    avg_loss = total_loss / len(dataloader)\n    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
  "# -*- coding: utf-8 -*-\n\"\"\"\nThis example demonstrates many of the 2D plotting capabilities\nin pyqtgraph. All of the plots may be panned/scaled by dragging with \nthe left/right mouse buttons. Right click on any plot to show a context menu.\n\"\"\"\n\nimport initExample ## Add path to library (just for examples; you do not need this)\n\n\nfrom pyqtgraph.Qt import QtGui, QtCore\nimport numpy as np\nimport pyqtgraph as pg\n\napp = pg.mkQApp(\"Plotting Example\")\n#mw = QtGui.QMainWindow()\n#mw.resize(800,800)\n\nwin = pg.GraphicsLayoutWidget(show=True, title=\"Basic plotting examples\")\nwin.resize(1000,600)\nwin.setWindowTitle('pyqtgraph example: Plotting')\n\n# Enable antialiasing for prettier plots\npg.setConfigOptions(antialias=True)\n\np1 = win.addPlot(title=\"Basic array plotting\", y=np.random.normal(size=100))\n\np2 = win.addPlot(title=\"Multiple curves\")\np2.plot(np.random.normal(size=100), pen=(255,0,0), name=\"Red curve\")\np2.plot(np.random.normal(size=110)+5, pen=(0,255,0), name=\"Green curve\")\np2.plot(np.random.normal(size=120)+10, pen=(0,0,255), name=\"Blue curve\")\n\np3 = win.addPlot(title=\"Drawing with points\")\np3.plot(np.random.normal(size=100), pen=(200,200,200), symbolBrush=(255,0,0), symbolPen='w')\n\n\nwin.nextRow()\n\np4 = win.addPlot(title=\"Parametric, grid enabled\")\nx = np.cos(np.linspace(0, 2*np.pi, 1000))\ny = np.sin(np.linspace(0, 4*np.pi, 1000))\np4.plot(x, y)\np4.showGrid(x=True, y=True)\n\np5 = win.addPlot(title=\"Scatter plot, axis labels, log scale\")\nx = np.random.normal(size=1000) * 1e-5\ny = x*1000 + 0.005 * np.random.normal(size=1000)\ny -= y.min()-1.0\nmask = x > 1e-15\nx = x[mask]\ny = y[mask]\np5.plot(x, y, pen=None, symbol='t', symbolPen=None, symbolSize=10, symbolBrush=(100, 100, 255, 50))\np5.setLabel('left', \"Y Axis\", units='A')\np5.setLabel('bottom', \"Y Axis\", units='s')\np5.setLogMode(x=True, y=False)\n\np6 = win.addPlot(title=\"Updating plot\")\ncurve = p6.plot(pen='y')\ndata = np.random.normal(size=(10,1000))\nptr = 0\ndef update():\n    global curve, data, ptr, p6\n    curve.setData(data[ptr%10])\n    if ptr == 0:\n        p6.enableAutoRange('xy', False)  ## stop auto-scaling after the first data set is plotted\n    ptr += 1\ntimer = QtCore.QTimer()\ntimer.timeout.connect(update)\ntimer.start(50)\n\n\nwin.nextRow()\n\np7 = win.addPlot(title=\"Filled plot, axis disabled\")\ny = np.sin(np.linspace(0, 10, 1000)) + np.random.normal(size=1000, scale=0.1)\np7.plot(y, fillLevel=-0.3, brush=(50,50,200,100))\np7.showAxis('bottom', False)\n\n\nx2 = np.linspace(-100, 100, 1000)\ndata2 = np.sin(x2) / x2\np8 = win.addPlot(title=\"Region Selection\")\np8.plot(data2, pen=(255,255,255,200))\nlr = pg.LinearRegionItem([400,700])\nlr.setZValue(-10)\np8.addItem(lr)\n\np9 = win.addPlot(title=\"Zoom on selected region\")\np9.plot(data2)\ndef updatePlot():\n    p9.setXRange(*lr.getRegion(), padding=0)\ndef updateRegion():\n    lr.setRegion(p9.getViewBox().viewRange()[0])\nlr.sigRegionChanged.connect(updatePlot)\np9.sigXRangeChanged.connect(updateRegion)\nupdatePlot()\n\nif __name__ == '__main__':\n    pg.exec()\n",
  "#!/usr/bin/env python3\n#\n# This example shows how to run a combined fluid-kinetic simulation with\n# with both the hot-tail and runaway electron grids.\n#\n# Run as\n#\n#   $ ./basic.py\n#   $ ../../build/iface/dreami dream_settings.h5\n#\n# ###################################################################\n\nimport numpy as np\nimport sys\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.CollisionHandler as Collisions\nimport DREAM.Settings.Equations.DistributionFunction as DistFunc\nimport DREAM.Settings.Equations.RunawayElectrons as Runaways\nimport DREAM.Settings.Equations.RunawayElectronDistribution as REDist\n\n\nds = DREAMSettings()\n\nE = 0.6     # Electric field strength (V/m)\nn = 5e19    # Electron density (m^-3)\nT = 1e3     # Temperature (eV)\n\nre_enabled = True\n\n# Set E_field\nds.eqsys.E_field.setPrescribedData(E)\n\n# Set temperature\nds.eqsys.T_cold.setPrescribedData(T)\n\n# Set ions\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_PRESCRIBED_FULLY_IONIZED, n=n)\n\n# Disable hot-tail grid\nds.hottailgrid.setEnabled(False)\n\nds.collisions.collfreq_mode = Collisions.COLLFREQ_MODE_ULTRA_RELATIVISTIC\n#ds.collisions.collfreq_mode = Collisions.COLLFREQ_MODE_SUPERTHERMAL\n\n# Set initial hot electron Maxwellian\n#ds.eqsys.f_hot.setInitialProfiles(n0=2*n, T0=T)\n\n# Include Dreicer and avalanche\nds.eqsys.n_re.setAvalanche(Runaways.AVALANCHE_MODE_FLUID)\nds.eqsys.n_re.setDreicer(Runaways.DREICER_RATE_NEURAL_NETWORK)\n\nds.eqsys.n_re.setInitialProfile(1e15)\n\n# Disable runaway grid\npmax_re = 0.5\nif re_enabled:\n    ds.runawaygrid.setNxi(50)\n    ds.runawaygrid.setNp(100)\n    ds.runawaygrid.setPmax(pmax_re)\n\n    # Use flux limiters\n    ds.eqsys.f_re.setAdvectionInterpolationMethod(ad_int=DistFunc.AD_INTERP_TCDF)\n\n    # Set initialization method\n    ds.eqsys.f_re.setInitType(REDist.INIT_ISOTROPIC)\n\nelse:\n    ds.runawaygrid.setEnabled(False)\n\n# Set up radial grid\nds.radialgrid.setB0(5)\nds.radialgrid.setMinorRadius(0.22)\nds.radialgrid.setWallRadius(0.22)\nds.radialgrid.setNr(1)\n\n# Use the linear solver\n#ds.solver.setType(Solver.LINEAR_IMPLICIT)\nds.solver.setType(Solver.NONLINEAR)\nds.solver.setVerbose(True)\n\nds.solver.tolerance.set('j_re', reltol=1e-4)\n\nds.other.include('fluid')\n\n# Set time stepper\nds.timestep.setTmax(1e-1)\nds.timestep.setNt(20)\n\n# Save settings to HDF5 file\nds.save('dream_settings.h5')\n\n",
  "import numpy as np\nimport scipy.sparse as sps\nimport os\nimport sys\n\nfrom porepy.viz import exporter\nfrom porepy.fracs import importer\n\nfrom porepy.params import tensor\nfrom porepy.params.bc import BoundaryCondition\nfrom porepy.params.data import Parameters\n\nfrom porepy.grids import coarsening as co\n\nfrom porepy.numerics.vem import dual\nfrom porepy.numerics.fv.transport import upwind\nfrom porepy.numerics.fv import tpfa, mass_matrix\n\n#------------------------------------------------------------------------------#\n\n\ndef add_data_darcy(gb, domain, tol):\n    gb.add_node_props(['param', 'if_tangent'])\n\n    apert = 1e-2\n\n    km = 7.5 * 1e-10  # 2.5*1e-11\n\n    kf = 5 * 1e-5\n\n    for g, d in gb:\n\n        param = Parameters(g)\n        d['if_tangent'] = True\n        if g.dim == gb.dim_max():\n            kxx = km\n        else:\n            kxx = kf\n\n        perm = tensor.SecondOrder(g.dim, kxx * np.ones(g.num_cells))\n        param.set_tensor(\"flow\", perm)\n\n        param.set_source(\"flow\", np.zeros(g.num_cells))\n\n        param.set_aperture(np.power(apert, gb.dim_max() - g.dim))\n\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            top = bound_face_centers[2, :] > domain['zmax'] - tol\n            bottom = bound_face_centers[2, :] < domain['zmin'] + tol\n\n            boundary = np.logical_or(top, bottom)\n\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[boundary] = ['dir']\n\n            bc_val = np.zeros(g.num_faces)\n            p = np.abs(domain['zmax'] - domain['zmin']) * 1e3 * 9.81\n            bc_val[bound_faces[bottom]] = p\n\n            param.set_bc(\"flow\", BoundaryCondition(g, bound_faces, labels))\n            param.set_bc_val(\"flow\", bc_val)\n        else:\n            param.set_bc(\"flow\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n\n        d['param'] = param\n\n    # Assign coupling permeability\n    gb.add_edge_prop('kn')\n    for e, d in gb.edges_props():\n        g = gb.sorted_nodes_of_edge(e)[0]\n        d['kn'] = kf / gb.node_prop(g, 'param').get_aperture()\n\n#------------------------------------------------------------------------------#\n\n\ndef add_data_advection(gb, domain, tol):\n\n    for g, d in gb:\n        param = d['param']\n\n        source = np.zeros(g.num_cells)\n        param.set_source(\"transport\", source)\n\n        param.set_porosity(1)\n        param.set_discharge(d['discharge'])\n\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            top = bound_face_centers[2, :] > domain['zmax'] - tol\n            bottom = bound_face_centers[2, :] < domain['zmin'] + tol\n            boundary = np.logical_or(top, bottom)\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[boundary] = ['dir']\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[bottom]] = 1\n\n            param.set_bc(\"transport\", BoundaryCondition(\n                g, bound_faces, labels))\n            param.set_bc_val(\"transport\", bc_val)\n        else:\n            param.set_bc(\"transport\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n        d['param'] = param\n\n    # Assign coupling discharge\n    gb.add_edge_prop('param')\n    for e, d in gb.edges_props():\n        g_h = gb.sorted_nodes_of_edge(e)[1]\n        discharge = gb.node_prop(g_h, 'param').get_discharge()\n        d['param'] = Parameters(g_h)\n        d['param'].set_discharge(discharge)\n\n#------------------------------------------------------------------------------#\n\n\nsys.path.append('../../example3')\nimport soultz_grid\n\nexport_folder = 'example_5_3_coarse'\ntol = 1e-6\n\nT = 40 * np.pi * 1e7\nNt = 100\ndeltaT = T / Nt\nexport_every = 5\nif_coarse = True\n\nmesh_kwargs = {}\nmesh_kwargs['mesh_size'] = {'mode': 'constant',\n                            'value': 75,\n                            'bound_value': 200,\n                            'meshing_algorithm': 4,\n                            'tol': tol}\nmesh_kwargs['num_fracs'] = 20\nmesh_kwargs['num_points'] = 10\nmesh_kwargs['file_name'] = 'soultz_fracs'\ndomain = {'xmin': -1200, 'xmax': 500,\n          'ymin': -600,  'ymax': 600,\n          'zmin': 600,   'zmax': 5500}\nmesh_kwargs['domain'] = domain\n\nprint(\"create soultz grid\")\ngb = soultz_grid.create_grid(**mesh_kwargs)\ngb.compute_geometry()\nif if_coarse:\n    co.coarsen(gb, 'by_volume')\ngb.assign_node_ordering()\n\nprint(\"solve Darcy problem\")\nfor g, d in gb:\n    d['cell_id'] = np.arange(g.num_cells)\n\nexporter.export_vtk(gb, 'grid', ['cell_id'], folder=export_folder)\n\n# Choose and define the solvers and coupler\ndarcy = dual.DualVEMMixDim(\"flow\")\n\n# Assign parameters\nadd_data_darcy(gb, domain, tol)\n\nA, b = darcy.matrix_rhs(gb)\n\nup = sps.linalg.spsolve(A, b)\ndarcy.split(gb, \"up\", up)\n\ngb.add_node_props(['pressure', \"P0u\", \"discharge\"])\ndarcy.extract_u(gb, \"up\", \"discharge\")\ndarcy.extract_p(gb, \"up\", 'pressure')\ndarcy.project_u(gb, \"discharge\", \"P0u\")\n\n# compute the flow rate\ntotal_flow_rate = 0\nfor g, d in gb:\n    bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n    if bound_faces.size != 0:\n        bound_face_centers = g.face_centers[:, bound_faces]\n        top = bound_face_centers[2, :] > domain['zmax'] - tol\n        flow_rate = d['discharge'][bound_faces[top]]\n        total_flow_rate += np.sum(flow_rate)\n\nprint(\"total flow rate\", total_flow_rate)\nexporter.export_vtk(gb, 'darcy', ['pressure', \"P0u\"], folder=export_folder)\n\n#################################################################\n\nphysics = 'transport'\nadvection = upwind.UpwindMixedDim(physics)\nmass = mass_matrix.MassMatrixMixedDim(physics)\ninvMass = mass_matrix.InvMassMatrixMixDim(physics)\n\n# Assign parameters\nadd_data_advection(gb, domain, tol)\n\ngb.add_node_prop('deltaT', prop=deltaT)\n\nU, rhs_u = advection.matrix_rhs(gb)\nM, _ = mass.matrix_rhs(gb)\nOF = advection.outflow(gb)\nM_U = M + U\n\nrhs = rhs_u\n\n# Perform an LU factorization to speedup the solver\nIE_solver = sps.linalg.factorized((M_U).tocsc())\n\ntheta = np.zeros(rhs.shape[0])\n\n# Loop over the time\ntime = np.empty(Nt)\nfile_name = \"theta\"\ni_export = 0\nstep_to_export = np.empty(0)\n\nproduction = np.zeros(Nt)\n\nfor i in np.arange(Nt):\n    print(\"Time step\", i, \" of \", Nt)\n    # Update the solution\n    production[i] = np.sum(OF.dot(theta)) / total_flow_rate\n    theta = IE_solver(M.dot(theta) + rhs)\n\n    if i % export_every == 0:\n        print(\"Export solution at\", i)\n        advection.split(gb, \"theta\", theta)\n        exporter.export_vtk(gb, file_name, [\"theta\"], time_step=i_export,\n                            folder=export_folder)\n        step_to_export = np.r_[step_to_export, i]\n        i_export += 1\n\nexporter.export_pvd(gb, file_name, step_to_export *\n                    deltaT, folder=export_folder)\n\nnp.savetxt(export_folder + '/production.txt', (deltaT * np.arange(Nt),\n                                               np.abs(production)),\n           delimiter=',')\n",
  "#!/usr/bin/env python3\n# \nimport sys\n\nimport numpy as np\nfrom scipy.sparse.linalg import spsolve, cg, LinearOperator, spilu\nfrom scipy.sparse import spdiags\n\nimport matplotlib.pyplot as plt\n\nfrom fealpy.decorator import cartesian\nfrom fealpy.mesh import TriangleMesh\nfrom fealpy.functionspace import LagrangeFiniteElementSpace\nfrom fealpy.boundarycondition import DirichletBC, NeumannBC\n\nimport pyamg\nfrom timeit import default_timer as timer\n\nclass BoxDomain2DData():\n    def __init__(self, E=1e+5, nu=0.2):\n        self.E = E \n        self.nu = nu\n        self.lam = self.nu*self.E/((1+self.nu)*(1-2*self.nu))\n        self.mu = self.E/(2*(1+self.nu))\n\n    def domain(self):\n        return [0, 1, 0, 1]\n\n    def init_mesh(self, n=3, meshtype='tri'):\n        node = np.array([\n            (0, 0),\n            (1, 0),\n            (1, 1),\n            (0, 1)], dtype=np.float)\n        cell = np.array([(1, 2, 0), (3, 0, 2)], dtype=np.int)\n        mesh = TriangleMesh(node, cell)\n        mesh.uniform_refine(n)\n        return mesh \n\n    @cartesian\n    def displacement(self, p):\n        return 0.0\n\n    @cartesian\n    def jacobian(self, p):\n        return 0.0\n\n    @cartesian\n    def strain(self, p):\n        return 0.0\n\n    @cartesian\n    def stress(self, p):\n        return 0.0\n\n    @cartesian\n    def source(self, p):\n        val = np.array([0.0, 0.0], dtype=np.float64)\n        shape = len(p.shape[:-1])*(1, ) + (2, )\n        return val.reshape(shape)\n\n    @cartesian\n    def dirichlet(self, p):\n        val = np.array([0.0, 0.0], dtype=np.float64)\n        shape = len(p.shape[:-1])*(1, ) + (2, )\n        return val.reshape(shape)\n\n    @cartesian\n    def neumann(self, p, n):\n        val = np.array([-500, 0.0], dtype=np.float64)\n        shape = len(p.shape[:-1])*(1, ) + (2, )\n        return val.reshape(shape)\n\n    @cartesian\n    def is_dirichlet_boundary(self, p):\n        x = p[..., 0]\n        y = p[..., 1]\n        flag = np.abs(x) < 1e-13\n        return flag\n\n    @cartesian\n    def is_neumann_boundary(self, p):\n        x = p[..., 0]\n        y = p[..., 1]\n        flag = np.abs(x - 1) < 1e-13\n        return flag\n\n    @cartesian\n    def is_fracture_boundary(self, p):\n        pass\n\nclass IterationCounter(object):\n    def __init__(self, disp=True):\n        self._disp = disp\n        self.niter = 0\n    def __call__(self, rk=None):\n        self.niter += 1\n        if self._disp:\n            print('iter %3i:' % (self.niter))\n\nclass LinearElasticityLFEMFastSolver():\n    def __init__(self, A, P, isBdDof):\n        \"\"\"\n        Notes\n        -----\n\n        这里的边界条件处理放到矩阵和向量的乘积运算当中, 所心不需要修改矩阵本身\n        \"\"\"\n        self.gdof = P.shape[0]\n        self.GD = A.shape[0]//self.gdof\n\n        self.A = A\n        self.isBdDof = isBdDof\n\n        # 处理预条件子的边界条件\n        bdIdx = np.zeros(P.shape[0], dtype=np.int_)\n        bdIdx[isBdDof] = 1\n        Tbd = spdiags(bdIdx, 0, P.shape[0], P.shape[0])\n        T = spdiags(1-bdIdx, 0, P.shape[0], P.shape[0])\n        P = T@P@T + Tbd\n        self.ml = pyamg.ruge_stuben_solver(P) \n\n    def preconditioner(self, b):\n        GD = self.GD\n        b = b.reshape(GD, -1)\n        r = np.zeros_like(b)\n        for i in range(GD):\n            r[i] = self.ml.solve(b[i], tol=1e-8, accel='cg')       \n        return r.reshape(-1)\n\n    def solve(self, uh, F, tol=1e-8):\n        \"\"\"\n\n        Notes\n        -----\n        uh 是初值, uh[isBdDof] 中的值已经设为 D 氏边界条件的值, uh[~isBdDof]==0.0\n        \"\"\"\n\n        GD = self.GD\n        gdof = self.gdof\n\n        counter = IterationCounter()\n        P = LinearOperator((GD*gdof, GD*gdof), matvec=self.preconditioner)\n        uh.T.flat, info = cg(self.A, F.T.flat, x0= uh.T.flat, M=P, tol=1e-8,\n                callback=counter)\n        print(\"Convergence info:\", info)\n        print(\"Number of iteration of pcg:\", counter.niter)\n\n        return uh \n\nn = int(sys.argv[1])\np = int(sys.argv[2])\nscale = float(sys.argv[3])\n\npde = BoxDomain2DData()\n\nmesh = pde.init_mesh(n=n)\n\narea = mesh.entity_measure('cell')\n\nspace = LagrangeFiniteElementSpace(mesh, p=p)\n\nbc0 = DirichletBC(space, pde.dirichlet, threshold=pde.is_dirichlet_boundary) \nbc1 = NeumannBC(space, pde.neumann, threshold=pde.is_neumann_boundary)\n\nuh = space.function(dim=2) # (gdof, 2) and vector fem function uh[i, j] \nP = space.stiff_matrix(c=2*pde.mu)\nA = space.linear_elasticity_matrix(pde.lam, pde.mu) # (2*gdof, 2*gdof)\nF = space.source_vector(pde.source, dim=2) \nF = bc1.apply(F)\nA, F = bc0.apply(A, F, uh)\n\nprint(A.shape)\n\n\nif False:\n    uh.T.flat[:] = spsolve(A, F) # (2, gdof ).flat\nelif False:\n    N = len(F)\n    print(N)\n    ilu = spilu(A.tocsc(), drop_tol=1e-6, fill_factor=40)\n    M = LinearOperator((N, N), lambda x: ilu.solve(x))\n    start = timer()\n    uh.T.flat[:], info = cg(A, F, tol=1e-8, M=M)   # solve with CG\n    print(info)\n    end = timer()\n    print('time:', end - start)\nelse:\n    isBdDof = space.set_dirichlet_bc(pde.dirichlet, uh,\n            threshold=pde.is_dirichlet_boundary)\n    solver = LinearElasticityLFEMFastSolver(A, P, isBdDof) \n    start = timer()\n    uh[:] = solver.solve(uh, F) \n    end = timer()\n    print('time:', end - start)\n\n\n# 原始的网格\nmesh.add_plot(plt)\n\n# 变形的网格\n#mesh.node += scale*uh\n#mesh.add_plot(plt)\n#plt.show()\n",
  "\"\"\"\n.. _ref_multi_stage_cyclic_advanced:\n\nMulti-stage cyclic symmetry using advanced customization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis example shows how to expand on selected sectors the mesh and results\nfrom a multi-stage cyclic analysis. It also shows how to use the cyclic support\nfor advanced postprocessing\n\"\"\"\nfrom ansys.dpf import core as dpf\nfrom ansys.dpf.core import examples\nfrom ansys.dpf.core import operators as ops\n\n###############################################################################\n# Create the model and display the state of the result.\ncyc = examples.download_multi_stage_cyclic_result()\nmodel = dpf.Model(cyc)\nprint(model)\n\n###############################################################################\n# Check the result info to verify that it's a multi-stage model\nresult_info = model.metadata.result_info\nprint(result_info.has_cyclic)\nprint(result_info.cyclic_symmetry_type)\n\n###############################################################################\n# Go over the cyclic support\ncyc_support = result_info.cyclic_support\nprint(\"num stages:\", cyc_support.num_stages)\nprint(\"num_sectors stage 0:\", cyc_support.num_sectors(0))\nprint(\"num_sectors stage 1:\", cyc_support.num_sectors(1))\nprint(\n    \"num nodes in the first stage's base sector: \",\n    len(cyc_support.base_nodes_scoping(0)),\n)\n\n###############################################################################\n# Expand displacement results\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# This example expands displacement results on chosen sectors.\n\n\n# Create displacement cyclic operator\nUCyc = dpf.operators.result.cyclic_expanded_displacement()\nUCyc.inputs.data_sources(model.metadata.data_sources)\n# Select the sectors to expand on the first stage\nUCyc.inputs.sectors_to_expand([0, 1, 2])\n# Or select the sectors to expand stage by stage\nsectors_scopings = dpf.ScopingsContainer()\nsectors_scopings.labels = [\"stage\"]\nsectors_scopings.add_scoping({\"stage\": 0}, dpf.Scoping(ids=[0, 1, 2]))\nsectors_scopings.add_scoping({\"stage\": 1}, dpf.Scoping(ids=[0, 1, 2, 3, 4, 5, 6]))\nUCyc.inputs.sectors_to_expand(sectors_scopings)\n\n# expand the displacements and get a total deformation\nnrm = dpf.Operator(\"norm_fc\")\nnrm.inputs.connect(UCyc.outputs)\nfields = nrm.outputs.fields_container()\n\n# # get the expanded mesh\nmesh_provider = model.metadata.mesh_provider\nmesh_provider.inputs.read_cyclic(2)\nmesh = mesh_provider.outputs.mesh()\n\n###############################################################################\n# Plot the expanded result on the expanded mesh\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nmesh.plot(fields)\n\n###############################################################################\n# Choose to expand only some sectors for the mesh\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncyc_support_provider = ops.metadata.cyclic_support_provider(\n    data_sources=model.metadata.data_sources\n)\ncyc_support_provider.inputs.sectors_to_expand(sectors_scopings)\nmesh_exp = ops.metadata.cyclic_mesh_expansion(cyclic_support=cyc_support_provider)\nselected_sectors_mesh = mesh_exp.outputs.meshed_region()\n\n# # plot the expanded result on the expanded mesh\nselected_sectors_mesh.plot(fields)\n\n###############################################################################\n# Check results precisely\n# ~~~~~~~~~~~~~~~~~~~~~~~\n\n# Print the time_freq_support to see the harmonic index\nprint(model.metadata.time_freq_support)\nprint(model.metadata.time_freq_support.get_harmonic_indices(stage_num=1).data)\n\n# Harmonic index 0 means that the results are symmetric sectors by sector\n# taking a node in the base sector of the first stage\nnode_id = cyc_support.base_nodes_scoping(0)[18]\nprint(node_id)\n\n# Check what are the expanded ids of this node\nexpanded_ids = cyc_support.expand_node_id(node_id, [0, 1, 2], 0)\nprint(expanded_ids.ids)\n\n# Verify that the displacement values are the same on all those nodes\nfor node in expanded_ids.ids:\n    print(fields[0].get_entity_data_by_id(node))\n",
  "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport fastdeploy as fd\nimport cv2\nimport os\n\n\ndef parse_arguments():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--det_model\", required=True, help=\"Path of Detection model of PPOCR.\")\n    parser.add_argument(\n        \"--rec_model\",\n        required=True,\n        help=\"Path of Recognization model of PPOCR.\")\n    parser.add_argument(\n        \"--table_model\",\n        required=True,\n        help=\"Path of Table recognition model of PPOCR.\")\n    parser.add_argument(\n        \"--rec_label_file\",\n        required=True,\n        help=\"Path of Recognization model of PPOCR.\")\n    parser.add_argument(\n        \"--table_char_dict_path\",\n        type=str,\n        required=True,\n        help=\"tabel recognition dict path.\")\n    parser.add_argument(\n        \"--rec_bs\",\n        type=int,\n        default=6,\n        help=\"Recognition model inference batch size\")\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\")\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='cpu',\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\")\n    parser.add_argument(\n        \"--device_id\",\n        type=int,\n        default=0,\n        help=\"Define which GPU card used to run model.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"default\",\n        help=\"Type of inference backend, support ort/trt/paddle/openvino, default 'openvino' for cpu, 'tensorrt' for gpu\"\n    )\n\n    return parser.parse_args()\n\n\ndef build_option(args):\n    det_option = fd.RuntimeOption()\n    rec_option = fd.RuntimeOption()\n    table_option = fd.RuntimeOption()\n\n    if args.device.lower() == \"gpu\":\n        det_option.use_gpu(args.device_id)\n        rec_option.use_gpu(args.device_id)\n        table_option.use_gpu(args.device_id)\n\n    if args.backend.lower() == \"trt\":\n        assert args.device.lower(\n        ) == \"gpu\", \"TensorRT backend require inference on device GPU.\"\n        det_option.use_trt_backend()\n        rec_option.use_trt_backend()\n        table_option.use_trt_backend()\n\n        # If use TRT backend, the dynamic shape will be set as follow.\n        # We recommend that users set the length and height of the detection model to a multiple of 32.\n        # We also recommend that users set the Trt input shape as follow.\n        det_option.set_trt_input_shape(\"x\", [1, 3, 64, 64], [1, 3, 640, 640],\n                                       [1, 3, 960, 960])\n\n        rec_option.set_trt_input_shape(\"x\", [1, 3, 48, 10],\n                                       [args.rec_bs, 3, 48, 320],\n                                       [args.rec_bs, 3, 48, 2304])\n\n        table_option.set_trt_input_shape(\"x\", [1, 3, 488, 488])\n\n        # Users could save TRT cache file to disk as follow.\n        det_option.set_trt_cache_file(args.det_model + \"/det_trt_cache.trt\")\n        rec_option.set_trt_cache_file(args.rec_model + \"/rec_trt_cache.trt\")\n        table_option.set_trt_cache_file(args.table_model +\n                                        \"/table_trt_cache.trt\")\n\n    elif args.backend.lower() == \"ort\":\n        det_option.use_ort_backend()\n        rec_option.use_ort_backend()\n        table_option.use_ort_backend()\n\n    elif args.backend.lower() == \"paddle\":\n        det_option.use_paddle_infer_backend()\n        rec_option.use_paddle_infer_backend()\n        table_option.use_paddle_infer_backend()\n\n    elif args.backend.lower() == \"openvino\":\n        assert args.device.lower(\n        ) == \"cpu\", \"OpenVINO backend require inference on device CPU.\"\n        det_option.use_openvino_backend()\n        rec_option.use_openvino_backend()\n        table_option.use_openvino_backend()\n\n    return det_option, rec_option, table_option\n\n\nargs = parse_arguments()\n\ndet_model_file = os.path.join(args.det_model, \"inference.pdmodel\")\ndet_params_file = os.path.join(args.det_model, \"inference.pdiparams\")\n\nrec_model_file = os.path.join(args.rec_model, \"inference.pdmodel\")\nrec_params_file = os.path.join(args.rec_model, \"inference.pdiparams\")\nrec_label_file = args.rec_label_file\n\ntable_model_file = os.path.join(args.table_model, \"inference.pdmodel\")\ntable_params_file = os.path.join(args.table_model, \"inference.pdiparams\")\ntable_char_dict_path = args.table_char_dict_path\n\n# Set the runtime option\ndet_option, rec_option, table_option = build_option(args)\n\ndet_model = fd.vision.ocr.DBDetector(\n    det_model_file, det_params_file, runtime_option=det_option)\n\nrec_model = fd.vision.ocr.Recognizer(\n    rec_model_file, rec_params_file, rec_label_file, runtime_option=rec_option)\n\ntable_model = fd.vision.ocr.StructureV2Table(\n    table_model_file,\n    table_params_file,\n    table_char_dict_path,\n    runtime_option=table_option)\n\ndet_model.preprocessor.max_side_len = 960\ndet_model.postprocessor.det_db_thresh = 0.3\ndet_model.postprocessor.det_db_box_thresh = 0.6\ndet_model.postprocessor.det_db_unclip_ratio = 1.5\ndet_model.postprocessor.det_db_score_mode = \"slow\"\ndet_model.postprocessor.use_dilation = False\n\nppstructurev2_table = fd.vision.ocr.PPStructureV2Table(\n    det_model=det_model, rec_model=rec_model, table_model=table_model)\n\nppstructurev2_table.rec_batch_size = args.rec_bs\n\n# Read the input image\nim = cv2.imread(args.image)\n\n# Predict and reutrn the results\nresult = ppstructurev2_table.predict(im)\n\nprint(result)\n\n# Visuliaze the results.\nvis_im = fd.vision.vis_ppocr(im, result)\ncv2.imwrite(\"visualized_result.jpg\", vis_im)\nprint(\"Visualized result save in ./visualized_result.jpg\")\n",
  "#!/usr/bin/env python\n#\n# Author: Qiming Sun <osirpt.sun@gmail.com>\n#\n\n'''\nThe force from QM region acting on the background MM particles.\n'''\n\nfrom functools import reduce\nimport numpy\nfrom pyscf import gto, scf, mp, qmmm\n\nmol = gto.M(atom='''\nC       1.1879  -0.3829 0.0000\nC       0.0000  0.5526  0.0000\nO       -1.1867 -0.2472 0.0000\nH       -1.9237 0.3850  0.0000\nH       2.0985  0.2306  0.0000\nH       1.1184  -1.0093 0.8869\nH       1.1184  -1.0093 -0.8869\nH       -0.0227 1.1812  0.8852\nH       -0.0227 1.1812  -0.8852\n            ''',\n            basis='3-21g')\n\nnumpy.random.seed(1)\ncoords = numpy.random.random((5,3)) * 10\ncharges = (numpy.arange(5) + 1.) * -.1\n\ndef force(dm):\n    # The interaction between QM atoms and MM particles\n    # \\sum_K d/dR (1/|r_K-R|) = \\sum_K (r_K-R)/|r_K-R|^3\n    qm_coords = mol.atom_coords()\n    qm_charges = mol.atom_charges()\n    dr = qm_coords[:,None,:] - coords\n    r = numpy.linalg.norm(dr, axis=2)\n    g = numpy.einsum('r,R,rRx,rR->Rx', qm_charges, charges, dr, r**-3)\n\n    # The interaction between electron density and MM particles\n    # d/dR <i| (1/|r-R|) |j> = <i| d/dR (1/|r-R|) |j> = <i| -d/dr (1/|r-R|) |j>\n    #   = <d/dr i| (1/|r-R|) |j> + <i| (1/|r-R|) |d/dr j>\n    for i, q in enumerate(charges):\n        with mol.with_rinv_origin(coords[i]):\n            v = mol.intor('int1e_iprinv')\n        f =(numpy.einsum('ij,xji->x', dm, v) +\n            numpy.einsum('ij,xij->x', dm, v.conj())) * -q\n        g[i] += f\n\n    # Force = -d/dR\n    return -g\n\n# The force from HF electron density\n# Be careful with the unit of the MM particle coordinates. The gradients are\n# computed in the atomic unit.\nmf = qmmm.mm_charge(scf.RHF(mol), coords, charges, unit='Bohr').run()\ne1_mf = mf.e_tot\ndm = mf.make_rdm1()\nmm_force_mf = force(dm)\nprint('HF force:')\nprint(mm_force_mf)\n\n# Verify HF force\ncoords[0,0] += 1e-3\nmf = qmmm.mm_charge(scf.RHF(mol), coords, charges, unit='Bohr').run()\ne2_mf = mf.e_tot\nprint(-(e2_mf-e1_mf)/1e-3, '==', mm_force_mf[0,0])\n\n\n#\n# For post-HF methods, the response of HF orbitals needs to be considered in\n# the analytical gradients. It is similar to the gradients code implemented in\n# the module pyscf.grad.\n#\n# Below we use MP2 gradients as example to demonstrate how to include the\n# orbital response effects in the force for MM particles.\n#\n\n# Based on the grad_elec function in pyscf.grad.mp2\ndef make_rdm1_with_orbital_response(mp):\n    import time\n    from pyscf import lib\n    from pyscf.grad.mp2 import _response_dm1, _index_frozen_active, _shell_prange\n    from pyscf.mp import mp2\n    from pyscf.ao2mo import _ao2mo\n    log = lib.logger.new_logger(mp)\n    time0 = time.clock(), time.time()\n    mol = mp.mol\n\n    log.debug('Build mp2 rdm1 intermediates')\n    d1 = mp2._gamma1_intermediates(mp, mp.t2)\n    doo, dvv = d1\n    time1 = log.timer_debug1('rdm1 intermediates', *time0)\n\n    with_frozen = not (mp.frozen is None or mp.frozen is 0)\n    OA, VA, OF, VF = _index_frozen_active(mp.get_frozen_mask(), mp.mo_occ)\n    orbo = mp.mo_coeff[:,OA]\n    orbv = mp.mo_coeff[:,VA]\n    nao, nocc = orbo.shape\n    nvir = orbv.shape[1]\n\n# Partially transform MP2 density matrix and hold it in memory\n# The rest transformation are applied during the contraction to ERI integrals\n    part_dm2 = _ao2mo.nr_e2(mp.t2.reshape(nocc**2,nvir**2),\n                            numpy.asarray(orbv.T, order='F'), (0,nao,0,nao),\n                            's1', 's1').reshape(nocc,nocc,nao,nao)\n    part_dm2 = (part_dm2.transpose(0,2,3,1) * 4 -\n                part_dm2.transpose(0,3,2,1) * 2)\n\n    offsetdic = mol.offset_nr_by_atom()\n    diagidx = numpy.arange(nao)\n    diagidx = diagidx*(diagidx+1)//2 + diagidx\n    Imat = numpy.zeros((nao,nao))\n\n# 2e AO integrals dot 2pdm\n    max_memory = max(0, mp.max_memory - lib.current_memory()[0])\n    blksize = max(1, int(max_memory*.9e6/8/(nao**3*2.5)))\n\n    for ia in range(mol.natm):\n        shl0, shl1, p0, p1 = offsetdic[ia]\n        ip1 = p0\n        for b0, b1, nf in _shell_prange(mol, shl0, shl1, blksize):\n            ip0, ip1 = ip1, ip1 + nf\n            dm2buf = lib.einsum('pi,iqrj->pqrj', orbo[ip0:ip1], part_dm2)\n            dm2buf+= lib.einsum('qi,iprj->pqrj', orbo, part_dm2[:,ip0:ip1])\n            dm2buf = lib.einsum('pqrj,sj->pqrs', dm2buf, orbo)\n            dm2buf = dm2buf + dm2buf.transpose(0,1,3,2)\n            dm2buf = lib.pack_tril(dm2buf.reshape(-1,nao,nao)).reshape(nf,nao,-1)\n            dm2buf[:,:,diagidx] *= .5\n\n            shls_slice = (b0,b1,0,mol.nbas,0,mol.nbas,0,mol.nbas)\n            eri0 = mol.intor('int2e', aosym='s2kl', shls_slice=shls_slice)\n            Imat += lib.einsum('ipx,iqx->pq', eri0.reshape(nf,nao,-1), dm2buf)\n            eri0 = None\n            dm2buf = None\n        time1 = log.timer_debug1('2e-part grad of atom %d'%ia, *time1)\n\n# Recompute nocc, nvir to include the frozen orbitals and make contraction for\n# the 1-particle quantities, see also the kernel function in ccsd_grad module.\n    mo_coeff = mp.mo_coeff\n    mo_energy = mp._scf.mo_energy\n    nao, nmo = mo_coeff.shape\n    nocc = numpy.count_nonzero(mp.mo_occ > 0)\n    Imat = reduce(numpy.dot, (mo_coeff.T, Imat, mp._scf.get_ovlp(), mo_coeff)) * -1\n\n    dm1mo = numpy.zeros((nmo,nmo))\n    if with_frozen:\n        dco = Imat[OF[:,None],OA] / (mo_energy[OF,None] - mo_energy[OA])\n        dfv = Imat[VF[:,None],VA] / (mo_energy[VF,None] - mo_energy[VA])\n        dm1mo[OA[:,None],OA] = doo + doo.T\n        dm1mo[OF[:,None],OA] = dco\n        dm1mo[OA[:,None],OF] = dco.T\n        dm1mo[VA[:,None],VA] = dvv + dvv.T\n        dm1mo[VF[:,None],VA] = dfv\n        dm1mo[VA[:,None],VF] = dfv.T\n    else:\n        dm1mo[:nocc,:nocc] = doo + doo.T\n        dm1mo[nocc:,nocc:] = dvv + dvv.T\n\n    dm1 = reduce(numpy.dot, (mo_coeff, dm1mo, mo_coeff.T))\n    vhf = mp._scf.get_veff(mp.mol, dm1) * 2\n    Xvo = reduce(numpy.dot, (mo_coeff[:,nocc:].T, vhf, mo_coeff[:,:nocc]))\n    Xvo+= Imat[:nocc,nocc:].T - Imat[nocc:,:nocc]\n\n    dm1mo += _response_dm1(mp, Xvo)\n\n    # Transform to AO basis\n    dm1 = reduce(numpy.dot, (mo_coeff, dm1mo, mo_coeff.T))\n    dm1 += mp._scf.make_rdm1(mp.mo_coeff, mp.mo_occ)\n    return dm1\n\n\n# The force from MP2 electron density (including orbital response)\nm = mp.MP2(mf).run()\ne1_mp2 = m.e_tot\ndm = make_rdm1_with_orbital_response(m)\nmm_force_mp2 = force(dm)\nprint('MP2 force:')\nprint(mm_force_mp2)\n\n# Verify MP2 force\ncoords[0,0] += 1e-3\nmf = qmmm.mm_charge(scf.RHF(mol), coords, charges, unit='Bohr').run()\nm = mp.MP2(mf).run()\ne2_mp2 = m.e_tot\nprint(-(e2_mp2-e1_mp2)/1e-3, '==', mm_force_mp2[0,0])\n",
  "#!/usr/bin/env python\n#\n# Urwid example lazy text editor suitable for tabbed and format=flowed text\n#    Copyright (C) 2004-2009  Ian Ward\n#\n#    This library is free software; you can redistribute it and/or\n#    modify it under the terms of the GNU Lesser General Public\n#    License as published by the Free Software Foundation; either\n#    version 2.1 of the License, or (at your option) any later version.\n#\n#    This library is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n#    Lesser General Public License for more details.\n#\n#    You should have received a copy of the GNU Lesser General Public\n#    License along with this library; if not, write to the Free Software\n#    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n#\n# Urwid web site: https://urwid.org/\n\n\"\"\"\nUrwid example lazy text editor suitable for tabbed and flowing text\n\nFeatures:\n- custom list walker for lazily loading text file\n\nUsage:\nedit.py <filename>\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport sys\n\nimport urwid\n\n\nclass LineWalker(urwid.ListWalker):\n    \"\"\"ListWalker-compatible class for lazily reading file contents.\"\"\"\n\n    def __init__(self, name):\n        self.file = open(name)\n        self.lines = []\n        self.focus = 0\n\n    def get_focus(self):\n        return self._get_at_pos(self.focus)\n\n    def set_focus(self, focus):\n        self.focus = focus\n        self._modified()\n\n    def get_next(self, start_from):\n        return self._get_at_pos(start_from + 1)\n\n    def get_prev(self, start_from):\n        return self._get_at_pos(start_from - 1)\n\n    def read_next_line(self):\n        \"\"\"Read another line from the file.\"\"\"\n\n        next_line = self.file.readline()\n\n        if not next_line or next_line[-1:] != '\\n':\n            # no newline on last line of file\n            self.file = None\n        else:\n            # trim newline characters\n            next_line = next_line[:-1]\n\n        expanded = next_line.expandtabs()\n\n        edit = urwid.Edit(\"\", expanded, allow_tab=True)\n        edit.set_edit_pos(0)\n        edit.original_text = next_line\n        self.lines.append(edit)\n\n        return next_line\n\n\n    def _get_at_pos(self, pos):\n        \"\"\"Return a widget for the line number passed.\"\"\"\n\n        if pos < 0:\n            # line 0 is the start of the file, no more above\n            return None, None\n\n        if len(self.lines) > pos:\n            # we have that line so return it\n            return self.lines[pos], pos\n\n        if self.file is None:\n            # file is closed, so there are no more lines\n            return None, None\n\n        assert pos == len(self.lines), \"out of order request?\"\n\n        self.read_next_line()\n\n        return self.lines[-1], pos\n\n    def split_focus(self):\n        \"\"\"Divide the focus edit widget at the cursor location.\"\"\"\n\n        focus = self.lines[self.focus]\n        pos = focus.edit_pos\n        edit = urwid.Edit(\"\",focus.edit_text[pos:], allow_tab=True)\n        edit.original_text = \"\"\n        focus.set_edit_text(focus.edit_text[:pos])\n        edit.set_edit_pos(0)\n        self.lines.insert(self.focus+1, edit)\n\n    def combine_focus_with_prev(self):\n        \"\"\"Combine the focus edit widget with the one above.\"\"\"\n\n        above, ignore = self.get_prev(self.focus)\n        if above is None:\n            # already at the top\n            return\n\n        focus = self.lines[self.focus]\n        above.set_edit_pos(len(above.edit_text))\n        above.set_edit_text(above.edit_text + focus.edit_text)\n        del self.lines[self.focus]\n        self.focus -= 1\n\n    def combine_focus_with_next(self):\n        \"\"\"Combine the focus edit widget with the one below.\"\"\"\n\n        below, ignore = self.get_next(self.focus)\n        if below is None:\n            # already at bottom\n            return\n\n        focus = self.lines[self.focus]\n        focus.set_edit_text(focus.edit_text + below.edit_text)\n        del self.lines[self.focus+1]\n\n\nclass EditDisplay:\n    palette = [\n        ('body','default', 'default'),\n        ('foot','dark cyan', 'dark blue', 'bold'),\n        ('key','light cyan', 'dark blue', 'underline'),\n        ]\n\n    footer_text = ('foot', [\n        \"Text Editor    \",\n        ('key', \"F5\"), \" save  \",\n        ('key', \"F8\"), \" quit\",\n        ])\n\n    def __init__(self, name):\n        self.save_name = name\n        self.walker = LineWalker(name)\n        self.listbox = urwid.ListBox(self.walker)\n        self.footer = urwid.AttrWrap(urwid.Text(self.footer_text),\n            \"foot\")\n        self.view = urwid.Frame(urwid.AttrWrap(self.listbox, 'body'),\n            footer=self.footer)\n\n    def main(self):\n        self.loop = urwid.MainLoop(self.view, self.palette,\n            unhandled_input=self.unhandled_keypress)\n        self.loop.run()\n\n    def unhandled_keypress(self, k):\n        \"\"\"Last resort for keypresses.\"\"\"\n\n        if k == \"f5\":\n            self.save_file()\n        elif k == \"f8\":\n            raise urwid.ExitMainLoop()\n        elif k == \"delete\":\n            # delete at end of line\n            self.walker.combine_focus_with_next()\n        elif k == \"backspace\":\n            # backspace at beginning of line\n            self.walker.combine_focus_with_prev()\n        elif k == \"enter\":\n            # start new line\n            self.walker.split_focus()\n            # move the cursor to the new line and reset pref_col\n            self.loop.process_input([\"down\", \"home\"])\n        elif k == \"right\":\n            w, pos = self.walker.get_focus()\n            w, pos = self.walker.get_next(pos)\n            if w:\n                self.listbox.set_focus(pos, 'above')\n                self.loop.process_input([\"home\"])\n        elif k == \"left\":\n            w, pos = self.walker.get_focus()\n            w, pos = self.walker.get_prev(pos)\n            if w:\n                self.listbox.set_focus(pos, 'below')\n                self.loop.process_input([\"end\"])\n        else:\n            return\n        return True\n\n\n    def save_file(self):\n        \"\"\"Write the file out to disk.\"\"\"\n\n        l = []\n        walk = self.walker\n        for edit in walk.lines:\n            # collect the text already stored in edit widgets\n            if edit.original_text.expandtabs() == edit.edit_text:\n                l.append(edit.original_text)\n            else:\n                l.append(re_tab(edit.edit_text))\n\n        # then the rest\n        while walk.file is not None:\n            l.append(walk.read_next_line())\n\n        # write back to disk\n        outfile = open(self.save_name, \"w\")\n\n        prefix = \"\"\n        for line in l:\n            outfile.write(prefix + line)\n            prefix = \"\\n\"\n\ndef re_tab(s):\n    \"\"\"Return a tabbed string from an expanded one.\"\"\"\n    l = []\n    p = 0\n    for i in range(8, len(s), 8):\n        if s[i-2:i] == \"  \":\n            # collapse two or more spaces into a tab\n            l.append(f\"{s[p:i].rstrip()}\\t\")\n            p = i\n\n    if p == 0:\n        return s\n    else:\n        l.append(s[p:])\n        return \"\".join(l)\n\n\n\ndef main():\n    try:\n        name = sys.argv[1]\n        assert open(name, \"a\")\n    except:\n        sys.stderr.write(__doc__)\n        return\n    EditDisplay(name).main()\n\n\nif __name__==\"__main__\":\n    main()\n",
  "from datetime import date\n\nfrom shiny import App, Inputs, Outputs, Session, reactive, ui\n\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Changing the values of inputs from the server\"),\n    ui.row(\n        ui.column(\n            4,\n            ui.panel_well(\n                ui.tags.h4(\"These inputs control the other inputs on the page\"),\n                ui.input_text(\n                    \"control_label\", \"This controls some of the labels:\", \"LABEL TEXT\"\n                ),\n                ui.input_slider(\n                    \"control_num\", \"This controls values:\", min=1, max=20, value=15\n                ),\n            ),\n        ),\n        ui.column(\n            4,\n            ui.panel_well(\n                ui.tags.h4(\"These inputs are controlled by the other inputs\"),\n                ui.input_text(\"inText\", \"Text input:\", value=\"start text\"),\n                ui.input_numeric(\n                    \"inNumber\", \"Number input:\", min=1, max=20, value=5, step=0.5\n                ),\n                ui.input_numeric(\n                    \"inNumber2\", \"Number input 2:\", min=1, max=20, value=5, step=0.5\n                ),\n                ui.input_slider(\"inSlider\", \"Slider input:\", min=1, max=20, value=15),\n                ui.input_slider(\n                    \"inSlider2\", \"Slider input 2:\", min=1, max=20, value=(5, 15)\n                ),\n                ui.input_slider(\n                    \"inSlider3\", \"Slider input 3:\", min=1, max=20, value=(5, 15)\n                ),\n                ui.input_date(\"inDate\", \"Date input:\"),\n                ui.input_date_range(\"inDateRange\", \"Date range input:\"),\n            ),\n        ),\n        ui.column(\n            4,\n            ui.panel_well(\n                ui.input_checkbox(\"inCheckbox\", \"Checkbox input\", value=False),\n                ui.input_checkbox_group(\n                    \"inCheckboxGroup\",\n                    \"Checkbox group input:\",\n                    {\n                        \"option1\": \"label 1\",\n                        \"option2\": \"label 2\",\n                    },\n                ),\n                ui.input_radio_buttons(\n                    \"inRadio\",\n                    \"Radio buttons:\",\n                    {\n                        \"option1\": \"label 1\",\n                        \"option2\": \"label 2\",\n                    },\n                ),\n                ui.input_select(\n                    \"inSelect\",\n                    \"Select input:\",\n                    {\n                        \"option1\": \"label 1\",\n                        \"option2\": \"label 2\",\n                    },\n                ),\n                ui.input_select(\n                    \"inSelect2\",\n                    \"Select input 2:\",\n                    {\n                        \"option1\": \"label 1\",\n                        \"option2\": \"label 2\",\n                    },\n                    multiple=True,\n                ),\n            ),\n            ui.navset_tab(\n                ui.nav(\"panel1\", ui.h2(\"This is the first panel.\")),\n                ui.nav(\"panel2\", ui.h2(\"This is the second panel.\")),\n                id=\"inTabset\",\n            ),\n        ),\n    ),\n)\n\n\ndef server(input: Inputs, output: Outputs, session: Session):\n    @reactive.Effect\n    def _():\n        # We'll use these multiple times, so use short var names for\n        # convenience.\n        c_label = input.control_label()\n        c_num = input.control_num()\n\n        # Text =====================================================\n        # Change both the label and the text\n        ui.update_text(\n            \"inText\",\n            label=\"New \" + c_label,\n            value=\"New text \" + str(c_num),\n        )\n\n        # Number ===================================================\n        # Change the value\n        ui.update_numeric(\"inNumber\", value=c_num)\n\n        # Change the label, value, min, and max\n        ui.update_numeric(\n            \"inNumber2\",\n            label=\"Number \" + c_label,\n            value=c_num,\n            min=c_num - 10,\n            max=c_num + 10,\n            step=5,\n        )\n\n        # Slider input =============================================\n        # Only label and value can be set for slider\n        ui.update_slider(\"inSlider\", label=\"Slider \" + c_label, value=c_num)\n\n        # Slider range input =======================================\n        # For sliders that pick out a range, pass in a vector of 2\n        # values.\n        ui.update_slider(\"inSlider2\", value=(c_num - 1, c_num + 1))\n\n        # Only change the upper handle\n        ui.update_slider(\"inSlider3\", value=(input.inSlider3()[0], c_num + 2))\n\n        # Date input ===============================================\n        # Only label and value can be set for date input\n        ui.update_date(\"inDate\", label=\"Date \" + c_label, value=date(2013, 4, c_num))\n\n        # Date range input =========================================\n        # Only label and value can be set for date range input\n        ui.update_date_range(\n            \"inDateRange\",\n            label=\"Date range \" + c_label,\n            start=date(2013, 1, c_num),\n            end=date(2013, 12, c_num),\n            min=date(2001, 1, c_num),\n            max=date(2030, 1, c_num),\n        )\n\n        # # Checkbox ===============================================\n        ui.update_checkbox(\"inCheckbox\", value=c_num % 2)\n\n        # Checkbox group ===========================================\n        # Create a list of new options, where the name of the items\n        # is something like 'option label x A', and the values are\n        # 'option-x-A'.\n        opt_labels = [f\"option label {c_num} {type}\" for type in [\"A\", \"B\"]]\n        opt_vals = [f\"option-{c_num}-{type}\" for type in [\"A\", \"B\"]]\n        opts_dict = dict(zip(opt_vals, opt_labels))\n\n        # Set the label, choices, and selected item\n        ui.update_checkbox_group(\n            \"inCheckboxGroup\",\n            label=\"Checkbox group \" + c_label,\n            choices=opts_dict,\n            selected=f\"option-{c_num}-A\",\n        )\n\n        # Radio group ==============================================\n        ui.update_radio_buttons(\n            \"inRadio\",\n            label=\"Radio \" + c_label,\n            choices=opts_dict,\n            selected=f\"option-{c_num}-A\",\n        )\n        # Select input =============================================\n        # Create a list of new options, where the name of the items\n        # is something like 'option label x A', and the values are\n        # 'option-x-A'.\n        ui.update_select(\n            \"inSelect\",\n            label=\"Select \" + c_label,\n            choices=opts_dict,\n            selected=f\"option-{c_num}-A\",\n        )\n\n        # Can also set the label and select an item (or more than\n        # one if it's a multi-select)\n        ui.update_select(\n            \"inSelect2\",\n            label=\"Select label \" + c_label,\n            choices=opts_dict,\n            selected=f\"option-{c_num}-B\",\n        )\n\n        # Tabset input =============================================\n        # Change the selected tab.\n        # The tabsetPanel must have been created with an 'id' argument\n        ui.update_navs(\"inTabset\", selected=\"panel2\" if c_num % 2 else \"panel1\")\n\n\napp = App(app_ui, server, debug=True)\n",
  "#!/usr/bin/python3\n'''\nExample script to generate ftml document from glyph_data.csv and UFO.\n\nTo try this with the Harmattan font project:\n    1) clone and build Harmattan:\n        clone https://github.com/silnrsi/font-harmattan\n        cd font-harmattan\n        smith configure\n        smith build ftml\n    2) run psfgenftml as follows:\n        python3 psfgenftml.py \\\n            -t \"AllChars\" \\\n            --ap \"_?dia[AB]$\" \\\n            --xsl ../tools/lib/ftml.xsl \\\n            --scale 200 \\\n            -i source/glyph_data.csv \\\n            -s \"url(../references/Harmattan-Regular-v1.ttf)=ver 1\" \\\n            -s \"url(../results/Harmattan-Regular.ttf)=Reg-GR\" \\\n            -s \"url(../results/tests/ftml/fonts/Harmattan-Regular_ot_arab.ttf)=Reg-OT\" \\\n            source/Harmattan-Regular.ufo tests/AllChars-dev.ftml\n    3) launch resulting output file, tests/AllChars-dev.ftml, in a browser.\n        (see https://silnrsi.github.io/FDBP/en-US/Browsers%20as%20a%20font%20test%20platform.html)\n        NB: Using Firefox will allow simultaneous display of both Graphite and OpenType rendering\n    4) As above but substitute:\n            -t \"Diac Test\"             for the -t parameter\n            tests/DiacTest-dev.ftml    for the final parameter\n       and launch tests/DiacTest-dev.ftml in a browser.\n'''\n__url__ = 'https://github.com/silnrsi/pysilfont'\n__copyright__ = 'Copyright (c) 2018,2021 SIL International  (https://www.sil.org)'\n__license__ = 'Released under the MIT License (https://opensource.org/licenses/MIT)'\n__author__ = 'Bob Hallissy'\n\nimport re\nfrom silfont.core import execute\nimport silfont.ftml_builder as FB\n\nargspec = [\n    ('ifont', {'help': 'Input UFO'}, {'type': 'infont'}),\n    ('output', {'help': 'Output file ftml in XML format', 'nargs': '?'}, {'type': 'outfile', 'def': '_out.ftml'}),\n    ('-i','--input', {'help': 'Glyph info csv file'}, {'type': 'incsv', 'def': 'glyph_data.csv'}),\n    ('-f','--fontcode', {'help': 'letter to filter for glyph_data'},{}),\n    ('-l','--log', {'help': 'Set log file name'}, {'type': 'outfile', 'def': '_ftml.log'}),\n    ('--langs', {'help':'List of bcp47 language tags', 'default': None}, {}),\n    ('--rtl', {'help': 'enable right-to-left features', 'action': 'store_true'}, {}),\n    ('--norendercheck', {'help': 'do not include the RenderingUnknown check', 'action': 'store_true'}, {}),\n    ('-t', '--test', {'help': 'name of the test to generate', 'default': None}, {}),\n    ('-s','--fontsrc', {'help': 'font source: \"url()\" or \"local()\" optionally followed by \"=label\"', 'action': 'append'}, {}),\n    ('--scale', {'help': 'percentage to scale rendered text (default 100)'}, {}),\n    ('--ap', {'help': 'regular expression describing APs to examine', 'default': '.'}, {}),\n    ('-w', '--width', {'help': 'total width of all <string> column (default automatic)'}, {}),\n    ('--xsl', {'help': 'XSL stylesheet to use'}, {}),\n]\n\n\ndef doit(args):\n    logger = args.logger\n\n    # Read input csv\n    builder = FB.FTMLBuilder(logger, incsv=args.input, fontcode=args.fontcode, font=args.ifont, ap=args.ap,\n                             rtlenable=True, langs=args.langs)\n\n    # Override default base (25CC) for displaying combining marks:\n    builder.diacBase = 0x0628   # beh\n\n    # Initialize FTML document:\n    # Default name for test: AllChars or something based on the csvdata file:\n    test = args.test or 'AllChars (NG)'\n    widths = None\n    if args.width:\n        try:\n            width, units = re.match(r'(\\d+)(.*)$', args.width).groups()\n            if len(args.fontsrc):\n                width = int(round(int(width)/len(args.fontsrc)))\n            widths = {'string': f'{width}{units}'}\n            logger.log(f'width: {args.width} --> {widths[\"string\"]}', 'I')\n        except:\n            logger.log(f'Unable to parse width argument \"{args.width}\"', 'W')\n    # split labels from fontsource parameter\n    fontsrc = []\n    labels = []\n    for sl in args.fontsrc:\n        try:\n            s, l = sl.split('=',1)\n            fontsrc.append(s)\n            labels.append(l)\n        except ValueError:\n            fontsrc.append(sl)\n            labels.append(None)\n    ftml = FB.FTML(test, logger, rendercheck=not args.norendercheck, fontscale=args.scale,\n                   widths=widths, xslfn=args.xsl, fontsrc=fontsrc, fontlabel=labels, defaultrtl=args.rtl)\n\n    if test.lower().startswith(\"allchars\"):\n        # all chars that should be in the font:\n        ftml.startTestGroup('Encoded characters')\n        for uid in sorted(builder.uids()):\n            if uid < 32: continue\n            c = builder.char(uid)\n            # iterate over all permutations of feature settings that might affect this character:\n            for featlist in builder.permuteFeatures(uids = (uid,)):\n                ftml.setFeatures(featlist)\n                builder.render((uid,), ftml)\n                # Don't close test -- collect consecutive encoded chars in a single row\n            ftml.clearFeatures()\n            for langID in sorted(c.langs):\n                ftml.setLang(langID)\n                builder.render((uid,), ftml)\n            ftml.clearLang()\n\n        # Add unencoded specials and ligatures -- i.e., things with a sequence of USVs in the glyph_data:\n        ftml.startTestGroup('Specials & ligatures from glyph_data')\n        for basename in sorted(builder.specials()):\n            special = builder.special(basename)\n            # iterate over all permutations of feature settings that might affect this special\n            for featlist in builder.permuteFeatures(uids = special.uids):\n                ftml.setFeatures(featlist)\n                builder.render(special.uids, ftml)\n                # close test so each special is on its own row:\n                ftml.closeTest()\n            ftml.clearFeatures()\n            if len(special.langs):\n                for langID in sorted(special.langs):\n                    ftml.setLang(langID)\n                    builder.render(special.uids, ftml)\n                    ftml.closeTest()\n                ftml.clearLang()\n\n        # Add Lam-Alef data manually\n        ftml.startTestGroup('Lam-Alef')\n        # generate list of lam and alef characters that should be in the font:\n        lamlist = list(filter(lambda x: x in builder.uids(), (0x0644, 0x06B5, 0x06B6, 0x06B7, 0x06B8, 0x076A, 0x08A6)))\n        aleflist = list(filter(lambda x: x in builder.uids(), (0x0627, 0x0622, 0x0623, 0x0625, 0x0671, 0x0672, 0x0673, 0x0675, 0x0773, 0x0774)))\n        # iterate over all combinations:\n        for lam in lamlist:\n            for alef in aleflist:\n                for featlist in builder.permuteFeatures(uids = (lam, alef)):\n                    ftml.setFeatures(featlist)\n                    builder.render((lam,alef), ftml)\n                    # close test so each combination is on its own row:\n                    ftml.closeTest()\n                ftml.clearFeatures()\n\n    if test.lower().startswith(\"diac\"):\n        # Diac attachment:\n\n        # Representative base and diac chars:\n        repDiac = list(filter(lambda x: x in builder.uids(), (0x064E, 0x0650, 0x065E, 0x0670, 0x0616, 0x06E3, 0x08F0, 0x08F2)))\n        repBase = list(filter(lambda x: x in builder.uids(), (0x0627, 0x0628, 0x062B, 0x0647, 0x064A, 0x77F, 0x08AC)))\n\n        ftml.startTestGroup('Representative diacritics on all bases that take diacritics')\n        for uid in sorted(builder.uids()):\n            # ignore some I don't care about:\n            if uid < 32 or uid in (0xAA, 0xBA): continue\n            c = builder.char(uid)\n            # Always process Lo, but others only if that take marks:\n            if c.general == 'Lo' or c.isBase:\n                for diac in repDiac:\n                    for featlist in builder.permuteFeatures(uids = (uid,diac)):\n                        ftml.setFeatures(featlist)\n                        # Don't automatically separate connecting or mirrored forms into separate lines:\n                        builder.render((uid,diac), ftml, addBreaks = False)\n                    ftml.clearFeatures()\n                ftml.closeTest()\n\n        ftml.startTestGroup('All diacritics on representative bases')\n        for uid in sorted(builder.uids()):\n            # ignore non-ABS marks\n            if uid < 0x600 or uid in range(0xFE00, 0xFE10): continue\n            c = builder.char(uid)\n            if c.general == 'Mn':\n                for base in repBase:\n                    for featlist in builder.permuteFeatures(uids = (uid,base)):\n                        ftml.setFeatures(featlist)\n                        builder.render((base,uid), ftml, keyUID = uid, addBreaks = False)\n                    ftml.clearFeatures()\n                ftml.closeTest()\n\n        ftml.startTestGroup('Special cases')\n        builder.render((0x064A, 0x065E), ftml, comment=\"Yeh + Fatha should keep dots\")\n        builder.render((0x064A, 0x0654), ftml, comment=\"Yeh + Hamza should loose dots\")\n        ftml.closeTest()\n\n    # Write the output ftml file\n    ftml.writeFile(args.output)\n\n\ndef cmd() : execute(\"UFO\",doit,argspec)\nif __name__ == \"__main__\": cmd()\n",
  "#!/usr/bin/env python3\n#\n# This example shows how to run a combined fluid-kinetic simulation with\n# with both the hot-tail and runaway electron grids.\n#\n# Run as\n#\n#   $ ./basic.py\n#   $ ../../build/iface/dreami dream_settings.h5\n#\n# ###################################################################\n\nimport numpy as np\nimport sys\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.CollisionHandler as Collisions\nimport DREAM.Settings.Equations.DistributionFunction as DistFunc\nimport DREAM.Settings.Equations.RunawayElectrons as Runaways\nimport DREAM.Settings.TransportSettings as Transport\n\n\nds = DREAMSettings()\n\nE = 0.6     # Electric field strength (V/m)\nn = 5e19    # Electron density (m^-3)\nT = 1e3     # Temperature (eV)\n\npstar=0.5\n\nNt = 3\nNr = 11; a0=0.22\nNp = 60\nNxi= 45\n\nt_data  = np.linspace(0,1e-2,Nt)\nr_data  = np.linspace(0,a0,Nr)\np_data  = np.linspace(0.0,1.5,Np)\nxi_data = np.linspace(-1.0,1.0,Nxi)\n\n\n\nAr  = 1.0 * np.ones((Nt,Nr,Nxi,Np));    \nDrr = 1.0e-2 * np.ones((Nt,Nr,Nxi,Np))\n\n## Tests with differently set coefficients.\nAr[:,r_data<0.05,:,:]  = 0.0\nDrr[:,r_data<0.05,:,:] = 0.0\n\n\n\n\n# Enable runaways\nre_enabled = True\n\n# Set E_field\nds.eqsys.E_field.setPrescribedData(E)\n\n# Set temperature\nds.eqsys.T_cold.setPrescribedData(T)\n\n# Set ions\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_PRESCRIBED_FULLY_IONIZED, n=n)\n\n# Disable hot-tail grid\nds.hottailgrid.setEnabled(False)\n\n# Set initial hot electron Maxwellian\nds.eqsys.f_hot.setInitialProfiles(n0=2*n, T0=T)\n\n# Set up momentum grid\nds.hottailgrid.setNp(15)\nds.hottailgrid.setNxi(5)\nds.hottailgrid.setPmax(1.5)\n\n#ds.collisions.collfreq_mode = Collisions.COLLFREQ_MODE_ULTRA_RELATIVISTIC\n#ds.collisions.collfreq_mode = Collisions.COLLFREQ_MODE_SUPERTHERMAL\n\n# Include Dreicer and avalanche\nds.eqsys.n_re.setAvalanche(Runaways.AVALANCHE_MODE_FLUID)\nds.eqsys.n_re.setDreicer(Runaways.DREICER_RATE_NEURAL_NETWORK)\n\n\n# Disable runaway grid\npmax_re = 0.5\nds.runawaygrid.setEnabled(False)\n\n# Set up radial grid\nds.radialgrid.setB0(5)\nds.radialgrid.setMinorRadius(a0)\nds.radialgrid.setNr(50)\nds.radialgrid.setWallRadius(a0*1.1)\n\n# Set Svensson transport coefficients\nds.eqsys.n_re.transport.setSvenssonPstar(pstar)\n\nds.eqsys.n_re.transport.setSvenssonInterp1dParam(Transport.SVENSSON_INTERP1D_PARAM_IP)\n\nds.eqsys.n_re.transport.setSvenssonAdvection(Ar ,t=t_data,r=r_data,p=p_data,xi=xi_data)\nds.eqsys.n_re.transport.setSvenssonDiffusion(Drr,t=t_data,r=r_data,p=p_data,xi=xi_data,\n                                             #interp3d=Transport.INTERP3D_NEAREST,\n                                             interp1d=Transport.INTERP1D_LINEAR)\n\n\n# Use the linear solver\n#ds.solver.setType(Solver.LINEAR_IMPLICIT)\nds.solver.setType(Solver.NONLINEAR)\nds.solver.setVerbose(False)\n\nds.other.include('fluid')\n\n# Set time stepper\nds.timestep.setTmax(1e-3)\nds.timestep.setNt(500)\n\n# Save settings to HDF5 file\nds.save('dream_settings.h5')\n\nprint()\nprint(\"Done!\")\n",
  "# Copyright 2019 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCreates deployment, service, and ingress objects. The ingress allows external\nnetwork access to the cluster.\n\"\"\"\n\nfrom kubernetes import client, config\n\n\ndef create_deployment(apps_v1_api):\n    container = client.V1Container(\n        name=\"deployment\",\n        image=\"gcr.io/google-appengine/fluentd-logger\",\n        image_pull_policy=\"Never\",\n        ports=[client.V1ContainerPort(container_port=5678)],\n    )\n    # Template\n    template = client.V1PodTemplateSpec(\n        metadata=client.V1ObjectMeta(labels={\"app\": \"deployment\"}),\n        spec=client.V1PodSpec(containers=[container]))\n    # Spec\n    spec = client.V1DeploymentSpec(\n        replicas=1,\n        template=template)\n    # Deployment\n    deployment = client.V1Deployment(\n        api_version=\"apps/v1\",\n        kind=\"Deployment\",\n        metadata=client.V1ObjectMeta(name=\"deployment\"),\n        spec=spec)\n    # Creation of the Deployment in specified namespace\n    # (Can replace \"default\" with a namespace you may have created)\n    apps_v1_api.create_namespaced_deployment(\n        namespace=\"default\", body=deployment\n    )\n\n\ndef create_service():\n    core_v1_api = client.CoreV1Api()\n    body = client.V1Service(\n        api_version=\"v1\",\n        kind=\"Service\",\n        metadata=client.V1ObjectMeta(\n            name=\"service-example\"\n        ),\n        spec=client.V1ServiceSpec(\n            selector={\"app\": \"deployment\"},\n            ports=[client.V1ServicePort(\n                port=5678,\n                target_port=5678\n            )]\n        )\n    )\n    # Creation of the Deployment in specified namespace\n    # (Can replace \"default\" with a namespace you may have created)\n    core_v1_api.create_namespaced_service(namespace=\"default\", body=body)\n\n\ndef create_ingress(networking_v1_beta1_api):\n    body = client.NetworkingV1beta1Ingress(\n        api_version=\"networking.k8s.io/v1beta1\",\n        kind=\"Ingress\",\n        metadata=client.V1ObjectMeta(name=\"ingress-example\", annotations={\n            \"nginx.ingress.kubernetes.io/rewrite-target\": \"/\"\n        }),\n        spec=client.NetworkingV1beta1IngressSpec(\n            rules=[client.NetworkingV1beta1IngressRule(\n                host=\"example.com\",\n                http=client.NetworkingV1beta1HTTPIngressRuleValue(\n                    paths=[client.NetworkingV1beta1HTTPIngressPath(\n                        path=\"/\",\n                        backend=client.NetworkingV1beta1IngressBackend(\n                            service_port=5678,\n                            service_name=\"service-example\")\n\n                    )]\n                )\n            )\n            ]\n        )\n    )\n    # Creation of the Deployment in specified namespace\n    # (Can replace \"default\" with a namespace you may have created)\n    networking_v1_beta1_api.create_namespaced_ingress(\n        namespace=\"default\",\n        body=body\n    )\n\n\ndef main():\n    # Fetching and loading local Kubernetes Information\n    config.load_kube_config()\n    apps_v1_api = client.AppsV1Api()\n    networking_v1_beta1_api = client.NetworkingV1beta1Api()\n\n    create_deployment(apps_v1_api)\n    create_service()\n    create_ingress(networking_v1_beta1_api)\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "# doc:slow-example\n\"\"\"\n==========================================\nFrom raw data to dSPM on SPM Faces dataset\n==========================================\n\nRuns a full pipeline using MNE-Python:\n- artifact removal\n- averaging Epochs\n- forward model computation\n- source reconstruction using dSPM on the contrast : \"faces - scrambled\"\n\nNote that this example does quite a bit of processing, so even on a\nfast machine it can take about 10 minutes to complete.\n\"\"\"\n# Authors: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#          Denis Engemann <denis.engemann@gmail.com>\n#\n# License: BSD (3-clause)\n\nimport os.path as op\nimport matplotlib.pyplot as plt\n\nimport mne\nfrom mne.datasets import spm_face\nfrom mne.preprocessing import ICA, create_eog_epochs\nfrom mne import io\nfrom mne.minimum_norm import make_inverse_operator, apply_inverse\n\nprint(__doc__)\n\ndata_path = spm_face.data_path()\nsubjects_dir = data_path + '/subjects'\n\n###############################################################################\n# Load and filter data, set up epochs\n\nraw_fname = data_path + '/MEG/spm/SPM_CTF_MEG_example_faces%d_3D_raw.fif'\n\nraw = io.Raw(raw_fname % 1, preload=True)  # Take first run\n\npicks = mne.pick_types(raw.info, meg=True, exclude='bads')\nraw.filter(1, 30, method='iir')\n\nevents = mne.find_events(raw, stim_channel='UPPT001')\n\n# plot the events to get an idea of the paradigm\nmne.viz.plot_events(events, raw.info['sfreq'])\n\nevent_ids = {\"faces\": 1, \"scrambled\": 2}\n\ntmin, tmax = -0.2, 0.6\nbaseline = None  # no baseline as high-pass is applied\nreject = dict(mag=5e-12)\n\nepochs = mne.Epochs(raw, events, event_ids, tmin, tmax,  picks=picks,\n                    baseline=baseline, preload=True, reject=reject)\n\n# Fit ICA, find and remove major artifacts\nica = ICA(n_components=0.95).fit(raw, decim=6, reject=reject)\n\n# compute correlation scores, get bad indices sorted by score\neog_epochs = create_eog_epochs(raw, ch_name='MRT31-2908', reject=reject)\neog_inds, eog_scores = ica.find_bads_eog(eog_epochs, ch_name='MRT31-2908')\nica.plot_scores(eog_scores, eog_inds)  # see scores the selection is based on\nica.plot_components(eog_inds)  # view topographic sensitivity of components\nica.exclude += eog_inds[:1]  # we saw the 2nd ECG component looked too dipolar\nica.plot_overlay(eog_epochs.average())  # inspect artifact removal\nepochs_cln = ica.apply(epochs, copy=True)  # clean data, default in place\n\nevoked = [epochs_cln[k].average() for k in event_ids]\n\ncontrast = evoked[1] - evoked[0]\n\nevoked.append(contrast)\n\nfor e in evoked:\n    e.plot(ylim=dict(mag=[-400, 400]))\n\nplt.show()\n\n# estimate noise covarariance\nnoise_cov = mne.compute_covariance(epochs_cln, tmax=0)\n\n###############################################################################\n# Visualize fields on MEG helmet\n\ntrans_fname = data_path + ('/MEG/spm/SPM_CTF_MEG_example_faces1_3D_'\n                           'raw-trans.fif')\n\nmaps = mne.make_field_map(evoked[0], trans_fname, subject='spm',\n                          subjects_dir=subjects_dir, n_jobs=1)\n\nevoked[0].plot_field(maps, time=0.170)\n\n\n###############################################################################\n# Compute forward model\n\n# Make source space\nsrc_fname = data_path + '/subjects/spm/bem/spm-oct-6-src.fif'\nif not op.isfile(src_fname):\n    src = mne.setup_source_space('spm', src_fname, spacing='oct6',\n                                 subjects_dir=subjects_dir, overwrite=True)\nelse:\n    src = mne.read_source_spaces(src_fname)\n\nbem = data_path + '/subjects/spm/bem/spm-5120-5120-5120-bem-sol.fif'\nforward = mne.make_forward_solution(contrast.info, trans_fname, src, bem)\nforward = mne.convert_forward_solution(forward, surf_ori=True)\n\n###############################################################################\n# Compute inverse solution\n\nsnr = 3.0\nlambda2 = 1.0 / snr ** 2\nmethod = 'dSPM'\n\ninverse_operator = make_inverse_operator(contrast.info, forward, noise_cov,\n                                         loose=0.2, depth=0.8)\n\n# Compute inverse solution on contrast\nstc = apply_inverse(contrast, inverse_operator, lambda2, method, pick_ori=None)\n# stc.save('spm_%s_dSPM_inverse' % constrast.comment)\n\n# Plot contrast in 3D with PySurfer if available\nbrain = stc.plot(hemi='both', subjects_dir=subjects_dir)\nbrain.set_time(170.0)  # milliseconds\nbrain.show_view('ventral')\n# brain.save_image('dSPM_map.png')\n",
  "\"\"\"\nMultiphysics: HFSS-Mechanical multiphysics analysis\n---------------------------------------------------\nThis example shows how you can use PyAEDT to create a multiphysics workflow that\nincludes Circuit, HFSS, and Mechanical.\n\"\"\"\n\n###############################################################################\n# Perform required imports\n# ~~~~~~~~~~~~~~~~~~~~~~~~\n# Perform required imports.\n\nimport os\nimport pyaedt\n\n###############################################################################\n# Set non-graphical mode\n# ~~~~~~~~~~~~~~~~~~~~~~\n# Set non-graphical mode. \n# You can set ``non_graphical`` either to ``True`` or ``False``.\n\nnon_graphical = False\n\n###############################################################################\n# Download and open project\n# ~~~~~~~~~~~~~~~~~~~~~~~~~\n# Download and open the project. Save it to the temporary folder.\n\nproject_temp_name = pyaedt.downloads.download_via_wizard(pyaedt.generate_unique_folder_name())\n\n###############################################################################\n# Start HFSS\n# ~~~~~~~~~~\n# Start HFSS and initialize the PyAEDT object.\n\nversion = \"2023.2\"\nhfss = pyaedt.Hfss(projectname=project_temp_name, specified_version=version, non_graphical=non_graphical,\n                   new_desktop_session=True)\npin_names = hfss.excitations\n\n###############################################################################\n# Start Circuit\n# ~~~~~~~~~~~~~\n# Start Circuit and add the HFSS dynamic link component to it.\n\ncircuit = pyaedt.Circuit()\nhfss_comp = circuit.modeler.schematic.add_subcircuit_dynamic_link(hfss)\n\n###############################################################################\n# Set up dynamic link options\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Set up dynamic link options. The argument for the ``set_sim_option_on_hfss_subcircuit``\n# method can be the component name, component ID, or component object.\n\ncircuit.modeler.schematic.refresh_dynamic_link(hfss_comp.composed_name)\ncircuit.modeler.schematic.set_sim_option_on_hfss_subcircuit(hfss_comp)\nhfss_setup_name = hfss.setups[0].name + \" : \" + hfss.setups[0].sweeps[0].name\ncircuit.modeler.schematic.set_sim_solution_on_hfss_subcircuit(hfss_comp.composed_name, hfss_setup_name)\n\n###############################################################################\n# Create ports and excitations\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Create ports and excitations. Find component pin locations and create interface\n# ports on them. Define the voltage source on the input port.\n\ncircuit.modeler.schematic.create_interface_port(\n    name=\"Excitation_1\", location=[hfss_comp.pins[0].location[0], hfss_comp.pins[0].location[1]]\n)\ncircuit.modeler.schematic.create_interface_port(\n    name=\"Excitation_2\", location=[hfss_comp.pins[1].location[0], hfss_comp.pins[1].location[1]]\n)\ncircuit.modeler.schematic.create_interface_port(\n    name=\"Port_1\", location=[hfss_comp.pins[2].location[0], hfss_comp.pins[2].location[1]]\n)\ncircuit.modeler.schematic.create_interface_port(\n    name=\"Port_2\", location=[hfss_comp.pins[3].location[0], hfss_comp.pins[3].location[1]]\n)\n\nvoltage = 1\nphase = 0\nports_list = [\"Excitation_1\", \"Excitation_2\"]\nsource = circuit.assign_voltage_sinusoidal_excitation_to_ports(ports_list)\nsource.ac_magnitude = voltage\nsource.phase = phase\n\n###############################################################################\n# Create setup\n# ~~~~~~~~~~~~\n# Create a setup.\n\nsetup_name = \"MySetup\"\nLNA_setup = circuit.create_setup(setupname=setup_name)\nbw_start = 4.3\nbw_stop = 4.4\nn_points = 1001\nunit = \"GHz\"\nsweep_list = [\"LINC\", str(bw_start) + unit, str(bw_stop) + unit, str(n_points)]\nLNA_setup.props[\"SweepDefinition\"][\"Data\"] = \" \".join(sweep_list)\n\n###############################################################################\n# Solve and push excitations\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Solve the circuit and push excitations to the HFSS model to calculate the\n# correct value of losses.\n\ncircuit.analyze()\ncircuit.push_excitations(instance_name=\"S1\", setup_name=setup_name)\n\n\n###############################################################################\n# Start Mechanical\n# ~~~~~~~~~~~~~~~~\n# Start Mechanical and copy bodies from the HFSS project.\n\nmech = pyaedt.Mechanical()\nmech.copy_solid_bodies_from(hfss)\n\n\n###############################################################################\n# Get losses from HFSS and assign convection to Mechanical\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Get losses from HFSS and assign the convection to Mechanical.\n\nmech.assign_em_losses(\n    designname=hfss.design_name,\n    setupname=hfss.setups[0].name,\n    sweepname=\"LastAdaptive\",\n    map_frequency=hfss.setups[0].props[\"Frequency\"],\n    surface_objects=hfss.get_all_conductors_names(),\n)\ndiels = [\"1_pd\", \"2_pd\", \"3_pd\", \"4_pd\", \"5_pd\"]\nfor el in diels:\n    mech.assign_uniform_convection(objects_list=[mech.modeler[el].top_face_y, mech.modeler[el].bottom_face_y],\n                                   convection_value=3)\n\n###############################################################################\n# Plot model\n# ~~~~~~~~~~\n# Plot the model.\n\nmech.plot(show=False, export_path=os.path.join(mech.working_directory, \"Mech.jpg\"), plot_air_objects=False)\n\n###############################################################################\n# Solve and plot thermal results\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Solve and plot the thermal results.\n\nmech.create_setup()\nmech.save_project()\nmech.analyze()\nsurfaces = []\nfor name in mech.get_all_conductors_names():\n    surfaces.extend(mech.modeler.get_object_faces(name))\nmech.post.create_fieldplot_surface(objlist=surfaces, quantityName=\"Temperature\")\n\n###############################################################################\n# Release AEDT\n# ~~~~~~~~~~~~\n# Release AEDT.\n\nmech.release_desktop(True, True)\n",
  "#!/usr/bin/env python\nr\"\"\"\nIn this example we solve a stage-II coil optimisation problem: the\ngoal is to find coils that generate a specific target normal field on\na given surface.  The target equilibrium is a W7X configuration with\naverage beta of 4%. Since it is not a vacuum field, the target\nB_{External}·n is nonzero. A virtual casing calculation is used to\ncompute this target B_{External}·n.\n\nThe objective is given by\n\n    J = (1/2) ∫ |B_{BiotSavart}·n - B_{External}·n|^2 ds\n        + LENGTH_PENALTY * Σ ½(CurveLength - L0)^2\n\"\"\"\n\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom simsopt.field import BiotSavart, Current, coils_via_symmetries\nfrom simsopt.geo import CurveLength, curves_to_vtk, create_equally_spaced_curves, SurfaceRZFourier\nfrom simsopt.mhd import VirtualCasing, Vmec\nfrom simsopt.objectives import QuadraticPenalty, SquaredFlux\nfrom simsopt.util import in_github_actions\n\n# Number of unique coil shapes, i.e. the number of coils per half field period:\n# (Since the configuration has nfp = 5 and stellarator symmetry, multiply ncoils by 5 * 2 to get the total number of coils.)\nncoils = 5\n\n# Major radius for the initial circular coils:\nR0 = 5.5\n\n# Minor radius for the initial circular coils:\nR1 = 1.25\n\n# Number of Fourier modes describing each Cartesian component of each coil:\norder = 6\n\n# Weight on the curve length penalties in the objective function:\nLENGTH_PENALTY = 1e0\n\n# Number of iterations to perform:\nMAXITER = 50 if in_github_actions else 500\n\n# File for the desired boundary magnetic surface:\nTEST_DIR = (Path(__file__).parent / \"..\" / \"..\" / \"tests\" / \"test_files\").resolve()\nfilename = 'wout_W7-X_without_coil_ripple_beta0p05_d23p4_tm_reference.nc'\nvmec_file = TEST_DIR / filename\n\n# Resolution on the plasma boundary surface:\n# nphi is the number of grid points in 1/2 a field period.\nnphi = 32\nntheta = 32\n\n# Resolution for the virtual casing calculation:\nvc_src_nphi = 80\n# (For the virtual casing src_ resolution, only nphi needs to be\n# specified; the theta resolution is computed automatically to\n# minimize anisotropy of the grid.)\n\n#######################################################\n# End of input parameters.\n#######################################################\n\n# Directory for output\nout_dir = Path(\"output\")\nout_dir.mkdir(parents=True, exist_ok=True)\n\n# Once the virtual casing calculation has been run once, the results\n# can be used for many coil optimizations. Therefore here we check to\n# see if the virtual casing output file alreadys exists. If so, load\n# the results, otherwise run the virtual casing calculation and save\n# the results.\nhead, tail = os.path.split(vmec_file)\nvc_filename = os.path.join(head, tail.replace('wout', 'vcasing'))\nprint('virtual casing data file:', vc_filename)\nif os.path.isfile(vc_filename):\n    print('Loading saved virtual casing result')\n    vc = VirtualCasing.load(vc_filename)\nelse:\n    # Virtual casing must not have been run yet.\n    print('Running the virtual casing calculation')\n    vc = VirtualCasing.from_vmec(vmec_file, src_nphi=vc_src_nphi, trgt_nphi=nphi, trgt_ntheta=ntheta)\n\n# Initialize the boundary magnetic surface:\ns = SurfaceRZFourier.from_wout(vmec_file, range=\"half period\", nphi=nphi, ntheta=ntheta)\ntotal_current = Vmec(vmec_file).external_current() / (2 * s.nfp)\n\n# Create the initial coils:\nbase_curves = create_equally_spaced_curves(ncoils, s.nfp, stellsym=True, R0=R0, R1=R1, order=order, numquadpoints=128)\n# Since we know the total sum of currents, we only optimize for ncoils-1\n# currents, and then pick the last one so that they all add up to the correct\n# value.\nbase_currents = [Current(total_current / ncoils * 1e-5) * 1e5 for _ in range(ncoils-1)]\n# Above, the factors of 1e-5 and 1e5 are included so the current\n# degrees of freedom are O(1) rather than ~ MA.  The optimization\n# algorithm may not perform well if the dofs are scaled badly.\ntotal_current = Current(total_current)\ntotal_current.fix_all()\nbase_currents += [total_current - sum(base_currents)]\n\ncoils = coils_via_symmetries(base_curves, base_currents, s.nfp, True)\nbs = BiotSavart(coils)\n\nbs.set_points(s.gamma().reshape((-1, 3)))\ncurves = [c.curve for c in coils]\ncurves_to_vtk(curves, out_dir / \"curves_init\")\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(out_dir / \"surf_init\", extra_data=pointData)\n\n# Define the objective function:\nJf = SquaredFlux(s, bs, target=vc.B_external_normal)\nJls = [CurveLength(c) for c in base_curves]\n\n# Form the total objective function. To do this, we can exploit the\n# fact that Optimizable objects with J() and dJ() functions can be\n# multiplied by scalars and added:\nJF = Jf \\\n    + LENGTH_PENALTY * sum(QuadraticPenalty(Jls[i], Jls[i].J(), \"identity\") for i in range(len(base_curves)))\n\n# We don't have a general interface in SIMSOPT for optimisation problems that\n# are not in least-squares form, so we write a little wrapper function that we\n# pass directly to scipy.optimize.minimize\n\n\ndef fun(dofs):\n    JF.x = dofs\n    J = JF.J()\n    grad = JF.dJ()\n    jf = Jf.J()\n    Bbs = bs.B().reshape((nphi, ntheta, 3))\n    BdotN = np.abs(np.sum(Bbs * s.unitnormal(), axis=2) - vc.B_external_normal) / np.linalg.norm(Bbs, axis=2)\n    BdotN_mean = np.mean(BdotN)\n    BdotN_max = np.max(BdotN)\n    outstr = f\"J={J:.1e}, Jf={jf:.1e}, ⟨|B·n|⟩={BdotN_mean:.1e}, max(|B·n|)={BdotN_max:.1e}\"\n    cl_string = \", \".join([f\"{J.J():.1f}\" for J in Jls])\n    outstr += f\", Len=sum([{cl_string}])={sum(J.J() for J in Jls):.1f}\"\n    outstr += f\", ║∇J║={np.linalg.norm(grad):.1e}\"\n    print(outstr)\n    return 1e-4*J, 1e-4*grad\n\n\nprint(\"\"\"\n################################################################################\n### Perform a Taylor test ######################################################\n################################################################################\n\"\"\")\nf = fun\ndofs = JF.x\nnp.random.seed(1)\nh = np.random.uniform(size=dofs.shape)\nJ0, dJ0 = f(dofs)\ndJh = sum(dJ0 * h)\nfor eps in [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]:\n    J1, _ = f(dofs + eps*h)\n    J2, _ = f(dofs - eps*h)\n    print(\"err\", (J1-J2)/(2*eps) - dJh)\n\nprint(\"\"\"\n################################################################################\n### Run the optimisation #######################################################\n################################################################################\n\"\"\")\nres = minimize(fun, dofs, jac=True, method='L-BFGS-B', options={'maxiter': MAXITER, 'maxcor': 300, 'ftol': 1e-20, 'gtol': 1e-20}, tol=1e-20)\ndofs = res.x\ncurves_to_vtk(curves, out_dir / \"curves_opt\")\nBbs = bs.B().reshape((nphi, ntheta, 3))\nBdotN = np.abs(np.sum(Bbs * s.unitnormal(), axis=2) - vc.B_external_normal) / np.linalg.norm(Bbs, axis=2)\npointData = {\"B_N\": BdotN[:, :, None]}\ns.to_vtk(out_dir / \"surf_opt\", extra_data=pointData)\n"
]